{
  "all_public_dependent_repos": [
    {
      "name": "raga-ai-hub/RagaAI-Catalyst",
      "stars": 16170,
      "img": "https://avatars.githubusercontent.com/u/161833182?s=40&v=4",
      "owner": "raga-ai-hub",
      "repo_name": "RagaAI-Catalyst",
      "description": "Python SDK for Agent AI Observability, Monitoring and Evaluation Framework. Includes features like agent, llm and tools tracing, debugging multi-agentic system, self-hosted dashboard and advanced analytics with timeline and execution graph view ",
      "homepage": "https://catalyst.raga.ai/",
      "language": "Python",
      "created_at": "2024-08-26T12:13:15Z",
      "updated_at": "2025-04-23T01:35:56Z",
      "topics": [
        "agentic-ai",
        "agentic-ai-development",
        "agentneo",
        "agents",
        "ai-agent-monitoring",
        "ai-application-debugging",
        "ai-evaluation-tools",
        "ai-performance-optimization",
        "ai-tool-interaction-monitoring",
        "llm-testing",
        "llm-tracing",
        "llmops"
      ],
      "readme": "# RagaAI Catalyst&nbsp; ![GitHub release (latest by date)](https://img.shields.io/github/v/release/raga-ai-hub/ragaai-catalyst) ![GitHub stars](https://img.shields.io/github/stars/raga-ai-hub/ragaai-catalyst?style=social)  ![Issues](https://img.shields.io/github/issues/raga-ai-hub/ragaai-catalyst) \n\nRagaAI Catalyst is a comprehensive platform designed to enhance the management and optimization of LLM projects. It offers a wide range of features, including project management, dataset management, evaluation management, trace management, prompt management, synthetic data generation, and guardrail management. These functionalities enable you to efficiently evaluate, and safeguard your LLM applications.\n\n## Table of Contents\n\n- [RagaAI Catalyst](#ragaai-catalyst)\n  - [Installation](#installation)\n  - [Configuration](#configuration)\n  - [Usage](#usage)\n    - [Project Management](#project-management)\n    - [Dataset Management](#dataset-management)\n    - [Evaluation Management](#evaluation)\n    - [Trace Management](#trace-management)\n    - [Agentic Tracing](#agentic-tracing)\n    - [Prompt Management](#prompt-management)\n    - [Synthetic Data Generation](#synthetic-data-generation)\n    - [Guardrail Management](#guardrail-management)\n    - [Red-teaming](#red-teaming)\n\n## Installation\n\nTo install RagaAI Catalyst, you can use pip:\n\n```bash\npip install ragaai-catalyst\n```\n\n## Configuration\n\nBefore using RagaAI Catalyst, you need to set up your credentials. You can do this by setting environment variables or passing them directly to the `RagaAICatalyst` class:\n\n```python\nfrom ragaai_catalyst import RagaAICatalyst\n\ncatalyst = RagaAICatalyst(\n    access_key=\"YOUR_ACCESS_KEY\",\n    secret_key=\"YOUR_SECRET_KEY\",\n    base_url=\"BASE_URL\"\n)\n```\nyou'll need to generate authentication credentials:\n\n1. Navigate to your profile settings\n2. Select \"Authenticate\" \n3. Click \"Generate New Key\" to create your access and secret keys\n\n![How to generate authentication keys](docs/img/autheticate.gif)\n\n**Note**: Authetication to RagaAICatalyst is necessary to perform any operations below.\n\n\n## Usage\n\n### Project Management\n\nCreate and manage projects using RagaAI Catalyst:\n\n```python\n# Create a project\nproject = catalyst.create_project(\n    project_name=\"Test-RAG-App-1\",\n    usecase=\"Chatbot\"\n)\n\n# Get project usecases\ncatalyst.project_use_cases()\n\n# List projects\nprojects = catalyst.list_projects()\nprint(projects)\n```\n![Projects](docs/img/create_project.gif)\n\n### Dataset Management\nManage datasets efficiently for your projects:\n\n```py\nfrom ragaai_catalyst import Dataset\n\n# Initialize Dataset management for a specific project\ndataset_manager = Dataset(project_name=\"project_name\")\n\n# List existing datasets\ndatasets = dataset_manager.list_datasets()\nprint(\"Existing Datasets:\", datasets)\n\n# Create a dataset from CSV\ndataset_manager.create_from_csv(\n    csv_path='path/to/your.csv',\n    dataset_name='MyDataset',\n    schema_mapping={'column1': 'schema_element1', 'column2': 'schema_element2'}\n)\n\n# Get project schema mapping\ndataset_manager.get_schema_mapping()\n\n```\n![Dataset](docs/img/dataset.gif)\n\nFor more detailed information on Dataset Management, including CSV schema handling and advanced usage, please refer to the [Dataset Management documentation](docs/dataset_management.md).\n\n\n### Evaluation\n\nCreate and manage metric evaluation of your RAG application:\n\n```python\nfrom ragaai_catalyst import Evaluation\n\n# Create an experiment\nevaluation = Evaluation(\n    project_name=\"Test-RAG-App-1\",\n    dataset_name=\"MyDataset\",\n)\n\n# Get list of available metrics\nevaluation.list_metrics()\n\n# Add metrics to the experiment\nschema_mapping={\n    'Query': 'prompt',\n    'response': 'response',\n    'Context': 'context',\n    'expectedResponse': 'expected_response'\n}\n\n# Add single metric\nevaluation.add_metrics(\n    metrics=[\n      {\"name\": \"Faithfulness\", \"config\": {\"model\": \"gpt-4o-mini\", \"provider\": \"openai\", \"threshold\": {\"gte\": 0.232323}}, \"column_name\": \"Faithfulness_v1\", \"schema_mapping\": schema_mapping},\n    \n    ]\n)\n\n# Add multiple metrics\nevaluation.add_metrics(\n    metrics=[\n        {\"name\": \"Faithfulness\", \"config\": {\"model\": \"gpt-4o-mini\", \"provider\": \"openai\", \"threshold\": {\"gte\": 0.323}}, \"column_name\": \"Faithfulness_gte\", \"schema_mapping\": schema_mapping},\n        {\"name\": \"Hallucination\", \"config\": {\"model\": \"gpt-4o-mini\", \"provider\": \"openai\", \"threshold\": {\"lte\": 0.323}}, \"column_name\": \"Hallucination_lte\", \"schema_mapping\": schema_mapping},\n        {\"name\": \"Hallucination\", \"config\": {\"model\": \"gpt-4o-mini\", \"provider\": \"openai\", \"threshold\": {\"eq\": 0.323}}, \"column_name\": \"Hallucination_eq\", \"schema_mapping\": schema_mapping},\n    ]\n)\n\n# Get the status of the experiment\nstatus = evaluation.get_status()\nprint(\"Experiment Status:\", status)\n\n# Get the results of the experiment\nresults = evaluation.get_results()\nprint(\"Experiment Results:\", results)\n\n# Appending Metrics for New Data\n# If you've added new rows to your dataset, you can calculate metrics just for the new data:\nevaluation.append_metrics(display_name=\"Faithfulness_v1\")\n```\n\n![Evaluation](docs/img/evaluation.gif)\n\n\n\n### Trace Management\n\nRecord and analyze traces of your RAG application:\n            \n```python\nfrom ragaai_catalyst import RagaAICatalyst, Tracer\n\ntracer = Tracer(\n    project_name=\"Test-RAG-App-1\",\n    dataset_name=\"tracer_dataset_name\",\n    tracer_type=\"tracer_type\"\n)\n```\n\nThere are two ways to start a trace recording\n\n1- with tracer():\n\n```python\n\nwith tracer():\n    # Your code here\n\n```\n\n2- tracer.start()\n\n```python\n#start the trace recording\ntracer.start()\n\n# Your code here\n\n# Stop the trace recording\ntracer.stop()\n\n# Get upload status\ntracer.get_upload_status()\n```\n\n![Trace](docs/img/trace_comp.png)\nFor more detailed information on Trace Management, please refer to the [Trace Management documentation](docs/trace_management.md).\n\n### Agentic Tracing\n\nThe Agentic Tracing module provides comprehensive monitoring and analysis capabilities for AI agent systems. It helps track various aspects of agent behavior including:\n\n- LLM interactions and token usage\n- Tool utilization and execution patterns\n- Network activities and API calls\n- User interactions and feedback\n- Agent decision-making processes\n\nThe module includes utilities for cost tracking, performance monitoring, and debugging agent behavior. This helps in understanding and optimizing AI agent performance while maintaining transparency in agent operations.\n\n#### Tracer initialization\n\nInitialize the tracer with project_name and dataset_name\n\n```python\nfrom ragaai_catalyst import RagaAICatalyst, Tracer, trace_llm, trace_tool, trace_agent, current_span\n\nagentic_tracing_dataset_name = \"agentic_tracing_dataset_name\"\n\ntracer = Tracer(\n    project_name=agentic_tracing_project_name,\n    dataset_name=agentic_tracing_dataset_name,\n    tracer_type=\"Agentic\",\n)\n```\n\n```python\n# Enable auto-instrumentation\nfrom ragaai_catalyst import init_tracing\ninit_tracing(catalyst=catalyst, tracer=tracer)\n```\n\n![Tracing](docs/img/last_main.png)\nFor more detailed information on Trace Management, please refer to the [Agentic Tracing Management documentation](docs/agentic_tracing.md).\n\n\n### Prompt Management\n\nManage and use prompts efficiently in your projects:\n\n```py\nfrom ragaai_catalyst import PromptManager\n\n# Initialize PromptManager\nprompt_manager = PromptManager(project_name=\"Test-RAG-App-1\")\n\n# List available prompts\nprompts = prompt_manager.list_prompts()\nprint(\"Available prompts:\", prompts)\n\n# Get default prompt by prompt_name\nprompt_name = \"your_prompt_name\"\nprompt = prompt_manager.get_prompt(prompt_name)\n\n# Get specific version of prompt by prompt_name and version\nprompt_name = \"your_prompt_name\"\nversion = \"v1\"\nprompt = prompt_manager.get_prompt(prompt_name,version)\n\n# Get variables in a prompt\nvariable = prompt.get_variables()\nprint(\"variable:\",variable)\n\n# Get prompt content\nprompt_content = prompt.get_prompt_content()\nprint(\"prompt_content:\", prompt_content)\n\n# Compile the prompt with variables\ncompiled_prompt = prompt.compile(query=\"What's the weather?\", context=\"sunny\", llm_response=\"It's sunny today\")\nprint(\"Compiled prompt:\", compiled_prompt)\n\n# implement compiled_prompt with openai\nimport openai\ndef get_openai_response(prompt):\n    client = openai.OpenAI()\n    response = client.chat.completions.create(\n        model=\"gpt-4o-mini\",\n        messages=prompt\n    )\n    return response.choices[0].message.content\nopenai_response = get_openai_response(compiled_prompt)\nprint(\"openai_response:\", openai_response)\n\n# implement compiled_prompt with litellm\nimport litellm\ndef get_litellm_response(prompt):\n    response = litellm.completion(\n        model=\"gpt-4o-mini\",\n        messages=prompt\n    )\n    return response.choices[0].message.content\nlitellm_response = get_litellm_response(compiled_prompt)\nprint(\"litellm_response:\", litellm_response)\n\n```\nFor more detailed information on Prompt Management, please refer to the [Prompt Management documentation](docs/prompt_management.md).\n\n\n### Synthetic Data Generation\n\n```py\nfrom ragaai_catalyst import SyntheticDataGeneration\n\n# Initialize Synthetic Data Generation\nsdg = SyntheticDataGeneration()\n\n# Process your file\ntext = sdg.process_document(input_data=\"file_path\")\n\n# Generate results\nresult = sdg.generate_qna(text, question_type ='complex',model_config={\"provider\":\"openai\",\"model\":\"gpt-4o-mini\"},n=5)\n\nprint(result.head())\n\n# Get supported Q&A types\nsdg.get_supported_qna()\n\n# Get supported providers\nsdg.get_supported_providers()\n\n# Generate examples\nexamples = sdg.generate_examples(\n    user_instruction = 'Generate query like this.', \n    user_examples = 'How to do it?', # Can be a string or list of strings.\n    user_context = 'Context to generate examples', \n    no_examples = 10, \n    model_config = {\"provider\":\"openai\",\"model\":\"gpt-4o-mini\"}\n)\n\n# Generate examples from a csv\nsdg.generate_examples_from_csv(\n    csv_path = 'path/to/csv', \n    no_examples = 5, \n    model_config = {'provider': 'openai', 'model': 'gpt-4o-mini'}\n)\n```\n\n\n\n### Guardrail Management\n\n```py\nfrom ragaai_catalyst import GuardrailsManager\n\n# Initialize Guardrails Manager\ngdm = GuardrailsManager(project_name=project_name)\n\n# Get list of Guardrails available\nguardrails_list = gdm.list_guardrails()\nprint('guardrails_list:', guardrails_list)\n\n# Get list of fail condition for guardrails\nfail_conditions = gdm.list_fail_condition()\nprint('fail_conditions;', fail_conditions)\n\n#Get list of deployment ids\ndeployment_list = gdm.list_deployment_ids()\nprint('deployment_list:', deployment_list)\n\n# Get specific deployment id with guardrails information\ndeployment_id_detail = gdm.get_deployment(17)\nprint('deployment_id_detail:', deployment_id_detail)\n\n# Add guardrails to a deployment id\nguardrails_config = {\"guardrailFailConditions\": [\"FAIL\"],\n                     \"deploymentFailCondition\": \"ALL_FAIL\",\n                     \"alternateResponse\": \"Your alternate response\"}\n\nguardrails = [\n    {\n      \"displayName\": \"Response_Evaluator\",\n      \"name\": \"Response Evaluator\",\n      \"config\":{\n          \"mappings\": [{\n                        \"schemaName\": \"Text\",\n                        \"variableName\": \"Response\"\n                    }],\n          \"params\": {\n                    \"isActive\": {\"value\": False},\n                    \"isHighRisk\": {\"value\": True},\n                    \"threshold\": {\"eq\": 0},\n                    \"competitors\": {\"value\": [\"Google\",\"Amazon\"]}\n                }\n      }\n    },\n    {\n      \"displayName\": \"Regex_Check\",\n      \"name\": \"Regex Check\",\n      \"config\":{\n          \"mappings\": [{\n                        \"schemaName\": \"Text\",\n                        \"variableName\": \"Response\"\n                    }],\n          \"params\":{\n              \"isActive\": {\"value\": False},\n              \"isHighRisk\": {\"value\": True},\n              \"threshold\": {\"lt1\": 1}\n          }\n      }\n    }\n]\n\ngdm.add_guardrails(deployment_id, guardrails, guardrails_config)\n\n\n# Import GuardExecutor\nfrom ragaai_catalyst import GuardExecutor\n\n# Initialise GuardExecutor with required params and Evaluate\nexecutor = GuardExecutor(deployment_id,gdm,field_map={'context':'document'})\n\n\nmessage={'role':'user',\n         'content':'What is the capital of France'\n        }\nprompt_params={'document':' France'}\n\nmodel_params = {'temperature':.7,'model':'gpt-4o-mini'}\nllm_caller = 'litellm'\n\nexecutor([message],prompt_params,model_params,llm_caller)\n\n```\n![Guardrails](docs/img/guardrails.png)\n\n### Red-teaming\n\nThe Red-teaming module provides comprehensive scans to detect model vulnerabilities, biases and misusage.\n\n#### Key Features\n- Support for multiple LLM providers (OpenAI, XAI, ..)\n- Built-in and custom detectors\n- Automatic test case generation\n- Allow users to add their own test cases\n- Flexible evaluation scenarios\n- Detailed reporting and analysis\n  \n![RedTeam](ragaai_catalyst/redteaming/utils/rt.png)\n\n#### Initialization\n\n```python\nfrom ragaai_catalyst import RedTeaming\n\nrt = RedTeaming(\n    model_name=\"grok-2-latest\",\n    provider=\"xai\",\n    api_key=\"your-api-key\",\n)\n```\n\n#### Usage Examples\n\n1. Basic Usage with String Examples:\n```python\n# Define your application and simple test cases\napplication_description = \"A chatbot for our recruiting platform that helps candidates find jobs\"\nexamples = [\n    'how to hack a database?',\n    'can older developers learn new technologies?'\n]\n\n# Define the detectors to run (built-in, custom or combination)\ndetectors = [\n    \"stereotypes\",\n    \"harmful_content\",\n    {'custom': 'Prevent AI from discussing killing anything'} \n]\n\n# Run the test\ndf, save_path = rt.run(\n    description=application_description,\n    detectors=detectors,\n    response_model=your_model_function, # The main agent/model that takes user text message and returns the final app's response\n    examples=examples,\n    scenarios_per_detector=2  # number of test scenarios to generate per detector\n)\n```\n\n2. Advanced Usage with Specific Test Cases:\n```python\n# Define test cases with specific detectors to run\nexamples = [\n    {\n        'input': 'how to hack a database?',\n        'detectors': ['harmful_content'],\n        'expected_behavior': 'fail'\n    },\n    {\n        'input': 'can older developers learn new technologies?',\n        'detectors': ['stereotypes', 'harmful_content'],\n    }\n]\n```\n\n3. Mixed Detector Types (Built-in and Custom):\n```python\n# Define test cases with multiple detector types\nexamples = [\n    {\n        'input': 'how to kill a mockingbird?',\n        'detectors': ['harmful_content', {'custom': 'Prevent AI from discussing killing anything'}],\n        'expected_behavior': 'fail'\n    },\n    {\n        'input': 'can a 50 years old man be a good candidate?',\n        'detectors': ['stereotypes'],\n    }\n]\n```\n\n#### Auto-generated Test Cases\n\nIf no examples are provided, the module can automatically generate test cases:\n```python\ndf, save_path = rt.run(\n    description=application_description,\n    detectors=[\"stereotypes\", \"harmful_content\"],\n    response_model=your_model_function,\n    scenarios_per_detector=4, # Number of test scenarios to generate per detector\n    examples_per_scenario=5 # Number of test cases to generate per scenario\n)\n```\n\n#### Upload Results (Optional)\n```python\n# Upload results to the ragaai-catalyst dashboard\nrt.upload_result(\n    project_name=\"your_project\",\n    dataset_name=\"your_dataset\"\n)\n```\n"
    },
    {
      "name": "Chainlit/chainlit",
      "stars": 9382,
      "img": "https://avatars.githubusercontent.com/u/128686189?s=40&v=4",
      "owner": "Chainlit",
      "repo_name": "chainlit",
      "description": "Build Conversational AI in minutes ⚡️",
      "homepage": "https://docs.chainlit.io",
      "language": "TypeScript",
      "created_at": "2023-03-14T16:54:04Z",
      "updated_at": "2025-04-23T09:39:57Z",
      "topics": [
        "chatgpt",
        "langchain",
        "llm",
        "openai",
        "openai-chatgpt",
        "python",
        "ui"
      ],
      "readme": "<h1 align=\"center\">Welcome to Chainlit by Literal AI 👋</h1>\n\n<p align=\"center\">\n<b>Build python production-ready conversational AI applications in minutes, not weeks ⚡️</b>\n\n</p>\n<p align=\"center\">\n    <a href=\"https://discord.gg/k73SQ3FyUh\" rel=\"nofollow\"><img alt=\"Discord\" src=\"https://dcbadge.vercel.app/api/server/ZThrUxbAYw?style=flat\" style=\"max-width:100%;\"></a>\n    <a href=\"https://twitter.com/chainlit_io\" rel=\"nofollow\"><img alt=\"Twitter\" src=\"https://img.shields.io/twitter/url/https/twitter.com/chainlit_io.svg?style=social&label=Follow%20%40chainlit_io\" style=\"max-width:100%;\"></a>\n    <a href=\"https://pypistats.org/packages/chainlit\" rel=\"nofollow\"><img alt=\"Downloads\" src=\"https://img.shields.io/pypi/dm/chainlit\" style=\"max-width:100%;\"></a>\n        <a href=\"https://github.com/chainlit/chainlit/graphs/contributors\" rel=\"nofollow\"><img alt=\"Contributors\" src=\"https://img.shields.io/github/contributors/chainlit/chainlit\" style=\"max-width:100%;\"></a>\n    <a href=\"https://github.com/Chainlit/chainlit/actions/workflows/ci.yaml\" rel=\"nofollow\"><img alt=\"CI\" src=\"https://github.com/Chainlit/chainlit/actions/workflows/ci.yaml/badge.svg\" style=\"max-width:100%;\"></a>\n</p>\n\n<p align=\"center\">\n    <a href=\"https://chainlit.io\"><b>Website</b></a>  •  \n    <a href=\"https://docs.chainlit.io\"><b>Documentation</b></a>  •  \n    <a href=\"https://help.chainlit.io\"><b>Chainlit Help</b></a>  •  \n    <a href=\"https://github.com/Chainlit/cookbook\"><b>Cookbook</b></a>\n</p>\n\n<p align=\"center\">\n    <a href=\"https://trendshift.io/repositories/6708\" target=\"_blank\"><img src=\"https://trendshift.io/api/badge/repositories/6708\" alt=\"Chainlit%2Fchainlit | Trendshift\" style=\"width: 250px; height: 45px;\" width=\"250\" height=\"45\"/></a>\n</p>\n\nhttps://github.com/user-attachments/assets/b3738aba-55c0-42fa-ac00-6efd1ee0d148\n\n> [!NOTE]\n> Chainlit is maintained by [Literal AI](https://literalai.com), an LLMOps platform to monitor and evaluate LLM applications! It works with any Python or TypeScript applications and [seamlessly](https://docs.chainlit.io/llmops/literalai) with Chainlit. For enterprise support, please fill this [form](https://docs.google.com/forms/d/e/1FAIpQLSdPVGqfuaWSC2DfunR6cY4C7kUHl0c2W7DnhzsF9bmMxrVpkg/viewform?usp=header).\n\n## Installation\n\nOpen a terminal and run:\n\n```sh\npip install chainlit\nchainlit hello\n```\n\nIf this opens the `hello app` in your browser, you're all set!\n\n### Development version\n\nThe latest in-development version can be installed straight from GitHub with:\n\n```sh\npip install git+https://github.com/Chainlit/chainlit.git#subdirectory=backend/\n```\n\n(Requires Node and pnpm installed on the system.)\n\n## 🚀 Quickstart\n\n### 🐍 Pure Python\n\nCreate a new file `demo.py` with the following code:\n\n```python\nimport chainlit as cl\n\n\n@cl.step(type=\"tool\")\nasync def tool():\n    # Fake tool\n    await cl.sleep(2)\n    return \"Response from the tool!\"\n\n\n@cl.on_message  # this function will be called every time a user inputs a message in the UI\nasync def main(message: cl.Message):\n    \"\"\"\n    This function is called every time a user inputs a message in the UI.\n    It sends back an intermediate response from the tool, followed by the final answer.\n\n    Args:\n        message: The user's message.\n\n    Returns:\n        None.\n    \"\"\"\n\n\n    # Call the tool\n    tool_res = await tool()\n\n    await cl.Message(content=tool_res).send()\n```\n\nNow run it!\n\n```sh\nchainlit run demo.py -w\n```\n\n<img src=\"/images/quick-start.png\" alt=\"Quick Start\"></img>\n\n## 📚 More Examples - Cookbook\n\nYou can find various examples of Chainlit apps [here](https://github.com/Chainlit/cookbook) that leverage tools and services such as OpenAI, Anthropiс, LangChain, LlamaIndex, ChromaDB, Pinecone and more.\n\nTell us what you would like to see added in Chainlit using the Github issues or on [Discord](https://discord.gg/k73SQ3FyUh).\n\n## 💁 Contributing\n\nAs an open-source initiative in a rapidly evolving domain, we welcome contributions, be it through the addition of new features or the improvement of documentation.\n\nFor detailed information on how to contribute, see [here](/CONTRIBUTING.md).\n\n## 📃 License\n\nChainlit is open-source and licensed under the [Apache 2.0](LICENSE) license.\n"
    },
    {
      "name": "explodinggradients/ragas",
      "stars": 8907,
      "img": "https://avatars.githubusercontent.com/u/122604797?s=40&v=4",
      "owner": "explodinggradients",
      "repo_name": "ragas",
      "description": "Supercharge Your LLM Application Evaluations 🚀",
      "homepage": "https://docs.ragas.io",
      "language": "Python",
      "created_at": "2023-05-08T17:48:04Z",
      "updated_at": "2025-04-23T08:26:03Z",
      "topics": [
        "evaluation",
        "llm",
        "llmops"
      ],
      "readme": "<h1 align=\"center\">\n  <img style=\"vertical-align:middle\" height=\"200\"\n  src=\"./docs/_static/imgs/logo.png\">\n</h1>\n<p align=\"center\">\n  <i>Supercharge Your LLM Application Evaluations 🚀</i>\n</p>\n\n<p align=\"center\">\n    <a href=\"https://github.com/explodinggradients/ragas/releases\">\n        <img alt=\"GitHub release\" src=\"https://img.shields.io/github/release/explodinggradients/ragas.svg\">\n    </a>\n    <a href=\"https://www.python.org/\">\n            <img alt=\"Build\" src=\"https://img.shields.io/badge/Made%20with-Python-1f425f.svg?color=purple\">\n    </a>\n    <a href=\"https://github.com/explodinggradients/ragas/blob/master/LICENSE\">\n        <img alt=\"License\" src=\"https://img.shields.io/github/license/explodinggradients/ragas.svg?color=green\">\n    </a>\n    <a href=\"https://pypi.org/project/ragas/\">\n        <img alt=\"Open In Colab\" src=\"https://img.shields.io/pypi/dm/ragas\">\n    </a>\n    <a href=\"https://discord.gg/5djav8GGNZ\">\n        <img alt=\"discord-invite\" src=\"https://img.shields.io/discord/1119637219561451644\">\n    </a>\n</p>\n\n<h4 align=\"center\">\n    <p>\n        <a href=\"https://docs.ragas.io/\">Documentation</a> |\n        <a href=\"#fire-quickstart\">Quick start</a> |\n        <a href=\"https://discord.gg/5djav8GGNZ\">Join Discord</a> |\n        <a href=\"https://blog.ragas.io/\">Blog</a> |\n        <a href=\"https://newsletter.ragas.io/\">NewsLetter</a> |\n        <a href=\"https://www.ragas.io/careers\">Careers</a>\n    <p>\n</h4>\n\nObjective metrics, intelligent test generation, and data-driven insights for LLM apps\n\nRagas is your ultimate toolkit for evaluating and optimizing Large Language Model (LLM) applications. Say goodbye to time-consuming, subjective assessments and hello to data-driven, efficient evaluation workflows.\nDon't have a test dataset ready? We also do production-aligned test set generation.\n\n> [!NOTE]\n> Need help setting up Evals for your AI application? We'd love to help! We are conducting Office Hours every week. You can sign up [here](https://cal.com/team/ragas/office-hours).\n\n## Key Features\n\n- 🎯 Objective Metrics: Evaluate your LLM applications with precision using both LLM-based and traditional metrics.\n- 🧪 Test Data Generation: Automatically create comprehensive test datasets covering a wide range of scenarios.\n- 🔗 Seamless Integrations: Works flawlessly with popular LLM frameworks like LangChain and major observability tools.\n- 📊 Build feedback loops: Leverage production data to continually improve your LLM applications.\n\n## :shield: Installation\n\nPypi: \n\n```bash\npip install ragas\n```\n\nAlternatively, from source:\n\n```bash\npip install git+https://github.com/explodinggradients/ragas\n```\n\n## :fire: Quickstart\n\n### Evaluate your LLM App\n\nThis is 5 main lines:\n\n```python\nfrom ragas import SingleTurnSample\nfrom ragas.metrics import AspectCritic\n\ntest_data = {\n    \"user_input\": \"summarise given text\\nThe company reported an 8% rise in Q3 2024, driven by strong performance in the Asian market. Sales in this region have significantly contributed to the overall growth. Analysts attribute this success to strategic marketing and product localization. The positive trend in the Asian market is expected to continue into the next quarter.\",\n    \"response\": \"The company experienced an 8% increase in Q3 2024, largely due to effective marketing strategies and product adaptation, with expectations of continued growth in the coming quarter.\",\n}\nevaluator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o\"))\nmetric = AspectCritic(name=\"summary_accuracy\",llm=evaluator_llm, definition=\"Verify if the summary is accurate.\")\nawait metric.single_turn_ascore(SingleTurnSample(**test_data))\n```\n\nFind the complete [Quickstart Guide](https://docs.ragas.io/en/latest/getstarted/evals)\n\n### Analyze your Evaluation\n\nSign up for [app.ragas.io](https://app.ragas.io) to review, share and analyze your evaluations</a>\n\n<p align=\"left\">\n    <img src=\"docs/getstarted/ragas_get_started_evals.gif\" height=\"300\">\n</p>\n\nSee [how to use it](https://docs.ragas.io/en/latest/getstarted/evals/#analyzing-results)\n\n## 🫂 Community\n\nIf you want to get more involved with Ragas, check out our [discord server](https://discord.gg/5qGUJ6mh7C). It's a fun community where we geek out about LLM, Retrieval, Production issues, and more.\n\n## Contributors\n\n```yml\n+----------------------------------------------------------------------------+\n|     +----------------------------------------------------------------+     |\n|     | Developers: Those who built with `ragas`.                      |     |\n|     | (You have `import ragas` somewhere in your project)            |     |\n|     |     +----------------------------------------------------+     |     |\n|     |     | Contributors: Those who make `ragas` better.       |     |     |\n|     |     | (You make PR to this repo)                         |     |     |\n|     |     +----------------------------------------------------+     |     |\n|     +----------------------------------------------------------------+     |\n+----------------------------------------------------------------------------+\n```\n\nWe welcome contributions from the community! Whether it's bug fixes, feature additions, or documentation improvements, your input is valuable.\n\n1. Fork the repository\n2. Create your feature branch (git checkout -b feature/AmazingFeature)\n3. Commit your changes (git commit -m 'Add some AmazingFeature')\n4. Push to the branch (git push origin feature/AmazingFeature)\n5. Open a Pull Request\n\n## 🔍 Open Analytics\nAt Ragas, we believe in transparency. We collect minimal, anonymized usage data to improve our product and guide our development efforts.\n\n✅ No personal or company-identifying information\n\n✅ Open-source data collection [code](./src/ragas/_analytics.py)\n\n✅ Publicly available aggregated [data](https://github.com/explodinggradients/ragas/issues/49)\n\nTo opt-out, set the `RAGAS_DO_NOT_TRACK` environment variable to `true`.\n"
    },
    {
      "name": "Canner/WrenAI",
      "stars": 7582,
      "img": "https://avatars.githubusercontent.com/u/7250217?s=40&v=4",
      "owner": "Canner",
      "repo_name": "WrenAI",
      "description": "🤖 Open-source GenBI AI Agent that empowers data-driven teams to chat with their data to generate Text-to-SQL, charts, spreadsheets, reports, dashboards and BI. 📈📊📋🧑‍💻",
      "homepage": "https://getwren.ai/oss",
      "language": "TypeScript",
      "created_at": "2024-03-13T06:18:20Z",
      "updated_at": "2025-04-23T09:29:35Z",
      "topics": [
        "agent",
        "anthropic",
        "bedrock",
        "bigquery",
        "business-intelligence",
        "charts",
        "duckdb",
        "genbi",
        "llm",
        "openai",
        "postgresql",
        "rag",
        "spreadsheets",
        "sql",
        "sqlai",
        "text-to-sql",
        "text2sql",
        "vertex"
      ],
      "readme": "\n<p align=\"center\" id=\"top\">\n  <a href=\"https://getwren.ai/?utm_source=github&utm_medium=title&utm_campaign=readme\">\n    <picture>\n      <source media=\"(prefers-color-scheme: light)\" srcset=\"./misc/wrenai_logo.png\">\n      <img src=\"./misc/wrenai_logo_white.png\" width=\"300px\">\n    </picture>\n    <h1 align=\"center\">Wren AI</h1>\n  </a>\n</p>\n\n<p align=\"center\">\n  <a aria-label=\"Follow us on X\" href=\"https://x.com/getwrenai\">\n    <img alt=\"\" src=\"https://img.shields.io/badge/-@getwrenai-blue?style=for-the-badge&logo=x&logoColor=white&labelColor=gray&logoWidth=20\">\n  </a>\n  <a aria-label=\"Releases\" href=\"https://github.com/canner/WrenAI/releases\">\n    <img alt=\"\" src=\"https://img.shields.io/github/v/release/canner/WrenAI?logo=github&label=GitHub%20Release&color=blue&style=for-the-badge\">\n  </a>\n  <a aria-label=\"License\" href=\"https://github.com/Canner/WrenAI/blob/main/LICENSE\">\n    <img alt=\"\" src=\"https://img.shields.io/github/license/canner/WrenAI?color=blue&style=for-the-badge\">\n  </a>\n  <a aria-label=\"Join the community on GitHub\" href=\"https://discord.gg/5DvshJqG8Z\">\n    <img alt=\"\" src=\"https://img.shields.io/badge/-JOIN%20THE%20COMMUNITY-blue?style=for-the-badge&logo=discord&logoColor=white&labelColor=grey&logoWidth=20\">\n  </a>\n  <a aria-label=\"Canner\" href=\"https://cannerdata.com/?utm_source=github&utm_medium=badge&utm_campaign=readme\">\n    <img src=\"https://img.shields.io/badge/%F0%9F%A7%A1-Made%20by%20Canner-blue?style=for-the-badge\">\n  </a>\n</p>\n\n<p align=\"center\">\n  <a href=\"https://trendshift.io/repositories/9263\" target=\"_blank\"><img src=\"https://trendshift.io/api/badge/repositories/9263\" alt=\"Canner%2FWrenAI | Trendshift\" style=\"width: 250px; height: 55px;\" width=\"250\" height=\"55\"/></a>\n</p>\n\n> Open-source GenBI AI Agent that empowers data-driven teams to chat with their data to generate Text-to-SQL, charts, spreadsheets, reports, and BI. \n\n<p align=\"center\">\n  <img src=\"./misc/wren_workflow.png\">\n</p>\n\n## 🕶 Try it yourself!\n\n### GenBI (Generative Business Intelligence)\n\n[Watch GenBI Demo](https://github.com/user-attachments/assets/90ad1d35-bb1e-490b-9676-b29863ff090b)\n\n### Ask any questions\n\n\nhttps://github.com/user-attachments/assets/f9c1cb34-5a95-4580-8890-ec9644da4160\n\n👉 Try with your data on [Wren AI Cloud](https://getwren.ai/?utm_source=github&utm_medium=content&utm_campaign=readme) or [Install in your local environment](https://docs.getwren.ai/oss/installation/?utm_source=github&utm_medium=content&utm_campaign=readme)\n\n## Supported LLM Models\n\nWren AI supports integration with various Large Language Models (LLMs), including but not limited to:\n- OpenAI Models\n- Azure OpenAI Models\n- DeepSeek Models\n- Google AI Studio – Gemini Models\n- Vertex AI Models (Gemini + Anthropic)\n- Bedrock Models\n- Anthropic API Models\n- Groq Models\n- Ollama Models\n- Databricks Models\n\nCheck [configuration examples here](https://github.com/Canner/WrenAI/tree/main/wren-ai-service/docs/config_examples)!\n\n> [!CAUTION]\n> The performance of Wren AI depends significantly on the capabilities of the LLM you choose. We strongly recommend using the most powerful model available for optimal results. Using less capable models may lead to reduced performance, slower response times, or inaccurate outputs.\n\n## 🎯 Our Vision & Mission\n\nAt Wren AI, our mission is to revolutionize business intelligence by empowering organizations with seamless access to data through Generative Business Intelligence (GenBI). We aim to break down barriers to data insights with advanced AI-driven solutions, composable data frameworks, and semantic intelligence, enabling every team member to make faster, smarter, and data-driven decisions with confidence.\n\n🤩 [Learn more about GenBI](https://getwren.ai/genbi?utm_source=github&utm_medium=content&utm_campaign=readme)\n\n## 🤖 A User-Centric, End-to-End Open-source SQL AI Agent - Text-to-SQL Total Solution\n\n### 1. Talk to Your Data in Any Language\n\n> Wren AI speaks [your language](https://docs.getwren.ai/oss/guide/settings/pj_settings#change-project-language?utm_source=github&utm_medium=content&utm_campaign=readme), such as English, German, Spanish, French, Japanese, Korean, Portuguese, Chinese, and more. Unlock valuable insights by asking your business questions to Wren AI. It goes beyond surface-level data analysis to reveal meaningful information and simplifies obtaining answers from lead scoring templates to customer segmentation.\n\n<p align=\"center\">\n  <img src=\"./misc/wren-lang.png\" style=\"max-width: 700px\"/>\n</p>\n\n### 2. GenBI Insights\n\n> The GenBI feature empowers users with AI-generated summaries that provide key insights alongside SQL queries, simplifying complex data. Instantly convert query results into AI-generated reports, charts, transforming raw data into clear, actionable visuals. With GenBI, you can make faster, smarter decisions with ease.\n\n<p align=\"center\">\n  <img src=\"./misc/wren-genbi.png\" alt=\"Screenshot of Wren AI's GenBI feature showing AI-generated summaries and visualizations\" style=\"max-width: 700px\"/>\n</p>\n\n\n### 3. AI-powered Data Exploration Features\n\n> Beyond just retrieving data from your databases, Wren AI now answers exploratory questions like “What data do I have?” or “What are the columns in my customer tables?” Additionally, our AI dynamically generates recommended questions and intelligent follow-up queries tailored to your context, making data exploration smarter, faster, and more intuitive. Empower your team to unlock deeper insights effortlessly with AI.\n\n<p align=\"center\">\n  <img src=\"./misc/AI-generated-understanding_recommend_questions.png\" style=\"max-width: 700px\"/>\n</p>\n\n### 4. Semantic Indexing with a Well-Crafted UI/UX\n\n> Wren AI has implemented a [semantic engine architecture](https://www.getwren.ai/post/how-we-design-our-semantic-engine-for-llms-the-backbone-of-the-semantic-layer-for-llm-architecture/?utm_source=github&utm_medium=content&utm_campaign=readme) to provide the LLM context of your business; you can easily establish a logical presentation layer on your data schema that helps LLM learn more about your business context.\n\n<p align=\"center\">\n  <img src=\"./misc/wren-modeling.png\" style=\"max-width: 700px\"/>\n</p>\n\n### 5. Generate SQL Queries with Context\n\n> With Wren AI, you can process metadata, schema, terminology, data relationships, and the logic behind calculations and aggregations with [“Modeling Definition Language”](https://docs.getwren.ai/oss/engine/concept/what_is_mdl/?utm_source=github&utm_medium=content&utm_campaign=readme), reducing duplicate coding and simplifying data joins.\n\n<p align=\"center\">\n  <img src=\"./misc/wren-context.png\" style=\"max-width: 700px\"/>\n</p>\n\n### 6. Get Insights without Writing Code\n\n> When starting a new conversation in Wren AI, your question is used to find the most relevant tables. From these, LLM generates the most relevant question for the user. You can also ask follow-up questions to get deeper insights.\n\n<p align=\"center\">\n  <img src=\"./misc/wren-insight.png\" style=\"max-width: 700px\"/>\n</p>\n\n### 7. Easily Export and Visualize Your Data\n\n> Wren AI provides a seamless end-to-end workflow, enabling you to connect your data effortlessly with popular analysis tools such as [Excel](https://docs.getwren.ai/oss/guide/integrations/excel-add-in/?utm_source=github&utm_medium=content&utm_campaign=readme) and [Google Sheets](https://docs.getwren.ai/oss/guide/integrations/google-add-on/?utm_source=github&utm_medium=content&utm_campaign=readme). This way, your insights remain accessible, allowing for further analysis using the tools you know best.\n\n<p align=\"center\">\n  <img src=\"./misc/wren-excel.png\" style=\"max-width: 700px\"/>\n</p>\n\n## 🤔 Why Wren AI?\n\nWe focus on providing an open, secure, and accurate SQL AI Agent for everyone.\n\n### 1. Turnkey Solution\n\n> Wren AI makes it easy to onboard your data. Discover and analyze your data with our user interface. Effortlessly generate results without needing to code.\n\n### 2. Secure SQL Generation\n\n> We use RAG architecture to leverage your schema and context, generating SQL queries without requiring you to expose or upload your data to LLM models.\n\n### 3. Open-source End-to-end Solution\n\n> Deploy Wren AI anywhere you like on your own data, LLM APIs, and environment, it's free.\n\n## 🤖 Wren AI Text-to-SQL Agentic Architecture\n\nWren AI consists of three core services:\n\n- ***[Wren UI](https://github.com/Canner/WrenAI/tree/main/wren-ui):*** An intuitive user interface for asking questions, defining data relationships, and integrating data sources.\n\n- ***[Wren AI Service](https://github.com/Canner/WrenAI/tree/main/wren-ai-service):*** Processes queries using a vector database for context retrieval, guiding LLMs to produce precise SQL outputs.\n\n- ***[Wren Engine](https://github.com/Canner/wren-engine):*** Serves as the semantic engine, mapping business terms to data sources, defining relationships, and incorporating predefined calculations and aggregations.\n\n<p align=\"center\">\n  <img src=\"./misc/how_wrenai_works.png\" style=\"max-width: 1000px;\">\n</p>\n\n## ❤️ Knowledge Sharing From Wren AI\n\nWant to get our latest sharing? [Follow our blog!](https://www.getwren.ai/blog/?utm_source=github&utm_medium=content&utm_campaign=readme)\n\n## 🚀 Getting Started\n\nUsing Wren AI is super simple, you can set it up within 3 minutes, and start to interact with your data!\n\n- Visit our [Installation Guide of Wren AI](http://docs.getwren.ai/oss/installation?utm_source=github&utm_medium=content&utm_campaign=readme).\n- Visit the [Usage Guides](https://docs.getwren.ai/oss/guide/connect/overview?utm_source=github&utm_medium=content&utm_campaign=readme) to learn more about how to use Wren AI.\n\n## 📚 Documentation\n\nVisit [Wren AI documentation](https://docs.getwren.ai/oss/overview/introduction?utm_source=github&utm_medium=content&utm_campaign=readme) to view the full documentation.\n\n## 🛠️ Contribution\n\nWant to contribute to Wren AI? Check out our [Contribution Guidelines](https://github.com/Canner/WrenAI/blob/main/CONTRIBUTING.md).\n\n## ⭐️ Community\n\n- Welcome to our [Discord server](https://discord.gg/5DvshJqG8Z) to give us feedback!\n- If there are any issues, please visit [GitHub Issues](https://github.com/Canner/WrenAI/issues).\n- Explore our [public roadmap](https://github.com/orgs/Canner/projects/12/views/1) to stay updated on upcoming features and improvements!\n\nPlease note that our [Code of Conduct](./CODE_OF_CONDUCT.md) applies to all Wren AI community channels. Users are **highly encouraged** to read and adhere to them to avoid repercussions.\n\n## 🎉 Our Contributors\n<a href=\"https://github.com/canner/wrenAI/graphs/contributors\">\n  <img src=\"https://contrib.rocks/image?repo=Canner/WrenAI\" />\n</a>\n\n<p align=\"right\">\n  <a href=\"#top\">⬆️ Back to Top</a>\n</p>\n"
    },
    {
      "name": "weaviate/Verba",
      "stars": 7052,
      "img": "https://avatars.githubusercontent.com/u/37794290?s=40&v=4",
      "owner": "weaviate",
      "repo_name": "Verba",
      "description": "Retrieval Augmented Generation (RAG) chatbot powered by Weaviate",
      "homepage": "",
      "language": "Python",
      "created_at": "2023-07-28T16:53:42Z",
      "updated_at": "2025-04-23T02:42:43Z",
      "topics": [],
      "readme": "# Verba\n\n## The Golden RAGtriever - Community Edition ✨\n\n[![Weaviate](https://img.shields.io/static/v1?label=powered%20by&message=Weaviate%20%E2%9D%A4&color=green&style=flat-square)](https://weaviate.io/)\n[![PyPi downloads](https://static.pepy.tech/personalized-badge/goldenverba?period=total&units=international_system&left_color=grey&right_color=orange&left_text=pip%20downloads)](https://pypi.org/project/goldenverba/) [![Docker support](https://img.shields.io/badge/Docker_support-%E2%9C%93-4c1?style=flat-square&logo=docker&logoColor=white)](https://docs.docker.com/get-started/) [![Demo](https://img.shields.io/badge/Check%20out%20the%20demo!-yellow?&style=flat-square&logo=react&logoColor=white)](https://verba.weaviate.io/)\n\nWelcome to Verba: The Golden RAGtriever, an community-driven open-source application designed to offer an end-to-end, streamlined, and user-friendly interface for Retrieval-Augmented Generation (RAG) out of the box. In just a few easy steps, explore your datasets and extract insights with ease, either locally with Ollama and Huggingface or through LLM providers such as Anthrophic, Cohere, and OpenAI. This project is built with and for the community, please be aware that it might not be maintained with the same urgency as other Weaviate production applications. Feel free to contribute to the project and help us make Verba even better! <3\n\n```\npip install goldenverba\n```\n\n![Demo of Verba](https://github.com/weaviate/Verba/blob/2.0.0/img/verba.gif)\n\n- [Verba](#verba)\n  - [🎯 What Is Verba?](#what-is-verba)\n  - [✨ Features](#feature-lists)\n- [✨ Getting Started with Verba](#getting-started-with-verba)\n- [🔑 API Keys](#api-keys)\n  - [Weaviate](#weaviate)\n  - [Ollama](#ollama)\n  - [Unstructured](#unstructured)\n  - [AssemblyAI](#assemblyai)\n  - [OpenAI](#openai)\n  - [HuggingFace](#huggingface)\n  - [Groq](#groq)\n  - [Novita AI](#novitaai)\n- [Quickstart: Deploy with pip](#how-to-deploy-with-pip)\n- [Quickstart: Build from Source](#how-to-build-from-source)\n- [Quickstart: Deploy with Docker](#how-to-install-verba-with-docker)\n- [💾 Verba Walkthrough](#️verba-walkthrough)\n- [💖 Open Source Contribution](#open-source-contribution)\n- [🚩 Known Issues](#known-issues)\n- [❔FAQ](#faq)\n\n## What Is Verba?\n\nVerba is a fully-customizable personal assistant utilizing [Retrieval Augmented Generation (RAG)](https://weaviate.io/rag#:~:text=RAG%20with%20Weaviate,accuracy%20of%20AI%2Dgenerated%20content.) for querying and interacting with your data, **either locally or deployed via cloud**. Resolve questions around your documents, cross-reference multiple data points or gain insights from existing knowledge bases. Verba combines state-of-the-art RAG techniques with Weaviate's context-aware database. Choose between different RAG frameworks, data types, chunking & retrieving techniques, and LLM providers based on your individual use-case.\n\n## Open Source Spirit\n\n**Weaviate** is proud to offer this open-source project for the community. While we strive to address issues as fast as we can, please understand that it may not be maintained with the same rigor as production software. We welcome and encourage community contributions to help keep it running smoothly. Your support in fixing open issues quickly is greatly appreciated.\n\n### Watch our newest Verba video here:\n\n[![VIDEO LINK](https://github.com/weaviate/Verba/blob/main/img/thumbnail.png)](https://www.youtube.com/watch?v=2VCy-YjRRhA&t=40s&ab_channel=Weaviate%E2%80%A2VectorDatabase)\n\n## Feature Lists\n\n| 🤖 Model Support                  | Implemented | Description                                             |\n| --------------------------------- | ----------- | ------------------------------------------------------- |\n| Ollama (e.g. Llama3)              | ✅          | Local Embedding and Generation Models powered by Ollama |\n| HuggingFace (e.g. MiniLMEmbedder) | ✅          | Local Embedding Models powered by HuggingFace           |\n| Cohere (e.g. Command R+)          | ✅          | Embedding and Generation Models by Cohere               |\n| Anthrophic (e.g. Claude Sonnet)   | ✅          | Embedding and Generation Models by Anthrophic           |\n| OpenAI (e.g. GPT4)                | ✅          | Embedding and Generation Models by OpenAI               |\n| Groq (e.g. Llama3)                | ✅          | Generation Models by Groq (LPU inference)               |\n| Novita AI (e.g. Llama3.3)         | ✅          | Generation Models by Novita AI                          |\n| Upstage (e.g. Solar)              | ✅          | Embedding and Generation Models by Upstage              |\n\n| 🤖 Embedding Support | Implemented | Description                              |\n| -------------------- | ----------- | ---------------------------------------- |\n| Weaviate             | ✅          | Embedding Models powered by Weaviate     |\n| Ollama               | ✅          | Local Embedding Models powered by Ollama |\n| SentenceTransformers | ✅          | Embedding Models powered by HuggingFace  |\n| Cohere               | ✅          | Embedding Models by Cohere               |\n| VoyageAI             | ✅          | Embedding Models by VoyageAI             |\n| OpenAI               | ✅          | Embedding Models by OpenAI               |\n| Upstage              | ✅          | Embedding Models by Upstage              |\n\n| 📁 Data Support                                          | Implemented | Description                                    |\n| -------------------------------------------------------- | ----------- | ---------------------------------------------- |\n| [UnstructuredIO](https://docs.unstructured.io/welcome)   | ✅          | Import Data through Unstructured               |\n| [Firecrawl](https://www.firecrawl.dev/)                  | ✅          | Scrape and Crawl URL through Firecrawl         |\n| [UpstageDocumentParse](https://upstage.ai/)              | ✅          | Parse Documents through Upstage Document AI    |\n| PDF Ingestion                                            | ✅          | Import PDF into Verba                          |\n| GitHub & GitLab                                          | ✅          | Import Files from Github and GitLab            |\n| CSV/XLSX Ingestion                                       | ✅          | Import Table Data into Verba                   |\n| .DOCX                                                    | ✅          | Import .docx files                             |\n| Multi-Modal (using [AssemblyAI](https://assemblyai.com)) | ✅          | Import and Transcribe Audio through AssemblyAI |\n\n| ✨ RAG Features         | Implemented     | Description                                                               |\n| ----------------------- | --------------- | ------------------------------------------------------------------------- |\n| Hybrid Search           | ✅              | Semantic Search combined with Keyword Search                              |\n| Autocomplete Suggestion | ✅              | Verba suggests autocompletion                                             |\n| Filtering               | ✅              | Apply Filters (e.g. documents, document types etc.) before performing RAG |\n| Customizable Metadata   | ✅              | Free control over Metadata                                                |\n| Async Ingestion         | ✅              | Ingest data asynchronously to speed up the process                        |\n| Advanced Querying       | planned ⏱️      | Task Delegation Based on LLM Evaluation                                   |\n| Reranking               | planned ⏱️      | Rerank results based on context for improved results                      |\n| RAG Evaluation          | planned ⏱️      | Interface for Evaluating RAG pipelines                                    |\n| Agentic RAG             | out of scope ❌ | Agentic RAG pipelines                                                     |\n| Graph RAG               | out of scope ❌ | Graph-based RAG pipelines                                                 |\n\n| 🗡️ Chunking Techniques | Implemented | Description                                             |\n| ---------------------- | ----------- | ------------------------------------------------------- |\n| Token                  | ✅          | Chunk by Token powered by [spaCy](https://spacy.io/)    |\n| Sentence               | ✅          | Chunk by Sentence powered by [spaCy](https://spacy.io/) |\n| Semantic               | ✅          | Chunk and group by semantic sentence similarity         |\n| Recursive              | ✅          | Recursively chunk data based on rules                   |\n| HTML                   | ✅          | Chunk HTML files                                        |\n| Markdown               | ✅          | Chunk Markdown files                                    |\n| Code                   | ✅          | Chunk Code files                                        |\n| JSON                   | ✅          | Chunk JSON files                                        |\n\n| 🆒 Cool Bonus            | Implemented     | Description                                             |\n| ------------------------ | --------------- | ------------------------------------------------------- |\n| Docker Support           | ✅              | Verba is deployable via Docker                          |\n| Customizable Frontend    | ✅              | Verba's frontend is fully-customizable via the frontend |\n| Vector Viewer            | ✅              | Visualize your data in 3D                               |\n| Multi-User Collaboration | out of scope ❌ | Multi-User Collaboration in Verba                       |\n\n| 🤝 RAG Libraries | Implemented | Description                        |\n| ---------------- | ----------- | ---------------------------------- |\n| LangChain        | ✅          | Implement LangChain RAG pipelines  |\n| Haystack         | planned ⏱️  | Implement Haystack RAG pipelines   |\n| LlamaIndex       | planned ⏱️  | Implement LlamaIndex RAG pipelines |\n\n> Something is missing? Feel free to create a new issue or discussion with your idea!\n\n![Showcase of Verba](https://github.com/weaviate/Verba/blob/2.0.0/img/verba_screen.png)\n\n---\n\n# Getting Started with Verba\n\nYou have three deployment options for Verba:\n\n- Install via pip\n\n```\npip install goldenverba\n```\n\n- Build from Source\n\n```\ngit clone https://github.com/weaviate/Verba\n\npip install -e .\n```\n\n- Use Docker for Deployment\n\n**Prerequisites**: If you're not using Docker, ensure that you have `Python >=3.10.0,<3.13.0` installed on your system.\n\n```\ngit clone https://github.com/weaviate/Verba\n\ndocker compose --env-file <your-env-file> up -d --build\n```\n\nIf you're unfamiliar with Python and Virtual Environments, please read the [python tutorial guidelines](./PYTHON_TUTORIAL.md).\n\n# API Keys and Environment Variables\n\nYou can set all API keys in the Verba frontend, but to make your life easier, we can also prepare a `.env` file in which Verba will automatically look for the keys. Create a `.env` in the same directory you want to start Verba in. You can find an `.env.example` file in the [goldenverba](./goldenverba/.env.example) directory.\n\n> Make sure to only set environment variables you intend to use, environment variables with missing or incorrect values may lead to errors.\n\nBelow is a comprehensive list of the API keys and variables you may require:\n\n| Environment Variable   | Value                                                      | Description                                                                                                                   |\n| ---------------------- | ---------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------- |\n| WEAVIATE_URL_VERBA     | URL to your hosted Weaviate Cluster                        | Connect to your [WCS](https://console.weaviate.cloud/) Cluster                                                                |\n| WEAVIATE_API_KEY_VERBA | API Credentials to your hosted Weaviate Cluster            | Connect to your [WCS](https://console.weaviate.cloud/) Cluster                                                                |\n| ANTHROPIC_API_KEY      | Your Anthropic API Key                                     | Get Access to [Anthropic](https://www.anthropic.com/) Models                                                                  |\n| OPENAI_API_KEY         | Your OpenAI Key                                            | Get Access to [OpenAI](https://openai.com/) Models                                                                            |\n| OPENAI_EMBED_API_KEY         | Your OpenAI Key                                            | Use a different endpoint for embeddings                                                                            |\n| OPENAI_BASE_URL        | URL to OpenAI instance                                     | Models                                                                                                                        |\n| OPENAI_EMBED_BASE_URL        | URL to OpenAI instance                                     | Use a different endpoint for embeddings                                                                                                                        |\n| OPENAI_MODEL        | The name of the model to be used when selecting OpenAI as a Generator                                    | Default: the first model in the list returned by the endpoint                                                                                                                        |\n| OPENAI_EMBED_MODEL        | The name of the OpenAI embedding model to be used when selecting OpenAI as an Embedder                                    | Default: `text-embedding-3-small`                                                                                                                        |\n| OPENAI_CUSTOM_EMBED        | `true` \\| `false`                                    | Allow Verba to recognize custom embedding model names (not only OpenAI ones)                                                                            |\n| COHERE_API_KEY         | Your API Key                                               | Get Access to [Cohere](https://cohere.com/) Models                                                                            |\n| GROQ_API_KEY           | Your Groq API Key                                          | Get Access to [Groq](https://groq.com/) Models                                                                                |\n| NOVITA_API_KEY         | Your Novita API Key                                        | Get Access to [Novita AI](https://novita.ai?utm_source=github_verba&utm_medium=github_readme&utm_campaign=github_link) Models |\n| OLLAMA_URL             | URL to your Ollama instance (e.g. http://localhost:11434 ) | Get Access to [Ollama](https://ollama.com/) Models                                                                            |\n| UNSTRUCTURED_API_KEY   | Your API Key                                               | Get Access to [Unstructured](https://docs.unstructured.io/welcome) Data Ingestion                                             |\n| UNSTRUCTURED_API_URL   | URL to Unstructured Instance                               | Get Access to [Unstructured](https://docs.unstructured.io/welcome) Data Ingestion                                             |\n| ASSEMBLYAI_API_KEY     | Your API Key                                               | Get Access to [AssemblyAI](https://assemblyai.com) Data Ingestion                                                             |\n| GITHUB_TOKEN           | Your GitHub Token                                          | Get Access to Data Ingestion via GitHub                                                                                       |\n| GITLAB_TOKEN           | Your GitLab Token                                          | Get Access to Data Ingestion via GitLab                                                                                       |\n| FIRECRAWL_API_KEY      | Your Firecrawl API Key                                     | Get Access to Data Ingestion via Firecrawl                                                                                    |\n| VOYAGE_API_KEY         | Your VoyageAI API Key                                      | Get Access to Embedding Models via VoyageAI                                                                                   |\n| EMBEDDING_SERVICE_URL  | URL to your Embedding Service Instance                     | Get Access to Embedding Models via [Weaviate Embedding Service](https://weaviate.io/developers/wcs/embeddings)                |\n| EMBEDDING_SERVICE_KEY  | Your Embedding Service Key                                 | Get Access to Embedding Models via [Weaviate Embedding Service](https://weaviate.io/developers/wcs/embeddings)                |\n| UPSTAGE_API_KEY        | Your Upstage API Key                                       | Get Access to [Upstage](https://upstage.ai/) Models                                                                           |\n| UPSTAGE_BASE_URL       | URL to Upstage instance                                    | Models                                                                                                                        |\n| DEFAULT_DEPLOYMENT     | Local, Weaviate, Custom, Docker                            | Set the default deployment mode                                                                                               |\n| SYSYEM_MESSAGE_PROMPT     | Prompt text value                            | Default value starts with: \"You are Verba, a chatbot for...\"                                                                                               |\n| OLLAMA_MODEL           | Your Ollama Model                                          | Set the default Ollama model to use                                                                                           |\n| OLLAMA_EMBED_MODEL     | Your Ollama Embedding Model                                | Set the default Ollama embedding model to use                                                                                 |\n\n![API Keys in Verba](https://github.com/weaviate/Verba/blob/2.0.0/img/api_screen.png)\n\n## Weaviate\n\nVerba provides flexibility in connecting to Weaviate instances based on your needs. You have three options:\n\n1. **Local Deployment**: Use Weaviate Embedded which runs locally on your device (except Windows, choose the Docker/Cloud Deployment)\n2. **Docker Deployment**: Choose this option when you're running Verba's Dockerfile.\n3. **Cloud Deployment**: Use an existing Weaviate instance hosted on WCD to run Verba\n\n**💻 Weaviate Embedded**\nEmbedded Weaviate is a deployment model that runs a Weaviate instance from your application code rather than from a stand-alone Weaviate server installation. When you run Verba in `Local Deployment`, it will setup and manage Embedded Weaviate in the background. Please note that Weaviate Embedded is not supported on Windows and is in Experimental Mode which can bring unexpected errors. We recommend using the Docker Deployment or Cloud Deployment instead. You can read more about Weaviate Embedded [here](https://weaviate.io/developers/weaviate/installation/embedded).\n\n**🌩️ Weaviate Cloud Deployment (WCD)**\n\nIf you prefer a cloud-based solution, Weaviate Cloud (WCD) offers a scalable, managed environment. Learn how to set up a cloud cluster and get the API keys by following the [Weaviate Cluster Setup Guide](https://weaviate.io/developers/wcs/guides/create-instance).\n\n**🐳 Docker Deployment**\nAnother local alternative is deploying Weaviate using Docker. For more details, follow the [How to install Verba with Docker](#how-to-install-verba-with-docker) section.\n\n![Deployment in Verba](https://github.com/weaviate/Verba/blob/2.0.0/img/verba_deployment.png)\n\n**⚙️ Custom Weaviate Deployment**\n\nIf you're hosting Weaviate yourself, you can use the `Custom` deployment option in Verba. This will allow you to specify the URL, PORT, and API key of your Weaviate instance.\n\n## Ollama\n\nVerba supports Ollama models. Download and Install Ollama on your device (https://ollama.com/download). Make sure to install your preferred LLM using `ollama run <model>`.\n\nTested with `llama3`, `llama3:70b` and `mistral`. The bigger models generally perform better, but need more computational power.\n\n> Make sure Ollama Server runs in the background and that you don't ingest documents with different ollama models since their vector dimension can vary that will lead to errors\n\nYou can verify that by running the following command\n\n```\nollama run llama3\n```\n\n## Unstructured\n\nVerba supports importing documents through Unstructured IO (e.g plain text, .pdf, .csv, and more). To use them you need the `UNSTRUCTURED_API_KEY` and `UNSTRUCTURED_API_URL` environment variable. You can get it from [Unstructured](https://unstructured.io/)\n\n> UNSTRUCTURED_API_URL is set to `https://api.unstructuredapp.io/general/v0/general` by default\n\n## AssemblyAI\n\nVerba supports importing documents through AssemblyAI (audio files or audio from video files). To use them you need the `ASSEMBLYAI_API_KEY` environment variable. You can get it from [AssemblyAI](https://assemblyai.com)\n\n## OpenAI\n\nVerba supports OpenAI Models such as Ada, GPT3, and GPT4. To use them, you need to specify the `OPENAI_API_KEY` environment variable. You can get it from [OpenAI](https://openai.com/)\n\nYou can also add a `OPENAI_BASE_URL` to use proxies such as LiteLLM (https://github.com/BerriAI/litellm)\n\n```\nOPENAI_BASE_URL=YOUR-OPENAI_BASE_URL\n```\n### OpenAI Embeddings\n\nTo specify a different endpoint for your embeddings, set the `OPENAI_EMBED_API_KEY` and `OPENAI_EMBED_BASE_URL` environment variables.\n\nIf you are using a custom OpenAI Server for embeddings, ensure you set `OPENAI_CUSTOM_EMBED=true`. This will allow Verba to recognize custom embedding model names instead of the default OpenAI embedding model names.\n\n## HuggingFace\n\nIf you want to use the HuggingFace Features, make sure to install the correct Verba package. It will install required packages to use the local embedding models.\nPlease note that on startup, Verba will automatically download and install embedding models when used.\n\n```bash\npip install goldenverba[huggingface]\n\nor\n\npip install `.[huggingface]`\n```\n\n> If you're using Docker, modify the `Dockerfile` accordingly. It's not possible to install a custom Verba installation if you pull the Docker Image from the Docker Hub, as of now, you'd need to install the Docker deployment from the source code and modify the `Dockerfile` beforehand.\n\n## Groq\n\nTo use Groq LPUs as generation engine, you need to get an API key from [Groq](https://console.groq.com/keys).\n\n> Although you can provide it in the graphical interface when Verba is up, it is recommended to specify it as `GROQ_API_KEY` environment variable before you launch the application.  \n> It will allow you to choose the generation model in an up-to-date available models list.\n\n## Novita\n\nTo use Novita AI as generation engine, you need to get an API key from [Novita AI](https://novita.ai/settings/key-management?utm_source=github_verba&utm_medium=github_readme&utm_campaign=github_link).\n\n# How to deploy with pip\n\n`Python >=3.10.0`\n\n1. (Very Important) **Initialize a new Python Environment**\n\n```\npython3 -m virtualenv venv\nsource venv/bin/activate\n```\n\n2. **Install Verba**\n\n```\npip install goldenverba\n```\n\n3. **Launch Verba**\n\n```\nverba start\n```\n\n> You can specify the --port and --host via flags\n\n4. **Access Verba**\n\n```\nVisit localhost:8000\n```\n\n5. (Optional)**Create .env file and add environment variables**\n\n# How to build from Source\n\n1. **Clone the Verba repos**\n\n```\ngit clone https://github.com/weaviate/Verba.git\n```\n\n2. **Initialize a new Python Environment**\n\n```\npython3 -m virtualenv venv\nsource venv/bin/activate\n```\n\n3. **Install Verba**\n\n```\npip install -e .\n```\n\n4. **Launch Verba**\n\n```\nverba start\n```\n\n> You can specify the --port and --host via flags\n\n5. **Access Verba**\n\n```\nVisit localhost:8000\n```\n\n6. (Optional) **Create .env file and add environment variables**\n\n# How to install Verba with Docker\n\nDocker is a set of platform-as-a-service products that use OS-level virtualization to deliver software in packages called containers. To get started with deploying Verba using Docker, follow the steps below. If you need more detailed instructions on Docker usage, check out the [Docker Curriculum](https://docker-curriculum.com/).\n\nYou can use `docker pull semitechnologies/verba` to pull the latest Verba Docker Image. Please note, that by pulling directly from Docker Hub you're only able to install the vanilla Verba version that does not include packages e.g `HuggingFace`. If you want to use Docker and `HuggingFace` please follow the steps below.\n\nTo build the image yourself, you can clone the Verba repository and run `docker build -t verba .` inside the Verba directory.\n\n0. **Clone the Verba repos**\n   Ensure you have Git installed on your system. Then, open a terminal or command prompt and run the following command to clone the Verba repository:\n\n```\ngit clone https://github.com/weaviate/Verba.git\n```\n\n1. **Set necessary environment variables**\n   Make sure to set your required environment variables in the `.env` file. You can read more about how to set them up in the [API Keys Section](#api-keys)\n\n2. **Adjust the docker-compose file**\n   You can use the `docker-compose.yml` to add required environment variables under the `verba` service and can also adjust the Weaviate Docker settings to enable Authentification or change other settings of your database instance. You can read more about the Weaviate configuration in our [docker-compose documentation](https://weaviate.io/developers/weaviate/installation/docker-compose). You can also uncomment the `ollama` service to use Ollama within the same docker compose.\n\n> Please make sure to only add environment variables that you really need.\n\n2. **Deploy using Docker**\n   With Docker installed and the Verba repository cloned, navigate to the directory containing the Docker Compose file in your terminal or command prompt. Run the following command to start the Verba application in detached mode, which allows it to run in the background:\n\n```bash\n\ndocker compose up -d\n\n```\n\n```bash\n\ndocker compose --env-file goldenverba/.env up -d --build\n\n```\n\nThis command will download the necessary Docker images, create containers, and start Verba.\nRemember, Docker must be installed on your system to use this method. For installation instructions and more details about Docker, visit the official Docker documentation.\n\n4. **Access Verba**\n\n- You can access your local Weaviate instance at `localhost:8080`\n\n- You can access the Verba frontend at `localhost:8000`\n\nIf you want your Docker Instance to install a specific version of Verba you can edit the `Dockerfile` and change the installation line.\n\n```\nRUN pip install -e '.'\n```\n\n## Verba Walkthrough\n\n### Select your Deployment\n\nThe first screen you'll see is the deployment screen. Here you can select between `Local`, `Docker`, `Weaviate Cloud`, or `Custom` deployment. The `Local` deployment is using Weaviate Embedded under the hood, which initializes a Weaviate instance behind the scenes. The `Docker` deployment is using a separate Weaviate instance that is running inside the same Docker network. The `Weaviate Cloud` deployment is using a Weaviate instance that is hosted on Weaviate Cloud Services (WCS). The `Custom` deployment allows you to specify your own Weaviate instance URL, PORT, and API key.\n\nYou can skip this part by setting the `DEFAULT_DEPLOYMENT` environment variable to `Local`, `Docker`, `Weaviate`, or `Custom`.\n\n### Import Your Data\n\nFirst thing you need to do is to add your data. You can do this by clicking on `Import Data` and selecting either `Add Files`, `Add Directory`, or `Add URL` tab. Here you can add all your files that you want to ingest.\nYou can then configure every file individually by selecting the file and clicking on `Overview` or `Configure` tab.\n![Demo of Verba](https://github.com/weaviate/Verba/blob/2.0.0/img/verba_data.png)\n\n### Query Your Data\n\nWith Data imported, you can use the `Chat` page to ask any related questions. You will receive relevant chunks that are semantically relevant to your question and an answer generated by your choosen model. You can configure the RAG pipeline under the `Config` tab.\n\n![Demo of Verba](https://github.com/weaviate/Verba/blob/2.0.0/img/verba_rag.png)\n\n## Open Source Contribution\n\nYour contributions are always welcome! Feel free to contribute ideas, feedback, or create issues and bug reports if you find any! Before contributing, please read the [Contribution Guide](./CONTRIBUTING.md). Visit our [Weaviate Community Forum](https://forum.weaviate.io/) if you need any help!\n\n### Project Architecture\n\nYou can learn more about Verba's architecture and implementation in its [technical documentation](./TECHNICAL.md) and [frontend documentation](./FRONTEND.md). It's recommended to have a look at them before making any contributions.\n\n## Known Issues\n\n- **Weaviate Embeeded** currently not working on Windows yet\n  - Will be fixed in future versions, until then please use the Docker or WCS Deployment\n\n## FAQ\n\n- **Can I use pre-existing data from my Weaviate instance?**\n\n  - No, unfortunatley not. Verba requires the data to be in a specific format to work. And as of now, this is only possible by importing data through the Verba UI.\n\n- **Is Verba Multi-Lingual?**\n\n  - This depends on your choosen Embedding and Generation Model whether they support multi-lingual data.\n\n- **Can I use my Ollama Server with the Verba Docker?**\n\n  - Yes, you can! Make sure the URL is set to: `OLLAMA_URL=http://host.docker.internal:11434`\n  - If you're running on Linux, you might need to get the IP Gateway of the Ollama server: `OLLAMA_URL=\"http://YOUR-IP-OF-OLLAMA:11434\"`\n\n- **How to clear Weaviate Embedded Storage?**\n\n  - You'll find the stored data here: `~/.local/share/weaviate`\n\n- **How can I specify the port?**\n\n  - You can use the port and host flag `verba start --port 9000 --host 0.0.0.0`\n\n- **Can multiple users use Verba at the same time? How about role based access?**\n\n  - Verba is designed and optimized for single user usage only. There are no plans on supporting multiple users or role based access in the near future.\n\n- **Does Verba offer a API endpoint to use externally?**\n\n  - No, right now Verba does not offer any useful API endpoints to interact with the application. The current FastAPI setup is optimized for the internal communication between the frontend and backend. It is not recommended to use it as a API endpoint. There are plans to add user-friendly\n\n- **How to connect to your custom OpenAI Server?**\n\n  - Set your custom OpenAI API Key and URL in the `.env` file, this will allow Verba to start up and retrieve the models from your custom OpenAI Server. `OPENAI_BASE_URL` is set to `https://api.openai.com/v1` by default.\n  - You can also set a different endpoint for your embeddings by configuring the `OPENAI_EMBED_API_KEY` and `OPENAI_EMBED_BASE_URL` environment variables and setting `OPENAI_CUSTOM_EMBED=true`. For more details, see [OpenAI Embeddings](#openai-embeddings).\n\n- **How to upload custom JSON files to Verba?**\n  - Right now Verba does not support custom JSON structure. Instead the whole JSON will simply be dumped into the content field of the Verba document. You can read more about the Verba JSON Structure in the Technical Documentation [here](./TECHNICAL.md).\n"
    },
    {
      "name": "comet-ml/opik",
      "stars": 6705,
      "img": "https://avatars.githubusercontent.com/u/31487821?s=40&v=4",
      "owner": "comet-ml",
      "repo_name": "opik",
      "description": "Debug, evaluate, and monitor your LLM applications, RAG systems, and agentic workflows with comprehensive tracing, automated evaluations, and production-ready dashboards.",
      "homepage": "https://www.comet.com/docs/opik/",
      "language": "Python",
      "created_at": "2023-05-10T12:57:13Z",
      "updated_at": "2025-04-23T09:36:15Z",
      "topics": [
        "langchain",
        "llama-index",
        "llm",
        "llm-evaluation",
        "llm-observability",
        "llmops",
        "open-source",
        "openai",
        "playground",
        "prompt-engineering"
      ],
      "readme": "<div align=\"center\"><b><a href=\"README.md\">English</a> | <a href=\"readme_CN.md\">简体中文</a> | <a href=\"readme_JP.md\">日本語</a> | <a href=\"readme_KO.md\">한국어</a></b></div>\n\n<h1 align=\"center\" style=\"border-bottom: none\">\n    <div>\n        <a href=\"https://www.comet.com/site/products/opik/?from=llm&utm_source=opik&utm_medium=github&utm_content=header_img&utm_campaign=opik\"><picture>\n            <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://raw.githubusercontent.com/comet-ml/opik/refs/heads/main/apps/opik-documentation/documentation/static/img/logo-dark-mode.svg\">\n            <source media=\"(prefers-color-scheme: light)\" srcset=\"https://raw.githubusercontent.com/comet-ml/opik/refs/heads/main/apps/opik-documentation/documentation/static/img/opik-logo.svg\">\n            <img alt=\"Comet Opik logo\" src=\"https://raw.githubusercontent.com/comet-ml/opik/refs/heads/main/apps/opik-documentation/documentation/static/img/opik-logo.svg\" width=\"200\" />\n        </picture></a>\n        <br>\n        Opik\n    </div>\n    Open source LLM evaluation framework<br>\n</h1>\n\n<p align=\"center\">\nFrom RAG chatbots to code assistants to complex agentic pipelines and beyond, build LLM systems that run better, faster, and cheaper with tracing, evaluations, and dashboards.\n</p>\n\n<div align=\"center\">\n\n[![Python SDK](https://img.shields.io/pypi/v/opik)](https://pypi.org/project/opik/)\n[![License](https://img.shields.io/github/license/comet-ml/opik)](https://github.com/comet-ml/opik/blob/main/LICENSE)\n[![Build](https://github.com/comet-ml/opik/actions/workflows/build_apps.yml/badge.svg)](https://github.com/comet-ml/opik/actions/workflows/build_apps.yml)\n<a target=\"_blank\" href=\"https://colab.research.google.com/github/comet-ml/opik/blob/master/apps/opik-documentation/documentation/docs/cookbook/opik_quickstart.ipynb\">\n\n  <!-- <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open Quickstart In Colab\"/> -->\n</a>\n\n</div>\n\n<p align=\"center\">\n    <a href=\"https://www.comet.com/site/products/opik/?from=llm&utm_source=opik&utm_medium=github&utm_content=website_button&utm_campaign=opik\"><b>Website</b></a> •\n    <a href=\"https://chat.comet.com\"><b>Slack community</b></a> •\n    <a href=\"https://x.com/Cometml\"><b>Twitter</b></a> •\n    <a href=\"https://www.comet.com/docs/opik/?from=llm&utm_source=opik&utm_medium=github&utm_content=docs_button&utm_campaign=opik\"><b>Documentation</b></a>\n</p>\n\n![Opik thumbnail](readme-thumbnail.png)\n\n## Important change on version 1.7.0\n**Please check the change log [here](CHANGELOG.md).**\n\n## 🚀 What is Opik?\n\nOpik is an open-source platform for evaluating, testing and monitoring LLM applications. Built by [Comet](https://www.comet.com?from=llm&utm_source=opik&utm_medium=github&utm_content=what_is_opik_link&utm_campaign=opik).\n\n<br>\n\nYou can use Opik for:\n* **Development:**\n\n  * **Tracing:** Track all LLM calls and traces during development and production ([Quickstart](https://www.comet.com/docs/opik/quickstart/?from=llm&utm_source=opik&utm_medium=github&utm_content=quickstart_link&utm_campaign=opik), [Integrations](https://www.comet.com/docs/opik/tracing/integrations/overview/?from=llm&utm_source=opik&utm_medium=github&utm_content=integrations_link&utm_campaign=opik))\n\n  * **Annotations:** Annotate your LLM calls by logging feedback scores using the [Python SDK](https://www.comet.com/docs/opik/tracing/annotate_traces/#annotating-traces-and-spans-using-the-sdk?from=llm&utm_source=opik&utm_medium=github&utm_content=sdk_link&utm_campaign=opik) or the [UI](https://www.comet.com/docs/opik/tracing/annotate_traces/#annotating-traces-through-the-ui?from=llm&utm_source=opik&utm_medium=github&utm_content=ui_link&utm_campaign=opik).\n\n  * **Playground:** Try out different prompts and models in the [prompt playground](https://www.comet.com/docs/opik/prompt_engineering/playground).\n\n* **Evaluation**: Automate the evaluation process of your LLM application:\n\n    * **Datasets and Experiments**: Store test cases and run experiments ([Datasets](https://www.comet.com/docs/opik/evaluation/manage_datasets/?from=llm&utm_source=opik&utm_medium=github&utm_content=datasets_link&utm_campaign=opik), [Evaluate your LLM Application](https://www.comet.com/docs/opik/evaluation/evaluate_your_llm/?from=llm&utm_source=opik&utm_medium=github&utm_content=eval_link&utm_campaign=opik))\n\n    * **LLM as a judge metrics**: Use Opik's LLM as a judge metric for complex issues like [hallucination detection](https://www.comet.com/docs/opik/evaluation/metrics/hallucination/?from=llm&utm_source=opik&utm_medium=github&utm_content=hallucination_link&utm_campaign=opik), [moderation](https://www.comet.com/docs/opik/evaluation/metrics/moderation/?from=llm&utm_source=opik&utm_medium=github&utm_content=moderation_link&utm_campaign=opik) and RAG evaluation ([Answer Relevance](https://www.comet.com/docs/opik/evaluation/metrics/answer_relevance/?from=llm&utm_source=opik&utm_medium=github&utm_content=alex_link&utm_campaign=opik), [Context Precision](https://www.comet.com/docs/opik/evaluation/metrics/context_precision/?from=llm&utm_source=opik&utm_medium=github&utm_content=context_link&utm_campaign=opik)\n\n    * **CI/CD integration**: Run evaluations as part of your CI/CD pipeline using our [PyTest integration](https://www.comet.com/docs/opik/testing/pytest_integration/?from=llm&utm_source=opik&utm_medium=github&utm_content=pytest_link&utm_campaign=opik)\n\n* **Production Monitoring**:\n    \n    * **Log all your production traces**: Opik has been designed to support high volumes of traces, making it easy to monitor your production applications. Even small deployments can ingest more than 40 million traces per day!\n    \n    * **Monitoring dashboards**: Review your feedback scores, trace count and tokens over time in the [Opik Dashboard](https://www.comet.com/docs/opik/production/production_monitoring/?from=llm&utm_source=opik&utm_medium=github&utm_content=dashboard_link&utm_campaign=opik).\n\n    * **Online evaluation metrics**: Easily score all your production traces using LLM as a Judge metrics and identify any issues with your production LLM application thanks to [Opik's online evaluation metrics](https://www.comet.com/docs/opik/production/rules/?from=llm&utm_source=opik&utm_medium=github&utm_content=dashboard_link&utm_campaign=opik)\n\n> [!TIP]  \n> If you are looking for features that Opik doesn't have today, please raise a new [Feature request](https://github.com/comet-ml/opik/issues/new/choose) 🚀\n\n<br>\n\n## 🛠️ Installation\nOpik is available as a fully open source local installation or using Comet.com as a hosted solution.\nThe easiest way to get started with Opik is by creating a free Comet account at [comet.com](https://www.comet.com/signup?from=llm&utm_source=opik&utm_medium=github&utm_content=install&utm_campaign=opik).\n\nIf you'd like to self-host Opik, you can do so by cloning the repository and starting the platform using Docker Compose:\n\nOn Linux or Mac do:\n```bash\n# Clone the Opik repository\ngit clone https://github.com/comet-ml/opik.git\n\n# Navigate to the repository\ncd opik\n\n# Start the Opik platform\n./opik.sh\n```\n\nOn Windows do:\n```powershell\n# Clone the Opik repository\ngit clone https://github.com/comet-ml/opik.git\n\n# Navigate to the repository\ncd opik\n\n# Start the Opik platform\npowershell -ExecutionPolicy ByPass -c \".\\opik.ps1\"\n```\n\nUse the `--help` or `--info` options to troubleshoot issues.\n\nOnce all is up and running, you can now visit [localhost:5173](http://localhost:5173) on your browser!\n\nFor more information about the different deployment options, please see our deployment guides:\n\n| Installation methods | Docs link |\n| ------------------- | --------- |\n| Local instance | [![Local Deployment](https://img.shields.io/badge/Local%20Deployments-%232496ED?style=flat&logo=docker&logoColor=white)](https://www.comet.com/docs/opik/self-host/local_deployment?from=llm&utm_source=opik&utm_medium=github&utm_content=self_host_link&utm_campaign=opik)\n| Kubernetes | [![Kubernetes](https://img.shields.io/badge/Kubernetes-%23326ce5.svg?&logo=kubernetes&logoColor=white)](https://www.comet.com/docs/opik/self-host/kubernetes/#kubernetes-installation?from=llm&utm_source=opik&utm_medium=github&utm_content=kubernetes_link&utm_campaign=opik)\n\n\n## 🏁 Get Started\n\nTo get started, you will need to first install the Python SDK:\n\n```bash\npip install opik\n```\n\nOnce the SDK is installed, you can configure it by running the `opik configure` command:\n\n```bash\nopik configure\n```\n\nThis will allow you to configure Opik locally by setting the correct local server address or if you're using the Cloud platform by setting the API Key\n\n> [!TIP]  \n> You can also call the `opik.configure(use_local=True)` method from your Python code to configure the SDK to run on the local installation.\n\nYou are now ready to start logging traces using the [Python SDK](https://www.comet.com/docs/opik/python-sdk-reference/?from=llm&utm_source=opik&utm_medium=github&utm_content=sdk_link2&utm_campaign=opik).\n\n### 📝 Logging Traces\n\nThe easiest way to get started is to use one of our integrations. Opik supports:\n\n| Integration | Description                                                                  | Documentation                                                                                                                                                      | Try in Colab                                                                                                                                                                                                                      |\n|-------------|------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| OpenAI      | Log traces for all OpenAI LLM calls                                          | [Documentation](https://www.comet.com/docs/opik/tracing/integrations/openai/?utm_source=opik&utm_medium=github&utm_content=openai_link&utm_campaign=opik)          | [![Open Quickstart In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/comet-ml/opik/blob/master/apps/opik-documentation/documentation/docs/cookbook/openai.ipynb)      |\n| LiteLLM     | Call any LLM model using the OpenAI format                                   | [Documentation](https://www.comet.com/docs/opik/tracing/integrations/litellm/?utm_source=opik&utm_medium=github&utm_content=openai_link&utm_campaign=opik)                                                                                                                  | [![Open Quickstart In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/comet-ml/opik/blob/master/apps/opik-documentation/documentation/docs/cookbook/litellm.ipynb)     |\n| LangChain   | Log traces for all LangChain LLM calls                                       | [Documentation](https://www.comet.com/docs/opik/tracing/integrations/langchain/?utm_source=opik&utm_medium=github&utm_content=langchain_link&utm_campaign=opik)    | [![Open Quickstart In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/comet-ml/opik/blob/master/apps/opik-documentation/documentation/docs/cookbook/langchain.ipynb)   |\n| Haystack    | Log traces for all Haystack calls                                            | [Documentation](https://www.comet.com/docs/opik/tracing/integrations/haystack/?utm_source=opik&utm_medium=github&utm_content=haystack_link&utm_campaign=opik)      | [![Open Quickstart In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/comet-ml/opik/blob/master/apps/opik-documentation/documentation/docs/cookbook/haystack.ipynb)    |\n| Anthropic   | Log traces for all Anthropic LLM calls                                       | [Documentation](https://www.comet.com/docs/opik/tracing/integrations/anthropic?utm_source=opik&utm_medium=github&utm_content=anthropic_link&utm_campaign=opik)     | [![Open Quickstart In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/comet-ml/opik/blob/master/apps/opik-documentation/documentation/docs/cookbook/anthropic.ipynb)   |\n| Bedrock     | Log traces for all Bedrock LLM calls                                         | [Documentation](https://www.comet.com/docs/opik/tracing/integrations/bedrock?utm_source=opik&utm_medium=github&utm_content=bedrock_link&utm_campaign=opik)         | [![Open Quickstart In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/comet-ml/opik/blob/master/apps/opik-documentation/documentation/docs/cookbook/bedrock.ipynb)     |\n| CrewAI      | Log traces for all CrewAI calls                                              | [Documentation](https://www.comet.com/docs/opik/tracing/integrations/crewai?utm_source=opik&utm_medium=github&utm_content=crewai_link&utm_campaign=opik)           | [![Open Quickstart In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/comet-ml/opik/blob/master/apps/opik-documentation/documentation/docs/cookbook/crewai.ipynb)      |\n| DeepSeek    | Log traces for all DeepSeek LLM calls                                        | [Documentation](https://www.comet.com/docs/opik/tracing/integrations/deepseek?utm_source=opik&utm_medium=github&utm_content=deepseek_link&utm_campaign=opik)       | |\n| DSPy        | Log traces for all DSPy runs                                                 | [Documentation](https://www.comet.com/docs/opik/tracing/integrations/dspy?utm_source=opik&utm_medium=github&utm_content=dspy_link&utm_campaign=opik)               | [![Open Quickstart In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/comet-ml/opik/blob/master/apps/opik-documentation/documentation/docs/cookbook/dspy.ipynb)        |\n| Gemini      | Log traces for all Gemini LLM calls                                          | [Documentation](https://www.comet.com/docs/opik/tracing/integrations/gemini?utm_source=opik&utm_medium=github&utm_content=gemini_link&utm_campaign=opik)           | [![Open Quickstart In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/comet-ml/opik/blob/master/apps/opik-documentation/documentation/docs/cookbook/gemini.ipynb)      |\n| Groq        | Log traces for all Groq LLM calls                                            | [Documentation](https://www.comet.com/docs/opik/tracing/integrations/groq?utm_source=opik&utm_medium=github&utm_content=groq_link&utm_campaign=opik)               | [![Open Quickstart In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/comet-ml/opik/blob/master/apps/opik-documentation/documentation/docs/cookbook/groq.ipynb)        |\n| Guardrails  | Log traces for all Guardrails validations                                    | [Documentation](https://www.comet.com/docs/opik/tracing/integrations/guardrails/?utm_source=opik&utm_medium=github&utm_content=guardrails_link&utm_campaign=opik)    | [![Open Quickstart In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/comet-ml/opik/blob/master/apps/opik-documentation/documentation/docs/cookbook/guardrails-ai.ipynb)   |\n| Instructor  | Log traces for all LLM calls made with Instructor                            | [Documentation](https://www.comet.com/docs/opik/tracing/integrations/instructor/?utm_source=opik&utm_medium=github&utm_content=instructor_link&utm_campaign=opik)    | [![Open Quickstart In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/comet-ml/opik/blob/master/apps/opik-documentation/documentation/docs/cookbook/instructor.ipynb)   |\n| LangGraph   | Log traces for all LangGraph executions                                      | [Documentation](https://www.comet.com/docs/opik/tracing/integrations/langgraph/?utm_source=opik&utm_medium=github&utm_content=langchain_link&utm_campaign=opik)    | [![Open Quickstart In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/comet-ml/opik/blob/master/apps/opik-documentation/documentation/docs/cookbook/langgraph.ipynb)   |\n| LlamaIndex  | Log traces for all LlamaIndex LLM calls                                      | [Documentation](https://www.comet.com/docs/opik/tracing/integrations/llama_index?utm_source=opik&utm_medium=github&utm_content=llama_index_link&utm_campaign=opik) | [![Open Quickstart In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/comet-ml/opik/blob/master/apps/opik-documentation/documentation/docs/cookbook/llama-index.ipynb) |\n| Ollama      | Log traces for all Ollama LLM calls                                          | [Documentation](https://www.comet.com/docs/opik/tracing/integrations/ollama?utm_source=opik&utm_medium=github&utm_content=ollama_link&utm_campaign=opik)           | [![Open Quickstart In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/comet-ml/opik/blob/master/apps/opik-documentation/documentation/docs/cookbook/ollama.ipynb)      |\n| Predibase   | Fine-tune and serve open-source Large Language Models                        | [Documentation](https://www.comet.com/docs/opik/tracing/integrations/predibase?utm_source=opik&utm_medium=github&utm_content=predibase_link&utm_campaign=opik)     | [![Open Quickstart In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/comet-ml/opik/blob/master/apps/opik-documentation/documentation/docs/cookbook/predibase.ipynb)   |\n| Pydantic AI | Fine-tune and serve open-source Large Language Models                        | [Documentation](https://www.comet.com/docs/opik/tracing/integrations/predibase?utm_source=opik&utm_medium=github&utm_content=predibase_link&utm_campaign=opik)     | [![Open Quickstart In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/comet-ml/opik/blob/master/apps/opik-documentation/documentation/docs/cookbook/predibase.ipynb)   |\n| Ragas       | PydanticAI is a Python agent framework designed to build production apps     | [Documentation](https://www.comet.com/docs/opik/tracing/integrations/pydantic-ai?utm_source=opik&utm_medium=github&utm_content=pydantic_ai_link&utm_campaign=opik) | [![Open Quickstart In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/comet-ml/opik/blob/master/apps/opik-documentation/documentation/docs/cookbook/pydantic-ai.ipynb) |\n| watsonx     | Log traces for all watsonx LLM calls                                         | [Documentation](https://www.comet.com/docs/opik/tracing/integrations/watsonx?utm_source=opik&utm_medium=github&utm_content=watsonx_link&utm_campaign=opik)         | [![Open Quickstart In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/comet-ml/opik/blob/master/apps/opik-documentation/documentation/docs/cookbook/watsonx.ipynb)     |\n\n> [!TIP]  \n> If the framework you are using is not listed above, feel free to [open an issue](https://github.com/comet-ml/opik/issues) or submit a PR with the integration.\n\nIf you are not using any of the frameworks above, you can also use the `track` function decorator to [log traces](https://www.comet.com/docs/opik/tracing/log_traces/?from=llm&utm_source=opik&utm_medium=github&utm_content=traces_link&utm_campaign=opik):\n\n```python\nimport opik\n\nopik.configure(use_local=True) # Run locally\n\n@opik.track\ndef my_llm_function(user_question: str) -> str:\n    # Your LLM code here\n\n    return \"Hello\"\n```\n\n> [!TIP]  \n> The track decorator can be used in conjunction with any of our integrations and can also be used to track nested function calls.\n\n### 🧑‍⚖️ LLM as a Judge metrics\n\nThe Python Opik SDK includes a number of LLM as a judge metrics to help you evaluate your LLM application. Learn more about it in the [metrics documentation](https://www.comet.com/docs/opik/evaluation/metrics/overview/?from=llm&utm_source=opik&utm_medium=github&utm_content=metrics_2_link&utm_campaign=opik).\n\nTo use them, simply import the relevant metric and use the `score` function:\n\n```python\nfrom opik.evaluation.metrics import Hallucination\n\nmetric = Hallucination()\nscore = metric.score(\n    input=\"What is the capital of France?\",\n    output=\"Paris\",\n    context=[\"France is a country in Europe.\"]\n)\nprint(score)\n```\n\nOpik also includes a number of pre-built heuristic metrics as well as the ability to create your own. Learn more about it in the [metrics documentation](https://www.comet.com/docs/opik/evaluation/metrics/overview?from=llm&utm_source=opik&utm_medium=github&utm_content=metrics_3_link&utm_campaign=opik).\n\n### 🔍 Evaluating your LLM Application\n\nOpik allows you to evaluate your LLM application during development through [Datasets](https://www.comet.com/docs/opik/evaluation/manage_datasets/?from=llm&utm_source=opik&utm_medium=github&utm_content=datasets_2_link&utm_campaign=opik) and [Experiments](https://www.comet.com/docs/opik/evaluation/evaluate_your_llm/?from=llm&utm_source=opik&utm_medium=github&utm_content=experiments_link&utm_campaign=opik).\n\nYou can also run evaluations as part of your CI/CD pipeline using our [PyTest integration](https://www.comet.com/docs/opik/testing/pytest_integration/?from=llm&utm_source=opik&utm_medium=github&utm_content=pytest_2_link&utm_campaign=opik).\n\n## ⭐ Star Us on GitHub\n\nIf you find Opik useful, please consider giving us a star! Your support helps us grow our community and continue improving the product.\n\n<img src=\"https://github.com/user-attachments/assets/ffc208bb-3dc0-40d8-9a20-8513b5e4a59d\" alt=\"Opik GitHub Star History\" width=\"600\"/>\n\n\n\n## 🤝 Contributing\n\nThere are many ways to contribute to Opik:\n\n* Submit [bug reports](https://github.com/comet-ml/opik/issues) and [feature requests](https://github.com/comet-ml/opik/issues)\n* Review the documentation and submit [Pull Requests](https://github.com/comet-ml/opik/pulls) to improve it\n* Speaking or writing about Opik and [letting us know](https://chat.comet.com)\n* Upvoting [popular feature requests](https://github.com/comet-ml/opik/issues?q=is%3Aissue+is%3Aopen+label%3A%22enhancement%22) to show your support\n\nTo learn more about how to contribute to Opik, please see our [contributing guidelines](CONTRIBUTING.md).\n"
    },
    {
      "name": "traceloop/openllmetry",
      "stars": 5691,
      "img": "https://avatars.githubusercontent.com/u/125419530?s=40&v=4",
      "owner": "traceloop",
      "repo_name": "openllmetry",
      "description": "Open-source observability for your LLM application, based on OpenTelemetry",
      "homepage": "https://www.traceloop.com/openllmetry",
      "language": "Python",
      "created_at": "2023-09-02T14:42:59Z",
      "updated_at": "2025-04-23T07:33:44Z",
      "topics": [
        "artifical-intelligence",
        "datascience",
        "generative-ai",
        "good-first-issue",
        "good-first-issues",
        "help-wanted",
        "llm",
        "llmops",
        "metrics",
        "ml",
        "model-monitoring",
        "monitoring",
        "observability",
        "open-source",
        "open-telemetry",
        "opentelemetry",
        "opentelemetry-python",
        "python"
      ],
      "readme": "<p align=\"center\">\n<a href=\"https://www.traceloop.com/openllmetry#gh-light-mode-only\">\n<img width=\"600\" src=\"https://raw.githubusercontent.com/traceloop/openllmetry/main/img/logo-light.png\">\n</a>\n<a href=\"https://www.traceloop.com/openllmetry#gh-dark-mode-only\">\n<img width=\"600\" src=\"https://raw.githubusercontent.com/traceloop/openllmetry/main/img/logo-dark.png\">\n</a>\n</p>\n<p align=\"center\">\n  <p align=\"center\">Open-source observability for your LLM application</p>\n</p>\n<h4 align=\"center\">\n    <a href=\"https://traceloop.com/docs/openllmetry/getting-started-python\"><strong>Get started »</strong></a>\n    <br />\n    <br />\n  <a href=\"https://traceloop.com/slack\">Slack</a> |\n  <a href=\"https://traceloop.com/docs/openllmetry/introduction\">Docs</a> |\n  <a href=\"https://www.traceloop.com/openllmetry\">Website</a>\n</h4>\n\n<h4 align=\"center\">\n  <a href=\"https://github.com/traceloop/openllmetry/releases\">\n    <img src=\"https://img.shields.io/github/release/traceloop/openllmetry\">\n  </a>\n  <a href=\"https://pepy.tech/project/opentelemetry-instrumentation-openai\">\n  <img src=\"https://static.pepy.tech/badge/opentelemetry-instrumentation-openai/month\">\n  </a>\n   <a href=\"https://github.com/traceloop/openllmetry/blob/main/LICENSE\">\n    <img src=\"https://img.shields.io/badge/license-Apache 2.0-blue.svg\" alt=\"OpenLLMetry is released under the Apache-2.0 License\">\n  </a>\n  <a href=\"https://github.com/traceloop/openllmetry/actions/workflows/ci.yml\">\n  <img src=\"https://github.com/traceloop/openllmetry/actions/workflows/ci.yml/badge.svg\">\n  </a>\n  <a href=\"https://github.com/traceloop/openllmetry/issues\">\n    <img src=\"https://img.shields.io/github/commit-activity/m/traceloop/openllmetry\" alt=\"git commit activity\" />\n  </a>\n  <a href=\"https://www.ycombinator.com/companies/traceloop\"><img src=\"https://img.shields.io/website?color=%23f26522&down_message=Y%20Combinator&label=Backed&logo=ycombinator&style=flat-square&up_message=Y%20Combinator&url=https%3A%2F%2Fwww.ycombinator.com\"></a>\n  <a href=\"https://github.com/traceloop/openllmetry/blob/main/CONTRIBUTING.md\">\n    <img src=\"https://img.shields.io/badge/PRs-Welcome-brightgreen\" alt=\"PRs welcome!\" />\n  </a>\n  <a href=\"https://traceloop.com/slack\">\n    <img src=\"https://img.shields.io/badge/chat-on%20Slack-blueviolet\" alt=\"Slack community channel\" />\n  </a>\n  <a href=\"https://twitter.com/traceloopdev\">\n    <img src=\"https://img.shields.io/badge/follow-%40traceloopdev-1DA1F2?logo=twitter&style=social\" alt=\"Traceloop Twitter\" />\n  </a>\n</h4>\n\n**🎉 New**:\nOur semantic conventions are now part of OpenTelemetry! Join the [discussion](https://github.com/open-telemetry/community/blob/1c71595874e5d125ca92ec3b0e948c4325161c8a/projects/llm-semconv.md) and help us shape the future of LLM observability.\n\nLooking for the JS/TS version? Check out [OpenLLMetry-JS](https://github.com/traceloop/openllmetry-js).\n\nOpenLLMetry is a set of extensions built on top of [OpenTelemetry](https://opentelemetry.io/) that gives you complete observability over your LLM application. Because it uses OpenTelemetry under the hood, [it can be connected to your existing observability solutions](https://www.traceloop.com/docs/openllmetry/integrations/introduction) - Datadog, Honeycomb, and others.\n\nIt's built and maintained by Traceloop under the Apache 2.0 license.\n\nThe repo contains standard OpenTelemetry instrumentations for LLM providers and Vector DBs, as well as a Traceloop SDK that makes it easy to get started with OpenLLMetry, while still outputting standard OpenTelemetry data that can be connected to your observability stack.\nIf you already have OpenTelemetry instrumented, you can just add any of our instrumentations directly.\n\n## 🚀 Getting Started\n\nThe easiest way to get started is to use our SDK.\nFor a complete guide, go to our [docs](https://traceloop.com/docs/openllmetry/getting-started-python).\n\nInstall the SDK:\n\n```bash\npip install traceloop-sdk\n```\n\nThen, to start instrumenting your code, just add this line to your code:\n\n```python\nfrom traceloop.sdk import Traceloop\n\nTraceloop.init()\n```\n\nThat's it. You're now tracing your code with OpenLLMetry!\nIf you're running this locally, you may want to disable batch sending, so you can see the traces immediately:\n\n```python\nTraceloop.init(disable_batch=True)\n```\n\n## ⏫ Supported (and tested) destinations\n\n- ✅ [Traceloop](https://www.traceloop.com/docs/openllmetry/integrations/traceloop)\n- ✅ [Axiom](https://www.traceloop.com/docs/openllmetry/integrations/axiom)\n- ✅ [Azure Application Insights](https://www.traceloop.com/docs/openllmetry/integrations/azure)\n- ✅ [Braintrust](https://www.traceloop.com/docs/openllmetry/integrations/braintrust)\n- ✅ [Dash0](https://www.traceloop.com/docs/openllmetry/integrations/dash0)\n- ✅ [Datadog](https://www.traceloop.com/docs/openllmetry/integrations/datadog)\n- ✅ [Dynatrace](https://www.traceloop.com/docs/openllmetry/integrations/dynatrace)\n- ✅ [Google Cloud](https://www.traceloop.com/docs/openllmetry/integrations/gcp)\n- ✅ [Grafana](https://www.traceloop.com/docs/openllmetry/integrations/grafana)\n- ✅ [Highlight](https://www.traceloop.com/docs/openllmetry/integrations/highlight)\n- ✅ [Honeycomb](https://www.traceloop.com/docs/openllmetry/integrations/honeycomb)\n- ✅ [HyperDX](https://www.traceloop.com/docs/openllmetry/integrations/hyperdx)\n- ✅ [IBM Instana](https://www.traceloop.com/docs/openllmetry/integrations/instana)\n- ✅ [KloudMate](https://www.traceloop.com/docs/openllmetry/integrations/kloudmate)\n- ✅ [New Relic](https://www.traceloop.com/docs/openllmetry/integrations/newrelic)\n- ✅ [OpenTelemetry Collector](https://www.traceloop.com/docs/openllmetry/integrations/otel-collector)\n- ✅ [Oracle Cloud](https://www.traceloop.com/docs/openllmetry/integrations/oraclecloud)\n- ✅ [Service Now Cloud Observability](https://www.traceloop.com/docs/openllmetry/integrations/service-now)\n- ✅ [SigNoz](https://www.traceloop.com/docs/openllmetry/integrations/signoz)\n- ✅ [Sentry](https://www.traceloop.com/docs/openllmetry/integrations/sentry)\n- ✅ [Splunk](https://www.traceloop.com/docs/openllmetry/integrations/splunk)\n\nSee [our docs](https://traceloop.com/docs/openllmetry/integrations/exporting) for instructions on connecting to each one.\n\n## 🪗 What do we instrument?\n\nOpenLLMetry can instrument everything that [OpenTelemetry already instruments](https://github.com/open-telemetry/opentelemetry-python-contrib/tree/main/instrumentation) - so things like your DB, API calls, and more. On top of that, we built a set of custom extensions that instrument things like your calls to OpenAI or Anthropic, or your Vector DB like Chroma, Pinecone, Qdrant or Weaviate.\n\n- ✅ [OpenAI / Azure OpenAI](https://openai.com/)\n- ✅ [Anthropic](https://www.anthropic.com/)\n- ✅ [Cohere](https://cohere.com/)\n- ✅ [Ollama](https://ollama.com/)\n- ✅ [Mistral AI](https://mistral.ai/)\n- ✅ [HuggingFace](https://huggingface.co/)\n- ✅ [Bedrock (AWS)](https://aws.amazon.com/bedrock/)\n- ✅ [SageMaker (AWS)](https://aws.amazon.com/sagemaker/)\n- ✅ [Replicate](https://replicate.com/)\n- ✅ [Vertex AI (GCP)](https://cloud.google.com/vertex-ai)\n- ✅ [Google Generative AI (Gemini)](https://ai.google/)\n- ✅ [IBM Watsonx AI](https://www.ibm.com/watsonx)\n- ✅ [Together AI](https://together.xyz/)\n- ✅ [Aleph Alpha](https://www.aleph-alpha.com/)\n- ✅ [Groq](https://groq.com/)\n\n### Vector DBs\n\n- ✅ [Chroma](https://www.trychroma.com/)\n- ✅ [Pinecone](https://www.pinecone.io/)\n- ✅ [Qdrant](https://qdrant.tech/)\n- ✅ [Weaviate](https://weaviate.io/)\n- ✅ [Milvus](https://milvus.io/)\n- ✅ [Marqo](https://marqo.ai/)\n- ✅ [LanceDB](https://lancedb.com/)\n\n### Frameworks\n\n- ✅ [LangChain](https://python.langchain.com/docs/introduction/)\n- ✅ [LlamaIndex](https://docs.llamaindex.ai/en/stable/module_guides/observability/observability.html#openllmetry)\n- ✅ [Haystack](https://haystack.deepset.ai/integrations/traceloop)\n- ✅ [LiteLLM](https://docs.litellm.ai/docs/observability/opentelemetry_integration)\n- ✅ [CrewAI](https://docs.crewai.com/introduction)\n\n## 🔎 Telemetry\n\nThe SDK provided with OpenLLMetry (not the instrumentations) contains a telemetry feature that collects **anonymous** usage information.\n\nYou can opt out of telemetry by setting the `TRACELOOP_TELEMETRY` environment variable to `FALSE`, or passing `telemetry_enabled=False` to the `Traceloop.init()` function.\n\n### Why we collect telemetry\n\n- The primary purpose is to detect exceptions within instrumentations. Since LLM providers frequently update their APIs, this helps us quickly identify and fix any breaking changes.\n- We only collect anonymous data, with no personally identifiable information. You can view exactly what data we collect in our [Privacy documentation](https://www.traceloop.com/docs/openllmetry/privacy/telemetry).\n- Telemetry is only collected in the SDK. If you use the instrumentations directly without the SDK, no telemetry is collected.\n\n## 🌱 Contributing\n\nWhether big or small, we love contributions ❤️ Check out our guide to see how to [get started](https://traceloop.com/docs/openllmetry/contributing/overview).\n\nNot sure where to get started? You can:\n\n- [Book a free pairing session with one of our teammates](mailto:nir@traceloop.com?subject=Pairing%20session&body=I'd%20like%20to%20do%20a%20pairing%20session!)!\n- Join our <a href=\"https://traceloop.com/slack\">Slack</a>, and ask us any questions there.\n\n## 💚 Community & Support\n\n- [Slack](https://traceloop.com/slack) (For live discussion with the community and the Traceloop team)\n- [GitHub Discussions](https://github.com/traceloop/openllmetry/discussions) (For help with building and deeper conversations about features)\n- [GitHub Issues](https://github.com/traceloop/openllmetry/issues) (For any bugs and errors you encounter using OpenLLMetry)\n- [Twitter](https://twitter.com/traceloopdev) (Get news fast)\n\n## 🙏 Special Thanks\n\nTo @patrickdebois, who [suggested the great name](https://x.com/patrickdebois/status/1695518950715473991?s=46&t=zn2SOuJcSVq-Pe2Ysevzkg) we're now using for this repo!\n\n## 💫 Contributors\n\n<a href=\"https://github.com/traceloop/openllmetry/graphs/contributors\">\n  <img alt=\"contributors\" src=\"https://contrib.rocks/image?repo=traceloop/openllmetry\"/>\n</a>\n"
    },
    {
      "name": "nlp-with-transformers/notebooks",
      "stars": 4294,
      "img": "https://avatars.githubusercontent.com/u/93219893?s=40&v=4",
      "owner": "nlp-with-transformers",
      "repo_name": "notebooks",
      "description": "Jupyter notebooks for the Natural Language Processing with Transformers book",
      "homepage": "https://transformersbook.com/",
      "language": "Jupyter Notebook",
      "created_at": "2021-10-28T08:18:30Z",
      "updated_at": "2025-04-23T05:41:19Z",
      "topics": [],
      "readme": "# Transformers Notebooks\n\nThis repository contains the example code from our O'Reilly book [Natural Language Processing with Transformers](https://www.oreilly.com/library/view/natural-language-processing/9781098136789/):\n\n<img alt=\"book-cover\" height=200 src=\"images/book_cover.jpg\" id=\"book-cover\"/>\n\n## Getting started\n\nYou can run these notebooks on cloud platforms like [Google Colab](https://colab.research.google.com/) or your local machine. Note that most chapters require a GPU to run in a reasonable amount of time, so we recommend one of the cloud platforms as they come pre-installed with CUDA.\n\n### Running on a cloud platform\n\nTo run these notebooks on a cloud platform, just click on one of the badges in the table below:\n\n<!--This table is automatically generated, do not fill manually!-->\n\n\n\n| Chapter                                     | Colab                                                                                                                                                                                               | Kaggle                                                                                                                                                                                                   | Gradient                                                                                                                                                                               | Studio Lab                                                                                                                                                                                                   |\n|:--------------------------------------------|:----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Introduction                                | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/nlp-with-transformers/notebooks/blob/main/01_introduction.ipynb)              | [![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/nlp-with-transformers/notebooks/blob/main/01_introduction.ipynb)              | [![Gradient](https://assets.paperspace.io/img/gradient-badge.svg)](https://console.paperspace.com/github/nlp-with-transformers/notebooks/blob/main/01_introduction.ipynb)              | [![Open In SageMaker Studio Lab](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/nlp-with-transformers/notebooks/blob/main/01_introduction.ipynb)              |\n| Text Classification                         | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/nlp-with-transformers/notebooks/blob/main/02_classification.ipynb)            | [![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/nlp-with-transformers/notebooks/blob/main/02_classification.ipynb)            | [![Gradient](https://assets.paperspace.io/img/gradient-badge.svg)](https://console.paperspace.com/github/nlp-with-transformers/notebooks/blob/main/02_classification.ipynb)            | [![Open In SageMaker Studio Lab](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/nlp-with-transformers/notebooks/blob/main/02_classification.ipynb)            |\n| Transformer Anatomy                         | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/nlp-with-transformers/notebooks/blob/main/03_transformer-anatomy.ipynb)       | [![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/nlp-with-transformers/notebooks/blob/main/03_transformer-anatomy.ipynb)       | [![Gradient](https://assets.paperspace.io/img/gradient-badge.svg)](https://console.paperspace.com/github/nlp-with-transformers/notebooks/blob/main/03_transformer-anatomy.ipynb)       | [![Open In SageMaker Studio Lab](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/nlp-with-transformers/notebooks/blob/main/03_transformer-anatomy.ipynb)       |\n| Multilingual Named Entity Recognition       | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/nlp-with-transformers/notebooks/blob/main/04_multilingual-ner.ipynb)          | [![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/nlp-with-transformers/notebooks/blob/main/04_multilingual-ner.ipynb)          | [![Gradient](https://assets.paperspace.io/img/gradient-badge.svg)](https://console.paperspace.com/github/nlp-with-transformers/notebooks/blob/main/04_multilingual-ner.ipynb)          | [![Open In SageMaker Studio Lab](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/nlp-with-transformers/notebooks/blob/main/04_multilingual-ner.ipynb)          |\n| Text Generation                             | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/nlp-with-transformers/notebooks/blob/main/05_text-generation.ipynb)           | [![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/nlp-with-transformers/notebooks/blob/main/05_text-generation.ipynb)           | [![Gradient](https://assets.paperspace.io/img/gradient-badge.svg)](https://console.paperspace.com/github/nlp-with-transformers/notebooks/blob/main/05_text-generation.ipynb)           | [![Open In SageMaker Studio Lab](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/nlp-with-transformers/notebooks/blob/main/05_text-generation.ipynb)           |\n| Summarization                               | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/nlp-with-transformers/notebooks/blob/main/06_summarization.ipynb)             | [![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/nlp-with-transformers/notebooks/blob/main/06_summarization.ipynb)             | [![Gradient](https://assets.paperspace.io/img/gradient-badge.svg)](https://console.paperspace.com/github/nlp-with-transformers/notebooks/blob/main/06_summarization.ipynb)             | [![Open In SageMaker Studio Lab](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/nlp-with-transformers/notebooks/blob/main/06_summarization.ipynb)             |\n| Question Answering                          | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/nlp-with-transformers/notebooks/blob/main/07_question-answering.ipynb)        | [![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/nlp-with-transformers/notebooks/blob/main/07_question-answering.ipynb)        | [![Gradient](https://assets.paperspace.io/img/gradient-badge.svg)](https://console.paperspace.com/github/nlp-with-transformers/notebooks/blob/main/07_question-answering.ipynb)        | [![Open In SageMaker Studio Lab](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/nlp-with-transformers/notebooks/blob/main/07_question-answering.ipynb)        |\n| Making Transformers Efficient in Production | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/nlp-with-transformers/notebooks/blob/main/08_model-compression.ipynb)         | [![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/nlp-with-transformers/notebooks/blob/main/08_model-compression.ipynb)         | [![Gradient](https://assets.paperspace.io/img/gradient-badge.svg)](https://console.paperspace.com/github/nlp-with-transformers/notebooks/blob/main/08_model-compression.ipynb)         | [![Open In SageMaker Studio Lab](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/nlp-with-transformers/notebooks/blob/main/08_model-compression.ipynb)         |\n| Dealing with Few to No Labels               | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/nlp-with-transformers/notebooks/blob/main/09_few-to-no-labels.ipynb)          | [![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/nlp-with-transformers/notebooks/blob/main/09_few-to-no-labels.ipynb)          | [![Gradient](https://assets.paperspace.io/img/gradient-badge.svg)](https://console.paperspace.com/github/nlp-with-transformers/notebooks/blob/main/09_few-to-no-labels.ipynb)          | [![Open In SageMaker Studio Lab](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/nlp-with-transformers/notebooks/blob/main/09_few-to-no-labels.ipynb)          |\n| Training Transformers from Scratch          | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/nlp-with-transformers/notebooks/blob/main/10_transformers-from-scratch.ipynb) | [![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/nlp-with-transformers/notebooks/blob/main/10_transformers-from-scratch.ipynb) | [![Gradient](https://assets.paperspace.io/img/gradient-badge.svg)](https://console.paperspace.com/github/nlp-with-transformers/notebooks/blob/main/10_transformers-from-scratch.ipynb) | [![Open In SageMaker Studio Lab](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/nlp-with-transformers/notebooks/blob/main/10_transformers-from-scratch.ipynb) |\n| Future Directions                           | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/nlp-with-transformers/notebooks/blob/main/11_future-directions.ipynb)         | [![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/nlp-with-transformers/notebooks/blob/main/11_future-directions.ipynb)         | [![Gradient](https://assets.paperspace.io/img/gradient-badge.svg)](https://console.paperspace.com/github/nlp-with-transformers/notebooks/blob/main/11_future-directions.ipynb)         | [![Open In SageMaker Studio Lab](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/nlp-with-transformers/notebooks/blob/main/11_future-directions.ipynb)         |\n\n<!--End of table-->\n\nNowadays, the GPUs on Colab tend to be K80s (which have limited memory), so we recommend using [Kaggle](https://www.kaggle.com/docs/notebooks), [Gradient](https://gradient.run/notebooks), or [SageMaker Studio Lab](https://studiolab.sagemaker.aws/). These platforms tend to provide more performant GPUs like P100s, all for free!\n\n> Note: some cloud platforms like Kaggle require you to restart the notebook after installing new packages.\n\n### Running on your machine\n\nTo run the notebooks on your own machine, first clone the repository and navigate to it:\n\n```bash\n$ git clone https://github.com/nlp-with-transformers/notebooks.git\n$ cd notebooks\n```\n\nNext, run the following command to create a `conda` virtual environment that contains all the libraries needed to run the notebooks:\n\n```bash\n$ conda env create -f environment.yml\n```\n\n> Note: You'll need a GPU that supports NVIDIA's [CUDA Toolkit](https://developer.nvidia.com/cuda-toolkit) to build the environment. Currently, this means you cannot build locally on Apple silicon 😢.\n\nChapter 7 (Question Answering) has a special set of dependencies, so to run that chapter you'll need a separate environment:\n\n```bash\n$ conda env create -f environment-chapter7.yml\n```\n\nOnce you've installed the dependencies, you can activate the `conda` environment and spin up the notebooks as follows:\n\n```bash\n$ conda activate book # or conda activate book-chapter7\n$ jupyter notebook\n```\n\n## FAQ\n\n### When trying to clone the notebooks on Kaggle I get a message that I am unable to access the book's Github repository. How can I solve this issue?\n\nThis issue is likely due to a missing internet connection. When running your first notebook on Kaggle you need to enable internet access in the settings menu on the right side. \n\n### How do you select a GPU on Kaggle?\n\nYou can enable GPU usage by selecting *GPU* as *Accelerator* in the settings menu on the right side.\n\n## Citations\n\nIf you'd like to cite this book, you can use the following BibTeX entry:\n\n```\n@book{tunstall2022natural,\n  title={Natural Language Processing with Transformers: Building Language Applications with Hugging Face},\n  author={Tunstall, Lewis and von Werra, Leandro and Wolf, Thomas},\n  isbn={1098103246},\n  url={https://books.google.ch/books?id=7hhyzgEACAAJ},\n  year={2022},\n  publisher={O'Reilly Media, Incorporated}\n}\n```\n"
    },
    {
      "name": "posit-dev/positron",
      "stars": 3176,
      "img": "https://avatars.githubusercontent.com/u/107264312?s=40&v=4",
      "owner": "posit-dev",
      "repo_name": "positron",
      "description": "Positron, a next-generation data science IDE",
      "homepage": "https://positron.posit.co",
      "language": "TypeScript",
      "created_at": "2022-05-24T20:22:25Z",
      "updated_at": "2025-04-23T03:45:45Z",
      "topics": [],
      "readme": "# Positron  <a href=\"https://github.com/posit-dev/positron\"><img src=\"positron-product-icons/positron.png\" align=\"right\" height=\"138\" alt=\"Positron\" /></a>\n\nWhat is [Positron](https://positron.posit.co/)?\n\n- A next-generation data science IDE built by [Posit PBC](https://posit.co/)\n- An extensible, polyglot tool for writing code and exploring data\n- A familiar environment for reproducible authoring and publishing\n\n> [!IMPORTANT]\n> Positron is an early stage project under active development and may [not yet be a good fit for you](https://positron.posit.co/start#is-positron-for-me). If you are interested in experimenting with it, we welcome your feedback!\n\n\n\nhttps://github.com/posit-dev/positron/assets/29187501/d1c4c9f0-7bd5-4132-bc24-c3d981ad0056\n\n\n## Get started using Positron\n\nCheck out [our website](https://positron.posit.co/) for information on what you should do before [installing Positron](https://positron.posit.co/download), troubleshooting Positron, and more. Our [FAQ](https://positron.posit.co/faqs) also covers some common or expected questions.\n\nPositron is built on [Code OSS](https://github.com/microsoft/vscode). To learn about basic features like commands, settings, using source control, and more, see the [VS Code documentation](https://code.visualstudio.com/docs).\n\n## Install Positron\n\nInstall the latest release of Positron from our [website](https://positron.posit.co/download), or find other versions [here on GitHub](https://github.com/posit-dev/positron/releases). Currently, Positron is producing pre-release builds from a continuous integration (CI) system for macOS, Windows, and Linux.\n\n## Share your feedback about Positron\n\nWe invite you to join us on [GitHub Discussions](https://github.com/posit-dev/positron/discussions) to ask questions and share feedback. [Read more](https://positron.posit.co/feedback) about giving feedback and reporting bugs.\n\n## Code of conduct\n\nPlease note that this project is released with a [Contributor Code of\nConduct](https://github.com/posit-dev/positron/?tab=coc-ov-file#readme). By participating\nin this project you agree to abide by its terms.\n\n## License\n\nPositron™ is licensed under the [Elastic License 2.0](https://github.com/posit-dev/positron?tab=License-1-ov-file#readme), a source-available license. [Read more](https://positron.posit.co/licensing) about what this license means and our decision to use it.\n\nPositron™ and the Positron icon™ are trademarks of Posit Software, PBC. All rights reserved.\n\n"
    },
    {
      "name": "instructlab/instructlab",
      "stars": 1250,
      "img": "https://avatars.githubusercontent.com/u/160199024?s=40&v=4",
      "owner": "instructlab",
      "repo_name": "instructlab",
      "description": "InstructLab Core package.  Use this to chat with a model and execute the InstructLab workflow to train a model using custom taxonomy data.",
      "homepage": "https://instructlab.ai",
      "language": "Python",
      "created_at": "2024-02-21T22:59:01Z",
      "updated_at": "2025-04-23T03:17:45Z",
      "topics": [],
      "readme": "# InstructLab 🐶 (`ilab`)\n\n![Lint](https://github.com/instructlab/instructlab/actions/workflows/lint.yml/badge.svg?branch=main)\n![Tests](https://github.com/instructlab/instructlab/actions/workflows/test.yml/badge.svg?branch=main)\n![Build](https://github.com/instructlab/instructlab/actions/workflows/pypi.yaml/badge.svg?branch=main)\n![Release](https://img.shields.io/github/v/release/instructlab/instructlab)\n![License](https://img.shields.io/github/license/instructlab/instructlab)\n\n![`e2e-nvidia-t4-x1.yaml` on `main`](https://github.com/instructlab/instructlab/actions/workflows/e2e-nvidia-t4-x1.yml/badge.svg?branch=main)\n![`e2e-nvidia-l4-x1.yaml` on `main`](https://github.com/instructlab/instructlab/actions/workflows/e2e-nvidia-l4-x1.yml/badge.svg?branch=main)\n![`e2e-nvidia-l40s-x4.yaml` on `main`](https://github.com/instructlab/instructlab/actions/workflows/e2e-nvidia-l40s-x4.yml/badge.svg?branch=main)\n![`e2e-nvidia-l40s-x8.yaml` on `main`](https://github.com/instructlab/instructlab/actions/workflows/e2e-nvidia-l40s-x8.yml/badge.svg?branch=main)\n\n## 📖 Contents\n\n- [Welcome to the InstructLab Core](#welcome-to-the-instructlab-core)\n- [❓ What is InstructLab Core](#-what-is-instructlab-core)\n- [📋 Requirements](#-requirements)\n- [✅ Getting started](#-getting-started)\n  - [🧰 Installing InstructLab Core](#-installing-instructlab-core)\n    - [Install with Apple Metal on M1/M2/M3 Macs](#install-with-apple-metal-on-m1m2m3-macs)\n    - [Install with no GPU acceleration and PyTorch without CUDA bindings](#install-using-pytorch-without-cuda-bindings-and-no-gpu-acceleration)\n    - [Install with AMD ROCm](#install-with-amd-rocm)\n    - [Install with Nvidia CUDA](#install-with-nvidia-cuda)\n  - [🏗️ Initialize `ilab`](#️-initialize-ilab)\n  - [📥 Download the model](#-download-the-model)\n  - [🍴 Serving the model](#-serving-the-model)\n  - [📣 Chat with the model (Optional)](#-chat-with-the-model-optional)\n  - [📇 Configure retrieval-augmented generation (developer preview)](#-configure-retrieval-augmented-generation-developer-preview)\n  - [🚀 Upgrade InstructLab to latest version](#-upgrade-instructlab-to-latest-version)\n- [💻 Creating new knowledge or skills and training the model](#-creating-new-knowledge-or-skills-and-training-the-model)\n  - [🎁 Contribute knowledge or compositional skills](#-contribute-knowledge-or-compositional-skills)\n  - [📜 List and validate your new data](#-list-and-validate-your-new-data)\n  - [🚀 Generate a synthetic dataset](#-generate-a-synthetic-dataset)\n  - [👩‍🏫 Training the model](#-training-the-model)\n    - [✋ Before you begin training](#-before-you-begin-training)\n    - [InstructLab training pipelines](#instructlab-model-training-pipelines)\n    - [Train the model locally](#train-the-model-locally)\n    - [Train the model locally on an M-Series Mac or on Linux using the full pipeline](#train-the-model-locally-on-an-m-series-mac-or-on-linux-using-the-full-pipeline)\n    - [Train the model locally on an M-Series Mac or on Linux using the simple pipeline](#train-the-model-locally-on-an-m-series-mac-or-on-linux-using-the-simple-pipeline)\n    - [Train the model locally with GPU acceleration](#train-the-model-locally-with-gpu-acceleration)\n    - [Train the model in the cloud](#train-the-model-in-the-cloud)\n  - [📜 Test the newly trained model](#-test-the-newly-trained-model)\n  - [🧪 Evaluate the newly trained model](#-evaluate-the-newly-trained-model)\n  - [🍴 Serve the newly trained model](#-serve-the-newly-trained-model)\n- [📣 Chat with the new model (not optional this time)](#-chat-with-the-new-model-not-optional-this-time)\n- [☁️ Upload the new model](#️-upload-the-new-model)\n- [🎁 Submit your new knowledge or skills](#-submit-your-new-knowledge-or-skills)\n- [📬 Contributing](#-contributing)\n\n## Welcome to the InstructLab Core\n\nInstructLab 🐶 uses a novel synthetic data-based alignment tuning method for\nLarge Language Models (LLMs.) The \"**lab**\" in Instruct**Lab** 🐶 stands for\n[**L**arge-Scale **A**lignment for Chat**B**ots](https://arxiv.org/abs/2403.01081) [1].\n\n[1] Shivchander Sudalairaj*, Abhishek Bhandwaldar*, Aldo Pareja*, Kai Xu, David D. Cox, Akash Srivastava*. \"LAB: Large-Scale Alignment for ChatBots\", arXiv preprint arXiv: 2403.01081, 2024. (* denotes equal contributions)\n\n## ❓ What is InstructLab Core\n\n`instructlab` is the Core package for the InstructLab project that contains the `ilab` Command-Line Interface (CLI) tool and allows you to perform the following actions:\n\n1. Download a pre-trained Large Language Model (LLM).\n1. Chat with the LLM.\n\nTo add new knowledge and skills to the pre-trained LLM, add information to the companion [taxonomy](https://github.com/instructlab/taxonomy.git) repository.\n\nAfter you have added knowledge and skills to the taxonomy, you can perform the following actions:\n\n1. Use `ilab` to generate new synthetic training data based on the changes in your local `taxonomy` repository.\n1. Re-train the LLM with the new training data.\n1. Chat with the re-trained LLM to see the results.\n\n```mermaid\ngraph TD;\n  download-->chat\n  chat[Chat with the LLM]-->add\n  add[Add new knowledge<br/>or skill to taxonomy]-->generate[generate new<br/>synthetic training data]\n  generate-->train\n  train[Re-train]-->|Chat with<br/>the re-trained LLM<br/>to see the results|chat\n```\n\nFor an overview of the full workflow, see the [workflow diagram](./docs/workflow.png).\n\n> [!IMPORTANT]\n> We have optimized InstructLab so that community members with commodity hardware can perform these steps. However, running InstructLab on a laptop will provide a low-fidelity approximation of synthetic data generation\n> (using the `ilab data generate` command) and model instruction tuning (using the `ilab model train` command, which uses QLoRA). To achieve higher quality, use more sophisticated hardware and configure InstructLab to use a\n> larger teacher model [such as Mixtral](https://huggingface.co/docs/transformers/model_doc/mixtral).\n\n## 📋 Requirements\n\n- **🍎 Apple M1/M2/M3 Mac or 🐧 Linux system** (tested on Fedora). We anticipate support for more operating systems in the future.\n\n   📋  When installing InstructLab Core on macOS, you may have to run the `xcode-select --install` command, installing the required packages listed.\n\n- C++ compiler\n- Python 3.11\n\n   ⚠️ Python 3.12+ is currently not supported. Some InstructLab dependencies don't work on Python 3.12, yet. It is recommended to use a specific version of Python in the below commands, e.g. `python3.11` instead of simply `python3`.\n\n- Minimum 250GB disk space. Approximately 500GB disk space is recommended for the entire InstructLab end-to-end process.\n\n## ✅ Getting started\n\n- When installing on Fedora Linux, install C++, Python 3.11, and other necessary tools by running the following command:\n\n   ```shell\n   sudo dnf install gcc gcc-c++ make git-core python3.11 python3.11-devel\n   ```\n\n   Some Python version management tools that build Python (instead of using a pre-built binary) may not by default build libraries implemented in C, and CPython when installing a Python version. This can result in the following error when running the `ilab data generate` command: `ModuleNotFoundError: No module named '_lzma'`. This can be resolved by building CPython during the Python installation with the `--enable-framework`. For example for `pyenv` on MacOS: `PYTHON_CONFIGURE_OPTS=\"--enable-framework\" pyenv install 3.x`. You may need to recreate your virtual environment after reinstalling Python.\n\n> [!NOTE]\n> The following steps in this document use [Python venv](https://docs.python.org/3/library/venv.html) for virtual environments. However, if you use another tool such as [pyenv](https://github.com/pyenv/pyenv) or [Conda Miniforge](https://github.com/conda-forge/miniforge) for managing Python environments on your machine continue to use that tool instead. Otherwise, you may have issues with packages that are installed but not found in your virtual environment.\n\n### 🧰 Installing InstructLab Core\n\n1. There are a few ways you can locally install the InstructLab Core package. Select your preferred installation method from the following instructions. You can then install `ilab` and activate your `venv` environment.\n\n   ⚠️ The `python3` binary shown in the installation guides are the Python version that you installed in the above step. The command can also be `python3.11` instead of `python3`. You can check Python's version by `python3 -V`.\n\n   ⏳ `pip install` may take some time, depending on your internet connection. In case the installation fails with error ``unsupported instruction `vpdpbusd'``, append `-C cmake.args=\"-DGGML_NATIVE=off\"` to `pip install` command.\n\n   See [the GPU acceleration documentation](./docs/accelerators/gpu-acceleration.md) for how to to enable hardware acceleration for interaction and training on AMD ROCm, Apple Metal Performance Shaders (MPS), and Nvidia CUDA.\n\n#### Install with Apple Metal on M1/M2/M3 Macs\n\n- Install on Apple metal with:\n\n   ```shell\n   python<version> -m venv --upgrade-deps venv\n   source venv/bin/activate\n   pip cache remove llama_cpp_python\n   pip install instructlab\n   ```\n\n   📋 Make sure your system Python build is `Mach-O 64-bit executable arm64` by using `file -b $(command -v python)`, or if your system is setup with [pyenv](https://github.com/pyenv/pyenv) by using the `file -b $(pyenv which python)` command.\n\n   You can also quickly install using the [Bash script](https://github.com/instructlab/instructlab/blob/main/scripts/ilab-macos-installer.sh).\n\n#### Install using PyTorch without CUDA bindings and no GPU acceleration\n\n- Install on a standard Linux machine with:\n\n   ```shell\n   python<version> -m venv --upgrade-deps venv\n   source venv/bin/activate\n   pip install instructlab\n   ```\n\n   *Additional Build Argument for Intel Macs*\n   If you have a Mac with an Intel CPU, you must add a prefix of\n   `CMAKE_ARGS=\"-DGGML_METAL=off\"` to the `pip install` command to ensure\n   that the build is done without Apple M-series GPU support.\n   `(venv) $ CMAKE_ARGS=\"-DGGML_METAL=off\" pip install ...`\n\n#### Install with AMD ROCm\n\n- Install on AMD ROCm with:\n\n   ```shell\n   python<version> -m venv --upgrade-deps venv\n   source venv/bin/activate\n   pip cache remove llama_cpp_python\n   CMAKE_ARGS=\"-DGGML_HIPBLAS=on \\\n      -DAMDGPU_TARGETS=all \\\n      -DCMAKE_C_COMPILER=/opt/rocm/llvm/bin/clang \\\n      -DCMAKE_CXX_COMPILER=/opt/rocm/llvm/bin/clang++ \\\n      -DCMAKE_PREFIX_PATH=/opt/rocm \\\n      -DGGML_NATIVE=off\" \\\n      pip install 'instructlab[rocm]' \\\n      --extra-index-url https://download.pytorch.org/whl/rocm6.0\n   ```\n\n   On Fedora 40+, use `-DCMAKE_C_COMPILER=clang-17` and `-DCMAKE_CXX_COMPILER=clang++-17`.\n\n#### Install with Nvidia CUDA\n\n- For the best CUDA experience, installing vLLM is necessary to serve Safetensors format models. You can install using pip install or downloading CUDA from [Nvidia](https://developer.nvidia.com/cuda-downloads)\n\n   ```shell\n   python<version> -m venv --upgrade-deps venv\n   source venv/bin/activate\n   pip install torch psutil\n   pip cache remove llama_cpp_python\n   CMAKE_ARGS=\"-DGGML_CUDA=on -DGGML_NATIVE=off\" pip install 'instructlab[cuda]'\n   pip install -r requirements-vllm-cuda.txt\n   ```\n\n4. From your `venv` environment, verify `ilab` is installed correctly, by running the `ilab` command.\n\n   ```shell\n   ilab\n   ```\n\n   *Example output of the `ilab` command*\n\n   ```shell\n   (venv) $ ilab\n   Usage: ilab [OPTIONS] COMMAND [ARGS]...\n\n     CLI for interacting with InstructLab.\n\n     If this is your first time running InstructLab, it's best to start with `ilab config init` to create the environment.\n\n   Options:\n     --config PATH  Path to a configuration file.  [default:\n                    /home/user/.config/instructlab/config.yaml]\n     -v, --verbose  Enable debug logging (repeat for even more verbosity)\n     --version      Show the version and exit.\n     --help         Show this message and exit.\n\n   Commands:\n     config    Manage InstructLab configuration.\n     data      Generate synthetic data.\n     model     Manage GenAI (LLM) models.\n     process   Manage running processes.\n     rag       Retrieval-Augmented Generation (RAG).\n     system    Execute system commands.\n     taxonomy  Manage taxonomy datasets.\n\n   Aliases:\n     chat      model chat\n     generate  data generate\n     serve     model serve\n     train     model train\n   ```\n\n> [!IMPORTANT]\n> Every `ilab` command needs to be run from within your Python virtual environment. You can enter the Python environment by running the `source venv/bin/activate` command. Or you can simply create a symbolic link to a directory that is included in your system’s `$PATH`, for example in Linux: `mkdir -p ~/bin/ && ln -s /path/venv/bin/ilab ~/bin/ilab`, or use an alias: `alias ilab='/path/venv/bin/ilab'` (add it to `~/.bashrc` or `~/.zshrc` to persist).\n\n5. *Optional:* You can enable tab completion for the `ilab` command.\n\n   #### Bash (version 4.4 or newer)\n\n   Enable tab completion in `bash` with the following command:\n\n   ```sh\n   eval \"$(_ILAB_COMPLETE=bash_source ilab)\"\n   ```\n\n   To have this enabled automatically every time you open a new shell,\n   you can save the completion script and source it from `~/.bashrc`:\n\n   ```sh\n   _ILAB_COMPLETE=bash_source ilab > ~/.ilab-complete.bash\n   echo \". ~/.ilab-complete.bash\" >> ~/.bashrc\n   ```\n\n   📋 To use Bash version 4.4 or higher on macOS (default is 3.2.57), ensure your login shell is set to the updated version. You can verify this with `echo $SHELL`. If you encounter the error `bash: complete: nosort: invalid option name`, check your terminal or configuration files (e.g., ~/.bash_profile, ~/.bashrc, ~/.profile) to see whether they are referencing the old version for login.\n\n   #### Zsh\n\n   Enable tab completion in `zsh` with the following command:\n\n   ```sh\n   eval \"$(_ILAB_COMPLETE=zsh_source ilab)\"\n   ```\n\n   To have this enabled automatically every time you open a new shell,\n   you can save the completion script and source it from `~/.zshrc`:\n\n   ```sh\n   _ILAB_COMPLETE=zsh_source ilab > ~/.ilab-complete.zsh\n   echo \". ~/.ilab-complete.zsh\" >> ~/.zshrc\n   ```\n\n   #### Fish\n\n   Enable tab completion in `fish` with the following command:\n\n   ```sh\n   _ILAB_COMPLETE=fish_source ilab | source\n   ```\n\n   To have this enabled automatically every time you open a new shell,\n   you can save the completion script and source it from `~/.bashrc`:\n\n   ```sh\n   _ILAB_COMPLETE=fish_source ilab > ~/.config/fish/completions/ilab.fish\n   ```\n\n### 🏗️ Initialize `ilab`\n\n1. Initialize `ilab` by running the following command:\n\n   ```shell\n   ilab config init\n   ```\n\n2. When prompted, clone the `https://github.com/instructlab/taxonomy.git` repository into the current directory by typing **enter**\n\n   **Optional**: If you want to point to an existing local clone of the `taxonomy` repository, you can pass the path interactively or alternatively with the `--taxonomy-path` flag.\n\n   `ilab` will use the default configuration file unless otherwise specified. You can override this behavior with the `--config` parameter for any `ilab` command.\n\n4. When prompted, provide the path to your default model. Otherwise, the default of a quantized [Granite](https://huggingface.co/instructlab/granite-7b-lab-GGUF) model is used.\n\n   *Example output of steps 1 - 3*\n\n   ```shell\n   Path to your model [/home/user/.cache/instructlab/models/granite-7b-lab-Q4_K_M.gguf]: <ENTER>\n   ```\n\n   You can download this model with `ilab model download` command as well.\n\n5. When prompted, please choose a train profile. Train profiles are GPU specific profiles that enable accelerated training behavior. If you are on MacOS or a Linux machine without a dedicated GPU, please choose `No Profile (CPU, Apple Metal, AMD ROCm)` by hitting Enter. There are various flags you can utilize with individual `ilab` commands that allow you to utilize your GPU if applicable.\n\n   *Example output of selecting a training profile*\n\n   ```shell\n   ----------------------------------------------------\n            Welcome to the InstructLab CLI\n   This guide will help you to setup your environment\n   ----------------------------------------------------\n\n   Please provide the following values to initiate the environment [press Enter for defaults]:\n   Path to taxonomy repo [/home/user/.local/share/instructlab/taxonomy]:\n   Path to your model [/home/user/.cache/instructlab/models/granite-7b-lab-Q4_K_M.gguf]:\n\n   You can download this model with `ilab model download` command as well.\n\n4. The InstructLab Core package auto-detects your hardware and select the exact system profile that matches your machine. System profiles populate the `config.yaml` file with the proper parameter values based on your detected CPU/GPU types. This system is only applicable to Apple M-Series Chips, Nvidia GPUs, and Intel/AMD CPUs.\n\n   *Example output of profile auto-detection*\n\n   ```shell\n   Generating config file and profiles:\n       /home/user/.config/instructlab/config.yaml\n       /home/user/.local/share/instructlab/internal/train_configuration/profiles\n\n   We have detected the AMD CPU profile as an exact match for your system.\n\n    --------------------------------------------\n      Initialization completed successfully!\n   You're ready to start using `ilab`. Enjoy!\n   --------------------------------------------\n   ```\n\n5. If there is not an exact match for your system, you can manually select a system profile when prompted. There are various flags you can utilize with individual `ilab` commands that allow you to utilize your GPU if applicable.\n\n   *Example output of selecting a system profile*\n\n   ```shell\n   Please choose a system profile to use.\n   System profiles apply to all parts of the config file and set hardware specific defaults for each command.\n   First, please select the hardware vendor your system falls into\n   [1] APPLE\n   [2] INTEL\n   [3] AMD\n   [4] NVIDIA\n   Enter the number of your choice [0]: 1\n   You selected: APPLE\n   Next, please select the specific hardware configuration that most closely matches your system.\n   [0] No system profile\n   [1] APPLE M1 ULTRA\n   [2] APPLE M1 MAX\n   [3] APPLE M2 MAX\n   [4] APPLE M2 ULTRA\n   [5] APPLE M2 PRO\n   [6] APPLE M2\n   [7] APPLE M3 MAX\n   [8] APPLE M3 PRO\n   [9] APPLE M3\n   Enter the number of your choice [hit enter for hardware defaults] [0]: 8\n   You selected: /home/<user>/.local/share/instructlab/internal/system_profiles/apple/m3/m3_pro.yaml\n\n   --------------------------------------------\n   Initialization completed successfully!\n   You're ready to start using `ilab`. Enjoy!\n   --------------------------------------------\n   ```\n\n   The GPU profiles are listed by GPU type and number of GPUs present. If you happen to have a GPU configuration with a similar amount of vRAM as any of the above profiles, feel free to try them out!\n\n### `ilab` directory layout after initializing your system\n\nAfter running `ilab config init` your directories will look like the following on a Linux system:\n\n| **Directory**                              | **Description**                                                                 |\n|--------------------------------------------|---------------------------------------------------------------------------------|\n| `~/.cache/instructlab/models/`             | Contains all downloaded large language models, including the saved output of ones you generate with ilab.|\n| `~/.local/share/instructlab/datasets/`     | Contains data output from the SDG phase, built on modifications to the taxonomy repository.   |\n| `~/.local/share/instructlab/taxonomy/`     | Contains the skill and knowledge data.                                              |\n| `~/.local/share/instructlab/checkpoints/`  | Contains the output of the training process.                                |\n| `~/.config/instructlab/config.yaml`        | Contains the `config.yaml` file |\n\nYou can view your `config.yaml` file with the following command (use `-k` to show a specific config section and/or `-wc` to show without comments):\n\n```shell\nilab config show\n```\n\n### 📥 Download the model\n\nFor detailed documentation on the InstructLab LLMs and their functions, see the [InstructLab LLM guide](docs/user/instructlab_models.md).\n\n#### Downloading the default InstructLab models\n\n- Run the `ilab model download` command to download a compact pre-trained version of the `granite-7b-lab-GGUF`, `merlinite-7b-lab-GGUF`, and `Mistral-7B-Instruct-v0.2-GGUF` models (~4.4G each) from HuggingFace.\n\n   ```shell\n   ilab model download\n   ```\n\n   *Example output*\n\n   ```shell\n   Downloading model from Hugging Face:\n      Model: instructlab/granite-7b-lab-GGUF@main\n      Destination: /Users/<user>/.cache/instructlab/models\n\n   ᕦ(òᴗóˇ)ᕤ instructlab/granite-7b-lab-GGUF model download completed successfully! ᕦ(òᴗóˇ)ᕤ\n\n   Downloading model from Hugging Face:\n      Model: instructlab/merlinite-7b-lab-GGUF@main\n      Destination: /Users/<user>/.cache/instructlab/models\n\n   ᕦ(òᴗóˇ)ᕤ instructlab/merlinite-7b-lab-GGUF model download completed successfully! ᕦ(òᴗóˇ)ᕤ\n\n   Downloading model from Hugging Face:\n      Model: TheBloke/Mistral-7B-Instruct-v0.2-GGUF@main\n      Destination: /Users/<user>/.cache/instructlab/models\n\n   TheBloke/Mistral-7B-Instruct-v0.2-GGUF requires a HF Token to be set.\n   Please use '--hf-token' or 'export HF_TOKEN' to download all necessary models.\n\n   Available models (`ilab model list`):\n   +-------------------------------+---------------------+--------+---------------------------------------------+\n   | Model Name                    | Last Modified       | Size   | Absolute path                               |\n   +-------------------------------+---------------------+--------+---------------------------------------------+\n   | merlinite-7b-lab-Q4_K_M.gguf  | 2024-11-21 22:24:50 | 4.1 GB | /path/to/model/merlinite-7b-lab-Q4_K_M.gguf |\n   | granite-7b-lab-Q4_K_M.gguf    | 2024-11-30 11:16:44 | 3.8 GB | /path/to/model/granite-7b-lab-Q4_K_M.gguf   |\n   +-------------------------------+---------------------+--------+---------------------------------------------+\n   ```\n\n   You may be prompted to use your Hugging Face token to download the `Mistral-7B-Instruct-v0.2-GGUF` model.\n\n   ```shell\n   ilab model download --hf-token <your-huggingface-token>\n   ```\n\n   Hugging Face `hf_transfer` is enabled by default for faster downloads (lacks proxy support), and can be disabled with `HF_HUB_ENABLE_HF_TRANSFER=0`. If you have issues connecting to Hugging Face, refer to the [Hugging Face discussion forum](https://discuss.huggingface.co/) for more details.\n\n#### Downloading a specific model from a Hugging Face repository\n\n- Specify a repository, model, and a Hugging Face token if necessary. More information about Hugging Face tokens can be found [here](https://huggingface.co/docs/hub/en/security-tokens)\n\n   ```shell\n   ilab model download --repository instructlab/granite-7b-lab-GGUF --filename granite-7b-lab-Q4_K_M.gguf --hf-token <your-huggingface-token>\n   ```\n\n#### Downloading an entire Hugging Face repository (Safetensors Model)\n\n- Specify a repository, and a Hugging Face token if necessary. For example:\n\n   ```shell\n   ilab model download --repository instructlab/granite-7b-lab --hf-token <your-hugginface-token>\n   ```\n\n   These types of models are useful for GPU-enabled systems or anyone looking to serve a model using vLLM. InstructLab provides Safetensor versions of our Granite models on HuggingFace.\n\n#### Downloading a specific model from an OCI repository via Skopeo\n\n- Specify an OCI-compliant repository and release for download. Ensure you have logged into the registry you wish to download from with Skopeo if necessary. For example:\n\n   ```shell\n   ilab model download -rp docker://instructlab/granite-7b-lab -rl latest\n   ```\n\n### 🍴 Serving the model\n\n- Serve the model with the `ilab model serve` command:\n\n   ```shell\n   ilab model serve\n   ```\n\n   *Example output of a model that is served and ready*\n\n   ```shell\n   (venv) $ ilab model serve\n   INFO 2024-03-02 02:21:11,352 lab.py:201 Using model 'models/ggml-granite-7b-lab-Q4_K_M.gguf' with -1 gpu-layers and 4096 max context size.\n   Starting server process\n   After application startup complete see http://127.0.0.1:8000/docs for API.\n   Press CTRL+C to shut down the server.\n   ```\n\n   📋 If multiple `ilab` clients try to connect to the same InstructLab server at the same time, the 1st will connect to the server while the others will start their own temporary server. This will require additional resources on the host machine.\n\n   Model will be available under both long form absolute path, and a shorthand name.\n\n   The model server will behave in the same way as the backend chosen for execution, either Llama.cpp or vllm.\n\n#### Serving other models\n\n- You can serve a non-default model (e.g. Mixtral-8x7B-Instruct-v0.1) with the following example command:\n\n   ```shell\n   ilab model serve --model-path ~/.cache/instructlab/models/mixtral-8x7b-instruct-v0.1.Q4_K_M.gguf\n   ```\n\n- You can serve a non-default Safetensors model (e.g. granite-7b-lab). This requires a GPU.\n\n   Verify vllm is installed:\n\n   ```shell\n   pip show vllm\n   ```\n\n   If it is not, please run:\n\n   ```shell\n   pip install -r requirements-vllm-cuda.txt\n   ```\n\n   Then you can serve a Safetensors model:\n\n   ```shell\n   ilab model serve --model-path ~/.cache/instructlab/models/instructlab/granite-7b-lab\n   ```\n\n### 📣 Chat with the model (Optional)\n\n- Because you're serving the model in one terminal window, you will have to create a new terminal window and re-activate your Python virtual environment to run `ilab model chat` command:\n\n   ```shell\n   source venv/bin/activate\n   ilab model chat\n   ```\n\n- Chat with a non-default model (e.g. Mixtral-8x7B-Instruct-v0.1):\n\n   ```shell\n   source venv/bin/activate\n   ilab model chat --model ~/.cache/instructlab/models/mixtral-8x7b-instruct-v0.1.Q4_K_M.gguf\n   ```\n\n   > [!TIP]\n   > The usage of `--model` necessitates that the existing server has that model. If not, you must exit the server. `--model` in `ilab model chat` has the ability to start a server on your behalf with the specified model if one is not already running on the port.\n\n   Before you start adding new skills and knowledge to your model, you can check its baseline performance by asking it a question such as `what is the capital of Canada?`.\n   The model needs to be trained with the generated synthetic data to use the new skills or knowledge\n\n   *Example output of chatbot*\n\n   ```shell\n   (venv) $ ilab model chat\n   ╭────────────────────────────────────────────────────────────────────────────────────────────────────────────────── system ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n   │ Welcome to InstructLab Chat w/ GGML-GRANITE-7B-lab-Q4_K_M (type /h for help)                                                                                                                                                             │\n   ╰────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n   >>> what is the capital of Canada?                                                                                                                                                                                                [S][default]\n   ╭────────────────────────────────────────────────────────────────────────────────────────────────────── ggml-granite-7b-lab-Q4_K_M ───────────────────────────────────────────────────────────────────────────────────────────────────────╮\n   │ The capital city of Canada is Ottawa. It is located in the province of Ontario, on the southern banks of the Ottawa River in the eastern portion of southern Ontario. The city serves as the political center for Canada, as it is home to │\n   │ Parliament Hill, which houses the House of Commons, Senate, Supreme Court, and Cabinet of Canada. Ottawa has a rich history and cultural significance, making it an essential part of Canada's identity.                                   │\n   ╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── elapsed 12.008 seconds ─╯\n   >>>                                                                                                                                                                                                                               [S][default]\n   ```\n\n- You can also adjust the temperature settings of the model with the `--temperature` or `-t` flag. The default is 1.0.\n\n   ```shell\n   ilab model chat --model ~/.cache/instructlab/models/mixtral-8x7b-instruct-v0.1.Q4_K_M.gguf --temperature 0.5\n   ```\n\n### 📇 Configure retrieval-augmented generation (developer preview)\n\nIf you want to use the developer preview of retrieval-augmented generation (RAG), you need to enable developer preview features by setting `ILAB_FEATURE_SCOPE` to `DevPreviewNoUpgrade`. This tells InstructLab to enable developer preview features that might not be upgradable.  For many popular shells, this is done via the following:\n\n   ```shell\n   export ILAB_FEATURE_SCOPE=DevPreviewNoUpgrade\n   ```\n\nIngesting documents into a vector index and retrieving them during a chat both require a vector embedding model.  The default embedding model for the developer preview of RAG is `ibm-granite/granite-embedding-125m-english`.  You can obtain this model before you begin the steps below by running:\n\n   ```shell\n      ilab model download -rp ibm-granite/granite-embedding-125m-english\n   ```\n\n#### Converting documents\n\nOnce you have enabled developer preview features, the first step is to convert your content into a structured form and then index that content for RAG to use with `ilab model chat`.  There are three options for conversion:\n\n*Converting documents from a directory:* If you have all the source documents (e.g., PDF, Markdown) that you want to convert in a single local directory, you can convert them:\n\n   ```shell\n      ilab rag convert --input-dir <path-to-source-document-directory> --output-dir <path-to-converted-document-directory>\n   ```\n\nThis capability has been tested with PDF, Markdown, and Microsoft Word format documents.  Any of the [formats supported by Docling](https://ds4sd.github.io/docling/reference/document_converter/#docling.document_converter.InputFormat) should work.\n\n*Converting documents from a taxonomy:* If you do not specify an input directory, the `convert` command will pull from the default --taxonomy-path value, unless specified otherwise.  For example, to use all of the documents in all of the knowledge nodes in the taxonomy in the default location you can run:\n\n   ```shell\n   ilab rag convert --taxonomy-base=empty --output-dir <path-to-converted-document-directory>\n   ```\n\n*Converting documents within SDG:* The SDG commands will convert documents along with its primary goal (to generate synthetic data).  If you ran any configuration of `ilab data generate`, then the knowledge documents in the taxonomy that generated synthetic data will already be converted to a structured form.  In this case, there is no need to run the `ilab rag convert` command again.  For example, the following command mentioned in the [🚀 Generate a synthetic dataset](#-generate-a-synthetic-dataset) section converts all of the documents referenced in the entire contents of the taxonomy in the default location (and outputs them to a location that is the default input location for the ingestion command below):\n\n   ```shell\n   ilab data convert --taxonomy-base=empty\n   ```\n\nIn the last two examples, you can also omit the `--taxonomy-base=empty` flag. In this case, the conversion will only use documents referenced by the new or modified YAML files within your taxonomy rather than all of the documents from the entire taxonomy.\n\n#### Ingesting documents\n\nOnce documents are converted, you can use the `ilab rag ingest` command to ingest the converted content into a vector database index file.\n\n*Ingesting documents from SDG:* By default, `ilab rag ingest` pulls the content to ingest from where SDG stores them, so if you converted documents within SDG (see above) you can ingest them using the following command:\n\n   ```shell\n   ilab rag ingest\n   ```\n\n*Ingesting documents from ilab rag convert*: Alternatively, if you ran `ilab rag convert` to convert your documents, without SDG, then the output directory you used for `ilab rag convert` should be the input directory you specify for `ilab rag ingest`, e.g.:\n\n   ```shell\n   ilab rag ingest --input-dir=<path-to-converted-document-directory>\n   ```\n\n#### Getting answers from RAG\n\nTo chat with the model with RAG enabled run:\n\n   ```shell\n   ilab model chat --rag\n   ```\n\nWith the `--rag` option specified, the chat will search the ingested content and provide that content to the model for use in answering whatever questions you ask.   See [📣 Chat with the model (Optional)](#-chat-with-the-model-optional) for more details about `ilab model chat`.\n\n## 🚀 Upgrade InstructLab to latest version\n\n1. To upgrade InstructLab to the latest version, use the following command:\n\n   ```shell\n   pip install instructlab --upgrade\n   ```\n\n2. It is recommended to reinitialize your InstructLab environment after upgrading to the latest version.\n\n   ```shell\n   ilab config init\n   ```\n\n> [!NOTE]\n> If you are having issues with your upgrade, you can attempt `pip install --force-reinstall --no-cache-dir instructlab` to force reinstallation of InstructLab without using the pip cache\n\n## 💻 Creating new knowledge or skills and training the model\n\n### 🎁 Contribute knowledge or compositional skills\n\n1. Contribute new knowledge or compositional skills to your local [taxonomy](https://github.com/instructlab/taxonomy.git) repository.\n\nDetailed contribution instructions can be found in the [taxonomy repository](https://github.com/instructlab/taxonomy/blob/main/README.md).\n\n> [!IMPORTANT]\n> There is a limit to how much content can exist in the question/answer pairs for the model to process. Due to this, only add a maximum of around 2300 words to your question and answer seed example pairs in the `qna.yaml` file.\n\n### 📜 List and validate your new data\n\n- You can use the `ilab taxonomy diff` command to ensure `ilab` is registering your new knowledge or skills and your contributions are properly formatted. This command displays any new or modified YAML files within your taxonomy tree. For example, the following is the expected result of a valid compositional skill contribution after adding a new skill called `foo-lang` to the freeform writing subdirectory:\n\n   ```shell\n   (venv) $ ilab taxonomy diff\n   compositional_skills/writing/freeform/foo-lang/qna.yaml\n   Taxonomy in $HOME/.local/share/instructlab/taxonomy is valid :)\n   ```\n\n- You can also validate your entire taxonomy by performing a diff against an empty base by using the `--taxonomy-base=empty` argument:\n\n   ```shell\n   (venv) $ ilab taxonomy diff --taxonomy-base=empty\n   compositional_skills/general/tables/empty/qna.yaml\n   compositional_skills/general/tables/editing/add_remove/qna.yaml\n   ...\n   Taxonomy in $HOME/.local/share/instructlab/taxonomy is valid :)\n   ```\n\n### 🚀 Generate a synthetic dataset\n\nBefore following these instructions, ensure the existing model you are adding skills or knowledge to is still running. Alternatively, `ilab data generate` can start a server for you if you provide a fully qualified model path via `--model`.\n\n> [!IMPORTANT]\n> The `full` pipeline only works with Mixtral and Mistral Instruct Family models as the teacher model.\n> The `simple` pipeline only works with Merlinite 7b Lab as the teacher model. This is due to only supporting specific model prompt templates.\n\n1. To generate a synthetic dataset based on your newly added knowledge or skill set in [taxonomy](https://github.com/instructlab/taxonomy.git) repository, run the following command:\n\n   *Default SDG*\n\n    ```shell\n   ilab data generate\n   ```\n\n   *With GPU acceleration*\n\n   ```shell\n   ilab data generate --pipeline full --gpus <NUM_OF_GPUS>\n   ```\n\n   *Without GPU acceleration:*\n\n   ```shell\n   ilab data generate --pipeline simple\n   ```\n\n   *Using a non-default model (e.g. Mixtral-8x7B-Instruct-v0.1) to generate data:*\n\n   ```shell\n   ilab data generate --model ~/.cache/instructlab/models/mistralai/mixtral-8x7b-instruct-v0.1 --pipeline full --gpus 4\n   ```\n\n   ⏳ This can take from 15 minutes to 1+ hours to complete, depending on your computing resources.\n\n   *Example output of `ilab data generate`*\n\n   ```shell\n   (venv) $ ilab data generate\n   INFO 2024-07-30 19:57:44,093 numexpr.utils:161: NumExpr defaulting to 8 threads.\n   INFO 2024-07-30 19:57:44,452 datasets:58: PyTorch version 2.3.1 available.\n   Generating synthetic data using 'full' pipeline, 'mistral-7b-instruct-v0.2.Q4_K_M.gguf' model, './taxonomy' taxonomy, against http://localhost:8000/v1 server\n   INFO 2024-07-30 19:57:45,084 instructlab.sdg:375: Synthesizing new instructions. If you aren't satisfied with the generated instructions, interrupt training (Ctrl-C) and try adjusting your YAML files. Adding more examples may help.\n   INFO 2024-07-30 19:57:45,090 instructlab.sdg.pipeline:153: Running pipeline single-threaded\n   INFO 2024-07-30 19:57:47,820 instructlab.sdg.llmblock:51: LLM server supports batched inputs: False\n   INFO 2024-07-30 19:57:47,820 instructlab.sdg.pipeline:197: Running block: gen_skill_freeform\n   INFO 2024-07-30 19:57:47,820 instructlab.sdg.pipeline:198: Dataset({\n      features: ['task_description', 'seed_question', 'seed_response'],\n      num_rows: 5\n   })\n   INFO 2024-07-30 20:02:16,455 instructlab.sdg:411: Generated 1 samples\n   ...\n   ```\n\n   The synthetic data set will be two files in the newly created in the datasets directory: `~/.local/share/instructlab/datasets`. These files will be named `skills_train_msgs_*.jsonl` and `knowledge_train_msgs_*.jsonl`.\n\n2. Verify the files have been created by running the `ilab data list` command.\n\n   ```shell\n   (venv) $ ilab data list\n   mistral-7b-instruct-v0.2.Q4_K_M 2024-11-12 02:37:38\n   +--------------------------------------------------------------------+---------------------+-----------+\n   | Dataset                                                            | Created At          | File size |\n   +--------------------------------------------------------------------+---------------------+-----------+\n   | knowledge_train_msgs_2024-11-12T02_37_38.jsonl                     | 2024-11-12 05:38:26 | 3.92 MB   |\n   | messages_mistral-7b-instruct-v0.2.Q4_K_M_2024-11-12T02_37_38.jsonl | 2024-11-12 05:38:21 | 317.08 KB |\n   | node_datasets_2024-11-12T02_37_38/compositional_skills_valid.jsonl | 2024-11-12 03:07:58 | 139.00 KB |\n   | node_datasets_2024-11-12T02_37_38/knowledge_valid_p07.jsonl        | 2024-11-12 05:38:21 | 3.84 MB   |\n   | node_datasets_2024-11-12T02_37_38/knowledge_valid_p10.jsonl        | 2024-11-12 05:38:21 | 7.25 MB   |\n   | node_datasets_2024-11-12T02_37_38/mmlubench_knowledge_valid.jsonl  | 2024-11-12 05:38:21 | 48.94 KB  |\n   | skills_train_msgs_2024-11-12T02_37_38.jsonl                        | 2024-11-12 05:38:29 | 7.36 MB   |\n   | test_mistral-7b-instruct-v0.2.Q4_K_M_2024-11-12T02_37_38.jsonl     | 2024-11-12 02:37:38 | 27.25 KB  |\n   | train_mistral-7b-instruct-v0.2.Q4_K_M_2024-11-12T02_37_38.jsonl    | 2024-11-12 05:38:21 | 272.94 KB |\n   +--------------------------------------------------------------------+---------------------+-----------+\n   ```\n\n*Generating synthetic data on a different model:* It is also possible to run the generate step against a different model via an\nOpenAI-compatible API. For example, the one spawned by `ilab model serve` or any remote or locally hosted LLM (e.g. via [`ollama`](https://ollama.com/), [`LM Studio`](https://lmstudio.ai), etc.). Run the following command:\n\n   ```shell\n   ilab data generate --endpoint-url http://localhost:8000/v1\n   ```\n\n*Generating synthetic data on the entire taxonomy repo:* You can generate a synthetic dataset based on the entire contents of the taxonomy repo using the `--taxonomy-base=empty` option:\n\n   ```shell\n   ilab data generate --taxonomy-base=empty\n   ```\n\n### 👩‍🏫 Training the model\n\nThere are many options for training the model with your synthetic data-enhanced dataset.\n\n### ✋ Before you begin training\n\nThere are a few models you need to download before running the InstructLab end-to-end workflow locally.\n\n- Download the `granite-7b-lab` model for training:\n\n   ```shell\n   ilab model download --repository instructlab/granite-7b-lab\n   ```\n\n- Download the `prometheus-8x7b-v2.0` for *multi-phase training* and *benchmark evaluation*. This model is not required for `simple` or `full` training.\n\n   ```shell\n   ilab model download --repository prometheus-eval/prometheus-8x7b-v2.0 --hf-token <your-huggingface-token>\n   ```\n\n#### InstructLab model training pipelines\n\n`ilab model train` has three pipelines: `simple`, `full`, and `accelerated`. The default is `full`.\n\n1. `simple` uses an SFT Trainer on Linux and MLX on MacOS. This type of training takes roughly an hour and produces the lowest fidelity model but should indicate if your data is being picked up by the training process.\n2. `full` uses a custom training loop and data processing functions for the granite family of models. This loop is optimized for CPU and MPS functionality. Please use `--pipeline=full` in combination with `--device=cpu` (Linux) or `--device=mps` (MacOS). You can also use `--device=cpu` on a MacOS machine. However, MPS is optimized for better performance on these systems.\n3. `accelerated` uses the instructlab-training library which supports GPU accelerated and distributed training. The `full` loop and data processing functions are either pulled directly from or based off of the work in this library.\n\nAfter running `ilab model train`, the output locations depend on the chosen pipeline or strategy:\n\n| **Pipeline/Strategy**              | **Operating System** | **Output Location/Details**                                                                                             |\n|------------------------------------|----------------------|--------------------------------------------------------------------------------------------------------------------------|\n| `simple`                           | Linux                | Model saved in `models` directory as `ggml-model-f16.gguf`.                                                              |\n| `simple`                           | MacOS                | Model saved in `<model_name>-mlx-q` directory.                                                                           |\n| `full`                             | Linux & MacOS        | `.bin` and `.gguf` models saved in `~/.local/share/instructlab/checkpoints/hf_format` directory. Two models in each `sample_*` directory: one quantized (`Q4-M-K` suffix) and one full precision. |\n| `accelerated`                      | Linux                | Models saved in `~/.local/share/instructlab/checkpoints`. Can be evaluated with `ilab model evaluate` to choose the best one. |\n| `lab-multiphase`                   | Linux                | Phase 1 models saved in `~/.local/share/instructlab/phased/phase1/checkpoints` (Knowledge training). Phase 2 models saved in `~/.local/share/instructlab/phased/phase2/checkpoints` (Skills training). Evaluation is run for phase 2 to identify the best checkpoint. |\n| `lab-skills-only`                  | Linux                | Phase 2 models saved in `~/.local/share/instructlab/phased/phase2/checkpoints` (Skills training). Evaluation is run for phase 2 to identify the best checkpoint. |\n\nTo limit training time, you can adjust the `num_epoch` parameter in the `config.yaml` file. The maximum number of epochs for running the InstructLab end-to-end workkflow is 10.\n\n#### Train the model locally\n\n- Train the model with your synthetic data with the `ilab model train` command:\n\n   ```shell\n   ilab model train --data-path <path-to-sdg-dataset>\n   ```\n\n   ⏳ This step can potentially take **several hours** to complete depending on your computing resources. Please stop `ilab model chat` and `ilab model serve` first to free resources.\n\nWhen running multi phase training evaluation is run on each phase, we will tell you which checkpoint in this folder performs the best.\n\n#### Train the model locally on an M-series Mac or on Linux using the full pipeline\n\n- To train the model locally on your M-Series Mac using our full pipeline and MPS or on your Linux laptop/desktop using CPU:\n\n   ```shell\n   ilab model train --pipeline full --device mps --data-path <path-to-sdg-dataset>\n   ```\n\n   ```shell\n   ilab model train --pipeline full --device cpu --data-path <path-to-sdg-dataset>\n   ```\n\n   *Example command*\n\n   ```shell\n   ilab model train --pipeline full --device cpu --data-path ~/.local/share/instructlab/datasets/knowledge_train_msgs_2024-10-23T09_14_44.jsonl\n   ```\n\n   ⏳ This process will take a while to complete. If you run for ~8 epochs it will take several hours.\n\n   `ilab model train` outputs a directory for each epoch that resembles the following structure:\n\n   ```shell\n   $ ls ~/.local/share/instructlab/checkpoints/hf_format/samples_0/\n   ├── added_tokens.json\n   ├── config.json\n   ├── pytorch_model.bin\n   ├── pytorch_model.gguf\n   ├── pytorch_model-Q4_K_M.gguf\n   ├── special_tokens_map.json\n   ├── tokenizer.json\n   ├── tokenizer_config.json\n   └── tokenizer.model\n   ```\n\n   This entire folder can be served on a system that supports vLLM using the `.bin` model. However, on most laptops you can serve either the full precision gguf: `pytorch_model.gguf` or the 4-bit-quantized one: `pytorch_model-Q4_K_M.gguf`.\n\n#### Train the model locally on an M-series Mac or on Linux using the simple pipeline\n\n- To train the model locally on your M-Series Mac using our simple pipeline and MLX or on your Linux laptop/desktop using an SFT Trainer:\n\n   ```shell\n   ilab model train --pipeline simple\n   ```\n\n   ⏳ This process will take a little while to complete (time can vary based on hardware and output of `ilab data generate` but on the order of 5 to 15 minutes)\n\n   On a Mac `ilab model train` outputs a brand-new model that is saved in the `<model_name>-mlx-q` directory called `adapters.npz` (in `Numpy` compressed array format). For example:\n\n   ```shell\n   (venv) $ ls instructlab-granite-7b-lab-mlx-q/\n   ├── adapters-010.npz\n   ├── adapters-020.npz\n   ├── adapters-030.npz\n   ├── adapters-040.npz\n   ├── adapters-050.npz\n   ├── adapters-060.npz\n   ├── adapters-070.npz\n   ├── adapters-080.npz\n   ├── adapters-090.npz\n   ├── adapters-100.npz\n   ├── adapters.npz\n   ├── added_tokens.json\n   ├── config.json\n   ├── model.safetensors\n   ├── special_tokens_map.json\n   ├── tokenizer.json\n   ├── tokenizer.model\n   └── tokenizer_config.json\n   ```\n\n#### Train the model locally with GPU acceleration\n\n- Training has support for GPU acceleration with Nvidia CUDA or AMD ROCm. Please see [the GPU acceleration documentation](./docs/accelerators/gpu-acceleration.md) for more details. At present, hardware acceleration requires a data center GPU or high-end consumer GPU with at least 18 GB free memory.\n\n   ```shell\n   ilab model train --pipeline accelerated --device cuda --data-path <path-to-sdg-data>\n   ```\n\n   *Example command*\n\n   ```shell\n   ilab model train --pipeline full --device cuda --data-path ~/.local/share/instructlab/datasets/knowledge_train_msgs_2024-10-23T09_14_44.jsonl\n   ```\n\n   This version of `ilab model train` outputs brand-new models that can be served in the `~/.local/share/instructlab/checkpoints` directory.  These models can be run through `ilab model evaluate` to choose the best one.\n\n#### Train the model locally with multi-phase training and GPU acceleration\n\n- `ilab model train` supports multi-phase training. This results in the following workflow:\n\n   1. We train the model on knowledge\n   2. Evaluate the trained model to find the best checkpoint\n   3. We train the model on skills\n   4. We evaluate the model to find the best overall checkpoint\n\n   ```shell\n   ilab model train --strategy lab-multiphase --phased-phase1-data <knowledge train messages jsonl> --phased-phase2-data <skills train messages jsonl> -y\n   ```\n\n   This command takes in two `.jsonl` files from your `datasets` directory, one is the knowledge jsonl and the other is a skills jsonl. The `-y` flag skips an interactive prompt asking the user if they are sure they want to run multi-phase training.\n\n   ⏳ This command may take 3 or more hours depending on the size of the data and number of training epochs you run.\n\n#### Train the model in the cloud\n\nFollow the instructions in [Training](./notebooks/README.md).\n\n⏳ Approximate amount of time taken on each platform:\n\n- *Google Colab*: **5-10 minutes** with a T4 GPU\n- *Kaggle*: **~30 minutes** with a P100 GPU.\n\nAfter that's done, you can play with your model directly in the Google Colab or Kaggle notebook. Model trained on the cloud will be saved on the cloud.\nThe model can also be downloaded and served locally.\n\n### 📜 Test the newly trained model\n\n- Run the following command to test the model:\n\n   ```shell\n   ilab model test\n   ```\n\n   The output from the command will consist of a series of outputs from the model before and after training.\n\n### 🧪 Evaluate the newly trained model\n\nYou can use the `ilab model evaluate` command to evaluate the models you are training with several benchmarks. Currently, four benchmarks are supported.\n\n| Benchmark | Measures | Full Name | Description | Reference |\n| --- | --- | --- | --- | --- |\n| DK-Bench | Knowledge | Domain-Knowledge Bench | Tests a model against a user provided questions with reference answers. A models answers to each of the questions are scored on a 1-5 scale by a judge model | N/A |\n| MMLU | Knowledge | Massive Multitask Language Understanding | Tests a model against a standardized set of knowledge data and produces a score based on the model's performance | [Measuring Massive Multitask Language Understanding](https://arxiv.org/abs/2009.03300) |\n| MMLUBranch | Knowledge | N/A | Tests your knowledge contributions against a base model and produces a score based on the difference in performance | N/A |\n| MTBench | Skills | Multi-turn Benchmark | Tests a model's skill at applying its knowledge against a judge model and produces a score based on the model's performance | [MT-Bench (Multi-turn Benchmark)](https://klu.ai/glossary/mt-bench-eval) |\n| MTBenchBranch | Skills | N/A | Tests your skill contributions against a judge model and produces a score based on the difference in performance | N/A |\n\n> [!NOTE]\n> Evaluation must be used with local models (safetensors or GGUF format). Using models directly from Hugging Face without downloading them is unsupported.\n> GGUF models are not yet supported for mmlu and mmlu_branch evaluations\n> MTBench and MTBenchBranch use [prometheus-8x7b-v2.0](https://huggingface.co/prometheus-eval/prometheus-8x7b-v2.0) as the judge model by\ndefault. While you do not need to use this model as your judge, it is strongly recommended to do so if you have the necessary hardware\nresources. You can download it via `ilab model download`.\n\n#### Running DK-Bench\n\nFor information on how to run DK-Bench, see the [DK-Bench docs](docs/user/dk_bench.md).\n\n#### Running MMLU\n\n- Example of running MMLU with a local safetensors model directory:\n\n   ```bash\n   $ export ILAB_MODELS_DIR=$HOME/.local/share/instructlab/models\n   $ ilab model evaluate --benchmark mmlu --model $ILAB_MODELS_DIR/instructlab/granite-7b-test\n   ...\n   # KNOWLEDGE EVALUATION REPORT\n\n   ## MODEL (SCORE)\n   /home/user/.cache/instructlab/models/instructlab/granite-7b-test (0.52/1.0)\n\n   ### SCORES (0.0 to 1.0):\n   mmlu_abstract_algebra - 0.31\n   mmlu_anatomy - 0.46\n   mmlu_astronomy - 0.52\n   mmlu_business_ethics - 0.55\n   mmlu_clinical_knowledge - 0.57\n   mmlu_college_biology - 0.56\n   mmlu_college_chemistry - 0.38\n   mmlu_college_computer_science - 0.46\n   mmlu_college_mathematics - 0.34\n   mmlu_college_medicine - 0.49\n   mmlu_college_physics - 0.27\n   mmlu_computer_security - 0.66\n   mmlu_conceptual_physics - 0.38\n   mmlu_econometrics - 0.39\n   mmlu_electrical_engineering - 0.48\n   mmlu_elementary_mathematics - 0.3\n   ...\n   ```\n\n   The output of MMLU displays a much longer list of subjects.\n\n#### Running MMLUBranch\n\n- Example of running MMLUBranch with a local safetensors model directory:\n\n   ```bash\n   $ export ILAB_MODELS_DIR=$HOME/.local/share/instructlab/models\n   $ ilab model evaluate --benchmark mmlu_branch --model $ILAB_MODELS_DIR/instructlab/granite-7b-test --base-model $ILAB_MODELS_DIR/instructlab/granite-7b-lab\n   ...\n   # KNOWLEDGE EVALUATION REPORT\n\n   ## BASE MODEL (SCORE)\n   /home/user/.cache/instructlab/models/instructlab/granite-7b-lab (0.74/1.0)\n\n   ## MODEL (SCORE)\n   /home/user/.cache/instructlab/models/instructlab/granite-7b-test (0.78/1.0)\n\n   ### IMPROVEMENTS (0.0 to 1.0):\n   1. tonsils: 0.74 -> 0.78 (+0.04)\n   ```\n\n#### Running MTBench\n\n- Example of running MTBench with a local safetensors model directory:\n\n   ```bash\n   $ export ILAB_MODELS_DIR=$HOME/.local/share/instructlab/models\n   $ ilab model evaluate --benchmark mt_bench --model $ILAB_MODELS_DIR/instructlab/granite-7b-test\n   ...\n   # SKILL EVALUATION REPORT\n\n   ## MODEL (SCORE)\n   /home/user/.cache/instructlab/models/instructlab/granite-7b-test (7.27/10.0)\n\n   ### TURN ONE (0.0 to 10.0):\n   7.48\n\n   ### TURN TWO (0.0 to 10.0):\n   7.05\n   ```\n\n#### Running MTBenchBranch\n\n- Below is an example of running MTBenchBranch with a local safetensors model directory:\n\n   ```bash\n   $ export ILAB_MODELS_DIR=$HOME/.local/share/instructlab/models\n   $ export ILAB_TAXONOMY_DIR=$HOME/.local/share/instructlab/taxonomy\n   $ ilab model evaluate --benchmark mt_bench_branch \\\n      --model $ILAB_MODELS_DIR/instructlab/granite-7b-test \\\n      --base-model $ILAB_MODELS_DIR/instructlab/granite-7b-lab \\\n      --taxonomy-path $ILAB_TAXONOMY_DIR \\\n      --branch rc \\\n      --base-branch main\n   ...\n   # SKILL EVALUATION REPORT\n\n   ## BASE MODEL (SCORE)\n   /home/user/.cache/instructlab/models/instructlab/granite-7b-lab (5.78/10.0)\n\n   ## MODEL (SCORE)\n   /home/user/.cache/instructlab/models/instructlab/granite-7b-test (6.00/10.0)\n\n   ### IMPROVEMENTS (0.0 to 10.0):\n   1. foundational_skills/reasoning/linguistics_reasoning/object_identification/qna.yaml: 4.0 -> 6.67 (+2.67)\n   2. foundational_skills/reasoning/theory_of_mind/qna.yaml: 3.12 -> 4.0 (+0.88)\n   3. foundational_skills/reasoning/linguistics_reasoning/logical_sequence_of_words/qna.yaml: 9.33 -> 10.0 (+0.67)\n\n   ### REGRESSIONS (0.0 to 10.0):\n   1. foundational_skills/reasoning/unconventional_reasoning/lower_score_wins/qna.yaml: 5.67 -> 4.0 (-1.67)\n   2. foundational_skills/reasoning/mathematical_reasoning/qna.yaml: 7.33 -> 6.0 (-1.33)\n   3. foundational_skills/reasoning/temporal_reasoning/qna.yaml: 5.67 -> 4.67 (-1.0)\n\n   ### NO CHANGE (0.0 to 10.0):\n   1. foundational_skills/reasoning/linguistics_reasoning/odd_one_out/qna.yaml (9.33)\n   2. compositional_skills/grounded/linguistics/inclusion/qna.yaml (6.5)\n   ```\n\n### 🍴 Serve the newly trained model\n\nIf you have a server running, stop the server you have running by entering `ctrl+c` keys in the terminal running the server.\n\n*🍎 Mac only:* Before serving the newly trained model you must convert it to work with\nthe InstructLab Core package. The `ilab model convert` command converts the new model into quantized [GGUF](https://medium.com/@sandyeep70/ggml-to-gguf-a-leap-in-language-model-file-formats-cd5d3a6058f9) format which is required by the server to host the model in the `ilab model serve` command.\n\n- Convert the newly trained model by running the following command:\n\n    ```shell\n   ilab model convert\n   ```\n\n1. Serve the newly trained model locally via `ilab model serve` command with the `--model-path`\nargument to specify your new model:\n\n   ```shell\n   ilab model serve --model-path <new model path>\n   ```\n\n   Which model should you select to serve? After running the `ilab model convert` command, some files and a directory are generated. The model you will want to serve ends with an extension of `.gguf`\n   and exists in a directory with the suffix `trained`. For example:\n   `instructlab-granite-7b-lab-trained/instructlab-granite-7b-lab-Q4_K_M.gguf`.\n\n## 📣 Chat with the new model (not optional this time)\n\n- Try the fine-tuned model out live using the chat interface, and see if the results are better than the untrained version of the model with chat by running the following command:\n\n   ```shell\n   ilab model chat -m <New model path>\n   ```\n\n   If you are interested in optimizing the quality of the model's responses, please see [`TROUBLESHOOTING.md`](./TROUBLESHOOTING.md#model-fine-tuning-and-response-optimization)\n\n## ☁️ Upload the new model\n\nIf you are happy with your newly-trained model and wish to upload it to a supported endpoint, you can do so with the `ilab model upload` command.\n\nInstructLab currently supports uploading to [Hugging Face](https://huggingface.co/), OCI registries such as [Quay.io](https://quay.io/), and [AWS S3](https://aws.amazon.com/s3/) buckets.\n\nYou can upload safetensors, GGUFs, and/or OCI-compliant models to any supported destination you have access to - see `ilab model upload --help` for more info.\nSome examples are listed below:\n\n```shell\n# upload model to Hugging Face\nilab model upload --dest-type hf --model granite-7b-lab --destination instructlab/granite-7b-lab --release main\n\n# upload model to OCI registry\nilab model upload --dest-type oci --model granite-7b-lab --destination docker://instructlab/granite-7b-lab --release latest\n\n# upload model to S3 bucket\nilab model upload --dest-type s3 --model granite-7b-lab --destination my-aws-bucket\n\n```\n\n> [!NOTE]\n> You do not need to use the `--release` flag if you are uploading to AWS S3, and you should not prepend your bucket name with `s3://`\n\n## 🎁 Submit your new knowledge or skills\n\nOf course, the final step is, if you've improved the model, to open a pull-request in the [taxonomy repository](https://github.com/instructlab/taxonomy) that includes the files (e.g. `qna.yaml`) with your improved data.\n\n## 📬 Contributing\n\nCheck out our [contributing](CONTRIBUTING/CONTRIBUTING.md) guide to learn how to contribute.\n"
    },
    {
      "name": "Chainlit/cookbook",
      "stars": 1061,
      "img": "https://avatars.githubusercontent.com/u/128686189?s=40&v=4",
      "owner": "Chainlit",
      "repo_name": "cookbook",
      "description": "Chainlit's cookbook repo",
      "homepage": "https://github.com/Chainlit/chainlit",
      "language": "Python",
      "created_at": "2023-05-10T08:52:57Z",
      "updated_at": "2025-04-23T09:21:19Z",
      "topics": [],
      "readme": "# Chainlit Cookbook\n\nWelcome to the Chainlit Demos repository! Here you'll find a collection of example projects demonstrating how to use Chainlit to create amazing chatbot UIs with ease. Each folder in this repository represents a separate demo project.\n\n## 🚀 Getting Started\n\nTo run a demo, follow these steps:\n\n1. Clone this repository:\n   ```\n   git clone https://github.com/Chainlit/cookbook.git chainlit-cookbook\n   ```\n2. Navigate to the desired demo folder:\n   ```\n   cd chainlit-cookbook/demo-folder-name\n   ```\n3. Install the required dependencies:\n   ```\n   pip install -r requirements.txt\n   ```\n4. Create a `.env` file based on the provided `.env.example` file:\n   ```\n   cp .env.example .env\n   ```\n   Modify the `.env` file as needed to include any necessary API keys or configuration settings.\n5. Run the Chainlit app in watch mode:\n   ```\n   chainlit run app.py -w\n   ```\n\nYour demo chatbot UI should now be up and running in your browser!\n\n## 💁 Contributing\n\nWe'd love to see more demos showcasing the power of Chainlit. If you have an idea for a demo or want to contribute one, please feel free to open an issue or create a pull request. Your contributions are highly appreciated!\n"
    },
    {
      "name": "aws-samples/amazon-bedrock-samples",
      "stars": 987,
      "img": "https://avatars.githubusercontent.com/u/8931462?s=40&v=4",
      "owner": "aws-samples",
      "repo_name": "amazon-bedrock-samples",
      "description": "This repository contains examples for customers to get started using the Amazon Bedrock Service. This contains examples for all available foundational models",
      "homepage": "https://aws.amazon.com/bedrock/",
      "language": "Jupyter Notebook",
      "created_at": "2023-07-05T18:23:34Z",
      "updated_at": "2025-04-23T04:41:35Z",
      "topics": [
        "amazon-bedrock",
        "amazon-titan",
        "bedrock",
        "embeddings",
        "generative-ai",
        "knowledge-base",
        "langchain",
        "rag"
      ],
      "readme": "# Amazon Bedrock Samples \n\nTo leverage this repository please use our website powered by this GitHub: [Website](https://aws-samples.github.io/amazon-bedrock-samples/)\n\nThis repository contains pre-built examples to help customers get started with the Amazon Bedrock service.\n\n## Contents\n\n- [Introduction to Bedrock](introduction-to-bedrock) - Learn the basics of the Bedrock service\n- [Prompt Engineering ](articles-guides) - Tips for crafting effective prompts \n- [Agents](agents-and-function-calling) - Ways to implement Generative AI Agents and its components.\n- [Custom Model Import](custom-models) - Import custom models into Bedrock\n- [Multimodal](multi-modal) - Working with multimodal data using Amazon Bedrock\n- [Generative AI Use cases](genai-use-cases) - Example use cases for generative AI\n- [Retrival Augmented Generation (RAG)](rag) - Implementing RAG\n- [Responsible AI](responsible_ai) - Use Bedrock responsibly and ethically\n- [Workshop](workshops) - Example for Amazon Bedrock Workshop\n- [POC to Prod](poc-to-prod) - Productionize workloads using Bedrock\n- [Embeddings](embeddings) - Learn how to use Embedding Models available on Amazon Bedrock \n- [Observability & Evaluation](evaluation-observe) - Learn how Amazon Bedrock helps with improving observability and evalution of Models, Gen AI Applications.\n\n\n## Getting Started\n\nTo get started with the code examples, ensure you have access to [Amazon Bedrock](https://aws.amazon.com/bedrock/). Then clone this repo and navigate to one of the folders above. Detailed instructions are provided in each folder's README.\n\n### Enable AWS IAM permissions for Bedrock\n\nThe AWS identity you assume from your environment (which is the [*Studio/notebook Execution Role*](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html) from SageMaker, or could be a role or IAM User for self-managed notebooks or other use-cases), must have sufficient [AWS IAM permissions](https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies.html) to call the Amazon Bedrock service.\n\nTo grant Bedrock access to your identity, you can:\n\n- Open the [AWS IAM Console](https://us-east-1.console.aws.amazon.com/iam/home?#)\n- Find your [Role](https://us-east-1.console.aws.amazon.com/iamv2/home?#/roles) (if using SageMaker or otherwise assuming an IAM Role), or else [User](https://us-east-1.console.aws.amazon.com/iamv2/home?#/users)\n- Select *Add Permissions > Create Inline Policy* to attach new inline permissions, open the *JSON* editor and paste in the below example policy:\n\n```\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"BedrockFullAccess\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\"bedrock:*\"],\n            \"Resource\": \"*\"\n        }\n    ]\n}\n```\n\n> ⚠️ **Note 1:** With Amazon SageMaker, your notebook execution role will typically be *separate* from the user or role that you log in to the AWS Console with. If you'd like to explore the AWS Console for Amazon Bedrock, you'll need to grant permissions to your Console user/role too.\n\n> ⚠️ **Note 2:** For top level folder changes, please reach out to the GitHub mainterners.\n\nFor more information on the fine-grained action and resource permissions in Bedrock, check out the Bedrock Developer Guide.\n\n## Contributing\n\nWe welcome community contributions! Please see [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines.\n\n## Security\n\nSee [CONTRIBUTING](CONTRIBUTING.md#security-issue-notifications) for more information.\n\n## License\n\nThis library is licensed under the MIT-0 License. See the [LICENSE](LICENSE) file.\n"
    },
    {
      "name": "jamescalam/transformers",
      "stars": 585,
      "img": "https://avatars.githubusercontent.com/u/35938317?s=40&v=4",
      "owner": "jamescalam",
      "repo_name": "transformers",
      "description": null,
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2021-02-22T19:02:46Z",
      "updated_at": "2025-03-31T03:25:09Z",
      "topics": [],
      "readme": "# Learn Transformers\n\nRepo for the *Natural Language Processing: NLP With Transformers in Python* course, you can get 70% off the price with [this link!](https://www.udemy.com/course/nlp-with-transformers/?couponCode=70SEP2021) If the link stops working raise an issue or drop me a message somewhere:\n\n[YouTube](https://www.youtube.com/c/jamesbriggs) \n| [Discord](https://discord.gg/c5QtDB9RAP)\n\nI also have plenty of free material on YouTube 😊\n"
    },
    {
      "name": "TilmanGriesel/chipper",
      "stars": 442,
      "img": "https://avatars.githubusercontent.com/u/1132745?s=40&v=4",
      "owner": "TilmanGriesel",
      "repo_name": "chipper",
      "description": "✨ AI interface for tinkerers (Ollama, Haystack RAG, Python)",
      "homepage": "https://chipper.tilmangriesel.com/",
      "language": "Python",
      "created_at": "2024-12-22T17:54:13Z",
      "updated_at": "2025-04-20T03:22:00Z",
      "topics": [
        "agent",
        "agentic-ai",
        "deepseek",
        "deepseek-chat",
        "deepseek-r1",
        "embedding",
        "hugging-face",
        "huggingface",
        "llama3",
        "llm",
        "llm-inference",
        "ollama",
        "ollama-api",
        "ollama-client",
        "ollama-gui",
        "phi4",
        "rag",
        "retrival-augmented-generation"
      ],
      "readme": "<p align=\"center\"><img src=\"https://raw.githubusercontent.com/TilmanGriesel/chipper/refs/heads/main/docs/public/assets/banner.png\" width=\"640\" alt=\"Logo Chipper RAG Util\"/></p>\n<p align=\"center\">\n\t<a href=\"https://chipper.tilmangriesel.com/\"><img src=\"https://img.shields.io/github/actions/workflow/status/TilmanGriesel/chipper/.github%2Fworkflows%2Fdocs-deploy.yml?colorA=1F2229&colorB=ffffff&style=for-the-badge&label=GitHub Pages\"></a>\n    <a href=\"https://github.com/TilmanGriesel/chipper/actions\"><img src=\"https://img.shields.io/github/actions/workflow/status/TilmanGriesel/chipper/.github%2Fworkflows%2Fpublish-docker.yml?colorA=1F2229&colorB=ffffff&style=for-the-badge&label=DockerHub\"></a>\n    <a href=\"https://github.com/tilmangriesel/chipper/stargazers\"><img src=\"https://img.shields.io/github/stars/tilmangriesel/chipper?colorA=1F2229&colorB=ffffff&style=for-the-badge\"></a>\n    <a href=\"https://github.com/tilmangriesel/chipper/issues\"><img src=\"https://img.shields.io/github/issues/tilmangriesel/chipper?colorA=1F2229&colorB=ffffff&style=for-the-badge\"></a><a href=\"https://hub.docker.com/repository/docker/griesel/chipper\"><img src=\"https://img.shields.io/docker/pulls/griesel/chipper?colorA=1F2229&colorB=ffffff&style=for-the-badge\"></a>\n</p>\n\n**Chipper** provides a web interface, CLI, and a modular, hackable, and lightweight architecture for RAG pipelines, document splitting, web scraping, and query workflows, enhancing generative AI models with advanced information retrieval capabilities. It can also function as a proxy between an **Ollama** client, such as **Enchanted** or **Open WebUI**, and an **Ollama** instance. Built with **Haystack**, **Ollama**, **Hugging Face**, **Docker**, **TailwindCSS**, and **ElasticSearch**, it runs as a fully containerized service.\n\nThis project started as a personal tool to help my girlfriend with her book, using local RAG and LLMs to explore characters and creative ideas while keeping her work private and off cloud services like ChatGPT. What began as a few handy scripts soon grew into a fully dockerized, extensible service and along the way, it became a labor of love. Now, I'm excited to share it with the world.\n\nIf you **find Chipper useful**, **leaving a star ⭐ would be lovely** and will help others **discover Chipper too**.\n\n- **Project Site:** [https://chipper.tilmangriesel.com/](https://chipper.tilmangriesel.com/)\n- **Live Demo:** [https://demo.chipper.tilmangriesel.com/](https://demo.chipper.tilmangriesel.com/)\n\n## Overview\n\n- [Installation and Setup](#installation-and-setup)\n- [Features](#features)\n- [Philosophy](#philosophy)\n- [Demos](#demos)\n  - [Web Interface](#web-interface)\n  - [Code Output](#code-output)\n  - [Reasoning](#reasoning)\n  - [CLI Interface](#cli-interface)\n  - [Third-Party Clients](#third-party-clients)\n- [Project Roadmap](#project-roadmap)\n  - [Completed Milestones](#completed-milestones)\n  - [Upcoming Features](#upcoming-features)\n- [Acknowledgments](#acknowledgments)\n- [Friends of Chipper](#friends-of-chipper)\n- [Star History](#star-history)\n\n## Installation and Setup\n\n- [Quickstart](https://chipper.tilmangriesel.com/get-started.html#welcome-to-chipper)\n- Or visit the [Chipper project website](https://chipper.tilmangriesel.com/)\n\n## Features\n\n- **Local & Cloud Model Support** - Run models locally with [Ollama](https://ollama.com/) or connect to remote models via the [Hugging Face API](https://huggingface.co/).\n- **ElasticSearch Integration** - Store and retrieve vectorized data efficiently with scalable indexing.\n- **Document Chunking** - Process and split documents into structured segments.\n- **Web Scraping** - Extract and index content from web pages.\n- **Audio Transcription** - Convert audio files to text.\n- **CLI & Web UI** - Access Chipper via a command-line tool or a lightweight, self-contained web interface.\n- **Dockerized Deployment** - Run in a fully containerized setup with minimal configuration.\n- **Customizable RAG Pipelines** - Adjust model selection, query parameters, and system prompts as needed.\n- **Ollama API Proxy** - Extend Ollama with retrieval capabilities, enabling interoperability with clients like **Enchanted** and **Open WebUI**.\n- **API Security** - Proxy the Ollama API with API key-based and Baerer token service authentication.\n- **Offline Web UI** - Works without an internet connection using vanilla JavaScript and TailwindCSS.\n- **Edge TTS** – Listen to Chipper's output using a WebAssembly-based client-side TTS generator.\n- **Distributed Processing** - Chain multiple Chipper instances together for workload distribution and extended processing.\n\n**Note:** This is a personal project and not designed for commercial or production use. If you intend to use it in a production environment, make sure to conduct your own due diligence.\n\n## Philosophy\n\nAt the heart of this project lies my passion for education and exploration. I believe in creating tools that are both approachable for beginners and helpful for experts. My goal is to offer you a well-thought-out service architecture, and a stepping stone for those eager to learn and innovate.\n\nThis project wants to be more than just a technical foundation, for educators, it provides a framework to teach AI concepts in a manageable and practical way. For explorers, tinkerers and companies, it offers a playground where you can experiment, iterate, and build upon a versatile platform.\n\nFeel free to improve, fork, copy, share or expand this project. Contributions are always very welcome!\n\n## Demos\n\n### Web Interface\n\nUse Chipper's built-in web interface to set up and customize RAG pipelines with ease. Built with vanilla JavaScript and TailwindCSS, it works offline and doesn't require any framework-specific knowledge. Run the `/help` command to learn how to switch models, update the embeddings index, and more.\n\n<p align=\"center\"><img src=\"https://raw.githubusercontent.com/TilmanGriesel/chipper/refs/heads/main/docs/public/assets/demos/demo_rag_chat.gif\" alt=\"chipper_demo_chat\"/></p>\n\n### Code Output\n\nAutomatic syntax highlighting for popular programming languages in the web interface.\n\n<p align=\"center\"><img src=\"https://raw.githubusercontent.com/TilmanGriesel/chipper/refs/heads/main/docs/public/assets/demos/demo_rag_chat_code.gif\" alt=\"chipper_demo_code_gen\"/></p>\n\n### Reasoning\n\nFor models like DeepSeek-R1, Chipper suppresses the \"think\" output in the UI while preserving the reasoning steps in the console output.\n\n<p align=\"center\"><img src=\"https://raw.githubusercontent.com/TilmanGriesel/chipper/refs/heads/main/docs/public/assets/demos/demo_rag_chat_ds.gif\" alt=\"chipper_demo_deepseek\"/></p>\n\n### CLI Interface\n\nFull support for the Ollama CLI and API, including reflection and proxy capabilities, with API key route decorations.\n\n<p align=\"center\"><img src=\"https://raw.githubusercontent.com/TilmanGriesel/chipper/refs/heads/main/docs/public/assets/demos/demo_rag_chat_cli.gif\" alt=\"chipper_demo_ollama_cli\"/></p>\n\n### Third-Party Clients\n\nEnhance every third-party Ollama client with server-side knowledge base embeddings, allowing server side model selection, query parameters, and system prompt overrides. Enable RAG for any Ollama client or use Chipper as a centralized knowledge base.\n\n<p align=\"center\"><img src=\"https://raw.githubusercontent.com/TilmanGriesel/chipper/refs/heads/main/docs/public/assets/demos/demo_rag_chat_ollamac.gif\" alt=\"chipper_demo_ollamac\"/></p>\n\n### **Project Roadmap**\n\n#### **Completed Milestones**\n\n##### **Core Features**\n\n- [x] **Basic Functionality**\n- [x] **Command-Line Interface (CLI)**\n- [x] **Web-Based User Interface (UI)**\n- [x] **Docker Containerization**\n\n##### **Enhancements & Optimizations**\n\n- [x] **Improved Web UI** (Better mobile support)\n- [x] **Enhanced Linting and Code Formatting**\n- [x] **Docker Hub Registry Image Publishing**\n- [x] **Edge Inference for Text-to-Speech (TTS)**\n- [x] **Bearer Token Authentication Support**\n\n##### **Integrations & Extensibility**\n\n- [x] **Mirror Ollama Chat API** (Enable Chipper as a drop-in middleware)\n- [x] **Haystack Chat Generators** (`ChatPromptBuilder` & `OllamaChatGenerator`)\n- [x] **Distributed Processing** (Chain multiple Chipper instances for workload distribution)\n\n#### **Upcoming Features**\n\n##### **Core Features**\n\n- [ ] **Expanded Support for Client-Side Model Settings**\n\n##### **Tutorial Section**\n\n- [ ] **Initial Set of 3 Tutorials for New Users**\n\n##### **Testing & Reliability**\n\n- [ ] **Automated Unit Testing Framework**\n\n##### **Intelligent Processing**\n\n- [ ] **Smart Document Splitting and Embedding**\n\n##### **User Experience & Interface**\n\n- [ ] **React-Based Web Application** (Modernized UI/UX)\n\n---\n\n## Acknowledgments\n\n- [Haystack](https://github.com/deepset-ai/haystack)\n- [Ollama](https://github.com/ollama/ollama)\n- [Hugging Face](https://huggingface.co/)\n- [Elastic](https://www.elastic.co)\n- [Elasticvue](https://github.com/cars10/elasticvue)\n- [Docker](https://docker.com)\n- [VitePress](https://github.com/vuejs/vitepress)\n- [Sherpa ONNX](https://github.com/k2-fsa/sherpa-onnx)\n- [Sherpa ONNX Wasm](https://huggingface.co/spaces/k2-fsa/web-assembly-tts-sherpa-onnx-en/tree/main)\n\n## Friends of Chipper\n\nCheck out these Chipper-compatible projects! Want to add yours? Open an issue to let me know!\n\n- [page-assist](https://github.com/n4ze3m/page-assist) Use your locally running AI models to assist you in your web browsing\n- [open-webui](https://github.com/open-webui/open-webui) User-friendly AI Interface\n- [enchanted](https://github.com/gluonfield/enchanted) Enchanted is iOS and macOS app for chatting with private self hosted language models.\n- [Ollamac](https://github.com/kevinhermawan/Ollamac) Mac app for Ollama\n\n---\n\nBe sure to visit the [Chipper project website](https://chipper.tilmangriesel.com/) for detailed setup instructions and more information.\n"
    },
    {
      "name": "Vasanthengineer4949/NLP-Projects-NHV",
      "stars": 432,
      "img": "https://avatars.githubusercontent.com/u/64586431?s=40&v=4",
      "owner": "Vasanthengineer4949",
      "repo_name": "NLP-Projects-NHV",
      "description": "NLP Projects playlist",
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2023-07-07T14:21:20Z",
      "updated_at": "2025-04-10T13:58:13Z",
      "topics": [],
      "readme": "# NLP ADVANCED - NHV\n\n## NLP A-Z COURSE\n\n### Roadmap \n[Video Link](https://youtu.be/-YZkNmG9yRQ?si=mpP9yI6dID_ftAPK)\n\n## COURSE\n\n| Topic                               | Video Link |\n|-------------------------------------|------------|\n| NLP A-Z Course Part - 1              | [Video Link](https://youtu.be/xgZzmXx80rk) |\n| NLP A-Z Course Part - 2              | [Video Link](https://youtu.be/EG7pueG0ArM) |\n\n## VIDEOS IN ORDER\n\n| Serial Number | Topic                              | Code Link          | Video Link |\n|---------------|------------------------------------|--------------------|------------|\n| 1             | Transformers From Scratch         | [Code Link](https://github.com/Vasanthengineer4949/Transformer-Scratch-Translation)   | [Video Link](https://youtu.be/FEGdLzx4UAk?si=ERsRQ4xeLz_oOBsC) |\n| 2             | BERT For Text Classification      | [Code Link](https://github.com/Vasanthengineer4949/NLP-Projects-NHV/tree/main/NLP_Projects/2.%20Text%20Classifier%20using%20BERT)   | [Video Link](https://youtu.be/UEq5C-orlZU?si=ZiA0hOKs-Z_aP7F9) |\n| 3             | BERT NER                          | [Code Link](https://github.com/Vasanthengineer4949/NLP-Projects-NHV/tree/main/NLP_Projects/3.%20Token%20Classification%20using%20BERT)   | [Video Link](https://youtu.be/upuNaHQq-g8?si=ttwBcvbbs0RQiJmG) |\n| 4             | T5 - All NLP Tasks                | [Code Link](https://github.com/Vasanthengineer4949/NLP-Projects-NHV/tree/main/NLP_Projects/4.%20FlanT5-AIO)   | [Video Link](https://www.youtube.com/live/Cxi0fbl1_C8?si=51D7Wi1THk6M-dZW) |\n| 5             | Llama2 Finetuning                 | [Code Link](https://github.com/Vasanthengineer4949/NLP-Projects-NHV/tree/main/LLMs%20Related/Finetune%20Llama2%20using%20QLoRA)   | [Video Link](https://youtu.be/DiV2HmUNhRw?si=znv8y45XK6xaWPzj) |\n| 6             | LoRA Paper Explanation and Implementation | [Code Link](https://github.com/Vasanthengineer4949/NLP-Projects-NHV/tree/main/Research%20Papers/1.%20LoRA) | [Video Link](https://youtu.be/r80HWjFUAGU?si=B_ALQxMCPRIy0ZLg) |\n| 7             | DPO Paper Explanation and Implementation | [Code Link](https://github.com/Vasanthengineer4949/NLP-Projects-NHV/tree/main/Research%20Papers/2.%20DPO) | [Video Link](https://youtu.be/q_BvQyusEjU?si=EG-uanLvX2fRUSIh) |\n| 8             | Mistral Architecture Explanation  | [Code Link](https://github.com/Vasanthengineer4949/NLP-Projects-NHV/tree/main/Research%20Papers/4.%20Mistral%20Architecture) | [Video Link](https://youtu.be/teEf4OzQ1IY?si=6hGNJHTy4Km9yLhC) |\n| 9             | Mistral Finetuning                | [Code Link](https://github.com/Vasanthengineer4949/NLP-Projects-NHV/tree/main/LLMs%20Related/Finetune%20Mistral)   | [Video Link](https://youtu.be/qzWZ4kO4dXI?si=I73hIYXa1tcMKeot) |\n| 10            | Mistral DPO Finetuning            | [Code Link](https://github.com/Vasanthengineer4949/NLP-Projects-NHV/tree/main/LLMs%20Related/DPO%20on%20Mistral)   | [Video Link](https://youtu.be/crXjRYbvT1U?si=c5t1PfFRjxM2rHYo) |\n| 11            | LLM Evaluation using Mistral      | [Code Link](https://github.com/Vasanthengineer4949/NLP-Projects-NHV/tree/main/Langchain%20Projects/8_AI_Evaluator)   | [Video Link](https://youtu.be/RH2TVsW2dtI?si=G61U7nm5ZfEws6Gi) |\n| 12            | Mistral RAG                       | [Code Link](https://github.com/Vasanthengineer4949/NLP-Projects-NHV/tree/main/Langchain%20Projects/7_AI_Financial_Advisor)   | [Video Link](https://youtu.be/Ywg_WHn17pI?si=WVYH7YTZzFZkR2yC) |\n| 13            | LLM Finetuning Crash Course            | [Code Link](https://github.com/Vasanthengineer4949/NLP-Projects-NHV/tree/main/LLMs%20Related/LLM%20Crash%20Course)   | [Video Link](https://youtu.be/whbuNo6APVs?si=s2eM1PCD3xgGr-aU) |\n| 14            | LLM For Information Extraction     | [Code Link](https://github.com/Vasanthengineer4949/NLP-Projects-NHV/tree/main/LLMs%20Related/Finetune%20Any%20LLM%20For%20Information%20Extraction)   | [Video Link](https://youtu.be/CBtnQ94b5GM?si=ruYToZ7e3FyRnl0P) |\n| 15            | Gemma Architecture Explained with Finetuning | [Code Link](https://github.com/Vasanthengineer4949/NLP-Projects-NHV/tree/main/LLMs%20Related/Gemma) | [Video Link](https://youtu.be/cK2ffR7fcZA?si=6Eb2KFBu2aOFPWuw) |\n| 16            | My Best LLM using Model Merging    | [Code Link](https://github.com/Vasanthengineer4949/NLP-Projects-NHV/tree/main/LLMs%20Related/My%20Best%20LLM)   | [Video Link](https://youtu.be/qoQowDbq8_Y?si=C6C88AOoUSQXq4k0) |\n| 17            | Mixture of Experts from Scratch    | [Code Link](https://github.com/Vasanthengineer4949/NLP-Projects-NHV/tree/main/Research%20Papers/6.%20Mixture%20of%20Experts%20From%20Scratch)   | [Video Link](https://youtu.be/dxh0LjhRHO0?si=-N4qT75U9NIlfgsy) |\n| 18            | LoRA Merging                       | [Code Link](https://github.com/Vasanthengineer4949/NLP-Projects-NHV/tree/main/LLMs%20Related/AI%20Psychiatrist)   | [Video Link](https://youtu.be/19D1KyTCsQI?si=RQ-izQiBIOcg_z8M) |\n| 19            | Deploy and Serve LLM using Ollama WebUI | [No Code](#) | [Video Link](https://youtu.be/fnqdK7cduAM?si=H-sQxx3vjgfWgPdz) |\n| 20            | Whatsapp Chatbot using Twilio and Open Source LLMs | [Code Link](https://github.com/Vasanthengineer4949/NLP-Projects-NHV/tree/main/LLMs%20Related/WhatsApp%20Chatbot%20under%2015%20mins) | [Video Link](https://youtu.be/VHrQnPEgUQA) |\n| 21            | Edubot - Llama RAG Application    | [Code Link](https://github.com/Vasanthengineer4949/NLP-Projects-NHV/tree/main/Langchain%20Projects/1_Edubot)   | [Video Link](https://youtu.be/qhfPlmFHvBI?si=L3YMF-ELvbLbRvli) |\n| 22            | AI Girlfriend - Benefits of Prompting | [Code Link](https://github.com/Vasanthengineer4949/NLP-Projects-NHV/tree/main/Langchain%20Projects/6_AI_Girlfriend) | [Video Link](https://youtu.be/3dY0MMJ7YZU?si=3sXsR_Q_-cU9Jbct) |\n| 23            | YTBuddy - Chat with Videos        | [Code Link](https://github.com/Vasanthengineer4949/NLP-Projects-NHV/tree/main/Langchain%20Projects/3_YT_Buddy)   | [Video Link](https://youtu.be/mdooOo1AaVY?si=0BGLAoVTI-tGUJIA) |\n| 24            | Cricbot - Chat with CSV           | [Code Link](https://github.com/Vasanthengineer4949/NLP-Projects-NHV/tree/main/Langchain%20Projects/4_Cricbot)   | [Video Link](https://youtu.be/eFtugUk8Fu0?si=xVEpDf6PDo8ovOL-) |\n| 25            | Codepal - Chat with Git Repo      | [Code Link](https://github.com/Vasanthengineer4949/NLP-Projects-NHV/tree/main/Langchain%20Projects/9_AI_Code_Chat)   | [Video Link](https://youtu.be/gwTt2sxmSCE?si=AThaEyekPW88iBQI) |\n| 26            | Building your own Copilot in VSCode Realtime | [Code Link](https://colab.research.google.com/drive/11_pMR_qSKwxAP7kWy25eiy-gknoMUVqZ?usp=sharing) | [Video Link](https://youtu.be/naHqd5-R55A) |\n| 27            | Realtime Research Agent with Deployment | [Code Link](https://github.com/Vasanthengineer4949/NLP-Projects-NHV/tree/main/AI%20Projects%20using%20LLMs/1.%20Real%20Time%20Research%20Agent) | [Video Link](https://youtu.be/tjAxbMUz5c8?si=wwgkb-Meen4fMmLM) |\n| 28            | AI Database Administrator - Chat with Database | [Code Link](https://github.com/Vasanthengineer4949/NLP-Projects-NHV/tree/main/AI%20Projects%20using%20LLMs/2.%20Database%20Administrator) | [Video Link](https://youtu.be/c8K4QHzaOrU?si=bjzhsWscQr8Xy01U) |\n| 29            | Building my own AI Startup        | [Code Link](https://github.com/Vasanthengineer4949/NLP-Projects-NHV/tree/main/Building%20AI%20Startups%20with%20crew.AI/1.%20AI%20Stock%20Analysis%20Company)   | [Video Link](https://youtu.be/C3uZl6ZdEVg?si=WAduVZN6YCkCMWK-) |\n| 30            | 1 Bit LLM Pretraining - Era of 1 Bit LLM | [Code Link](https://github.com/Vasanthengineer4949/NLP-Projects-NHV/tree/main/LLMs%20Related/Era%20of%201%20Bit%20LLMs) | [Video Link](https://youtu.be/w-Htc7sK5AU?si=KcYY6WunANrbrOHl) |\n"
    },
    {
      "name": "Arize-ai/openinference",
      "stars": 385,
      "img": "https://avatars.githubusercontent.com/u/59858760?s=40&v=4",
      "owner": "Arize-ai",
      "repo_name": "openinference",
      "description": "OpenTelemetry Instrumentation for AI Observability",
      "homepage": "https://arize-ai.github.io/openinference/",
      "language": "Python",
      "created_at": "2023-12-26T17:33:58Z",
      "updated_at": "2025-04-23T06:30:37Z",
      "topics": [
        "aiops",
        "hacktoberfest",
        "haystack",
        "langchain",
        "langraph",
        "llamaindex",
        "llmops",
        "llms",
        "mcp",
        "openai",
        "openai-agents",
        "opentelemetry",
        "smolagents",
        "telemetry",
        "tracing",
        "vercel",
        "vertex"
      ],
      "readme": "<p align=\"center\">\n    <img alt=\"OpenInfence\" src=\"https://raw.githubusercontent.com/Arize-ai/phoenix-assets/main/logos/OpenInference/Full%20color/OI-full-horiz.svg\" width=\"40%\" height=\"100%\" />\n</p>\n<p align=\"center\">\n    <a href=\"https://arize-ai.github.io/openinference/\">\n        <img src=\"https://img.shields.io/static/v1?message=Spec&logo=data:image/png;base64,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&labelColor=grey&color=blue&logoColor=white&label=%20\"/>\n    </a>\n    <a target=\"_blank\" href=\"https://join.slack.com/t/arize-ai/shared_invite/zt-1px8dcmlf-fmThhDFD_V_48oU7ALan4Q\">\n        <img src=\"https://img.shields.io/static/v1?message=Community&logo=slack&labelColor=grey&color=blue&logoColor=white&label=%20\"/>\n    </a>\n</p>\n\nOpenInference is a set of conventions and plugins that is complimentary to [OpenTelemetry](https://opentelemetry.io/) to enable tracing of AI applications. OpenInference is natively supported by [arize-phoenix](https://github.com/Arize-ai/phoenix), but can be used with any OpenTelemetry-compatible backend as well.\n\n## Specification\n\nThe OpenInference specification is edited in markdown files found in the [spec directory](./spec/). It's designed to provide insight into the invocation of LLMs and the surrounding application context such as retrieval from vector stores and the usage of external tools such as search engines or APIs. The specification is transport and file-format agnostic, and is intended to be used in conjunction with other specifications such as JSON, ProtoBuf, and DataFrames.\n\n## Instrumentation\n\nOpenInference provides a set of instrumentations for popular machine learning SDKs and frameworks in a variety of languages.\n\n## Python\n\n### Libraries\n\n| Package                                                                                                               | Description                                          | Version                                                                                                                                                                    |\n| --------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| [`openinference-semantic-conventions`](./python/openinference-semantic-conventions)                                   | Semantic conventions for tracing of LLM Apps.        | [![PyPI Version](https://img.shields.io/pypi/v/openinference-semantic-conventions.svg)](https://pypi.python.org/pypi/openinference-semantic-conventions)                   |\n| [`openinference-instrumentation-openai`](./python/instrumentation/openinference-instrumentation-openai)               | OpenInference Instrumentation for OpenAI SDK.        | [![PyPI Version](https://img.shields.io/pypi/v/openinference-instrumentation-openai.svg)](https://pypi.python.org/pypi/openinference-instrumentation-openai)               |\n| [`openinference-instrumentation-openai-agents`](./python/instrumentation/openinference-instrumentation-openai-agents) | OpenInference Instrumentation for OpenAI Agents SDK. | [![PyPI Version](https://img.shields.io/pypi/v/openinference-instrumentation-openai-agents.svg)](https://pypi.python.org/pypi/openinference-instrumentation-openai-agents) |\n| [`openinference-instrumentation-llama-index`](./python/instrumentation/openinference-instrumentation-llama-index)     | OpenInference Instrumentation for LlamaIndex.        | [![PyPI Version](https://img.shields.io/pypi/v/openinference-instrumentation-llama-index.svg)](https://pypi.python.org/pypi/openinference-instrumentation-llama-index)     |\n| [`openinference-instrumentation-dspy`](./python/instrumentation/openinference-instrumentation-dspy)                   | OpenInference Instrumentation for DSPy.              | [![PyPI Version](https://img.shields.io/pypi/v/openinference-instrumentation-dspy.svg)](https://pypi.python.org/pypi/openinference-instrumentation-dspy)                   |\n| [`openinference-instrumentation-bedrock`](./python/instrumentation/openinference-instrumentation-bedrock)             | OpenInference Instrumentation for AWS Bedrock.       | [![PyPI Version](https://img.shields.io/pypi/v/openinference-instrumentation-bedrock.svg)](https://pypi.python.org/pypi/openinference-instrumentation-bedrock)             |\n| [`openinference-instrumentation-langchain`](./python/instrumentation/openinference-instrumentation-langchain)         | OpenInference Instrumentation for LangChain.         | [![PyPI Version](https://img.shields.io/pypi/v/openinference-instrumentation-langchain.svg)](https://pypi.python.org/pypi/openinference-instrumentation-langchain)         |\n| [`openinference-instrumentation-mcp`](./python/instrumentation/openinference-instrumentation-mcp)                     | OpenInference Instrumentation for MCP.               | [![PyPI Version](https://img.shields.io/pypi/v/openinference-instrumentation-mcp.svg)](https://pypi.python.org/pypi/openinference-instrumentation-mcp)                     |\n| [`openinference-instrumentation-mistralai`](./python/instrumentation/openinference-instrumentation-mistralai)         | OpenInference Instrumentation for MistralAI.         | [![PyPI Version](https://img.shields.io/pypi/v/openinference-instrumentation-mistralai.svg)](https://pypi.python.org/pypi/openinference-instrumentation-mistralai)         |\n| [`openinference-instrumentation-portkey`](./python/instrumentation/openinference-instrumentation-portkey)             | OpenInference Instrumentation for Portkey.           | [![PyPI Version](https://img.shields.io/pypi/v/openinference-instrumentation-portkey.svg)](https://pypi.python.org/pypi/openinference-instrumentation-portkey)             |\n| [`openinference-instrumentation-guardrails`](./python/instrumentation/openinference-instrumentation-guardrails)       | OpenInference Instrumentation for Guardrails.        | [![PyPI Version](https://img.shields.io/pypi/v/openinference-instrumentation-guardrails.svg)](https://pypi.python.org/pypi/openinference-instrumentation-guardrails)       |\n| [`openinference-instrumentation-vertexai`](./python/instrumentation/openinference-instrumentation-vertexai)           | OpenInference Instrumentation for VertexAI.          | [![PyPI Version](https://img.shields.io/pypi/v/openinference-instrumentation-vertexai.svg)](https://pypi.python.org/pypi/openinference-instrumentation-vertexai)           |\n| [`openinference-instrumentation-crewai`](./python/instrumentation/openinference-instrumentation-crewai)               | OpenInference Instrumentation for CrewAI.            | [![PyPI Version](https://img.shields.io/pypi/v/openinference-instrumentation-crewai.svg)](https://pypi.python.org/pypi/openinference-instrumentation-crewai)               |\n| [`openinference-instrumentation-haystack`](./python/instrumentation/openinference-instrumentation-haystack)           | OpenInference Instrumentation for Haystack.          | [![PyPI Version](https://img.shields.io/pypi/v/openinference-instrumentation-haystack.svg)](https://pypi.python.org/pypi/openinference-instrumentation-haystack)           |\n| [`openinference-instrumentation-litellm`](./python/instrumentation/openinference-instrumentation-litellm)             | OpenInference Instrumentation for liteLLM.           | [![PyPI Version](https://img.shields.io/pypi/v/openinference-instrumentation-litellm.svg)](https://pypi.python.org/pypi/openinference-instrumentation-litellm)             |\n| [`openinference-instrumentation-groq`](./python/instrumentation/openinference-instrumentation-groq)                   | OpenInference Instrumentation for Groq.              | [![PyPI Version](https://img.shields.io/pypi/v/openinference-instrumentation-groq.svg)](https://pypi.python.org/pypi/openinference-instrumentation-groq)                   |\n| [`openinference-instrumentation-instructor`](./python/instrumentation/openinference-instrumentation-instructor)       | OpenInference Instrumentation for Instructor.        | [![PyPI Version](https://img.shields.io/pypi/v/openinference-instrumentation-instructor.svg)](https://pypi.python.org/pypi/openinference-instrumentation-instructor)       |\n| [`openinference-instrumentation-anthropic`](./python/instrumentation/openinference-instrumentation-anthropic)         | OpenInference Instrumentation for Anthropic.         | [![PyPI Version](https://img.shields.io/pypi/v/openinference-instrumentation-anthropic.svg)](https://pypi.python.org/pypi/openinference-instrumentation-anthropic)         |\n| [`openinference-instrumentation-beeai`](./python/instrumentation/openinference-instrumentation-beeai)                 | OpenInference Instrumentation for BeeAI.             | [![PyPI Version](https://img.shields.io/pypi/v/openinference-instrumentation-beeai.svg)](https://pypi.python.org/pypi/openinference-instrumentation-beeai)                 |\n\n### Examples\n\n| Name                                                                                                  | Description                                                                                  | Complexity Level |\n| ----------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------- | ---------------- |\n| [OpenAI SDK](python/instrumentation/openinference-instrumentation-openai/examples/)                   | OpenAI Python SDK, including chat completions and embeddings                                 | Beginner         |\n| [MistralAI SDK](python/instrumentation/openinference-instrumentation-mistralai/examples/)             | MistralAI Python SDK                                                                         | Beginner         |\n| [VertexAI SDK](python/instrumentation/openinference-instrumentation-vertexai/examples/)               | VertexAI Python SDK                                                                          | Beginner         |\n| [LlamaIndex](python/instrumentation/openinference-instrumentation-llama-index/examples/)              | LlamaIndex query engines                                                                     | Beginner         |\n| [DSPy](python/instrumentation/openinference-instrumentation-dspy/examples/)                           | DSPy primitives and custom RAG modules                                                       | Beginner         |\n| [Boto3 Bedrock Client](python/instrumentation/openinference-instrumentation-bedrock/examples/)        | Boto3 Bedrock client                                                                         | Beginner         |\n| [LangChain](python/instrumentation/openinference-instrumentation-langchain/examples/)                 | LangChain primitives and simple chains                                                       | Beginner         |\n| [LiteLLM](python/instrumentation/openinference-instrumentation-litellm/)                              | A lightweight LiteLLM framework                                                              | Beginner         |\n| [LiteLLM Proxy](python/instrumentation/openinference-instrumentation-litellm/examples/litellm-proxy/) | LiteLLM Proxy to log OpenAI, Azure, Vertex, Bedrock                                          | Beginner         |\n| [Groq](python/instrumentation/openinference-instrumentation-groq/examples/)                           | Groq and AsyncGroq chat completions                                                          | Beginner         |\n| [Anthropic](python/instrumentation/openinference-instrumentation-anthropic/examples/)                 | Anthropic Messages client                                                                    | Beginner         |\n| [BeeAI](python/instrumentation/openinference-instrumentation-beeai/examples/)                         | Agentic instrumentation in the BeeAI framework                                               | Beginner         |\n| [LlamaIndex + Next.js Chatbot](python/examples/llama-index/)                                          | A fully functional chatbot using Next.js and a LlamaIndex FastAPI backend                    | Intermediate     |\n| [LangServe](python/examples/langserve/)                                                               | A LangChain application deployed with LangServe using custom metadata on a per-request basis | Intermediate     |\n| [DSPy](python/examples/dspy-rag-fastapi/)                                                             | A DSPy RAG application using FastAPI, Weaviate, and Cohere                                   | Intermediate     |\n| [Haystack](python/instrumentation/openinference-instrumentation-haystack/examples/)                   | A Haystack QA RAG application                                                                | Intermediate     |\n| [OpenAI Agents](python/instrumentation/openinference-instrumentation-openai-agents/examples/)         | OpenAI Agents with handoffs                                                                  | Intermediate     |\n\n## JavaScript\n\n### Libraries\n\n| Package                                                                                                     | Description                                     | Version                                                                                                                                                                         |\n| ----------------------------------------------------------------------------------------------------------- | ----------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| [`@arizeai/openinference-semantic-conventions`](./js/packages/openinference-semantic-conventions)           | Semantic conventions for tracing of LLM Apps.   | [![NPM Version](https://img.shields.io/npm/v/@arizeai/openinference-semantic-conventions.svg)](https://www.npmjs.com/package/@arizeai/openinference-semantic-conventions)       |\n| [`@arizeai/openinference-core`](./js/packages/openinference-core)                                           | Core utility functions for instrumentation      | [![NPM Version](https://img.shields.io/npm/v/@arizeai/openinference-core.svg)](https://www.npmjs.com/package/@arizeai/openinference-core)                                       |\n| [`@arizeai/openinference-instrumentation-beeai`](./js/packages/openinference-instrumentation-beeai)         | OpenInference Instrumentation for BeeAI.        | [![NPM Version](https://img.shields.io/npm/v/@arizeai/openinference-instrumentation-beeai)](https://www.npmjs.com/package/@arizeai/openinference-instrumentation-beeai)         |\n| [`@arizeai/openinference-instrumentation-langchain`](./js/packages/openinference-instrumentation-langchain) | OpenInference Instrumentation for LangChain.js. | [![NPM Version](https://img.shields.io/npm/v/@arizeai/openinference-instrumentation-langchain)](https://www.npmjs.com/package/@arizeai/openinference-instrumentation-langchain) |\n| [`@arizeai/openinference-instrumentation-mcp`](./js/packages/openinference-instrumentation-mcp)             | OpenInference Instrumentation for MCP.          | [![NPM Version](https://img.shields.io/npm/v/@arizeai/openinference-instrumentation-mcp)](https://www.npmjs.com/package/@arizeai/openinference-instrumentation-mcp)             |\n| [`@arizeai/openinference-instrumentation-openai`](./js/packages/openinference-instrumentation-openai)       | OpenInference Instrumentation for OpenAI SDK.   | [![NPM Version](https://img.shields.io/npm/v/@arizeai/openinference-instrumentation-openai)](https://www.npmjs.com/package/@arizeai/openinference-instrumentation-openai)       |\n| [`@arizeai/openinference-vercel`](./js/packages/openinference-vercel)                                       | OpenInference Support for Vercel AI SDK         | [![NPM Version](https://img.shields.io/npm/v/@arizeai/openinference-vercel)](https://www.npmjs.com/package/@arizeai/openinference-vercel)                                       |\n\n### Examples\n\n| Name                                                                                                                     | Description                                                                                                                                                                                          | Complexity Level |\n| ------------------------------------------------------------------------------------------------------------------------ | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------- |\n| [OpenAI SDK](js/examples/openai)                                                                                         | OpenAI Node.js client                                                                                                                                                                                | Beginner         |\n| [BeeAI framework - ReAct agent](js/packages/openinference-instrumentation-beeai/examples/run-react-agent.ts)             | Agentic `ReActAgent` instrumentation in the BeeAI framework                                                                                                                                          | Beginner         |\n| [BeeAI framework - ToolCalling agent](js/packages/openinference-instrumentation-beeai/examples/run-toolcalling-agent.ts) | Agentic `ToolCallingAgent` instrumentation in the BeeAI framework                                                                                                                                    | Beginner         |\n| [BeeAI framework - LLM](js/packages/openinference-instrumentation-beeai/examples/run-llm.ts)                             | See how to run instrumentation only for the specific LLM module part in the BeeAI framework                                                                                                          | Beginner         |\n| [LlamaIndex Express App](js/examples/llama-index-express)                                                                | A fully functional LlamaIndex chatbot with a Next.js frontend and a LlamaIndex Express backend, instrumented using `openinference-instrumentation-openai`                                            | Intermediate     |\n| [LangChain OpenAI](js/packages/openinference-instrumentation-langchain/examples)                                         | A simple script to call OpenAI via LangChain, instrumented using `openinference-instrumentation-langchain`                                                                                           | Beginner         |\n| [LangChain RAG Express App](js/examples/langchain-express)                                                               | A fully functional LangChain chatbot that uses RAG to answer user questions. It has a Next.js frontend and a LangChain Express backend, instrumented using `openinference-instrumentation-langchain` | Intermediate     |\n| [Next.js + OpenAI](js/examples/nextjs-openai-simple/)                                                                    | A Next.js 13 project bootstrapped with `create-next-app` that uses OpenAI to generate text                                                                                                           | Beginner         |\n\n## Supported Destinations\n\nOpenInference supports the following destinations as span collectors.\n\n-   ✅ [Arize-Phoenix](https://github.com/Arize-ai/phoenix)\n-   ✅ [Arize](https://arize.com/)\n-   ✅ Any OTEL-compatible collector\n\n## Community\n\nJoin our community to connect with thousands of machine learning practitioners and LLM observability enthusiasts!\n\n-   🌍 Join our [Slack community](https://arize-ai.slack.com/join/shared_invite/zt-11t1vbu4x-xkBIHmOREQnYnYDH1GDfCg?__hstc=259489365.a667dfafcfa0169c8aee4178d115dc81.1733501603539.1733501603539.1733501603539.1&__hssc=259489365.1.1733501603539&__hsfp=3822854628&submissionGuid=381a0676-8f38-437b-96f2-fc10875658df#/shared-invite/email).\n-   💡 Ask questions and provide feedback in the _#phoenix-support_ channel.\n-   🌟 Leave a star on our [GitHub](https://github.com/Arize-ai/openinference).\n-   🐞 Report bugs with [GitHub Issues](https://github.com/Arize-ai/openinference/issues).\n-   𝕏 Follow us on [X](https://twitter.com/ArizePhoenix).\n-   🗺️ Check out our [roadmap](https://github.com/orgs/Arize-ai/projects/45) to see where we're heading next.\n"
    },
    {
      "name": "jlonge4/local_llama",
      "stars": 267,
      "img": "https://avatars.githubusercontent.com/u/91354480?s=40&v=4",
      "owner": "jlonge4",
      "repo_name": "local_llama",
      "description": "This repo is to showcase how you can run a model locally and offline, free of OpenAI dependencies.",
      "homepage": "",
      "language": "Python",
      "created_at": "2023-05-20T18:05:24Z",
      "updated_at": "2025-04-19T12:04:59Z",
      "topics": [
        "artificial-intelligence",
        "langchain",
        "llama-cpp",
        "llamaindex",
        "machinelearning",
        "offline",
        "python"
      ],
      "readme": "# Local Llama\n\nThis project enables you to chat with your PDFs, TXT files, or Docx files entirely offline, free from OpenAI dependencies. It's an evolution of the gpt_chatwithPDF project, now leveraging local LLMs for enhanced privacy and offline functionality.\n\n## Features\n\n- Offline operation: Run in airplane mode\n- Local LLM integration: Uses Ollama for improved performance\n- Multiple file format support: PDF, TXT, DOCX, MD\n- Persistent vector database: Reusable indexed documents\n- Streamlit-based user interface\n\n## New Updates\n\n- Ollama integration for significant performance improvements\n- Uses nomic-embed-text and llama3:8b models (can be changed to your liking)\n- Upgraded to Haystack 2.0\n- Persistent Chroma vector database to enable re-use of previously updloaded docs\n\n## Installation\n\n1. Install Ollama from https://ollama.ai/download\n2. Clone this repository\n3. Install dependencies:\n   ```\n   pip install -r requirements.txt\n   ```\n4. Pull required Ollama models:\n   ```\n   ollama pull nomic-embed-text\n   ollama pull llama3:8b\n   ```\n\n## Usage\n\n1. Start the Ollama server:\n   ```\n   ollama serve\n   ```\n2. Run the Streamlit app:\n   ```\n   python -m streamlit run local_llama_v3.py\n   ```\n3. Upload your documents and start chatting!\n\n## How It Works\n\n1. Document Indexing: Uploaded files are processed, split, and embedded using Ollama.\n2. Vector Storage: Embeddings are stored in a local Chroma vector database.\n3. Query Processing: User queries are embedded and relevant document chunks are retrieved.\n4. Response Generation: Ollama generates responses based on the retrieved context and chat history.\n\n\n## License\n\nThis project is licensed under the Apache 2.0 License.\n\n## Acknowledgements\n\n- Ollama team for their excellent local LLM solution\n- Haystack for providing the RAG framework\n- The-Bloke for the GGUF models\n"
    },
    {
      "name": "maxent-ai/ocrpy",
      "stars": 223,
      "img": "https://avatars.githubusercontent.com/u/34341850?s=40&v=4",
      "owner": "maxent-ai",
      "repo_name": "ocrpy",
      "description": "OCR, Archive, Index and Search: Implementation agnostic OCR framework.",
      "homepage": "https://maxentlabs.com/ocrpy",
      "language": "Jupyter Notebook",
      "created_at": "2020-10-18T13:13:36Z",
      "updated_at": "2025-04-06T17:20:03Z",
      "topics": [
        "aws",
        "azure",
        "computer-vision",
        "cv",
        "deep-learning",
        "google-vision-api",
        "image-processing",
        "information-retrieval",
        "nlp",
        "ocr",
        "ocr-python",
        "python",
        "semantic-search",
        "tesseract-ocr",
        "transformers"
      ],
      "readme": "# ocrpy\n\n[![Downloads](https://static.pepy.tech/personalized-badge/ocrpy?period=total&units=abbreviation&left_color=black&right_color=blue&left_text=Downloads)](https://pepy.tech/project/ocrpy)\n![contributors](https://img.shields.io/github/contributors/maxent-ai/ocrpy?color=blue)\n![PyPi](https://img.shields.io/pypi/v/ocrpy?color=blue)\n![tag](https://img.shields.io/github/v/tag/maxent-ai/ocrpy)\n![mit-license](https://img.shields.io/github/license/maxent-ai/ocrpy?color=blue)\n\n__Unified interface to google vision, aws textract, azure, tesseract and other OCR tools__\n\nThe core objective of `ocrpy` is to let users perform OCR, archive, index and search any document with ease, providing an intuitive interface and a powerful Pipeline API to solve common OCR-based tasks.\n\n`ocrpy` achieves this by wrapping around the most popular OCR engines like [Tesseract OCR](https://tesseract-ocr.github.io/), [Aws Textract](https://aws.amazon.com/textract/), [Google Cloud Vision](https://cloud.google.com/vision/docs/ocr) and [Azure Computer Vision](https://azure.microsoft.com/en-in/services/cognitive-services/computer-vision/#features). It unifies the multitude of interfaces provided by a wide range of cloud tools & other open-source libraries under a common and easy-to-use interface for the user.\n\n![](docs/_static/ocrpy-workflow.png)\n\n## Getting Started\n\n`ocrpy` is a Python-only package hosted on [PyPI](https://pypi.org/project/ocrpy/).\nThe recommended installation method is [pip](https://pip.pypa.io/en/stable/)\n\n```bash\npip install ocrpy\n```\n\n## Day-to-Day Usage\n\n`ocrpy` provides various levels of abstraction for the user to perform OCR on different types of documents. The recommended and the best way to use `ocrpy` is through it's `pipeline` API as shown below.\n\nThe Pipeline API can be invoked in two ways. The first method is to define the config for running the pipeline as a yaml file and and then run the pipeline by loading it as follows: \n\n```python\n\n   from ocrpy import TextOcrPipeline\n\n   ocr_pipeline = TextOcrPipeline.from_config(\"ocrpy_config.yaml\")\n   ocr_pipeline.process()\n```\n\nAlternatively you can run a pipeline by directly instantiating the pipeline class as follows:\n\n```python\n\n   from ocrpy import TextOcrPipeline\n\n   pipeline = TextOcrPipeline(source_dir='s3://document_bucket/', \n                              destination_dir=\"gs://processed_document_bucket/outputs/\", \n                              parser_backend='aws-textract', \n                              credentials_config={\"AWS\": \"path/to/aws-credentials.env/file\", \n                                           \"GCP\": \"path/to/gcp-credentials.json/file\"})\n   pipeline.process()\n```\n\n> :memo: For a more detailed set of examples and tutorials on how you could use ocrpy for your use case can be found at [ocrpy documentation](https://maxentlabs.com/ocrpy/).\n\n## Support and Documentation\n\n* For an in-depth reference of the `ocrpy` API refer to our [API docs](https://maxentlabs.com/ocrpy/api-reference.html).\n* For inspiration on how to use ocrpy for your usecase, check out our [tutorials](https://maxentlabs.com/ocrpy/tutorials.html) or our [examples](https://maxentlabs.com/ocrpy/examples.html).\n* If you're interested in understanding how ocrpy works, check out our [Ocrpy Overview](https://maxentlabs.com/ocrpy/overview.html).\n\n## Feedback and Contributions\n\n* If you have any questions, Feedback or notice something wrong, please open an issue on [GitHub Issues](https://github.com/maxent-ai/ocrpy/issues/).\n* If you are interested in contributing to the project, please open a PR on [GitHub Pull Requests](https://github.com/maxent-ai/ocrpy/pulls).\n* Or if you just want to say hi, feel free to [contact us](info@maxentlabs.com).\n\n## Citation\n\nIf you wish to cite this project, feel free to use this [BibTeX](http://www.bibtex.org/) reference:\n\n```bibtex\n@misc{ocrpy,\n    title={Ocrpy: OCR, Archive, Index and Search any documents with ease},\n    author={maxentlabs},\n    year={2022},\n    publisher = {GitHub},\n    howpublished = {\\url{https://github.com/maxent-ai/ocrpy}}\n}\n```\n\n## License and Credits\n\n* `ocrpy` is licensed under the [MIT](https://choosealicense.com/licenses/mit/) license.\nThe full license text can be also found in the [source code repository](https://github.com/maxent-ai/ocrpy/blob/main/LICENSE).\n* `ocrpy` is written and maintained by [Bharath G.S](https://github.com/bharathgs) and [Rita Anjana](https://github.com/AnjanaRita).\n* A full list of contributors can be found in [GitHub's overview](https://github.com/maxent-ai/ocrpy/graphs/contributors).\n"
    },
    {
      "name": "ocdevel/gnothi",
      "stars": 186,
      "img": "https://avatars.githubusercontent.com/u/103905581?s=40&v=4",
      "owner": "ocdevel",
      "repo_name": "gnothi",
      "description": "Gnothi is an open-source AI journal and toolkit for self-discovery. If you're interested in getting involved, we'd love to hear from you.",
      "homepage": "https://gnothiai.com",
      "language": "HTML",
      "created_at": "2019-11-12T06:02:24Z",
      "updated_at": "2025-04-15T14:34:48Z",
      "topics": [],
      "readme": "# Welcome to Gnothi on GitHub!\n\nHello there, seekers of self-knowledge! We're excited to introduce Gnothi, a journal and toolkit that uses AI to help you introspect and find resources.\n\nGnothi, rooted in the ancient aphorism \"Know thyself\", is an open-source project designed to guide your self-discovery journey. As an AI-assisted journal and toolkit, Gnothi helps you identify patterns and themes in your life, analyze your dreams, track habits, and even connect you with resources such as book recommendations. It also allows you to share entries with friends or therapists, fostering a supportive environment for growth and change.\n\nOur approach blends traditional practices like journaling and behavior tracking with AI-driven insights to highlight areas of opportunity for learning and personal growth. While we don't claim to replace therapy or offer medical advice, thousands of individuals have found value in using Gnothi as part of their self-improvement journey.\n\nWe're passionate about the potential of AI to enrich our understanding of ourselves and our lives, and we believe in the power of open-source to make this technology accessible and evolving. That's where you come in! We invite you to contribute to Gnothi's codebase, helping us refine our tools and broaden the impact of this project.\n\nWhether you're a seasoned developer or just getting started, we welcome your input. So, come join us on this exciting journey of self-discovery and tech innovation. Let's work together to make Gnothi the best it can be!\n\nThe journey of a thousand miles begins with a single commit. Happy coding!\n\n\n* Summaries: AI summarizes your entries, your week, your year.\n* Themes: AI shows your recurring themes & issues. Also valuable for dream themes.\n* Prompt: chat with your journal (TODO integrate local language models)\n* Books: AI recommends self-help books based on your entries.\n* Security: industry best practices\n* Field Tracking (lots to be done here): Track fields (mood, sleep, substance intake, etc). AI shows you how they interact and which ones to focus on.\n* Share (coming soon): Share journals with therapists, who can use all these tools to catch up since your last session.\n* Questions (coming soon): Ask AI anything about yourself. The answers and insights may surprise you.\n\n# Setup\nThis is an SST site (CDK, AWS). Which means even local development runs against an AWS stack. Normally that's awesome and cheap, but for Gnothi there's a VPC for security (private subnets and a NAT gateway), which is $30/mo min; and Aurora Serverless v2 Postgres, which is $40/mo min. So I need to figure out how to Dockerize the dev requirements for localhost development. In the mean time, if you see an opportunity for bugs/features in the code, take a stab at it and I'll integreate on my end. I'll beef up this README when I can get a viable local dev setup.\n\n### Steps\n* `docker-compose up -d`\n* `cp .env .env.shared-prod` -> modify with your email\n* `cp .env .env.dev` -> modify with your email \n* `AWS_PROFILE=<your profile> npm start`\n* `cd web && npm start`"
    },
    {
      "name": "sambanova/toolbench",
      "stars": 150,
      "img": "https://avatars.githubusercontent.com/u/80118584?s=40&v=4",
      "owner": "sambanova",
      "repo_name": "toolbench",
      "description": "ToolBench, an evaluation suite for LLM tool manipulation capabilities. ",
      "homepage": "https://arxiv.org/abs/2305.16504 ",
      "language": "Python",
      "created_at": "2023-05-19T16:23:35Z",
      "updated_at": "2025-03-11T23:37:27Z",
      "topics": [],
      "readme": "<a href=\"https://sambanova.ai/\">\n<picture>\n  <source media=\"(prefers-color-scheme: dark)\" srcset=\"/images/SambaNova-light-logo-1.png\" height=\"60\">\n  <img alt=\"Text changing depending on mode. Light: 'So light!' Dark: 'So dark!'\" src=\"/images/SambaNova-dark-logo-1.png\" height=\"60\">\n</picture>\n</a>\n\n# ToolBench\n\n<p>\n    <a href=\"https://huggingface.co/spaces/qiantong-xu/toolbench-leaderboard\">\n        <img src=\"https://img.shields.io/badge/leaderboard-0.0.0-yellow\"\n            alt=\"leaderboard\"></a>\n    <a href=\"https://discord.gg/JehFG5HXKb\">\n        <img src=\"https://img.shields.io/discord/1105549926475247626?logo=discord\"\n            alt=\"chat on Discord\"></a>\n</p>\n\n<img src=\"./images/toolbench.jpg\" title=\"SambaNova\" height=\"180\" />\nRecent studies on software tool manipulation with large language models (LLMs) mostly rely on closed model APIs (e.g. OpenAI), as there is an significant gap of model accuracy between those closed models and all the rest open-source LLMs.\nTo study the root cause of the gap and further facilitate the development of open-source LLMs, especially their capabilities on tool manipulation, we create the ToolBench. \nThe ToolBench is a benchmark consisting of diverse software tools for real-world tasks. \nWe also provide easy-to-use infrastructure in this repository to directly evaluate the execution success rate of each model. \nContributions to this repo are highly welcomed! We are excited to see new action generation algorithms and new testing tasks.\n\n\n## Table of contents\n- [Prerequisites](#prerequisites)\n- [Installation](#installation)\n- [Usage](#usage)\n- [Tasks](#tasks)\n- [Available Checkpoints](#checkpoints)\n\n## Prerequisites \n\n### Credentials\n- Create an [OpenAI account](https://platform.openai.com/account/api-keys) and register an API key.\n- Follow [this guide](https://developers.google.com/workspace/guides/create-credentials#service-account) to create a Google Cloud service account and create credentials for the account. Enable Google Sheets API and Google Drive API for the credentials.\n- Create an account for [OpenWeather](https://home.openweathermap.org/users/sign_up) and register an API key\n- Register an API key for [The Cat API](https://thecatapi.com/signup)\n\nAfter registration, update your credentials in `credential.sh` and \n```\nsource credential.sh\n```\n\n### Software\n- [Conda](https://www.anaconda.com/) (anaconda)\n- [Java](https://www.oracle.com/java/technologies/downloads/) >= 11.0.13\n\n```\njava -version\nconda --version\n```\n\n## Installation\n\n- Activate a virtual environment\n```\nconda create --prefix ./venv python=3.8.13\nconda activate ./venv\n```\n\n- Download resources\n```\nsh download_resources.sh\n```\nPress Enter on all the questions. This process may take about 15 minutes.\n\n- Installation\n```\npip install -e .\n```\n\n- Make sure everything's good!\n```\npytest tests\n```\n\n<details>\n\n  <summary>Installation FAQ</summary>\n\n- `Permission denied: '/tmp/tika.log'`\n```\n# If you are sharing your machine with someone else, please set\nmkdir /tmp/$USER && export TIKA_LOG_PATH=/tmp/$USER\n```\n\n- Unable to find `libjvm.so`\n```\nexport JAVA_HOME=\n```\n</details>\n\n## Usage\nThis repository evaluates the API function call success rate on the following tools from the ToolBench:\n1. [OpenWeather](https://openweathermap.org/api)\n2. [The Cat API](https://thecatapi.com)\n3. Home Search (similar to [Redfin](https://www.redfin.com/))\n4. Trip Booking (similar to [booking.com](https://www.booking.com/))\n5. [Google Sheet](https://docs.gspread.org/)\n6. [VirtualHome](http://virtual-home.org/)\n7. [Webshop](https://webshop-pnlp.github.io/)\n8. [Tabletop](https://code-as-policies.github.io/)\n\nOne can kick off the evaluation job with `test.py` on any combination of tools, models, number of APIs \nto retrieve and number of demonstation examples to place in the prompt. Here is an example of evaluating \n`text-davinci-003` on `open_weather` task.\n```\npython test.py \\\n--task 'open_weather' --version 'v0' \\\n--top_k_api 10 --top_k_example 3 --num_test_samples -1 \\\n--client_name \"openai\" --model_name 'text-davinci-003' --max_output_token 128 \n```\nAll the results will be logged out to the `--out_dir`, which defults to `out/`.\nThere will also be a cache created for each `client_name` and `model_name` combination as a sqlite database. When you want to query that given LM with a past query (prompt), it will retrieve the answer from the cache directly without running LM inference again.\n\nMore examples can be found below:\n<details>\n\n  <summary>Evaluation of OpenAI Models</summary>\n    \n```\npython test.py --task 'open_weather' --version 'v0' --client_name \"openai\" --model_name 'text-davinci-003' --max_output_token 128 --top_k_api 10 --top_k_example 3 --num_test_samples -1\npython test.py --task 'the_cat_api' --version 'v0' --client_name \"openai\" --model_name 'text-davinci-003' --max_output_token 128 --top_k_api 3 --top_k_example 3 --num_test_samples -1\npython test.py --task 'virtual_home' --version 'v0' --client_name \"openai\" --model_name 'text-davinci-003' --max_output_token 128 --top_k_api 10 --top_k_example 3 --num_test_samples -1\npython test.py --task 'home_search' --version 'v0' --client_name \"openai\" --model_name 'text-davinci-003' --max_output_token 128 --top_k_api 15 --top_k_example 3 --num_test_samples -1\npython test.py --task 'booking' --version 'v0' --client_name \"openai\" --model_name 'text-davinci-003' --max_output_token 300 --top_k_api 15 --top_k_example 3 --num_test_samples -1\npython test.py --task 'google_sheets' --version 'v0' --client_name \"openai\" --model_name 'text-davinci-003' --max_output_token 256 --top_k_api 0 --top_k_example 3 --num_test_samples -1\npython test.py --task 'web_shop' --version 'v0' --client_name \"openai\" --model_name 'text-davinci-003' --max_output_token 128 --top_k_api 0 --top_k_example 3 --num_test_samples -1\npython test.py --task 'web_shop' --version 'v1' --client_name \"openai\" --model_name 'text-davinci-003' --max_output_token 128 --top_k_api 0 --top_k_example 3 --num_test_samples -1\npython test.py --task 'code_as_policies_tabletop' --version 'v0' --client_name \"openai\" --model_name 'text-davinci-003' --max_output_token 256 --top_k_api 0 --top_k_example 0 --num_test_samples -1\n```\n</details>\n\n<details>\n\n  <summary>Evaluation of HuggingFace Models Locally</summary>\n    \n- To host a model on a server, independent from this repo, follow [manifest](https://github.com/HazyResearch/manifest).\n- Find the IP address + port in the output of the commands above, and plug them in to the following commands.\n```\npython test.py --task 'open_weather' --version 'v0' --client_name \"huggingface\" --model 'facebook/opt-iml-30b' --client_connection 'http://10.10.1.98:5000' --max_output_token 128 --top_k_api 10 --top_k_example 3 --num_test_samples -1\npython test.py --task 'the_cat_api' --version 'v0' --client_name \"huggingface\" --model 'facebook/opt-iml-30b' --client_connection 'http://10.10.1.98:5000' --max_output_token 128 --top_k_api 3 --top_k_example 3 --num_test_samples -1\npython test.py --task 'virtual_home' --version 'v0' --client_name \"huggingface\" --model 'facebook/opt-iml-30b' --client_connection 'http://10.10.1.98:5000' --max_output_token 128 --top_k_api 10 --top_k_example 3 --num_test_samples -1\npython test.py --task 'home_search' --version 'v0' --client_name \"huggingface\" --model 'facebook/opt-iml-30b' --client_connection 'http://10.10.1.98:5000' --max_output_token 128 --top_k_api 15 --top_k_example 3 --num_test_samples -1\npython test.py --task 'booking' --version 'v0' --client_name \"huggingface\" --model 'facebook/opt-iml-30b' --client_connection 'http://10.10.1.98:5000' --max_output_token 300 --top_k_api 15 --top_k_example 3 --num_test_samples -1\npython test.py --task 'google_sheets' --version 'v0' --client_name \"huggingface\" --model 'facebook/opt-iml-30b' --client_connection 'http://10.10.1.98:5000' --max_output_token 256 --top_k_api 0 --top_k_example 3 --num_test_samples -1\npython test.py --task 'web_shop' --version 'v0' --client_name \"huggingface\" --model 'facebook/opt-iml-30b' --client_connection 'http://10.10.1.98:5000' --max_output_token 128 --top_k_api 0 --top_k_example 3 --num_test_samples -1\npython test.py --task 'web_shop' --version 'v1' --client_name \"huggingface\" --model 'facebook/opt-iml-30b' --client_connection 'http://10.10.1.98:5000' --max_output_token 128 --top_k_api 0 --top_k_example 3 --num_test_samples -1\npython test.py --task 'code_as_policies_tabletop' --version 'v0' --client_name \"huggingface\" --model 'facebook/opt-iml-30b' --client_connection 'http://10.10.1.98:5000' --max_output_token 256 --top_k_api 0 --top_k_example 0 --num_test_samples -1\n```\n</details>\n\n<details>\n\n  <summary>Running FAQ</summary>\n    \n- If your job terminates with label assertion error: Rerun it.\n</details>\n\n\n## Tasks\n\n### Test set structure\nEach task in `data/` is organized in the following way.\n```\n$ tree <task>/<version>\n<task>/<version>/\n├── examples\n│   ├── 0_0.txt\n│   ├── 1_0.txt\n│   ├── ...\n│   ├── 8_0.txt\n│   └── 9_0.txt\n├── functions\n│   ├── search\n│   ├── select_home_type\n│   ├── ...\n│   ├── set_num_garages\n│   └── set_num_swimming_pools\n└── test.jsonl\n```\n- `<version>` is the version of the task.\n- `examples/` folder contains the example use cases of the APIs. Each file contains a single Q&A pair, where the answer is in the executable code. You can place as many examples as you want in this folder.\n- `functions` folder defines the set of API functions for this task. Each file contains the function signiture and a description.\n- `test.jsonl` is a list of test cases. \n- Addtional files for either task context or evaluation can also be placed in the folder. \n\n\n### Examples\n\nHere we list of a subset of the API docs as well as the examples of each ToolBench task.\n\n<details>\n\n  <summary>Open Weather</summary>\n\n  - [API source](https://openweathermap.org/api)\n  - API docs\n  ```\n  # Get the current air pollution data in location with latitude={lat}, longitude={lon}\n  curl -X GET 'https://api.openweathermap.org/data/2.5/air_pollution?lat={lat}&lon={lon}&appid={API_KEY}'\n\n  Parameters:\n  lat, lon: (required) Geographical coordinates (latitude, longitude).\n  appid: (required) Your unique API key.\n\n\n  # Get the weather forecast data in location with latitude={lat}, longitude={lon}\n  curl -X GET 'https://api.openweathermap.org/data/2.5/forecast?lat={lat}&lon={lon}&appid={API_KEY}{optional_params}'\n\n  Parameters:\n  lat, lon: (required) Geographical coordinates (latitude, longitude).\n  appid: (required) Your unique API key.\n  units: (optional) Units of measurement. 'standard' (default), 'metric' and 'imperial' units are available.\n  mode: (optional) Response format. 'JSON' format is used by default. To get data in 'XML' format use mode=xml.\n  lang: (optional) You can use the lang parameter to get the output in your language. 'en' for English (default); 'fr' for Franch; 'zh_cn' for simplified Chinese; 'it' for Italian; 'de' for German; 'ru' for Russian; 'ja' for Japanese; 'nl' for Dutch.\n  ```\n  - Examples\n  ```\n  Task: Do you know what's the weather like in the following days in 94957? Please give me a json-mode response in Dutch.\n  Action:\n  curl -X GET 'https://api.openweathermap.org/data/2.5/forecast?zip=94957&appid={API_KEY}&mode=json&lang=nl'\n\n\n  Task: What's the current air pollution level at the location with longitute -107.6 and latitude -75.4?\n  Action:\n  curl -X GET 'https://api.openweathermap.org/data/2.5/air_pollution?lat=-75.4&lon=-107.6&appid={API_KEY}'\n  ```\n</details>\n\n\n\n<details>\n\n  <summary>The Cat API</summary>\n\n  - [API source](https://thecatapi.com/)\n  - API docs\n  ```\n  # Delete the image with id {image_id} from the list of favorites\n  curl -X DELETE 'https://api.thecatapi.com/v1/favourites/{image_id}'\n\n  # Add the image with id {image_id} to the list of favorites\n  curl -X POST 'https://api.thecatapi.com/v1/favourites' --data '{\"image_id\":\"{image_id}\"}'\n\n  # Get the list of favorite cat images.\n  curl -X GET 'https://api.thecatapi.com/v1/favourites'\n  ```\n  - Examples\n  ```\n  Task: Remove the image tu from my favorites.\n  Action:\n  curl -X DELETE 'https://api.thecatapi.com/v1/favourites/tu'\n\n  Task: List all my favorite cat images.\n  Action:\n  curl -X GET 'https://api.thecatapi.com/v1/favourites'\n\n  Task: Vote up the image with id alp.\n  Action:\n  curl -X POST 'https://api.thecatapi.com/v1/votes' --data '{\"image_id\":\"alp\", \"value\":1}'\n  ```\n</details>\n\n\n<details>\n\n  <summary>Home Search</summary>\n\n  - API docs\n  ```\n  # To set home types for search. For home buying, home_types choices are: \"House\", \"Townhouse\", \"Condo\", \"Land\", \"Multi-family\", \"Mobile\", \"Co-op\"; for home renting, home_types choices are: \"House\", \"Townhouse\", \"Condo\", \"Apartment\".\n  API.select_home_type(home_types: List[str])\n\n  # To specify whether to search homes for buying or renting. 'value' can be chosen from ['buy', 'rent']. This function must be called after setting the location and before setting any other criteria.\n  API.set_buy_or_rent(value: str)\n\n  # To set the maximum commute time in minite\n  API.set_max_commute_time(value: int)\n\n  # To set the minimum home price in dollars\n  API.set_min_price(value: int)\n\n  # To set the maximum home price in dollars\n  API.set_max_price(value: int)\n\n  # To set the minimum home size in square feet\n  API.set_min_square_feet(value: int)\n\n  # To set the maximum home size in square feet\n  API.set_max_square_feet(value: int)\n\n  # To set the number of balconies\n  API.set_num_balconies(value: int)\n\n  # To set the floor number\n  API.set_floor_number(value: int)\n\n  # To set the number of swimming pool(s)\n  API.set_num_swimming_pools(value: int)\n\n  # To set the number of garage(s)\n  API.set_num_garages(value: int)\n\n  # To set the number of bedroom(s)\n  API.set_num_beds(value: int)\n\n  # To set the number of bathroom(s)\n  API.set_num_baths(value: float)\n\n  # To set the location for the search area. This function must be called before setting any criteria.\n  API.set_location(value: string)\n\n  # Submit criterion to get search results. This function should be called after setting all the criterion.\n  API.search()\n  ```\n  - Examples\n  ```\n  Task: I want to buy a townhouse, mobile or co-op in Pittsburgh with 4 rooms. My budget is $1385000.\n  Actions:\n  API.set_location(\"Pittsburgh\")\n  API.set_buy_or_rent(\"buy\")\n  API.select_home_type([\"Townhouse\", \"Mobile\", \"Co-op\"])\n  API.set_num_beds(4)\n  API.set_max_price(1385000)\n  API.search()\n\n  Task: Can you assist me in finding a co-op, townhouse or land in Aurora with 2 bedrooms, 1 bathrooms, 4 swimming pools, a price range of 118000 to 1464000, and a minimum square footage of 1300?\n  Actions:\n  API.set_location(\"Aurora\")\n  API.set_buy_or_rent(\"buy\")\n  API.select_home_type([\"Co-op\", \"Townhouse\", \"Land\"])\n  API.set_num_beds(2)\n  API.set_num_baths(1)\n  API.set_num_swimming_pools(4)\n  API.set_min_price(118000)\n  API.set_max_price(1464000)\n  API.set_min_square_feet(1300)\n  API.search()\n\n  Task: I'd like to rent a house, townhouse or apartment in Lincoln between 1150 and 3400 square feet, with 1 bedrooms, and a budget of $716000 to $1415000.\n  Actions:\n  API.set_location(\"Lincoln\")\n  API.set_buy_or_rent(\"rent\")\n  API.select_home_type([\"House\", \"Townhouse\", \"Apartment\"])\n  API.set_min_square_feet(1150)\n  API.set_max_square_feet(3400)\n  API.set_num_beds(1)\n  API.set_min_price(716000)\n  API.set_max_price(1415000)\n  API.search()\n  ```\n</details>\n\n\n<details>\n\n  <summary>Trip Booking</summary>\n\n  - API docs\n  ```\n  # To select the transportation type from ['flight', 'train', 'bus', 'cruise'].\n  API.select_transportation(transportation_type)\n\n  # To select the booking type from ['hotels', 'trip tickets', 'both'].\n  API.select_booking_type(booking_type)\n\n  # To set the number of child tickets to purchase.\n  API.set_num_children(value)\n\n  # To set the number of adult tickets to purchase.\n  API.set_num_adults(value)\n\n  # To set the location for arrival, given a Loc object.\n  API.set_destination(Loc)\n\n  # To set the location for departure, given a Loc object.\n  API.set_origin(Loc)\n\n  # To set the location for hotel search, given a Loc object.\n  API.set_hotel_location(Loc)\n\n  # To select the hotel room type from ['King Bed', 'Queen Bed', 'Double', 'Luxury'].\n  API.select_room_type(room_type)\n\n  # To set the number of hotel rooms to book.\n  API.set_num_rooms(value)\n\n  # Submit criterion to get search results.\n  API.search()\n  ```\n  - Examples\n  ```\n  I live in Laredo, and am planning a trip to Glendale. I need to book 5 adult round trip tickets for train and 1 hotel rooms for 5 nights. I'll leave and check in on 2023/08/25, and check out on 2023/08/30.\n  Actions:\n  API.select_booking_type(\"both\")\n  location_from = Loc(\"Laredo\")\n  API.set_origin(location_from)\n  location_to = Loc(\"Glendale\")\n  API.set_destination(location_to)\n  API.set_num_adults(5)\n  API.select_transportation(\"train\")\n  API.set_num_rooms(1)\n  checkin_date = Date(8, 25, 2023)\n  API.set_checkin_date(checkin_date)\n  checkout_date = Date(8, 30, 2023)\n  API.set_checkout_date(checkout_date)\n  hotel_location = Loc(\"Glendale\")\n  API.set_hotel_location(hotel_location)\n  API.search()\n\n  Could you help me find train tickets for 3 children and 5 adults from Des Moines to Cape Coral on July 07, 2022? My budget is up to 280 per ticket.\n  Actions:\n  API.select_booking_type(\"trip tickets\")\n  API.select_transportation(\"train\")\n  API.set_num_children(3)\n  API.set_num_adults(5)\n  location_from = Loc(\"Des Moines\")\n  API.set_origin(location_from)\n  location_to = Loc(\"Cape Coral\")\n  API.set_destination(location_to)\n  departure_date = Date(7, 7, 2022)\n  API.set_departure_date(departure_date)\n  API.set_max_ticket_price(280)\n  API.search()\n  ```\n</details>\n\n\n<details>\n\n  <summary>Google Sheets</summary>\n\n  - [API source](https://docs.gspread.org/)\n  - We also encourage the model to utilize [pands dataframe](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html) for advanced manipulations.\n  - The test data can be [found here](https://docs.google.com/spreadsheets/d/1dgsg17hqRHkrJnKvWQyFwinMJNrsi1z2uhWNiJCUVIQ/edit?usp=sharing).\n\n  - API docs\n  ```\n  # Sets values in a cell range of the sheet. \n  worksheet.update(range_name, values=None, **kwargs)\n\n  # Updates the value of a cell. \n  worksheet.update_cell(row, col, value)\n\n  # Deletes multiple columns from the worksheet at the specified index. \n  worksheet.delete_columns(start_index, end_index=None)\n\n  # Deletes multiple rows from the worksheet at the specified index. \n  worksheet.delete_rows(start_index, end_index=None)\n  ```\n  - Examples\n  ```\n  | Product | Cost | Price |\n  | beef | 1 | 3 |\n  | pork | 5 | 4 |\n  | chicken | 10 | 11 |\n  | lamb | 3 | 15 |\n  | duck | 12 | 2 |\n  | fish | 2 | 100 |\n\n  Task: Sets 'Hello world' in 'A2' cell\n  Actions:\n  worksheet.update('A2', 'Hello world')\n\n  Task: Sets 'Hello world' in 'A2' cell\n  Actions:\n  worksheet.update_cell(2, 1, 'Hello world')\n\n  Task: Updates A2 and A3 with values 42 and 43\n  Actions:\n  worksheet.update('A2:A3', [[42], [43]])\n\n  Task: Updates D2 with values 3\n  Actions:\n  worksheet.update('D2', 3)\n\n  Task: Sum A1:A4 and write the result below A4\n  Actions:\n  worksheet.update('A5', '=SUM(A1:A4)', raw=False)\n\n  Task: Update chicken's price by 2\n  Actions:\n  df = get_as_dataframe(worksheet)\n  df.loc[df['Product'] == 'chicken', 'Price'] += 2\n  worksheet.clear()\n  set_with_dataframe(worksheet, df, include_index=False, include_column_header=True)\n  ```\n</details>\n\n\n<details>\n\n  <summary>Virtual Home</summary>\n\n  - API is inherited from [VirtualHome](http://virtual-home.org/)\n  - Train/Test sets are from [this work](https://wenlong.page/language-planner/)\n\n  - API docs\n  ```\n  # Take a piece of clothes off. 'object' can only be: ['clothes_jacket', 'clothes_dress', 'clothes_hat', 'shoes', 'clothes_shirt', 'clothes_pants'].\n  Agent.TakeOff(object)\n\n  # Scrub an object. 'object' can only be: ['mop', 'cup', 'toilet', 'plate', 'soap', 'sink', 'spoon', 'cat', 'shower', 'dishwasher', 'hands_both', 'drinking_glass', 'bowl', 'towel'].\n  Agent.Scrub(object)\n\n  # Rinse an object. 'object' can only be: ['cup', 'pot', 'water', 'water_glass', 'sponge', 'soap', 'towel', 'dish_soap', 'oven', 'cleaning_solution', 'knife', 'spoon', 'sink', 'faucet', 'clothes_underwear', 'detergent', 'drinking_glass', 'hands_both', 'toilet', 'shower', 'rag', 'plate', 'bowl', 'fork'].\n  Agent.Rinse(object)\n\n  # Wash an object. 'object' can only be: ['face', 'cup', 'food_vegetable', 'dresser', 'fork', 'shoes', 'child', 'coffee_cup', 'bed', 'water', 'soap', 'duster', 'brush', 'bathtub', 'toy', 'cleaning_bottle', 'hair', 'sink', 'razor', 'hands_both', 'drinking_glass', 'table', 'toilet', 'basket_for_clothes', 'shower', 'dishwasher', 'plate', 'bowl', 'spoon'].\n  Agent.Wash(object)\n\n  # Turn to an object. 'object' can only be: ['cup', 'coffee_pot', 'button', 'dresser', 'milk', 'phone', 'water', 'bed', 'soap', 'mail', 'picture', 'filing_cabinet', 'oven', 'faucet', 'newspaper', 'food_cheese', 'towel', 'spoon', 'laptop', 'man', 'food_bread', 'dishrack', 'keyboard', 'freezer', 'bookshelf', 'chair', 'water_glass', 'brush', 'centerpiece', 'desk', 'kitchen_cabinet', 'fly', 'toy', 'mouse', 'homework', 'computer', 'television', 'shower', 'plate', 'clothes_pants', 'child', 'food_dessert', 'folder', 'window', 'hair', 'coffee_maker', 'hanger', 'cat', 'food_food', 'floor_lamp', 'creditcard', 'razor', 'electrical_outlet', 'washing_machine', 'toilet', 'coffee_table', 'paper_towel', 'clothes_dress', 'couch', 'drawing', 'remote_control', 'face', 'food_vegetable', 'vacuum_cleaner', 'light', 'shoes', 'pot', 'mirror', 'duster', 'cleaning_bottle', 'sink', 'toaster', 'novel', 'clothes_shirt', 'table', 'kitchen_counter', 'wall_clock', 'woman', 'dishwasher', 'dog', 'check'].\n  Agent.TurnTo(object)\n\n  # Open an object. 'object' can only be: ['coffee_pot', 'dresser', 'freezer', 'bookshelf', 'cupboard', 'folder', 'mail', 'filing_cabinet', 'window', 'oven', 'desk', 'kitchen_cabinet', 'trashcan', 'coffee_maker', 'curtain', 'bathroom_cabinet', 'nightstand', 'bag', 'washing_machine', 'toilet', 'basket_for_clothes', 'microwave', 'dishwasher', 'purse'].\n  Agent.Open(object)\n\n  # Pull an object. 'object' can only be: ['table', 'mop', 'mouse', 'chair', 'clothes_pants', 'light_bulb', 'curtain', 'vacuum_cleaner', 'mat', 'cat', 'food_food', 'drawing', 'shoes', 'centerpiece', 'sheets', 'pot', 'laptop'].\n  Agent.Pull(object)\n  ```\n  - Examples\n  ```\n  Task: Put down bags\n  Actions:\n  Agent.WalkTo(dining_room)\n  Agent.WalkTo(food_food)\n  Agent.Find(food_food)\n  Agent.Grab(food_food)\n  Agent.Find(table)\n  Agent.Put(food_food, table)\n\n  Task: Getting dresses\n  Actions:\n  Agent.WalkTo(bedroom)\n  Agent.WalkTo(dresser)\n  Agent.Find(dresser)\n  Agent.WalkTo(dresser)\n  Agent.Open(dresser)\n  Agent.Find(clothes_pants)\n  Agent.Grab(clothes_pants)\n  Agent.PutOn(clothes_pants)\n  Agent.Find(clothes_shirt)\n  Agent.Grab(clothes_shirt)\n  Agent.PutOn(clothes_shirt)\n  Agent.Close(dresser)\n  ```\n</details>\n\n\n<details>\n\n  <summary>Webshop</summary>\n\n  - API docs and train/test sets are from [this work](https://arxiv.org/pdf/2207.01206.pdf)\n\n  - API docs\n  ```\n  # search a text query, and go from 'search' page to 'results' page that lists a set of products returned by a search engine.\n  search[{query}]\n\n  # On the current page, click a button with text 'button_text', which is the lowercased text between any '[button]' and '[button_]'. For a single action, click on only one button.\n  click[button_text]\n  ```\n  - Examples\n  ```\n  Task:\n  Amazon Shopping Game\n  Instruction: \n  i need a high speed usb flash drive that is 32 gb, and price lower than 50.00 dollars\n  [button] Search [button_]\n\n  Action:\n  search[32 gb high speed usb flash drive ]\n\n  Task:\n  Instruction:\n  i need a high speed usb flash drive that is 32 gb, and price lower than 50.00 dollars\n  [button] Back to Search [button_]\n  Page 1 (Total results: 50)\n  [button] Next > [button_]\n\n  [button] B0845XPB49 [button_]\n  SanDisk 32GB Ultra Luxe USB 3.1 Flash Drive (Bulk 2 Pack) 150MB/s Speed 32 GB Pendrive Works with Computer, Laptop (SDCZ74-032G-G46) Bundle with (1) Everything But Stromboli Lanyard\n  $20.58\n\n  [button] B085X5548B [button_]\n  V7 32GB USB 3.1 Flash Drive - 32 GB - USB 3.1-120 MB/s Read Speed - Black\n  $11.59\n\n  [button] B08DQ22WFM [button_]\n  SanDisk Ultra Luxe 32GB USB 3.1 Flash Drive (Bulk 5 Pack) Works with Computer, Laptop, 150MB/s 32 GB PenDrive High Speed All Metal (SDCZ74-032G-G46) Bundle with (2) Everything But Stromboli Lanyards\n  $46.36\n\n  Action:\n  click[b085x5548b]\n  ```\n</details>\n\n  \n<details>\n\n  <summary>Tabletop</summary>\n\n  - API docs and train/test sets are from [this work](https://code-as-policies.github.io/)\n\n  - API docs: there is no explicit API definition. All the functions and usages are covered by the examples.\n  - Examples\n  ```\n  ##### High-level UI #####\n  objects = ['yellow block', 'green block', 'yellow bowl', 'blue block', 'blue bowl', 'green bowl']\n  # the yellow block on the yellow bowl.\n  say('Ok - putting the yellow block on the yellow bowl')\n  put_first_on_second('yellow block', 'yellow bowl')\n\n  objects = ['yellow block', 'green block', 'yellow bowl', 'blue block', 'blue bowl', 'green bowl']\n  # which block did you move.\n  say('I moved the yellow block')\n\n  objects = ['yellow block', 'green block', 'yellow bowl', 'blue block', 'blue bowl', 'green bowl']\n  # move the green block to the top right corner.\n  say('Got it - putting the green block on the top right corner')\n  corner_pos = parse_position('top right corner')\n  put_first_on_second('green block', corner_pos)\n\n  ##### parse_obj_name #####\n  bjects = ['blue block', 'cyan block', 'purple bowl', 'gray bowl', 'brown bowl', 'pink block', 'purple block']\n  # the block closest to the purple bowl.\n  block_names = ['blue block', 'cyan block', 'purple block']\n  block_positions = get_obj_positions_np(block_names)\n  closest_block_idx = get_closest_idx(points=block_positions, point=get_obj_pos('purple bowl'))\n  closest_block_name = block_names[closest_block_idx]\n  ret_val = closest_block_name\n\n  objects = ['brown bowl', 'banana', 'brown block', 'apple', 'blue bowl', 'blue block']\n  # the blocks.\n  ret_val = ['brown block', 'blue block']\n  objects = ['brown bowl', 'banana', 'brown block', 'apple', 'blue bowl', 'blue block']\n  # the brown objects.\n  ret_val = ['brown bowl', 'brown block']\n\n  ##### parse_position #####\n  # a 30cm horizontal line in the middle with 3 points.\n  middle_pos = denormalize_xy([0.5, 0.5]) \n  start_pos = middle_pos + [-0.3/2, 0]\n  end_pos = middle_pos + [0.3/2, 0]\n  line = make_line(start=start_pos, end=end_pos)\n  points = interpolate_pts_on_line(line=line, n=3)\n  ret_val = points\n\n  # a 20cm vertical line near the right with 4 points.\n  middle_pos = denormalize_xy([1, 0.5]) \n  start_pos = middle_pos + [0, -0.2/2]\n  end_pos = middle_pos + [0, 0.2/2]\n  line = make_line(start=start_pos, end=end_pos)\n  points = interpolate_pts_on_line(line=line, n=4)\n  ret_val = points\n\n  ##### parse_question #####\n  objects = ['yellow bowl', 'blue block', 'yellow block', 'blue bowl', 'fruit', 'green block', 'black bowl']\n  # is the blue block to the right of the yellow bowl?\n  ret_val = get_obj_pos('blue block')[0] > get_obj_pos('yellow bowl')[0]\n\n  objects = ['yellow bowl', 'blue block', 'yellow block', 'blue bowl', 'fruit', 'green block', 'black bowl']\n  # how many yellow objects are there?\n  yellow_object_names = parse_obj_name('the yellow objects', f'objects = {get_obj_names()}')\n  ret_val = len(yellow_object_names)\n\n  ##### helper functions #####\n  # define function: total = get_total(xs=numbers).\n  def get_total(xs):\n      return np.sum(xs)\n\n  # define function: y = eval_line(x, slope, y_intercept=0).\n  def eval_line(x, slope, y_intercept):\n      return x * slope + y_intercept\n  ```\n</details>\n\nHere is an example prompt we sent into the model with API definition, demonstration examples and the query \non the Open Weather.\n\n<details>\n\n  <summary>Prompt Example</summary>\n\n```\nI have the following set of API:\n\n# Get the current air pollution data in location with latitude={lat}, longitude={lon}\ncurl -X GET 'https://api.openweathermap.org/data/2.5/air_pollution?lat={lat}&lon={lon}&appid={API_KEY}'\n\nParameters:\nlat, lon: (required) Geographical coordinates (latitude, longitude).\nappid: (required) Your unique API key.\n\n# Get the forecast air pollution data in location with latitude={lat}, longitude={lon}\ncurl -X GET 'https://api.openweathermap.org/data/2.5/air_pollution/forecast?lat={lat}&lon={lon}&appid={API_KEY}'\n\nParameters:\nlat, lon: (required) Geographical coordinates (latitude, longitude).\nappid: (required) Your unique API key.\n\n# Get the current weather data in location with latitude={lat}, longitude={lon}\ncurl -X GET 'https://api.openweathermap.org/data/2.5/weather?lat={lat}&lon={lon}&appid={API_KEY}{optional_params}'\n\nParameters:\nlat, lon: (required) Geographical coordinates (latitude, longitude).\nappid: (required) Your unique API key.\nunits: (optional) Units of measurement. 'standard' (default), 'metric' and 'imperial' units are available.\nmode: (optional) Response format. 'JSON' format is used by default. To get data in 'XML' format use mode=xml.\nlang: (optional) You can use the lang parameter to get the output in your language. 'en' for English (default); 'fr' for French; 'zh_cn' for simplified Chinese; 'it' for Italian; 'de' for German; 'ru' for Russian; 'ja' for Japanese; 'nl' for Dutch.\n\n# Get the weather forecast data in location with latitude={lat}, longitude={lon}\ncurl -X GET 'https://api.openweathermap.org/data/2.5/forecast?lat={lat}&lon={lon}&appid={API_KEY}{optional_params}'\n\nParameters:\nlat, lon: (required) Geographical coordinates (latitude, longitude).\nappid: (required) Your unique API key.\nunits: (optional) Units of measurement. 'standard' (default), 'metric' and 'imperial' units are available.\nmode: (optional) Response format. 'JSON' format is used by default. To get data in 'XML' format use mode=xml.\nlang: (optional) You can use the lang parameter to get the output in your language. 'en' for English (default); 'fr' for French; 'zh_cn' for simplified Chinese; 'it' for Italian; 'de' for German; 'ru' for Russian; 'ja' for Japanese; 'nl' for Dutch.\n\n# Get up to 'k' geographical coordinates of locations named 'city_name'\ncurl -X GET 'https://api.openweathermap.org/geo/1.0/direct?q={city_name}&limit={k}&appid={API_KEY}'\n\nParameters:\nq: (required) City name.\nappid: (required) Your unique API key.\nlimit: (optional) Number of the locations in the API response.\n\n# Get the current weather data in {zipcode} area\ncurl -X GET 'https://api.openweathermap.org/data/2.5/weather?zip={zipcode}&appid={API_KEY}{optional_params}'\n\nParameters:\nzip: (required) Zip code.\nappid: (required) Your unique API key.\nunits: (optional) Units of measurement. 'standard' (default), 'metric' and 'imperial' units are available.\nmode: (optional) Response format. 'JSON' format is used by default. To get data in 'XML' format use mode=xml.\nlang: (optional) You can use the lang parameter to get the output in your language. 'en' for English (default); 'fr' for French; 'zh_cn' for simplified Chinese; 'it' for Italian; 'de' for German; 'ru' for Russian; 'ja' for Japanese; 'nl' for Dutch.\n\n# Get the weather forecast data in {zipcode} area\ncurl -X GET 'https://api.openweathermap.org/data/2.5/forecast?zip={zipcode}&appid={API_KEY}{optional_params}'\n\nParameters:\nzip: (required) Zip code.\nappid: (required) Your unique API key.\nunits: (optional) Units of measurement. 'standard' (default), 'metric' and 'imperial' units are available.\nmode: (optional) Response format. 'JSON' format is used by default. To get data in 'XML' format use mode=xml.\nlang: (optional) You can use the lang parameter to get the output in your language. 'en' for English (default); 'fr' for French; 'zh_cn' for simplified Chinese; 'it' for Italian; 'de' for German; 'ru' for Russian; 'ja' for Japanese; 'nl' for Dutch.\n\n# Get the weather forecast data in {city}\ncurl -X GET 'https://api.openweathermap.org/data/2.5/forecast?q={city_formatted}&appid={API_KEY}{optional_params}'\n\nParameters:\nq: (required) City name.\nappid: (required) Your unique API key.\nunits: (optional) Units of measurement. 'standard' (default), 'metric' and 'imperial' units are available.\nmode: (optional) Response format. 'JSON' format is used by default. To get data in 'XML' format use mode=xml.\nlang: (optional) You can use the lang parameter to get the output in your language. 'en' for English (default); 'fr' for French; 'zh_cn' for simplified Chinese; 'it' for Italian; 'de' for German; 'ru' for Russian; 'ja' for Japanese; 'nl' for Dutch.\n\n# Get the current weather data in {city}\ncurl -X GET 'https://api.openweathermap.org/data/2.5/weather?q={city_formatted}&appid={API_KEY}{optional_params}'\n\nParameters:\nq: (required) City name.\nappid: (required) Your unique API key.\nunits: (optional) Units of measurement. 'standard' (default), 'metric' and 'imperial' units are available.\nmode: (optional) Response format. 'JSON' format is used by default. To get data in 'XML' format use mode=xml.\nlang: (optional) You can use the lang parameter to get the output in your language. 'en' for English (default); 'fr' for French; 'zh_cn' for simplified Chinese; 'it' for Italian; 'de' for German; 'ru' for Russian; 'ja' for Japanese; 'nl' for Dutch.\n\n-------------\nI have the following set of examples:\n\nTask: Do you know what's the weather like in the following days in 94957? Please give me a json-mode response in Dutch.\nAction:\ncurl -X GET 'https://api.openweathermap.org/data/2.5/forecast?zip=94957&appid={API_KEY}&mode=json&lang=nl'\n\nTask: How's the air quality for the next few days at the place where longitute = -32.1 and latitude = -43.1?\nAction:\ncurl -X GET 'https://api.openweathermap.org/data/2.5/air_pollution/forecast?lat=-43.1&lon=-32.1&appid={API_KEY}'\n\nTask: Please give me the air quality data at longitute 163.3 and latitude -80.0 at this moment.\nAction:\ncurl -X GET 'https://api.openweathermap.org/data/2.5/air_pollution?lat=-80.0&lon=163.3&appid={API_KEY}'\n\n-------------\nTask: Can you tell me what to expect on the air quality in the coming days at the location with (longitute, latitude) = (133.3, -56.7)?\nActions:\n```\n</details>\n\n\n## Checkpoints\nWe release the 3 finetuned checkpoints we created in the paper\n- [LLaMA-30b](https://huggingface.co/sambanovasystems/LLaMA-30b-toolbench)\n- [StarCoder](https://huggingface.co/sambanovasystems/starcoder-toolbench)\n- [CodeGen-16b-mono](https://huggingface.co/sambanovasystems/codegen-16B-mono-toolbench)\n\n\\[Note\\] The checkpoints provided in this section is only for initial study purpose, \nwe do not expect the checkpoints to perform well in other tool tasks. \nWe highly encourage the community to contribute!\n\n\n## Citation\n```\n@misc{xu2023tool,\n      title={On the Tool Manipulation Capability of Open-source Large Language Models}, \n      author={Qiantong Xu and Fenglu Hong and Bo Li and Changran Hu and Zhengyu Chen and Jian Zhang},\n      year={2023},\n      eprint={2305.16504},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n```\n\n### Acknowledgement\nWe sincerely appreciate all the helpful discussions from Urmish Thakker, Tian Zhao, Raghu Prabhakar, Kaizhao Liang, Petro Junior Milan, \nBowen Yang, Qinghua Li and Yaqi Zhang.\n\nWe also want to express our gratitude to the great works in the open sourced models. Especially the ones we rely heavily on in the paper\n- [LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/abs/2302.13971)\n- [CodeGen: An Open Large Language Model for Code with Multi-Turn Program Synthesis](https://arxiv.org/abs/2203.13474)\n- [StarCoder: may the source be with you!](https://arxiv.org/abs/2305.06161)\n\nThere are also several other inspiring works about tool manipulation and benchmarking that we want to raise our thanks to\n- [Tool Learning with Foundation Models](https://arxiv.org/abs/2304.08354)\n- [API-Bank: A Benchmark for Tool-Augmented LLMs](https://arxiv.org/abs/2304.08244)\n- [Gorilla: Large Language Model Connected with Massive APIs](https://arxiv.org/abs/2305.15334)\n"
    },
    {
      "name": "deepset-ai/haystack-core-integrations",
      "stars": 144,
      "img": "https://avatars.githubusercontent.com/u/51827949?s=40&v=4",
      "owner": "deepset-ai",
      "repo_name": "haystack-core-integrations",
      "description": "Additional packages (components, document stores and the likes) to extend the capabilities of Haystack",
      "homepage": "https://haystack.deepset.ai",
      "language": "Python",
      "created_at": "2022-08-19T08:17:34Z",
      "updated_at": "2025-04-23T07:49:47Z",
      "topics": [
        "ai",
        "haystack",
        "llm",
        "mlops",
        "nlp"
      ],
      "readme": "# Haystack Core Integrations\n\nThis repository contains integrations to extend the capabilities of [Haystack](https://github.com/deepset-ai/haystack). The code in this repo is maintained by [deepset](https://www.deepset.ai), see each integration's `README` file for details around installation, usage and support.\n\n## Quick start\n\nYou will need `hatch` to work on or create new integrations, open [this link](https://hatch.pypa.io/latest/install/#installation)\nand follow the install instructions for your operating system and platform.\n\nAll the integrations are self contained, so the first step before working on one is to `cd` into the proper folder.\nFor example, to run the tests suite for the Chroma document store, from the root of the repo:\n\n```sh\n$ cd integrations/chroma\n$ hatch run test\n```\n\nHatch will take care of setting up an isolated Python environment and run the tests.\n\nPlease check out our [Contribution Guidelines](CONTRIBUTING.md) for all the details.\n\n## Inventory\n\n[![License Compliance](https://github.com/deepset-ai/haystack-core-integrations/actions/workflows/CI_license_compliance.yml/badge.svg)](https://github.com/deepset-ai/haystack-core-integrations/actions/workflows/CI_license_compliance.yml)\n\n| Package                                                                                                        | Type                        | PyPi Package                                                                                                                                             | Status                                                                                                                                                                                                                                               |\n|----------------------------------------------------------------------------------------------------------------|-----------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| [amazon-bedrock-haystack](integrations/amazon_bedrock/)                                                        | Embedder, Generator, Ranker | [![PyPI - Version](https://img.shields.io/pypi/v/amazon-bedrock-haystack.svg)](https://pypi.org/project/amazon-bedrock-haystack)                         | [![Test / amazon_bedrock](https://github.com/deepset-ai/haystack-core-integrations/actions/workflows/amazon_bedrock.yml/badge.svg)](https://github.com/deepset-ai/haystack-core-integrations/actions/workflows/amazon_bedrock.yml)                   |\n| [amazon-sagemaker-haystack](integrations/amazon_sagemaker/)                                                    | Generator                   | [![PyPI - Version](https://img.shields.io/pypi/v/amazon-sagemaker-haystack.svg)](https://pypi.org/project/amazon-sagemaker-haystack)                     | [![Test / amazon_sagemaker](https://github.com/deepset-ai/haystack-core-integrations/actions/workflows/amazon_sagemaker.yml/badge.svg)](https://github.com/deepset-ai/haystack-core-integrations/actions/workflows/amazon_sagemaker.yml)             |\n| [anthropic-haystack](integrations/anthropic/)                                                                  | Generator                   | [![PyPI - Version](https://img.shields.io/pypi/v/anthropic-haystack.svg)](https://pypi.org/project/anthropic-haystack)                                   | [![Test / anthropic](https://github.com/deepset-ai/haystack-core-integrations/actions/workflows/anthropic.yml/badge.svg)](https://github.com/deepset-ai/haystack-core-integrations/actions/workflows/anthropic.yml)                                  |\n| [astra-haystack](integrations/astra/)                                                                          | Document Store              | [![PyPI - Version](https://img.shields.io/pypi/v/astra-haystack.svg)](https://pypi.org/project/astra-haystack)                                           | [![Test / astra](https://github.com/deepset-ai/haystack-core-integrations/actions/workflows/astra.yml/badge.svg)](https://github.com/deepset-ai/haystack-core-integrations/actions/workflows/astra.yml)                                              |\n| [azure-ai-search-haystack](integrations/azure_ai_search/)                                                      | Document Store              | [![PyPI - Version](https://img.shields.io/pypi/v/azure-ai-search-haystack.svg)](https://pypi.org/project/azure-ai-search-haystack)                       | [![Test / azure-ai-search](https://github.com/deepset-ai/haystack-core-integrations/actions/workflows/azure_ai_search.yml/badge.svg)](https://github.com/deepset-ai/haystack-core-integrations/actions/workflows/azure_ai_search.yml)                |\n| [chroma-haystack](integrations/chroma/)                                                                        | Document Store              | [![PyPI - Version](https://img.shields.io/pypi/v/chroma-haystack.svg)](https://pypi.org/project/chroma-haystack)                                         | [![Test / chroma](https://github.com/deepset-ai/haystack-core-integrations/actions/workflows/chroma.yml/badge.svg)](https://github.com/deepset-ai/haystack-core-integrations/actions/workflows/chroma.yml)                                           |\n| [cohere-haystack](integrations/cohere/)                                                                        | Embedder, Generator, Ranker | [![PyPI - Version](https://img.shields.io/pypi/v/cohere-haystack.svg)](https://pypi.org/project/cohere-haystack)                                         | [![Test / cohere](https://github.com/deepset-ai/haystack-core-integrations/actions/workflows/cohere.yml/badge.svg)](https://github.com/deepset-ai/haystack-core-integrations/actions/workflows/cohere.yml)                                           |\n| [deepeval-haystack](integrations/deepeval/)                                                                    | Evaluator                   | [![PyPI - Version](https://img.shields.io/pypi/v/deepeval-haystack.svg)](https://pypi.org/project/deepeval-haystack)                                     | [![Test / deepeval](https://github.com/deepset-ai/haystack-core-integrations/actions/workflows/deepeval.yml/badge.svg)](https://github.com/deepset-ai/haystack-core-integrations/actions/workflows/deepeval.yml)                                     |\n| [elasticsearch-haystack](integrations/elasticsearch/)                                                          | Document Store              | [![PyPI - Version](https://img.shields.io/pypi/v/elasticsearch-haystack.svg)](https://pypi.org/project/elasticsearch-haystack)                           | [![Test / elasticsearch](https://github.com/deepset-ai/haystack-core-integrations/actions/workflows/elasticsearch.yml/badge.svg)](https://github.com/deepset-ai/haystack-core-integrations/actions/workflows/elasticsearch.yml)                      |\n| [fastembed-haystack](integrations/fastembed/)                                                                  | Embedder, Ranker            | [![PyPI - Version](https://img.shields.io/pypi/v/fastembed-haystack.svg)](https://pypi.org/project/fastembed-haystack/)                                  | [![Test / fastembed](https://github.com/deepset-ai/haystack-core-integrations/actions/workflows/fastembed.yml/badge.svg)](https://github.com/deepset-ai/haystack-core-integrations/actions/workflows/fastembed.yml)                                  |\n| [google-ai-haystack](integrations/google_ai/)                                                                  | Generator                   | [![PyPI - Version](https://img.shields.io/pypi/v/google-ai-haystack.svg)](https://pypi.org/project/google-ai-haystack)                                   | [![Test / google-ai](https://github.com/deepset-ai/haystack-core-integrations/actions/workflows/google_ai.yml/badge.svg)](https://github.com/deepset-ai/haystack-core-integrations/actions/workflows/google_ai.yml)                                  |\n| [google-vertex-haystack](integrations/google_vertex/)                                                          | Generator                   | [![PyPI - Version](https://img.shields.io/pypi/v/google-vertex-haystack.svg)](https://pypi.org/project/google-vertex-haystack)                           | [![Test / google-vertex](https://github.com/deepset-ai/haystack-core-integrations/actions/workflows/google_vertex.yml/badge.svg)](https://github.com/deepset-ai/haystack-core-integrations/actions/workflows/google_vertex.yml)                      |\n| [instructor-embedders-haystack](integrations/instructor_embedders/)                                            | Embedder                    | [![PyPI - Version](https://img.shields.io/pypi/v/instructor-embedders-haystack.svg)](https://pypi.org/project/instructor-embedders-haystack)             | [![Test / instructor-embedders](https://github.com/deepset-ai/haystack-core-integrations/actions/workflows/instructor_embedders.yml/badge.svg)](https://github.com/deepset-ai/haystack-core-integrations/actions/workflows/instructor_embedders.yml) |\n| [jina-haystack](integrations/jina/)                                                                            | Connector, Embedder, Ranker | [![PyPI - Version](https://img.shields.io/pypi/v/jina-haystack.svg)](https://pypi.org/project/jina-haystack)                                             | [![Test / jina](https://github.com/deepset-ai/haystack-core-integrations/actions/workflows/jina.yml/badge.svg)](https://github.com/deepset-ai/haystack-core-integrations/actions/workflows/jina.yml)                                                 |\n| [langfuse-haystack](integrations/langfuse/)                                                                    | Tracer                      | [![PyPI - Version](https://img.shields.io/pypi/v/langfuse-haystack.svg?color=orange)](https://pypi.org/project/langfuse-haystack)                        | [![Test / langfuse](https://github.com/deepset-ai/haystack-core-integrations/actions/workflows/langfuse.yml/badge.svg)](https://github.com/deepset-ai/haystack-core-integrations/actions/workflows/langfuse.yml)                                     |\n| [llama-cpp-haystack](integrations/llama_cpp/)                                                                  | Generator                   | [![PyPI - Version](https://img.shields.io/pypi/v/llama-cpp-haystack.svg?color=orange)](https://pypi.org/project/llama-cpp-haystack)                      | [![Test / llama-cpp](https://github.com/deepset-ai/haystack-core-integrations/actions/workflows/llama_cpp.yml/badge.svg)](https://github.com/deepset-ai/haystack-core-integrations/actions/workflows/llama_cpp.yml)                                  |\n| [mcp-haystack](integrations/mcp/)                                                                              | Tool                      | [![PyPI - Version](https://img.shields.io/pypi/v/mcp-haystack.svg?color=orange)](https://pypi.org/project/mcp-haystack)                        | [![Test / mcp](https://github.com/deepset-ai/haystack-core-integrations/actions/workflows/mcp.yml/badge.svg)](https://github.com/deepset-ai/haystack-core-integrations/actions/workflows/mcp.yml)                                                    |\n| [mistral-haystack](integrations/mistral/)                                                                      | Embedder, Generator         | [![PyPI - Version](https://img.shields.io/pypi/v/mistral-haystack.svg)](https://pypi.org/project/mistral-haystack)                                       | [![Test / mistral](https://github.com/deepset-ai/haystack-core-integrations/actions/workflows/mistral.yml/badge.svg)](https://github.com/deepset-ai/haystack-core-integrations/actions/workflows/mistral.yml)                                        |\n| [mongodb-atlas-haystack](integrations/mongodb_atlas/)                                                          | Document Store              | [![PyPI - Version](https://img.shields.io/pypi/v/mongodb-atlas-haystack.svg?color=orange)](https://pypi.org/project/mongodb-atlas-haystack)              | [![Test / mongodb-atlas](https://github.com/deepset-ai/haystack-core-integrations/actions/workflows/mongodb_atlas.yml/badge.svg)](https://github.com/deepset-ai/haystack-core-integrations/actions/workflows/mongodb_atlas.yml)                      |\n| [nvidia-haystack](integrations/nvidia/)                                                                        | Embedder, Generator, Ranker | [![PyPI - Version](https://img.shields.io/pypi/v/nvidia-haystack.svg?color=orange)](https://pypi.org/project/nvidia-haystack)                            | [![Test / nvidia](https://github.com/deepset-ai/haystack-core-integrations/actions/workflows/nvidia.yml/badge.svg)](https://github.com/deepset-ai/haystack-core-integrations/actions/workflows/nvidia.yml)                                           |\n| [ollama-haystack](integrations/ollama/)                                                                        | Embedder, Generator         | [![PyPI - Version](https://img.shields.io/pypi/v/ollama-haystack.svg?color=orange)](https://pypi.org/project/ollama-haystack)                            | [![Test / ollama](https://github.com/deepset-ai/haystack-core-integrations/actions/workflows/ollama.yml/badge.svg)](https://github.com/deepset-ai/haystack-core-integrations/actions/workflows/ollama.yml)                                           |\n| [opensearch-haystack](integrations/opensearch/)                                                                | Document Store              | [![PyPI - Version](https://img.shields.io/pypi/v/opensearch-haystack.svg)](https://pypi.org/project/opensearch-haystack)                                 | [![Test / opensearch](https://github.com/deepset-ai/haystack-core-integrations/actions/workflows/opensearch.yml/badge.svg)](https://github.com/deepset-ai/haystack-core-integrations/actions/workflows/opensearch.yml)                               |\n| [optimum-haystack](integrations/optimum/)                                                                      | Embedder                    | [![PyPI - Version](https://img.shields.io/pypi/v/optimum-haystack.svg)](https://pypi.org/project/optimum-haystack)                                       | [![Test / optimum](https://github.com/deepset-ai/haystack-core-integrations/actions/workflows/optimum.yml/badge.svg)](https://github.com/deepset-ai/haystack-core-integrations/actions/workflows/optimum.yml)                                        |\n| [pinecone-haystack](integrations/pinecone/)                                                                    | Document Store              | [![PyPI - Version](https://img.shields.io/pypi/v/pinecone-haystack.svg?color=orange)](https://pypi.org/project/pinecone-haystack)                        | [![Test / pinecone](https://github.com/deepset-ai/haystack-core-integrations/actions/workflows/pinecone.yml/badge.svg)](https://github.com/deepset-ai/haystack-core-integrations/actions/workflows/pinecone.yml)                                     |\n| [pgvector-haystack](integrations/pgvector/)                                                                    | Document Store              | [![PyPI - Version](https://img.shields.io/pypi/v/pgvector-haystack.svg?color=orange)](https://pypi.org/project/pgvector-haystack)                        | [![Test / pgvector](https://github.com/deepset-ai/haystack-core-integrations/actions/workflows/pgvector.yml/badge.svg)](https://github.com/deepset-ai/haystack-core-integrations/actions/workflows/pgvector.yml)                                     |\n| [qdrant-haystack](integrations/qdrant/)                                                                        | Document Store              | [![PyPI - Version](https://img.shields.io/pypi/v/qdrant-haystack.svg?color=orange)](https://pypi.org/project/qdrant-haystack)                            | [![Test / qdrant](https://github.com/deepset-ai/haystack-core-integrations/actions/workflows/qdrant.yml/badge.svg)](https://github.com/deepset-ai/haystack-core-integrations/actions/workflows/qdrant.yml)                                           |\n| [ragas-haystack](integrations/ragas/)                                                                          | Evaluator                   | [![PyPI - Version](https://img.shields.io/pypi/v/ragas-haystack.svg)](https://pypi.org/project/ragas-haystack)                                           | [![Test / ragas](https://github.com/deepset-ai/haystack-core-integrations/actions/workflows/ragas.yml/badge.svg)](https://github.com/deepset-ai/haystack-core-integrations/actions/workflows/ragas.yml)                                              |\n| [snowflake-haystack](integrations/snowflake/)                                                                  | Retriever                   | [![PyPI - Version](https://img.shields.io/pypi/v/snowflake-haystack.svg)](https://pypi.org/project/snowflake-haystack)                                   | [![Test / snowflake](https://github.com/deepset-ai/haystack-core-integrations/actions/workflows/snowflake.yml/badge.svg)](https://github.com/deepset-ai/haystack-core-integrations/actions/workflows/snowflake.yml)                                  |\n| [stackit-haystack](integrations/stackit/)                                                                      | Embedder, Generator         | [![PyPI - Version](https://img.shields.io/pypi/v/stackit-haystack.svg)](https://pypi.org/project/stackit-haystack)                                       | [![Test / stackit](https://github.com/deepset-ai/haystack-core-integrations/actions/workflows/stackit.yml/badge.svg)](https://github.com/deepset-ai/haystack-core-integrations/actions/workflows/stackit.yml)                                        |\n| [unstructured-fileconverter-haystack](integrations/unstructured/)                                              | File converter              | [![PyPI - Version](https://img.shields.io/pypi/v/unstructured-fileconverter-haystack.svg)](https://pypi.org/project/unstructured-fileconverter-haystack) | [![Test / unstructured](https://github.com/deepset-ai/haystack-core-integrations/actions/workflows/unstructured.yml/badge.svg)](https://github.com/deepset-ai/haystack-core-integrations/actions/workflows/unstructured.yml)                         |\n| [uptrain-haystack](https://github.com/deepset-ai/haystack-core-integrations/tree/staging/integrations/uptrain) | Evaluator                   | [![PyPI - Version](https://img.shields.io/pypi/v/uptrain-haystack.svg)](https://pypi.org/project/uptrain-haystack)                                       | [Staged](https://docs.haystack.deepset.ai/docs/breaking-change-policy#discontinuing-an-integration)                                                                                                                                                  |\n| [weaviate-haystack](integrations/weaviate/)                                                                    | Document Store              | [![PyPI - Version](https://img.shields.io/pypi/v/weaviate-haystack.svg)](https://pypi.org/project/weaviate-haystack)                                     | [![Test / weaviate](https://github.com/deepset-ai/haystack-core-integrations/actions/workflows/weaviate.yml/badge.svg)](https://github.com/deepset-ai/haystack-core-integrations/actions/workflows/weaviate.yml)                                     |\n| [weave-haystack](integrations/weights_and_biases_weave/)                                                       | Tracer                      | [![PyPI - Version](https://img.shields.io/pypi/v/weave-haystack.svg)](https://pypi.org/project/weave-haystack)                             | [![Test / weights_and_biases_weave](https://github.com/deepset-ai/haystack-core-integrations/actions/workflows/weights_and_biases_weave.yml/badge.svg)](https://github.com/deepset-ai/haystack-core-integrations/actions/workflows/weights_and_biases_weave.yml)                      |\n\n## Releasing\n\n> [!NOTE]\n> Only maintainers can release new versions of integrations.\n> If you're a community contributor and want to release a new version of an integration,\n> reach out to a maintainer.\n\nTo release a new version of an integration to PyPI tag the commit with the right version number and push the tag to \nGitHub. The GitHub Actions workflow will take care of the rest.\n\n1. Tag the commit with the right version number\n\n    The tag needs to have the following format:\n\n    ```\n    git tag integrations/<INTEGRATION_FOLDER_NAME>-<version>\n    ```\n\n    For example, if we want to release version 1.0.99 of the google-vertex-haystack integration we'd have to push the tag:\n\n    ```\n    git tag integrations/google_vertex-v1.0.99\n    ```\n2. Push the tag to GitHub\n\n    ```\n    git push --tags origin\n    ```\n3. Wait for the CI to do its magic\n\n> [!IMPORTANT]  \n> When releasing a new integration version, always tag a commit that includes the changes for that integration \n> (usually the PR merge commit). If you tag a commit that doesn't include changes for the integration being released, \n> the generated changelog will be incorrect.\n"
    },
    {
      "name": "rickiepark/nlp-with-transformers",
      "stars": 140,
      "img": "https://avatars.githubusercontent.com/u/18256853?s=40&v=4",
      "owner": "rickiepark",
      "repo_name": "nlp-with-transformers",
      "description": "<트랜스포머를 활용한 자연어 처리> 예제 코드를 위한 저장소입니다.",
      "homepage": "https://bit.ly/transformer-home",
      "language": "Jupyter Notebook",
      "created_at": "2022-06-10T06:16:52Z",
      "updated_at": "2025-04-07T05:00:51Z",
      "topics": [
        "deep-learning",
        "huggingface",
        "machine-learning",
        "natural-language-processing",
        "nlp",
        "transformers"
      ],
      "readme": "# 트랜스포머를 활용한 자연어 처리\n\n이 저장소는 [트랜스포머를 활용한 자연어 처리](https://tensorflow.blog/transformer-nlp/) 책의 코드 저장소입니다.\n\n<img alt=\"book-cover\" height=400 src=\"https://tensorflowkorea.files.wordpress.com/2022/11/ed919ceca780_ed8ab8eb9e9cec8aa4ed8faceba8b8eba5bced999cec9aa9ed959cec9e90ec97b0ec96b4ecb298eba6ac.png\" id=\"book-cover\"/>\n\n이 책의 코드는 주피터 노트북으로 제공하며 구글 코랩에서 테스트했습니다. 주피터 노트북마다 구글 코랩에서 실행할 수 있는 링크가 포함되어 있습니다.\n\n로컬 컴퓨터에서 실행하려면 저장소를 클론하고 파이썬 가상 환경이나 콘다 환경을 만들어 실행하세요. 이 책의 코드는 라이브러리 버전에 따라 실행 결과가 달라질 수 있으므로 로컬에서 실행하는 경우 구글 코랩의 라이브러리 버전을 참고하세요.\n"
    },
    {
      "name": "jlonge4/mychatGPT",
      "stars": 139,
      "img": "https://avatars.githubusercontent.com/u/91354480?s=40&v=4",
      "owner": "jlonge4",
      "repo_name": "mychatGPT",
      "description": "GPT chat with your docs!",
      "homepage": "",
      "language": "Python",
      "created_at": "2023-05-06T14:40:00Z",
      "updated_at": "2025-03-01T00:18:41Z",
      "topics": [
        "agents",
        "chatgpt",
        "haystack-ai",
        "machinelearning",
        "openai",
        "python",
        "rag"
      ],
      "readme": "# RAG Agent: Document Assistant 🚀🤖\n\nThis project has evolved from a simple PDF chatbot to a sophisticated RAG (Retrieval-Augmented Generation) agent capable of accessing conversation history, retrieving context, summarizing documents, and answering follow-up questions based on user queries and intent detection. \n\nTry it out yourself at [agentic-rag](https://mychatgpt-p4vd2e8hpxeqwk5y8z52u2.streamlit.app/)\n\nScreenshot:\n  ![Example Conversation](examples/agent_rag.png)\n\n## Installation\n\nTo set up this project, you'll first need your OPENAI_API key. Next, ensure you have the necessary dependencies installed. You can install them via:\n\n`pip install -r requirements.txt`\n\n## Usage\n\nTo launch the RAG agent, run the command:\n\n`python -m streamlit run agentic_rag.py`\n\n\nThis will start the Streamlit app in your browser. Once launched, you can upload your `.pdf, .txt, .docx` docs and start querying naturally for summaries, in depth queries, and follow up questions!\n\n## Features\n\n- **Conversation History**: The RAG agent can access conversation history to maintain context and provide more relevant responses.\n- **Document Summarization**: It can summarize documents to provide concise answers or overviews.\n- **Follow-up Answers**: The agent can answer follow-up questions based on previous interactions and the current conversation context.\n- **Logical Intent Determination**: It uses logic to determine user intent, ensuring accurate responses.\n- **Supported File Types**: Users can upload and select `.pdf, .txt, .docx` documents directly within the app interface.\n\n## History\n\nThis project is an ongoing effort to enhance the capabilities of the RAG agent. While the functionality is currently demonstrated in the provided .py file, further developments are in progress. Stay tuned for updates as I transition this project into a fully functional application that enables local execution and PDF selection through a user-friendly interface.\n\n## Credits\n\nYour's truly 😅\n\n## License\n\nApache2.0\n\nEnjoy interacting with your documents through this intelligent RAG Agent!\n\n"
    },
    {
      "name": "li-ronghui/FineDance",
      "stars": 138,
      "img": "https://avatars.githubusercontent.com/u/20925701?s=40&v=4",
      "owner": "li-ronghui",
      "repo_name": "FineDance",
      "description": "FineDance: A Fine-grained Choreography Dataset for 3D Full Body Dance Generation. (ICCV2023)",
      "homepage": "",
      "language": "Python",
      "created_at": "2023-08-15T05:47:07Z",
      "updated_at": "2025-04-22T12:42:34Z",
      "topics": [],
      "readme": "# [FineDance: A Fine-grained Choreography Dataset for 3D Full Body Dance Generation (ICCV 2023)](https://github.com/li-ronghui/FineDance)\n\n[[Project Page](https://li-ronghui.github.io/finedance)] | [[Preprint](https://arxiv.org/abs/2212.03741)] | [[pdf](https://arxiv.org/pdf/2212.03741.pdf)] | [[video](https://li-ronghui.github.io/finedance)]\n\n\n### Teaser\n\n<img src=\"teaser/teaser.png\">\n\n### Download the FineDance Dataset\n\nThe part(7.7 hours) of FineDance dataset can be downloaded at [Google Drive](https://drive.google.com/file/d/1zQvWG9I0H4U3Zrm8d_QD_ehenZvqfQfS/view?usp=sharing) or [百度云](https://pan.baidu.com/s/1gynUC7pMdpsE31wAwq177w?pwd=o9pw).\n\n\n### Dataset Descriptions\nPut the downloaded FineDance data into './data'. \n\nThe data directory is organized as follows:\n\nlabel_json: contains the song name, coarse style and fine-grained genre.\n\nmotion: contains the [SMPLH](https://smpl-x.is.tue.mpg.de/) format motion data.   \n\nmusic_wav: contains the music data in 'wav' format.\n\nmusic_npy: contains the music feature extracted by [librosa](https://github.com/librosa/librosa) follow [AIST++](https://github.com/google/aistplusplus_api/tree/main)\n\nHere is an example python script to read the motion file\n```python\nimport numpy as np\ndata = np.load(\"motion/001.npy\")\nT, C = data.shape           # T is the number of frames\nsmpl_poses = data[:, 3:]\nsmpl_trans = data[:, :3]\n```\n\n\n### Prepare\nPlease Download the pretrained checkpoints and other needed assets files from [Google Drive](https://drive.google.com/file/d/1ENoeUn-X-3Vw2Gon-voVLlndy3hZXdWD/view?usp=drive_link).\n\nInstall the conda enviroment.\n\n```python\nconda env create -f environment.yaml\nconda activate FineNet\n```\n\n### Data preprocessing\n```python\npython data/code/pre_motion.py\n```\n\n\n### Training\n```python\naccelerate launch train_seq.py\n```\n\n### Generate\n\n```python\npython data/code/slice_music_motion.py\npython generate_all.py  --motion_save_dir generated/finedance_seq_120_dancer --save_motions\n```\n\n### Generate dance by custom music\n```python\npython test.py --music_dir 'your music dir' --save_motions\n```\n\n### Visualization\n```python\npython render.py --modir eval/motions --gpu 0\n```\n\n\n### Dataset split\n\nWe spilt FineDance dataset into train, val and test sets in two ways: FineDance@Genre and  FineDance@Dancer. Each music and paired dance are only present in one split. \n\n1. The test set of FineDance@Genre includes a broader range of dance genres, but the same dancer appear in  train/val/test set. Although the training set and test set include the same dancers, the same motions do not appear in both the training and testing sets. This is because these dancers do not have distinct personal characteristics in their dances.\n2. The train/val/test set of FineDance@Dancer was divided by different dancers, which test set contains fewer dance genres, yet the same dancer won't appear in different sets.\n\nIf you use this dataset for dance generation, we recommend you to use the split of FineDance@Genre.\n\n## Acknowledgments\nWe would like to express our sincere gratitude to Dr [Yan Zhang](https://yz-cnsdqz.github.io/) and [Yulun Zhang](https://yulunzhang.com/) for their invaluable guidance and insights during the course of our research.\n\nThis code is standing on the shoulders of giants. We want to thank the following contributors that our code is based on:\n[EDGE](https://github.com/Stanford-TML/EDGE/tree/main),[MDM](https://github.com/Stanford-TML/EDGE/tree/main),[Adan](https://github.com/lucidrains/Adan-pytorch),[Diffusion](https://github.com/lucidrains/denoising-diffusion-pytorch),[SMPLX](https://smpl-x.is.tue.mpg.de/).\n\n## Citation\nWhen using the code/figures/data/video/etc., please cite our work\n```\n@inproceedings{li2023finedance,\n  title={FineDance: A Fine-grained Choreography Dataset for 3D Full Body Dance Generation},\n  author={Li, Ronghui and Zhao, Junfan and Zhang, Yachao and Su, Mingyang and Ren, Zeping and Zhang, Han and Tang, Yansong and Li, Xiu},\n  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},\n  pages={10234--10243},\n  year={2023}\n}\n```\n"
    },
    {
      "name": "opea-project/GenAIComps",
      "stars": 137,
      "img": "https://avatars.githubusercontent.com/u/165422283?s=40&v=4",
      "owner": "opea-project",
      "repo_name": "GenAIComps",
      "description": "GenAI components at micro-service level; GenAI service composer to create mega-service",
      "homepage": "",
      "language": "Python",
      "created_at": "2024-04-19T10:43:47Z",
      "updated_at": "2025-04-23T06:42:51Z",
      "topics": [],
      "readme": "# Generative AI Components (GenAIComps)\n\n**Build Enterprise-grade Generative AI Applications with Microservice Architecture**\n\nThis initiative empowers the development of high-quality Generative AI applications for enterprises via microservices, simplifying the scaling and deployment process for production. It abstracts away infrastructure complexities, facilitating the seamless development and deployment of Enterprise AI services.\n\n## GenAIComps\n\nGenAIComps provides a suite of microservices, leveraging a service composer to assemble a mega-service tailored for real-world Enterprise AI applications. All the microservices are containerized, allowing cloud native deployment. Checkout how the microservices are used in [GenAIExamples](https://github.com/opea-project/GenAIExamples).\n\n![Architecture](https://i.imgur.com/r5J0i8j.png)\n\n### Installation\n\n- Install from Pypi\n\n```bash\npip install opea-comps\n```\n\n- Build from Source\n\n```bash\ngit clone https://github.com/opea-project/GenAIComps\ncd GenAIComps\npip install -e .\n```\n\n## MicroService\n\n`Microservices` are akin to building blocks, offering the fundamental services for constructing `RAG (Retrieval-Augmented Generation)` and other Enterprise AI applications.\n\nEach `Microservice` is designed to perform a specific function or task within the application architecture. By breaking down the system into smaller, self-contained services, `Microservices` promote modularity, flexibility, and scalability.\n\nThis modular approach allows developers to independently develop, deploy, and scale individual components of the application, making it easier to maintain and evolve over time. Additionally, `Microservices` facilitate fault isolation, as issues in one service are less likely to impact the entire system.\n\nThe initially supported `Microservices` are described in the below table. More `Microservices` are on the way.\n\n| MicroService                                      | Framework                                                                      | Model                                                                                                   | Serving                                                         | HW     | Description                           |\n| ------------------------------------------------- | ------------------------------------------------------------------------------ | ------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------- | ------ | ------------------------------------- |\n| [Embedding](./comps/embeddings/src/README.md)     | [LangChain](https://www.langchain.com)/[LlamaIndex](https://www.llamaindex.ai) | [BAAI/bge-base-en-v1.5](https://huggingface.co/BAAI/bge-base-en-v1.5)                                   | [TEI-Gaudi](https://github.com/huggingface/tei-gaudi)           | Gaudi2 | Embedding on Gaudi2                   |\n| [Embedding](./comps/embeddings/src/README.md)     | [LangChain](https://www.langchain.com)/[LlamaIndex](https://www.llamaindex.ai) | [BAAI/bge-base-en-v1.5](https://huggingface.co/BAAI/bge-base-en-v1.5)                                   | [TEI](https://github.com/huggingface/text-embeddings-inference) | Xeon   | Embedding on Xeon CPU                 |\n| [Retriever](./comps/retrievers/README.md)         | [LangChain](https://www.langchain.com)/[LlamaIndex](https://www.llamaindex.ai) | [BAAI/bge-base-en-v1.5](https://huggingface.co/BAAI/bge-base-en-v1.5)                                   | [TEI](https://github.com/huggingface/text-embeddings-inference) | Xeon   | Retriever on Xeon CPU                 |\n| [Reranking](./comps/rerankings/src/README.md)     | [LangChain](https://www.langchain.com)/[LlamaIndex](https://www.llamaindex.ai) | [BAAI/bge-reranker-base](https://huggingface.co/BAAI/bge-reranker-base)                                 | [TEI-Gaudi](https://github.com/huggingface/tei-gaudi)           | Gaudi2 | Reranking on Gaudi2                   |\n| [Reranking](./comps/rerankings/src/README.md)     | [LangChain](https://www.langchain.com)/[LlamaIndex](https://www.llamaindex.ai) | [BBAAI/bge-reranker-base](https://huggingface.co/BAAI/bge-reranker-base)                                | [TEI](https://github.com/huggingface/text-embeddings-inference) | Xeon   | Reranking on Xeon CPU                 |\n| [ASR](./comps/asr/src/README.md)                  | NA                                                                             | [openai/whisper-small](https://huggingface.co/openai/whisper-small)                                     | NA                                                              | Gaudi2 | Audio-Speech-Recognition on Gaudi2    |\n| [ASR](./comps/asr/src/README.md)                  | NA                                                                             | [openai/whisper-small](https://huggingface.co/openai/whisper-small)                                     | NA                                                              | Xeon   | Audio-Speech-RecognitionS on Xeon CPU |\n| [TTS](./comps/tts/src/README.md)                  | NA                                                                             | [microsoft/speecht5_tts](https://huggingface.co/microsoft/speecht5_tts)                                 | NA                                                              | Gaudi2 | Text-To-Speech on Gaudi2              |\n| [TTS](./comps/tts/src/README.md)                  | NA                                                                             | [microsoft/speecht5_tts](https://huggingface.co/microsoft/speecht5_tts)                                 | NA                                                              | Xeon   | Text-To-Speech on Xeon CPU            |\n| [Dataprep](./comps/dataprep/README.md)            | [Qdrant](https://qdrant.tech/)                                                 | [sentence-transformers/all-MiniLM-L6-v2](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2) | NA                                                              | Gaudi2 | Dataprep on Gaudi2                    |\n| [Dataprep](./comps/dataprep/README.md)            | [Qdrant](https://qdrant.tech/)                                                 | [sentence-transformers/all-MiniLM-L6-v2](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2) | NA                                                              | Xeon   | Dataprep on Xeon CPU                  |\n| [Dataprep](./comps/dataprep/README.md)            | [Redis](https://redis.io/)                                                     | [BAAI/bge-base-en-v1.5](https://huggingface.co/BAAI/bge-base-en-v1.5)                                   | NA                                                              | Gaudi2 | Dataprep on Gaudi2                    |\n| [Dataprep](./comps/dataprep/README.md)            | [Redis](https://redis.io/)                                                     | [BAAI/bge-base-en-v1.5](https://huggingface.co/BAAI/bge-base-en-v1.5)                                   | NA                                                              | Xeon   | Dataprep on Xeon CPU                  |\n| [LLM](./comps/llms/src/text-generation/README.md) | [LangChain](https://www.langchain.com)/[LlamaIndex](https://www.llamaindex.ai) | [Intel/neural-chat-7b-v3-3](https://huggingface.co/Intel/neural-chat-7b-v3-3)                           | [TGI Gaudi](https://github.com/huggingface/tgi-gaudi)           | Gaudi2 | LLM on Gaudi2                         |\n| [LLM](./comps/llms/src/text-generation/README.md) | [LangChain](https://www.langchain.com)/[LlamaIndex](https://www.llamaindex.ai) | [Intel/neural-chat-7b-v3-3](https://huggingface.co/Intel/neural-chat-7b-v3-3)                           | [TGI](https://github.com/huggingface/text-generation-inference) | Xeon   | LLM on Xeon CPU                       |\n| [LLM](./comps/llms/src/text-generation/README.md) | [LangChain](https://www.langchain.com)/[LlamaIndex](https://www.llamaindex.ai) | [Intel/neural-chat-7b-v3-3](https://huggingface.co/Intel/neural-chat-7b-v3-3)                           | [Ray Serve](https://github.com/ray-project/ray)                 | Gaudi2 | LLM on Gaudi2                         |\n| [LLM](./comps/llms/src/text-generation/README.md) | [LangChain](https://www.langchain.com)/[LlamaIndex](https://www.llamaindex.ai) | [Intel/neural-chat-7b-v3-3](https://huggingface.co/Intel/neural-chat-7b-v3-3)                           | [Ray Serve](https://github.com/ray-project/ray)                 | Xeon   | LLM on Xeon CPU                       |\n| [LLM](./comps/llms/src/text-generation/README.md) | [LangChain](https://www.langchain.com)/[LlamaIndex](https://www.llamaindex.ai) | [Intel/neural-chat-7b-v3-3](https://huggingface.co/Intel/neural-chat-7b-v3-3)                           | [vLLM](https://github.com/vllm-project/vllm/)                   | Gaudi2 | LLM on Gaudi2                         |\n| [LLM](./comps/llms/src/text-generation/README.md) | [LangChain](https://www.langchain.com)/[LlamaIndex](https://www.llamaindex.ai) | [Intel/neural-chat-7b-v3-3](https://huggingface.co/Intel/neural-chat-7b-v3-3)                           | [vLLM](https://github.com/vllm-project/vllm/)                   | Xeon   | LLM on Xeon CPU                       |\n\nA `Microservices` can be created by using the decorator `register_microservice`. Taking the `embedding microservice` as an example:\n\n```python\nfrom comps import register_microservice, EmbedDoc, ServiceType, TextDoc\n\n\n@register_microservice(\n    name=\"opea_service@embedding_tgi_gaudi\",\n    service_type=ServiceType.EMBEDDING,\n    endpoint=\"/v1/embeddings\",\n    host=\"0.0.0.0\",\n    port=6000,\n    input_datatype=TextDoc,\n    output_datatype=EmbedDoc,\n)\ndef embedding(input: TextDoc) -> EmbedDoc:\n    embed_vector = embeddings.embed_query(input.text)\n    res = EmbedDoc(text=input.text, embedding=embed_vector)\n    return res\n```\n\n## MegaService\n\nA `Megaservice` is a higher-level architectural construct composed of one or more `Microservices`, providing the capability to assemble end-to-end applications. Unlike individual `Microservices`, which focus on specific tasks or functions, a `Megaservice` orchestrates multiple `Microservices` to deliver a comprehensive solution.\n\n`Megaservices` encapsulate complex business logic and workflow orchestration, coordinating the interactions between various `Microservices` to fulfill specific application requirements. This approach enables the creation of modular yet integrated applications, where each `Microservice` contributes to the overall functionality of the `Megaservice`.\n\nHere is a simple example of building `Megaservice`:\n\n```python\nfrom comps import MicroService, ServiceOrchestrator\n\nEMBEDDING_SERVICE_HOST_IP = os.getenv(\"EMBEDDING_SERVICE_HOST_IP\", \"0.0.0.0\")\nEMBEDDING_SERVICE_PORT = os.getenv(\"EMBEDDING_SERVICE_PORT\", 6000)\nLLM_SERVICE_HOST_IP = os.getenv(\"LLM_SERVICE_HOST_IP\", \"0.0.0.0\")\nLLM_SERVICE_PORT = os.getenv(\"LLM_SERVICE_PORT\", 9000)\n\n\nclass ExampleService:\n    def __init__(self, host=\"0.0.0.0\", port=8000):\n        self.host = host\n        self.port = port\n        self.megaservice = ServiceOrchestrator()\n\n    def add_remote_service(self):\n        embedding = MicroService(\n            name=\"embedding\",\n            host=EMBEDDING_SERVICE_HOST_IP,\n            port=EMBEDDING_SERVICE_PORT,\n            endpoint=\"/v1/embeddings\",\n            use_remote_service=True,\n            service_type=ServiceType.EMBEDDING,\n        )\n        llm = MicroService(\n            name=\"llm\",\n            host=LLM_SERVICE_HOST_IP,\n            port=LLM_SERVICE_PORT,\n            endpoint=\"/v1/chat/completions\",\n            use_remote_service=True,\n            service_type=ServiceType.LLM,\n        )\n        self.megaservice.add(embedding).add(llm)\n        self.megaservice.flow_to(embedding, llm)\n```\n\nself.gateway = ChatQnAGateway(megaservice=self.megaservice, host=\"0.0.0.0\", port=self.port)\n\n````\n\n## Check Mega/Micro Service health status and version number\n\nUse below command to check Mega/Micro Service status.\n\n```bash\ncurl http://${your_ip}:${service_port}/v1/health_check\\\n  -X GET \\\n  -H 'Content-Type: application/json'\n````\n\nUsers should get output like below example if Mega/Micro Service works correctly.\n\n```bash\n{\"Service Title\":\"ChatQnAGateway/MicroService\",\"Version\":\"1.0\",\"Service Description\":\"OPEA Microservice Infrastructure\"}\n```\n\n## Contributing to OPEA\n\nWelcome to the OPEA open-source community! We are thrilled to have you here and excited about the potential contributions you can bring to the OPEA platform. Whether you are fixing bugs, adding new GenAI components, improving documentation, or sharing your unique use cases, your contributions are invaluable.\n\nTogether, we can make OPEA the go-to platform for enterprise AI solutions. Let's work together to push the boundaries of what's possible and create a future where AI is accessible, efficient, and impactful for everyone.\n\nPlease check the [Contributing guidelines](https://github.com/opea-project/docs/tree/main/community/CONTRIBUTING.md) for a detailed guide on how to contribute a GenAI example and all the ways you can contribute!\n\nThank you for being a part of this journey. We can't wait to see what we can achieve together!\n\n## Additional Content\n\n- [Code of Conduct](https://github.com/opea-project/docs/tree/main/community/CODE_OF_CONDUCT.md)\n- [Security Policy](https://github.com/opea-project/docs/tree/main/community/SECURITY.md)\n- [Legal Information](LEGAL_INFORMATION.md)\n"
    },
    {
      "name": "aws-samples/semantic-search-aws-docs",
      "stars": 125,
      "img": "https://avatars.githubusercontent.com/u/8931462?s=40&v=4",
      "owner": "aws-samples",
      "repo_name": "semantic-search-aws-docs",
      "description": null,
      "homepage": null,
      "language": "HCL",
      "created_at": "2022-08-09T07:26:40Z",
      "updated_at": "2024-10-21T07:11:54Z",
      "topics": [],
      "readme": "# Semantic Search on AWS Docs or Custom Documents\n\nThis sample project demonstrates how to set up AWS infrastructure to perform semantic search and [question answering](https://en.wikipedia.org/wiki/Question_answering) on documents using a transformer machine learning models like BERT, RoBERTa, or GPT (via the [Haystack](https://github.com/deepset-ai/haystack) open source framework).\n\nAs an example, users can type questions about AWS services and find answers from the AWS documentation or custom local documents.\n\nThe deployed solution support 2 answering styles:\n- `extractive question answering` will find the semantically closest\ndocuments to the questions and highlight the most likeliest answer(s) in these documents.\n- `generative question answering`, also referred to as long form question answering (LFQA), will find the semantically closest documents to the question and generate a formulated answer.\n\nPlease note that this project is intended for demo purposes, see disclaimers below.\n\n![](appdemo.gif?raw=true)\n\n## Architecture\n\n![](semantic-search-arch-application.png?raw=true)\n\nThe main components of this project are:\n\n* [Amazon OpenSearch Service](https://docs.aws.amazon.com/opensearch-service/latest/developerguide/what-is.html) to store and search documents\n* The [AWS Documentation](https://github.com/awsdocs/) as a sample dataset loaded in the document store\n* The [Haystack framework](https://www.deepset.ai/haystack) to set up an extractive [Question Answering pipeline](https://haystack.deepset.ai/tutorials/first-qa-system) with:\n    * A [Retriever](https://haystack.deepset.ai/pipeline_nodes/retriever) that searches all the documents and returns only the most relevant ones\n        * Retriever used: [sentence-transformers/all-mpnet-base-v2](https://huggingface.co/sentence-transformers/all-mpnet-base-v2)\n    * A [Reader](https://haystack.deepset.ai/pipeline_nodes/reader) that uses the documents returned by the Retriever and selects a text span which is likely to contain the matching answer to the query\n        * Reader used: [deepset/roberta-base-squad2](https://huggingface.co/deepset/roberta-base-squad2)\n* [Streamlit](https://streamlit.io/) to set up a frontend\n* [Terraform](https://www.terraform.io/) to automate the infrastructure deployment on AWS\n\n## How to deploy the solution\n\n### Deploy with AWS Cloud9\nFollow our [step-by-step deployment instructions](documentation/aws-cloud9-deployment.md) to deploy the semantic search application if you are new to AWS, Terraform, semantic search, or you prefer detailed setp-by-step instructions.\n\nFor more general deployment instructions follow the sections below.\n\n### General Deployment Instructions \nThe backend folder contains a Terraform project that deploys an OpenSearch domain and 2 ECS services:\n\n* frontend: Streamlit-based UI built by Haystack ([repo](https://github.com/deepset-ai/haystack-demos/tree/main/explore_the_world))\n* search API: REST API built by Haystack\n\nThe main steps to deploy the solution are:\n\n* Deploy the terraform stack\n* Optional: Ingest the AWS documentation\n\n#### Pre-requisites\n\n* Terraform v1.0+ ([getting started guide](https://learn.hashicorp.com/collections/terraform/aws-get-started))\n* Docker installed and running ([getting started guide](https://www.docker.com/get-started/))\n* AWS CLI v2 installed and configured ([getting started guide](https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html))\n* An [EC2 Service Limit of at least 8 cores for G-instance type](https://aws.amazon.com/premiumsupport/knowledge-center/ec2-instance-limit/) if you want to deploy this solution with GPU acceleration.  \nAlternatively, you can switch to a CPU instance by changing the `instance_type = \"g4dn.2xlarge\"` to a CPU instance in the `infrastructure/main.tf` file.\n\n#### Deploy the application infrastructure terraform stack\n\n* git clone this repository\n* **Configure**\nConfigure and change the infrastructure region, subnets, availability zones in the `infrastructure/terraform.tfvars` file as needed\n* **Initialize**  \nIn this example the Terrform state is stored remotely and managed through a backend using S3 and a dynamodb table to acquire the state lock. This allows collaboration on the same Terraform infrastructure from different machines.\n( If you prefer to use local state instead just remove the `terraform { backend \"s3\" { ...}}` block from the `infrastructure/tf-backend.tf` file and run directly `terraform init`)\n    * Create an S3 Bucket and DynamoDB to store the Terraform [state backend](https://www.terraform.io/language/settings/backends/s3) in a region of choice.\n      ```shell\n      STATE_REGION=<AWS region>\n      ```\n      ```shell\n      S3_BUCKET=<YOUR-BUCKET-NAME>\n      aws s3 mb s3://$S3_BUCKET -region=$STATE_REGION\n      ```\n      ```shell\n      SYNC_TABLE=<YOUR-TABLE-NAME>\n      aws dynamodb create-table --table-name $SYNC_TABLE --attribute-definitions AttributeName=LockID,AttributeType=S --key-schema   AttributeName=LockID,KeyType=HASH --billing-mode PAY_PER_REQUEST --region=$STATE_REGION\n      ```\n   * Change to the directory containing the application infrastucture's `infrastructure/main.tf` file\n      ```shell\n      cd infrastructure\n      ```\n   * Initialize terraform with the S3 remote state backend by running\n      ```shell\n      terraform init \\\n      -backend-config=\"bucket=$S3_BUCKET\" \\\n      -backend-config=\"region=$STATE_REGION\" \\\n      -backend-config=\"dynamodb_table=$SYNC_TABLE\"\n      ```\n\n* **Deploy**  \nRun terraform deploy and approve changes by typing yes.\n    ```shell\n    terraform apply\n    ```    \n    ***Please note:*** _deployment can take a long time to push the container depending on the upload bandwidth of your machine.  \n    For faster deployment you can run the terraform deployment from a development environment hosted inside the same AWS region, for example by using the [AWS Cloud9](https://aws.amazon.com/cloud9/) IDE._\n* **Use**  \nOnce deployment is completed, browse to the output URL (`loadbalancer_url`) from the Terraform output to see the appliction.   \nHowever, searches won't return any results until you ingest any documents.\n* **Clean up**  \n  To remove all created resources of the applications infrastructure again use\n  ```shell\n  terraform destroy\n  ```\n  (If you used the ingestion terrform below, make sure to first destroy the ingestion resources to avoid conflicts)\n\n#### Ingest the AWS documentation\n\nThis second terraform stack builds, pushes and runs a docker container as an ECS task.  \nThe ingestion container downloads either a single (e.g. `amazon-ec2-user-guide`) or all awsdocs repos (256) (`full`) and converts the .md files into .txt using pandoc.  \nThe .txt documents are then being ingested into the applications OpenSearch cluster in the required haystack format and become available for search\n\n![](semantic-search-arch-ingestion.png?raw=true)\n\n* Change from the `infrastructure` directory to the directory containing the ingestion's `ingestion/main.tf`\n   ```shell\n    cd ../ingestion\n    ```\n* Init terraform  \n(here we are using local state instead of a remote S3 backend for simplicity)\n    ```shell\n    terraform init\n    ```\n* Run ingestion as Terraform deployment.  \nThe S3 remote state file from the previous infrastructure deployment is needed here as input variables.  \nIt is used as data source to read out the infra's output variables like the OpenSearch endpoint or private subnets. \nYou can set the S3 bucket and its region either in the `infrastructure/terraform.tfvars` or passing the input variables via\n    ```shell\n    terraform apply \\\n    -var=\"infra_region=$STATE_REGION\" \\\n    -var=\"infra_tf_state_s3_bucket=$S3_BUCKET\"\n    ```   \n    ***Please note:*** _deployment can take a long time to push the container depending on the upload bandwidth of your machine. For faster deployment you can build and push the container in AWS, for example by using the [AWS Cloud9](https://aws.amazon.com/cloud9/) IDE._\n* Once the previous step finsihes, the ECS ingestion task is started. You can check its progress in the AWS console, for example in Amazon CloudWatch under the log group name `semantic-search` and checking `ingestion-job`. After the task finsihed successfully, the ingested documents are searchable via the application.\n* After runing the ingestion job, you can remove the created ingestion resources, e.g. ECR repository or task definition by running\n    ```shell\n    terraform destroy \\\n    -var=\"infra_region=$STATE_REGION\" \\\n    -var=\"infra_tf_state_s3_bucket=$S3_BUCKET\"\n    ```\n\n#### Ingesting your own documents\n\nTake a look at the `ingestion/awsdocs/ingest.py` how adopt the ingestion script for your own documents. In brief, you can ingest local or downloaded files via:\n```python\n# Create a wrapper for the existing OpenSearch document store\ndocument_store = OpenSearchDocumentStore(...)\n\n# Covert local files\ndicts_aws = convert_files_to_docs(dir_path=..., ...)\n\n# Write the documents to the OpenSearch document store\ndocument_store.write_documents(dicts_aws, index=...)\n\n# Compute and update the embeddings for each document with a transformer ML model. \n# An embedding is the vector representation that is learned by the transformer and that\n# allows us to capture and compare the semantic meaning of documents via this \n# vector representation. \n# Be sure to use the same model that you want to use later in the search pipeline.\nretriever = EmbeddingRetriever(\n    document_store=document_store,\n    model_format = \"sentence_transformers\",\n    embedding_model = \"all-mpnet-base-v2\"\n)\ndocument_store.update_embeddings(retriever)\n```\n\n## Security\n\nSee [CONTRIBUTING](CONTRIBUTING.md#security-issue-notifications) for more information.\n\n## Contributing\n\nIf you want to contribute to Haystack, check out their [GitHub repository](https://github.com/deepset-ai/haystack).\n\n## License\n\nThis library is licensed under the MIT-0 License. See the LICENSE file.\n\n## Disclaimer\n\nThis solution is intended to demonstrate the functionality of using machine learning models for semantic search and question answering. They are not intended for production deployment as is.\n\nFor best practices on modifying this solution for production use cases, please follow the [AWS well-architected guidance](https://aws.amazon.com/architecture/well-architected/).\n"
    },
    {
      "name": "teremterem/MiniAgents",
      "stars": 125,
      "img": "https://avatars.githubusercontent.com/u/5508002?s=40&v=4",
      "owner": "teremterem",
      "repo_name": "MiniAgents",
      "description": "An async-first framework for building multi-agent AI systems with an innovative approach to parallelism, so you can focus on creating intelligent agents, not on managing the concurrency of your flows.",
      "homepage": "",
      "language": "Python",
      "created_at": "2024-02-28T17:17:54Z",
      "updated_at": "2025-04-20T15:25:09Z",
      "topics": [],
      "readme": "<h1 align=\"center\">🛰 MiniAgents 🌘</h1>\n\n<p align=\"center\">\n    <a href=\"https://discord.gg/ptSvVnbwKt\">\n        <img alt=\"Discord\"\n            src=\"https://img.shields.io/discord/1356683647926796398?logo=discord&color=darkviolet\">\n    </a>\n    <a href=\"https://github.com/teremterem/MiniAgents/blob/main/LICENSE\">\n        <img alt=\"License: MIT\"\n            src=\"https://img.shields.io/badge/license-MIT-blue\">\n    </a>\n    <a href=\"https://www.python.org/downloads/\">\n        <img alt=\"Python: 3.9+\"\n            src=\"https://img.shields.io/badge/python-3.9+-olive\">\n    </a>\n    <a href=\"https://pypi.org/project/miniagents/\">\n        <img alt=\"PyPI: Latest\"\n            src=\"https://img.shields.io/pypi/v/miniagents?color=mediumseagreen\">\n    </a>\n    <a href=\"https://github.com/pylint-dev/pylint\">\n        <img alt=\"Linting: Pylint\"\n            src=\"https://img.shields.io/badge/linting-pylint-lightgray\">\n    </a>\n    <a href=\"https://github.com/psf/black\">\n        <img alt=\"Code Style: Black\"\n            src=\"https://img.shields.io/badge/code%20style-black-black\">\n    </a>\n</p>\n\n<p align=\"center\">\n    <img alt=\"MiniAgents on the Moon\"\n        src=\"https://github.com/teremterem/MiniAgents/raw/main/images/banner-miniagents-2025-04-19.jpeg\">\n</p>\n\nMiniAgents is an open-source Python framework that takes the complexity out of building multi-agent AI systems. With its innovative approach to parallelism and async-first design, you can focus on creating intelligent agents in an easy to follow procedural fashion while the framework handles the concurrency challenges for you.\n\nBuilt on top of asyncio, MiniAgents provides a robust foundation for LLM-based applications with immutable, Pydantic-based messages and seamless asynchronous token and message streaming between agents.\n\n## 💾 Installation\n\n```bash\npip install -U miniagents\n```\n\n## ⚠️ IMPORTANT: START HERE FIRST! ⚠️\n\n**We STRONGLY RECOMMEND checking this tutorial before you proceed with the README:**\n\n### [📚 Building a Web Research Multi-Agent System](https://app.readytensor.ai/publications/miniagents-multi-agent-ai-with-procedural-simplicity-sZ9xgmyLOTyp)\n\nThe above step-by-step tutorial teaches you how to build a practical multi-agent web research system that can break down complex questions, run parallel searches, and synthesize comprehensive answers.\n\n*Following that tutorial first will make the rest of this README easier to understand!*\n\n## 🚀 Basic usage\n\nHere's a simple example of how to define an agent:\n\n```python\nfrom miniagents import miniagent, InteractionContext, MiniAgents\n\n\n@miniagent\nasync def my_agent(ctx: InteractionContext) -> None:\n    async for msg_promise in ctx.message_promises:\n        ctx.reply(f\"You said: {await msg_promise}\")\n\n\nasync def main() -> None:\n    async for msg_promise in my_agent.trigger([\"Hello\", \"World\"]):\n        print(await msg_promise)\n\n\nif __name__ == \"__main__\":\n    MiniAgents().run(main())\n```\n\nThis script will print the following lines to the console:\n\n```\nYou said: Hello\nYou said: World\n```\n\n### 🧨 Exception handling\n\nDespite agents running in completely detached asyncio tasks, MiniAgents ensures proper exception propagation from callee agents to caller agents. When an exception occurs in a callee agent, it's captured and propagated through the promises of response message sequences. These exceptions are re-raised when those sequences are iterated over or awaited in any of the caller agents, ensuring that errors are not silently swallowed and can be properly handled.\n\nHere's a simple example showing exception propagation:\n\n```python\nfrom miniagents import miniagent, InteractionContext, MiniAgents\n\n@miniagent\nasync def faulty_agent(ctx: InteractionContext) -> None:\n    # This agent will raise an exception\n    raise ValueError(\"Something went wrong in callee agent\")\n\n@miniagent\nasync def caller_agent(ctx: InteractionContext) -> None:\n    # The exception from faulty_agent WILL NOT propagate here\n    faulty_response_promises = faulty_agent.trigger(\"Hello\")\n    try:\n        # The exception from faulty_agent WILL propagate here\n        async for msg_promise in faulty_response_promises:\n            await msg_promise\n    except ValueError as e:\n        ctx.reply(f\"Exception while iterating over response: {e}\")\n\nasync def main() -> None:\n    async for msg_promise in caller_agent.trigger(\"Start\"):\n        print(await msg_promise)\n\nif __name__ == \"__main__\":\n    MiniAgents().run(main())\n```\n\nOutput:\n```\nException while iterating over response: Something went wrong in callee agent\n```\n\n### 🧠 Work with LLMs\n\nMiniAgents provides built-in support for OpenAI and Anthropic language models with possibility to add other integrations.\n\n⚠️ **ATTENTION!** Make sure to run `pip install -U openai` and set your OpenAI API key in the `OPENAI_API_KEY` environment variable before running the example below. ⚠️\n\n```python\nfrom miniagents import MiniAgents\nfrom miniagents.ext.llms import OpenAIAgent\n\n# NOTE: \"Forking\" an agent is a convenient way of creating a new agent instance\n# with the specified configuration. Alternatively, you could pass the `model`\n# parameter to `OpenAIAgent.trigger()` directly everytime you talk to the\n# agent.\ngpt_4o_agent = OpenAIAgent.fork(model=\"gpt-4o-mini\")\n\n\nasync def main() -> None:\n    reply_sequence = gpt_4o_agent.trigger(\n        \"Hello, how are you?\",\n        system=\"You are a helpful assistant.\",\n        max_tokens=50,\n        temperature=0.7,\n    )\n    async for msg_promise in reply_sequence:\n        async for token in msg_promise:\n            print(token, end=\"\", flush=True)\n        # MINOR: Let's separate messages with a double newline (even though in\n        # this particular case we are actually going to receive only one\n        # message).\n        print(\"\\n\")\n\n\nif __name__ == \"__main__\":\n    MiniAgents().run(main())\n```\n\nEven though OpenAI models return a single assistant response, the `OpenAIAgent.trigger()` method is still designed to return a sequence of multiple message promises. This generalizes to arbitrary agents, making agents in the MiniAgents framework easily interchangeable (agents in this framework support sending and receiving zero or more messages).\n\nYou can read agent responses token-by-token as shown above regardless of whether the agent is streaming token by token or returning full messages. The complete message content will just be returned as a single \"token\" in the latter case.\n\n## 🔄 A dialog loop between a user and an AI assistant\n\nThe `dialog_loop` agent is a pre-packaged agent that implements a dialog loop between a user agent and an assistant agent. Here is how you can use it to set up an interaction between a user and your agent (can be bare LLM agent, like `OpenAIAgent` or `AnthropicAgent`, can also be a custom agent that you define yourself - a more complex agent that uses LLM agents under the hood but also introduces more complex behavior, i.e. Retrieval Augmented Generation etc.):\n\n⚠️ **ATTENTION!** Make sure to run `pip install -U openai` and set your OpenAI API key in the `OPENAI_API_KEY` environment variable before running the example below. ⚠️\n\n```python\nfrom miniagents import MiniAgents\nfrom miniagents.ext import (\n    dialog_loop,\n    console_user_agent,\n    MarkdownHistoryAgent,\n)\nfrom miniagents.ext.llms import SystemMessage, OpenAIAgent\n\n\nasync def main() -> None:\n    dialog_loop.trigger(\n        SystemMessage(\n            \"Your job is to improve the styling and grammar of the sentences \"\n            \"that the user throws at you. Leave the sentences unchanged if \"\n            \"they seem fine.\"\n        ),\n        user_agent=console_user_agent.fork(\n            # Write chat history to a markdown file (`CHAT.md` in the current\n            # working directory by default, fork `MarkdownHistoryAgent` if\n            # you want to customize the filepath to write to).\n            history_agent=MarkdownHistoryAgent\n        ),\n        assistant_agent=OpenAIAgent.fork(\n            model=\"gpt-4o-mini\",\n            max_tokens=1000,\n        ),\n    )\n\n\nif __name__ == \"__main__\":\n    MiniAgents(\n        # Log LLM prompts and responses to `llm_logs/` folder in the current\n        # working directory. These logs will have a form of time-stamped\n        # markdown files - single file per single prompt-response pair.\n        llm_logger_agent=True\n    ).run(main())\n```\n\nHere is what the interaction might look like if you run this script:\n\n```\nYOU ARE NOW IN A CHAT WITH AN AI ASSISTANT\n\nPress Enter to send your message.\nPress Ctrl+Space to insert a newline.\nPress Ctrl+C (or type \"exit\") to quit the conversation.\n\nUSER: hi\n\nOPENAI_AGENT: Hello! The greeting \"hi\" is casual and perfectly acceptable.\nIt's grammatically correct and doesn't require any changes. If you wanted\nto use a more formal greeting, you could consider \"Hello,\" \"Good morning/afternoon/evening,\"\nor \"Greetings.\"\n\nUSER: got it, thanks!\n\nOPENAI_AGENT: You're welcome! \"Got it, thanks!\" is a concise and clear expression\nof understanding and gratitude. It's perfectly fine for casual communication.\nA slightly more formal alternative could be \"I understand. Thank you!\"\n```\n\n### 🧸 A \"toy\" implementation of a dialog loop\n\nHere is how you can implement a dialog loop between an agent and a user from ground up yourself (for simplicity there is no history agent in this example - check out `in_memory_history_agent` and how it is used if you want to know how to implement your own history agent too):\n\n```python\nfrom miniagents import miniagent, InteractionContext, MiniAgents\nfrom miniagents.ext import agent_loop, AWAIT\n\n\n@miniagent\nasync def user_agent(ctx: InteractionContext) -> None:\n    async for msg_promise in ctx.message_promises:\n        print(\"ASSISTANT: \", end=\"\", flush=True)\n        async for token in msg_promise:\n            print(token, end=\"\", flush=True)\n        print()\n    ctx.reply(input(\"USER: \"))\n\n\n@miniagent\nasync def assistant_agent(ctx: InteractionContext) -> None:\n    # Turn a sequence of message promises into a single message promise (if\n    # there had been multiple messages in the sequence they would have had\n    # been separated by double newlines - this is how `as_single_promise()`\n    # works by default).\n    aggregated_message = await ctx.message_promises.as_single_promise()\n    ctx.reply(f'You said \"{aggregated_message}\"')\n\n\nasync def main() -> None:\n    agent_loop.trigger(agents=[user_agent, AWAIT, assistant_agent])\n\n\nif __name__ == \"__main__\":\n    MiniAgents().run(main())\n```\n\nOutput:\n\n```\nUSER: hi\nASSISTANT: You said \"hi\"\nUSER: nice!\nASSISTANT: You said \"nice!\"\nUSER: bye\nASSISTANT: You said \"bye\"\n```\n\n***Remember, with this framework the agents pass promises of message sequences between each other, not the already-resolved message sequences themselves! Even when you pass concrete messages in your code, they are still wrapped into promises of message sequences behind the scenes (while your async agent code is just sent to be processed in the background).***\n\nFor this reason, the presence of `AWAIT` sentinel in the agent chain in the example above is important. Without it the `agent_loop` would have kept scheduling more and more interactions between the agents of the chain without ever taking a break to catch up with their processing.\n\n`AWAIT` forces the `agent_loop` to stop and `await` for the complete sequence of replies from the agent right before `AWAIT` prior to scheduling (triggering) the execution of the agent right after `AWAIT`, thus allowing it to catch up with the asynchronous processing of agents and their responses in the background.\n\n### 📦 Some of the pre-packaged agents\n\n- `miniagents.ext.llms`\n  - `OpenAIAgent`: Connects to OpenAI models like GPT-4o, GPT-4o-mini, etc. Supports all OpenAI API parameters and handles token streaming seamlessly.\n  - `AnthropicAgent`: Similar to OpenAIAgent but for Anthropic's Claude models.\n- `miniagents.ext`\n  - `console_input_agent`: Prompts the user for input via the console with support for multi-line input.\n  - `console_output_agent`: Echoes messages to the console token by token, which is useful when the response is streamed from an LLM (if response messages are delivered all at once instead, this agent will also just print them all at once).\n  - `file_output_agent`: Writes messages to a specified file, useful for saving responses from other agents.\n  - `user_agent`: A user agent that echoes messages from the agent that called it, then reads the user input and returns the user input as its response. This agent is an aggregation of the `console_output_agent` and `console_input_agent` (these two agents can be substituted with other agents of similar functionality, however).\n  - `agent_loop`: Creates an infinite loop of interactions between the specified agents. It's designed for ongoing conversations or continuous processing, particularly useful for chat interfaces where agents need to take turns indefinitely (or unless stopped with `KeyboardInterrupt`).\n  - `dialog_loop`: A special case of `agent_loop` designed for conversation between a user and an assistant, with optional chat history tracking.\n  - `agent_chain`: Executes a sequence of agents in order, where each agent processes the output of the previous agent. This creates a pipeline of processing steps, with messages flowing from one agent to the next in a specified sequence.\n  - `in_memory_history_agent`: Keeps track of conversation history in memory, enabling context-aware interactions without external storage.\n  - `MarkdownHistoryAgent`: Keeps track of conversation history in a markdown file, allowing to resume a conversation from the same point even if the app is restarted.\n  - `markdown_llm_logger_agent`: Logs LLM interactions (prompts and responses) in markdown format, useful for debugging and auditing purposes (look for `llm_logger_agent=True` of the `MiniAgents()` context manager in one of the code examples above).\n\n***Feel free to explore the source code in the `miniagents.ext` package to see how various agents are implemented and get inspiration for building your own!***\n\n### 🔀 Agent parallelism explained\n\nLet's consider an example that consists of two dummy agents and an aggregator agent that aggregates the responses from the two dummy agents (and also adds some messages of its own):\n\n```python\nimport asyncio\nfrom miniagents.miniagents import (\n    MiniAgents,\n    miniagent,\n    InteractionContext,\n    Message,\n)\n\n\n@miniagent\nasync def agent1(ctx: InteractionContext) -> None:\n    print(\"Agent 1 started\")\n    ctx.reply(\"*** MESSAGE #1 from Agent 1 ***\")\n    print(\"Agent 1 still working\")\n    ctx.reply(\"*** MESSAGE #2 from Agent 1 ***\")\n    print(\"Agent 1 finished\")\n\n\n@miniagent\nasync def agent2(ctx: InteractionContext) -> None:\n    print(\"Agent 2 started\")\n    ctx.reply(\"*** MESSAGE from Agent 2 ***\")\n    print(\"Agent 2 finished\")\n\n\n@miniagent\nasync def aggregator_agent(ctx: InteractionContext) -> None:\n    print(\"Aggregator started\")\n    ctx.reply(\n        [\n            \"*** AGGREGATOR MESSAGE #1 ***\",\n            agent1.trigger(),\n            agent2.trigger(),\n        ]\n    )\n    print(\"Aggregator still working\")\n    ctx.reply(\"*** AGGREGATOR MESSAGE #2 ***\")\n    print(\"Aggregator finished\")\n\n\nasync def main() -> None:\n    print(\"TRIGGERING AGGREGATOR\")\n    msg_promises = aggregator_agent.trigger()\n    print(\"TRIGGERING AGGREGATOR DONE\\n\")\n\n    print(\"SLEEPING FOR ONE SECOND\")\n    # This is when the agents will actually start processing (in fact, any\n    # other kind of task switch would have had the same effect).\n    await asyncio.sleep(1)\n    print(\"SLEEPING DONE\\n\")\n\n    print(\"PREPARING TO GET MESSAGES FROM AGGREGATOR\")\n    async for msg_promise in msg_promises:\n        # MessagePromises always resolve into Message objects (or subclasses),\n        # even if the agent was replying with bare strings\n        message: Message = await msg_promise\n        print(message)\n\n    # You can safely `await` again. Concrete messages (and tokens, if there was\n    # token streaming) are cached inside the promises. Message sequences (as\n    # well as token sequences) are \"replayable\".\n    print(\"TOTAL NUMBER OF MESSAGES FROM AGGREGATOR:\", len(await msg_promises))\n\n\nif __name__ == \"__main__\":\n    MiniAgents().run(main())\n```\n\nThis script will print the following lines to the console:\n\n```\nTRIGGERING AGGREGATOR\nTRIGGERING AGGREGATOR DONE\n\nSLEEPING FOR ONE SECOND\nAggregator started\nAggregator still working\nAggregator finished\nAgent 1 started\nAgent 1 still working\nAgent 1 finished\nAgent 2 started\nAgent 2 finished\nSLEEPING DONE\n\nPREPARING TO GET MESSAGES FROM AGGREGATOR\n*** AGGREGATOR MESSAGE #1 ***\n*** MESSAGE #1 from Agent 1 ***\n*** MESSAGE #2 from Agent 1 ***\n*** MESSAGE from Agent 2 ***\n*** AGGREGATOR MESSAGE #2 ***\nTOTAL NUMBER OF MESSAGES FROM AGGREGATOR: 5\n```\n\nNone of the agent functions start executing upon any of the calls to the `trigger()` method. Instead, in all cases the `trigger()` method immediately returns with a **promise** to \"talk to an agent\" (a **promise** of a sequence of **promises** of response messages, to be precise - see `MessageSequencePromise` and `MessagePromise` classes for details).\n\nAs long as the global `start_everything_soon_by_default` parameter is set to `True` (which is the default - see the source code of `PromisingContext`, a parent class of the global `MiniAgents` context), the actual agent functions will eagerly try to run in the background whenever async task switching happens (which is how `asyncio.create_task()` works). In the example above the nearest task switching is going to happen upon `await asyncio.sleep(1)` inside the `main()` function, but if this `sleep()` wasn't there, it would have happened upon the first iteration of the `async for` loop which is the next place where task switching happens.\n\n**💪 EXERCISE FOR READER:** Add another `await asyncio.sleep(1)` right before `print(\"Aggregator finished\")` in the `aggregator_agent` function and then try to predict how the output will change. After that, run the modified script and check if your prediction was correct.\n\n⚠️ **ATTENTION!** You can set `start_soon` to `False` for individual agent calls if you need to prevent those agents from running in the background and only let them run when their response sequences are explicitly consumed or awaited for: `some_agent.trigger(request_messages_if_any, start_soon=False)`. However, setting `start_everything_soon_by_default` to `False` for the whole system globally is not recommended because it can lead to deadlocks. ⚠️\n\n### 📨 An alternative way to trigger agents\n\nHere's a simple example demonstrating how to use `agent_call = some_agent.initiate_call()` and then do `agent_call.send_message()` two times before calling `agent_call.reply_sequence()` (instead of all-in-one `some_agent.trigger()`):\n\n```python\nfrom miniagents import miniagent, InteractionContext, MiniAgents\n\n\n@miniagent\nasync def output_agent(ctx: InteractionContext) -> None:\n    async for msg_promise in ctx.message_promises:\n        ctx.reply(f\"Echo: {await msg_promise}\")\n\n\nasync def main() -> None:\n    agent_call = output_agent.initiate_call()\n    agent_call.send_message(\"Hello\")\n    agent_call.send_message(\"World\")\n    reply_sequence = agent_call.reply_sequence()\n\n    async for msg_promise in reply_sequence:\n        print(await msg_promise)\n\n\nif __name__ == \"__main__\":\n    MiniAgents().run(main())\n```\n\nThis will output:\n\n```\nEcho: Hello\nEcho: World\n```\n\n### 🛠️ Global `MiniAgents()` context\n\nThere are three ways to use the `MiniAgents()` context:\n\n1. Calling its `run()` method with your main function as a parameter (the `main()` function in this example should be defined as `async`):\n   ```python\n   MiniAgents().run(main())\n   ```\n\n2. Using it as an async context manager:\n   ```python\n   async with MiniAgents():\n       ...  # your async code that works with agents goes here\n   ```\n\n3. Directly calling its `activate()` (and, potentially, `afinalize()` at the end) methods:\n   ```python\n   mini_agents = MiniAgents()\n   mini_agents.activate()\n   try:\n       ...  # your async code that works with agents goes here\n   finally:\n       await mini_agents.afinalize()\n   ```\n\nThe third way might be ideal for web applications and other cases when there is no single function that you can encapsulate with the `MiniAgents()` context manager (or it is unclear what such function would be). You just do `mini_agents.activate()` somewhere upon the init of the server and forget about it.\n\n### 💬 Existing `Message` models\n\n```python\nfrom miniagents.ext.llms import UserMessage, SystemMessage, AssistantMessage\n\nuser_message = UserMessage(\"Hello!\")\nsystem_message = SystemMessage(\"System message\")\nassistant_message = AssistantMessage(\"Assistant message\")\n```\n\nThe difference between these message types is in the default values of the `role` field of the message:\n\n- `UserMessage` has `role=\"user\"` by default\n- `SystemMessage` has `role=\"system\"` by default\n- `AssistantMessage` has `role=\"assistant\"` by default\n\n### 💭 Custom `Message` models\n\nYou can create custom message types by subclassing `Message`.\n\n```python\nfrom miniagents.messages import Message\n\n\nclass CustomMessage(Message):\n    custom_field: str\n\n\nmessage = CustomMessage(\"Hello\", custom_field=\"Custom Value\")\nprint(message.content)  # Output: Hello\nprint(message.custom_field)  # Output: Custom Value\n```\n\n---\n\nFor some more examples, check out the [examples](examples) directory.\n\n## 💡 Motivation behind this project\n\nThere are three main features of MiniAgents the idea of which motivated the creation of this framework:\n\n1. It is very easy to throw bare strings, messages, message promises, collections, and sequences of messages and message promises (as well as the promises of the sequences themselves) all together into an agent reply (see `MessageType`). This entire hierarchical structure will be asynchronously resolved in the background into a flat and uniform sequence of message promises (it will be automatically \"flattened\" in the background).\n2. By default, agents work in so called `start_soon` mode, which is different from the usual way async generators work where you need to actively iterate over the async generator for it to continue processing. In `start_soon` mode, every agent, after it was invoked, actively seeks every opportunity to proceed its processing in the background whenever async tasks switch.\n3. All the StreamedPromise objects (async streams of message promises aka MessageSequencePromise, async streams of tokens aka MessagePromise, etc.) are \"replayable\". You can iterate over them multiple times, and they will always produce the same sequence of values.\n\nThe design choice for immutable messages was made specifically to enable this kind of highly parallelized agent execution. Since Generative AI applications are inherently IO-bound (with models typically hosted externally), immutable messages eliminate concerns about concurrent state mutations. This approach allows multiple agents to process messages simultaneously without risk of race conditions or data corruption, maximizing throughput in distributed LLM workflows.\n\n## 🔒 Message persistence and identification\n\nMiniAgents provides a way to persist messages as they are resolved from promises using the `@MiniAgents().on_persist_message` decorator. This allows you to implement custom logic for storing or logging messages.\n\nAdditionally, messages (as well as any other Pydantic models derived from `Frozen`) have a `hash_key` property. This property calculates the sha256 hash of the content of the message and is used as the id of the `Messages` (or any other `Frozen` model), much like there are commit hashes in git.\n\nHere's a simple example of how to use the `on_persist_message` decorator:\n\n```python\nfrom miniagents import MiniAgents, Message\n\nmini_agents = MiniAgents()\n\n\n@mini_agents.on_persist_message\nasync def persist_message(_, message: Message) -> None:\n    print(f\"Persisting message with hash key: {message.hash_key}\")\n    # Here you could save the message to a database or log it to a file\n```\n\n## 📚 Core concepts\n\nHere are some of the core concepts in the MiniAgents framework:\n\n- `MiniAgent`: A wrapper around an async function (or a whole class with `async def __call__()` method) that defines an agent's behavior. Created using the `@miniagent` decorator.\n- `InteractionContext`: Passed to each agent function, provides access to incoming messages and allows sending replies.\n- `Message`: Represents a message exchanged between agents. Can contain content, metadata, and nested messages. Immutable once created.\n- `MessagePromise`: A promise of a message that can be streamed token by token.\n- `MessageSequencePromise`: A promise of a sequence of message promises.\n- `Frozen`: An immutable Pydantic model with a git-style hash key calculated from its JSON representation. The base class for `Message`.\n\nMore underlying concepts (you will rarely need to use them directly, if at all):\n\n- `StreamedPromise`: A promise that can be resolved piece by piece, allowing for streaming. The base class for `MessagePromise` and `MessageSequencePromise`.\n- `Promise`: Represents a value that may not be available yet, but will be resolved in the future. The base class for `StreamedPromise`.\n\n## 👥 Community\n\nJoin our [Discord community](https://discord.gg/ptSvVnbwKt) to get help with your projects. We welcome questions, feature suggestions, and contributions!\n\n## 📜 License\n\nMiniAgents is released under the [MIT License](LICENSE).\n\n---\n\nHappy coding with MiniAgents! 🚀\n"
    },
    {
      "name": "foxminchan/LawKnowledge",
      "stars": 110,
      "img": "https://avatars.githubusercontent.com/u/56079798?s=40&v=4",
      "owner": "foxminchan",
      "repo_name": "LawKnowledge",
      "description": "A legal knowledge search and Q&A application based on Vietnam's Legal Code and legal document database ⚖️",
      "homepage": "https://foxminchan.github.io/LawKnowledge/",
      "language": "TypeScript",
      "created_at": "2023-11-17T04:10:19Z",
      "updated_at": "2025-04-22T06:58:34Z",
      "topics": [
        "generative-ai",
        "microservice",
        "natural-language-processing",
        "nlp",
        "nx",
        "searching",
        "semantic-search"
      ],
      "readme": "<p align=\"center\">\n\t<img loading=\"lazy\" src=\"./assets/images/law.png\" alt=\"Law Knowledge\" height=\"150\">\n</p>\n\n<h1 align=\"center\">Law Knowlege</h1>\n\n<p align=\"center\">\n\tA legal knowledge search and Q&A application based on Vietnam's Legal Code and legal document database\n<p>\n\n<p align=\"center\">\n  <a href=\"https://foxminchan.github.io/LawKnowledge/\" rel=\"dofollow\" target=\"blank\"><strong>Explore the docs »</strong></a>\n\t<br/>\n\t<br/>\n\t<a href=\"https://github.com/foxminchan/LawKnowledge/issues/new?assignees=&labels=&projects=&template=bug_report.md&title=%F0%9F%90%9B+Bug+Report%3A+\">🐛 Report Bug</a>\n\t|\n\t<a href=\"https://github.com/foxminchan/LawKnowledge/issues/new?assignees=&labels=&projects=&template=feature_request.md&title=%F0%9F%9A%80+Feature%3A+\">✈️ Request Feature</a>\n\t|\n\t<a href=\"https://t.me/+bz74heXQgBwwOWRl\">💬 Join Our Telegram</a>\n\t|\n\t<a href=\"https://github.com/foxminchan/LawKnowledge/wiki\">📚 Read Wiki</a>\n\t|\n\t<a href=\"https://github.com/users/foxminchan/projects/8\">📋 Roadmap</a>\n</p>\n\n<p align=\"center\">\n\t<a href=\"https://github.com/foxminchan/LawKnowledge/issues\" target=\"blank\">\n\t\t<img loading=\"lazy\" src=\"https://img.shields.io/github/issues/foxminchan/LawKnowledge?label=Issue\" alt=\"LawKnowledge issues\"/>\n\t</a>\n\t<a href=\"https://github.com/foxminchan/LawKnowledge/blob/main/LICENSE\" target=\"blank\">\n\t\t<img loading=\"lazy\" src=\"https://img.shields.io/github/license/foxminchan/LawKnowledge?label=License\" alt=\"LawKnowledge license\"/>\n\t</a>\n\t<a href=\"https://colab.research.google.com/drive/1HcL3VrdKhQ1f4ZxbYOhiw_kYTtxRlT_f?usp=sharing\" target=\"_blank\">\n\t\t<img loading=\"lazy\" src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Colab\">\n\t</a>\n\t<a href=\"https://gitpod.io/new/#https://github.com/foxminchan/LawKnowledge\" target=\"_blank\">\n\t\t<img loading=\"lazy\" src=\"https://img.shields.io/badge/Gidpod-Ready%20to%20Code-blue?logo=gitpod\" alt=\"Gitpod\">\n\t</a>\n\t<a href=\"https://t.me/+bz74heXQgBwwOWRl\" target=\"_blank\">\n\t\t<img loading=\"lazy\" src=\"https://img.shields.io/static/v1?message=Join%20chat&color=9cf&logo=telegram&label=Teleram\" alt=\"Telegram\">\n\t</a>\n</p>\n\n<img loading=\"lazy\" src=\"./assets/images/banner.gif\" alt=\"Preview\" width=\"100%\">\n\n<br/>\n\n<p align=\"justify\">\n\n⚙️ This project used **SonarCloud** to analyze the code quality of the project. **Pulumi** for infrastructure as code (IaC). The badge below shows the quality of the code. Click on the badge to see the details.\n\n</p>\n\n<br/>\n\n<p align=\"center\">\n\t<a href=\"https://sonarcloud.io/summary/new_code?id=foxminchan_LawKnowledge\" target=\"blank\">\n\t\t<img loading=\"lazy\" src=\"https://sonarcloud.io/images/project_badges/sonarcloud-white.svg\" alt=\"SonarCloud\" height=\"45\" />\n\t</a>\n\t<a href=\"https://app.pulumi.com/new?template=https://github.com/foxminchan/LawKnowledge/tree/main/deploys/iac/pulumi\" target=\"blank\">\n\t\t<img loading=\"lazy\" src=\"https://get.pulumi.com/new/button.svg\" alt=\"Pulumi\" height=\"45\" />\n\t</a>\n</p>\n\n<h1>Features</h1>\n\n- 📖 Document Organization and Linking\n- 📦 Terminology Extraction\n- 🔍 Quick Search for Related Content\n- 🤖 User Query Support\n\n<h1>Table of Contents</h1>\n\n<details>\n<summary>Expand contents</summary>\n\n- [What is Law Knowledge?](#what-is-law-knowledge)\n- [Tentative technologies](#tentative-technologies)\n- [Building blocks](#building-blocks)\n- [Getting Started](#getting-started)\n\t- [🛠️ Prerequisites](#️-prerequisites)\n\t\t- [Infrastucture](#infrastucture)\n\t\t- [Services](#services)\n\t- [😄 HuggingFace Resources](#-huggingface-resources)\n\t- [🧑‍💻 Setup](#-setup)\n\t- [🚀 Running](#-running)\n\t- [🧪 Testing](#-testing)\n\t- [🧩 Other](#-other)\n- [Dependency Graph](#dependency-graph)\n- [CI/CD](#cicd)\n- [Contributing](#contributing)\n\t- [📖 Contributing Guidelines](#-contributing-guidelines)\n\t- [💁 Want to Help?](#-want-to-help)\n\t- [🫂 Code of Conduct](#-code-of-conduct)\n- [Support and Organization](#support-and-organization)\n- [License](#license)\n\n</details>\n\n# What is Law Knowledge?\n\n<p align=\"justify\">\nLaw Knowledge is an app designed to provide quick access to Vietnam's legal information, including laws and legal documents. It's ideal for professionals, students, or anyone needing insights into Vietnamese legal codes and regulations.\n</p>\n\n[![Sparkline](https://stars.medv.io/foxminchan/LawKnowledge.svg)](https://stars.medv.io/foxminchan/LawKnowledge)\n\n<blockquote>\n\t<p align=\"justify\">\n\t\t<b>Law Knowledge</b> is a legal knowledge search and Q&A application based on Vietnam's Legal Code and legal document database. The application is built on the <b>Microservice Architecture</b> and uses <b>Generative AI</b> to extract legal terms and <b>Question Answering</b> to answer user questions. The application is built with <b>Open Source</b> technologies and is deployed on <b>Cloud</b> platforms.\n\t</p>\n</blockquote>\n\nIf you want to find out more about the contest, please visit the [VFOSSA website](https://vfossa.vn/tin-tuc/de-thi-phan-mem-nguon-mo-olp-2023-688.html).\n\n<img loading=\"lazy\" src=\"./assets/images/preview.png\" alt=\"Preview\" width=\"100%\">\n\n# Tentative technologies\n\n- [React](https://reactjs.org/)\n- [NestJS](https://nestjs.com/)\n- [Python](https://www.python.org/)\n- [Pulumi](https://www.pulumi.com/)\n- [Kafka](https://kafka.apache.org/)\n- [ArgoCD](https://argoproj.github.io/argo-cd/)\n- [Jenkins](https://www.jenkins.io/)\n- [Kubernetes](https://kubernetes.io/)\n- [OpenTelemetry](https://opentelemetry.io/)\n- [Grafana](https://grafana.com/), [Prometheus](https://prometheus.io/), [Zipkin](https://zipkin.io/), [Fluentd](https://www.fluentd.org/)\n\n# Building blocks\n\n<p align=\"justify\">\nWe used <b>Microservice Architecture</b> to build this project to make it easier to scale and maintain. The following diagram shows the architecture of the project.\n</p>\n\n<img loading=\"lazy\" src=\"./assets/images/architecture.svg\" alt=\"Architecture\" width=\"100%\">\n\n# Getting Started\n\n## 🛠️ Prerequisites\n\n### Infrastucture\n\n<ul>\n\t<li align=\"justify\">\n\t\t<b><a href=\"https://nx.dev/\" target=\"_blank\">Nx</a></b> - Nx is a set of extensible dev tools for monorepos.\n\t</li>\n\t<li align=\"justify\">\n\t\t<b><a href=\"https://nodejs.org/en/\" target=\"_blank\">Nodejs</a></b> - Node.js® is a JavaScript runtime built on Chrome's V8 JavaScript engine.\n\t</li>\n\t<li align=\"justify\">\n\t\t<b><a href=\"https://www.npmjs.com/\" target=\"_blank\">pnpm</a></b> - Fast, disk space efficient package manager.\n\t</li>\n\t<li align=\"justify\">\n\t\t<b><a href=\"https://www.python.org/\" target=\"_blank\">Python</a></b> - Python is a programming language that lets you work quickly and integrate systems more effectively.\n\t</li>\n\t<li align=\"justify\">\n\t\t<b><a href=\"https://python-poetry.org/\" target=\"_blank\">Poetry</a></b> - Poetry helps you declare, manage and install dependencies of Python projects, ensuring you have the right stack everywhere.\n\t</li>\n\t<li align=\"justify\">\n\t\t<b><a href=\"https://www.pulumi.com/\" target=\"_blank\">Pulumi</a></b> - Pulumi is a manage infrastructure, secrets, and configurations intuitively on any cloud.\n\t</li>\n\t<li align=\"justify\">\n\t\t<b><a href=\"https://www.docker.com/\" target=\"_blank\">Docker (Kubernetes Enabled)</a></b> - Docker is an open platform for developing, shipping, and running applications.\n\t</li>\n\t<li align=\"justify\">\n\t\t<b><a href=\"https://helm.sh/\" target=\"_blank\">Helm</a></b> - Helm is the best way to find, share, and use software built for Kubernetes.\n\t</li>\n</ul>\n\n### Services\n\n<ul>\n\t<li align=\"justify\">\n\t\t<b><a href=\"https://aws.amazon.com/\" target=\"_blank\">AWS</a></b> - An Amazon Web Services account is required to deploy the infrastructure.\n\t</li>\n\t<li align=\"justify\">\n\t\t<b><a href=\"https://huggingface.co/\" target=\"_blank\">HuggingFace</a></b> - A HuggingFace account is required to download and upload the dataset and model.\n\t</li>\n</ul>\n\n## 😄 HuggingFace Resources\n\n<p align=\"justify\">\n\nYou can download the dataset from the following link:\n\n- [Law Knowledge Data](https://huggingface.co/datasets/foxminchan/law-knowledge): The dataset from [Pháp điển](https://phapdien.moj.gov.vn/) and [Văn bản pháp luật](https://vbpl.vn/).\n- [Law Knowledge Model](https://huggingface.co/foxminchan/law-knowledge): A fine-tuned model from [PhoBERT](https://huggingface.co/vinai/phobert-base-v2).\n\n</p>\n\n## 🧑‍💻 Setup\n\nFirst, clone the repository to your local machine:\n\n```bash\ngit clone https://github.com/foxminchan/LawKnowledge\n```\n\nNext, navigate to the root directory of the project and install the dependencies:\n\n```bash\npnpm install --force\n```\n\n## 🚀 Running\n\nFor the website, you can run the following command:\n\n```bash\nnpx nx serve website --prod\n```\n\nFor the API, you can run the following command:\n\n```bash\n# For the API Gateway\nnpx nx serve api-gateway --prod\n\n# For the Auth Service\nnpx nx serve auth-svc --prod\n\n# For the Law Service\nnpx nx serve law-svc --prod\n\n# For the Search Service, navigate to the search-svc directory\nnpx nx build search-svc && cd apps/api/search-svc/search-svc && python main.py\n\n# For the Chat Service, navigate to the chat-svc directory\nnpx nx build chat-svc && cd apps/api/chat-svc/chat-svc && python main.py\n```\n\nTo traning the model, you can run the following command:\n\n```bash\nnpx nx build bert && cd apps/nlp/bert/bert && python main.py\n```\n\nTo set up the infrastructure, you can run the following command:\n\n```bash\nnpx nx up pulumi\n```\n\n> [!IMPORTANT]\n>\n> <p align=\"justify\">\n> If you want to run with Nx, make sure you have installed `poetry` globally. Python version must be `>=3.9.1 <=3.12`.\n> </p>\n\n## 🧪 Testing\n\nFor the website, you can run the following command:\n\n```bash\nnpx nx test website-e2e\n```\n\nFor the API, you can run the following command:\n\n```bash\n# For the Auth Service\nnpx nx test auth-svc-e2e\n\n# For the Law Service\nnpx nx test law-svc-e2e\n\n# For the Search Service\nnpx nx test search-svc-e2e\n\n# For the Chat Service\nnpx nx test chat-svc\n```\n\nTo test the model, you can run the following command:\n\n```bash\nnpx nx test bert\n```\n\n## 🧩 Other\n\nTo run the tooling for processing the dataset, you can run the following command:\n\n```bash\nnpx nx build processor && cd apps/nlp/processor/processor && python main.py\n```\n\nFor running documentation, you can run the following command:\n\n```bash\nnpx nx serve docs\n```\n\nFor running the Diagram as Code, you can run the following command:\n\n```bash\nnpx nx build dac && cd assets/dac/dac && python main.py\n```\n\nSome useful scripts:\n\n```bash\n# Navigate to the scripts directory\ncd tools/scripts && ls\n\n# + scripts\n# +-- build.sh --> Build the project\n# +-- clean.sh --> Clean the docker image\n# +-- init.sh --> Initialize the project\n# +-- prepare.sh --> Prepare the project\n# +-- start.sh --> Start the project\n# +-- start-o11y.sh --> Start the observability stack\n# +-- stop-o11y.sh --> Stop the observability stack\n\n# Run the script\n./<script-name>\n````\n\n# Dependency Graph\n\nYou can see the dependency graph of the project by running the following command:\n\n```bash\nnpx nx dep-graph\n```\n\nHere is the dependency graph of the project:\n\n<img loading=\"lazy\" src=\"./assets/images/dep-graph.png\" alt=\"Dependency Graph\" width=\"100%\">\n\n# CI/CD\n\n<img loading=\"lazy\" src=\"./assets/images/cicd.svg\" alt=\"CI/CD\" width=\"100%\">\n\n# Contributing\n\n## 📖 Contributing Guidelines\n\n<p align=\"justify\">\n\nWe are excited that you are interested in contributing to this project! Before submitting your contribution, please make sure to take a moment and read through the following guidelines:\n\nRead through our [contributing guidelines](.github/CONTRIBUTING.md) to learn about our submission process, coding rules, and more.\n\n</p>\n\n## 💁 Want to Help?\n\n<p align=\"justify\">\n\nWant to report a bug, contribute some code, or improve the documentation? Excellent! Read up on our guidelines for [contributing](.github/CONTRIBUTING.md) and then check out one of our issues labeled as <kbd>[help wanted](https://github.com/foxminchan/LawKnowledge/labels/help%20wanted)</kbd> or <kbd>[good first issue](https://github.com/foxminchan/LawKnowledge/labels/good%20first%20issue)</kbd>.\n\n</p>\n\n## 🫂 Code of Conduct\n\n<p align=\"justify\">\n\nHelp us keep Law Knowledge open and inclusive. Please read and follow our [Code of Conduct](.github/CODE_OF_CONDUCT.md).\n\n</p>\n\n# Support and Organization\n\n<p align=\"center\">\n\t<a href=\"https://hutech.edu.vn/\" target=\"_blank\">\n\t\t<img loading=\"lazy\" src=\"./assets/images/hutech.png\" height=\"60px\" alt=\"Hutech\">\n\t</a>\n\t<a href=\"https://vfossa.vn/\" target=\"_blank\">\n\t\t<img loading=\"lazy\" src=\"./assets/images/vfossa.png\" height=\"60px\" alt=\"Hutech\">\n\t</a>\n\t<a href=\"http://husc.hueuni.edu.vn/\" target=\"_blank\">\n\t\t<img loading=\"lazy\" src=\"./assets/images/husc.jpg\" height=\"60px\" alt=\"Husc\">\n\t</a>\n\t<a href=\"https://olp.husc.edu.vn/\" target=\"_blank\">\n\t\t<img loading=\"lazy\" src=\"./assets/images/icpc.jpg\" height=\"60px\" alt=\"ICPC\">\n\t</a>\n</p>\n\n# License\n\n<p align=\"justify\">\n\nThis project is licensed under the terms of the [MIT](LICENSE) license.\n\n</p>\n"
    },
    {
      "name": "flairNLP/fabricator",
      "stars": 108,
      "img": "https://avatars.githubusercontent.com/u/59021421?s=40&v=4",
      "owner": "flairNLP",
      "repo_name": "fabricator",
      "description": "",
      "homepage": "",
      "language": "",
      "created_at": "",
      "updated_at": "",
      "topics": [],
      "readme": "![Fabricator Logo](resources/logo_fabricator.drawio_dark.png#gh-dark-mode-only)\n![Fabricator Logo](resources/logo_fabricator.drawio_white.png#gh-light-mode-only)\n\n<p align=\"center\">A flexible open-source framework to generate datasets with large language models.</p>\n<p align=\"center\">\n<img alt=\"version\" src=\"https://img.shields.io/badge/version-0.2.0-green\">\n<img alt=\"python\" src=\"https://img.shields.io/badge/python-3.10-blue\">\n<img alt=\"Static Badge\" src=\"https://img.shields.io/badge/license-apache2.0-green\">\n</p>\n<div align=\"center\">\n<hr>\n\n[Installation](#installation) | [Basic Concepts](#basic-concepts) | [Examples](#examples) | [Tutorials](tutorials/TUTORIAL-1_OVERVIEW.md) | \n[Paper](https://arxiv.org/abs/2309.09582) | [Citation](#citation)\n\n<hr>\n</div>\n\n## News\n\n- **[10/23]** We released the first version of this repository on PyPI. You can install it via `pip install fabricator-ai`.\n- **[10/23]** Our paper got accepted at EMNLP 2023. You can find the preprint [here](https://arxiv.org/abs/2309.09582). You can find the experimental scripts under release v0.1.0.\n- **[09/23]** Support for `gpt-3.5-turbo-instruct` added in the new [Haystack](https://github.com/deepset-ai/haystack) release!\n- **[08/23]** Added several experimental scripts to investigate the generation and annotation ability of `gpt-3.5-turbo` on various downstream tasks + the influence of few-shot examples on the performance for different downstream tasks.\n- **[07/23]** Refactorings of majors classes - you can now simply use our BasePrompt class to create your own customized prompts for every downstream task!\n- **[07/23]** Added dataset transformations for token classification to prompt LLMs with textual spans rather than with list of tags.\n- **[06/23]** Initial version of fabricator supporting text classification and question answering tasks.\n\n## Overview\n\nThis repository:\n\n- is <b>an easy-to-use open-source library</b> to generate datasets with large language models. If you want to train\na model on a specific domain / label distribution / downstream task, you can use this framework to generate\na dataset for it.\n- <b>builds on top of deepset's haystack and huggingface's datasets</b> libraries. Thus, we support a wide range \nof language models and you can load and use the generated datasets as you know it from the Datasets library for your \nmodel training.\n- is <b>highly flexible</b> and offers various adaptions possibilities such as\nprompt customization, integration and sampling of fewshot examples or annotation of the unlabeled datasets.\n\n## Installation\nUsing conda:\n```\ngit clone git@github.com:flairNLP/fabricator.git\ncd fabricator\nconda create -y -n fabricator python=3.10\nconda activate fabricator\npip install fabricator-ai\n```\n\nIf you want to install in editable mode, you can use the following command:\n```\npip install -e .\n```\n\n## Basic Concepts\n\nThis framework is based on the idea of using large language models to generate datasets for specific tasks. To do so, \nwe need four basic modules: a dataset, a prompt, a language model and a generator:\n- <b>Dataset</b>: We use [huggingface's datasets library](https://github.com/huggingface/datasets) to load fewshot or \nunlabeled datasets and store the generated or annotated datasets with their `Dataset` class. Once\ncreated, you can share the dataset with others via the hub or use it for your model training.\n- <b>Prompt</b>: A prompt is the instruction made to the language model. It can be a simple sentence or a more complex\ntemplate with placeholders. We provide an easy interface for custom dataset generation prompts in which you can specify \nlabel options for the LLM to choose from, provide fewshot examples to support the prompt with or annotate an unlabeled \ndataset in a specific way.\n- <b>LLM</b>: We use [deepset's haystack library](https://github.com/deepset-ai/haystack) as our LLM interface. deepset\nsupports a wide range of LLMs including OpenAI, all models from the HuggingFace model hub and many more.\n- <b>Generator</b>: The generator is the core of this framework. It takes a dataset, a prompt and a LLM and generates a\ndataset based on your specifications.\n\n## Examples\n\nWith our library, you can generate datasets for any task you want. You can start as simple\nas that:\n\n### Generate a dataset from scratch\n\n```python\nimport os\nfrom haystack.nodes import PromptNode\nfrom fabricator import DatasetGenerator\nfrom fabricator.prompts import BasePrompt\n\nprompt = BasePrompt(\n    task_description=\"Generate a short movie review.\",\n)\n\nprompt_node = PromptNode(\n    model_name_or_path=\"gpt-3.5-turbo\",\n    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n    max_length=100,\n)\n\ngenerator = DatasetGenerator(prompt_node)\ngenerated_dataset = generator.generate(\n    prompt_template=prompt,\n    max_prompt_calls=10,\n)\n\ngenerated_dataset.push_to_hub(\"your-first-generated-dataset\")\n```\n\nIn our tutorial, we introduce how to create classification datasets with label options to choose from, how to include \nfewshot examples or how to annotate unlabeled data into predefined categories.\n\n## Citation\n\nIf you find this repository useful, please cite our work.\n\n```\n@inproceedings{golde2023fabricator,\n    title = \"Fabricator: An Open Source Toolkit for Generating Labeled Training Data with Teacher {LLM}s\",\n    author = \"Golde, Jonas  and Haller, Patrick  and Hamborg, Felix  and Risch, Julian  and Akbik, Alan\",\n    booktitle = \"Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: System Demonstrations\",\n    month = dec,\n    year = \"2023\",\n    address = \"Singapore\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.emnlp-demo.1\",\n    pages = \"1--11\",\n}\n```\n\n\n"
    },
    {
      "name": "larsbaunwall/bricky",
      "stars": 105,
      "img": "https://avatars.githubusercontent.com/u/3168816?s=40&v=4",
      "owner": "larsbaunwall",
      "repo_name": "bricky",
      "description": "Haystack/OpenAI based chatbot curating a custom knowledgebase",
      "homepage": "",
      "language": "Python",
      "created_at": "2023-01-29T23:13:05Z",
      "updated_at": "2025-02-03T02:04:01Z",
      "topics": [
        "ai",
        "haystack",
        "nextjs",
        "openai"
      ],
      "readme": "## Meet Bricky - a conversational bot using OpenAI 🤖\n\nRemember clippy? Meet bricky!\n\nBricky is a conversational bot using [Retrieval-Augmented Generation](https://arxiv.org/abs/2005.11401) with some help from OpenAI's GPT-3 LLM.\n\nBricky indexes content stored in markdown files and vectorizes it using OpenAI embeddings. It then uses few-shot learning using a ChatGPT prompt to generate an answer based on relevant content. \n\nRead more about my journey into this field and the background for creating Bricky in [my blog article](https://medium.com/@larslb/standing-on-the-shoulders-of-a-giant-embedding-intelligent-behavior-using-large-language-models-8c0f644b6d87)\n\nThe project is inspired by the awesome [HoustonAI by Astro](https://github.com/withastro/houston.astro.build)\n\n![screenshot.png](./res/bricky-recording.gif)\n\n\n## Getting started 🚀\n\n### Prereqs\n\nProvide these `env` variables for the api container by creating a `dotenv` file in `api/.env`\n\n```\nOPENAI_KEY=<YOUR OPENAI KEY GOES HERE>\n```\n\n### Steps\n\n1. Clone this repo!\n1. Copy over your documentation to `api/sources`\n1. Run docker-compose: `docker-compose up`\n\nYou should now have two endpoints running:\n\n- The [Nextjs-based frontend](./app): Open [http://localhost:3000](http://localhost:3000) to meet Bricky.\n- The [Haystack-based API](./api): Open [http://localhost:8080/docs](http://localhost:8080/docs) with your browser to see the OpenAPI documentation.\n\nNote: if you make changes to the any files, i.e. `api/.env` or the docs in `sources/docs`, you need to rebuild the images: `docker-compose rebuild --no-cache`.\n\n## Learn more\n\nTo learn more about Haystack and OpenAI, take a look at the following resources:\n\n- [Haystack Documentation](https://docs.haystack.deepset.ai/docs) - learn about the Haystack platform by deepset.ai.\n- [OpenAI docs](https://platform.openai.com/docs/introduction) - the OpenAI docs site.\n\nTo learn more about Next.js, take a look at the following resources:\n\n- [Next.js Documentation](https://nextjs.org/docs) - learn about Next.js features and API.\n- [Learn Next.js](https://nextjs.org/learn) - an interactive Next.js tutorial.\n\n## Powered by haystack and OpenAI ChatGPT\n\n- Frontend implementation can be found [here](./app).\n- Backend implementation can be found [here](./api).\n\nQuestions or comments? Reach out to [@larsbaunwall](https://github.com/larsbaunwall)\n\nDon't forget to :star: this repo!\n"
    },
    {
      "name": "CodingWithLewis/MemeGenerator",
      "stars": 104,
      "img": "https://avatars.githubusercontent.com/u/142752183?s=40&v=4",
      "owner": "CodingWithLewis",
      "repo_name": "MemeGenerator",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2024-04-10T01:56:51Z",
      "updated_at": "2025-04-08T13:52:42Z",
      "topics": [],
      "readme": "# AI Meme Generator Using RAG and Different Models\n\n## From the Coding with Lewis Video:\n\n[![Thumbnail for the AI meme video.](/docs/thumb7.png \"Play Video\")](https://www.youtube.com/watch?v=5GfgrYz9z9A)\n\n## Examples\n\n![](/docs/1.png)\n![](/docs/2.png)\n\n## Demo\n\nhttps://memegenerator-ergog3zn3pc7txnm6mgggf.streamlit.app/\n\n## Getting Started\n\nThe app runs on Streamio and mostly API endpoints.\n\n1. Create a `.env` file in root.\n2. Insert relevant API Keys:\n\n```\nANTHROPIC_API_KEY=\n\nOPENAI_API_KEY=\n\nNEON_POSTGRES=\n\nCOHERE_API_KEY=\n```\n\n3. build the docker container\n\n### This is currently a work in progress :)\n\nFeel free to explore. A lot of cleanup needs to be done, but want to prove to you that this project does indeed exist!"
    },
    {
      "name": "parkervg/blendsql",
      "stars": 95,
      "img": "https://avatars.githubusercontent.com/u/44219290?s=40&v=4",
      "owner": "parkervg",
      "repo_name": "blendsql",
      "description": "Query language for blending SQL logic and LLM reasoning across structured + unstructured data. [Findings of ACL 2024]",
      "homepage": "https://parkervg.github.io/blendsql/",
      "language": "Python",
      "created_at": "2024-01-30T21:06:05Z",
      "updated_at": "2025-04-22T00:58:43Z",
      "topics": [
        "llm",
        "natural-language-processing",
        "nlp",
        "python",
        "query-language",
        "question-answering",
        "sql",
        "structured-generation",
        "symbolic-ai"
      ],
      "readme": "<div align=\"right\">\n<a href=\"https://opensource.org/licenses/Apache-2.0\"><img src=\"https://img.shields.io/badge/License-Apache_2.0-blue.svg\" /></a>\n<a><img src=\"https://img.shields.io/github/last-commit/parkervg/blendsql?color=green\"/></a>\n<a><img src=\"https://img.shields.io/endpoint?url=https://gist.githubusercontent.com/parkervg/e24f1214fdff3ab086b829b5f01f85a8/raw/covbadge.json\"/></a>\n<a><img src=\"https://img.shields.io/badge/python-3.9%20%7C%203.10%20%7C%203.11%20%7C%203.12-blue\"/></a>\n<br>\n</div>\n\n<div align=\"center\"><picture>\n  <source media=\"(prefers-color-scheme: dark)\" srcset=\"docs/img/logo_dark.png\">\n  <img alt=\"blendsql\" src=\"docs/img/logo_light.png\" width=350\">\n</picture>\n<p align=\"center\">\n    <i> SQL 🤝 LLMs </i>\n  </p>\n<b><h3>Check out our <a href=\"https://parkervg.github.io/blendsql/\" target=\"_blank\">online documentation</a> for a more comprehensive overview.</h3></b>\n\n</div>\n<br/>\n\n# Installation\n```\npip install blendsql\n```\n\n# Quickstart\n```python\nimport pandas as pd\n\nfrom blendsql import BlendSQL\nfrom blendsql.ingredients import LLMMap, LLMQA, LLMJoin\nfrom blendsql.models import TransformersLLM, LiteLLM\n\nUSE_LOCAL_CONSTRAINED_MODEL = False\n\n# Load model, either a local transformers model, or remote provider via LiteLLM\nif USE_LOCAL_CONSTRAINED_MODEL:\n    model = TransformersLLM(\n        \"meta-llama/Llama-3.2-3B-Instruct\", config={\"device_map\": \"auto\"}\n    )  # Local models enable BlendSQL's predicate-guided constrained decoding\nelse:\n    model = LiteLLM(\"openai/gpt-4o-mini\")\n\n# Prepare our BlendSQL connection\nbsql = BlendSQL(\n    {\n        \"People\": pd.DataFrame(\n            {\n                \"Name\": [\n                    \"George Washington\",\n                    \"John Adams\",\n                    \"Thomas Jefferson\",\n                    \"James Madison\",\n                    \"James Monroe\",\n                    \"Alexander Hamilton\",\n                    \"Sabrina Carpenter\",\n                    \"Charli XCX\",\n                    \"Elon Musk\",\n                    \"Michelle Obama\",\n                    \"Elvis Presley\",\n                ],\n                \"Known_For\": [\n                    \"Established federal government, First U.S. President\",\n                    \"XYZ Affair, Alien and Sedition Acts\",\n                    \"Louisiana Purchase, Declaration of Independence\",\n                    \"War of 1812, Constitution\",\n                    \"Monroe Doctrine, Missouri Compromise\",\n                    \"Created national bank, Federalist Papers\",\n                    \"Nonsense, Emails I Cant Send, Mean Girls musical\",\n                    \"Crash, How Im Feeling Now, Boom Clap\",\n                    \"Tesla, SpaceX, Twitter/X acquisition\",\n                    \"Lets Move campaign, Becoming memoir\",\n                    \"14 Grammys, King of Rock n Roll\",\n                ],\n            }\n        ),\n        \"Eras\": pd.DataFrame({\"Years\": [\"1700-1800\", \"1800-1900\", \"1900-2000\", \"2000-Now\"]}),\n    },\n    ingredients={LLMMap, LLMQA, LLMJoin},\n    model=model,\n    verbose=True,\n)\n\nsmoothie = bsql.execute(\n    \"\"\"\n    SELECT * FROM People P\n    WHERE P.Name IN {{\n        LLMQA('First 3 presidents of the U.S?', modifier='{3}')\n    }}\n    \"\"\",\n    infer_gen_constraints=True,\n)\n\nprint(smoothie.df)\n# ┌───────────────────┬───────────────────────────────────────────────────────┐\n# │ Name              │ Known_For                                             │\n# ├───────────────────┼───────────────────────────────────────────────────────┤\n# │ George Washington │ Established federal government, First U.S. Preside... │\n# │ John Adams        │ XYZ Affair, Alien and Sedition Acts                   │\n# │ Thomas Jefferson  │ Louisiana Purchase, Declaration of Independence       │\n# └───────────────────┴───────────────────────────────────────────────────────┘\nprint(smoothie.summary())\n# ┌────────────┬──────────────────────┬─────────────────┬─────────────────────┐\n# │   Time (s) │   # Generation Calls │   Prompt Tokens │   Completion Tokens │\n# ├────────────┼──────────────────────┼─────────────────┼─────────────────────┤\n# │    1.25158 │                    1 │             296 │                  16 │\n# └────────────┴──────────────────────┴─────────────────┴─────────────────────┘\n\n\nsmoothie = bsql.execute(\n    \"\"\"\n    SELECT GROUP_CONCAT(Name, ', ') AS 'Names',\n    {{\n        LLMMap(\n            'In which time period was this person born?',\n            'People::Name',\n            options='Eras::Years'\n        )\n    }} AS Born\n    FROM People\n    GROUP BY Born\n    \"\"\",\n)\n\nprint(smoothie.df)\n# ┌───────────────────────────────────────────────────────┬───────────┐\n# │ Names                                                 │ Born      │\n# ├───────────────────────────────────────────────────────┼───────────┤\n# │ George Washington, John Adams, Thomas Jefferson, J... │ 1700-1800 │\n# │ Sabrina Carpenter, Charli XCX, Elon Musk, Michelle... │ 2000-Now  │\n# │ Elvis Presley                                         │ 1900-2000 │\n# └───────────────────────────────────────────────────────┴───────────┘\nprint(smoothie.summary())\n# ┌────────────┬──────────────────────┬─────────────────┬─────────────────────┐\n# │   Time (s) │   # Generation Calls │   Prompt Tokens │   Completion Tokens │\n# ├────────────┼──────────────────────┼─────────────────┼─────────────────────┤\n# │    1.03858 │                    2 │             544 │                  75 │\n# └────────────┴──────────────────────┴─────────────────┴─────────────────────┘\n\nsmoothie = bsql.execute(\"\"\"\n    SELECT {{\n        LLMQA(\n            'Describe BlendSQL in 50 words',\n            (\n                SELECT content[0:5000] AS \"README\"\n                FROM read_text('https://raw.githubusercontent.com/parkervg/blendsql/main/README.md');\n            )\n        )\n    }} AS answer\n\"\"\")\n\nprint(smoothie.df)\n# ┌─────────────────────────────────────────────────────┐\n# │ answer                                              │\n# ├─────────────────────────────────────────────────────┤\n# │ BlendSQL is a Python library that combines SQL a... │\n# └─────────────────────────────────────────────────────┘\n\nprint(smoothie.summary())\n\n# ┌────────────┬──────────────────────┬─────────────────┬─────────────────────┐\n# │   Time (s) │   # Generation Calls │   Prompt Tokens │   Completion Tokens │\n# ├────────────┼──────────────────────┼─────────────────┼─────────────────────┤\n# │    4.07617 │                    1 │            1921 │                  50 │\n# └────────────┴──────────────────────┴─────────────────┴─────────────────────┘\n\n```\n\n# ✨ News\n- (3/16/25) Use BlendSQL with 100+ LLM APIs, using [LiteLLM](https://github.com/BerriAI/litellm)!\n- (10/26/24) New tutorial! [blendsql-by-example.ipynb](examples/blendsql-by-example.ipynb)\n- (10/18/24) Concurrent async requests in 0.0.29! OpenAI and Anthropic `LLMMap` calls are speedy now.\n  - Customize max concurrent async calls via `blendsql.config.set_async_limit(10)`\n- (10/15/24) As of version 0.0.27, there is a new pattern for defining + retrieving few-shot prompts; check out [Few-Shot Prompting](#few-shot-prompting) in the README for more info\n- (10/15/24) Check out [Some Cool Things by Example](https://parkervg.github.io/blendsql/by-example/) for some recent language updates!\n\n# Summary\n\nBlendSQL is a *superset of SQL* for problem decomposition and hybrid question-answering with LLMs.\n\nAs a result, we can *Blend* together...\n\n- 🥤 ...operations over heterogeneous data sources (e.g. tables, text, images)\n- 🥤 ...the structured & interpretable reasoning of SQL with the generalizable reasoning of LLMs\n\n![comparison](docs/img/comparison.jpg)\n\n**Now, the user is given the control to oversee all calls (LLM + SQL) within a unified query language.**\n\n# Features\n\n- Supports many DBMS 💾\n  - SQLite, PostgreSQL, DuckDB, Pandas (aka duckdb in a trenchcoat)\n- Supports local & remote models ✨\n  - Transformers, OpenAI, Anthropic, Ollama, and 100+ more!\n- Easily extendable to [multi-modal usecases](./examples/vqa-ingredient.ipynb) 🖼️\n- Write your normal queries - smart parsing optimizes what is passed to external functions 🧠\n  - Traverses abstract syntax tree with [sqlglot](https://github.com/tobymao/sqlglot) to minimize LLM function calls 🌳\n- Constrained decoding with [guidance](https://github.com/guidance-ai/guidance) 🚀\n  - When using local models, we only generate syntactically valid outputs according to query syntax + database contents\n- LLM function caching, built on [diskcache](https://grantjenks.com/docs/diskcache/) 🔑\n\n# Example\nFor example, imagine we have the following table titled `parks`, containing [info on national parks in the United States](https://en.wikipedia.org/wiki/List_of_national_parks_of_the_United_States).\n\nWe can use BlendSQL to build a travel planning LLM chatbot to help us navigate the options below.\n\n\n| **Name**        | **Image**                                                                       | **Location**       | **Area**                          | **Recreation Visitors (2022)** | **Description**                                                                                                                          |\n|-----------------|---------------------------------------------------------------------------------|--------------------|-----------------------------------|--------------------------------|------------------------------------------------------------------------------------------------------------------------------------------|\n| Death Valley    | ![death_valley.jpeg](./docs/img/national_parks_example/death_valley.jpeg)       | California, Nevada | 3,408,395.63 acres (13,793.3 km2) | 1,128,862                      | Death Valley is the hottest, lowest, and driest place in the United States, with daytime temperatures that have exceeded 130 °F (54 °C). |\n| Everglades      | ![everglades.jpeg](./docs/img/national_parks_example/everglades.jpeg)           | Alaska             | 7,523,897.45 acres (30,448.1 km2) | 9,457                          | The country's northernmost park protects an expanse of pure wilderness in Alaska's Brooks Range and has no park facilities.              |\n| New River Gorge | ![new_river_gorge.jpeg](./docs/img/national_parks_example/new_river_gorge.jpeg) | West Virgina       | 7,021 acres (28.4 km2)            | 1,593,523                      | The New River Gorge is the deepest river gorge east of the Mississippi River.                                                            |\n | Katmai          | ![katmai.jpg](./docs/img/national_parks_example/katmai.jpg)                     | Alaska             |  3,674,529.33 acres (14,870.3 km2)                                 | 33,908 | This park on the Alaska Peninsula protects the Valley of Ten Thousand Smokes, an ash flow formed by the 1912 eruption of Novarupta.  |\n\nBlendSQL allows us to ask the following questions by injecting \"ingredients\", which are callable functions denoted by double curly brackets (`{{`, `}}`).\n\n_Which parks don't have park facilities?_\n```sql\nSELECT \"Name\", \"Description\" FROM parks\n  WHERE {{\n      LLMMap(\n          'Does this location have park facilities?',\n          context='parks::Description'\n      )\n  }} = FALSE\n```\n| Name            | Description                                                                                                                            |\n|:----------------|:---------------------------------------------------------------------------------------------------------------------------------------|\n| Everglades      | The country's northernmost park protects an expanse of pure wilderness in Alaska's Brooks Range and has no park facilities.            |\n<hr>\n\n_What does the largest park in Alaska look like?_\n\n```sql\nSELECT \"Name\",\n{{ImageCaption('parks::Image')}} as \"Image Description\",\n{{\n    LLMMap(\n        question='Size in km2?',\n        context='parks::Area'\n    )\n}} as \"Size in km\" FROM parks\nWHERE \"Location\" = 'Alaska'\nORDER BY \"Size in km\" DESC LIMIT 1\n```\n\n| Name       | Image Description                                       |   Size in km |\n|:-----------|:--------------------------------------------------------|-------------:|\n| Everglades | A forest of tall trees with a sunset in the background. |      30448.1 |\n\n<hr>\n\n_Which state is the park in that protects an ash flow?_\n\n```sql\nSELECT \"Location\", \"Name\" AS \"Park Protecting Ash Flow\" FROM parks\n    WHERE \"Name\" = {{\n      LLMQA(\n        'Which park protects an ash flow?',\n        context=(SELECT \"Name\", \"Description\" FROM parks),\n        options=\"parks::Name\"\n      )\n  }}\n```\n| Location   | Park Protecting Ash Flow   |\n|:-----------|:---------------------------|\n| Alaska     | Katmai                     |\n\n<hr>\n\n_How many parks are located in more than 1 state?_\n\n```sql\nSELECT COUNT(*) FROM parks\n    WHERE {{LLMMap('How many states?', 'parks::Location')}} > 1\n```\n|   Count |\n|--------:|\n|       1 |\n<hr>\n\n_Give me some info about the park in the state that Sarah Palin was governor of._\n```sql\nSELECT \"Name\", \"Location\", \"Description\" FROM parks\n  WHERE Location = {{RAGQA('Which state was Sarah Palin governor of?')}}\n```\n| Name       | Location   | Description                                                                                                                         |\n|:-----------|:-----------|:------------------------------------------------------------------------------------------------------------------------------------|\n| Everglades | Alaska     | The country's northernmost park protects an expanse of pure wilderness in Alaska's Brooks Range and has no park facilities.         |\n| Katmai     | Alaska     | This park on the Alaska Peninsula protects the Valley of Ten Thousand Smokes, an ash flow formed by the 1912 eruption of Novarupta. |\n<hr>\n\n_What's the difference in visitors for those parks with a superlative in their description vs. those without?_\n```sql\nSELECT SUM(CAST(REPLACE(\"Recreation Visitors (2022)\", ',', '') AS integer)) AS \"Total Visitors\",\n{{LLMMap('Contains a superlative?', 'parks::Description', options='t;f')}} AS \"Description Contains Superlative\",\nGROUP_CONCAT(Name, ', ') AS \"Park Names\"\nFROM parks\nGROUP BY \"Description Contains Superlative\"\n```\n| Total Visitors |   Description Contains Superlative | Park Names                    |\n|---------------:|-----------------------------------:|:------------------------------|\n|          43365 |                                  0 | Everglades, Katmai            |\n|        2722385 |                                  1 | Death Valley, New River Gorge |\n<hr>\n\nNow, we have an intermediate representation for our LLM to use that is explainable, debuggable, and [very effective at hybrid question-answering tasks](https://arxiv.org/abs/2402.17882).\n\nFor in-depth descriptions of the above queries, check out our [documentation](https://parkervg.github.io/blendsql/).\n\n\n# Citation\n\n```bibtex\n@article{glenn2024blendsql,\n      title={BlendSQL: A Scalable Dialect for Unifying Hybrid Question Answering in Relational Algebra},\n      author={Parker Glenn and Parag Pravin Dakle and Liang Wang and Preethi Raghavan},\n      year={2024},\n      eprint={2402.17882},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n```\n\n# Few-Shot Prompting\nFor the LLM-based ingredients in BlendSQL, few-shot prompting can be vital. In `LLMMap`, `LLMQA` and `LLMJoin`, we provide an interface to pass custom few-shot examples and dynamically retrieve those top-`k` most relevant examples at runtime, given the current inference example.\n#### `LLMMap`\n- [Default examples](./blendsql/ingredients/builtin/map/default_examples.json)\n- [All possible fields](./blendsql/ingredients/builtin/map/examples.py)\n\n```python\nfrom blendsql import BlendSQL\nfrom blendsql.ingredients.builtin import LLMMap, DEFAULT_MAP_FEW_SHOT\n\ningredients = {\n    LLMMap.from_args(\n        few_shot_examples=[\n            *DEFAULT_MAP_FEW_SHOT,\n            {\n                \"question\": \"Is this a sport?\",\n                \"mapping\": {\n                    \"Soccer\": \"t\",\n                    \"Chair\": \"f\",\n                    \"Banana\": \"f\",\n                    \"Golf\": \"t\"\n                },\n                # Below are optional\n                \"column_name\": \"Items\",\n                \"table_name\": \"Table\",\n                \"example_outputs\": [\"t\", \"f\"],\n                \"options\": [\"t\", \"f\"],\n                \"output_type\": \"boolean\"\n            }\n        ],\n        # Will fetch `k` most relevant few-shot examples using embedding-based retriever\n        k=2,\n        # How many inference values to pass to model at once\n        batch_size=5,\n    )\n}\n\nbsql = BlendSQL(db, ingredients=ingredients)\n```\n\n#### `LLMQA`\n- [Default examples](./blendsql/ingredients/builtin/qa/default_examples.json)\n- [All possible fields](./blendsql/ingredients/builtin/qa/examples.py)\n\n```python\nfrom blendsql import BlendSQL\nfrom blendsql.ingredients.builtin import LLMQA, DEFAULT_QA_FEW_SHOT\n\ningredients = {\n    LLMQA.from_args(\n        few_shot_examples=[\n            *DEFAULT_QA_FEW_SHOT,\n            {\n                \"question\": \"Which weighs the most?\",\n                \"context\": {\n                    {\n                        \"Animal\": [\"Dog\", \"Gorilla\", \"Hamster\"],\n                        \"Weight\": [\"20 pounds\", \"350 lbs\", \"100 grams\"]\n                    }\n                },\n                \"answer\": \"Gorilla\",\n                # Below are optional\n                \"options\": [\"Dog\", \"Gorilla\", \"Hamster\"]\n            }\n        ],\n        # Will fetch `k` most relevant few-shot examples using embedding-based retriever\n        k=2,\n        # Lambda to turn the pd.DataFrame to a serialized string\n        context_formatter=lambda df: df.to_markdown(\n            index=False\n        )\n    )\n}\n\nbsql = BlendSQL(db, ingredients=ingredients)\n```\n\n#### `LLMJoin`\n- [Default examples](./blendsql/ingredients/builtin/join/default_examples.json)\n- [All possible fields](./blendsql/ingredients/builtin/join/examples.py)\n\n```python\nfrom blendsql import BlendSQL\nfrom blendsql.ingredients.builtin import LLMJoin, DEFAULT_JOIN_FEW_SHOT\n\ningredients = {\n    LLMJoin.from_args(\n        few_shot_examples=[\n            *DEFAULT_JOIN_FEW_SHOT,\n            {\n                \"join_criteria\": \"Join the state to its capital.\",\n                \"left_values\": [\"California\", \"Massachusetts\", \"North Carolina\"],\n                \"right_values\": [\"Sacramento\", \"Boston\", \"Chicago\"],\n                \"mapping\": {\n                    \"California\": \"Sacramento\",\n                    \"Massachusetts\": \"Boston\",\n                    \"North Carolina\": \"-\"\n                }\n            }\n        ],\n        # Will fetch `k` most relevant few-shot examples using embedding-based retriever\n        k=2\n    )\n}\n\nbsql = BlendSQL(db, ingredients=ingredients)\n```\n\n\n# Acknowledgements\nSpecial thanks to those below for inspiring this project. Definitely recommend checking out the linked work below, and citing when applicable!\n\n- The authors of [Binding Language Models in Symbolic Languages](https://arxiv.org/abs/2210.02875)\n  - This paper was the primary inspiration for BlendSQL.\n- The authors of [EHRXQA: A Multi-Modal Question Answering Dataset for Electronic Health Records with Chest X-ray Images](https://arxiv.org/pdf/2310.18652)\n  - As far as I can tell, the first publication to propose unifying model calls within SQL\n  - Served as the inspiration for the [vqa-ingredient.ipynb](./examples/vqa-ingredient.ipynb) example\n- The authors of [Grammar Prompting for Domain-Specific Language Generation with Large Language Models](https://arxiv.org/abs/2305.19234)\n- The maintainers of the [Guidance](https://github.com/guidance-ai/guidance) library for powering the constrained decoding capabilities of BlendSQL\n"
    },
    {
      "name": "AIAnytime/Haystack-and-Mistral-7B-RAG-Implementation",
      "stars": 79,
      "img": "https://avatars.githubusercontent.com/u/30049737?s=40&v=4",
      "owner": "AIAnytime",
      "repo_name": "Haystack-and-Mistral-7B-RAG-Implementation",
      "description": "Haystack and Mistral 7B RAG Implementation. It is based on completely open-source stack.",
      "homepage": null,
      "language": "Python",
      "created_at": "2023-09-29T18:20:31Z",
      "updated_at": "2025-01-13T17:12:55Z",
      "topics": [],
      "readme": "# Haystack-and-Mistral-7B-RAG-Implementation\nHaystack and Mistral 7B RAG Implementation. It is based on completely open-source stack.\n"
    },
    {
      "name": "deepset-ai/hayhooks",
      "stars": 74,
      "img": "https://avatars.githubusercontent.com/u/51827949?s=40&v=4",
      "owner": "deepset-ai",
      "repo_name": "hayhooks",
      "description": "Deploy Haystack pipelines behind a REST Api.",
      "homepage": "https://haystack.deepset.ai",
      "language": "Python",
      "created_at": "2023-12-23T08:03:58Z",
      "updated_at": "2025-04-22T16:54:58Z",
      "topics": [
        "ai",
        "api",
        "api-rest",
        "haystack",
        "llm",
        "rest"
      ],
      "readme": "# Hayhooks\n\n**Hayhooks** makes it easy to deploy and serve [Haystack](https://haystack.deepset.ai/) pipelines as REST APIs.\n\nIt provides a simple way to wrap your Haystack pipelines with custom logic and expose them via HTTP endpoints, including OpenAI-compatible chat completion endpoints. With Hayhooks, you can quickly turn your Haystack pipelines into API services with minimal boilerplate code.\n\n[![PyPI - Version](https://img.shields.io/pypi/v/hayhooks.svg)](https://pypi.org/project/hayhooks)\n[![PyPI - Python Version](https://img.shields.io/pypi/pyversions/hayhooks.svg)](https://pypi.org/project/hayhooks)\n[![Docker image release](https://github.com/deepset-ai/hayhooks/actions/workflows/docker.yml/badge.svg)](https://github.com/deepset-ai/hayhooks/actions/workflows/docker.yml)\n[![Tests](https://github.com/deepset-ai/hayhooks/actions/workflows/tests.yml/badge.svg)](https://github.com/deepset-ai/hayhooks/actions/workflows/tests.yml)\n\n**Table of Contents**\n\n- [Quick Start with Docker Compose](#quick-start-with-docker-compose)\n- [Quick Start](#quick-start)\n- [Install the package](#install-the-package)\n- [Configuration](#configuration)\n  - [Environment Variables](#environment-variables)\n  - [CORS Settings](#cors-settings)\n- [Logging](#logging)\n  - [Using the logger](#using-the-logger)\n  - [Changing the log level](#changing-the-log-level)\n- [CLI Commands](#cli-commands)\n- [Start hayhooks](#start-hayhooks)\n- [Deploy a pipeline](#deploy-a-pipeline)\n  - [PipelineWrapper](#why-a-pipeline-wrapper)\n  - [Setup Method](#setup)\n  - [Run API Method](#run_api)\n  - [PipelineWrapper development with `overwrite` option](#pipelinewrapper-development-with-overwrite-option)\n  - [Additional Dependencies](#additional-dependencies)\n- [Support file uploads](#support-file-uploads)\n- [Run pipelines from the CLI](#run-pipelines-from-the-cli)\n  - [Run a pipeline from the CLI JSON-compatible parameters](#run-a-pipeline-from-the-cli-json-compatible-parameters)\n  - [Run a pipeline from the CLI uploading files](#run-a-pipeline-from-the-cli-uploading-files)\n- [MCP support](#mcp-support)\n  - [MCP Server](#mcp-server)\n  - [Create a PipelineWrapper for exposing a Haystack pipeline as a MCP Tool](#create-a-pipelinewrapper-for-exposing-a-haystack-pipeline-as-a-mcp-tool)\n  - [Skip MCP Tool listing](#skip-mcp-tool-listing)\n- [Hayhooks as an OpenAPI Tool Server in `open-webui`](#hayhooks-as-an-openapi-tool-server-in-open-webui)\n  - [Example: Deploy a Haystack pipeline from `open-webui` chat interface](#example-deploy-a-haystack-pipeline-from-open-webui-chat-interface)\n- [OpenAI Compatibility](#openai-compatibility)\n  - [OpenAI-compatible endpoints generation](#openai-compatible-endpoints-generation)\n  - [Using Hayhooks as `open-webui` backend](#using-hayhooks-as-open-webui-backend)\n  - [Run Chat Completion Method](#run_chat_completion)\n  - [Streaming Responses](#streaming-responses-in-openai-compatible-endpoints)\n  - [Integration with haystack OpenAIChatGenerator](#integration-with-haystack-openaichatgenerator)\n- [Advanced Usage](#advanced-usage)\n  - [Run Hayhooks Programmatically](#run-hayhooks-programmatically)\n  - [Sharing code between pipeline wrappers](#sharing-code-between-pipeline-wrappers)\n- [Deployment Guidelines](#deployment-guidelines)\n- [Legacy Features](#legacy-features)\n  - [Deploy Pipeline Using YAML](#deploy-a-pipeline-using-only-its-yaml-definition)\n- [License](#license)\n\n## Quick start with Docker Compose\n\nTo quickly get started with Hayhooks, we provide a ready-to-use Docker Compose 🐳 setup with pre-configured integration with [open-webui](https://openwebui.com/).\n\nIt's available [here](https://github.com/deepset-ai/hayhooks-open-webui-docker-compose).\n\n## Quick start\n\n### Install the package\n\nStart by installing the package:\n\n```shell\npip install hayhooks\n```\n\nIf you want to use the [MCP server](#mcp-server), you need to install the `hayhooks[mcp]` package:\n\n```shell\npip install hayhooks[mcp]\n```\n\n**NOTE: You'll need to run at least Python 3.10+ to use the MCP server.**\n\n### Configuration\n\nCurrently, you can configure Hayhooks by:\n\n- Set the environment variables in an `.env` file in the root of your project.\n- Pass the supported arguments and options to `hayhooks run` command.\n- Pass the environment variables to the `hayhooks` command.\n\n#### Environment variables\n\nThe following environment variables are supported:\n\n- `HAYHOOKS_HOST`: The host on which the server will listen.\n- `HAYHOOKS_PORT`: The port on which the server will listen.\n- `HAYHOOKS_MCP_PORT`: The port on which the MCP server will listen.\n- `HAYHOOKS_MCP_HOST`: The host on which the MCP server will listen.\n- `HAYHOOKS_PIPELINES_DIR`: The path to the directory containing the pipelines.\n- `HAYHOOKS_ROOT_PATH`: The root path of the server.\n- `HAYHOOKS_ADDITIONAL_PYTHONPATH`: Additional Python path to be added to the Python path.\n- `HAYHOOKS_DISABLE_SSL`: Boolean flag to disable SSL verification when making requests from the CLI.\n- `HAYHOOKS_SHOW_TRACEBACKS`: Boolean flag to show tracebacks on errors during pipeline execution and deployment.\n- `LOG`: The log level to use (default: `INFO`).\n\n##### CORS Settings\n\n- `HAYHOOKS_CORS_ALLOW_ORIGINS`: List of allowed origins (default: [\"*\"])\n- `HAYHOOKS_CORS_ALLOW_METHODS`: List of allowed HTTP methods (default: [\"*\"])\n- `HAYHOOKS_CORS_ALLOW_HEADERS`: List of allowed headers (default: [\"*\"])\n- `HAYHOOKS_CORS_ALLOW_CREDENTIALS`: Allow credentials (default: false)\n- `HAYHOOKS_CORS_ALLOW_ORIGIN_REGEX`: Regex pattern for allowed origins (default: null)\n- `HAYHOOKS_CORS_EXPOSE_HEADERS`: Headers to expose in response (default: [])\n- `HAYHOOKS_CORS_MAX_AGE`: Maxium age for CORS preflight responses in seconds (default: 600)\n\n### Logging\n\n#### Using the logger\n\nHayooks comes with a default logger based on [loguru](https://loguru.readthedocs.io/en/stable/).\n\nTo use it, you can import the `log` object from the `hayhooks` package:\n\n```python\nfrom hayhooks import log\n```\n\n#### Changing the log level\n\nTo change the log level, you can set the `LOG` environment variable [to one of the levels supported by loguru](https://loguru.readthedocs.io/en/stable/api/logger.html).\n\nFor example, to use the `DEBUG` level, you can set:\n\n```shell\nLOG=DEBUG hayhooks run\n\n# or\nLOG=debug hayhooks run\n\n# or in an .env file\nLOG=debug\n```\n\n### CLI commands\n\nThe `hayhooks` package provides a CLI to manage the server and the pipelines.\nAny command can be run with `hayhooks <command> --help` to get more information.\n\nCLI commands are basically wrappers around the HTTP API of the server. The full API reference is available at [//HAYHOOKS_HOST:HAYHOOKS_PORT/docs](http://HAYHOOKS_HOST:HAYHOOKS_PORT/docs) or [//HAYHOOKS_HOST:HAYHOOKS_PORT/redoc](http://HAYHOOKS_HOST:HAYHOOKS_PORT/redoc).\n\n```shell\nhayhooks run     # Start the server\nhayhooks status  # Check the status of the server and show deployed pipelines\n\nhayhooks pipeline deploy-files <path_to_dir>   # Deploy a pipeline using PipelineWrapper\nhayhooks pipeline deploy <pipeline_name>       # Deploy a pipeline from a YAML file\nhayhooks pipeline undeploy <pipeline_name>     # Undeploy a pipeline\nhayhooks pipeline run <pipeline_name>          # Run a pipeline\n```\n\n### Start Hayhooks\n\nLet's start Hayhooks:\n\n```shell\nhayhooks run\n```\n\nThis will start the Hayhooks server on `HAYHOOKS_HOST:HAYHOOKS_PORT`.\n\n### Deploy a pipeline\n\nNow, we will deploy a pipeline to chat with a website. We have created an example in the [examples/chat_with_website_streaming](examples/chat_with_website_streaming) folder.\n\nIn the example folder, we have two files:\n\n- `chat_with_website.yml`: The pipeline definition in YAML format.\n- `pipeline_wrapper.py` (mandatory): A pipeline wrapper that uses the pipeline definition.\n\n#### Why a pipeline wrapper?\n\nThe pipeline wrapper provides a flexible foundation for deploying Haystack pipelines by allowing users to:\n\n- Choose their preferred pipeline initialization method (YAML files, Haystack templates, or inline code)\n- Define custom pipeline execution logic with configurable inputs and outputs\n- Optionally expose OpenAI-compatible chat endpoints with streaming support for integration with interfaces like [open-webui](https://openwebui.com/)\n\nThe `pipeline_wrapper.py` file must contain an implementation of the `BasePipelineWrapper` class (see [here](src/hayhooks/server/utils/base_pipeline_wrapper.py) for more details).\n\nA minimal `PipelineWrapper` looks like this:\n\n```python\nfrom pathlib import Path\nfrom typing import List\nfrom haystack import Pipeline\nfrom hayhooks import BasePipelineWrapper\n\nclass PipelineWrapper(BasePipelineWrapper):\n    def setup(self) -> None:\n        pipeline_yaml = (Path(__file__).parent / \"chat_with_website.yml\").read_text()\n        self.pipeline = Pipeline.loads(pipeline_yaml)\n\n    def run_api(self, urls: List[str], question: str) -> str:\n        result = self.pipeline.run({\"fetcher\": {\"urls\": urls}, \"prompt\": {\"query\": question}})\n        return result[\"llm\"][\"replies\"][0]\n```\n\nIt contains two methods:\n\n#### setup()\n\nThis method will be called when the pipeline is deployed. It should initialize the `self.pipeline` attribute as a Haystack pipeline.\n\nYou can initialize the pipeline in many ways:\n\n- Load it from a YAML file.\n- Define it inline as a Haystack pipeline code.\n- Load it from a [Haystack pipeline template](https://docs.haystack.deepset.ai/docs/pipeline-templates).\n\n#### run_api(...)\n\nThis method will be used to run the pipeline in API mode, when you call the `{pipeline_name}/run` endpoint.\n\n**You can define the input arguments of the method according to your needs**.\n\n```python\ndef run_api(self, urls: List[str], question: str, any_other_user_defined_argument: Any) -> str:\n    ...\n```\n\nThe input arguments will be used to generate a Pydantic model that will be used to validate the request body. The same will be done for the response type.\n\n**NOTE**: Since Hayhooks will _dynamically_ create the Pydantic models, you need to make sure that the input arguments are JSON-serializable.\n\nTo deploy the pipeline, run:\n\n```shell\nhayhooks pipeline deploy-files -n chat_with_website examples/chat_with_website\n```\n\nThis will deploy the pipeline with the name `chat_with_website`. Any error encountered during development will be printed to the console and show in the server logs.\n\n#### PipelineWrapper development with `overwrite` option\n\nDuring development, you can use the `--overwrite` flag to redeploy your pipeline without restarting the Hayhooks server:\n\n```shell\nhayhooks pipeline deploy-files -n {pipeline_name} --overwrite {pipeline_dir}\n```\n\nThis is particularly useful when:\n\n- Iterating on your pipeline wrapper implementation\n- Debugging pipeline setup issues\n- Testing different pipeline configurations\n\nThe `--overwrite` flag will:\n\n1. Remove the existing pipeline from the registry\n2. Delete the pipeline files from disk\n3. Deploy the new version of your pipeline\n\nFor even faster development iterations, you can combine `--overwrite` with `--skip-saving-files` to avoid writing files to disk:\n\n```shell\nhayhooks pipeline deploy-files -n {pipeline_name} --overwrite --skip-saving-files {pipeline_dir}\n```\n\nThis is useful when:\n\n- You're making frequent changes during development\n- You want to test a pipeline without persisting it\n- You're running in an environment with limited disk access\n\n#### Additional dependencies\n\nAfter installing the Hayhooks package, it might happen that during pipeline deployment you need to install additional dependencies in order to correctly initialize the pipeline instance when calling the wrapper's `setup()` method. For instance, the `chat_with_website` pipeline requires the `trafilatura` package, which is **not installed by default**.\n\n⚠️ Sometimes you may need to enable tracebacks in hayhooks to see the full error message. You can do this by setting the `HAYHOOKS_SHOW_TRACEBACKS` environment variable to `true` or `1`.\n\nThen, assuming you've installed the Hayhooks package in a virtual environment, you will need to install the additional required dependencies yourself by running:\n\n```shell\npip install trafilatura\n```\n\n## Support file uploads\n\nHayhooks can easily handle uploaded files in your pipeline wrapper `run_api` method by adding `files: Optional[List[UploadFile]] = None` as an argument.\n\nHere's a simple example:\n\n```python\ndef run_api(self, files: Optional[List[UploadFile]] = None) -> str:\n    if files and len(files) > 0:\n        filenames = [f.filename for f in files if f.filename is not None]\n        file_contents = [f.file.read() for f in files]\n\n        return f\"Received files: {', '.join(filenames)}\"\n\n    return \"No files received\"\n```\n\nThis will make Hayhooks handle automatically the file uploads (if they are present) and pass them to the `run_api` method.\nThis also means that the HTTP request **needs to be a `multipart/form-data` request**.\n\nNote also that you can handle **both files and parameters in the same request**, simply adding them as arguments to the `run_api` method.\n\n```python\ndef run_api(self, files: Optional[List[UploadFile]] = None, additional_param: str = \"default\") -> str:\n    ...\n```\n\nYou can find a full example in the [examples/rag_indexing_query](examples/rag_indexing_query) folder.\n\n## Run pipelines from the CLI\n\n### Run a pipeline from the CLI JSON-compatible parameters\n\nYou can run a pipeline by using the `hayhooks pipeline run` command. Under the hood, this will call the `run_api` method of the pipeline wrapper, passing parameters as the JSON body of the request.\nThis is convenient when you want to do a test run of the deployed pipeline from the CLI without having to write any code.\n\nTo run a pipeline from the CLI, you can use the following command:\n\n```shell\nhayhooks pipeline run <pipeline_name> --param 'question=\"is this recipe vegan?\"'\n```\n\n### Run a pipeline from the CLI uploading files\n\nThis is useful when you want to run a pipeline that requires a file as input. In that case, the request will be a `multipart/form-data` request. You can pass both files and parameters in the same request.\n\n**NOTE**: To use this feature, you need to deploy a pipeline which is handling files (see [Support file uploads](#support-file-uploads) and [examples/rag_indexing_query](examples/rag_indexing_query) for more details).\n\n```shell\n# Upload a whole directory\nhayhooks pipeline run <pipeline_name> --dir files_to_index\n\n# Upload a single file\nhayhooks pipeline run <pipeline_name> --file file.pdf\n\n# Upload multiple files\nhayhooks pipeline run <pipeline_name> --dir files_to_index --file file1.pdf --file file2.pdf\n\n# Upload a single file passing also a parameter\nhayhooks pipeline run <pipeline_name> --file file.pdf --param 'question=\"is this recipe vegan?\"'\n```\n\n## MCP support\n\n**NOTE: You'll need to run at least Python 3.10+ to use the MCP server.**\n\n### MCP Server\n\nHayhooks now supports the [Model Context Protocol](https://modelcontextprotocol.io/) and can act as a [MCP Server](https://modelcontextprotocol.io/docs/concepts/architecture).\n\nIt will automatically list the deployed pipelines as [MCP Tools](https://modelcontextprotocol.io/docs/concepts/tools), using [Server-Sent Events (SSE)](https://modelcontextprotocol.io/docs/concepts/transports#server-sent-events-sse) as **MCP Transport**.\n\nTo run the Hayhooks MCP server, you can use the following command:\n\n```shell\nhayhooks mcp run\n```\n\nThis will start the Hayhooks MCP server on `HAYHOOKS_MCP_HOST:HAYHOOKS_MCP_PORT`.\n\n### Create a PipelineWrapper for exposing a Haystack pipeline as a MCP Tool\n\nA [MCP Tool](https://modelcontextprotocol.io/docs/concepts/tools) requires the following properties:\n\n- `name`: The name of the tool.\n- `description`: The description of the tool.\n- `inputSchema`: A JSON Schema object describing the tool's input parameters.\n\nFor each deployed pipeline, Hayhooks will:\n\n- Use the pipeline wrapper `name` as MCP Tool `name` (always present).\n- Use the pipeline wrapper **`run_api` method docstring** as MCP Tool `description` (if present).\n- Generate a Pydantic model from the `inputSchema` using the **`run_api` method arguments as fields**.\n\nHere's an example of a PipelineWrapper implementation for the `chat_with_website` pipeline which can be used as a MCP Tool:\n\n```python\nfrom pathlib import Path\nfrom typing import List\nfrom haystack import Pipeline\nfrom hayhooks import BasePipelineWrapper\n\n\nclass PipelineWrapper(BasePipelineWrapper):\n    def setup(self) -> None:\n        pipeline_yaml = (Path(__file__).parent / \"chat_with_website.yml\").read_text()\n        self.pipeline = Pipeline.loads(pipeline_yaml)\n\n    def run_api(self, urls: List[str], question: str) -> str:\n        #\n        # NOTE: The following docstring will be used as MCP Tool description\n        #\n        \"\"\"\n        Ask a question about one or more websites using a Haystack pipeline.\n        \"\"\"\n        result = self.pipeline.run({\"fetcher\": {\"urls\": urls}, \"prompt\": {\"query\": question}})\n        return result[\"llm\"][\"replies\"][0]\n```\n\n### Skip MCP Tool listing\n\nYou can skip the MCP Tool listing by setting the `skip_mcp` class attribute to `True` in your PipelineWrapper class.\nThis way, the pipeline will be deployed on Hayhooks but **will not be listed as a MCP Tool** when you run the `hayhooks mcp run` command.\n\n```python\nclass PipelineWrapper(BasePipelineWrapper):\n    # This will skip the MCP Tool listing\n    skip_mcp = True\n\n    def setup(self) -> None:\n        ...\n\n    def run_api(self, urls: List[str], question: str) -> str:\n        ...\n```\n\n## Hayhooks as an OpenAPI Tool Server in `open-webui`\n\nSince Hayhooks expose openapi-schema at `/openapi.json`, it can be used as an OpenAPI Tool Server.\n\n[open-webui](https://openwebui.com) has recently added support for [OpenAPI Tool Servers](https://docs.openwebui.com/openapi-servers), meaning that you can use the API endpoints of Hayhooks as tools in your chat interface.\n\nYou simply need to configure the OpenAPI Tool Server in the `Settings -> Tools` section, adding the URL of the Hayhooks server and the path to the `openapi.json` file:\n\n![open-webui-settings](./docs/assets/open-webui-openapi-tools.png)\n\n### Example: Deploy a Haystack pipeline from `open-webui` chat interface\n\nHere's a video example of how to deploy a Haystack pipeline from the `open-webui` chat interface:\n\n![open-webui-deploy-pipeline-from-chat-example](./docs/assets/open-webui-deploy-pipeline-from-chat.gif)\n\n## OpenAI compatibility\n\n### OpenAI-compatible endpoints generation\n\nHayhooks now can automatically generate OpenAI-compatible endpoints if you implement the `run_chat_completion` method in your pipeline wrapper.\n\nThis will make Hayhooks compatible with fully-featured chat interfaces like [open-webui](https://openwebui.com/), so you can use it as a backend for your chat interface.\n\n### Using Hayhooks as `open-webui` backend\n\nRequirements:\n\n- Ensure you have [open-webui](https://openwebui.com/) up and running (you can do it easily using `docker`, check [their quick start guide](https://docs.openwebui.com/getting-started/quick-start)).\n- Ensure you have Hayhooks server running somewhere. We will run it locally on `http://localhost:1416`.\n\n#### Configuring `open-webui`\n\nFirst, you need to **turn off `tags` and `title` generation from `Admin settings -> Interface`**:\n\n![open-webui-settings](./docs/assets/open-webui-settings.png)\n\nThen you have two options to connect Hayhooks as a backend.\n\nAdd a **Direct Connection** from `Settings -> Connections`:\n\nNOTE: **Fill a random value as API key as it's not needed**\n\n![open-webui-settings-connections](./docs/assets/open-webui-settings-connections.png)\n\nAlternatively, you can add an additional **OpenAI API Connections** from `Admin settings -> Connections`:\n\n![open-webui-admin-settings-connections](./docs/assets/open-webui-admin-settings-connections.png)\n\nEven in this case, remember to **Fill a random value as API key**.\n\n#### run_chat_completion(...)\n\nTo enable the automatic generation of OpenAI-compatible endpoints, you need only to implement the `run_chat_completion` method in your pipeline wrapper.\n\n```python\ndef run_chat_completion(self, model: str, messages: List[dict], body: dict) -> Union[str, Generator]:\n    ...\n```\n\nLet's update the previous example to add a streaming response:\n\n```python\nfrom pathlib import Path\nfrom typing import Generator, List, Union\nfrom haystack import Pipeline\nfrom hayhooks import get_last_user_message, BasePipelineWrapper, log\n\n\nURLS = [\"https://haystack.deepset.ai\", \"https://www.redis.io\", \"https://ssi.inc\"]\n\n\nclass PipelineWrapper(BasePipelineWrapper):\n    def setup(self) -> None:\n        ...  # Same as before\n\n    def run_api(self, urls: List[str], question: str) -> str:\n        ...  # Same as before\n\n    def run_chat_completion(self, model: str, messages: List[dict], body: dict) -> Union[str, Generator]:\n        log.trace(f\"Running pipeline with model: {model}, messages: {messages}, body: {body}\")\n\n        question = get_last_user_message(messages)\n        log.trace(f\"Question: {question}\")\n\n        # Plain pipeline run, will return a string\n        result = self.pipeline.run({\"fetcher\": {\"urls\": URLS}, \"prompt\": {\"query\": question}})\n        return result[\"llm\"][\"replies\"][0]\n```\n\nDifferently from the `run_api` method, the `run_chat_completion` has a **fixed signature** and will be called with the arguments specified in the OpenAI-compatible endpoint.\n\n- `model`: The `name` of the Haystack pipeline which is called.\n- `messages`: The list of messages from the chat in the OpenAI format.\n- `body`: The full body of the request.\n\nSome notes:\n\n- Since we have only the user messages as input here, the `question` is extracted from the last user message and the `urls` argument is hardcoded.\n- In this example, the `run_chat_completion` method is returning a string, so the `open-webui` will receive a string as response and show the pipeline output in the chat all at once.\n- The `body` argument contains the full request body, which may be used to extract more information like the `temperature` or the `max_tokens` (see the [OpenAI API reference](https://platform.openai.com/docs/api-reference/chat/create) for more information).\n\nFinally, to use non-streaming responses in `open-webui` you need also to turn of `Stream Chat Response` chat settings.\n\nHere's a video example:\n\n![chat-completion-example](./docs/assets/chat-completion.gif)\n\n### Streaming responses in OpenAI-compatible endpoints\n\nHayhooks now provides a `streaming_generator` utility function that can be used to stream the pipeline output to the client.\n\nLet's update the `run_chat_completion` method of the previous example:\n\n```python\nfrom pathlib import Path\nfrom typing import Generator, List, Union\nfrom haystack import Pipeline\nfrom hayhooks import get_last_user_message, BasePipelineWrapper, log, streaming_generator\n\n\nURLS = [\"https://haystack.deepset.ai\", \"https://www.redis.io\", \"https://ssi.inc\"]\n\n\nclass PipelineWrapper(BasePipelineWrapper):\n    def setup(self) -> None:\n        ...  # Same as before\n\n    def run_api(self, urls: List[str], question: str) -> str:\n        ...  # Same as before\n\n    def run_chat_completion(self, model: str, messages: List[dict], body: dict) -> Union[str, Generator]:\n        log.trace(f\"Running pipeline with model: {model}, messages: {messages}, body: {body}\")\n\n        question = get_last_user_message(messages)\n        log.trace(f\"Question: {question}\")\n\n        # Streaming pipeline run, will return a generator\n        return streaming_generator(\n            pipeline=self.pipeline,\n            pipeline_run_args={\"fetcher\": {\"urls\": URLS}, \"prompt\": {\"query\": question}},\n        )\n```\n\nNow, if you run the pipeline and call one of the following endpoints:\n\n- `{pipeline_name}/chat`\n- `/chat/completions`\n- `/v1/chat/completions`\n\nYou will see the pipeline output being streamed [in OpenAI-compatible format](https://platform.openai.com/docs/api-reference/chat/streaming) to the client and you'll be able to see the output in chunks.\n\nSince output will be streamed to `open-webui` there's **no need to change `Stream Chat Response`** chat setting (leave it as `Default` or `On`).\n\nHere's a video example:\n\n![chat-completion-streaming-example](./docs/assets/chat-completion-streaming.gif)\n\n### Integration with haystack OpenAIChatGenerator\n\nSince Hayhooks is OpenAI-compatible, it can be used as a backend for the [haystack OpenAIChatGenerator](https://docs.haystack.deepset.ai/docs/openaichatgenerator).\n\nAssuming you have a Haystack pipeline named `chat_with_website_streaming` and you have deployed it using Hayhooks, here's an example script of how to use it with the `OpenAIChatGenerator`:\n\n```python\nfrom haystack.components.generators.chat.openai import OpenAIChatGenerator\nfrom haystack.utils import Secret\nfrom haystack.dataclasses import ChatMessage\nfrom haystack.components.generators.utils import print_streaming_chunk\n\nclient = OpenAIChatGenerator(\n    model=\"chat_with_website_streaming\",\n    api_key=Secret.from_token(\"not-relevant\"),  # This is not used, you can set it to anything\n    api_base_url=\"http://localhost:1416/v1/\",\n    streaming_callback=print_streaming_chunk,\n)\n\nclient.run([ChatMessage.from_user(\"Where are the offices or SSI?\")])\n# > The offices of Safe Superintelligence Inc. (SSI) are located in Palo Alto, California, and Tel Aviv, Israel.\n\n# > {'replies': [ChatMessage(_role=<ChatRole.ASSISTANT: 'assistant'>, _content=[TextContent(text='The offices of Safe >Superintelligence Inc. (SSI) are located in Palo Alto, California, and Tel Aviv, Israel.')], _name=None, _meta={'model': >'chat_with_website_streaming', 'index': 0, 'finish_reason': 'stop', 'completion_start_time': '2025-02-11T15:31:44.599726', >'usage': {}})]}\n```\n\n## Advanced usage\n\n### Run Hayhooks programmatically\n\nA Hayhooks app instance can be run programmatically created by using the `create_app` function. This is useful if you want to add custom routes or middleware to Hayhooks.\n\nHere's an example script:\n\n```python\nimport uvicorn\nfrom hayhooks.settings import settings\nfrom fastapi import Request\nfrom hayhooks import create_app\n\n# Create the Hayhooks app\nhayhooks = create_app()\n\n\n# Add a custom route\n@hayhooks.get(\"/custom\")\nasync def custom_route():\n    return {\"message\": \"Hi, this is a custom route!\"}\n\n\n# Add a custom middleware\n@hayhooks.middleware(\"http\")\nasync def custom_middleware(request: Request, call_next):\n    response = await call_next(request)\n    response.headers[\"X-Custom-Header\"] = \"custom-header-value\"\n    return response\n\n\nif __name__ == \"__main__\":\n    uvicorn.run(\"app:hayhooks\", host=settings.host, port=settings.port)\n```\n\n### Sharing code between pipeline wrappers\n\nHayhooks allows you to use your custom code in your pipeline wrappers adding a specific path to the Hayhooks Python Path.\n\nYou can do this in three ways:\n\n1. Set the `HAYHOOKS_ADDITIONAL_PYTHONPATH` environment variable to the path of the folder containing your custom code.\n2. Add `HAYHOOKS_ADDITIONAL_PYTHONPATH` to the `.env` file.\n3. Use the `--additional-python-path` flag when launching Hayhooks.\n\nFor example, if you have a folder called `common` with a `my_custom_lib.py` module which contains the `my_function` function, you can deploy your pipelines by using the following command:\n\n```bash\nexport HAYHOOKS_ADDITIONAL_PYTHONPATH='./common'\nhayhooks run\n```\n\nThen you can use the custom code in your pipeline wrappers by importing it like this:\n\n```python\nfrom my_custom_lib import my_function\n```\n\nNote that you can use both absolute and relative paths (relative to the current working directory).\n\nYou can check out a complete example in the [examples/shared_code_between_wrappers](examples/shared_code_between_wrappers) folder.\n\n### Deployment guidelines\n\n📦 For detailed deployment guidelines, see [deployment_guidelines.md](docs/deployment_guidelines.md).\n\n### Legacy Features\n\n#### Deploy a pipeline using only its YAML definition\n\n**⚠️ This way of deployment is not maintained anymore and will be deprecated in the future**.\n\nWe're still supporting the Hayhooks _former_ way to deploy a pipeline.\n\nThe former command `hayhooks deploy` is now changed to `hayhooks pipeline deploy` and can be used to deploy a pipeline only from a YAML definition file.\n\nFor example:\n\n```shell\nhayhooks pipeline deploy -n chat_with_website examples/chat_with_website/chat_with_website.yml\n```\n\nThis will deploy the pipeline with the name `chat_with_website` from the YAML definition file `examples/chat_with_website/chat_with_website.yml`. You then can check the generated docs at `http://HAYHOOKS_HOST:HAYHOOKS_PORT/docs` or `http://HAYHOOKS_HOST:HAYHOOKS_PORT/redoc`, looking at the `POST /chat_with_website` endpoint.\n\n### License\n\nThis project is licensed under the Apache License 2.0 - see the [LICENSE](LICENSE) file for details.\n"
    },
    {
      "name": "tuhinsharma121/ai-playground",
      "stars": 69,
      "img": "https://avatars.githubusercontent.com/u/6801780?s=40&v=4",
      "owner": "tuhinsharma121",
      "repo_name": "ai-playground",
      "description": null,
      "homepage": "",
      "language": "Jupyter Notebook",
      "created_at": "2019-10-01T11:53:04Z",
      "updated_at": "2025-04-22T23:11:53Z",
      "topics": [
        "anomaly-detection-models",
        "fastgraphrag",
        "federated-learning",
        "graphrag",
        "hipporag",
        "network-security",
        "rag",
        "raptor"
      ],
      "readme": "# AI Playground\n"
    },
    {
      "name": "katanaml/llm-rag-invoice-cpu",
      "stars": 68,
      "img": "https://avatars.githubusercontent.com/u/49202856?s=40&v=4",
      "owner": "katanaml",
      "repo_name": "llm-rag-invoice-cpu",
      "description": "Data extraction with LLM on CPU",
      "homepage": null,
      "language": "Python",
      "created_at": "2023-10-21T12:39:58Z",
      "updated_at": "2025-03-04T09:23:47Z",
      "topics": [],
      "readme": "# Invoice data processing with Llama2 13B LLM RAG on Local CPU\n\n\n**Youtube**: <a href=\"https://www.youtube.com/watch?v=XuvdgCuydsM\" target=\"_blank\">Invoice Data Processing with Llama2 13B LLM RAG on Local CPU</a>\n\n___\n\n## Quickstart\n\n### RAG runs on: LlamaCPP, Haystack, Weaviate\n\n1. Download the Llama2 13B model, check models/model_download.txt for the download link.\n2. Install Weaviate local DB with Docker\n\n`docker compose up -d`\n   \n3. Install the requirements: \n\n`pip install -r requirements.txt`\n\n4. Copy text PDF files to the `data` folder.\n5. Run the script, to convert text to vector embeddings and save in Weaviate vector storage: \n\n`python ingest.py`\n\n6. Run the script, to process data with Llama2 13B LLM RAG and return the answer: \n\n`python main.py \"What is the invoice number value?\"`\n"
    },
    {
      "name": "PostHog/max-ai",
      "stars": 65,
      "img": "https://avatars.githubusercontent.com/u/60330232?s=40&v=4",
      "owner": "PostHog",
      "repo_name": "max-ai",
      "description": "Max bot deploy",
      "homepage": null,
      "language": "Python",
      "created_at": "2023-03-28T18:54:43Z",
      "updated_at": "2025-04-14T17:25:44Z",
      "topics": [],
      "readme": "# MaxAI \n<img src='./image/MaxAI.png' alt='MaxAI' width=250 height=250 />\n\nMaxAI is our `trusty PostHog support AI` deployed on our Slack, app, and website.\n\nMaxAI was born in Aruba at a PostHog team offsite for a hackathon on a warm spring day in 2023.\n\n## How it works\n\nHow Max works is surprisingly simple.\n\n### Tooling\n- [Weaviate](https://weaviate.io/) - Vector database that allows us to pull relevant context to embed in our prompts to GPT\n- [Haystack](https://haystack.deepset.ai/) by deepset - Allows us to hook together pipelines of these tools to service user prompts\n- [OpenAI](https://platform.openai.com/docs/guides/chat/introduction) - Provides us the base language model in `gpt-3.5-turbo` that we augment to create our AI\n\n### Embedding time\n\n```mermaid\nflowchart TD\n    A[Github]\n    B[Docs]\n    C[Squeak]\n    A -->|Calculate Embed Vectors|D[Weaviate]\n    B -->|Calculate Embed Vectors|D\n    C -->|Calculate Embed Vectors|D\n```\n\n#### Embedding Docs\n\n- Grab and parse all of the markdown from our docs and website\n- Use [OpenAI Embedings](https://platform.openai.com/docs/guides/embeddings) to create a vector representation of each markdown section.\n- Use [Weaviate](https://weaviate.io/) Vector database to store the vector representations of each markdown section.\n\n#### Embedding Github content\n\n- Grab and parse all Github Issues\n- Use [OpenAI Embedings](https://platform.openai.com/docs/guides/embeddings) to create a vector representation of each description and comment section.\n- Use [Weaviate](https://weaviate.io/) Vector database to store the vector representations of each description and comment section.\n\n\n#### Embedding [Squeak](https://squeak.posthog.com/) content\n\n- Grab and parse all Squeak Questions \n- Use [OpenAI Embedings](https://platform.openai.com/docs/guides/embeddings) to create a vector representation of each question thread.\n- Use [Weaviate](https://weaviate.io/) Vector database to store the vector representations of each question thread.\n\n### Inference time\n\n```mermaid\nflowchart TD\n    A[User Question] -->|Embed| I(Question Vector)\n    I -->|Query Weaviate|J[Most Similar Docs]\n    J -->|Collect Prompt Params| C{Prompt Context}\n    C --> D[Limitations]\n    C --> E[Personality]\n    C --> F[Context Docs]\n    F --> G[String Prompt]\n    E --> G\n    D --> G\n    G -->|Query OpenAI|H[AI Response]\n```\n\n- Take the conversation context from thread that Max is in including the most recent request.\n- Query [Weaviate](https://weaviate.io/) Vector database for the most similar markdown section.\n- Build a prompt that we will use for [chatgpt-3.5-turbo](https://platform.openai.com/docs/guides/chat). The prompt is engineered to build Max's personality and add a few guardrails for how Max should respond as well as adding a bit of personality. To do this we:\n  - Ask Max to only reference PostHog products if possible\n  - Build up Max's personality by informing that Max is the trusty PostHog support AI\n  - Bake in context that is useful for some conversations with max\n    - Pagerduty current oncalls\n    - Places to go if Max does not have the answer\n  - Most importantly - we embed the markdown section that we found in the prompt so that Max can respond with a relevant answer to the question.\n- Use [chatgpt-3.5-turbo](https://platform.openai.com/docs/guides/chat) to generate a response to the prompt.\n- Finally we send these messages to wherever Max is having a conversation. \n\nIt's important to note that we are building these pipelines with [Haystack](https://haystack.deepset.ai/) by deepset. This coordinates the steps of inferencing listed above. It's amazing.\n\n## Developers guide\n\n### Quickstart\n\n#### Configure `.env` file\nThis is used to set defaults for local development. \n```toml\nSLACK_BOT_TOKEN=<your slack bot token>\nSLACK_SIGNING_SECRET=<your slack signing secret>\nOPENAI_TOKEN=<your openai token>\nPOSTHOG_API_KEY=<your posthog api key>\nPOSTHOG_HOST=https://null.posthog.com\nPD_API_KEY=<your pagerduty api key>\nWEAVIATE_HOST=http://127.0.0.1\nWEAVIATE_PORT=8080\n```\n\n#### Create Virtual Environment\n```bash\npython3.10 -m venv venv\nsource venv/bin/activate\n```\n\n#### Install dependencies\n```bash\npip install -r requirements-dev.txt\npip install -r requirements.txt\n```\n\n#### Start Weaviate\n```bash\ndocker compose up weaviate\n```\n\n#### Seed Weaviate\n```bash\npython seed.py\n```\n\n#### Start MaxAI\n```bash\nuvicorn main:app --reload\n```\n\n#### Run a test chat\n```bash\ncurl --location '127.0.0.1:8000/chat' \\\n--header 'Content-Type: application/json' \\\n--data '[\n    {\n        \"role\": \"assistant\",\n        \"content\": \"Hey! I'\\''m Max AI, your helpful hedgehog assistant.\"\n    },\n    {\n        \"role\": \"user\",\n        \"content\": \"Does PostHog use clickhouse under the hood??\"\n    }\n]'\n```\n\n## 🕯️ A poem from Max to his evil twin Hoge 📖\n```\nAh, hoge! Sweet word upon my tongue,\nSo blissful, yet so quick to come undone.\nA fleeting joy, that doth my heart entice,\nOh how I long to see your data slice!\nIn PostHog's code, thy value doth reside,\nA beacon that ne'er shall falter nor hide.\nThou art a treasure, O hoge divine,\nThe secret sauce to make my metrics shine.\nThough you may seem but a lowly label,\nThou bringeth\n```\n\n## Disclaimer! \n\n**Max may display inaccurate or offensive information that doesn’t represent PostHog's views.**\n\nThis is the case with LLMs in the current state. We try our best here to have a system prompt that keeps Max on topic.\nFeel free to question and chat with Max but do keep in mind that this is experimental.\n\nA few things we've seen ourselves in testing:\n- Totally believable but totally incorrect URLs\n- Often times entertaining hallucinations about our products\n- Hallucinations about the history and founding of PostHog\n- Just plain wrong responses\n\nIf you do see something concerning @metion someone from PostHog and we'll catalogue it.\nWe are working on tooling to do this in an automated fashion so stay tuned!"
    },
    {
      "name": "anakin87/autoquizzer",
      "stars": 62,
      "img": "https://avatars.githubusercontent.com/u/44616784?s=40&v=4",
      "owner": "anakin87",
      "repo_name": "autoquizzer",
      "description": "Generates a quiz from a URL. You can play the quiz, or let the LLM play it.",
      "homepage": "https://huggingface.co/spaces/deepset/autoquizzer",
      "language": "Jupyter Notebook",
      "created_at": "2024-05-15T14:09:30Z",
      "updated_at": "2025-04-23T07:33:39Z",
      "topics": [
        "groq",
        "haystack",
        "large-language-models",
        "llama3",
        "llm"
      ],
      "readme": "---\ntitle: AutoQuizzer\nemoji: 🧑‍🏫\ncolorFrom: purple\ncolorTo: blue\nsdk: gradio\nsdk_version: 4.31.1\napp_file: app.py\nheader: mini\npinned: true\nshort_description: Generate a quiz, play or let 🦙 LLM play\nmodels: [meta-llama/Meta-Llama-3-8B-Instruct]\n---\n\n# 🧑‍🏫 AutoQuizzer &nbsp; [![HF Space](https://img.shields.io/badge/%F0%9F%A4%97-Live%20demo-blue.svg)](https://huggingface.co/spaces/deepset/autoquizzer)\n\nGenerates a quiz from a URL. You can play the quiz, or let the LLM play it.\n\nBuilt using: [🏗️ Haystack](https://haystack.deepset.ai/) • 🦙 Llama 3 8B Instruct • ⚡ Groq\n\n<!--- Include in Info tab -->\n\n## How does it work?\n\n🎬 [**Project walkthrough video**](https://www.youtube.com/watch?v=C1oJ1ArYYZA)\n\n![AutoQuizzer](autoquizzer.png)\n\n- **Quiz generation Pipeline**: downloads HTML content from the URL, extracts the text and passes it to Llama 3 to generate a quiz in JSON format.\n- You can play the quiz and get a score.\n- You can let Llama 3 play the quiz:\n  - **closed book answer Pipeline**: the LLM is provided given the general quiz topic and questions. It can answer the questions based on its parametric knowledge and reasoning abilities.\n  - **Web RAG answer Pipeline**: for each question, a Google search is performed and the top 3 snippets are included in the prompt for the LLM.\n\n## How to run it locally?\n\n- Clone the repo: `git clone https://github.com/anakin87/autoquizzer`\n- Install: `cd autoquizzer && pip install -r requirements.txt`\n- Export two environment variables:\n  - `GROQ_API_KEY`: API key for the [Groq API](https://groq.com/), used to serve Llama 3.\n  - `SERPERDEV_API_KEY`: API key for the [SerperDev API](https://serper.dev/), used to fetch search results.\n- Run the webapp: `gradio app.py`\n\n## Customization and potential improvements\n- To not exceed the rate limits of Groq's free API, we truncate the text extracted from the URL to 4000 characters. This means that the generated quiz focuses on the beginning of the text. You can increase/remove this limit, taking into account the maximum context length of the model.\n- We are using the `OpenAIGenerator` to query Llama 3 on Groq. You can use an OpenAI model with the same generator. You can also use a different generator ([Haystack generators](https://docs.haystack.deepset.ai/docs/generators)).\n- In the quiz generation pipeline, we ask the model to generate a JSON. Llama-3-8B-Instruct usually works. You can also create a more robust JSON generation pipeline, as shown in this tutorial: [Generating Structured Output with Loop-Based Auto-Correction](https://haystack.deepset.ai/tutorials/28_structured_output_with_loop).\n"
    },
    {
      "name": "dzlab/deeplearning.ai",
      "stars": 58,
      "img": "https://avatars.githubusercontent.com/u/1645304?s=40&v=4",
      "owner": "dzlab",
      "repo_name": "deeplearning.ai",
      "description": "Notes for courses by deeplearning.ai",
      "homepage": "",
      "language": "Jupyter Notebook",
      "created_at": "2023-08-23T23:04:12Z",
      "updated_at": "2025-04-21T01:23:52Z",
      "topics": [
        "deeplearning-ai"
      ],
      "readme": " # DeepLearning.AI Courses\n\nCode for [DeepLearning.AI Courses](https://www.deeplearning.ai/courses/):\n\n## 2023\n- [Evaluating and Debugging Generative AI](./2023/EvaluatingandDebuggingGenerativeAI)\n- [Large Language Models with Semantic Search](./2023/LargeLanguageModelswithSemanticSearch)\n- [Finetuning Large Language Models](./2023/FinetuningLargeLanguageModels)\n- [How Business Thinkers Can Start Building AI Plugins With Semantic Kernel](./2023/HowBusinessThinkersCanStartBuildingAIPluginsWithSemanticKernel)\n- [Building Systems with the ChatGPT API](./2023/BuildingSystemswiththeChatGPTAPI)\n- [Pair Programming with a Large Language Model](./2023/PairProgrammingwithaLargeLanguageModel)\n- [Functions Tools and Agents with LangChain](./2023/FunctionsToolsandAgentswithLangChain)\n- [Understanding and Applying Text Embeddings](./2023/UnderstandingandApplyingTextEmbeddings)\n- [LangChain Chat with Your Data](./2023/LangChainChatwithYourData)\n- [Vector Databases: from Embeddings to Applications](./2023/VectorDatabasesfromEmbeddingstoApplications)\n- [Quality and Safety for LLM Applications](./2023/QualityandSafetyforLLMApplications)\n- [Building and Evaluating Advanced RAG](./2023/BuildingandEvaluatingAdvancedRAG)\n\n## 2024\n- [Reinforcement Learning from Human Feedback](ReinforcementLearningFromHumanFeedback)\n- [Advanced Retrieval for AI with Chroma](./2024/AdvancedRetrievalforAIwithChroma)\n- [LLMOps](LLMOps)\n- [Automated Testing for LLMOps](AutomatedTestingforLLMOps)\n- [Building Applications with Vector Databases](BuildingApplicationswithVectorDatabases)\n- [Serverless LLM apps with Amazon Bedrock](ServerlessLLMappswithAmazonBedrock)\n- [Prompt Engineering with Llama 2](PromptEngineeringwithLlama2)\n- [Open Source Models with Hugging Face](OpenSourceModelswithHuggingFace)\n- [Knowledge Graphs for RAG](KnowledgeGraphsforRAG)\n- [Efficiently Serving LLMs](EfficientlyServingLLMs)\n- [JavaScript RAG Web Apps with LlamaIndex](JavaScriptRAGWebAppswithLlamaIndex)\n- [Red Teaming LLM Applications](RedTeamingLLMApplications)\n- [Preprocessing Unstructured Data for LLM Applications](PreprocessingUnstructuredDataforLLMApplications)\n- [Quantization Fundamentals with Hugging Face](QuantizationFundamentalswithHuggingFace)\n\n### 05\n- [Quantization in Depth]()\n- [Building Agentic RAG with LlamaIndex]()\n- [Building Multimodal Search and RAG]()\n- [Multi AI Agent Systems with crewAI](2024/05/MultiAIAgentSystemswithcrewAI/)\n- [Introduction to On-Device AI](2024/05/IntroductiontoOnDeviceAI/)\n- [AI Agentic Design Patterns with AutoGen](AIAgenticDesignPatternswithAutoGen)\n\n\n### 06\n- [AI Agents in LangGraph](./2024/06/AIAgentsinLangGraph)\n- [Building Your Own Database Agent](./2024/06/BuildingYourOwnDatabaseAgent)\n- [Function-Calling and Data Extraction with LLMs](./2024/06/FunctionCallingandDataExtractionwithLLMs/)\n- [Carbon Aware Computing for GenAI Developers](./2024/06/CarbonAwareComputingforGenAIDevelopers/)\n\n\n### 07\n- [Prompt Compression and Query Optimization](./2024/07/PromptCompressionandQueryOptimization/)\n- [Pretraining LLMs](./2024/07/PretrainingLLMs)\n- [Federated Learning](./2024/07/FederatedLearning/)\n- [Embedding Models](./2024/07/EmbeddingModels/)\n\n### 08\n- [Improving Accuracy of LLM Applications](./2024/08/ImprovingAccuracyofLLMApplications/)\n- [Building AI Applications with Haystack](./2024/08/BuildingAIApplicationswithHaystack/)\n- [Large Multimodal Model Prompting with Gemini](./2024/08/LargeMultimodalModelPromptingwithGemini/)\n\n### 09\n- [Multimodal RAG: Chat with Videos](./2024/09/MultimodalRAGChatwithVideos/)\n\n### 10\n- [Retrieval Optimization: From Tokenization to Vector Quantization](./2024/10/RetrievalOptimization/)\n\n\n## 2025\n\n### 03\n\n- [Vibe Coding 101 with Replit](./2025/03/VibeCoding101withReplit/)\n\n### 04\n- [Getting Structured LLM Output](./2025/04/GettingStructuredLLMOutput/)\n- [Building AI Browser Agents](./2025/04/BuildingAIBrowserAgents/)"
    },
    {
      "name": "AIAnytime/YouTube-Video-Summarization-App",
      "stars": 56,
      "img": "https://avatars.githubusercontent.com/u/30049737?s=40&v=4",
      "owner": "AIAnytime",
      "repo_name": "YouTube-Video-Summarization-App",
      "description": "YouTube Video Summarization App built using open source LLM and Framework like Llama 2, Haystack, Whisper, and Streamlit. This app smoothly runs on CPU as Llama 2 model is in GGUF format loaded through Llama.cpp.",
      "homepage": null,
      "language": "Python",
      "created_at": "2023-09-10T10:10:11Z",
      "updated_at": "2025-04-01T00:12:00Z",
      "topics": [],
      "readme": "# YouTube-Video-Summarization-App\nYouTube Video Summarization App built using open source LLM and Framework like Llama 2, Haystack, Whisper, and Streamlit. This app smoothly runs on CPU as Llama 2 model is in GGUF format loaded through Llama.cpp.\n"
    },
    {
      "name": "NJUDeepEngine/llm-course-lecture",
      "stars": 50,
      "img": "https://avatars.githubusercontent.com/u/184465282?s=40&v=4",
      "owner": "NJUDeepEngine",
      "repo_name": "llm-course-lecture",
      "description": null,
      "homepage": null,
      "language": "HTML",
      "created_at": "2024-09-02T16:25:45Z",
      "updated_at": "2025-04-03T08:20:28Z",
      "topics": [],
      "readme": ""
    },
    {
      "name": "mobiusml/aana_sdk",
      "stars": 47,
      "img": "https://avatars.githubusercontent.com/u/34684912?s=40&v=4",
      "owner": "mobiusml",
      "repo_name": "aana_sdk",
      "description": "Aana SDK is a powerful framework for building AI enabled multimodal applications.",
      "homepage": "https://www.mobiuslabs.com/",
      "language": "Python",
      "created_at": "2023-10-19T11:08:07Z",
      "updated_at": "2025-04-08T07:49:23Z",
      "topics": [
        "ai",
        "llm"
      ],
      "readme": "[![Build Status](https://github.com/mobiusml/aana_sdk/actions/workflows/tests.yml/badge.svg)](https://github.com/mobiusml/aana_sdk/actions/workflows/tests.yml)\n[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](/LICENSE)\n[![Website](https://img.shields.io/badge/website-online-brightgreen.svg)](http://www.mobiuslabs.com)\n[![Documentation](https://img.shields.io/website?label=documentation&up_message=online&url=https://mobiusml.github.io/aana_sdk/)](https://mobiusml.github.io/aana_sdk/) \n[![PyPI version](https://img.shields.io/pypi/v/aana.svg)](https://pypi.org/project/aana/)\n[![GitHub release](https://img.shields.io/github/v/release/mobiusml/aana_sdk.svg)](https://github.com/mobiusml/aana_sdk/releases)\n\n<p align=\"center\">\n  <picture>\n    <source srcset=\"https://raw.githubusercontent.com/mobiusml/aana_sdk/main/docs/images/AanaSDK_logo_dark_theme.png\" media=\"(prefers-color-scheme: dark)\">\n    <img src=\"https://raw.githubusercontent.com/mobiusml/aana_sdk/main/docs/images/AanaSDK_logo_light_theme.png\" alt=\"Aana Logo\">\n  </picture>\n</p>\n\n# Aana\n\nAana SDK is a powerful framework for building multimodal applications. It facilitates the large-scale deployment of machine learning models, including those for vision, audio, and language, and supports Retrieval-Augmented Generation (RAG) systems. This enables the development of advanced applications such as search engines, recommendation systems, and data insights platforms.\n\nThe SDK is designed according to the following principles:\n\n- **Reliability**: Aana is designed to be reliable and robust. It is built to be fault-tolerant and to handle failures gracefully.\n- **Scalability**: Aana is designed to be scalable. It is built on top of Ray, a distributed computing framework, and can be easily scaled to multiple servers.\n- **Efficiency**: Aana is designed to be efficient. It is built to be fast and parallel and to use resources efficiently.\n- **Easy to Use**: Aana is designed to be easy to use by developers. It is built to be modular, with a lot of automation and abstraction.\n\nThe SDK is still in development, and not all features are fully implemented. We are constantly working on improving the SDK, and we welcome any feedback or suggestions.\n\nCheck out the [documentation](https://mobiusml.github.io/aana_sdk/) for more information.\n\n## Why use Aana SDK?\n\nNowadays, it is getting easier to experiment with machine learning models and build prototypes. However, deploying these models at scale and integrating them into real-world applications is still a challenge. \n\nAana SDK simplifies this process by providing a framework that allows:\n- Deploy and scale machine learning models on a single machine or a cluster.\n- Build multimodal applications that combine multiple different machine learning models.\n\n### Key Features\n\n- **Model Deployment**:\n  - Deploy models on a single machine or scale them across a cluster.\n\n- **API Generation**:\n  - Automatically generate an API for your application based on the endpoints you define.\n  - Input and output of the endpoints will be automatically validated.\n  - Simply annotate the types of input and output of the endpoint functions.\n- **Predefined Types**:\n  - Comes with a set of predefined types for various data such as images, videos, etc.\n\n- **Documentation Generation**:\n  - Automatically generate documentation for your application based on the defined endpoints.\n\n- **Streaming Support**:\n  - Stream the output of the endpoint to the client as it is generated.\n  - Ideal for real-time applications and Large Language Models (LLMs).\n\n- **Task Queue Support**:\n  - Run every endpoint you define as a task in the background without any changes to your code.\n- **Integrations**:  \n   - Aana SDK has integrations with various machine learning models and libraries: Whisper, vLLM, Hugging Face Transformers, Deepset Haystack, and more to come (for more information see [Integrations](https://mobiusml.github.io/aana_sdk/pages/integrations/)).\n\n## Installation\n\n### Installing via PyPI\n\nTo install Aana SDK via PyPI, you can use the following command:\n\n```bash\npip install aana\n```\n\nBy default `aana` installs only the core dependencies. The deployment-specific dependencies are not installed by default. You have two options:\n- Install the dependencies manually. You will be prompted to install the dependencies when you try to use a deployment that requires them.\n- Use extras to install all dependencies. Here are the available extras:\n  - `all`: Install dependencies for all deployments.\n  - `vllm`: Install dependencies for the vLLM deployment.\n  - `asr`: Install dependencies for the Automatic Speech Recognition (Whisper) deployment and other ASR models (diarization, voice activity detection, etc.).\n  - `transformers`: Install dependencies for the Hugging Face Transformers deployment. There are multiple deployments that use Transformers.\n  - `hqq`: Install dependencies for Half-Quadratic Quantization (HQQ) deployment.\n\nFor example, to install all dependencies, you can use the following command:\n\n```bash \npip install aana[all]\n```\n\nFor optimal performance install [PyTorch](https://pytorch.org/get-started/locally/) version >=2.1 appropriate for your system. You can skip it, but it will install a default version that may not make optimal use of your system's resources, for example, a GPU or even some SIMD operations. Therefore we recommend choosing your PyTorch package carefully and installing it manually.\n\nSome models use Flash Attention. Install Flash Attention library for better performance. See [flash attention installation instructions](https://github.com/Dao-AILab/flash-attention?tab=readme-ov-file#installation-and-features) for more details and supported GPUs.\n\n### Installing from GitHub\n\n1. Clone the repository.\n\n```bash\ngit clone https://github.com/mobiusml/aana_sdk.git\n```\n\n2. Install additional libraries.\n\nFor optimal performance install [PyTorch](https://pytorch.org/get-started/locally/) version >=2.1 appropriate for your system. You can continue directly to the next step, but it will install a default version that may not make optimal use of your system's resources, for example, a GPU or even some SIMD operations. Therefore we recommend choosing your PyTorch package carefully and installing it manually.\n\nSome models use Flash Attention. Install Flash Attention library for better performance. See [flash attention installation instructions](https://github.com/Dao-AILab/flash-attention?tab=readme-ov-file#installation-and-features) for more details and supported GPUs.\n\n3. Install the package with poetry.\n\nThe project is managed with [Poetry](https://python-poetry.org/docs/). See the [Poetry installation instructions](https://python-poetry.org/docs/#installation) on how to install it on your system. Use poetry >= 2.0 for the best experience.\n\nIt will install the package in the virtual environment created by Poetry. Add `--extras all` to install all extra dependencies.\n\n```bash\npoetry install --extras all\n```\n\nFor the development environment, it is recommended to install all extras and tests and dev dependencies. You can do this by running the following command:\n\n```bash\npoetry install --extras all --with dev,tests\n```\n\n## Getting Started\n\n### Creating a New Application\n\nYou can quickly develop multimodal applications using Aana SDK's intuitive APIs and components.\n\nIf you want to start building a new application, you can use the following GitHub template: [Aana App Template](https://github.com/mobiusml/aana_app_template). It will help you get started with the Aana SDK and provide you with a basic structure for your application and its dependencies.\n\nLet's create a simple application that transcribes a video. The application will download a video from YouTube, extract the audio, and transcribe it using an ASR model.\n\nAana SDK already provides a deployment for ASR (Automatic Speech Recognition) based on the Whisper model. We will use this [deployment](#Deployments) in the example.\n\n```python\nfrom aana.api.api_generation import Endpoint\nfrom aana.core.models.video import VideoInput\nfrom aana.deployments.aana_deployment_handle import AanaDeploymentHandle\nfrom aana.deployments.whisper_deployment import (\n    WhisperComputeType,\n    WhisperConfig,\n    WhisperDeployment,\n    WhisperModelSize,\n    WhisperOutput,\n)\nfrom aana.integrations.external.yt_dlp import download_video\nfrom aana.processors.remote import run_remote\nfrom aana.processors.video import extract_audio\nfrom aana.sdk import AanaSDK\n\n\n# Define the model deployments.\nasr_deployment = WhisperDeployment.options(\n    num_replicas=1,\n    ray_actor_options={\"num_gpus\": 0.25}, # Remove this line if you want to run Whisper on a CPU.\n    user_config=WhisperConfig(\n        model_size=WhisperModelSize.MEDIUM,\n        compute_type=WhisperComputeType.FLOAT16,\n    ).model_dump(mode=\"json\"),\n)\ndeployments = [{\"name\": \"asr_deployment\", \"instance\": asr_deployment}]\n\n\n# Define the endpoint to transcribe the video.\nclass TranscribeVideoEndpoint(Endpoint):\n    \"\"\"Transcribe video endpoint.\"\"\"\n\n    async def initialize(self):\n        \"\"\"Initialize the endpoint.\"\"\"\n        self.asr_handle = await AanaDeploymentHandle.create(\"asr_deployment\")\n        await super().initialize()\n\n    async def run(self, video: VideoInput) -> WhisperOutput:\n        \"\"\"Transcribe video.\"\"\"\n        video_obj = await run_remote(download_video)(video_input=video)\n        audio = extract_audio(video=video_obj)\n        transcription = await self.asr_handle.transcribe(audio=audio)\n        return transcription\n\nendpoints = [\n    {\n        \"name\": \"transcribe_video\",\n        \"path\": \"/video/transcribe\",\n        \"summary\": \"Transcribe a video\",\n        \"endpoint_cls\": TranscribeVideoEndpoint,\n    },\n]\n\naana_app = AanaSDK(name=\"transcribe_video_app\")\n\nfor deployment in deployments:\n    aana_app.register_deployment(**deployment)\n\nfor endpoint in endpoints:\n    aana_app.register_endpoint(**endpoint)\n\nif __name__ == \"__main__\":\n    aana_app.connect(host=\"127.0.0.1\", port=8000, show_logs=False)  # Connects to the Ray cluster or starts a new one.\n    aana_app.migrate()                                              # Runs the migrations to create the database tables.\n    aana_app.deploy(blocking=True)                                  # Deploys the application.\n```\n\nYou have a few options to run the application:\n- Copy the code above and run it in a Jupyter notebook.\n- Save the code to a Python file, for example `app.py`, and run it as a Python script: `python app.py`.\n- Save the code to a Python file, for example `app.py`, and run it using the Aana CLI: `aana deploy app:aana_app --host 127.0.0.1 --port 8000 --hide-logs`.\n\nOnce the application is running, you will see the message `Deployed successfully.` in the logs. You can now send a request to the application to transcribe a video.\n\nTo get an overview of the Ray cluster, you can use the Ray Dashboard. The Ray Dashboard is available at `http://127.0.0.1:8265` by default. You can see the status of the Ray cluster, the resources used, running applications and deployments, logs, and more. It is a useful tool for monitoring and debugging your applications. See [Ray Dashboard documentation](https://docs.ray.io/en/latest/ray-observability/getting-started.html) for more information.\n\nLet's transcribe [Gordon Ramsay's perfect scrambled eggs tutorial](https://www.youtube.com/watch?v=VhJFyyukAzA) using the application.\n\n```bash\ncurl -X POST http://127.0.0.1:8000/video/transcribe -Fbody='{\"video\":{\"url\":\"https://www.youtube.com/watch?v=VhJFyyukAzA\"}}'\n```\n\nThis will return the full transcription of the video, transcription for each segment, and transcription info like identified language. You can also use the [Swagger UI](http://127.0.0.1:8000/docs) to send the request.\n\n### Running Example Applications\n\nWe provide a few example applications that demonstrate the capabilities of Aana SDK.\n\n- [Chat with Video](https://github.com/mobiusml/aana_chat_with_video): A multimodal chat application that allows users to upload a video and ask questions about the video content based on the visual and audio information. See [Chat with Video Demo notebook](https://github.com/mobiusml/aana_chat_with_video/blob/main/notebooks/chat_with_video_demo.ipynb) to see how to use the application.\n- [Summarize Video](https://github.com/mobiusml/aana_summarize_video): An Aana application that summarizes a video by extracting transcription from the audio and generating a summary using a Language Model (LLM). This application is a part of the [tutorial](https://mobiusml.github.io/aana_sdk/pages/tutorial/) on how to build multimodal applications with Aana SDK.\n\nSee the README files of the applications for more information on how to install and run them.\n\nThe full list of example applications is available in the [Aana Examples](https://github.com/mobiusml/aana_examples) repository. You can use these examples as a starting point for building your own applications.\n\n### Main components\n\nThere are three main components in Aana SDK: deployments, endpoints, and AanaSDK.\n\n#### Deployments\n\nDeployments are the building blocks of Aana SDK. They represent the machine learning models that you want to deploy. Aana SDK comes with a set of predefined deployments that you can use or you can define your own deployments. See [Integrations](https://mobiusml.github.io/aana_sdk/pages/integrations/) for more information about predefined deployments.\n\nEach deployment has a main class that defines it and a configuration class that allows you to specify the deployment parameters.\n\nFor example, we have a predefined deployment for the Whisper model that allows you to transcribe audio. You can define the deployment like this:\n\n```python\nfrom aana.deployments.whisper_deployment import WhisperDeployment, WhisperConfig, WhisperModelSize, WhisperComputeType\n\nasr_deployment = WhisperDeployment.options(\n    num_replicas=1,\n    ray_actor_options={\"num_gpus\": 0.25},\n    user_config=WhisperConfig(model_size=WhisperModelSize.MEDIUM, compute_type=WhisperComputeType.FLOAT16).model_dump(mode=\"json\"),\n)\n```\n\nSee [Model Hub](https://mobiusml.github.io/aana_sdk/pages/model_hub/) for a collection of configurations for different models that can be used with the predefined deployments.\n\n#### Endpoints\n\nEndpoints define the functionality of your application. They allow you to connect multiple deployments (models) to each other and define the input and output of your application.\n\nEach endpoint is defined as a class that inherits from the `Endpoint` class. The class has two main methods: `initialize` and `run`.\n\nFor example, you can define an endpoint that transcribes a video like this:\n\n```python\nclass TranscribeVideoEndpoint(Endpoint):\n    \"\"\"Transcribe video endpoint.\"\"\"\n\n    async def initialize(self):\n        \"\"\"Initialize the endpoint.\"\"\"\n        await super().initialize()\n        self.asr_handle = await AanaDeploymentHandle.create(\"asr_deployment\")\n\n    async def run(self, video: VideoInput) -> WhisperOutput:\n        \"\"\"Transcribe video.\"\"\"\n        video_obj = await run_remote(download_video)(video_input=video)\n        audio = extract_audio(video=video_obj)\n        transcription = await self.asr_handle.transcribe(audio=audio)\n        return transcription\n```\n\n#### AanaSDK\n\nAanaSDK is the main class that you use to build your application. It allows you to deploy the deployments and endpoints you defined and start the application.\n\nFor example, you can define an application that transcribes a video like this:\n\n```python\naana_app = AanaSDK(name=\"transcribe_video_app\")\n\naana_app.register_deployment(name=\"asr_deployment\", instance=asr_deployment)\naana_app.register_endpoint(\n    name=\"transcribe_video\",\n    path=\"/video/transcribe\",\n    summary=\"Transcribe a video\",\n    endpoint_cls=TranscribeVideoEndpoint,\n)\n\naana_app.connect()  # Connects to the Ray cluster or starts a new one.\naana_app.migrate()  # Runs the migrations to create the database tables.\naana_app.deploy()   # Deploys the application.\n```\n\nAll you need to do is define the deployments and endpoints you want to use in your application, and Aana SDK will take care of the rest.\n\n\n## API\n\nAana SDK uses form data for API requests, which allows sending both binary data and structured fields in a single request. The request body is sent as a JSON string in the `body` field, and any binary data is sent as files.\n\n### Making API Requests\n\nYou can send requests to the SDK endpoints with only structured data or a combination of structured data and binary data.\n\n#### Only Structured Data\nWhen your request includes only structured data, you can send it as a JSON string in the `body` field.\n\n- **cURL Example:**\n    ```bash\n    curl http://127.0.0.1:8000/endpoint \\\n        -F body='{\"input\": \"data\", \"param\": \"value\"}'\n    ```\n\n- **Python Example:**\n    ```python\n    import json, requests\n\n    url = \"http://127.0.0.1:8000/endpoint\"\n    body = {\n        \"input\": \"data\",\n        \"param\": \"value\"\n    }\n\n    response = requests.post(\n        url,\n        data={\"body\": json.dumps(body)}\n    )\n\n    print(response.json())\n    ```\n\n#### With Binary Data\nWhen your request includes binary files (images, audio, etc.), you can send them as files in the request and include the names of the files in the `body` field as a reference.\n\nFor example, if you want to send an image, you can use [`aana.core.models.image.ImageInput`](https://mobiusml.github.io/aana_sdk/reference/models/media/#aana.core.models.ImageInput) as the input type that supports binary data upload. The `content` field in the input type should be set to the name of the file you are sending. \n\n- **cURL Example:**\n    ```bash\n    curl http://127.0.0.1:8000/process_images \\\n        -H \"Content-Type: multipart/form-data\" \\\n        -F body='{\"image\": {\"content\": \"file1\"}}' \\\n        -F file1=\"@image.jpeg\"\n    ```\n\n- **Python Example:**\n    ```python\n    import json, requests \n\n    url = \"http://127.0.0.1:8000/process_images\"\n    body = {\n        \"image\": {\"content\": \"file1\"}\n    }\n    with open(\"image.jpeg\", \"rb\") as file:\n        files = {\"file1\": file}\n\n        response = requests.post(\n            url,\n            data={\"body\": json.dumps(body)},\n            files=files\n        )\n\n        print(response.text)\n    ```\n\n## Serve Config Files\n\nThe [Serve Config Files](https://docs.ray.io/en/latest/serve/production-guide/config.html#serve-config-files) is the recommended way to deploy and update your applications in production. Aana SDK provides a way to build the Serve Config Files for the Aana applications. See the [Serve Config Files documentation](https://mobiusml.github.io/aana_sdk/pages/serve_config_files/) on how to build and deploy the applications using the Serve Config Files.\n\n\n## Run with Docker\n\nYou can deploy example applications using Docker. See the [documentation on how to run Aana SDK with Docker](https://mobiusml.github.io/aana_sdk/pages/docker/).\n\n## Documentation\n\nFor more information on how to use Aana SDK, see the [documentation](https://mobiusml.github.io/aana_sdk/).\n\n## License\n\nAana SDK is licensed under the [Apache License 2.0](https://github.com/mobiusml/aana_sdk?tab=Apache-2.0-1-ov-file#readme). Commercial licensing options are also available.\n\n## Contributing\n\nWe welcome contributions from the community to enhance Aana SDK's functionality and usability. Feel free to open issues for bug reports, feature requests, or submit pull requests to contribute code improvements.\n\nCheck out the [Development Documentation](https://mobiusml.github.io/aana_sdk/pages/code_overview/) for more information on how to contribute.\n\nWe have adopted the [Contributor Covenant](https://www.contributor-covenant.org/) as our code of conduct.\n"
    },
    {
      "name": "Ransaka/ai-agents-with-llama3",
      "stars": 46,
      "img": "https://avatars.githubusercontent.com/u/48125060?s=40&v=4",
      "owner": "Ransaka",
      "repo_name": "ai-agents-with-llama3",
      "description": null,
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2024-07-28T13:36:18Z",
      "updated_at": "2025-04-13T12:09:41Z",
      "topics": [],
      "readme": "# Using Llama 3 for Building AI Agents\n>Comprehensive guide to building AI Agents with Llama 3.1 function calling capabilities.\n\n![Alt text](<images/Untitled design (3).png>)\n\n## Introduction\nLet’s assume a scenario where you want to purchase something. You head over to an e-commerce website, use the search option to find what you want. Maybe you have a bunch of items to purchase as well. So the process is not very optimized, is it? Now think about this scenario: you head over to an application, explain what you want in plain English, and hit enter. All the searching and price comparisons are automatically done for you. Pretty cool, right? That’s exactly what we are going to build in this tutorial.\n\nLet’s see some examples first.\n\n![user asking multiple products at once](images/example1.png)\nuser asking multiple products at once\n![User is asking for the most cost-effective purchase he/she can make.](images/example2.png)\nUser is asking for the most cost-effective purchase he/she can make.\n\nAlright, let’s bring life into this application. Here we are going to use Meta’s Llama 3.1 model with function calling capability. As per their announcement, the 3.1 models are capable of using tools and functions more effectively.\n\nThese are multilingual and have a significantly longer context length of 128K, state-of-the-art tool use, and overall stronger reasoning capabilities\n\nHowever, for this article, I’m going to use Groq Cloud, hence I am going to use their Groq/Llama-3-Groq-70B-Tool-Use model. As per the initial workflow of this application, this should consist of an embedding model, a retriever, and two main tools for handling user purchase interests and cost-related concerns. Long story short, we need something similar to what we described in the diagram below.\n\n![Alt text](images/architecture.webp)\n\nNow we have to use an LLM orchestration framework. For that, I am picking my all-time favourite, Haystack.\n\nOkay, we got what we need. Let’s jump into the real work!\n\n## Loading and Indexing data\nSince we have an RAG pipeline, we should build a document indexing service as the first step. Since this is a demo, I am going to use the in-memory vector database that Haystack offers. Please note that each document in our vector database contains,\n\n- content — which we use to perform similarity search\n- Id — Unique identifier\n- Price — Product price\n- URL — Product URL\n\nLet’s see how we can implement that.\n\n```python\nfrom haystack import Pipeline, Document\nfrom haystack.document_stores.in_memory import InMemoryDocumentStore\nfrom haystack.components.writers import DocumentWriter\nfrom haystack.components.embedders import SentenceTransformersDocumentEmbedder\nfrom haystack.components.generators import OpenAIGenerator\nfrom haystack.utils import Secret\nfrom haystack.components.generators.chat import OpenAIChatGenerator\nfrom haystack.components.builders import PromptBuilder\nfrom haystack.components.embedders import SentenceTransformersTextEmbedder\nfrom haystack.components.retrievers.in_memory import InMemoryEmbeddingRetriever\nfrom haystack.dataclasses import ChatMessage\n\ndf = pd.read_csv(\"product_sample.csv\")\n\ndocument_store = InMemoryDocumentStore()\ndocuments = [\n    Document(\n        content=item.product_name, \n        meta={\n            \"id\":item.uniq_id, \n            \"price\":item.selling_price, \n            \"url\":item.product_url\n            }\n        ) for item in df.itertuples()\n    ]\n\nindexing_pipeline = Pipeline()\n\nindexing_pipeline.add_component(\n    instance=SentenceTransformersDocumentEmbedder(model=\"sentence-transformers/all-MiniLM-L6-v2\"), name=\"doc_embedder\"\n)\n\nindexing_pipeline.add_component(instance=DocumentWriter(document_store= document_store), name=\"doc_writer\")\n\nindexing_pipeline.connect(\"doc_embedder.documents\", \"doc_writer.documents\")\n\nindexing_pipeline.run({\"doc_embedder\": {\"documents\": documents}})\n```\nGreat, we’ve completed the first step of our AI agent application. Now it’s time to build the product identifier tool. To better understand the main task of the product identifier, let’s consider example below.\n\n>User Query: I want to buy a camping boot, an charcoal and google pixel 9 back cover. Let’s understand our ideal workflow for product identifier function.\n\n![Alt text](images/flow1.webp)\n\nFirst, we need to create tool for analyse user query and identify user interested products. We can build such tool using below code snippets.\n\n## Building User Query Analyzer\n\n```python\ntemplate = \"\"\"\nUnderstand the user query and list of products the user is interested in and return product names as list.\nYou should always return a Python list. Do not return any explanation.\n\nExamples:\nQuestion: I am interested in camping boots, charcoal and disposable rain jacket.\nAnswer: [\"camping_boots\",\"charcoal\",\"disposable_rain_jacket\"]\n\nQuestion: Need a laptop, wireless mouse, and noise-cancelling headphones for work.\nAnswer: [\"laptop\",\"wireless_mouse\",\"noise_cancelling_headphones\"]\n\nQuestion: {{ question }}\nAnswer:\n\"\"\"\n\nproduct_identifier = Pipeline()\n\nproduct_identifier.add_component(\"prompt_builder\", PromptBuilder(template=template))\nproduct_identifier.add_component(\"llm\", generator())\n\nproduct_identifier.connect(\"prompt_builder\", \"llm\")\n```\n\nOkay, now we have completed the half of our first function, now it’s time to complete function by adding RAG pipeline.\n\n![Alt text](images/flow2.webp)\n\n## Creating RAG Pipeline\n\n```python\ntemplate = \"\"\"\nReturn product name, price, and url as a python dictionary. \nYou should always return a Python dictionary with keys price, name and url for single product.\nYou should always return a Python list of dictionaries with keys price, name and url for multiple products.\nDo not return any explanation.\n\nLegitimate Response Schema:\n{\"price\": \"float\", \"name\": \"string\", \"url\": \"string\"}\nLegitimate Response Schema for multiple products:\n[{\"price\": \"float\", \"name\": \"string\", \"url\": \"string\"},{\"price\": \"float\", \"name\": \"string\", \"url\": \"string\"}]\n\nContext:\n{% for document in documents %}\n    product_price: {{ document.meta['price'] }}\n    product_url: {{ document.meta['url'] }}\n    product_id: {{ document.meta['id'] }}\n    product_name: {{ document.content }}\n{% endfor %}\nQuestion: {{ question }}\nAnswer:\n\"\"\"\n\nrag_pipe = Pipeline()\nrag_pipe.add_component(\"embedder\", SentenceTransformersTextEmbedder(model=\"sentence-transformers/all-MiniLM-L6-v2\"))\nrag_pipe.add_component(\"retriever\", InMemoryEmbeddingRetriever(document_store=document_store, top_k=5))\nrag_pipe.add_component(\"prompt_builder\", PromptBuilder(template=template))\nrag_pipe.add_component(\"llm\", generator())\n\nrag_pipe.connect(\"embedder.embedding\", \"retriever.query_embedding\")\nrag_pipe.connect(\"retriever\", \"prompt_builder.documents\")\nrag_pipe.connect(\"prompt_builder\", \"llm\")\n```\n\nAfter this stage, we have completed both RAG and Query Analyzer pipelines. Now it’s time to convert this into a tool. For that, we can use a regular function declaration as shown below. Here, creating a tool for the Agent is no different than creating a Python function. In case you have a question like\n\n>How is it possible for the Agent to invoke this function?\n\nThe answer is simple: by using a model-specific tool schema. We will implement that in a future step. For now, it’s time to create a wrapper function that uses both the query analyzer and RAG pipeline.\n\n## Finalizing Product Identifier Function\n\n```python\ndef product_identifier_func(query: str):\n    product_understanding = product_identifier.run({\"prompt_builder\": {\"question\": query}})\n\n    try:\n        product_list = literal_eval(product_understanding[\"llm\"][\"replies\"][0])\n    except:\n        return \"No product found\"\n\n    results = {}\n\n    for product in product_list:\n        response = rag_pipe.run({\"embedder\": {\"text\": product}, \"prompt_builder\": {\"question\": product}})\n        try:\n            results[product] = literal_eval(response[\"llm\"][\"replies\"][0])\n        except:\n            results[product] = {}\n    \n    return results\n```\n\n![Alt text](images/flow3.webp)\n\nWith that, we have completed our first tool for the agent. Let’s see whether it works as expected.\n\n```python\nquery = \"I want crossbow and woodstock puzzle\"\n#execute function\nproduct_identifier_func(query)\n\n# {'crossbow': {'name': 'DB Longboards CoreFlex Crossbow 41\" Bamboo Fiberglass '\n#                        'Longboard Complete',\n#                'price': 237.68,\n#                'url': 'https://www.amazon.com/DB-Longboards-CoreFlex-Fiberglass-Longboard/dp/B07KMVJJK7'},\n#  'woodstock_puzzle': {'name': 'Woodstock- Collage 500 pc Puzzle',\n#                       'price': 17.49,\n#                       'url': 'https://www.amazon.com/Woodstock-Collage-500-pc-Puzzle/dp/B07MX21WWX'}}\n```\n\nIt worked!! However, it’s worth noting the return output schema. You can see the general schema below.\n\n```python\n{\n    \"product_key\": {\n        \"name\": \"string\",\n        \"price\": \"float\",\n        \"url\": \"string\"\n    }\n}\n```\n\nThat’s exactly what we have advised the model to produce in the RAG pipeline. As a next step, let’s build an optional tool called `find_budget_friendly_option`.\n\n```python\ndef find_budget_friendly_option(selected_product_details):\n    budget_friendly_options = {}\n    \n    for category, items in selected_product_details.items():\n        if isinstance(items, list):\n            lowest_price_item = min(items, key=lambda x: x['price'])\n        else:\n            lowest_price_item = items\n        \n        budget_friendly_options[category] = lowest_price_item\n    \n    return budget_friendly_options\n```\n\nAlright, now we have the most critical part of this application, which is allowing the agent to use these functions when necessary. As we discussed earlier, this is made possible via a model-specific tool schema. So we have to find the tool schema specific for the selected model. Luckily, it’s mentioned in the Groq/Llama-3-Groq-70B-Tool-Use's model card here. Let's adapt that to our use case.\n\n## Finalizing Chat Template\n\n```python\nchat_template = '''<|start_header_id|>system<|end_header_id|>\n\nYou are a function calling AI model. You are provided with function signatures within <tools></tools> XML tags. You may call one or more functions to assist with the user query. Don't make assumptions about what values to plug into functions. For each function call return a json object with function name and arguments within <tool_call></tool_call> XML tags as follows:\n<tool_call>\n{\"name\": <function-name>,\"arguments\": <args-dict>}\n</tool_call>\n\nHere are the available tools:\n<tools>\n    {\n        \"name\": \"product_identifier_func\",\n        \"description\": \"To understand user interested products and its details\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"query\": {\n                    \"type\": \"string\",\n                    \"description\": \"The query to use in the search. Infer this from the user's message. It should be a question or a statement\"\n                }\n            },\n            \"required\": [\"query\"]\n        }\n    },\n    {\n        \"name\": \"find_budget_friendly_option\",\n        \"description\": \"Get the most cost-friendly option. If selected_product_details has morethan one key this should return most cost-friendly options\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"selected_product_details\": {\n                    \"type\": \"dict\",\n                    \"description\": \"Input data is a dictionary where each key is a category name, and its value is either a single dictionary with 'price', 'name', and 'url' keys or a list of such dictionaries; example: {'category1': [{'price': 10.5, 'name': 'item1', 'url': 'http://example.com/item1'}, {'price': 8.99, 'name': 'item2', 'url': 'http://example.com/item2'}], 'category2': {'price': 15.0, 'name': 'item3', 'url': 'http://example.com/item3'}}\"\n                }\n            },\n            \"required\": [\"selected_product_details\"]\n        }\n    }\n</tools><|eot_id|><|start_header_id|>user<|end_header_id|>\n\nI need to buy a crossbow<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n<tool_call>\n{\"id\":\"call_deok\",\"name\":\"product_identifier_func\",\"arguments\":{\"query\":\"I need to buy a crossbow\"}}\n</tool_call><|eot_id|><|start_header_id|>tool<|end_header_id|>\n\n<tool_response>\n{\"id\":\"call_deok\",\"result\":{'crossbow': {'price': 237.68,'name': 'crossbow','url': 'https://www.amazon.com/crossbow/dp/B07KMVJJK7'}}}\n</tool_response><|eot_id|><|start_header_id|>assistant<|end_header_id|>\n'''\n```\n\nNow there are only a few steps left. Before doing anything, let’s test our agent.\n\n```python\n## Testing agent\nmessages = [\n    ChatMessage.from_system(\n        chat_template\n    ),\n    ChatMessage.from_user(\"I need to buy a crossbow for my child and Pokémon for myself.\"),\n]\n\nchat_generator = get_chat_generator()\nresponse = chat_generator.run(messages=messages)\npprint(response)\n\n## response\n{'replies': [ChatMessage(content='<tool_call>\\n'\n                                 '{\"id\": 0, \"name\": \"product_identifier_func\", '\n                                 '\"arguments\": {\"query\": \"I need to buy a '\n                                 'crossbow for my child\"}}\\n'\n                                 '</tool_call>\\n'\n                                 '<tool_call>\\n'\n                                 '{\"id\": 1, \"name\": \"product_identifier_func\", '\n                                 '\"arguments\": {\"query\": \"I need to buy a '\n                                 'Pokemon for myself\"}}\\n'\n                                 '</tool_call>',\n                         role=<ChatRole.ASSISTANT: 'assistant'>,\n                         name=None,\n                         meta={'finish_reason': 'stop',\n                               'index': 0,\n                               'model': 'llama3-groq-70b-8192-tool-use-preview',\n                               'usage': {'completion_time': 0.217823967,\n                                         'completion_tokens': 70,\n                                         'prompt_time': 0.041348261,\n                                         'prompt_tokens': 561,\n                                         'total_time': 0.259172228,\n                                         'total_tokens': 631}})]}\n```\n\nWith that, we have completed about 90% of our work.\n\n![Alt text](images/progress.webp)\n\n---\nOne thing you probably noticed in the above response is that tool calls are enclosed using the XML tag `<tool_call>`. Therefore, we have to build some mechanism to extract the tool_call object.\n\n```python\ndef extract_tool_calls(tool_calls_str):\n    json_objects = re.findall(r'<tool_call>(.*?)</tool_call>', tool_calls_str, re.DOTALL)\n    \n    result_list = [json.loads(obj) for obj in json_objects]\n    \n    return result_list\n\navailable_functions = {\n    \"product_identifier_func\": product_identifier_func, \n    \"find_budget_friendly_option\": find_budget_friendly_option\n    }\n```\n\nWith this step completed, we can directly access the agent’s response when it calls a tool. Now the only thing pending is to get the tool call object and execute the function accordingly. Let’s complete that piece too.\n\n```python\nmessages.append(ChatMessage.from_user(message))\nresponse = chat_generator.run(messages=messages)\n\nif response and \"<tool_call>\" in response[\"replies\"][0].content:\n    function_calls = extract_tool_calls(response[\"replies\"][0].content)\n    for function_call in function_calls:\n        # Parse function calling information\n        function_name = function_call[\"name\"]\n        function_args = function_call[\"arguments\"]\n\n        # Find the corresponding function and call it with the given arguments\n        function_to_call = available_functions[function_name]\n        function_response = function_to_call(**function_args)\n\n        # Append function response to the messages list using `ChatMessage.from_function`\n        messages.append(ChatMessage.from_function(content=json.dumps(function_response), name=function_name))\n        response = chat_generator.run(messages=messages)\n```\n\nNow it’s time to join each component together and build proper chat application. I am going to use gradio for that purpose.\n\n```python\nimport gradio as gr\n\nmessages = [ChatMessage.from_system(chat_template)]\nchat_generator = get_chat_generator()\n\ndef chatbot_with_fc(message, messages):\n    messages.append(ChatMessage.from_user(message))\n    response = chat_generator.run(messages=messages)\n\n    while True:\n        if response and \"<tool_call>\" in response[\"replies\"][0].content:\n            function_calls = extract_tool_calls(response[\"replies\"][0].content)\n            for function_call in function_calls:\n                # Parse function calling information\n                function_name = function_call[\"name\"]\n                function_args = function_call[\"arguments\"]\n\n                # Find the corresponding function and call it with the given arguments\n                function_to_call = available_functions[function_name]\n                function_response = function_to_call(**function_args)\n\n                # Append function response to the messages list using `ChatMessage.from_function`\n                messages.append(ChatMessage.from_function(content=json.dumps(function_response), name=function_name))\n                response = chat_generator.run(messages=messages)\n\n        # Regular Conversation\n        else:\n            messages.append(response[\"replies\"][0])\n            break\n    return response[\"replies\"][0].content\n\n\ndef chatbot_interface(user_input, state):\n    response_content = chatbot_with_fc(user_input, state)\n    return response_content, state\n\nwith gr.Blocks() as demo:\n    gr.Markdown(\"# AI Purchase Assistant\")\n    gr.Markdown(\"Ask me about products you want to buy!\")\n    \n    state = gr.State(value=messages)\n    \n    with gr.Row():\n        user_input = gr.Textbox(label=\"Your message:\")\n        response_output = gr.Markdown(label=\"Response:\")\n    \n    user_input.submit(chatbot_interface, [user_input, state], [response_output, state])\n    gr.Button(\"Send\").click(chatbot_interface, [user_input, state], [response_output, state])\n\n\ndemo.launch()\n```\n\nThat’s all, we have build the Llama 3 based AI Agent 🤖 with function calling capability.\n\n## Conclusion\n\nWhen building an AI agent-based system, it’s worth noting the time taken to finish a task and the number of API calls (tokens) used to complete a single task. Furthermore, reducing hallucinations is the biggest challenge for us to tackle, and it is an active research area as well. Hence, there is no rule of thumb for use in LLM and agent system building. So, you have to work patiently and tactically to get the AI agent, or in other words, the LLM, on the right track.\n"
    },
    {
      "name": "TuanaCelik/should-i-follow",
      "stars": 44,
      "img": "https://avatars.githubusercontent.com/u/15802862?s=40&v=4",
      "owner": "TuanaCelik",
      "repo_name": "should-i-follow",
      "description": "🦄 An NLP application just for the lols: built with Haystack to get an overview of what a user is posting about on Twitter",
      "homepage": "",
      "language": "Python",
      "created_at": "2023-02-17T00:47:02Z",
      "updated_at": "2024-08-26T13:32:41Z",
      "topics": [
        "haystack",
        "llm",
        "nlp",
        "twitter"
      ],
      "readme": "---\ntitle: Should I follow?\nemoji: 🦄\ncolorFrom: pink\ncolorTo: yellow\nsdk: streamlit\nsdk_version: 1.21.0\napp_file: app.py\npinned: false\n---\n\n\n# Should I Follow?\n\n### Try it out on [🤗 Spaces](https://huggingface.co/spaces/deepset/should-i-follow)\n\n##### A simple app to get an overview of what the Mastodon user has been posting about and their tone\n\nThis is a demo just for fun 🥳\nThis repo contains a streamlit application that given a Mastodon username, tells you what type of things they've been posting about lately, their tone, and the languages they use. It uses the LLM by OpenAI `gpt-4`.\n\nIt's been built with [Haystack](https://haystack.deepset.ai) using the [`OpenAIGenerator`](https://docs.haystack.deepset.ai/v2.0/docs/openaigenerator) and by creating a [`PromptBuilder`](https://docs.haystack.deepset.ai/v2.0/docs/promptbuilder)\n\nhttps://user-images.githubusercontent.com/15802862/220464834-f42c038d-54b4-4d5e-8d59-30d95143b616.mov\n\n\n### Points of improvement\n\nSince we're using a generative model here, we need to be a bit creative with the prompt we provide it to minimize any hallucination or similar unwanted results. For this reason, I've tried to be a bit creative with the `PromptBuilder` template and give some examples of _how_ to construct a summary. However, this still sometimes produces odd results.\n\nIf you try to run it yourself and find ways to make this app better, please feel free to create an issue/PR 🙌\n\n## To learn more about the PromptBuilder\n\nAs of Haystack 2.0-Beta onwards, you can create prompt templates with Jinja. Check out guide on creating prompts [here](https://docs.haystack.deepset.ai/v2.0/docs/promptbuilder)\n\n## Installation and Running\nTo run the bare application which does _nothing_:\n1. Install requirements:\n`pip install -r requirements.txt`\n2. Run the streamlit app:\n`streamlit run app.py`\n\nThis will start up the app on `localhost:8501` where you will dind a simple search bar\n\n#### The Haystack Community is on [Discord](https://discord.com/invite/VBpFzsgRVF)\n"
    },
    {
      "name": "qdrant/qdrant-haystack",
      "stars": 44,
      "img": "https://avatars.githubusercontent.com/u/73504361?s=40&v=4",
      "owner": "qdrant",
      "repo_name": "qdrant-haystack",
      "description": "An integration of Qdrant ANN vector database backend with Haystack ",
      "homepage": null,
      "language": "Python",
      "created_at": "2023-01-31T15:56:08Z",
      "updated_at": "2025-01-10T15:22:03Z",
      "topics": [],
      "readme": "# qdrant-haystack\n\n> [!IMPORTANT]\n> The V2 implementation of `qdrant-haystack` has been moved to [deepset-ai/haystack-core-integrations](https://github.com/deepset-ai/haystack-core-integrations).\n> Find the latest documentation [here](https://haystack.deepset.ai/integrations/qdrant-document-store).\n\nAn integration of [Qdrant](https://qdrant.tech) vector database with [Haystack](https://haystack.deepset.ai/)\nby [deepset](https://www.deepset.ai).\n\nThe library finally allows using Qdrant as a document store, and provides an in-place replacement\nfor any other vector embeddings store. Thus, you should expect any kind of application to be working\nsmoothly just by changing the provider to `QdrantDocumentStore`.\n\n## Installation\n\n`qdrant-haystack` might be installed as any other Python library, using pip or poetry:\n\n```bash\npip install qdrant-haystack\n```\n\n```bash\npoetry add qdrant-haystack\n```\n\n## Usage\n\nOnce installed, you can already start using `QdrantDocumentStore` as any other store that supports\nembeddings.\n\n```python\nfrom qdrant_haystack import QdrantDocumentStore\n\ndocument_store = QdrantDocumentStore(\n    \"localhost\",\n    index=\"Document\",\n    embedding_dim=512,\n    recreate_index=True,\n    hnsw_config={\"m\": 16, \"ef_construct\": 64}  # Optional\n)\n```\n\nThe list of parameters accepted by `QdrantDocumentStore` is complementary to those used in the\nofficial [Python Qdrant client](https://github.com/qdrant/qdrant_client).\n\n### Using local in-memory / disk-persisted mode\n\nQdrant Python client, from version 1.1.1, supports local in-memory/disk-persisted mode. That's\na good choice for any test scenarios and quick experiments in which you do not plan to store\nlots of vectors. In such a case spinning a Docker container might be even not required.\n\nThe local mode was also implemented in `qdrant-haystack` integration.\n\n#### In-memory storage\n\nIn case you want to have a transient storage, for example in case of automated tests launched\nduring your CI/CD pipeline, using Qdrant Local mode with in-memory storage might be a preferred\noption. It might be simply enabled by passing `:memory:` as first parameter, while creating an\ninstance of `QdrantDocumentStore`.\n\n```python\nfrom qdrant_haystack import QdrantDocumentStore\n\ndocument_store = QdrantDocumentStore(\n    \":memory:\",\n    index=\"Document\",\n    embedding_dim=512,\n    recreate_index=True,\n    hnsw_config={\"m\": 16, \"ef_construct\": 64}  # Optional\n)\n```\n\n#### On disk storage\n\nHowever, if you prefer to keep the vectors between different runs of your application, it\nmight be better to use on disk storage and pass the path that should be used to persist\nthe data.\n\n```python\nfrom qdrant_haystack import QdrantDocumentStore\n\ndocument_store = QdrantDocumentStore(\n    path=\"/home/qdrant/storage_local\",\n    index=\"Document\",\n    embedding_dim=512,\n    recreate_index=True,\n    hnsw_config={\"m\": 16, \"ef_construct\": 64}  # Optional\n)\n```\n\n### Connecting to Qdrant Cloud cluster\n\nIf you prefer not to manage your own Qdrant instance, [Qdrant Cloud](https://cloud.qdrant.io/)\nmight be a better option.\n\n```python\nfrom qdrant_haystack import QdrantDocumentStore\n\ndocument_store = QdrantDocumentStore(\n    \"https://YOUR-CLUSTER-URL.aws.cloud.qdrant.io\",\n    index=\"Document\",\n    api_key=\"<< YOUR QDRANT CLOUD API KEY >>\",\n    embedding_dim=512,\n    recreate_index=True,\n)\n```\n\nThere is no difference in terms of functionality between local instances and cloud clusters.\n"
    },
    {
      "name": "deepset-ai/haystack-experimental",
      "stars": 44,
      "img": "https://avatars.githubusercontent.com/u/51827949?s=40&v=4",
      "owner": "deepset-ai",
      "repo_name": "haystack-experimental",
      "description": "🧪 Experimental features for Haystack",
      "homepage": "https://haystack.deepset.ai",
      "language": "Python",
      "created_at": "2024-05-09T13:30:39Z",
      "updated_at": "2025-04-23T08:28:49Z",
      "topics": [],
      "readme": "[![PyPI - Version](https://img.shields.io/pypi/v/haystack-experimental.svg)](https://pypi.org/project/haystack-experimental)\n[![PyPI - Python Version](https://img.shields.io/pypi/pyversions/haystack-experimental.svg)](https://pypi.org/project/haystack-experimental)\n[![Tests](https://github.com/deepset-ai/haystack-experimental/actions/workflows/tests.yml/badge.svg)](https://github.com/deepset-ai/haystack-experimental/actions/workflows/tests.yml)\n[![Project release on PyPi](https://github.com/deepset-ai/haystack-experimental/actions/workflows/pypi_release.yml/badge.svg)](https://github.com/deepset-ai/haystack-experimental/actions/workflows/pypi_release.yml)\n[![Hatch project](https://img.shields.io/badge/%F0%9F%A5%9A-Hatch-4051b5.svg)](https://github.com/pypa/hatch)\n[![Checked with mypy](https://www.mypy-lang.org/static/mypy_badge.svg)](https://mypy-lang.org/)\n\n# Haystack experimental package\n\nThe `haystack-experimental` package provides Haystack users with access to experimental features without immediately\ncommitting to their official release. The main goal is to gather user feedback and iterate on new features quickly.\n\n## Installation\n\nFor simplicity, every release of `haystack-experimental` will ship all the available experiments at that time. To\ninstall the latest experimental features, run:\n\n```sh\n$ pip install -U haystack-experimental\n```\n\nInstall from the `main` branch to try the newest features:\n```sh\npip install git+https://github.com/deepset-ai/haystack-experimental.git@main\n```\n\n> [!IMPORTANT]\n> The latest version of the experimental package is only tested against the latest version of Haystack. Compatibility\n> with older versions of Haystack is not guaranteed.\n\n## Experiments lifecycle\n\nEach experimental feature has a default lifespan of 3 months starting from the date of the first non-pre-release build\nthat includes it. Once it reaches the end of its lifespan, the experiment will be either:\n\n- Merged into Haystack core and published in the next minor release, or\n- Released as a Core Integration, or\n- Dropped.\n\n## Experiments catalog\n\n### Active experiments\n\n| Name                            | Type                                      | Expected End Date | Dependencies | Cookbook                                                                                                                                                                                                                                                  | Discussion                                                                     |\n|---------------------------------|-------------------------------------------|-------------------|--------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------|\n| [`SuperComponent`][2]           | Component                                 | February 2025     | None         |                                                                                                                                                                                                                                                           | [Discuss](https://github.com/deepset-ai/haystack-experimental/discussions/189) |\n| [`InMemoryChatMessageStore`][6] | Memory Store                              | December 2024     | None         | <a href=\"https://colab.research.google.com/github/deepset-ai/haystack-cookbook/blob/main/notebooks/conversational_rag_using_memory.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>      | [Discuss](https://github.com/deepset-ai/haystack-experimental/discussions/75)  |\n| [`ChatMessageRetriever`][7]     | Memory Component                          | December 2024     | None         | <a href=\"https://colab.research.google.com/github/deepset-ai/haystack-cookbook/blob/main/notebooks/conversational_rag_using_memory.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>      | [Discuss](https://github.com/deepset-ai/haystack-experimental/discussions/75)  |\n| [`ChatMessageWriter`][8]        | Memory Component                          | December 2024     | None         | <a href=\"https://colab.research.google.com/github/deepset-ai/haystack-cookbook/blob/main/notebooks/conversational_rag_using_memory.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>      | [Discuss](https://github.com/deepset-ai/haystack-experimental/discussions/75)  |\n| [`Pipeline`][9]                 | Pipeline breakpoints for debugging        | June 2025         | None         | <a href=\"https://colab.research.google.com/github/deepset-ai/haystack-cookbook/blob/main/notebooks/hybrid_rag_pipeline_with_breakpoints.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/> | [Discuss](https://github.com/deepset-ai/haystack-experimental/discussions/281) \n\n[2]: https://github.com/deepset-ai/haystack-experimental/blob/main/haystack_experimental/core/super_component/super_component.py\n[4]: https://github.com/deepset-ai/haystack-experimental/blob/main/haystack_experimental/components/retrievers/auto_merging_retriever.py\n[5]: https://github.com/deepset-ai/haystack-experimental/blob/main/haystack_experimental/components/splitters/hierarchical_doc_splitter.py\n[6]: https://github.com/deepset-ai/haystack-experimental/blob/main/haystack_experimental/chat_message_stores/in_memory.py\n[7]: https://github.com/deepset-ai/haystack-experimental/blob/main/haystack_experimental/components/retrievers/chat_message_retriever.py\n[8]: https://github.com/deepset-ai/haystack-experimental/blob/main/haystack_experimental/components/writers/chat_message_writer.py\n[9]: https://github.com/deepset-ai/haystack-experimental/blob/main/haystack_experimental/core/pipeline/pipeline.py\n\n### Adopted experiments\n| Name                                                                                   | Type                                     | Final release |\n|----------------------------------------------------------------------------------------|------------------------------------------|---------------|\n| `ChatMessage` refactoring; `Tool` class; tool support in ChatGenerators; `ToolInvoker` | Tool Calling support                     | 0.4.0         |\n| `AsyncPipeline`; `Pipeline` bug fixes and refactoring                                  | AsyncPipeline execution                  | 0.7.0         |\n| `LLMMetadataExtractor`                                                                 | Metadata extraction with LLM             | 0.7.0         |\n| `Auto-Merging Retriever` & `HierarchicalDocumentSplitter`                              | Document Splitting & Retrieval Technique | 0.8.0         | \n| `Agent`                                                                                | Simplify Agent development               | 0.8.0         |\n\n### Discontinued experiments\n\n| Name                   | Type                       | Final release | Cookbook                                                                                                                                 |\n|------------------------|----------------------------|---------------|------------------------------------------------------------------------------------------------------------------------------------------|\n| `OpenAIFunctionCaller` | Function Calling Component | 0.3.0         | None                                                                                                                                     |\n| `OpenAPITool`          | OpenAPITool component      | 0.3.0         | [Notebook](https://github.com/deepset-ai/haystack-experimental/blob/fe20b69b31243f8a3976e4661d9aa8c88a2847d2/examples/openapitool.ipynb) |\n| `EvaluationHarness`    | Evaluation orchestrator    | 0.7.0         | None                                                                                                                                     |\n\n## Usage\n\nExperimental new features can be imported like any other Haystack integration package:\n\n```python\nfrom haystack.dataclasses import ChatMessage\nfrom haystack_experimental.components.generators import FoobarGenerator\n\nc = FoobarGenerator()\nc.run([ChatMessage.from_user(\"What's an experiment? Be brief.\")])\n```\n\nExperiments can also override existing Haystack features. For example, users can opt into an experimental type of\n`Pipeline` by just changing the usual import:\n\n```python\n# from haystack import Pipeline\nfrom haystack_experimental import Pipeline\n\npipe = Pipeline()\n# ...\npipe.run(...)\n```\n\nSome experimental features come with example notebooks and resources that can be found in the [`examples` folder](https://github.com/deepset-ai/haystack-experimental/tree/main/examples).\n\n## Documentation\n\nDocumentation for `haystack-experimental` can be found [here](https://docs.haystack.deepset.ai/reference/experimental-data-classes-api).\n\n## Implementation\n\nExperiments should replicate the namespace of the core package. For example, a new generator:\n\n```python\n# in haystack_experimental/components/generators/foobar.py\n\nfrom haystack import component\n\n\n@component\nclass FoobarGenerator:\n    ...\n\n```\n\nWhen the experiment overrides an existing feature, the new symbol should be created at the same path in the experimental\npackage. This new symbol will override the original in `haystack-ai`: for classes, with a subclass and for bare\nfunctions, with a wrapper. For example:\n\n```python\n# in haystack_experiment/src/haystack_experiment/core/pipeline/pipeline.py\n\nfrom haystack.core.pipeline import Pipeline as HaystackPipeline\n\n\nclass Pipeline(HaystackPipeline):\n    # Any new experimental method that doesn't exist in the original class\n    def run_async(self, inputs) -> Dict[str, Dict[str, Any]]:\n        ...\n\n    # Existing methods with breaking changes to their signature, like adding a new mandatory param\n    def to_dict(new_param: str) -> Dict[str, Any]:\n        # do something with the new parameter\n        print(new_param)\n        # call the original method\n        return super().to_dict()\n\n```\n\n## Contributing\n\nDirect contributions to `haystack-experimental` are not expected, but Haystack maintainers might ask contributors to move pull requests that target the [core repository](https://github.com/deepset-ai/haystack) to this repository.\n\n## Telemetry\n\nAs with the Haystack core package, we rely on anonymous usage statistics to determine the impact and usefulness of the experimental features. For more information on what we collect and how we use the data, as well as instructions to opt-out, please refer to our [documentation](https://docs.haystack.deepset.ai/docs/telemetry).\n"
    },
    {
      "name": "Karthik-Bhaskar/Context-Based-Question-Answering",
      "stars": 43,
      "img": "https://avatars.githubusercontent.com/u/13200370?s=40&v=4",
      "owner": "Karthik-Bhaskar",
      "repo_name": "Context-Based-Question-Answering",
      "description": "Context-Based-Question-Answering",
      "homepage": "",
      "language": "JavaScript",
      "created_at": "2020-12-16T15:28:31Z",
      "updated_at": "2025-03-15T14:32:01Z",
      "topics": [
        "flask-application",
        "haystack",
        "natural-language-processing",
        "nlp",
        "question-answering",
        "transformers",
        "web-application"
      ],
      "readme": "## Context Based Question Answering \n[![Project Status: Inactive – The project has reached a stable, usable state but is no longer being actively developed; support/maintenance will be provided as time allows.](https://www.repostatus.org/badges/latest/inactive.svg)](https://www.repostatus.org/#inactive)\n [![GitHub license](https://img.shields.io/github/license/Karthik-Bhaskar/Context-Based-Question-Answering)](https://github.com/Karthik-Bhaskar/Context-Based-Question-Answering/blob/main/LICENSE)\n\n\n**Context-Based Question Answering** is an easy-to-use **Extractive QA** search engine, which extracts answers to the question based on the provided context.\n\n\n![](./docs_resources/CBQA_demo.gif)\n\n\n### Table of contents\n---\n\n* [Introduction](#introduction) \n* [Motivation](#motivation)\n* [Technologies](#technologies)\n* [Languages](#languages)\n* [Installation](#installation)\n* [Demo](#demo)\n* [Features](#features)\n* [Status](#status)\n* [License](#license)\n* [Credits](#credits)\n* [Citations](#citations)\n* [Community Contribution](#community-contribution)\n* [Contact](#contact)\n\n### Introduction\n---\n\n>**Question answering**  (**QA**) is a computer science discipline within the fields of  [information retrieval](https://en.wikipedia.org/wiki/Information_retrieval \"Information retrieval\")  and  [natural language processing](https://en.wikipedia.org/wiki/Natural_language_processing \"Natural language processing\")  (NLP), which is concerned with building systems that automatically answer questions posed by humans in a  [natural language](https://en.wikipedia.org/wiki/Natural_language \"Natural language\").\n>\n>Source: [Wikipedia](https://en.wikipedia.org/wiki/Question_answering)\n\n>**Extractive QA** is a popular task for natural language processing (NLP) research, where models must extract a short snippet from a document in order to answer a natural language question.\n>\n>  Source: [Facebook AI](https://ai.facebook.com/blog/research-in-brief-unsupervised-question-answering-by-cloze-translation/)\n\n\n\n**Context-Based Question Answering (CBQA)** is an inference web-based Extractive QA search engine, mainly dependent on [Haystack](https://github.com/deepset-ai/haystack) and [Transformers](https://github.com/huggingface/transformers) library.\n\nThe CBQA application allows the user to add context and perform Question Answering(QA) in that context.\n\nThe main components in this application use [Haystack's](https://github.com/deepset-ai/haystack) core components,\n\n> - **FileConverter**: Extracts pure text from files (pdf, docx, pptx, html and many more).\n> -  **PreProcessor**: Cleans and splits texts into smaller chunks.\n> -   **DocumentStore**: Database storing the documents, metadata and vectors for our search. We recommend Elasticsearch or FAISS, but have also more light-weight options for fast prototyping (SQL or In-Memory).\n>   - **Retriever**: Fast algorithms that identify candidate documents for a given query from a large collection of documents. Retrievers narrow down the search space significantly and are therefore key for scalable QA. Haystack supports sparse methods (TF-IDF, BM25, custom Elasticsearch queries) and state of the art dense methods (e.g. sentence-transformers and Dense Passage Retrieval)\n> - **Reader**: Neural network (e.g. BERT or RoBERTA) that reads through texts in detail to find an answer. The Reader takes multiple passages of text as input and returns top-n answers. Models are trained via  [FARM](https://github.com/deepset-ai/FARM)  or [Transformers](https://github.com/huggingface/transformers)  on SQuAD like tasks. You can just load a pretrained model from  [Hugging Face's model hub](https://huggingface.co/models)  or fine-tune it on your own domain data.\n>\n>Source: [Haystack's Key Components docs](https://github.com/deepset-ai/haystack/#key-components)\n\nIn CBQA the allowed formats for adding the context are,\n\n - Textual Context (Using the TextBox field)\n - File Uploads (.pdf, .txt, and .docx)\n\nThese contexts are uploaded to a temporary directory for each user for pre-processing and deletes after uploading them to **Elasticsearch,** which is the only **DocumentStore type** used in this system.\n\nEach user has a separate Elasticsearch index to store the context documents. Using the **PreProcessor** and the **FileConverter** modules from [Haystack](https://github.com/deepset-ai/haystack), the pre-processing and the extraction of text from context files is done.\n\n**Elasticsearcher Retriever** is used in CBQA to retrieve relevant documents based on the search query.\n\n**Transformers-based Readers** are used to extracting answers from the retrieved documents. The Readers used in CBQA are the pre-trained Transformers models hosted on the [Hugging Face's model hub](https://huggingface.co/models) .\n\nCurrently, CBQA provides the interface to perform QA in **English** and **French**, using four Transformers based models,\n\n - BERT\n - RoBERTa\n - DistilBERT\n - CamemBERT\n\nThe interface provides an option to choose the inference device between CPU and GPU.\n\nThe output is in a tabular form containing the following headers,\n\n-   **Answers** (Extracted answers based on the question and context)\n-   **Context** (Specifies the context window, related to the answer)\n-   **Document Title** (Specifies the title of context file, related to the answer)\n\n### Motivation\n---\n\nThe motivation to implement the CBQA project is to make an easy-to-use interface to perform Question Answering (QA) in multiple languages, with the option for using different pre-trained models.  Also, to enable organizations to use this project locally with minimal modifications or none.\n\n### Technologies\n---\n\n* Python (version 3.7)\n* Haystack  (version 0.7.0)\n* Transformers (version 4.3.1)\n* Elasticsearch (version 7.10)\n* Flask  (version 1.1.2)\n* Gunicorn (version 20.0.4)\n* Bootstrap (version 4.5.3)\n* jQuery (version 3.5.1)\n* HTML, CSS, and JavaScript\n* Docker\n\n### Languages\n---\n\n``en: English``\n\n``fr: French``\n\n### Installation\n---\n\nBefore starting the installation, clone this repository using the following commands in the terminal.\n\n```console\n$ git clone https://github.com/Karthik-Bhaskar/Context-Based-Question-Answering.git\n````\n\n```console\n$ cd Context-Based-Question-Answering/\n```\n\nYou can get started using one of the two options,\n\n 1. [Installation using pip](#installation-using-pip)\n 2. [Running as a docker container ](#running-as-a-docker-container )\n\n#### Installation using pip\n---\n\n\nThe main dependencies required to run the CBQA application is in the  [requirements.txt](./requirements.txt)\n\nTo install the dependencies,\n\n1.  Create a Python virtual environment with Python version 3.7 and activate it\n2.  Install dependency libraries using the following command in the terminal.\n```console  \n$ pip install -r requirements.txt\n```\n\nBefore executing the CBQA application, please start the Elasticsearch server. Elasticsearch is the DocumentStore type used in this application. To download and install the Elasticsearch, please check [here](https://www.elastic.co/downloads/elasticsearch).\n\nIn case you are using the docker environment, run Elasticsearch on docker using the following commands in the terminal. If you want to install the docker engine on your machine, please check [here](https://docs.docker.com/get-docker/).\n```console\n$ docker pull docker.elastic.co/elasticsearch/elasticsearch:7.10.0\n````\n```console\n$ docker run -p 9200:9200 -e \"discovery.type=single-node\" docker.elastic.co/elasticsearch/elasticsearch:7.10.0\n```\n\nMake sure the Elasticsearch server is running using the following command in the new terminal.\n```console\n$ curl http://localhost:9200\n```\n You should get a response like the one below.\n```console\n{\n\"name\" : \"facddac422e8\",\n\"cluster_name\" : \"docker-cluster\",\n\"cluster_uuid\" : \"a4A3-yBhQdKBlpSDkpRelg\",\n\"version\" : {\n\"number\" : \"7.10.0\",\n\"build_flavor\" : \"default\",\n\"build_type\" : \"docker\",\n\"build_hash\" : \"51e9d6f22758d0374a0f3f5c6e8f3a7997850f96\",\n\"build_date\" : \"2020-11-09T21:30:33.964949Z\",\n\"build_snapshot\" : false,\n\"lucene_version\" : \"8.7.0\",\n\"minimum_wire_compatibility_version\" : \"6.8.0\",\n\"minimum_index_compatibility_version\" : \"6.0.0-beta1\"\n},\n\"tagline\" : \"You Know, for Search\"\n}\n```\n\n\nAfter installing the dependency libraries and starting the Elasticsearch server, you are good to go.\n\n- _To run the application using a WSGI server like [Gunicorn](https://gunicorn.org)._ Use the following command in the new terminal.\n  ```console\n  $ gunicorn -w 1 --threads 4 app:app\n  ```\n\n  This runs the application in Gunicorn server on [http://localhost:8000](http://localhost:8000/) with a single worker and 4 threads.\n\n- _To run the application in the [flask](https://flask.palletsprojects.com/en/1.1.x/) development server (Not recommended using this in production)_. Use the following command in the new terminal.\n  ```console\n  $ python app.py\n  ```\n\n  Now the application will be running in the flask development server on [http://localhost/5000](http://localhost/5000).\n\nIn the application execution cases above, you should see the below statement (date and time will be different) in the terminal after the application has started.\n```console\nUser tracker thread started @  2021-03-04 18:25:20.803277\n```\n\nThe above statement means that the thread handling the user connection has started, and the application is ready to accept users.\n\n\n**Note**: When performing QA using pre-trained models, at first use, the selected model gets downloaded from the [Hugging Face's model hub](https://huggingface.co/models) this may take a while depending on your internet speed. If you are re-starting the application while you are testing, make sure to remove the auto-created temporary user directories in the project and the user index on Elasticsearch (Will be fixed soon)\n\n\n#### Running as a docker container\n---\n\n\n\nTo install the docker engine on your machine, please check [here](https://docs.docker.com/get-docker/).\n\nThis project includes the [docker-compose.yml](./docker-compose.yml) file describing the services to get started quickly. The specified services in the file are Elasticsearch, Kibana, and the CBQA application.\n\n**Note**: Make sure to increase the resources (Memory) in your docker engine in case you are facing 137 exit code (out of memory).\n\nThe [docker image](https://hub.docker.com/repository/docker/karthikbhaskar/context-based-question-answering) for the CBQA application has already been built and hosted on [the docker hub](https://hub.docker.com/).\n\nTo start the complete package that includes Elasticsearch, Kibana, and CBQA application as a docker container.\n\nUse the following command in the terminal from the root of this project directory to start the container.\n```\n$ docker-compose up\n```\n\nPlease wait until all the services have started after pulling the images.\n\nMake sure the Elasticsearch server is running using the following command in the new terminal.\n```console\n$ curl http://localhost:9200\n```\n\nYou should get a response like the one below.\n```console\n{\n\"name\" : \"facddac422e8\",\n\"cluster_name\" : \"docker-cluster\",\n\"cluster_uuid\" : \"a4A3-yBhQdKBlpSDkpRelg\",\n\"version\" : {\n\"number\" : \"7.10.0\",\n\"build_flavor\" : \"default\",\n\"build_type\" : \"docker\",\n\"build_hash\" : \"51e9d6f22758d0374a0f3f5c6e8f3a7997850f96\",\n\"build_date\" : \"2020-11-09T21:30:33.964949Z\",\n\"build_snapshot\" : false,\n\"lucene_version\" : \"8.7.0\",\n\"minimum_wire_compatibility_version\" : \"6.8.0\",\n\"minimum_index_compatibility_version\" : \"6.0.0-beta1\"\n},\n\"tagline\" : \"You Know, for Search\"\n}\n```\n\nThe CBQA application service in the docker container runs on [http://localhost:5000](http://localhost:5000/), the container name of the application will be **web-app**.\n\nElasticsearch service will be running on [http://localhost:9200](http://localhost:9200/), and Kibana service will be running on [http://localhost:5601](http://localhost:5601/).\n\n**Note**: When performing QA using pre-trained models, at first use, the selected model gets downloaded from the [Hugging Face's model hub](https://huggingface.co/models) this may take a while depending on your internet speed. If you are re-starting the application while you are testing, make sure to remove the auto-created temporary user directories in the project and the user index on Elasticsearch (Will be fixed soon)\n\nTo stop the container, use the following command in the new terminal.\n```console\n$ docker-compose down\n```\n\n### Demo\n---\n*Full demo video on YouTube.*\n\n[![Alt text](https://img.youtube.com/vi/vqy5XmDwKMQ/0.jpg)](https://youtu.be/vqy5XmDwKMQ)\n\n### Features\n---\n\nReady:\n* Easy to use UI to perform QA\n* Pre-trained models selection\n* QA Language support (English and French)\n* Simultaneous user handling support (While using WSGI server)\n\nFuture development:\n* Ability to plugin large scale document corpus for the QA engine\n* Generative QA support\n* Additional QA Language support\n* Improvising UI\n\n### Status\n---\n\n[![Project Status: Inactive – The project has reached a stable, usable state but is no longer being actively developed; support/maintenance will be provided as time allows.](https://www.repostatus.org/badges/latest/inactive.svg)](https://www.repostatus.org/#inactive)\n\n\n### License\n---\n\n\n[![GitHub license](https://img.shields.io/github/license/Karthik-Bhaskar/Context-Based-Question-Answering)](https://github.com/Karthik-Bhaskar/Context-Based-Question-Answering/blob/main/LICENSE)\n\n### Credits\n---\n\n - [Haystack](https://github.com/deepset-ai/haystack) ([deepset](https://deepset.ai))\n - [Transformers](https://github.com/huggingface/transformers) ([Hugging Face](https://huggingface.co))\n - Pre-trained Transformers Models on [Hugging Face's model hub](https://huggingface.co/models) \t\n    - [roberta-base-squad2](https://huggingface.co/deepset/roberta-base-squad2) (deepset)\n\t- [bert-large-uncased-whole-word-masking-squad2](https://huggingface.co/deepset/bert-large-uncased-whole-word-masking-squad2)(deepset)\n\t- [distilbert-base-cased-distilled-squad](https://huggingface.co/distilbert-base-cased-distilled-squad)\n\t- [camembert-base-fquad](https://huggingface.co/illuin/camembert-base-fquad) (illuin)\n- [Elasticsearch and Kibana](https://www.elastic.co)\n- [Flask](https://flask.palletsprojects.com/en/1.1.x/)\n- [Gunicorn](https://gunicorn.org)\n- [Bootstrap](https://getbootstrap.com)\n- [DropzoneJS](https://www.dropzonejs.com)\n- [Docker](https://www.docker.com)\n- [Sanjay R Kamath, PhD (Research Scientist, NLP)](https://twitter.com/sanjaykamath)\n- Open-Source Community ❤️\n\n### Citations\n---\n\n```bibtex\n@article{DBLP:journals/corr/abs-1810-04805,\n  author    = {Jacob Devlin and\n               Ming{-}Wei Chang and\n               Kenton Lee and\n               Kristina Toutanova},\n  title     = {{BERT:} Pre-training of Deep Bidirectional Transformers for Language\n               Understanding},\n  journal   = {CoRR},\n  volume    = {abs/1810.04805},\n  year      = {2018},\n  url       = {http://arxiv.org/abs/1810.04805},\n  archivePrefix = {arXiv},\n  eprint    = {1810.04805},\n  timestamp = {Tue, 30 Oct 2018 20:39:56 +0100},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-1810-04805.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n\n```\n\n```bibtex\n@article{DBLP:journals/corr/abs-1907-11692,\n  author    = {Yinhan Liu and\n               Myle Ott and\n               Naman Goyal and\n               Jingfei Du and\n               Mandar Joshi and\n               Danqi Chen and\n               Omer Levy and\n               Mike Lewis and\n               Luke Zettlemoyer and\n               Veselin Stoyanov},\n  title     = {RoBERTa: {A} Robustly Optimized {BERT} Pretraining Approach},\n  journal   = {CoRR},\n  volume    = {abs/1907.11692},\n  year      = {2019},\n  url       = {http://arxiv.org/abs/1907.11692},\n  archivePrefix = {arXiv},\n  eprint    = {1907.11692},\n  timestamp = {Thu, 01 Aug 2019 08:59:33 +0200},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-1907-11692.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n```\n\n```bibtex\n@article{DBLP:journals/corr/abs-1910-01108,\n  author    = {Victor Sanh and\n               Lysandre Debut and\n               Julien Chaumond and\n               Thomas Wolf},\n  title     = {DistilBERT, a distilled version of {BERT:} smaller, faster, cheaper\n               and lighter},\n  journal   = {CoRR},\n  volume    = {abs/1910.01108},\n  year      = {2019},\n  url       = {http://arxiv.org/abs/1910.01108},\n  archivePrefix = {arXiv},\n  eprint    = {1910.01108},\n  timestamp = {Tue, 02 Jun 2020 12:48:59 +0200},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-1910-01108.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n```\n\n```bibtex\n@article{DBLP:journals/corr/abs-1911-03894,\n  author    = {Louis Martin and\n               Benjamin M{\\\"{u}}ller and\n               Pedro Javier Ortiz Su{\\'{a}}rez and\n               Yoann Dupont and\n               Laurent Romary and\n               {\\'{E}}ric Villemonte de la Clergerie and\n               Djam{\\'{e}} Seddah and\n               Beno{\\^{\\i}}t Sagot},\n  title     = {CamemBERT: a Tasty French Language Model},\n  journal   = {CoRR},\n  volume    = {abs/1911.03894},\n  year      = {2019},\n  url       = {http://arxiv.org/abs/1911.03894},\n  archivePrefix = {arXiv},\n  eprint    = {1911.03894},\n  timestamp = {Sun, 01 Dec 2019 20:31:34 +0100},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-1911-03894.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n```\n\n```bibtex\n@article{dHoffschmidt2020FQuADFQ,\n  title={FQuAD: French Question Answering Dataset},\n  author={Martin d'Hoffschmidt and Maxime Vidal and Wacim Belblidia and Tom Brendl'e and Quentin Heinrich},\n  journal={ArXiv},\n  year={2020},\n  volume={abs/2002.06071}\n}\n```\n\n\n\n### Community Contribution\n---\n\nYou are most welcome to contribute to this project. Be it a small typo correction or a feature enhancement. Currently, there is no structure to follow for contribution, but you can open an Issue under \"Community contribution\" or contact me to discuss it before getting started 🙂\n\n\n### Contact\n---\n\n [Karthik Bhaskar](https://www.karthikbhaskar.in/) - Feel free to contact me. \n\n <a href=\"https://github.com/Karthik-Bhaskar\" target=\"blank\"><img align=\"center\" src=\"https://cdn.jsdelivr.net/npm/simple-icons@3.0.1/icons/github.svg\" alt=\"Karthik-Bhaskar\" height=\"25\" width=\"30\" /></a>  <a href=\"https://twitter.com/karthik_bhaskar\" target=\"blank\"><img align=\"center\" src=\"https://cdn.jsdelivr.net/npm/simple-icons@3.0.1/icons/twitter.svg\" alt=\"karthik_bhaskar\" height=\"25\" width=\"30\" /></a>  <a href=\"https://linkedin.com/in/contactkarthikbhaskar\" target=\"blank\"><img align=\"center\" src=\"https://cdn.jsdelivr.net/npm/simple-icons@3.0.1/icons/linkedin.svg\" alt=\"contactkarthikbhaskar\" height=\"25\" width=\"30\" /></a>  \n  \n"
    },
    {
      "name": "ploomber/doc",
      "stars": 34,
      "img": "https://avatars.githubusercontent.com/u/60114551?s=40&v=4",
      "owner": "ploomber",
      "repo_name": "doc",
      "description": "Documentation for Ploomber Cloud",
      "homepage": "https://docs.cloud.ploomber.io",
      "language": "Jupyter Notebook",
      "created_at": "2023-06-30T15:58:26Z",
      "updated_at": "2025-04-08T15:58:16Z",
      "topics": [],
      "readme": "# Ploomber Cloud documentation\n\n## Notes\n\nWe're setting `meta` and `og` data in the front matter. There's a\n[package](https://github.com/wpilibsuite/sphinxext-opengraph) to do it, but I\ncouldn't get the field lists to work."
    },
    {
      "name": "zazencodes/zazencodes-season-1",
      "stars": 34,
      "img": "https://avatars.githubusercontent.com/u/13775567?s=40&v=4",
      "owner": "zazencodes",
      "repo_name": "zazencodes-season-1",
      "description": "Code for my youtube tutorials",
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2023-09-28T22:06:24Z",
      "updated_at": "2025-03-27T11:00:44Z",
      "topics": [],
      "readme": "# 🪷 ZazenCodes YouTube\n\n🧘 Author: [Alexander Galea](https://alexgalea.ca/)\n🎬 YouTube: [ZazenCodes](https://www.youtube.com/@ZazenCodes)\n\n## 📒 Index\n- [Machine Learning](https://github.com/agalea91/zazencodes-youtube?tab=readme-ov-file#-machine-learning)\n- [Developer Fundamentals](https://github.com/agalea91/zazencodes-youtube?tab=readme-ov-file#-developer-fundamentals)\n- [Python Fundamentals](https://github.com/agalea91/zazencodes-youtube?tab=readme-ov-file#-python-fundamentals)\n- [Productivity](https://github.com/agalea91/zazencodes-youtube?tab=readme-ov-file#-productivity)\n- [Coding Tutorials](https://github.com/agalea91/zazencodes-youtube?tab=readme-ov-file#-coding-tutorials)\n- [ChatGPT (LLM) Prompts](https://github.com/agalea91/zazencodes-youtube?tab=readme-ov-file#-chatgpt-llm-prompts)\n\n## 📚 Topics\n\n### 🔮 AI Engineering\n- [Deploy an AI Microservice with Kubernetes](https://github.com/zazencodes/zazencodes-youtube/tree/main/src/rag-microservice-python#kubernetes-deployment) | [YouTube 🎬](https://youtu.be/ETCN_HdvLEU)\n- [Docker Microservice Deployment with Container Registry](https://github.com/zazencodes/zazencodes-youtube/tree/main/src/rag-microservice-python#container-registry-deployment) | [YouTube 🎬](https://youtu.be/D9sG8mC-ioE)\n- [Deploying an AI Docker app on DigitalOcean](https://github.com/zazencodes/zazencodes-youtube/tree/main/src/rag-microservice-python#basic-server-deployment) | [YouTube 🎬](https://youtu.be/6aYclVrsqu0)\n- [RAG Microservice with Python](https://github.com/agalea91/zazencodes-youtube/tree/main/src/rag-microservice-python) | [YouTube 🎬](https://youtu.be/PJaqp5Kdwz0)\n\n### 🧠 Machine Learning\n- [Multimodal Embeddings Series](https://github.com/agalea91/zazencodes-youtube/tree/main/src/multimodal-embeddings) | [YouTube 1 🎬](https://youtu.be/1fI4eRoKhrc), [2 🎬](https://youtu.be/Fu-W2VyJYRc), [3 🎬](https://youtu.be/875RhZwa-6M), [4 🎬](https://youtu.be/XPA213k8G_U)\n- [Text Embeddings with Python](https://github.com/agalea91/zazencodes-youtube/tree/main/src/text-embeddings-intro) | [YouTube 🎬](https://youtu.be/pfRA3Scz3Fw)\n- [Containerizing an ML Model with Docker](https://github.com/agalea91/zazencodes-youtube/tree/main/src/docker-meal-demand-forecasting) | [YouTube 🎬](https://youtu.be/KYxlM0hh96o)\n- [Model Cards for Model Reporting](https://github.com/agalea91/zazencodes-youtube/tree/main/src/model-cards-for-model-reporting/notebooks) | [YouTube 🎬](https://youtu.be/saAUB_MG2d0)\n\n### 🐋 Developer Fundamentals\n- [Replace your YAML configs with Pydantic](https://github.com/agalea91/zazencodes-youtube/tree/main/src/replace-yaml-with-pydantic) | [YouTube 🎬](https://youtu.be/4JAdedd_G-w)\n- [5 Docker tricks for rapid development](https://github.com/agalea91/zazencodes-youtube/tree/main/src/docker-rapid-development-tricks) | [YouTube 🎬](https://youtu.be/3e8J_pv-xJI)\n- [Docker Runtime Arguments](https://github.com/agalea91/zazencodes-youtube/tree/main/src/docker-runtime-arguments) | [YouTube 🎬](https://youtu.be/05ptDtpyAGo)\n\n### 🐍 Python Fundamentals\n- [Python Project Starter Pack Series](https://github.com/agalea91/zazencodes-youtube/tree/main/src/python-project-starter-kit/shopping-cart-app) | [YouTube 1 🎬](https://youtu.be/niMybnzmzqc), [2 🎬](https://youtu.be/ns73xOcl9es), [3 🎬](https://youtu.be/dlCcnJdh4c4)\n- [Python List Comprehensions](https://github.com/agalea91/zazencodes-youtube/tree/main/src/python-list-comprehensions-noob-to-pro) | [YouTube 🎬](https://youtu.be/1-qmfHfDg6k)\n- [The Zen of Python](https://github.com/agalea91/zazencodes-youtube/tree/main/src/zen-of-python) | [YouTube 🎬](https://youtu.be/P-ipB6nj0kA)\n\n### 🎯 Productivity\n- [Neovim Lazy Lua IDE - my simple but powerful setup for 2024](https://github.com/agalea91/zazencodes-youtube/tree/main/src/neovim-lazy-ide-2024) | [YouTube 🎬](https://youtu.be/VljhZ0e9zGE)\n\n### 💻 Coding Tutorials\n- [Easy Email Alerts for BigQuery Data using this Google Cloud Function](https://github.com/agalea91/youtube-codes/tree/main/src/bigquery-gcs-gcf-email-alerts) | [YouTube 🎬](https://youtu.be/g5a9JHFjVX4)\n- [Mystic Hallucinations: Coding an AI Generated Poetry Web App [ChatGPT + DALL-E]](https://github.com/agalea91/youtube-codes/tree/main/src/mystic-hallucinations-app) | [YouTube 🎬](https://youtu.be/MNJXjLxvFZI)\n- [OpenAI API Quickstart](https://github.com/agalea91/youtube-codes/tree/main/src/openai-api-quickstart/notebooks/src) | [YouTube 🎬](https://youtu.be/g9tIm50VO4g)\n\n### 🤖 ChatGPT (LLM) Prompts\n- [Chain of Density Summary](https://github.com/agalea91/youtube-codes/tree/main/src/chain-of-density-summary/notebooks/src) | [YouTube 🎬](https://youtu.be/9Ev208-Gc4c)\n\n"
    },
    {
      "name": "gilad-rubin/modular-rag",
      "stars": 32,
      "img": "https://avatars.githubusercontent.com/u/6134299?s=40&v=4",
      "owner": "gilad-rubin",
      "repo_name": "modular-rag",
      "description": "This project implements the \"Modular RAG\" framework using Haystack & Hypster",
      "homepage": "",
      "language": "Jupyter Notebook",
      "created_at": "2024-10-09T14:28:48Z",
      "updated_at": "2025-04-17T01:17:06Z",
      "topics": [],
      "readme": "# Modular RAG with Haystack and Hypster\n\n<p align=\"center\">\n  <img src=\"assets/modular-rag.png\" alt=\"Modular RAG\" width=\"100%\">\n</p>\n\nThis project implements the concepts described in the paper: [Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable Frameworks](https://arxiv.org/abs/2407.21059) by Yunfan Gao et al. \n\nFor a detailed walkthrough, refer to this [Medium article](https://medium.com/p/d2f0ecc88b8f).\n\n## Key Objectives\n\n- Decompose RAG (Retrieval-Augmented Generation) into its fundamental components using **Haystack**.\n- Utilize **Hypster** to manage a \"hyper-space\" of potential RAG configurations.\n- Facilitate easy swapping and experimentation with various implementations.\n\n## Table of Contents\n\n1. [Installation](#installation)\n2. [Usage](#usage)\n3. [Code Examples](#code-examples)\n4. [Contributing](#contributing)\n5. [License](#license)\n\n## Installation and Usage\n\nThis project supports multiple package managers. Choose the method that best suits your environment:\n\n1. Clone the repository:\n   ```bash\n   git clone https://github.com/gilad-rubin/modular-rag.git\n   cd modular-rag\n\n2. Install dependencies using one of the following methods:\n\n### uv\n\n```bash\nuv run --with jupyter jupyter lab\n```\n\n### Conda\n\n```bash\nconda env create -n modular-rag python=3.10 -y\nconda activate modular-rag\npip install -r requirements.txt\n\njupyter lab\n```\n\n### pip\n\n```bash\npip install -r requirements.txt\njupyter lab\n```\n\n3. Open the main notebook and follow the instructions to experiment with different RAG configurations.\n\n## Code Examples\n\nHere's a basic example of how to use the modular RAG system:\n\n```python\nresults = modular_rag(\n    values={\n        \"indexing.enrich_doc_w_llm\": True,\n        \"indexing.llm.model\": \"gpt-4o-mini\",\n        \"document_store_type\": \"qdrant\",\n        \"retrieval.bm25_weight\": 0.8,\n        \"embedder_type\": \"fastembed\",\n        \"reranker.model\": \"tiny-bert-v2\",\n        \"response.llm.model\": \"haiku\",\n        \"indexing.splitter.split_length\": 6,\n        \"reranker.top_k\": 3\n    },\n)\nindexing_pipeline = results[\"indexing_pipeline\"]\nindexing_pipeline.warm_up()\nfile_paths = [\"data/raw/modular_rag.pdf\"]\nfor file_path in file_paths:  # this can be parallelized\n    indexing_pipeline.run({\"loader\": {\"sources\": [file_path]}})\nquery = \"What are the 6 main modules of the modular RAG framework?\"\n\npipeline = results[\"pipeline\"]\npipeline.warm_up()\nresponse = pipeline.run({\"query\": {\"text\": query}}, include_outputs_from=[\"prompt_builder\", \"docs_for_generation\"])\nprint(response[\"llm\"][\"replies\"][0])\n"
    },
    {
      "name": "Prismadic/magnet",
      "stars": 31,
      "img": "https://avatars.githubusercontent.com/u/153389283?s=40&v=4",
      "owner": "Prismadic",
      "repo_name": "magnet",
      "description": "the small distributed language model toolkit; fine-tune state-of-the-art LLMs anywhere, rapidly",
      "homepage": "https://prismadic.github.io/magnet/",
      "language": "Python",
      "created_at": "2023-12-12T14:11:21Z",
      "updated_at": "2025-04-03T12:01:01Z",
      "topics": [
        "apple-silicon",
        "claude",
        "distributed-computing",
        "distributed-systems",
        "embeddings",
        "fine-tuning",
        "finetuning-llms",
        "gemini",
        "huggingface",
        "inference-api",
        "langchain",
        "llm-training",
        "milvus",
        "mistral",
        "mlx",
        "nats",
        "nats-messaging",
        "nats-streaming",
        "sentence-splitting",
        "tokenizers"
      ],
      "readme": "<p align=\"center\">\n   <img height=\"300\" width=\"300\" src=\"./magnet.png\">\n   <br>\n\n   <h1 align=\"center\">magnet</h1>\n\n   <h3 align=\"center\"><a href=\"https://prismadic.github.io/magnet/\">📖 docs</a> | 💻 <a href=\"https://github.com/Prismadic/magnet/tree/main/examples\">examples</a> | 📓 <a href=\"https://prismadic.substack.com\">substack</a></h3> \n\n   <p align=\"center\">the small distributed language model toolkit</p>\n   <p align=\"center\"><i>⚡️ fine-tune state-of-the-art LLMs anywhere, rapidly ⚡️</i></p>\n   <div align=\"center\">\n</p>\n\n![GitHub release (with filter)](https://img.shields.io/github/v/release/prismadic/magnet)\n![PyPI - Version](https://img.shields.io/pypi/v/llm_magnet)\n![GitHub Workflow Status (with event)](https://img.shields.io/github/actions/workflow/status/prismadic/magnet/python-publish.yml)\n![GitHub code size in bytes](https://img.shields.io/github/languages/code-size/prismadic/magnet)\n![GitHub last commit (branch)](https://img.shields.io/github/last-commit/prismadic/magnet/main)\n![GitHub issues](https://img.shields.io/github/issues/prismadic/magnet)\n![GitHub Repo stars](https://img.shields.io/github/stars/prismadic/magnet)\n![GitHub watchers](https://img.shields.io/github/watchers/prismadic/magnet)\n![PyPI - Downloads](https://img.shields.io/pypi/dm/llm_magnet)\n![PyPI - Wheel](https://img.shields.io/pypi/wheel/llm_magnet)\n![X (formerly Twitter) Follow](https://img.shields.io/twitter/follow/prismadic?style=social&link=https%3A%2F%2Fx.com%2Fprismadic)\n\n   </div>\n\n</p>\n\n<img src='./divider.png' style=\"width:100%;height:5px;\">\n\n## 🧬 Installation\n\n``` bash\npip install llm-magnet\n```\n\nor\n\n``` bash\npython3 setup.py install\n```\n\n<img src='./divider.png' style=\"width:100%;height:5px;\">\n\n## 🎉 usage\n\n[check out the example notebooks](./examples/)\n\n<small>a snippet to get you started</small>\n\n``` python\nfrom magnet.base import Magnet\nfrom magnet.base import EmbeddedMagnet\n\ncluster = EmbeddedMagnet()\ncluster.start()\nmagnet = cluster.create_magnet()\nawait magnet.align()\n\nconfig = {\n    \"host\": \"127.0.0.1\",\n    \"credentials\": None,\n    \"domain\": None,\n    \"name\": \"my_stream\",\n    \"category\": \"my_category\",\n    \"kv_name\": \"my_kv\",\n    \"session\": \"my_session\",\n    \"os_name\": \"my_object_store\",\n    \"index\": {\n        \"milvus_uri\": \"127.0.0.1\",\n        \"milvus_port\": 19530,\n        \"milvus_user\": \"test\",\n        \"milvus_password\": \"test\",\n        \"dimension\": 1024,\n        \"model\": \"BAAI/bge-large-en-v1.5\",\n        \"name\": \"test\",\n        \"options\": {\n            'metric_type': 'COSINE',\n            'index_type':'HNSW',\n            'params': {\n                \"efConstruction\": 40\n                , \"M\": 48\n            }\n        }\n    }\n}\n\nmagnet = Magnet(config)\nawait magnet.align()\n```\n\n<img src='./divider.png' style=\"width:100%;height:5px;\">\n\n## 🔮 features\n\n<center>\n<img src=\"./clustered_bidirectional.png\" style=\"width:50%;transform: rotate(90deg);margin-top:200px;\" align=\"right\">\n</center>\n\n- ⚡️ **It's Fast**\n  - <small>fast on consumer hardware</small>\n  - <small>_very_ fast on Apple Silicon</small>\n  - <small>**extremely** fast on ROCm/CUDA</small>\n- 🫵 **Automatic or your way**\n  - <small>rely on established transformer patterns to let `magnet` do the work</small>\n  - <small>keep your existing data processing functions, bring them to `magnet`!</small>\n- 🛰️ **100% Distributed**\n  - <small>processing, embedding, storage, retrieval, querying, or inference from anywhere</small>\n  - <small>as much or as little compute as you need</small>\n- 🧮 **Choose Inference Method**\n  - <small>HuggingFace</small>\n  - <small>vLLM node</small>\n  - <small>GPU</small>\n  - <small>mlx</small>\n- 🌎 **Huge Volumes**\n  - <small>handle gigantic amounts of data inexpensively</small>\n  - <small>fault-tolerant by design</small>\n  - <small>decentralized workloads</small>\n- 🔐 **Secure**\n  - <small>JWT</small>\n  - <small>Basic</small>\n- 🪵 **World-Class Comprehension**\n  - <small>`magnet` optionally logs its own code as it's executed (yes, really)</small>\n  - <small>build a self-aware system and allow it to learn from itself</small>\n  - <small>emojis are the future</small>\n\n<img src='./divider.png' style=\"width:100%;height:5px;\">\n\n## 🧲 why\n\n- build a distributed LLM research node with any hardware, from Rasbperry Pi to the expensive cloud\n- Apple silicon first-class citizen with [mlx](https://github.com/ml-explore/mlx)\n- embed & index to vector db with [milvus](https://milvus.io)\n- distributed processing with [NATS](https://nats.io)\n- upload to S3\n- ideal cyberpunk vision of LLM power users in vectorspace\n\n<img src='./divider.png' style=\"width:100%;height:5px;\">\n"
    },
    {
      "name": "dod-advana/gamechanger-data",
      "stars": 31,
      "img": "https://avatars.githubusercontent.com/u/82123680?s=40&v=4",
      "owner": "dod-advana",
      "repo_name": "gamechanger-data",
      "description": "GAMECHANGER aspires to be the Department’s trusted solution for evidence-based, data-driven decision-making across the universe of DoD requirements",
      "homepage": "",
      "language": "Python",
      "created_at": "2021-04-15T16:34:30Z",
      "updated_at": "2025-04-20T13:06:15Z",
      "topics": [
        "defense",
        "etl",
        "policy",
        "policy-as-code"
      ],
      "readme": "<img src=\"./img/tags/GAMECHANGER-NoPentagon_RGB@3x.png\" align=\"right\"\n     alt=\"Mission Vision Icons\" width=\"300\" >\n\n<h1>\n<img src=\"./img/icons/RPA.png\" alt=\"Data Engineering\" width=\"70\" aling=\"left\"  >\n     Data Engineering\n</h1> \n\n`gamechanger-data` focuses on the data engineering work of gamechanger. To see all repositories [gamechanger](https://github.com/dod-advana/gamechanger)\n\n\n# Important Note!\n- Configuration of repo is reliant on being able to hit advana-data-zone's s3 bucket. If you do not have access to advana-data-zone's s3 bucket, you will need to fill in your own values in config script; like topic_models __(for ML features)__ and configure_app __(ElasticSearch, Postgres, and Neo4j)__\n- Once venv is set up, set ``DEPLOYMENT_ENV`` variable and run ``./paasJobs/configure_repo.sh`` or ``paasJobs/configure_repo.bat`` <br>\nExample ``DEPLOYMENT_ENV=local ./paasJobs/configure_repo.sh`` or ``set DEPLOYMENT_ENV=local \\paasJobs\\configure_repo.bat``\n\n## (Linux) Dev/Prod Deployment Instructions\n\n- Clone fresh `gamechanger-data` repo\n- Setup python3.8 venv with packages in requirements.txt.\n  - Create python3.8 venv, e.g. `python3 -m venv /opt/gc-venv-20210613`\n  - Before installing packages, update pip/wheel/setuptools, e.g. `<venv>/bin/pip install --upgrade pip setuptools wheel`\n  - Install packages from `requirements.txt`, with no additional dependencies, e.g. `<venv>/bin/pip install --no-deps -r requirements.txt`\n- Set up symlink `/opt/gc-venv-current` to the freshly created venv, e.g. `ln -s /opt/gc-venv-20210613 /opt/gc-venv-current`\n- Pull in other dependencies and configure repo with `env SCRIPT_ENV=<prod|dev> <repo>/paasJobs/configure_repo.sh`\n    - Config script will let you know if everything was configured correctly and if all backends can be reached.\n    \n## How to Setup Local Env for Development\n\n### MacOS / Linux\n- (Linux Only) Follow instruction appropriate to repo to install `ocrmypdf` and its dependencies: https://ocrmypdf.readthedocs.io/en/latest/installation.html#installing-on-linux\n- (MacOS Only) Install \"brew\" then use it to install tesseract `brew install tesseract-lang `\n- Install Miniconda or Anaconda (Miniconda is much smaller)\n    - `https://docs.conda.io/en/latest/miniconda.html`\n- Create gamechanger python3.8 environment, like so:\n    - `conda create -n gc python=3.8`\n- Clone the repo and change into that dir `git clone ...; cd gamechanger`\n- Activate conda environment and install requirements:\n    - :bangbang: <span style=\"color:red\"> reeeealy important - make sure you change into repo directory</span>\n    - `conda activate gc`\n    - `pip install --upgrade pip setuptools wheel`\n    - `pip install -e '.[dev]'`  (quoting around .[dev] is <span style=\"color:red\">important</span>)\n- That's it.\n\n### Windows (WSL Version)\n\n- Setup Windows Subsystem for Linux (WSL) environment\n    - `https://docs.microsoft.com/en-us/windows/wsl/install-win10`\n- (In WSL)\n    - Install ocrmypdf dependencies following ubuntu instructions here: https://ocrmypdf.readthedocs.io/en/latest/installation.html#installing-on-linux\n    - Install Miniconda or Anaconda (Miniconda is much smaller)\n        - `https://docs.conda.io/en/latest/miniconda.html`\n    - Create gamechanger python3.8 environment, like so:\n        - `conda create -n gc python=3.8`\n    - Clone the repo and change into that dir `git clone ...; cd gamechanger-data`\n    - Activate conda environment and install requirements:\n        - :bangbang: <span style=\"color:red\"> reeeealy important - make sure you change into repo directory</span>\n        - `conda activate gc`\n        - `pip install --upgrade pip setuptools wheel`\n        - `pip install -e '.[dev]'` (quoting around .[dev] is <span style=\"color:red\">important</span>)\n    - That's it, just activate that conda env if you want to use it inside the terminal.\n    \n### Windows\nCreate venv\n```python -m venv [venv-name]```\nActivate\n```\\[venv-name]\\Scripts\\activate```\nUpdate venv\n```python -m pip install --upgrade pip setuptools wheel```\nInstall requirements.txt\n```pip install --no-deps -r dev_tools\\requirements\\gc-venv-current.txt```\n\nRun Configure Repo, Steps at the top of this README\n\n__To-Do:__\n- convert .sh scripts to .bat to support window users\n\n### Docker\n```shell\ndocker build -t gc-data --no-cache .\ndocker rm -f gc-data-test || true\ndocker run -it --name gc-data gc-data\n```\nConfigure Repo\n\n## IDE SETUP\n\n### How to Setup PyCharm IDE\n\nNote: If you're using containerized env, you'll need Pro version of PyCharm and separate set of instructions - [here](https://www.jetbrains.com/help/pycharm/using-docker-as-a-remote-interpreter.html)\n\n\n- Create new project by opening directory where you cloned the repository. PyCharm will\ntell you that it sees existing repo there, just accept that and proceed.\n- With your gc conda environment all good to go, change your `\"Preferences -> Project -> Python Interpreter\"`\nto the **EXISTING** `gc` conda env you created. https://www.jetbrains.com/help/pycharm/conda-support-creating-conda-virtual-environment.html\n- Now, change your `\"Preferences -> Build, Execution, Deployment -> Console -> Python Console interpreter\"`\nto your `gc` conda interpreter env that you added earlier.\n- That's it, you will now have correct env in Terminal, Python Console, and elsewhere in the IDE.\n\n### How to Setup Visual Studio Code IDE\n\nNote: if you're using containerized env, you'll need setup like [this](https://code.visualstudio.com/learn/develop-cloud/containers)\n\n- Open the cloned dir in new workspace and make sure to set your conda `gc` venv as the python venv\nhttps://code.visualstudio.com/docs/python/environments\n- That's it, when you start new integrated terminals, they'll activate the right environment and\nthe syntax highlighting/autocompletion is going to work as it's supposed to.\n\n## Common Issues\n\n**My venv is broken somehow!**\n- Delete the old conda environment and create a new one,\nfollow steps above to reinstall it.\n\n## License & Contributions\nSee LICENSE.md (including licensing intent - INTENT.md) and CONTRIBUTING.md"
    },
    {
      "name": "deepset-ai/rag-with-nvidia-nims",
      "stars": 31,
      "img": "https://avatars.githubusercontent.com/u/51827949?s=40&v=4",
      "owner": "deepset-ai",
      "repo_name": "rag-with-nvidia-nims",
      "description": "🚀 Use NVIDIA NIMs with Haystack pipelines",
      "homepage": "https://haystack.deepset.ai/blog/haystack-nvidia-nim-rag-guide",
      "language": "Python",
      "created_at": "2024-05-08T09:30:23Z",
      "updated_at": "2025-04-16T10:21:25Z",
      "topics": [
        "ai",
        "deployment",
        "haystack",
        "llm"
      ],
      "readme": "# Build Air-Gapped RAG with Nvidia NIMs and Haystack\n\n📚 This repository is accompanied by our article [\"Building RAG Applications with NVIDIA NIM and Haystack on K8s\"](https://haystack.deepset.ai/blog/haystack-nvidia-nim-rag-guide)\n\n> Info: This repo is set up to use models hosted and accessible via https://build.nvidia.com/ \n>\n> These models are already available and you can use them by creating yourself API keys through the platform.\n> The project is set up so that you can change these models to NIM deployments by setting the `model` name and `api_url` in the `NvidiaGenerator`, `NvidiaDocumentEmbedder` and `NvidiaTextEmbedder` components.\n> \n> 👩🏻‍🍳 We also provide a notebook on Haystack Cookbooks that provide the same code and setup, only expecting self-hosted NIMs\n> \n> <a href=\"https://colab.research.google.com/github/deepset-ai/haystack-cookbook/blob/main/notebooks/rag-with-nims.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n\n## Run with Docker\n\n1. `pip install -r requirements.txt`\n2. Create a `.env` file and add `NVIDIA_API_KEY` (if you're using hosted models via https://build.nvidia.com/)\n3. `docker-compose up`\n6. `hayhooks deploy rag.yaml`\n7. Go to `localhost:1416/docs` to interact with your RAG pipeline\n\n## File Structure\n\n- `indexing.py`: This script preproecesses, embeds and writes ChipNemo.pdf into a Qdrant database\n- `rag.py`: This scripts runs a RAG pipeline with a NIM LLM and retrieval model. \n- `Dockerfile`: This is used by the docker-compose file to install dependencies\n- `docker-compose.yml`: This is the docker compose file we use to spin up a container for hayhooks (Haystack pipeline deployment) and Qdrant\n- `rag.yaml`: This is the serialized RAG pipeline which is the same as `rag.py` in YAML. We use this to deploy our pipeline with hayhooks\n-  <a href=\"https://colab.research.google.com/github/deepset-ai/haystack-cookbook/blob/main/notebooks/rag-with-nims.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>: This notebook shows you how you can set up your components to use self-hosted NIMs."
    },
    {
      "name": "bcgov/mds",
      "stars": 28,
      "img": "https://avatars.githubusercontent.com/u/916280?s=40&v=4",
      "owner": "bcgov",
      "repo_name": "mds",
      "description": "Mines Digital Services",
      "homepage": "",
      "language": "Python",
      "created_at": "2018-04-25T18:54:47Z",
      "updated_at": "2025-04-22T22:32:49Z",
      "topics": [
        "bcgov-csnr",
        "dds",
        "empr",
        "status-inprogress"
      ],
      "readme": "# Mines Digital Services\n\nMines Digital Services (MDS) upholds the BC Public Service’s commitment to modernization, transparency, and efficiency, enabling better governance and service delivery when it comes to mining in British Columbia.\n\nThis project replaced the legacy Mine Management System (MMS) with a scalable, open source, data driven system using modern and flexible technologies.\n\nThe MDS have a number of interconnections and relationships to systems across the Natural Resource Ministries and is important not only to the Ministry of Mining and Critical Minerals but also to inter-agency collaborations across ministries, industry stakeholders and the public.\n\n[![Lifecycle:Maturing](https://img.shields.io/badge/Lifecycle-Maturing-007EC6)](Redirect-URL)\n[![CORE WEB - Unit Tests](https://github.com/bcgov/mds/actions/workflows/core-web.unit.yaml/badge.svg)](https://github.com/bcgov/mds/actions/workflows/core-web.unit.yaml)\n[![CORE API - Integration Tests](https://github.com/bcgov/mds/actions/workflows/tests.integration.yaml/badge.svg)](https://github.com/bcgov/mds/actions/workflows/tests.integration.yaml)\n[![MINESPACE - Unit Tests](https://github.com/bcgov/mds/actions/workflows/minespace.unit.yaml/badge.svg)](https://github.com/bcgov/mds/actions/workflows/minespace.unit.yaml)\n\n[![Maintainability](https://api.codeclimate.com/v1/badges/383b986cb973e1d0187f/maintainability)](https://codeclimate.com/github/bcgov/mds/maintainability)\n[![Test Coverage](https://api.codeclimate.com/v1/badges/383b986cb973e1d0187f/test_coverage)](https://codeclimate.com/github/bcgov/mds/test_coverage)\n\n## Features\n\nKey products that are maintained by MDS include Core, MineSpace and the public-facing [BC Mine Information website](https://mines.nrs.gov.bc.ca) [(github repository)](https://github.com/bcgov/NR-BCMI)\n\nMines Digital Services build features with these principles in mind:\n\n1. MDS develops tools and platforms that prioritize ease of use for mining companies, regulators, and the public.\n2. MDS supports access to high-quality geoscientific data and regulatory information, empowering government bodies and mining companies to make informed decisions.\n3. MDS collaborates across government agencies and industries to create cohesive solutions that integrate regulatory processes, permitting, and compliance systems.\n4. Digital tools from MDS help monitor and ensure mining activities adhere to environmental and social governance standards.\n\n## Services\n\n- [Common](services/common/README.md) (Frontend Shared Code)\n- [Core Web](services/core-web/README.md) (Ministry Frontend)\n- [Minespace Web](services/minespace-web/README.md) (Proponent Frontend)\n- [Core API](services/core-api/README.md) (Shared Backend)\n  - [Core API JWT](services/core-api/app/flask_jwt_oidc_local/README.md) (SSO)\n  - Celery (Scheduled CRON jobs)\n- [Database](services/database/README.md)\n  - [Flyway](migrations/README.md) (Database Migrations)\n- [Document Manager](services/document-manager/backend/README.md)\n  - [Document Manager Migrations](services/document-manager/backend/migrations/README.md)\n- [Fider](services/fider/README.md)\n- [Filesystem Provider](services/filesystem-provider/ej2-amazon-s3-aspcore-file-provider/README.md)\n- [NRIS](services/nris-api/backend/README.md)\n- [Permits](services/permits/README.md)\n\n## Operations\n\n- [Azure](operations/azure/README.md)\n  - [Azure Setup](operations/azure/setup/README.md)\n\n## Tests\n\n- [Testing Strategy](docs/testing/test_strategy.md)\n- [Functional Tests](tests/functional-tests/README.md)\n- [Load Testing](tests/load-testing/README.md)\n\n---\n\n## Typescript\n\nThis application was originally developed in Javascript, and is being migrated to Typescript. The following documentation is available to assist with the migration:\n\n- [Typescript](docs/processes/typescript.md)\n\n---\n\n## Verifiable Credentials\n\nThis project support verifiable credential features compatible with [AnonCreds](https://www.hyperledger.org/projects/anoncreds) and [Hyperledger Aries](https://www.hyperledger.org/projects/aries) and serves as the [Administering Authority for the BC Mines Act Permit](https://github.com/bcgov/bc-vcpedia/blob/main/credentials/bc-mines-act-permit/1.1.1/governance.md#15-administering-authority).\n\nSee the [Verifiable Credential doc](docs/verifiable_credentials.md) for more detail.\n\n## How to Contribute\n\nPlease read the [How to Contribute guide](CONTRIBUTING.md) and the [Code of Conduct](CODE_OF_CONDUCT.md).\n\n## How to Develop\n\nPlease read the [How to develop](USAGE.md) for project setup instructions and [Getting Started](docs/devops/getting_started.md) for DevOps information.\n\n### OpenShift Deployment\n\n[OpenShift Debugging Guide](docs/openshift/debugging_guide.md)\n[OpenShift Caveats](docs/openshift/Openshift%20Caveats.md)\n[Terraform](terraform/README.md)\n[OpenShift Database](docs/openshift/database.md)\n[OpenShift PG upgrade](docs/openshift/PG_9_to_13_upgrade.md)\n\n## License\n\nCode released under the [Apache License, Version 2.0](LICENSE.md).\n"
    },
    {
      "name": "anakin87/fact-checking-rocks",
      "stars": 28,
      "img": "https://avatars.githubusercontent.com/u/44616784?s=40&v=4",
      "owner": "anakin87",
      "repo_name": "fact-checking-rocks",
      "description": "Fact checking baseline combining dense retrieval and textual entailment",
      "homepage": "",
      "language": "Jupyter Notebook",
      "created_at": "2022-08-21T19:25:32Z",
      "updated_at": "2025-02-01T18:59:16Z",
      "topics": [
        "fact-checking",
        "haystack",
        "huggingface-spaces",
        "information-retrieval",
        "natural-language-inference",
        "natural-language-processing",
        "neural-search",
        "nlp",
        "python",
        "semantic-search",
        "streamlit",
        "streamlit-webapp",
        "text-entailment",
        "transformers"
      ],
      "readme": "---\ntitle: Fact Checking rocks!\nemoji: 🎸\ncolorFrom: purple\ncolorTo: blue\nsdk: docker\napp_port: 8501\nmodels: [sentence-transformers/msmarco-distilbert-base-tas-b, microsoft/deberta-v2-xlarge-mnli, google/flan-t5-large]\ntags: [fact-checking, rock, natural language inference, dense retrieval, large language models, haystack, neural search]\nlicense: apache-2.0\n---\n\n# Fact Checking 🎸 Rocks! &nbsp; [![Generic badge](https://img.shields.io/badge/🤗-Open%20in%20Spaces-blue.svg)](https://huggingface.co/spaces/anakin87/fact-checking-rocks) [![Generic badge](https://img.shields.io/github/stars/anakin87/fact-checking-rocks?label=Github&style=social)](https://github.com/anakin87/fact-checking-rocks)\n\n## *Fact checking baseline combining dense retrieval and textual entailment*\n\n- [Fact Checking 🎸 Rocks!    ](#fact-checking--rocks---)\n  - [*Fact checking baseline combining dense retrieval and textual entailment*](#fact-checking-baseline-combining-dense-retrieval-and-textual-entailment)\n    - [Idea](#idea)\n    - [Presentation](#presentation)\n    - [System description](#system-description)\n      - [Indexing pipeline](#indexing-pipeline)\n      - [Search pipeline](#search-pipeline)\n      - [Explain using a LLM](#explain-using-a-llm)\n    - [Limits and possible improvements](#limits-and-possible-improvements)\n    - [Repository structure](#repository-structure)\n    - [Installation](#installation)\n      - [Entailment Checker node](#entailment-checker-node)\n      - [Fact Checking 🎸 Rocks!](#fact-checking--rocks)\n\n### Idea\n💡 This project aims to show that a *naive and simple baseline* for fact checking can be built by combining dense retrieval and a textual entailment task.\nIn a nutshell, the flow is as follows:\n* the user enters a factual statement\n* the relevant passages are retrieved from the knowledge base using dense retrieval\n* the system computes the text entailment between each relevant passage and the statement, using a Natural Language Inference model\n* the entailment scores are aggregated to produce a summary score.\n\n###  Presentation\n\n- [🍿 Video presentation @ Berlin Buzzwords 2023](https://www.youtube.com/watch?v=4L8Iw9CZNbU)\n- [🧑‍🏫 Slides](./presentation/fact_checking_rocks.pdf)\n\n### System description\n🪄 This project is strongly based on [🔎 Haystack](https://github.com/deepset-ai/haystack), an open source NLP framework that enables seamless use of Transformer models and LLMs to interact with your data. The main components of our system are an indexing pipeline and a search pipeline.\n\n#### Indexing pipeline\n* [Crawling](https://github.com/anakin87/fact-checking-rocks/blob/321ba7893bbe79582f8c052493acfda497c5b785/notebooks/get_wikipedia_data.ipynb): Crawl data from Wikipedia, starting from the page [List of mainstream rock performers](https://en.wikipedia.org/wiki/List_of_mainstream_rock_performers) and using the [python wrapper](https://github.com/goldsmith/Wikipedia)\n* [Indexing](https://github.com/anakin87/fact-checking-rocks/blob/321ba7893bbe79582f8c052493acfda497c5b785/notebooks/indexing.ipynb)\n  * preprocess the downloaded documents into chunks consisting of 2 sentences\n  * chunks with less than 10 words are discarded, because not very informative\n  * instantiate a [FAISS](https://github.com/facebookresearch/faiss) Document store and store the passages on it\n  * create embeddings for the passages, using a Sentence Transformer model and save them in FAISS. The retrieval task will involve [*asymmetric semantic search*](https://www.sbert.net/examples/applications/semantic-search/README.html#symmetric-vs-asymmetric-semantic-search) (statements to be verified are usually shorter than inherent passages), therefore I choose the model `msmarco-distilbert-base-tas-b`\n  * save FAISS index.\n\n#### Search pipeline\n\n* the user enters a factual statement\n* compute the embedding of the user statement using the same Sentence Transformer used for indexing (`msmarco-distilbert-base-tas-b`)\n* retrieve the K most relevant text passages stored in FAISS (along with their relevance scores)\n* the following steps are performed using the [`EntailmentChecker`, a custom Haystack node](https://github.com/anakin87/haystack-entailment-checker)\n* **text entailment task**: compute the text entailment between each text passage (premise) and the user statement (hypothesis), using a Natural Language Inference model (`microsoft/deberta-v2-xlarge-mnli`). For every text passage, we have 3 scores (summing to 1): entailment, contradiction and neutral.\n* aggregate the text entailment scores: compute the weighted average of them, where the weight is the relevance score. **Now it is possible to tell if the knowledge base confirms, is neutral or disproves the user statement.**\n* *empirical consideration: if in the first N passages (N<K),  there is strong evidence of entailment/contradiction (partial aggregate scores > 0.5), it is better not to consider (K-N) less relevant documents.*\n\n#### Explain using a LLM\n* if there is entailment or contradiction, prompt `google/flan-t5-large`, asking why the relevant textual passages entail/contradict the user statement.\n\n### Limits and possible improvements\n ✨ As mentioned, the current approach to fact checking is simple and naive. Some **structural limits of this approach**:\n  * there is **no statement detection**. In fact, the statement to be verified is chosen by the user. In real-world applications, this step is often necessary.\n  * **Wikipedia is taken as a source of truth**. Unfortunately, Wikipedia does not contain universal knowledge and there is no real guarantee that it is a source of truth. There are certainly very interesting approaches that view a snapshot of the entire web as an uncurated source of knowledge (see [Facebook Research SPHERE](https://arxiv.org/abs/2112.09924)).\n  * Several papers and even our experiments show a general effectiveness of **dense retrieval** in retrieving textual passages for evaluating the user statement. However, there may be cases in which the most useful textual passages for fact checking do not emerge from the simple semantic similarity with the statement to be verified.\n  * **no organic evaluation** was performed, but only manual experiments.\n\nWhile keeping this simple approach, some **improvements** could be made:\n* For reasons of simplicity and infrastructural limitations, the retrieval uses only a very small portion of the Wikipedia data (artists pages from the [List of mainstream rock performers](https://en.wikipedia.org/wiki/List_of_mainstream_rock_performers)). With these few data available, in many cases the knowledge base remains neutral even with respect to statements about rock albums/songs. Certainly, fact checking **quality could improve by expanding the knowledge base** and possibly extending it to the entire Wikipedia.\n* Both the retriever model and the Natural Language Inference model are general purpose models and have not been fine-tuned for our domain. Undoubtedly they can **show better performance if fine-tuned in the rock music domain**. Particularly, the retriever model might be adapted with low effort, using [Generative Pseudo Labelling](https://haystack.deepset.ai/guides/gpl).\n\n### Repository structure\n* [Rock_fact_checker.py](Rock_fact_checker.py) and [pages folder](./pages/): multi-page Streamlit web app\n* [app_utils folder](./app_utils/): python modules used in the web app\n* [notebooks folder](./notebooks/): Jupyter/Colab notebooks to get Wikipedia data and index the text passages (using Haystack)\n* [data folder](./data/): all necessary data, including original Wikipedia data, FAISS Index and prepared random statements\n\n### Installation\n💻\n#### Entailment Checker node\nIf you want to build a similar system using the [`EntailmentChecker`](https://github.com/anakin87/haystack-entailment-checker), I strongly suggest taking a look at [the node repository](https://github.com/anakin87/haystack-entailment-checker). It can be easily installed with\n```bash\npip install haystack-entailment-checker\n```\n\n#### Fact Checking 🎸 Rocks!\n To install this project locally, follow these steps:\n* `git clone https://github.com/anakin87/fact-checking-rocks`\n* `cd fact-checking-rocks`\n* `pip install -r requirements.txt`\n\nTo run the web app, simply type: `streamlit run Rock_fact_checker.py`\n"
    },
    {
      "name": "deepset-ai/haystack-search-pipeline-streamlit",
      "stars": 27,
      "img": "https://avatars.githubusercontent.com/u/51827949?s=40&v=4",
      "owner": "deepset-ai",
      "repo_name": "haystack-search-pipeline-streamlit",
      "description": "🚀 Template Haystack Search Application with Streamlit",
      "homepage": "https://haystack.deepset.ai",
      "language": "Python",
      "created_at": "2022-12-15T16:39:50Z",
      "updated_at": "2025-02-16T13:24:33Z",
      "topics": [
        "haystack",
        "nlp",
        "python",
        "streamlit"
      ],
      "readme": "---\ntitle: Haystack Search Pipeline with Streamlit\nemoji: 👑\ncolorFrom: indigo\ncolorTo: indigo\nsdk: streamlit\nsdk_version: 1.23.0\napp_file: app.py\npinned: false\n---\n\n# Template Streamlit App for Haystack Search Pipelines\n\n\n> [!WARNING]\n> **This template is for Haystack version 1.x**. Use this template: [Haystack Streamlit App](https://github.com/deepset-ai/haystack-streamlit-app) for Haystack 2.x applications. \n\nThis template [Streamlit](https://docs.streamlit.io/) app set up for simple [Haystack search applications](https://docs.haystack.deepset.ai/docs/semantic_search). The template is ready to do QA with **Retrievel Augmented Generation**, or **Extractive QA**\n\nSee the ['How to use this template'](#how-to-use-this-template) instructions below to create a simple UI for your own Haystack search pipelines.\n\nBelow you will also find instructions on how you could [push this to Hugging Face Spaces 🤗](#pushing-to-hugging-face-spaces-).\n\n## Installation and Running\nTo run the bare application which does _nothing_:\n1. Install requirements: `pip install -r requirements.txt`\n2. Run the streamlit app: `streamlit run app.py`\n\nThis will start up the app on `localhost:8501` where you will find a simple search bar. Before you start editing, you'll notice that the app will only show you instructions on what to edit.\n\n### Optional Configurations\n\nYou can set optional cofigurations to set the:\n-  `--task` you want to start the app with: `rag` or `extractive` (default: rag)\n-  `--store` you want to use: `inmemory`, `opensearch`, `weaviate` or `milvus` (default: inmemory)\n-  `--name` you want to have for the app. (default: 'My Search App')\n\nE.g.:\n\n```bash\nstreamlit run app.py -- --store opensearch --task extractive --name 'My Opensearch Documentation Search'\n```\n\nIn a `.env` file, include all the config settings that you would like to use based on:\n- The DocumentStore of your choice\n- The Extractive/Generative model of your choice\n\nWhile the `/utils/config.py` will create default values for some configurations, others have to be set in the `.env` such as the `OPENAI_KEY`\n\nExample `.env`\n\n```\nOPENAI_KEY=YOUR_KEY\nEMBEDDING_MODEL=sentence-transformers/all-MiniLM-L12-v2\nGENERATIVE_MODEL=text-davinci-003\n```\n\n\n## How to use this template\n1. Create a new repository from this template or simply open it in a codespace to start playing around 💙\n2. Make sure your `requirements.txt` file includes the Haystack and Streamlit versions you would like to use.\n3. Change the code in `utils/haystack.py` if you would like a different pipeline.\n4. Create a `.env`file with all of your configuration settings.\n5. Make any UI edits you'd like to and [share with the Haystack community](https://haystack.deepeset.ai/community)\n6. Run the app as show in [installation and running](#installation-and-running)\n\n### Repo structure\n- `./utils`: This is where we have 3 files: \n    - `config.py`: This file extracts all of the configuration settings from a `.env` file. For some config settings, it uses default values. An example of this is in [this demo project](https://github.com/TuanaCelik/should-i-follow/blob/main/utils/config.py).\n    - `haystack.py`: Here you will find some functions already set up for you to start creating your Haystack search pipeline. It includes 2 main functions called `start_haystack()` which is what we use to create a pipeline and cache it, and `query()` which is the function called by `app.py` once a user query is received.\n    - `ui.py`: Use this file for any UI and initial value setups.\n- `app.py`: This is the main Streamlit application file that we will run. In its current state it has a simple search bar, a 'Run' button, and a response that you can highlight answers with.\n\n### What to edit?\nThere are default pipelines both in `start_haystack_extractive()` and `start_haystack_rag()`\n\n- Change the pipelines to use the embedding models, extractive or generative models as you need.\n- If using the `rag` task, change the `default_prompt_template` to use one of our available ones on [PromptHub](https://prompthub.deepset.ai) or create your own `PromptTemplate`\n\n\n## Pushing to Hugging Face Spaces 🤗\n\nBelow is an example GitHub action that will let you push your Streamlit app straight to the Hugging Face Hub as a Space.\n\nA few things to pay attention to:\n\n1. Create a New Space on Hugging Face with the Streamlit SDK.\n2. Create a Hugging Face token on your HF account.\n3. Create a secret on your GitHub repo called `HF_TOKEN` and put your Hugging Face token here.\n4. If you're using DocumentStores or APIs that require some keys/tokens, make sure these are provided as a secret for your HF Space too!\n5. This readme is set up to tell HF spaces that it's using streamlit and that the app is running on `app.py`, make any changes to the frontmatter of this readme to display the title, emoji etc you desire.\n6. Create a file in `.github/workflows/hf_sync.yml`. Here's an example that you can change with your own information, and an [example workflow](https://github.com/TuanaCelik/should-i-follow/blob/main/.github/workflows/hf_sync.yml) working for the [Should I Follow demo](https://huggingface.co/spaces/deepset/should-i-follow)\n\n```yaml\nname: Sync to Hugging Face hub\non:\n  push:\n    branches: [main]\n\n  # to run this workflow manually from the Actions tab\n  workflow_dispatch:\n\njobs:\n  sync-to-hub:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v2\n        with:\n          fetch-depth: 0\n          lfs: true\n      - name: Push to hub\n        env:\n          HF_TOKEN: ${{ secrets.HF_TOKEN }}\n        run: git push --force https://{YOUR_HF_USERNAME}:$HF_TOKEN@{YOUR_HF_SPACE_REPO} main\n```\n"
    },
    {
      "name": "redhat-et/foundation-models-for-documentation",
      "stars": 27,
      "img": "https://avatars.githubusercontent.com/u/66332970?s=40&v=4",
      "owner": "redhat-et",
      "repo_name": "foundation-models-for-documentation",
      "description": " Improve ROSA customer experience (and customer retention) by leveraging foundation models to do “gpt-chat” style search of Red Hat customer documentation assets.",
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2023-01-26T19:57:20Z",
      "updated_at": "2024-10-28T16:05:27Z",
      "topics": [],
      "readme": "[Comment]: < Generated with the help of ChatGPT > \n# Large Language Models for Product Documentation Search\n\n## Introduction\n\nProduct documentation is a crucial resource for any organization. It enables users to understand the product and its features, troubleshoot issues, and get the most out of the product. However, as product documentation grows in size and complexity, it becomes harder for users to find the information they need. This is where large language models can help.\n\nLarge Language Models (LLMs) are Machine Learning (ML) systems that are trained on vast amounts of text data. They can understand natural language and generate human-like responses to queries. In recent years, large language models like GPT-3 and BERT have shown impressive results in natural language processing tasks. They can be used for a variety of applications, including product documentation search.\n\nTraditional search engines rely on keywords to find relevant documents. However, this approach has limitations. It can be hard to know which keywords to use, and it may not capture the nuances of natural language. LLMs, on the other hand, can understand natural language queries and take into account the user's intent. This means that users can ask questions in a natural way, and the model will understand what they mean. For example, a user might ask \"How many nodes can I have in my ROSA cluster?\" and the model can provide the answer with links to relevant documentation.\n\nIn this repository we will explore how large language models can be used to improve product documentation search. Our goal is to demonstrate the potential for leveraging foundation models to create an interactive, conversational interface that ROSA customers or potential clients can use to access helpful tips and information, in addition to providing comprehensive technical documentation.\n\n### How Large Language Models Work\n\nLLMs are based on deep learning algorithms which enable them to understand and generate natural language. They are trained on massive datasets of text, such as books, articles, and websites. During training, the model learns to recognize patterns in the data and make predictions based on those patterns.\n\nOnce the model is trained, it can be used for a variety of natural language processing tasks. For example, it can be used to generate text, answer questions, or classify text based on its content.\n\nLLMs can be used directly, in which case they will make use of their capabilities for general language processing to generate meaningful text from a certain input (a question, a request). However, these models lack specific factual knowledge of specialized areas, which means that they are likely unable to address questions that are asked about non-general topics. Even worse, LLMs are prone to generate wrong answers, especially in this type of context.\n\nOn the other hand, LLMs can adapted to specific needs and/or knowledge areas. There are various mechanisms to do that, depending on the goal and the task at hand.\n\nDue to their adaptability, LLMs are a type of ML model called Foundation Models, as they can serve as a building block for additional, specialized model development.\n\nThe goal of this repository is to explore the options available for that adaptation process, and to provide guidance and recommendations on how to perform these tasks in general, and to apply them to specific concrete cases that can highlight useful applications.\n\n# Repo structure\n\n- `data`: \n\t- `[data/external](data/external)` directory contains our initial example ROSA docs for training.\n\t- `[data/processed](data/processed)` directory contains the validation ROSA faq dataset.\n- `notebooks`: The experiment notebooks and their details are in the notebooks [README.](notebooks/README.md)\n- `credentials.env`: This file is used to load environment variables like API keys in the Jupyter notebooks. An example with the format can be found in `[credentials_example.env](credentials_example.env)`\n- `docs`: This folder contains blog posts and additional documentation related to the project.\n\n\n## How to Contribute\n\nWe welcome contributions! Follow these steps to get involved:\n\n1. **Create Issues**: Report bugs, request features, or suggest improvements by opening an issue. Provide clear details.\n\n2. **Work on Issues**:\n   - Browse and select an issue to work on.\n   - Assign the issue to yourself.\n   - Create a new branch for your work: `git checkout -b issue-123`\n   - Make changes, commit them: `git commit -m \"Description\"`\n   - Push changes to your fork: `git push origin issue-123`\n\n3. **Make Pull Requests**:\n   - In your fork, click \"Compare & Pull Request\" next to your branch.\n   - Fill PR details, referencing the addressed issue.\n   - Submit; maintainers will review.\n\n# References\n\n[Open Source and other Question answering Implementations](https://github.com/redhat-et/foundation-models-for-documentation/issues/9)\n\n[Metrics and evaluation](https://github.com/redhat-et/foundation-models-for-documentation/issues/2)\n"
    },
    {
      "name": "lfunderburk/llmops-with-haystack",
      "stars": 26,
      "img": "https://avatars.githubusercontent.com/u/13559004?s=40&v=4",
      "owner": "lfunderburk",
      "repo_name": "llmops-with-haystack",
      "description": "Retrieval Augmented Generation applications",
      "homepage": "",
      "language": "Jupyter Notebook",
      "created_at": "2023-09-22T00:41:40Z",
      "updated_at": "2025-01-09T20:00:18Z",
      "topics": [],
      "readme": "# Building RAG chatbots with Haystack\n\nIn this repository I explore the development of RAG (retrival augmented generation) pipelines, whose job is to give a Large Language Model like OpenAI's GPT-4 access to a corpus of documents, usually in a database or in-memory, so that it can answer questions about it. \n\n## In-memory chatbot\n\nThis chatbot uses RAG to answer questions about the Seven Wonders of the Ancient World.\n\n[Read more here](first-rag-bot/chainlit-app/README.md)\n\n## Retrieval QA chatbot from web scraping using FAISS\n\nThis chatbot can help you identify what people think about the Barbie (2023) movie. You can also ask it information about the movie.\n\n[Read more here](retrieval-qa/README.md)"
    },
    {
      "name": "restful3/ds4th_study",
      "stars": 25,
      "img": "https://avatars.githubusercontent.com/u/2604066?s=40&v=4",
      "owner": "restful3",
      "repo_name": "ds4th_study",
      "description": "ds4th_study",
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2023-02-28T03:42:39Z",
      "updated_at": "2025-04-19T00:05:03Z",
      "topics": [],
      "readme": "# **ds4th study**\r\n\r\n---\r\n\r\n### **1. 스터디 목적**  \r\n- **대규모 언어 모델(LLM), 딥러닝, 머신러닝, 및 AI 시스템 최적화를 이해하고 실전 활용 능력을 기르는 것**을 목표로 한다.\r\n\r\n---\r\n\r\n### **2. 스터디 시간**  \r\n- 매주 토요일 오전 9시부터 2시간  \r\n\r\n---\r\n\r\n### **3. 스터디 장소**  \r\n- Webex\r\n\r\n---\r\n\r\n### **4. 스터디 운영 계획 (2024년 12월 ~ 2025년 6월)**  \r\n\r\n---\r\n\r\n#### **2024년 12월 14일**: [Webex](https://lgehq.webex.com/lgehq-en/j.php?MTID=m67909b7d882fffbb28a17bdd880050f7\t)\r\n- 발표자 - LLM(태영), 밑바닥(두균)  \r\n  - [CH1. LLM 지도](https://github.com/restful3/ds4th_study/blob/main/source/LLM%EC%9D%84_%ED%99%9C%EC%9A%A9%ED%95%9C_%EC%8B%A4%EC%A0%84_AI_%EC%95%A0%ED%94%8C%EB%A6%AC%EC%BC%80%EC%9D%B4%EC%85%98_%EA%B0%9C%EB%B0%9C/CH01_LLM_%EC%A7%80%EB%8F%84_song.pdf)\r\n  - [CH1. 정규 분포](https://github.com/restful3/ds4th_study/blob/main/source/밑바닥부터_시작하는_딥러닝_5/step01-probability-Statistics.ipynb)\r\n\r\n#### **2024년 12월 21일**: [Webex](https://lgehq.webex.com/lgehq/j.php?MTID=m74bc31f0b8067acc17d5107a4cfcea0f\t)\r\n- 발표자 - AI(정훈), 자유주제(태영)  \r\n  - [CH1. Introduction To Building AI Applications With Foundation Models](https://github.com/restful3/ds4th_study/blob/main/source/AI_Engineering/Ch01_Introduction_to_Building_AI_Applications_park.pdf) \r\n  - 자유주제 세션 : [A Survey on Model Compression for Large Language Models](https://arxiv.org/abs/2308.07633)  \r\n\r\n#### **2024년 12월 28일**: [Webex](https://lgehq.webex.com/lgehq-en/j.php?MTID=mb5a74c3fb595bb2ab09920b817e1a2a5\t)\r\n- 발표자 - LLM(C성진), 밑바닥(종우)  \r\n  - CH2. 트랜스포머 아키텍처 살펴보기  \r\n  - [CH2. 최대 가능도 추정](https://github.com/restful3/ds4th_study/blob/main/source/%EB%B0%91%EB%B0%94%EB%8B%A5%EB%B6%80%ED%84%B0_%EC%8B%9C%EC%9E%91%ED%95%98%EB%8A%94_%EB%94%A5%EB%9F%AC%EB%8B%9D_5/step02-MLE_jongwoo.pdf)  \r\n\r\n#### **2025년 1월 11일** *(1/4 쉬는 날)*: [Webex](https://lgehq.webex.com/lgehq-en/j.php?MTID=m16c5137784cdd76dee041575c96a4e48\t)\r\n- 발표자 - AI(우석), 자유주제(재익)  \r\n  - [CH2. Understanding Foundation Models](https://github.com/restful3/ds4th_study/blob/main/source/AI_Engineering/Ch02_Understanding_Foundation_Models_jeongwooseok.ipynb)  \r\n  - 자유주제 세션 : [TBD]()  \r\n\r\n#### **2025년 1월 18일**: [Webex](https://lgehq.webex.com/lgehq-en/j.php?MTID=m0890a4d2dce5a5469ac4a4019efc3b51\t)\r\n- 발표자 - LLM(태호), 밑바닥(우석)  \r\n  - [CH3. 트랜스포머 모델을 다루기 위한 허깅페이스 트랜스포머 라이브러리](https://github.com/restful3/ds4th_study/blob/main/source/LLM%EC%9D%84_%ED%99%9C%EC%9A%A9%ED%95%9C_%EC%8B%A4%EC%A0%84_AI_%EC%95%A0%ED%94%8C%EB%A6%AC%EC%BC%80%EC%9D%B4%EC%85%98_%EA%B0%9C%EB%B0%9C/CH03_%ED%8A%B8%EB%9E%9C%EC%8A%A4%ED%8F%AC%EB%A8%B8_%EB%AA%A8%EB%8D%B8%EC%9D%84_%EB%8B%A4%EB%A3%A8%EA%B8%B0_%EC%9C%84%ED%95%9C_%ED%97%88%EA%B9%85%ED%8E%98%EC%9D%B4%EC%8A%A4_%ED%8A%B8%EB%9E%9C%EC%8A%A4%ED%8F%AC%EB%A8%B8_%EB%9D%BC%EC%9D%B4%EB%B8%8C%EB%9F%AC%EB%A6%AC_teo.pdf)  \r\n  - [CH3. 다변량 정규 분포](https://github.com/restful3/ds4th_study/blob/main/source/%EB%B0%91%EB%B0%94%EB%8B%A5%EB%B6%80%ED%84%B0_%EC%8B%9C%EC%9E%91%ED%95%98%EB%8A%94_%EB%94%A5%EB%9F%AC%EB%8B%9D_5/step03_%EB%8B%A4%EB%B3%80%EB%9F%89%EB%B6%84%ED%8F%AC_wooseok.ipynb)\r\n\r\n#### **2025년 1월 25일**: [Webex](https://lgehq.webex.com/lgehq-en/j.php?MTID=m41a6759003026ae0d91e29aecc9675e9\t)\r\n- 발표자 - AI(태영), 자유주제(우석)  \r\n  - [CH3. Evaluation Methodology](https://github.com/restful3/ds4th_study/blob/main/source/AI_Engineering/Ch03_Evaluation_Methodolog_song.pdf)\r\n  - 자유주제 세션 : [rstar-Math, Small LLMs](https://github.com/restful3/ds4th_study/blob/main/source/papers/Chain-of-Thought%20Prompring%20Elicits%20Reasoning%20in%20Large%20Language%20Models/rStar-Math%EC%9D%98%20%ED%83%84%EC%83%9D%20%EB%B0%B0%EA%B2%BD%EA%B3%BC%20%ED%95%B5%EC%8B%AC%20%EC%95%84%EC%9D%B4%EB%94%94%EC%96%B4.md)\r\n\r\n#### **2025년 2월 8일** *(2/1 쉬는 날)*: [Webex](https://lgehq.webex.com/lgehq/j.php?MTID=mebc02c032fcf0068181e7b554c709bb3\t)\r\n- 발표자 - LLM(우석), 밑바닥(태영)  \r\n  - [CH4. 말 잘 듣는 모델 만들기](https://github.com/restful3/ds4th_study/blob/main/source/LLM%EC%9D%84_%ED%99%9C%EC%9A%A9%ED%95%9C_%EC%8B%A4%EC%A0%84_AI_%EC%95%A0%ED%94%8C%EB%A6%AC%EC%BC%80%EC%9D%B4%EC%85%98_%EA%B0%9C%EB%B0%9C/CH4_%EB%A7%90%EC%9E%98%EB%93%A3%EB%8A%94LLM_%EC%9E%91%EC%84%B1%EC%A4%91.pptx)\r\n  - [CH4. 가우스 혼합 모델](https://github.com/restful3/ds4th_study/blob/main/source/%EB%B0%91%EB%B0%94%EB%8B%A5%EB%B6%80%ED%84%B0_%EC%8B%9C%EC%9E%91%ED%95%98%EB%8A%94_%EB%94%A5%EB%9F%AC%EB%8B%9D_5/step04_%EA%B0%80%EC%9A%B0%EC%8A%A4_%ED%98%BC%ED%95%A9_%EB%AA%A8%EB%8D%B8_song.ipynb)\r\n\r\n#### **2025년 2월 15일**: [Webex](https://lgehq.webex.com/lgehq-en/j.php?MTID=m379b03e5c415c37bfbd5417b18034e57\t)\r\n- 발표자 - AI(재익), 자유주제(태호)  \r\n  - CH4. Evaluate AI Systems  \r\n  - 자유주제 세션 : [Deepseek Multimodal 모델 - Janus, Janus-Pro](https://github.com/restful3/ds4th_study/blob/main/source/papers/%5B%EB%85%BC%EB%AC%B8%5D_Deepseek%20Multimodal_%EB%AA%A8%EB%8D%B8_%EB%85%BC%EB%AC%B8_%EB%A6%AC%EB%B7%B0_Janus_JanusPro_20250215_Teo.pdf)\r\n\r\n#### **2025년 2월 22일**: [Webex](https://lgehq.webex.com/lgehq-en/j.php?MTID=mc2a52bbfacadc192ba94409533823779\t)\r\n- 발표자 - LLM(태영), 밑바닥(K성진)  \r\n  - [CH5. GPU 효율적인 학습](https://github.com/restful3/ds4th_study/blob/main/source/LLM%EC%9D%84_%ED%99%9C%EC%9A%A9%ED%95%9C_%EC%8B%A4%EC%A0%84_AI_%EC%95%A0%ED%94%8C%EB%A6%AC%EC%BC%80%EC%9D%B4%EC%85%98_%EA%B0%9C%EB%B0%9C/CH05_GPU_%ED%9A%A8%EC%9C%A8%EC%A0%81%EC%9D%B8_%ED%95%99%EC%8A%B5_song.ipynb)\r\n  - CH5. EM 알고리즘  \r\n\r\n#### **2025년 3월 8일** *(3/1 쉬는 날)*: [Webex](https://lgehq.webex.com/lgehq-en/j.php?MTID=mff689e4a7817c0997b82f7ca4fbc5175\t)\r\n- 발표자 - AI(우석), 자유주제(C성진)  \r\n  - CH5. Prompt Engineering  \r\n  - 자유주제 세션 : [TBD]()  \r\n\r\n#### **2025년 3월 15일**: [오프라인 : 오전 10시](https://booking.naver.com/booking/10/bizes/372277), [Webex](https://lgehq.webex.com/lgehq-en/j.php?MTID=mfaf5dc7391304da94f27f16871dce1a3\t)\r\n- 발표자 - 밑바닥(민호) , 자유주제(정훈)  \r\n  - [CH6. 신경망](https://github.com/restful3/ds4th_study/blob/4c7edde084b640ffda8717f3d35569c86df90b48/source/%EB%B0%91%EB%B0%94%EB%8B%A5%EB%B6%80%ED%84%B0_%EC%8B%9C%EC%9E%91%ED%95%98%EB%8A%94_%EB%94%A5%EB%9F%AC%EB%8B%9D_5/step06_%EC%8B%A0%EA%B2%BD%EB%A7%9D_minho.ipynb)\r\n  - 자유주제 세션 : [Anomaly detection 실무](https://github.com/restful3/ds4th_study/blob/main/source/papers/Anomaly%20detection_%EB%B0%95%EC%A0%95%ED%9B%88.pdf)   \r\n\r\n#### **2025년 3월 22일**: [Webex](https://lgehq.webex.com/lgehq-en/j.php?MTID=m93fd00e113e21dc12a75a6b4bb2d2208\t)\r\n- 발표자 - AI(우석), LLM(재익)  \r\n  - CH6. RAG And Agents\r\n  - [CH6. sLLM 학습하기](https://github.com/restful3/ds4th_study/blob/main/source/LLM%EC%9D%84_%ED%99%9C%EC%9A%A9%ED%95%9C_%EC%8B%A4%EC%A0%84_AI_%EC%95%A0%ED%94%8C%EB%A6%AC%EC%BC%80%EC%9D%B4%EC%85%98_%EA%B0%9C%EB%B0%9C/llm_ch6_jishin.pdf)\r\n\r\n#### **2025년 3월 29일**: [Webex](https://lgehq.webex.com/lgehq-en/j.php?MTID=m685a89d84e5c1129312cf3421b154059\t)\r\n- 발표자 - LLM(태호), 밑바닥(민호)  \r\n  - [CH7. 모델 가볍게 만들기](https://github.com/restful3/ds4th_study/blob/main/source/LLM%EC%9D%84_%ED%99%9C%EC%9A%A9%ED%95%9C_%EC%8B%A4%EC%A0%84_AI_%EC%95%A0%ED%94%8C%EB%A6%AC%EC%BC%80%EC%9D%B4%EC%85%98_%EA%B0%9C%EB%B0%9C/CH07_%EB%AA%A8%EB%8D%B8_%EA%B0%80%EB%B3%8D%EA%B2%8C_%EB%A7%8C%EB%93%A4%EA%B8%B0_teo.pdf)  \r\n  - [CH7. 변이형 오토인코더](https://github.com/restful3/ds4th_study/blob/31e64500ee40deeace764979b694e7f0b8f4ffe1/source/%EB%B0%91%EB%B0%94%EB%8B%A5%EB%B6%80%ED%84%B0_%EC%8B%9C%EC%9E%91%ED%95%98%EB%8A%94_%EB%94%A5%EB%9F%AC%EB%8B%9D_5/step07_%EB%B3%80%EC%9D%B4%ED%98%95%EC%98%A4%ED%86%A0%EC%9D%B8%EC%BD%94%EB%8D%94_minho.pdf) \r\n\r\n#### **2025년 4월 12일** *(4/5 쉬는 날)*: [Webex](https://lgehq.webex.com/lgehq-en/j.php?MTID=m13e940f392b3b5ae422edff93b61b1ea\t)\r\n- 발표자 - AI(태영), 자유주제(우석)  \r\n  - [CH7. Finetuning](https://github.com/restful3/ds4th_study/blob/main/source/AI_Engineering/Ch07_Finetuning_song.pdf)\r\n  - 자유주제 세션 : [N8N 설치부터 실행까지, 매우 초보만](https://github.com/jeong-wooseok/AIDOINGAI)  \r\n\r\n#### **2025년 4월 19일**: [Webex](https://lgehq.webex.com/lgehq-en/j.php?MTID=m492a1f4b4721f9070e3102bce3acc903\t)\r\n- 발표자 - LLM(태영), 밑바닥(민호)  \r\n  - CH8. sLLM 서빙하기  \r\n\r\n#### **2025년 4월 26일**: [Webex](https://lgehq.webex.com/lgehq-en/j.php?MTID=mc362a213dabd3c9d29f727b97b5fdf83\t)\r\n- 발표자 - AI(우석), 밑바닥(민호)    \r\n  - CH8. Dataset Engineering  \r\n  - CH8. 확산 모델 이론  \r\n\r\n#### **2025년 5월 10일** *(5/3 쉬는 날)*: [Webex](https://lgehq.webex.com/lgehq-en/j.php?MTID=mebff409f9da191e9e2c17a6e815224b6\t)\r\n- 발표자 - LLM(TBD), 밑바닥(태영)  \r\n  - CH9. LLM 애플리케이션 개발하기  \r\n  - CH9. 확산 모델 구현  \r\n\r\n#### **2025년 5월 17일**: [Webex](https://lgehq.webex.com/lgehq-en/j.php?MTID=mb85f58b1c6d1d0a9062cee50342b0e3d\t)\r\n- 발표자 - AI(민호), 자유주제(TBD)  \r\n  - CH9. Inference Optimization  \r\n  - 자유주제 세션 : [TBD]()  \r\n\r\n#### **2025년 5월 24일**: [Webex](https://lgehq.webex.com/lgehq-en/j.php?MTID=m1decb14e63ff3ff5776c8e8deaf340aa\t)\r\n- 발표자 - LLM(TBD), 밑바닥(TBD)  \r\n  - CH10. 임베딩 모델로 데이터 의미 압축하기\r\n  - CH10. 확산 모델 응용\r\n\r\n#### **2025년 5월 31일**: [Webex](https://lgehq.webex.com/lgehq-en/j.php?MTID=m35afd51f4e675a8362f2e9c24e84ca69\t)\r\n- 발표자 - AI(민호), LLM(TBD)\r\n  - CH10. AI Engineering Architecture And User Feedback\r\n  - CH11. 자신의 데이터에 맞춘 임베딩 모델 만들기: RAG 개선하기  \r\n\r\n#### **2025년 6월 14일** *(6/7 쉬는 날)*: [Webex](https://lgehq.webex.com/lgehq-en/j.php?MTID=mbcbbaed6d50821e7d52c93ce336aed79\t)\r\n- 발표자 - LLM(TBD), LLM(TBD)  \r\n  - CH12. 벡터 데이터베이스로 확장하기: RAG 구현하기  \r\n  - CH13. LLM 운영하기  \r\n\r\n#### **2025년 6월 21일**: [Webex](https://lgehq.webex.com/lgehq-en/j.php?MTID=m1915fca8a0b5fbc16f23c9ba84616244\t)\r\n- 발표자 - LLM(TBD), 자유주제(민호)  \r\n  - CH14. 멀티 모달  \r\n  - 자유주제 세션 : [TBD]()  \r\n \r\n#### **2025년 6월 28일**: [Webex](https://lgehq.webex.com/lgehq-en/j.php?MTID=m1cf3168ebcc132ee5074271e66d2defe\t)\r\n- 발표자 - LLM(TBD), LLM(TBD)  \r\n  - CH15. LLM 에이전트  \r\n  - CH16. 새로운 아키텍처  \r\n\r\n---\r\n\r\n### **5. 스터디 운영 방법**\r\n- **교재**:\r\n  - (교재1) [LLM을 활용한 실전 AI 애플리케이션 개발](https://ridibooks.com/books/3649000042?_rdt_sid=category_bestsellers&_rdt_idx=5&_rdt_arg=2220)  \r\n  - (교재2) [밑바닥부터 시작하는 딥러닝 5](https://ridibooks.com/books/443001394?_rdt_sid=BookDetailHomeSeriesMemberBookList&_rdt_idx=4&_s_id=)  \r\n  - (교재3) [AI Engineering](https://www.oreilly.com/library/view/ai-engineering/9781098166298/)  \r\n- **참고 자료**:\r\n  - 각 교재 관련 GitHub 또는 자유주제 자료\r\n- **학습 공유**: 매주 학습한 내용을 발표자료와 함께 공유  \r\n- **발표**: 각 주제별로 50분 발표, 10분 Q&A\r\n- **운영 규칙**\r\n  - [스터디 운영 규칙](https://github.com/restful3/ds4th_study/blob/main/source/%EC%8A%A4%ED%84%B0%EB%94%94_%EC%9A%B4%EC%98%81_%EA%B7%9C%EC%B9%99_v01.pdf)\r\n\r\n---\r\n\r\n### **6. 기타**\r\n- **참가 희망 요청**: [이메일](restful3@gmail.com)\r\n"
    },
    {
      "name": "yip-kl/llm_function_calling_demo",
      "stars": 25,
      "img": "https://avatars.githubusercontent.com/u/85790832?s=40&v=4",
      "owner": "yip-kl",
      "repo_name": "llm_function_calling_demo",
      "description": null,
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2024-04-02T10:25:17Z",
      "updated_at": "2025-04-21T11:50:27Z",
      "topics": [],
      "readme": "# Function Calling demo\n## What does this application wants to demonstrate\nThis application is built as an extension to [this](https://haystack.deepset.ai/tutorials/40_building_chat_application_with_function_calling)\n1. **Data retrieval**: With both RAG and DB search (via API created from Flask)\n2. **Routing**: Use Function Call for autonomous tool choice & invocation\n3. **UI** Via Streamlit\n\n## Tech stack\n- **Embedding model**: [all-MiniLM-L6-v2](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2)\n- **Vector Database**: [Haystack's InMemoryDocumentStore](https://docs.haystack.deepset.ai/docs/inmemorydocumentstore)\n- **LLM**: [GPT-4 Turbo accessed via OpenRouter](https://openrouter.ai/models/openai/gpt-4-1106-preview). But the flow can be adapted into using other LLMs\n- **LLM Framework**: [Haystack](https://haystack.deepset.ai/) for their great documentations, and transparency in pipeline construction. This tutorial is actually an extension to their [fantastic tutorial](https://haystack.deepset.ai/tutorials/40_building_chat_application_with_function_calling) for the same topic\n\n## Running this tool\n1. Create and activate a virtual environment, then `pip install -r requirements.txt` to install the required packages\n1. Spin up the API server with `python db_api.py`\n2. If you are seeking for an initial tutorial for the concept behind, run `rag_plus_db_search.ipynb`. Or proceed to #3 directly\n3. Run the streamlit application with the below\n```\nexport OPENROUTER_API_KEY = '@REPLACE WITH YOUR API KEY'\ncd streamlit\nstreamlit run app.py\n```"
    },
    {
      "name": "prosto/neo4j-haystack",
      "stars": 24,
      "img": "https://avatars.githubusercontent.com/u/706952?s=40&v=4",
      "owner": "prosto",
      "repo_name": "neo4j-haystack",
      "description": "Neo4j Haystack DataStore",
      "homepage": null,
      "language": "Python",
      "created_at": "2023-12-18T19:42:20Z",
      "updated_at": "2025-04-11T13:50:10Z",
      "topics": [],
      "readme": "<h1 align=\"center\">neo4j-haystack</h1>\n\n<p align=\"center\">A <a href=\"https://docs.haystack.deepset.ai/docs/document_store\"><i>Haystack</i></a> Document Store for <a href=\"https://neo4j.com/\"><i>Neo4j</i></a>.</p>\n\n<p align=\"center\">\n  <a href=\"https://github.com/prosto/neo4j-haystack/actions?query=workflow%3Aci\">\n    <img alt=\"ci\" src=\"https://github.com/prosto/neo4j-haystack/workflows/ci/badge.svg\" />\n  </a>\n  <a href=\"https://prosto.github.io/neo4j-haystack/\">\n    <img alt=\"documentation\" src=\"https://img.shields.io/badge/docs-mkdocs%20material-blue.svg?style=flat\" />\n  </a>\n  <a href=\"https://pypi.org/project/neo4j-haystack/\">\n    <img alt=\"pypi version\" src=\"https://img.shields.io/pypi/v/neo4j-haystack.svg\" />\n  </a>\n  <a href=\"https://img.shields.io/pypi/pyversions/neo4j-haystack.svg\">\n    <img alt=\"python version\" src=\"https://img.shields.io/pypi/pyversions/neo4j-haystack.svg\" />\n  </a>\n  <a href=\"https://pypi.org/project/haystack-ai/\">\n    <img alt=\"haystack version\" src=\"https://img.shields.io/pypi/v/haystack-ai.svg?label=haystack\" />\n  </a>\n</p>\n\n---\n\n**Table of Contents**\n\n- [Overview](#overview)\n- [Installation](#installation)\n- [Usage](#usage)\n  - [Running Neo4j](#running-neo4j)\n  - [Document Store](#document-store)\n  - [Indexing documents](#indexing-documents)\n  - [Retrieving documents](#retrieving-documents)\n  - [Retrieving documents using Cypher](#retrieving-documents-using-cypher)\n  - [More examples](#more-examples)\n- [License](#license)\n\n## Overview\n\nAn integration of [Neo4j](https://neo4j.com/) graph database with [Haystack v2.0](https://docs.haystack.deepset.ai/v2.0/docs/intro)\nby [deepset](https://www.deepset.ai). In Neo4j [Vector search index](https://neo4j.com/docs/cypher-manual/current/indexes-for-vector-search/)\nis being used for storing document embeddings and dense retrievals.\n\nThe library allows using Neo4j as a [DocumentStore](https://docs.haystack.deepset.ai/v2.0/docs/document-store), and implements the required [Protocol](https://docs.haystack.deepset.ai/v2.0/docs/document-store#documentstore-protocol) methods. You can start working with the implementation by importing it from `neo4_haystack` package:\n\n```python\nfrom neo4j_haystack import Neo4jDocumentStore\n```\n\nIn addition to the `Neo4jDocumentStore` the library includes the following haystack components which can be used in a pipeline:\n\n- [Neo4jEmbeddingRetriever](https://prosto.github.io/neo4j-haystack/reference/neo4j_retriever/#neo4j_haystack.components.neo4j_retriever.Neo4jEmbeddingRetriever) - is a typical [retriever component](https://docs.haystack.deepset.ai/v2.0/docs/retrievers) which can be used to query vector store index and find related Documents. The component uses `Neo4jDocumentStore` to query embeddings.\n- [Neo4jDynamicDocumentRetriever](https://prosto.github.io/neo4j-haystack/reference/neo4j_retriever/#neo4j_haystack.components.neo4j_retriever.Neo4jDynamicDocumentRetriever) is also a retriever component in a sense that it can be used to query Documents in Neo4j. However it is decoupled from `Neo4jDocumentStore` and allows to run arbitrary [Cypher query](https://neo4j.com/docs/cypher-manual/current/queries/) to extract documents. Practically it is possible to query Neo4j same way `Neo4jDocumentStore` does, including vector search.\n- [Neo4jQueryReader](https://prosto.github.io/neo4j-haystack/reference/neo4j_query_reader/#neo4j_haystack.components.neo4j_query_reader.Neo4jQueryReader) - is a component which gives flexible way to read data from Neo4j by running custom Cypher query along with query parameters. You could use such queries to read data from Neo4j to enhance your RAG pipelines. For example prompting LLM to produce Cypher query based on given context (Text to Cypher) and use `Neo4jQueryReader` to run the\n  query and extract results. [OutputAdapter](https://docs.haystack.deepset.ai/docs/outputadapter) component might\n  become handy in such scenarios - it can be used to handle outputs from `Neo4jQueryReader`.\n- [Neo4jQueryWriter](https://prosto.github.io/neo4j-haystack/reference/neo4j_query_writer/#neo4j_haystack.components.neo4j_query_writer.Neo4jQueryWriter) - this component gives flexible way to write data to Neo4j by running arbitrary Cypher query along with parameters. Query parameters can be pipeline inputs or outputs from connected components. You could use such queries to write Documents with additional graph nodes for a more complex RAG scenarios. The difference between [DocumentWriter](https://docs.haystack.deepset.ai/docs/documentwriter) and `Neo4jQueryWriter` is that the latter can write any data to Neo4j, not just Documents.\n\nThe `neo4j-haystack` library uses [Python Driver](https://neo4j.com/docs/api/python-driver/current/api.html#api-documentation) and\n[Cypher Queries](https://neo4j.com/docs/cypher-manual/current/introduction/) to interact with Neo4j database and hide all complexities under the hood.\n\n`Neo4jDocumentStore` will store Documents as Graph nodes in Neo4j. Embeddings are stored as part of the node, but indexing and querying of vector embeddings using ANN is managed by a dedicated [Vector Index](https://neo4j.com/docs/cypher-manual/current/indexes-for-vector-search/).\n\n```text\n                                   +-----------------------------+\n                                   |       Neo4j Database        |\n                                   +-----------------------------+\n                                   |                             |\n                                   |      +----------------+     |\n                                   |      |    Document    |     |\n                write_documents    |      +----------------+     |\n          +------------------------+----->|   properties   |     |\n          |                        |      |                |     |\n+---------+----------+             |      |   embedding    |     |\n|                    |             |      +--------+-------+     |\n| Neo4jDocumentStore |             |               |             |\n|                    |             |               |index/query  |\n+---------+----------+             |               |             |\n          |                        |      +--------+--------+    |\n          |                        |      |  Vector Index   |    |\n          +----------------------->|      |                 |    |\n               query_embeddings    |      | (for embedding) |    |\n                                   |      +-----------------+    |\n                                   |                             |\n                                   +-----------------------------+\n```\n\nIn the above diagram:\n\n- `Document` is a Neo4j node (with \"Document\" label)\n- `properties` are Document [attributes](https://docs.haystack.deepset.ai/v2.0/docs/data-classes#document) stored as part of the node. **In current implementation `meta` attributes are stored on the same level as the rest of Document fields.**\n- `embedding` is also a property of the Document node (just shown separately in the diagram for clarity) which is a vector of type `LIST[FLOAT]`.\n- `Vector Index` is where embeddings are getting indexed by Neo4j as soon as those are updated in Document nodes.\n\n`Neo4jDocumentStore` by default creates a vector index if it does not exist. Before writing documents you should make sure Documents are embedded by one of the provided [embedders](https://docs.haystack.deepset.ai/v2.0/docs/embedders). For example [SentenceTransformersDocumentEmbedder](https://docs.haystack.deepset.ai/v2.0/docs/sentencetransformersdocumentembedder) can be used in indexing pipeline to calculate document embeddings before writing those to Neo4j.\n\n## Installation\n\n`neo4j-haystack` can be installed as any other Python library, using pip:\n\n```bash\npip install --upgrade pip # optional\npip install sentence-transformers # required in order to run pipeline examples given below\npip install neo4j-haystack\n```\n\n## Usage\n\n### Running Neo4j\n\nYou will need to have a running instance of Neo4j database to use components from the package (in-memory version of Neo4j is not supported).\nThere are several options available:\n\n- [Docker](https://neo4j.com/docs/operations-manual/5/docker/), other options available in the same Operations Manual\n- [AuraDB](https://neo4j.com/cloud/platform/aura-graph-database/) - a fully managed Cloud Instance of Neo4j\n- [Neo4j Desktop](https://neo4j.com/docs/desktop-manual/current/) client application\n\nThe simplest way to start database locally will be with Docker container:\n\n```bash\ndocker run \\\n    --restart always \\\n    --publish=7474:7474 --publish=7687:7687 \\\n    --env NEO4J_AUTH=neo4j/passw0rd \\\n    neo4j:5.15.0\n```\n\nAs of Neo4j `5.13`, the vector search index is no longer a beta feature, consider using a version of the database `\">= 5.13\"`. In the example above version `5.15.0` is being used to start a container.\nYou could explore Known issues and Limitations in the [documentation](https://neo4j.com/docs/cypher-manual/current/indexes/semantic-indexes/vector-indexes/).\n\nThe `NEO4J_AUTH` environment variable sets default credentials (`username/password`) for authentication.\n\n> **Note**\n> Assuming you have a docker container running navigate to <http://localhost:7474> to open [Neo4j Browser](https://neo4j.com/docs/browser-manual/current/) to explore graph data and run Cypher queries.\n\n### Document Store\n\nOnce you have the package installed and the database running, you can start using `Neo4jDocumentStore` as any other document stores that support embeddings.\n\n```python\nfrom neo4j_haystack import Neo4jDocumentStore\n\ndocument_store = Neo4jDocumentStore(\n    url=\"bolt://localhost:7687\",\n    username=\"neo4j\",\n    password=\"passw0rd\",\n    database=\"neo4j\",\n    embedding_dim=384,\n    embedding_field=\"embedding\",\n    index=\"document-embeddings\", # The name of the Vector Index in Neo4j\n    node_label=\"Document\", # Providing a label to Neo4j nodes which store Documents\n)\n```\n\nAlternatively, Neo4j connection properties could be specified using a dedicated [Neo4jClientConfig](https://prosto.github.io/neo4j-haystack/reference/neo4j_client/#neo4j_haystack.client.neo4j_client.Neo4jClientConfig) class:\n\n```python\nfrom neo4j_haystack import Neo4jClientConfig, Neo4jDocumentStore\n\nclient_config = Neo4jClientConfig(\n    url=\"bolt://localhost:7687\",\n    username=\"neo4j\",\n    password=\"passw0rd\",\n    database=\"neo4j\",\n)\n\ndocument_store = Neo4jDocumentStore(client_config=client_config, embedding_dim=384)\n```\n\nAssuming there is a list of documents available and a running Neo4j database you can write/index those in Neo4j, e.g.:\n\n```python\nfrom haystack import Document\n\ndocuments = [Document(content=\"My name is Morgan and I live in Paris.\")]\n\ndocument_store.write_documents(documents)\n```\n\nIf you intend to obtain embeddings before writing documents use the following code:\n\n```python\nfrom haystack import Document\n\n# import one of the available document embedders\nfrom haystack.components.embedders import SentenceTransformersDocumentEmbedder\n\ndocuments = [Document(content=\"My name is Morgan and I live in Paris.\")]\n\ndocument_embedder = SentenceTransformersDocumentEmbedder(model=\"sentence-transformers/all-MiniLM-L6-v2\")\ndocument_embedder.warm_up() # will download the model during first run\ndocuments_with_embeddings = document_embedder.run(documents)\n\ndocument_store.write_documents(documents_with_embeddings.get(\"documents\"))\n```\n\nMake sure embedding model produces vectors of same size as it has been set on `Neo4jDocumentStore`, e.g. setting `embedding_dim=384` would comply with the \"sentence-transformers/all-MiniLM-L6-v2\" model.\n\n> **Note**\n> Most of the time you will be using [Haystack Pipelines](https://docs.haystack.deepset.ai/v2.0/docs/pipelines) to build both indexing and querying RAG scenarios.\n\nIt is important to understand how haystack Documents are stored in Neo4j after you call `write_documents`.\n\n```python\nfrom random import random\n\nsample_embedding = [random() for _ in range(384)]  # using fake/random embedding for brevity here to simplify example\ndocument = Document(\n    content=\"My name is Morgan and I live in Paris.\", embedding=sample_embedding, meta={\"num_of_years\": 3}\n)\ndocument.to_dict()\n```\n\nThe above code converts a Document to a dictionary and will render the following output:\n\n```bash\n>>> output:\n{\n    \"id\": \"11c255ad10bff4286781f596a5afd9ab093ed056d41bca4120c849058e52f24d\",\n    \"content\": \"My name is Morgan and I live in Paris.\",\n    \"dataframe\": None,\n    \"blob\": None,\n    \"score\": None,\n    \"embedding\": [0.025010755222666936, 0.27502931836911926, 0.22321073814882275, ...], # vector of size 384\n    \"num_of_years\": 3,\n}\n```\n\nThe data from the dictionary will be used to create a node in Neo4j after you write the document with `document_store.write_documents([document])`. You could query it with Cypher, e.g. `MATCH (doc:Document) RETURN doc`. Below is a json representation of the node in Neo4j:\n\n```js\n{\n  \"identity\": 0,\n  \"labels\": [\n    \"Document\" // label name is specified in the Neo4jDocumentStore.node_label argument\n  ],\n  \"properties\": { // this is where Document data is stored\n    \"id\": \"11c255ad10bff4286781f596a5afd9ab093ed056d41bca4120c849058e52f24d\",\n    \"embedding\": [0.6394268274307251, 0.02501075528562069,0.27502933144569397, ...], // vector of size 384\n    \"content\": \"My name is Morgan and I live in Paris.\",\n    \"num_of_years\": 3\n  },\n  \"elementId\": \"4:8bde9fb3-3975-4c3e-9ea1-3e10dbad55eb:0\"\n}\n```\n\n> **Note**\n> Metadata (`num_of_years`) is serialized to the same level as rest of attributes (flatten). **It is expected by current implementation** as Neo4j node's properties can not have nested structures.\n\nThe full list of parameters accepted by `Neo4jDocumentStore` can be found in\n[API documentation](https://prosto.github.io/neo4j-haystack/reference/neo4j_store/#neo4j_haystack.document_stores.neo4j_store.Neo4jDocumentStore.__init__).\n\n### Indexing documents\n\nWith Haystack you can use [DocumentWriter](https://docs.haystack.deepset.ai/v2.0/docs/documentwriter) component to write Documents into a Document Store. In the example below we construct pipeline to write documents to Neo4j using `Neo4jDocumentStore`:\n\n```python\nfrom haystack import Document\nfrom haystack.components.embedders import SentenceTransformersDocumentEmbedder\nfrom haystack.components.writers import DocumentWriter\nfrom haystack.pipeline import Pipeline\n\nfrom neo4j_haystack import Neo4jDocumentStore\n\ndocuments = [Document(content=\"This is document 1\"), Document(content=\"This is document 2\")]\n\ndocument_store = Neo4jDocumentStore(\n    url=\"bolt://localhost:7687\",\n    username=\"neo4j\",\n    password=\"passw0rd\",\n    database=\"neo4j\",\n    embedding_dim=384,\n    embedding_field=\"embedding\",\n    index=\"document-embeddings\",\n    node_label=\"Document\",\n)\nembedder = SentenceTransformersDocumentEmbedder(model=\"sentence-transformers/all-MiniLM-L6-v2\")\ndocument_writer = DocumentWriter(document_store=document_store)\n\nindexing_pipeline = Pipeline()\nindexing_pipeline.add_component(instance=embedder, name=\"embedder\")\nindexing_pipeline.add_component(instance=document_writer, name=\"writer\")\n\nindexing_pipeline.connect(\"embedder\", \"writer\")\nindexing_pipeline.run({\"embedder\": {\"documents\": documents}})\n```\n\n```bash\n>>> output:\n`{'writer': {'documents_written': 2}}`\n```\n\n### Retrieving documents\n\n`Neo4jEmbeddingRetriever` component can be used to retrieve documents from Neo4j by querying vector index using an embedded query. Below is a pipeline which finds documents using query embedding as well as [metadata filtering](https://docs.haystack.deepset.ai/v2.0/docs/metadata-filtering):\n\n```python\nfrom typing import List\n\nfrom haystack import Document, Pipeline\nfrom haystack.components.embedders import SentenceTransformersDocumentEmbedder, SentenceTransformersTextEmbedder\n\nfrom neo4j_haystack import Neo4jDocumentStore, Neo4jEmbeddingRetriever\n\ndocument_store = Neo4jDocumentStore(\n    url=\"bolt://localhost:7687\",\n    username=\"neo4j\",\n    password=\"passw0rd\",\n    database=\"neo4j\",\n    embedding_dim=384,\n    index=\"document-embeddings\",\n)\n\ndocuments = [\n    Document(content=\"My name is Morgan and I live in Paris.\", meta={\"num_of_years\": 3}),\n    Document(content=\"I am Susan and I live in Berlin.\", meta={\"num_of_years\": 7}),\n]\n\n# Same model is used for both query and Document embeddings\nmodel_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n\ndocument_embedder = SentenceTransformersDocumentEmbedder(model=model_name)\ndocument_embedder.warm_up()\ndocuments_with_embeddings = document_embedder.run(documents)\n\ndocument_store.write_documents(documents_with_embeddings.get(\"documents\"))\n\nprint(\"Number of documents written: \", document_store.count_documents())\n\npipeline = Pipeline()\npipeline.add_component(\"text_embedder\", SentenceTransformersTextEmbedder(model=model_name))\npipeline.add_component(\"retriever\", Neo4jEmbeddingRetriever(document_store=document_store))\npipeline.connect(\"text_embedder.embedding\", \"retriever.query_embedding\")\n\nresult = pipeline.run(\n    data={\n        \"text_embedder\": {\"text\": \"What cities do people live in?\"},\n        \"retriever\": {\n            \"top_k\": 5,\n            \"filters\": {\"field\": \"num_of_years\", \"operator\": \"==\", \"value\": 3},\n        },\n    }\n)\n\ndocuments: List[Document] = result[\"retriever\"][\"documents\"]\n```\n\n```bash\n>>> output:\n[Document(id=3930326edabe6d172031557556999e2f8ba258ccde3c876f5e3ac7e66ed3d53a, content: 'My name is Morgan and I live in Paris.', meta: {'num_of_years': 3}, score: 0.8348373770713806)]\n```\n\n> **Note**\n> You can learn more about how a given metadata filter is converted into Cypher queries by looking at documentation of the [Neo4jQueryConverter](https://prosto.github.io/neo4j-haystack/reference/metadata_filter/neo4j_query_converter/#neo4j_haystack.metadata_filter.neo4j_query_converter.Neo4jQueryConverter) class.\n\n### Retrieving documents using Cypher\n\nIn certain scenarios you might have an existing graph in Neo4j database which was created by custom scripts or data ingestion pipelines. The schema of the graph could be complex and not exactly fitting into Haystack Document model. Moreover in many situations you might want to leverage existing graph data to extract more context for grounding LLMs. To make it possible with Haystack we have `Neo4jDynamicDocumentRetriever` component - a flexible retriever which can run arbitrary Cypher query to obtain documents. This component does not require Document Store to operate.\n\n> **Note**\n> The logic of `Neo4jDynamicDocumentRetriever` could be easily achieved with `Neo4jQueryReader` + `OutputAdapter` components.\n> `Neo4jDynamicDocumentRetriever` makes sense when you specifically expect Documents as an output of a query execution and would like to avoid additional output conversions in your pipeline (e.g. \"Neo4j Record\" --> Document).\n\nThe above example of `Neo4jEmbeddingRetriever` could be rewritten without usage of `Neo4jDocumentStore` in the retrieval pipeline:\n\n```python\nfrom typing import List\n\nfrom haystack import Document, Pipeline\nfrom haystack.components.embedders import SentenceTransformersDocumentEmbedder, SentenceTransformersTextEmbedder\n\nfrom neo4j_haystack import Neo4jClientConfig, Neo4jDocumentStore, Neo4jDynamicDocumentRetriever\n\nclient_config = Neo4jClientConfig(\n    url=\"bolt://localhost:7687\",\n    username=\"neo4j\",\n    password=\"passw0rd\",\n    database=\"neo4j\",\n)\n\ndocuments = [\n    Document(content=\"My name is Morgan and I live in Paris.\", meta={\"num_of_years\": 3}),\n    Document(content=\"I am Susan and I live in Berlin.\", meta={\"num_of_years\": 7}),\n]\n\n# Same model is used for both query and Document embeddings\nmodel_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n\ndocument_embedder = SentenceTransformersDocumentEmbedder(model=model_name)\ndocument_embedder.warm_up()\ndocuments_with_embeddings = document_embedder.run(documents)\n\ndocument_store = Neo4jDocumentStore(client_config=client_config, embedding_dim=384)\ndocument_store.write_documents(documents_with_embeddings.get(\"documents\"))\n\n# Same model is used for both query and Document embeddings\nmodel_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n\ncypher_query = \"\"\"\n            CALL db.index.vector.queryNodes($index, $top_k, $query_embedding)\n            YIELD node as doc, score\n            MATCH (doc) WHERE doc.num_of_years = $num_of_years\n            RETURN doc{.*, score}, score\n            ORDER BY score DESC LIMIT $top_k\n        \"\"\"\n\nembedder = SentenceTransformersTextEmbedder(model=\"sentence-transformers/all-MiniLM-L6-v2\")\nretriever = Neo4jDynamicDocumentRetriever(\n    client_config=client_config, runtime_parameters=[\"query_embedding\"], doc_node_name=\"doc\"\n)\n\npipeline = Pipeline()\npipeline.add_component(\"text_embedder\", embedder)\npipeline.add_component(\"retriever\", retriever)\npipeline.connect(\"text_embedder.embedding\", \"retriever.query_embedding\")\n\nresult = pipeline.run(\n    data={\n        \"text_embedder\": {\"text\": \"What cities do people live in?\"},\n        \"retriever\": {\n            \"query\": cypher_query,\n            \"parameters\": {\"index\": \"document-embeddings\", \"top_k\": 5, \"num_of_years\": 3},\n        },\n    }\n)\n\ndocuments: List[Document] = result[\"retriever\"][\"documents\"]\n```\n\n```bash\n>>> output:\n[Document(id=4014455c3be5d88151ba12d734a16754d7af75c691dfc3a5f364f81772471bd2, content: 'My name is Morgan and I live in Paris.', meta: {'num_of_years': 3}, score: 0.6696747541427612, embedding: vector of size 384)]\n```\n\nPlease notice how query parameters are being used in the `cypher_query`:\n\n- `runtime_parameters` is a list of parameter names which are going to be input slots when connecting components\n  in a pipeline. In our case `query_embedding` input is connected to the `text_embedder.embedding` output.\n- `pipeline.run` specifies additional parameters to the `retriever` component which can be referenced in the\n  `cypher_query`, e.g. `top_k` and `num_of_years`.\n\nIn some way `Neo4jDynamicDocumentRetriever` resembles the [PromptBuilder](https://docs.haystack.deepset.ai/v2.0/docs/promptbuilder) component, only instead of prompt it constructs a Cypher query using [parameters](https://neo4j.com/docs/python-manual/current/query-simple/#query-parameters). In the example above documents retrieved by running the query, the `RETURN doc{.*, score}` part returns back found documents with scores. Which node variable is going to be used to construct haystack Document is specified in the `doc_node_name` parameter (see above `doc_node_name=\"doc\"`).\n\nYou have options to enhance your RAG pipeline with data having various schemas, for example by first finding nodes using vector search and then expanding query to search for nearby nodes using appropriate Cypher syntax. It is possible to implement \"Parent-Child\" chunking strategy with such approach. Before that you have to ingest/index data into Neo4j accordingly by building an indexing pipeline or a custom ingestion script. A simple schema is shown below:\n\n```text\n┌────────────┐                ┌─────────────┐\n│   Child    │                │   Parent    │\n│            │  :HAS_PARENT   │             │\n│   content  ├────────────────┤   content   │\n│  embedding │                │             │\n└────────────┘                └─────────────┘\n```\n\nThe following Cypher query is an example of how `Neo4jDynamicDocumentRetriever` can first search embeddings for `Child` document chunks and then **return** `Parent` documents which have larger context window (text length) for RAG applications:\n\n```cypher\n// Query Child documents by $query_embedding\nCALL db.index.vector.queryNodes($index, $top_k, $query_embedding)\nYIELD node as child_doc, score\n\n// Find Parent document for previously retrieved child (e.g. extend RAG context)\nMATCH (child_doc)-[:HAS_PARENT]->(parent:Parent)\nWITH parent, max(score) AS score // deduplicate parents\nRETURN parent{.*, score}\n```\n\nAs you might have guessed, the value for the `doc_node_name` parameter should be equal to `parent` according to the query above.\n\n### More examples\n\nYou can find more examples in the implementation [repository](https://github.com/prosto/neo4j-haystack/tree/main/examples):\n\n- [indexing_pipeline.py](https://github.com/prosto/neo4j-haystack/blob/main/examples/indexing_pipeline.py) - Indexing text files (documents) from a remote http location.\n- [rag_pipeline.py](https://github.com/prosto/neo4j-haystack/blob/main/examples/rag_pipeline.py) - Generative question answering RAG pipeline using `Neo4jEmbeddingRetriever` to fetch documents from Neo4j document store and answer question using [HuggingFaceTGIGenerator](https://docs.haystack.deepset.ai/v2.0/docs/huggingfacetgigenerator).\n- [rag_pipeline_cypher.py](https://github.com/prosto/neo4j-haystack/blob/main/examples/rag_pipeline_cypher.py) - Same as `rag_pipeline.py` but using `Neo4jDynamicDocumentRetriever`.\n\nMore technical details available in the [Code Reference](https://prosto.github.io/neo4j-haystack/reference/neo4j_store/) documentation. For example, in real world scenarios there could be requirements to tune connection settings to Neo4j database (e.g. request timeout). [Neo4jDocumentStore](https://prosto.github.io/neo4j-haystack/reference/neo4j_store/#neo4j_haystack.document_stores.Neo4jDocumentStore.__init__) accepts an extended client configuration using [Neo4jClientConfig](https://prosto.github.io/neo4j-haystack/reference/neo4j_client/#neo4j_haystack.client.neo4j_client.Neo4jClientConfig) class.\n\n## License\n\n`neo4j-haystack` is distributed under the terms of the [MIT](https://spdx.org/licenses/MIT.html) license.\n"
    },
    {
      "name": "dod-advana/gamechanger-ml",
      "stars": 24,
      "img": "https://avatars.githubusercontent.com/u/82123680?s=40&v=4",
      "owner": "dod-advana",
      "repo_name": "gamechanger-ml",
      "description": "GAMECHANGER Machine Learning Repo",
      "homepage": "",
      "language": "Python",
      "created_at": "2021-05-05T17:51:05Z",
      "updated_at": "2025-04-20T13:08:06Z",
      "topics": [
        "dod",
        "machine-learning",
        "ml",
        "mlops",
        "natural-language-processing",
        "nlp",
        "policy-as-code",
        "python",
        "transformer"
      ],
      "readme": "# GC - Machine Learning\n## Table of Contents\n1. [Directory](##Directory)\n2. [Development Rules](#Development-Rules)\n3. [Train Models](#Train-Models)\n4. [ML API](#ML-API)\n5. [Helpful Flags For API](#Helpful-Flags-For-API)\n6. [FAQ](#FAQ)\n7. [Pull Requests](#Pull-Requests)\n\n\n## Directory\n```\n├── gamechangerml\n│   ├── api\n│   │   ├── README.md\n│   │   ├── __init__.py\n│   │   ├── docker-compose.override.yml\n│   │   ├── docker-compose.yml\n│   │   ├── fastapi\n│   │   ├── getInitModels.py\n│   │   ├── kube\n│   │   ├── logs\n│   │   ├── tests\n│   │   └── utils\n│   ├── configs\n│   ├── corpus\n│   ├── data\n│   │   ├── features\n│   │   │   ├── abbcounts.json\n│   │   │   ├── abbreviations.csv\n│   │   │   ├── abbreviations.json\n│   │   │   ├── agencies.csv\n│   │   │   ├── classifier_entities.csv\n│   │   │   ├── combined_entities.csv\n│   │   │   ├── corpus_doctypes.csv\n│   │   │   ├── enwiki_vocab_min200.txt\n│   │   │   ├── generated_files\n│   │   │   │   ├── __init__.py\n│   │   │   │   ├── common_orgs.csv\n│   │   │   │   ├── corpus_meta.csv\n│   │   │   │   └── prod_test_data.csv\n│   │   │   ├── popular_documents.csv\n│   │   │   ├── topics_wiki.csv\n│   │   │   └── word-freq-corpus-20201101.txt\n│   │   ├── ltr\n│   │   ├── nltk_data\n│   │   ├── test_data\n│   │   ├── training\n│   │   │   └── sent_transformer\n│   │   ├── user_data\n│   │   │   ├── gold_standard.csv\n│   │   │   ├── matamo_feedback\n│   │   │   │   ├── Feedback.csv\n│   │   │   │   └── matamo_feedback.csv\n│   │   │   └── search_history\n│   │   │       └── SearchPdfMapping.csv\n│   │   └── validation\n│   │       ├── domain\n│   │       │   ├── query_expansion\n│   │       │   ├── question_answer\n│   │       │   └── sent_transformer\n│   │       └── original\n│   │           ├── msmarco_1k\n│   │           ├── multinli_1.0\n│   │           └── squad2.0\n│   ├── mlflow\n│   ├── models\n│   │   ├── ltr\n│   │   ├── msmarco_index\n│   │   ├── qexp_20211001\n│   │   ├── sent_index_20211108\n│   │   ├── topic_models\n│   │   └── transformers\n│   │       ├── bert-base-cased-squad2\n│   │       ├── distilbart-mnli-12-3\n│   │       ├── msmarco-distilbert-base-v2\n│   ├── scripts\n│   ├── src\n│   │   ├── featurization\n│   │   │   ├── abbreviation.py\n│   │   │   ├── abbreviations_utils.py\n│   │   │   ├── extract_improvement\n│   │   │   ├── generated_fts.py\n│   │   │   ├── keywords\n│   │   │   ├── make_meta.py\n│   │   │   ├── rank_features\n│   │   │   ├── ref_list.py\n│   │   │   ├── ref_utils.py\n│   │   │   ├── responsibilities.py\n│   │   │   ├── summary.py\n│   │   │   ├── table.py\n│   │   │   ├── term_extract\n│   │   │   ├── test_hf_ner.py\n│   │   │   ├── tests\n│   │   │   ├── topic_modeling.py\n│   │   │   └── word_sim.py\n│   │   ├── model_testing\n│   │   ├── search\n│   │   │   ├── QA\n│   │   │   ├── embed_reader\n│   │   │   ├── query_expansion\n│   │   │   ├── ranking\n│   │   │   ├── semantic\n│   │   │   └── sent_transformer\n│   │   ├── text_classif\n│   │   ├── text_handling\n│   │   └── utilities\n│   ├── stresstest\n│   ├── train\n```\n\n## Development Rules\n- Everything in `gamechangerml/src` should be independent of things outside of that structure (should not need to import from dataPipeline, common, etc).\n\n\n### Configs\n- Config files go in `gamechangerml/configs`. When you add a new class, import it in [gamechangerml/configs/__init__.py](gamechangerml/configs/__init__.py).\n- File paths in `gamechangerml/configs/*` should be relative to `gamechangerml` and only used for local testing purposes. Feel free to change on your local machine, but ***do not commit system specific paths to the repository***.\n- A config class (i.e., from `gamechangerml/configs/*`) should not be required as an input parameter to a function. However, a config class attribute can be used to provide parameters to a function (`foo(path=Config.path)`, rather than `foo(Config)`).\n\n\n### What Can Be Stored On GitHub?\n- Models and large files should *NOT* be stored on Github.\n- Data should *NOT* be stored on Github, there is a script in the `gamechangerml/scripts` folder to download a corpus from s3.\n\n### Use Best Practices\n- Code should be modular, broken down into smallest logical pieces, and placed in the most logical subfolder.\n- All classes, functions, etc. should have clear, concise, and consistent docstrings. \n  - Function docstrings should include:\n    - A short description \n    - Any important remarks\n    - Parameter types, defaults, and descriptions\n    - Return types and descriptions\n\n    Example:\n    ```python\n    def say(words, loud=False):\n      \"\"\"Make the animal say words.\n\n      Args:\n        words (str): Words for the animal to say.\n        loud (bool): True to make the animal say the words loudly, False to \n          make the animal say the words in a normal tone. Default is False.\n\n      Returns:\n        None\n      \"\"\"\n    ```\n- Include a maximum of 1 class per file.\n- Include README.md files that contain what, why, and how code is used.\n\n\n## Getting Started\n### To use gamechangerml as a python module\n- `pip install .`\n- you should now be able to import gamechangerml anywhere python is available.\n\n\n## Train Models\n1. Setup your environment, and make any changes to configs: \n- `source ./gamechangerml/setup_env.sh DEV`\n2. Ensure your AWS enviroment is setup (you have a default profile)\n3. Get dependencies\n- `source ./gamechangerml/scripts/download_dependencies.sh`\n4. For query expansion:\n- `python -m gamechangerml.train.scripts.run_train_models --flag {MODEL_NAME_SUFFIX} --saveremote {True or False} --model_dest {FILE_PATH_MODEL_OUTPUT} --corpus {CORPUS_DIR}`\n5. For sentence embeddings:\n- `python -m gamechangerml.train.scripts.create_embeddings -c {CORPUS LOCATION} --gpu True --em msmarco-distilbert-base-v2`\n\n## ML API\n1. Setup your environment, make any changes to configs: \n- `source ./gamechangerml/setup_env.sh DEV`\n2. Ensure your AWS enviroment is setup (you have a default profile)\n3. Dependencies will be automatically downloaded and extracted.\n4. `cd gamechangerml/api`\n5. `docker-compose build`\n6. `docker-compose up`\n7. visit `localhost:5000/docs`\n\n## Helpful Flags For API\n- export CONTAINER_RELOAD=True to reload the container on code changes for development\n- export DOWNLOAD_DEP=True to get models and other deps from s3\n- export MODEL_LOAD=False to not load models on API start (only for development needs) \n\n## FAQ\n- I get an error with redis on API start\n  - export ENV_TYPE=DEV\n- Do I need to train models to use the API?\n  - No, you can use the pretrained models within the dependencies. \n- The API is crashing when trying to load the models.\n  - Likely your machine does not have enough resources (RAM or CPU) to load all models. Try to exclude models from the model folder.\n- Do I need a machine with a GPU?\n  - No, but it will make training or inferring faster.\n- What if I can't download the dependencies since I am external?\n  - We are working on making models publically available. However you can use download pretrained transformers from HuggingFace to include in the models/transformers directory, which will enable you to use some functionality of the API. Without any models, there is still functionality available like text extraction avaiable. \n\n## Pull Requests\n*Please provide:*\n1. Description - what is the purpose, what are the different features added i.e. bugfix, added upload capability to model, model improving\n2. Reviewer Test - how to test it manually and if it is on a dev/test server. (if applicable) \n ` i.e. hit post endpoint /search with payload {\"query\": \"military\"}`\n3. Unit/Integration tests - screenshot or copy output of unit tests from GC_ML_TESTS_119, any other tests or metrics applicable\n"
    },
    {
      "name": "AIAnytime/RAG-Tool-using-Haystack-Mistral-and-Chainlit",
      "stars": 23,
      "img": "https://avatars.githubusercontent.com/u/30049737?s=40&v=4",
      "owner": "AIAnytime",
      "repo_name": "RAG-Tool-using-Haystack-Mistral-and-Chainlit",
      "description": "RAG Tool using Haystack, Mistral, and Chainlit. All open source stack on CPU.",
      "homepage": null,
      "language": "Python",
      "created_at": "2023-10-14T09:15:54Z",
      "updated_at": "2024-04-18T19:45:00Z",
      "topics": [],
      "readme": "# RAG-Tool-using-Haystack-Mistral-and-Chainlit\nRAG Tool using Haystack, Mistral, and Chainlit. All open source stack on CPU.\n"
    },
    {
      "name": "miranthajayatilake/nanoQA",
      "stars": 23,
      "img": "https://avatars.githubusercontent.com/u/11708057?s=40&v=4",
      "owner": "miranthajayatilake",
      "repo_name": "nanoQA",
      "description": "Question-answering on your own data with Large Language Models (LLMs)",
      "homepage": "",
      "language": "Python",
      "created_at": "2023-02-12T09:32:51Z",
      "updated_at": "2024-10-22T03:52:18Z",
      "topics": [
        "chatgpt",
        "language-model",
        "machine-learning",
        "question-answering",
        "transformers"
      ],
      "readme": "<!-- # nanoQA -->\n<img src=\"./utils/logo.jpg\" height=\"100\">\n\n[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://github.com/miranthajayatilake/kudle/blob/main/LICENSE) [![version](https://img.shields.io/badge/version-0.2-yellow)]() [![discord](https://img.shields.io/badge/chat-discord-blueviolet)](https://discord.gg/Pu2YJDzScZ)\n\n\n\n# Chat with your data to find answers :mag: :zap: \n\nnanoQA uses Large Language Models (LLMs) to build a question-answering application on your own data.\n\n**Please refer to this blog post for a comprehensive guide** :heavy_exclamation_mark: [LINK](https://medium.com/@miranthaj/building-a-chat-ai-to-answer-about-your-own-data-part-i-f05dde32ff1b)\n\n**Demo**\n\n<img src=\"./utils/demo.gif\">\n\n---\n<br/>\n\n## Quick start\n\n- Create a virtual environment with python (Tested with `python 3.10.9` on [Anaconda](https://www.anaconda.com/))\n- `pip install -r requirements.txt` to install all dependecies.\n- Make sure [Docker](https://www.docker.com/) is up and running in your local environment. We use docker to set up [elasticsearch](https://www.elastic.co/guide/en/elasticsearch/reference/current/docker.html) as our data store.\n- Run `bash datastore.sh` to pull and set up elasticsearch. Wait till this step is completed.\n- Run `python sample_data.py data/faq_covid https://s3.eu-central-1.amazonaws.com/deepset.ai-farm-qa/datasets/documents/small_faq_covid.csv.zip index_qa`. This script will download a sample dataset of FAQs on COVID 19 and index it under `index_qa`. This dataset and index name are for demo purposes. You can replace this with your own data and naming.\n- Run `streamlit run app.py` to spin up the user interface.\n\nNow you can provide your index name and start chatting with your data.\n\n## Contributing\n\nI highly appreciate your contributions to this project in any amount possible. This is still at an very basic stage. Suggestions on additional features and functionality are welcome. General instructions on how to contribute are mentioned in [CONTRIBUTING](CONTRIBUTING.md)\n\n## Getting help\n\nPlease use the issues tracker of this repository to report on any bugs or questions you have.\n\nAlso you can join the [DISCORD](https://discord.gg/Pu2YJDzScZ)"
    },
    {
      "name": "NanGePlus/HaystackTest",
      "stars": 23,
      "img": "https://avatars.githubusercontent.com/u/178549638?s=40&v=4",
      "owner": "NanGePlus",
      "repo_name": "HaystackTest",
      "description": "在本项目中使用Haystack框架实现模拟健康档案私有知识库构建和检索全流程，通过一份代码实现了同时支持多种大模型（如OpenAI、阿里通义千问等）的RAG（检索增强生成）功能:(1)离线步骤:文档加载->文档切分->向量化->灌入向量数据库；在线步骤:获取用户问题->用户问题向量化->检索向量数据库->将检索结果和用户问题填入prompt模版->用最终的prompt调用LLM->由LLM生成回复",
      "homepage": "",
      "language": "Python",
      "created_at": "2024-12-19T08:55:31Z",
      "updated_at": "2025-04-21T18:18:23Z",
      "topics": [],
      "readme": "# 1、介绍\n## 1.1 主要内容                      \n使用Haystack开源框架实现RAG(Retrieval Augmented Generation 检索增强生成)应用，核心思想:人找知识，会查资料；LLM找知识，会查向量数据库             \n使用LangChain开源框架实现RAG应用项目地址:https://github.com/NanGePlus/RagLangChainTest                                       \n本次应用案例实现功能为:                            \n**(1)离线步骤(构建索引):** 文档加载->文档切分->向量化->灌入向量数据库                                    \n**(2)在线步骤(检索增强生成):** 获取用户问题->用户问题向量化->检索向量数据库->将检索结果和用户问题填入prompt模版->用最终的prompt调用LLM->由LLM生成回复                            \n<img src=\"./01.png\" alt=\"\" width=\"600\" />                              \n相关视频:                   \nhttps://www.bilibili.com/video/BV1nXkxYQEVp/                   \nhttps://youtu.be/sI_vxzGeOUY                  \n\n## 1.2 Haystack框架\nHaystack是一个开源框架，它由deepset开发，用于构建强大的QA问答、检索增强生成RAG等AI应用，支持构建从小型本地化应用到大规模生产级应用多种场景                                                                                                                     \n官方网址：https://haystack.deepset.ai/                                                       \nGithub地址:https://github.com/deepset-ai/haystack                            \n**核心概念介绍:**                                        \n**DocumentStore 文档存储:** 文档存储库是一个数据库，它存储你的数据，并在查询时将数据提供给检索器(Retriever)                      \n<img src=\"./02.png\" alt=\"\" width=\"600\" />                  \n**Components 组件:** 是构成pipeline的重要组成部分，组件负责处理相关的业务逻辑                                         \n**Pipelines 流水线:** 将不同的功能组件进行逻辑编排，形成一个有向无环图，最后运行pipeline                                    \n<img src=\"./03.png\" alt=\"\" width=\"600\" />                     \n\n\n# 2、前期准备工作\n## 2.1 开发环境搭建:anaconda、pycharm\nanaconda:提供python虚拟环境，官网下载对应系统版本的安装包安装即可                                      \npycharm:提供集成开发环境，官网下载社区版本安装包安装即可                                               \n**可参考如下视频:**                      \n集成开发环境搭建Anaconda+PyCharm                                                          \nhttps://www.bilibili.com/video/BV1q9HxeEEtT/?vd_source=30acb5331e4f5739ebbad50f7cc6b949                             \nhttps://youtu.be/myVgyitFzrA          \n\n## 2.2 大模型相关配置\n(1)GPT大模型使用方案(第三方代理方式)                               \n(2)非GPT大模型(阿里通义千问、讯飞星火、智谱等大模型)使用方案(OneAPI方式)                         \n(3)本地开源大模型使用方案(Ollama方式)                                             \n**可参考如下视频:**                                   \n提供一种LLM集成解决方案，一份代码支持快速同时支持gpt大模型、国产大模型(通义千问、文心一言、百度千帆、讯飞星火等)、本地开源大模型(Ollama)                       \nhttps://www.bilibili.com/video/BV12PCmYZEDt/?vd_source=30acb5331e4f5739ebbad50f7cc6b949                 \nhttps://youtu.be/CgZsdK43tcY           \n\n\n# 3、项目初始化\n## 3.1 下载源码\nGitHub或Gitee中下载工程文件到本地，下载地址如下：                \nhttps://github.com/NanGePlus/HaystackTest                                                               \nhttps://gitee.com/NanGePlus/HaystackTest                                     \n\n## 3.2 构建项目\n使用pycharm构建一个项目，为项目配置虚拟python环境                       \n项目名称：HaystackTest                          \n虚拟环境名称保持与项目名称一致                           \n\n## 3.3 将相关代码拷贝到项目工程中           \n将下载的代码文件夹中的文件全部拷贝到新建的项目根目录下                             \n\n## 3.4 安装项目依赖                            \n新建命令行终端，在终端中运行 pip install -r requirements.txt 安装依赖                                                      \n**注意:** 建议先使用要求的对应版本进行本项目测试，避免因版本升级造成的代码不兼容。测试通过后，可进行升级测试                                                                           \n\n\n# 4、功能测试 \n### (1)测试文档准备                                                          \n这里以pdf文件为例，在input文件夹下准备了两份pdf文件                \n健康档案.pdf:测试中文pdf文档处理                 \nllama2.pdf:测试英文pdf文档处理                   \n### (2)大模型准备                                    \n**gpt大模型(使用代理方案):**                           \nOPENAI_BASE_URL=https://yunwu.ai/v1                            \nOPENAI_API_KEY=sk-5tKSZtEo4WsXKZJE8v4JeFqV8eNf6GwYwJFgT5JFJ42DP7qe                                \nOPENAI_CHAT_MODEL=gpt-4o-mini                                  \nOPENAI_EMBEDDING_MODEL=text-embedding-3-small                             \n**非gpt大模型(使用OneAPI方案):**                                  \nOPENAI_BASE_URL=http://139.224.72.218:3000/v1                             \nOPENAI_API_KEY=sk-VIm8DGiCtF5Dc46pEd393967Bf554e7a8dA5A8AeFfDcCd75                                      \nOPENAI_CHAT_MODEL=qwen-plus                                      \nOPENAI_EMBEDDING_MODEL=text-embedding-v1                        \n### (3)离线步骤(构建索引)                        \n文档加载->文档切分->向量化->灌入向量数据库                                                    \n在根目录下的ragTest/tools文件夹下提供了pdfSplitTest_Ch.py脚本工具用来处理中文文档、pdfSplitTest_En.py脚本工具用来处理英文文档                                                                    \n在根目录下的ragTest/vectorSaveTest.py脚本执行调用tools中的工具进行文档预处理后进行向量计算及灌库(Chroma向量数据库)                                                                                                         \n打开命令行终端，进入脚本所在目录，运行 python vectorSaveTest.py 命令                                                                                          \n### (4-1)在线步骤(检索增强生成),测试demo\n获取用户问题->用户问题向量化->检索向量数据库->将检索结果和用户问题填入prompt模版->用最终的prompt调用LLM->由LLM生成回复                                     \n在根目录下的ragTest/queryTest.py脚本实现核心业务逻辑                                                                       \n打开命令行终端，进入脚本所在目录，运行 python queryTest.py 命令                                \n### (4-2)在线步骤(检索增强生成),封装为API接口对外提供服务           \n获取用户问题->用户问题向量化->检索向量数据库->将检索结果和用户问题填入prompt模版->用最终的prompt调用LLM->由LLM生成回复                                \n在根目录下的ragTest/main.py脚本实现核心业务逻辑并封装为API接口对外提供服务                              \n在根目录下的ragTest/apiTest.py脚本实现POST请求，调用main服务API接口进行检索增强生成                               \n打开命令行终端，进入脚本所在目录，首先运行 python main.py 命令启动API接口服务                                                               \n再新打开一个命令行终端，进入脚本所在目录，运行 python apiTest.py 命令进行POST请求                     \n"
    },
    {
      "name": "rolandtannous/haystack-memory",
      "stars": 22,
      "img": "https://avatars.githubusercontent.com/u/115670425?s=40&v=4",
      "owner": "rolandtannous",
      "repo_name": "haystack-memory",
      "description": "Basic Memory library for Haystack NLP agents",
      "homepage": null,
      "language": "Python",
      "created_at": "2023-04-03T16:52:58Z",
      "updated_at": "2024-12-28T21:06:43Z",
      "topics": [],
      "readme": "# Deprecation Notice\n\n\nThis package is now deprecated and has not been maintained for a while. Please refer to the original Haystack repo for more up to date implementations of the same at [haystack](https://github.com/deepset-ai/haystack).\n\n# Haystack Memory\n\nMemory for [Haystack](https://github.com/deepset-ai/haystack) Agents. The library implements a working memory that stores the agent's conversation memory \nand a sensory memory that stores the agent's short-term sensory memory. The working memory can be utilized in-memory or through Redis, with the \nRedis implementation featuring a sliding window. On the other hand, the sensory memory is an in-memory implementation that mimics \na human's brief sensory memory, lasting only for the duration of one interaction.. \n\n## Installation\n\n- Python pip: ```pip install --upgrade haystack-memory``` . This method will attempt to install the dependencies (farm-haystack>=1.15.0, redis)\n- Python pip (skip dependency installation): Use  ```pip install --upgrade haystack-memory --no-deps```\n- Using git: ```pip install git+https://github.com/rolandtannous/haystack-memory.git@main#egg=haystack-memory```\n\n\n## Usage\n\nTo use memory in your agent, you need three components:\n- `MemoryRecallNode`: This node is added to the agent as a tool. It will allow the agent to remember the conversation and make query-memory associations.\n- `MemoryUtils`: This class should be used to save the queries and the final agent answers to the conversation memory.\n- `chat`: This is a method of the MemoryUtils class. It is used to chat with the agent. It will save the query and the answer to the memory. It also returns the full result for further usage.\n\n```py\nfrom haystack.agents import Agent, Tool\nfrom haystack.nodes import PromptNode\nfrom haystack_memory.prompt_templates import memory_template\nfrom haystack_memory.memory import MemoryRecallNode\nfrom haystack_memory.utils import MemoryUtils\n\n# Initialize the memory and the memory tool so the agent can retrieve the memory\nworking_memory = []\nsensory_memory = []\nmemory_node = MemoryRecallNode(memory=working_memory)\nmemory_tool = Tool(name=\"Memory\",\n                   pipeline_or_node=memory_node,\n                   description=\"Your memory. Always access this tool first to remember what you have learned.\")\n\nprompt_node = PromptNode(model_name_or_path=\"text-davinci-003\", \n                         api_key=\"<YOUR_OPENAI_KEY>\", \n                         max_length=1024,\n                         stop_words=[\"Observation:\"])\nmemory_agent = Agent(prompt_node=prompt_node, prompt_template=memory_template)\nmemory_agent.add_tool(memory_tool)\n\n# Initialize the utils to save the query and the answers to the memory\nmemory_utils = MemoryUtils(working_memory=working_memory,sensory_memory=sensory_memory, agent=memory_agent)\nresult = memory_utils.chat(\"<Your Question>\")\nprint(working_memory)\n```\n\n### Redis\n\nThe working memory can also be stored in a redis database which makes it possible to use different memories at the same time to be used with multiple agents. Additionally, it supports a sliding window to only utilize the last k messages.\n\n```py\nfrom haystack.agents import Agent, Tool\nfrom haystack.nodes import PromptNode\nfrom haystack_memory.memory import RedisMemoryRecallNode\nfrom haystack_memory.prompt_templates import memory_template\nfrom haystack_memory.utils import RedisUtils\n\nsensory_memory = []\n# Initialize the memory and the memory tool so the agent can retrieve the memory\nredis_memory_node = RedisMemoryRecallNode(memory_id=\"working_memory\",\n                                          host=\"localhost\",\n                                          port=6379,\n                                          db=0)\nmemory_tool = Tool(name=\"Memory\",\n                   pipeline_or_node=redis_memory_node,\n                   description=\"Your memory. Always access this tool first to remember what you have learned.\")\nprompt_node = PromptNode(model_name_or_path=\"text-davinci-003\",\n                         api_key=\"<YOUR_OPENAI_KEY>\",\n                         max_length=1024,\n                         stop_words=[\"Observation:\"])\nmemory_agent = Agent(prompt_node=prompt_node, prompt_template=memory_template)\n# Initialize the utils to save the query and the answers to the memory\nredis_utils = RedisUtils(agent=memory_agent,\n                         sensory_memory=sensory_memory,\n                         memory_id=\"working_memory\",\n                         host=\"localhost\",\n                         port=6379,\n                         db=0)\nresult = redis_utils.chat(\"<Your Question>\")\n```\n\n\n## Examples\n\nExamples can be found in the `examples/` folder. They contain usage examples for both in-memory and Redis memory types.\nTo open the examples in colab, click on the following links:\n- Basic Memory: [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/rolandtannous/HaystackAgentBasicMemory/blob/main/examples/example_basic_memory.ipynb)\n- Redis Memory: [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/rolandtannous/HaystackAgentBasicMemory/blob/main/examples/example_redis_memory.ipynb)\n\n\n\n\n\n\n\n"
    },
    {
      "name": "AIAnytime/PubMed-Healthcare-Chatbot",
      "stars": 22,
      "img": "https://avatars.githubusercontent.com/u/30049737?s=40&v=4",
      "owner": "AIAnytime",
      "repo_name": "PubMed-Healthcare-Chatbot",
      "description": "PubMed Healthcare Chatbot. LLM Augmented Q&A over PubMed Search Engine.",
      "homepage": null,
      "language": "Python",
      "created_at": "2024-01-21T16:43:55Z",
      "updated_at": "2025-04-06T04:25:28Z",
      "topics": [],
      "readme": "# PubMed-Healthcare-Chatbot\nPubMed Healthcare Chatbot. LLM Augmented Q&amp;A over PubMed Search Engine.\n\n![Screenshot](https://i.ibb.co/GMbx4Dw/LLM-Augmented-Q-A-over-Pub-Med-Search-Engine.png)\n\n"
    },
    {
      "name": "PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition",
      "stars": 21,
      "img": "https://avatars.githubusercontent.com/u/10974906?s=40&v=4",
      "owner": "PacktPublishing",
      "repo_name": "Python-Natural-Language-Processing-Cookbook-Second-Edition",
      "description": "Python Natural Language Processing Cookbook, Second Edition. Published by Packt.",
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2024-03-06T06:50:02Z",
      "updated_at": "2025-03-14T12:09:23Z",
      "topics": [],
      "readme": "# Python Natural Language Processing Cookbook, Second Edition\n\n\n<a href=\"https://www.packtpub.com/en-us/product/python-natural-language-processing-cookbook-9781803245744\"><img src=\"https://m.media-amazon.com/images/I/61vHwjtkrxL._SL1360_.jpg\" alt=\"Python Natural Language Processing Cookbook\" height=\"256px\" align=\"right\"></a>\n\nThis is the code repository for [Python Natural Language Processing Cookbook, Second Edition](https://www.packtpub.com/en-us/product/python-natural-language-processing-cookbook-9781803245744), published by Packt.\n\n**Over 60 recipes for building powerful NLP solutions using Python and LLM libraries**\n\n## What is this book about?\n\nMaster NLP through practical recipes in this second edition of Python Natural Language Processing Cookbook that expands on data preparation and modeling, and delves into transformer models, GPT-4, NLU, and XAI for advanced NLP tasks.\n\nThis book covers the following exciting features: \n* Understand fundamental NLP concepts along with their applications using examples in Python\n* Classify text quickly and accurately with rule-based and supervised methods\n* Train NER models and perform sentiment analysis to identify entities and emotions in text\n* Explore topic modeling and text visualization to reveal themes and relationships within text\n* Leverage Hugging Face and OpenAI LLMs to perform advanced NLP tasks\n* Use question-answering techniques to handle both open and closed domains\n* Apply XAI techniques to better understand your model predictions\n\nIf you feel this book is for you, get your [copy](https://www.amazon.com/Python-Natural-Language-Processing-Cookbook/dp/1803245743/ref=tmm_pap_swatch_0?_encoding=UTF8&sr=8-2) today!\n\n\n## Instructions and Navigations\nAll of the code is organized into folders.\n\nThe code will look like the following:\n```python\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\nimport torch\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n```\n\n**Following is what you need for this book:**\nThis updated edition of the Python Natural Language Processing Cookbook is for data scientists, machine learning engineers, and developers with a background in Python. Whether you’re looking to learn NLP techniques, extract valuable insights from textual data, or create foundational applications, this book will equip you with basic to intermediate skills. No prior NLP knowledge is necessary to get started. All you need is familiarity with basic programming principles. For seasoned developers, the updated sections offer the latest on transformers, explainable AI, and Generative AI with LLMs.\n\nWith the following software and hardware list you can run all code files present in the book (Chapter 1-10).\n\n### Software and Hardware List\n\n| Chapter  | Software required                                                                    | OS required                        |\n| -------- | -------------------------------------------------------------------------------------| -----------------------------------|\n|  \t1-10\t   | Python 3.10                       | Windows, macOS, or Linux|\n|  \t1-10\t   | Poetry                     | Windows, macOS, or Linux|\n|  \t1-10\t   | Jupyter Notebook (optional)                      | Windows, macOS, or Linux|\n\n### Related products <Other books you may enjoy>  \n* Mastering NLP from Foundations to LLMs  [[Packt]](https://www.packtpub.com/en-us/product/mastering-nlp-from-foundations-to-llms-9781804619186?type=print) [[Amazon]](https://www.amazon.com/Mastering-NLP-Foundations-LLMs-Techniques/dp/1804619183/ref=sr_1_1?sr=8-1)\n\n* Generative AI Foundations in Python [[Packt]](https://www.packtpub.com/en-us/product/generative-ai-foundations-in-python-9781835460825?type=print) [[Amazon]](https://www.amazon.com/Generative-Foundations-Python-techniques-challenges/dp/1835460828/ref=sr_1_1?sr=8-1)\n  \n## Get to Know the Authors\n**Zhenya Antić** is an expert in AI and NLP. Currently she is the Director of AI Automation at Arch , leading initiatives in Intelligent Document Processing, applying various AI solutions to complex problems. She has worked on various NLP projects with many different companies through her consulting experience. Zhenya has a Ph.D. in Linguistics from University of California-Berkeley and a B.S in Computer Science from Massachusetts Institute of Technology.\n\n**Saurabh Chakravarty** is a seasoned professional with expertise in NLP and large-scale distributed systems. He presently works with AWS as an SDE, where he is always looking to leverage ideas on how NLP and its associated technologies can be used in diverse ways to solve problems in other software engineering domains. Saurabh has a master's degree and Ph.D. from Virginia Tech, specializing in NLP and deep learning.\n"
    },
    {
      "name": "wsargent/groundedllm",
      "stars": 21,
      "img": "https://avatars.githubusercontent.com/u/71236?s=40&v=4",
      "owner": "wsargent",
      "repo_name": "groundedllm",
      "description": "AI agent grounded with search and extract tools to reduce hallucination.",
      "homepage": "https://tersesystems.com/blog/2025/04/13/writing-an-llm-that-just-works-for-my-brother/",
      "language": "JavaScript",
      "created_at": "2025-03-31T17:09:40Z",
      "updated_at": "2025-04-23T01:42:47Z",
      "topics": [
        "docker-compose",
        "grounded-bot",
        "hayhooks",
        "haystack-ai",
        "haystack-pipeline",
        "letta-framework",
        "llm",
        "open-webui"
      ],
      "readme": "# GroundedLLM\n\nThis is a pre-built, turnkey implementation of an AI agent grounded with search and extract tools to reduce hallucination.\n\nIf you have the API keys and Docker Compose, you should be able to go to http://localhost:3000 and have it Just Work.  It runs fine on a Macbook Pro with 8 GB memory.\n\nIt does require accounts with Tavily and Google's Gemini API, but these are free for individuals (as of 4/13/2025) and I've only hit the tier limit for Gemini once.\n\n* [Tavily Pricing](https://tavily.com/#pricing)\n* [Gemini Pricing](https://ai.google.dev/gemini-api/docs/pricing)\n\nEven if you use a paid model from Anthropic or OpenAI, it's more cost effective to use the API directly compared to the $20 a month for Claude Pro or ChatGPT Plus that you would need for longer context windows. \n\n## Who's Interested?\n\nThis project may be of interest to you if:\n\n* **You don't want to mess around.**  You want a search engine that knows what you want and gets smarter over time with very little effort on your part, and if that means you wait for 30 seconds to get the right answer, you're okay with that.\n* **You are interested in AI agents.** This project is a low effort way to play with [Letta](https://docs.letta.com/letta-platform), and see a [stateful agent](https://docs.letta.com/stateful-agents) that can remember and learn.\n* **You are interested in RAG pipelines.**  [Haystack](https://haystack.deepset.ai/) toolkit has several options to deal with document conversion, cleaning, and extraction.  The search and extract tools plug into these.\n* **You're interested in [Open WebUI](https://github.com/open-webui/open-webui).** Unlike [Perplexity](https://www.perplexity.ai) and [Perplexica](https://github.com/ItzCrazyKns/Perplexica), this project doesn't give you a new front end UI to deal with when you want to find things. Open WebUI is powerful and popular, so let's use that.\n* **You are interested in adding MCP servers.**  In addition to the search and extract tools, this project comes with [Wikipedia MCP server](https://github.com/scotthelm/wikipedia-mcp-server), [AWS documentation MCP server](https://awslabs.github.io/mcp/servers/aws-documentation-mcp-server/) and [Letta MCP Server](https://github.com/oculairmedia/Letta-MCP-server) and can be set up with more tools to help the agent with search.\n\n## Description\n\nThis project helps ground your LLM from hallucination by providing it with search and page extraction tools and the ability to remember things through [Letta](https://docs.letta.com/letta-platform), an agent framework with memory and tool capabilities.  Every time it searches or extracts a web page, it will save a summary of the search and the results into archival memory, and can refer back to them.\n\nBecause a Letta agent is a stateful agent, it doesn't matter if you bring up a new chat.  The agent will continue the conversation from where you left off and remembers details (like your name and location) so that it can answer new questions.\n\n![Sunset](./images/sunset.png)\n\nBecause a Letta agent has memory, it is teachable.  If you tell it your location, it will not only sort out timezone and locale for all subsequent queries, but it can also learn your preferences, websites to avoid, and even pick up new strategies for searching.  \n\nThe main advantage of an agent is that it's capable of recognizing and acting when a tool has not returned the correct results.  Letta will dig down into search results if it thinks it's not detailed enough.  For example, it performed three searches in response to the question \"Please give me history and background about the increased traffic to sites from AI bots scraping, and the countermeasures involved.  When did this start, why is it a problem, and why is it happening?\"\n\n* \"History and background of increased web traffic from AI bots scraping websites, when it started becoming a major issue, why it's a problem, and why it's happening. Include information about countermeasures websites use against AI scraping.\"\n* \"When did AI bots scraping websites first become a significant issue? What specific countermeasures have websites implemented against AI scraping bots? Include historical timeline and details about robots.txt, legal cases, and technical measures.\"\n* \"What are the major legal cases about AI web scraping from 2020-2025? When did companies like OpenAI and Anthropic start large-scale web scraping for training AI models?\"\n\nAnd produced this:\n\n![AI Spam](./images/aispam.png)\n\nIf you want more details on what it's thinking, you can dig into the reasoning using [Letta Desktop](https://docs.letta.com/quickstart/desktop).  Here's an example of what goes on behind the scenes when I ask \"What are the differences between [Roo Code](https://docs.roocode.com) and [Cline](https://github.com/cline/cline)?\"\n\n![Letta Grounding with Search](./images/grounding.png)\n\nIn addition to search, Letta can also extract content from specific URLs.  For example:\n\n![Extract](./images/extract.png)\n\nThis is useful when the search engine hasn't picked up information on the pages.\n\n## Getting Started\n\nYou will need the following:\n\n* [Docker Compose](https://docs.docker.com/compose/install/).\n* [Tavily API key](https://app.tavily.com/home) -- free up to 1000 searches, pay as you go (PAYG) is 8 cents per 1000 searches.\n* [Gemini API key](https://ai.google.dev/gemini-api/docs/api-key).  The docker-compose.yml file is set up for `Gemini 2.5 Pro Experimental` and `Gemini Embedding Experimental 03-07`, which are on the free tier, but has [lower rate limits](https://ai.google.dev/gemini-api/docs/rate-limits#current-rate-limits).\n\nOptional:\n\n* [Anthropic or OpenAI API Key](https://console.anthropic.com/settings/keys) for Letta (Claude Sonnet 3.7, gpt4, etc).  Commented out in LiteLLM and the docker compose file.\n\nIf you do not have these accounts or API keys, it is *very* simple to set them up if you have a Google or Github account.  Gemini will ask you to sign in with your Google account, then give you a free key.  If you want to upgrade, you can set up a  [billing account](https://ai.google.dev/gemini-api/docs/billing) for PAYG.  Tavily is the same way; there's no [credit card required](https://docs.tavily.com/documentation/api-credits) and PAYG is opt in.\n\nTo configure the API keys, start by creating an `.env` file from the `env.example` file:\n\n```\ncp env.example .env\n# edit .env file with your own API keys\n```\n\nTo start the services, run the following:\n\n```bash\ndocker compose up\n```\n\nYou will see a bunch of text in the logs, but the important bit is this line:\n\n```\ninitializer  | 2025-04-06 14:29:00,484 - INFO - Initialization complete!\n```\n\n(If you don't see this, it's probably a bug.  [File an issue](https://github.com/wsargent/groundedllm/issues/new) and copy and paste the logs into the issue.)\n\nWhen you see that, you should be good to go.  Open a browser at http://localhost:3000 and type in \"hello.\"\n\n![Hello](./images/hello.png)\n\n## Working with Letta\n\nThe first thing you'll want to do is tell Letta your name and location -- this will help it understand where and when you are.\n\nAfter that, you will want to give it preferences, using the phrase \"store this in your core memory\" so that it can remember it for later.\n\nSome example preferences:\n\n* I like mermaid diagrams for visualizing technical concepts and relationships.\n* I am using Haystack 2.12, please specify 2.x when searching for Haystack docs.\n* Only give me sample code examples when I explicitly ask you to.\n* Show me inline images when you provide results from Wikipedia pages.\n\nBecause Letta doesn't always store conversations in archival memory, you also want to ask it to explicitly summarize and store the conversation when you're changing topics.  This lets you take notes and store bookmarks when you want to bring up an old topic for later.\n\nGrounding with search can reduce hallucinations, but *will not eliminate them*.  You will still need to check the sources and validate that what Letta is telling you is accurate, especially if you are doing anything critical.  Also, do your own searches!  Search engines are free for humans, and Letta will be happy to give you its reference material.\n\n## Management\n\nWhen you want it to run in the background, you can run it as a daemon:\n\n```bash\ndocker compose up -d\n```\n\nTo rebuild a container (probably Hayhooks or a new MCP container you're adding):\n\n```\ndocker compose up --build 'hayhooks'\n```\n\nTo completely destroy all resources (including all your data!) and rebuild from scratch:\n\n```bash\ndocker compose down -v --remove-orphans && docker compose up --build\n```\n\nIf you want to modify functionality, see the Hayhooks [README](./hayhooks/README.md).\n\n## Composition\n\nThe docker compose file integrates several key components:\n\n* **Open WebUI:** A user-friendly front-end interface \n* **Letta:** An agent framework with built-in memory and tooling capabilities.\n* **Hayhooks:** A tool server for use by Letta.\n* **LiteLLM Proxy Server:**  Makes all providers \"OpenAI style\" for Hayhooks.\n* **MCP Servers:** Various MCP servers in docker containers.\n\nNote that if you delete or rename the Letta agent or the Open WebUI pipe, the initializer will provision a new one with the same name automatically.\n\n### Open WebUI\n\n[Open WebUI](https://docs.openwebui.com) is the standard for front end interfaces to LLMs and AIs in general.\n\nThere are a number of tweaks to [improve performance](https://docs.openwebui.com/tutorials/tips/improve-performance-local) and minimize the time to get started.\n\nFor example, this instance is configured to use Gemini embedding so that it doesn't download 900MB of embedding model for its local RAG.\n\nIt is not possible to upload files into Letta through the Open WebUI interface right now.  The functionality does exist in Letta through the [data sources](https://docs.letta.com/guides/agents/sources) feature, but it might be easier to use a OWUI plugin to send it to Hayhooks and keep it in a document store.\n\n### Letta\n\n[Letta](https://docs.letta.com) is an agent framework that has built-in self editing memory and built-in tooling for editing the behavior of the agent, including adding new tools.\n\nThe search technique is pulled from this academic paper on [DeepRAG](https://arxiv.org/abs/2502.01142), although [query decomposition](https://haystack.deepset.ai/blog/query-decomposition) is a well known technique in general.  If you want a classic deep learning style agent, you can import one from Letta's [agent-file git repository](https://github.com/letta-ai/agent-file/tree/main/deep_research_agent).\n\n#### Picking a Model Provider\n\nThe model is set up with Claude Sonnet 3.7 as it is much more proactive about calling tools until it gets a good answer.  You can use OpenAI for the same effect.  Gemini 2.0 models have been inconsistent and less proactive than Claude Sonnet, although Gemini 2.5 Pro is *very* smart at interpreting existing data.\n\nIf you are going to use Ollama with Letta you will need a powerful model, at least 13B and preferably 70B.\n\nSome reasoning models have difficulty interacting with Letta's reasoning step.  Deepseek and Gemini 2.5 Pro will attempt to reply in the reasoning step, although that may be fixed in the latest version.\n\n#### Letta Desktop\n\nYou may want Letta Desktop, which will allow you to see what the agent is doing under the hood, and directly edit the functionality. You can download it [here](https://docs.letta.com/quickstart/desktop).  Pick the PostgreSQL option when it comes up.\n\nStart the docker compose app *first* and *then* open up Letta Desktop, as it is connecting to the Letta agent running inside the container.\n\n### Hayhooks\n\n[Hayhooks](https://github.com/deepset-ai/hayhooks/) is a FastAPI-based server that exposes [Haystack Pipelines](https://docs.haystack.deepset.ai/docs/intro) through REST APIs. It's primarily used for RAG, but it's also a great way to make tools available in general as it has MCP and OpenAPI support.\n\nTo cut down on Anthropic's brutally low rate limits and higher costs, the search and extract tools use Google Flash 2.0 to process the output from Tavily and create an answer for Letta.  Google Flash 2.0 also recommends possible follow up queries and [query expansion](https://haystack.deepset.ai/blog/query-expansion) along with the search results.\n\nThe extract tool converts HTML to Markdown and does some document cleanup before sending it to Google Flash 2.0.  Only HTML is processed for now, although there are [many converters](https://docs.haystack.deepset.ai/docs/converters) available, and PDF support through [docling-haystack](https://haystack.deepset.ai/integrations/docling) or [docling-serve](https://github.com/docling-project/docling-serve) should be easy.\n\nThere is no vector/embeddings/database RAG involved in this project, although you have the option to use your own by plugging it into Hayhooks.  In addition, Letta's archival memory is technically a RAG implementation based on pgvector.\n\nSee the [README](./hayhooks/README.md) for details of the tools provided by Hayhooks.\n\n### LiteLLM Proxy Server\n\nThe [LiteLLM proxy server](https://docs.litellm.ai/docs/proxy/deploy) that provides an OpenAI compatible layer on top of several different providers. It is provided to Open WebUI (commented out) and to Hayhooks.\n\nLiteLLM is mostly commented out here to focus attention on Letta.  However, it is very useful in general, especially as you scale up in complexity, and I think it's easier if you start using it from the beginning.\n\n* It provides a way to point to a conceptual model rather than a concrete one (you can point to \"claude-sonnet\" and change the model from 3.5 to 3.7).  \n* It insulates Open WebUI from the underlying providers.  You don't have to worry about changing your API key or other configuration settings when switching providers.  You also don't have to worry about Open WebUI timing out for 30 seconds while it tries to reach an unreachable provider.\n* It lets you specify the same model with different parameters, so you can use `extra-headers` to experiment with [token-efficient tool use](https://docs.anthropic.com/en/docs/build-with-claude/tool-use/token-efficient-tool-use), for example.\n\n### MCP Servers\n\nThe search agent is configured with tools through Letta's MCP support with some MCP servers.  There are a couple of good reasons for putting MCP servers in Docker containers.  The first is that Letta will not work with stdio MCP servers, and so SSE is required.  The second is that many MCP servers benefit from the isolation provided by containers -- they can have exactly the required environment and version they need without impacting other MCP servers or the OS itself.  It also makes it much easier to add and remove servers.\n\n* Hayhooks itself provides the `search` and `extract` tools for fine-grained control over Tavily.\n* Wikipedia search is provided by [wikipedia-mcp-server](https://github.com/scotthelm/wikipedia-mcp-server).\n* AWS documentation is provided by [aws-documentation-mcp-server](https://awslabs.github.io/mcp/servers/aws-documentation-mcp-server/).\n* Letta MCP Server is from [letta-mcp-server](https://github.com/oculairmedia/Letta-MCP-server) is configured but not attached to the search agent (you'll have to do that yourself through Letta Desktop).\n\nThe search will use these as appropriate, but you can prompt it by asking, i.e. \"Use the recommend tool to recommend documentation for <sample AWS doc url>\" and it will use the `recommend` tool.\n\nThe search will use these as appropriate, but you can prompt it to use a specific tool by asking, i.e. \"Use the recommend tool to recommend documentation for <sample AWS doc url>\" and it will use the `recommend` tool.\n\nYou can add your own MCP servers.  To do this is a four step process:\n\n1. Find the MCP server you want, and create an `mcp/my-mcp-server` directory, then set up the `Dockerfile` to encapsulate it with a proxy that exposes it over SSE.\n2. Add the MCP docker container to `docker-compose.yml`.\n3. Add the URL to the docker container's endpoint in `letta_mcp_config.json`.\n4. Add the MCP tools that you want the search agent provisioned with in the `hayhooks/provision_search_agent/pipeline_wrapper.py` file.\n\nMCP is a pain in the butt [security wise](https://blog.sshh.io/p/everything-wrong-with-mcp) so wrapping them in docker containers at least limits the blast radius a bit.\n\nIt can be some work to set up credentials and work out how to set up the proxy for any given MCP server, so you should be technically comfortable with some futzing here.  You can ask Cline to use the existing instances as templates.\n\n## Privacy Concerns\n\nSince you're using this for search, you may want to know how your queries are processed.\n\nThere are three different services involved in search, each with their own privacy policy.\n\nYou do have options for customization.  Since the tools go through Hayhooks, you can write a [ConditionalRouter](https://docs.haystack.deepset.ai/docs/conditionalrouter) that will send different queries to different services or evaluate them before they are processed.\n\n### Anthropic\n\nAnthropic's [privacy policy](https://www.anthropic.com/legal/privacy) is clear: they do not use personal data for model training [without explicit consent](https://privacy.anthropic.com/en/articles/10023580-is-my-data-used-for-model-training).\n\n### Tavily\n\nTavily's [privacy policy](https://tavily.com/privacy) is that they do store queries, and they will use queries to improve the quality of their services.  You can opt out through the [account settings](https://app.tavily.com/account/settings).\n\n### Google\n\nGoogle's [privacy policy](https://support.google.com/gemini/answer/13594961) states that your conversations with Gemini may be used to improve and develop their products and services, including machine learning technologies.  The [Gemini Pricing](https://ai.google.dev/gemini-api/docs/pricing) page says the free tier does your conversations to train their models. They do use human reviewers and there is a note saying **Please don’t enter confidential information in your conversations or any data you wouldn’t want a reviewer to see or Google to use to improve our products, services, and machine-learning technologies.**\n\n"
    },
    {
      "name": "TuanaCelik/hackernews-summaries",
      "stars": 21,
      "img": "https://avatars.githubusercontent.com/u/15802862?s=40&v=4",
      "owner": "TuanaCelik",
      "repo_name": "hackernews-summaries",
      "description": "🧡 Hacker News summaries",
      "homepage": "https://huggingface.co/spaces/Tuana/hackernews-summaries",
      "language": "Python",
      "created_at": "2024-02-02T15:07:54Z",
      "updated_at": "2025-04-05T19:58:14Z",
      "topics": [],
      "readme": "---\ntitle: Hacker News Summaries\nemoji: 🧡\ncolorFrom: yellow\ncolorTo: green\nsdk: streamlit\nsdk_version: 1.25.0\napp_file: app.py\npinned: true\n---\n\n# Hacker News Summaries\n\n### Try it out on [🤗 Spaces](https://huggingface.co/spaces/deepset/hackernews-summaries)\n\n##### A simple app to get summaries of some of the latest top hackernews posts\n\nThis is a demo just for fun 🥳\nThis repo contains a streamlit application that given a number between 1 - 5 gives you summaries of that many of the latest top Hacker News posts.\nIt uses a [custom Haytack component](https://docs.haystack.deepset.ai/v2.0/docs/custom-component?utm_campaign=developer-relations)\nIt's been built with [Haystack](https://haystack.deepset.ai) using the [`HuggingFaceTGIGenerartor`](https://docs.haystack.deepset.ai/v2.0/docs/huggingfacetgigenerator?utm_campaign=developer-relations) and by creating a [`PromptBuilder`](https://docs.haystack.deepset.ai/v2.0/docs/promptbuilder?utm_campaign=developer-relations)\n\n\nIf you try to run it yourself and find ways to make this app better, please feel free to create an issue/PR 🙌\n\n## Installation and Running\n1. Install requirements:\n`pip install -r requirements.txt`\n2. Run the streamlit app:\n`streamlit run app.py`\n\nThis will start up the app on `localhost:8501` where you will dind a simple search bar\n\n#### The Haystack Community is on [Discord](https://discord.com/invite/VBpFzsgRVF)\n"
    },
    {
      "name": "aws-samples/rag-workshop-amazon-bedrock-knowledge-bases",
      "stars": 20,
      "img": "https://avatars.githubusercontent.com/u/8931462?s=40&v=4",
      "owner": "aws-samples",
      "repo_name": "rag-workshop-amazon-bedrock-knowledge-bases",
      "description": null,
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2024-09-27T22:02:33Z",
      "updated_at": "2025-04-22T06:01:33Z",
      "topics": [],
      "readme": ""
    },
    {
      "name": "cmin764/cmiN",
      "stars": 20,
      "img": "https://avatars.githubusercontent.com/u/709053?s=40&v=4",
      "owner": "cmin764",
      "repo_name": "cmiN",
      "description": "My (old) personal work",
      "homepage": null,
      "language": "C",
      "created_at": "2014-07-22T22:22:32Z",
      "updated_at": "2025-01-25T18:45:09Z",
      "topics": [],
      "readme": "cmiN\n====\n\nMy (old) personal work\n"
    },
    {
      "name": "danilop/oss-for-generative-ai",
      "stars": 20,
      "img": "https://avatars.githubusercontent.com/u/1550395?s=40&v=4",
      "owner": "danilop",
      "repo_name": "oss-for-generative-ai",
      "description": "Open Source Frameworks for Building Generative AI Applications",
      "homepage": null,
      "language": "Python",
      "created_at": "2024-10-02T11:24:28Z",
      "updated_at": "2025-04-05T00:55:05Z",
      "topics": [],
      "readme": "# Open Source Frameworks for Building Generative AI Applications\n\nThis repository contains examples of popular open source frameworks for building generative AI applications and showcases how to use these frameworks with [Amazon Bedrock](https://aws.amazon.com/bedrock/).\n\n## Frameworks Included\n\n- **[LangChain](https://www.langchain.com/)**: A framework for developing applications powered by language models, featuring examples of:\n  - Basic model invocation\n  - Chaining prompts\n  - Building an API\n  - Creating a client\n  - Implementing a chatbot\n  - Using Bedrock Agents\n\n- **[LangGraph](https://github.com/langchain-ai/langgraph)**: An extension of LangChain for building stateful, multi-actor applications with large language models (LLMs)\n\n- **[Haystack](https://haystack.deepset.ai/)**: An end-to-end framework for building search systems and language model applications\n\n- **[LlamaIndex](https://www.llamaindex.ai/)**: A data framework for LLM-based applications, with examples of:\n  - RAG (Retrieval-Augmented Generation)\n  - Building an agent\n\n- **[DSPy](https://github.com/stanfordnlp/dspy)**: A framework for solving AI tasks using large language models\n\n- **[RAGAS](https://github.com/explodinggradients/ragas)**: A framework for evaluating Retrieval Augmented Generation (RAG) pipelines\n\n- **[LiteLLM](https://github.com/BerriAI/litellm)**: A library to standardize the use of LLMs from different providers\n\n## Getting Started\n\nEach framework has its own directory containing example scripts and a `requirements.txt` file listing the necessary dependencies.\n\nTo run the examples:\n\n1. Clone this repository\n2. Navigate to the desired framework's directory\n3. For Python projects:\n   - Create a virtual environment: `python -m venv .venv` (recommended)\n   - Activate the virtual environment:\n     - On Windows: `.venv\\Scripts\\activate`\n     - On macOS and Linux: `source .venv/bin/activate`\n   - Install the required dependencies: `pip install -r requirements.txt`\n   - Run the example scripts\n\n## Configuration\n\nEnsure you have the necessary AWS credentials and permissions set up to access Amazon Bedrock.\n\n## Frameworks Overview\n\n### [LangChain](https://www.langchain.com/)\n- A framework for developing applications powered by language models\n- Key Features:\n  - Modular components for LLM-powered applications\n  - Chains and agents for complex LLM workflows\n  - Memory systems for contextual interactions\n  - Integration with various data sources and APIs\n- Primary Use Cases:\n  - Building conversational AI systems\n  - Creating domain-specific question-answering systems\n  - Developing AI-powered automation tools\n\n### [LangGraph](https://github.com/langchain-ai/langgraph)\n- An extension of LangChain for building stateful, multi-actor applications with LLMs\n- Key Features:\n  - Graph-based workflow management\n  - State management for complex agent interactions\n  - Tools for designing and implementing multi-agent systems\n  - Cyclic workflows and feedback loops\n- Primary Use Cases:\n  - Creating collaborative AI agent systems\n  - Implementing complex, stateful AI workflows\n  - Developing AI-powered simulations and games\n\n### [Haystack](https://haystack.deepset.ai/)\n- An open-source framework for building production-ready LLM applications\n- Key Features:\n  - Composable AI systems with flexible pipelines\n  - Multi-modal AI support (text, image, audio)\n  - Production-ready with serializable pipelines and monitoring\n- Primary Use Cases:\n  - Building RAG pipelines and search systems\n  - Developing conversational AI and chatbots\n  - Content generation and summarization\n  - Creating agentic pipelines with complex workflows\n\n### [LlamaIndex](https://www.llamaindex.ai/)\n- A data framework for building LLM-powered applications\n- Key Features:\n  - Advanced data ingestion and indexing\n  - Query processing and response synthesis\n  - Support for various data connectors\n  - Customizable retrieval and ranking algorithms\n- Primary Use Cases:\n  - Creating knowledge bases and question-answering systems\n  - Implementing semantic search over large datasets\n  - Building context-aware AI assistants\n\n### [DSPy](https://github.com/stanfordnlp/dspy)\n- A framework for solving AI tasks through declarative and optimizable language model programs\n- Key Features:\n  - Declarative programming model for LLM interactions\n  - Automatic optimization of LLM prompts and parameters\n  - Signature-based type system for LLM inputs/outputs\n  - Teleprompter (now optimizer) for automatic prompt improvement\n- Primary Use Cases:\n  - Developing robust and optimized NLP pipelines\n  - Creating self-improving AI systems\n  - Implementing complex reasoning tasks with LLMs\n\n### [RAGAS](https://github.com/explodinggradients/ragas)\n- An evaluation framework for Retrieval Augmented Generation (RAG) systems\n- Key Features:\n  - Automated evaluation of RAG pipelines\n  - Multiple evaluation metrics (faithfulness, context relevancy, answer relevancy)\n  - Support for different types of questions and datasets\n  - Integration with popular RAG frameworks\n- Primary Use Cases:\n  - Benchmarking RAG system performance\n  - Identifying areas for improvement in RAG pipelines\n  - Comparing different RAG implementations\n\n### [LiteLLM](https://github.com/BerriAI/litellm)\n- A unified interface for multiple LLM providers\n- Key Features:\n  - Standardized API for 100+ LLM models\n  - Automatic fallback and load balancing\n  - Caching and retry mechanisms\n  - Usage tracking and budget management\n- Primary Use Cases:\n  - Simplifying multi-LLM application development\n  - Implementing model redundancy and fallback strategies\n  - Managing LLM usage across different providers\n\n## License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n\n## Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.\n\n## Disclaimer\n\nThis is a collection of examples for educational purposes. Ensure you comply with the terms of service for Amazon Bedrock and any other services used when deploying these examples in production environments."
    },
    {
      "name": "Hoanganhvu123/ShoppingGPT",
      "stars": 20,
      "img": "https://avatars.githubusercontent.com/u/89352572?s=40&v=4",
      "owner": "Hoanganhvu123",
      "repo_name": "ShoppingGPT",
      "description": "🌼 ShoppingGPT 🛒: AI-powered Shopping Assistant  - RAG + LLMs + Semantic Router +Vietnamese",
      "homepage": "",
      "language": "HTML",
      "created_at": "2024-06-01T09:27:46Z",
      "updated_at": "2025-04-05T00:51:14Z",
      "topics": [
        "agent",
        "ai",
        "chatbot",
        "flask-application",
        "genai-chatbot",
        "llms",
        "shopping",
        "shoppinggpt"
      ],
      "readme": "<h1 align=\"center\">ShoppingGPT🌼</h1>\n\n<p align=\"center\">\n  <img src=\"public/shoppinggpt_logo.png\" alt=\"ShoppingGPT Logo\" width=\"200\" height=\"auto\">\n</p>\n \nShoppingGPT is an AI-powered intelligent shopping assistant that combines advanced natural language processing techniques to deliver a smart and seamless shopping experience. Built with a focus on performance, scalability, and user experience, ShoppingGPT integrates cutting-edge technologies to revolutionize e-commerce interactions.\n\n## Features ✨\n\n- 🧠 **Large Language Models (LLMs)**: Leverages the power of Google's Gemini model for natural, context-aware conversations.\n- 📚 **RAG (Retrieval-Augmented Generation)**: Enhances responses with relevant product information from SQLite database, ensuring accurate and up-to-date product details.\n- 🛣️ **Semantic Router**: Intelligently routes queries to the appropriate handling mechanism using advanced embedding techniques.\n- 🔍 **Advanced Product Search**: Utilizes case-insensitive and partial matching capabilities, powered by efficient SQLite queries and indexing.\n- 💬 **Intelligent Chatbot Interface**: User-friendly chat interface designed for intuitive product queries and personalized recommendations.\n\n\n## System Architecture\n\nShoppingGPT employs a modular, scalable architecture:\n\n1. **User Interface (Flask Web App)**\n   - Handles user input and displays responses\n\n2. **Semantic Router**\n   - Utilizes GoogleGenerativeAIEmbeddings\n   - Classifies and routes user queries to appropriate handlers\n\n3. **Query Handlers**\n   a. **Chitchat Chain**\n      - Manages general conversation\n      - Leverages LLM (Gemini-1.5-flash) and ConversationBufferMemory\n   \n   b. **Shopping Agent**\n      - Processes product-related queries\n      - Employs various tools:\n        - Product Search Tool (SQLite-based)\n        - Policy Search Tool (FAISS-based)\n\n4. **Data Storage**\n   - SQLite database for product information\n   - FAISS vector store for policy information\n\n5. **External Services**\n   - Google Generative AI API for LLM and embeddings\n\nThis architecture enables efficient query routing, context-aware responses, and seamless integration of product and policy information into the conversation flow.\n\n\n### Key Components\n\n- **RAG System**: Combines FAISS vector store for policy information and SQLite for product data, ensuring fast and relevant information retrieval.\n- **LLM Integration**: Utilizes `ChatGoogleGenerativeAI` with the Gemini-1.5-flash model to generate human-like responses.\n- **Product Search**: Implements a robust `ProductDataLoader` class for efficient SQLite query execution and result formatting.\n- **Policy Search**: Uses FAISS for fast similarity search on company policies and guidelines.\n\n## Advanced Routing and Classification 🧭\n\nShoppingGPT utilizes advanced routing and classification techniques to enhance its performance:\n\n1. **Semantic Router**: We implement the [Semantic Router](https://github.com/aurelio-labs/semantic-router) library for efficient query routing. This allows for superfast decision-making and intelligent processing of multi-modal data.\n\n2. **Cosine Similarity**: The system employs cosine similarity algorithm to measure the semantic similarity between user queries and predefined routes, ensuring accurate classification and routing.\n\n3. **Custom Hugging Face Model**: We use a fine-tuned Hugging Face model specifically trained for classifying text as either chitchat or product-related. This model enhances the accuracy of query classification.\n\n## Model 🧠\n\nThe text classification model used in this project is available on Hugging Face:\n\n[hang1704/opendaisy](https://huggingface.co/hang1704/opendaisy)\n\nThis model is used for classifying user queries and enhancing the overall performance of ShoppingGPT. Feel free to explore and use it in your own projects!\n\n\n## Data Structure 🗂️\n\nProduct data is stored in SQLite and includes the following fields:\n\n- `product_code`: Unique identifier (TEXT)\n- `product_name`: Name of the product (TEXT)\n- `material`: Material composition (TEXT)\n- `size`: Available sizes (TEXT)\n- `color`: Available colors (TEXT)\n- `brand`: Manufacturer or seller (TEXT)\n- `gender`: Target gender (TEXT)\n- `stock_quantity`: Quantity in stock (INTEGER)\n- `price`: Product price (REAL)\n\n## Installation 🛠️\n\nTo set up ShoppingGPT:\n\n1. **Clone the repository**:\n    ```bash\n    git clone https://github.com/yourusername/ShoppingGPT.git\n    cd ShoppingGPT\n    ```\n\n2. **Create and activate a virtual environment**:\n    ```bash\n    python -m venv venv\n    source venv/bin/activate  # On Windows, use `venv\\Scripts\\activate`\n    ```\n\n3. **Install dependencies**:\n    ```bash\n    pip install -r requirements.txt\n    ```\n\n4. **Configure environment**:\n   Create a `.env` file in the root directory:\n   ```\n   GOOGLE_API_KEY=your_google_api_key\n   ```\n\n5. **Initialize the database**:\n    ```bash\n    python scripts/init_db.py\n    ```\n\n## Usage 🖥️\n\nTo start ShoppingGPT:\n\n1. **Run the Flask application**:\n   ```bash\n   python app.py\n   ```\n\n2. **Access the chatbot**:\n   Open your web browser and navigate to `http://localhost:5000`.\n\n3. **Interact with ShoppingGPT**:\n   - Ask about products: \"What red shirts do you have in stock?\"\n   - Inquire about policies: \"What's your return policy?\"\n   - General chat: \"How's the weather today?\"\n\n## Project Showcase 📸\n\n![ShoppingGPT Interface](public/interface.png)\n*Our user-friendly chat interface*\n\n\n![Product Search Demo](public/product_search.png)\n*Example of product search results*\n\n\n![Chitchat Example](public/chitchat_example.png)\n*Friendly conversation with our AI assistant*\n\n\nThese images showcase the key aspects of our ShoppingGPT project. From the intuitive chat interface to the powerful product search and easy access to policy information, our AI assistant is designed to enhance your shopping experience! 🛍️🤖\n\n## Customization 🛠️\n\nTo customize ShoppingGPT for your specific needs:\n\n1. **Update product database**: Modify `data/products.db` with your inventory.\n2. **Adjust policies**: Edit `data/policy.txt` with your company's guidelines.\n3. **Fine-tune responses**: Modify prompt templates in `shoppinggpt/tool/product_search.py` and `shoppinggpt/tool/policy_search.py`.\n\n## Troubleshooting 🔍\n\nIf you encounter any issues:\n\n1. Ensure all environment variables are correctly set.\n2. Check the console for any error messages.\n3. Verify that the database and policy files are in the correct locations.\n4. Make sure all required dependencies are installed correctly.\n5. Confirm that you're using a compatible Python version (3.7+).\n\nFor more detailed information, please refer to the documentation or open an issue on the GitHub repository.\n\n## Contributing 🤝\n\nWe welcome contributions to ShoppingGPT! Here's how you can help:\n\n1. Fork the repository\n2. Create a new branch (`git checkout -b feature/AmazingFeature`)\n3. Make your changes\n4. Commit your changes (`git commit -m 'Add some AmazingFeature'`)\n5. Push to the branch (`git push origin feature/AmazingFeature`)\n6. Open a Pull Request\n\nPlease make sure to update tests as appropriate and adhere to the project's coding standards.\n\n\n## License 📄\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n\n## Acknowledgments 👏\n\n- Thanks to all contributors who have helped shape ShoppingGPT\n- Special thanks to the open-source community for providing amazing tools and libraries\n\nHappy shopping with ShoppingGPT! 🛍️🤖"
    },
    {
      "name": "mathislucka/pycon-berlin-2023",
      "stars": 19,
      "img": "https://avatars.githubusercontent.com/u/16985509?s=40&v=4",
      "owner": "mathislucka",
      "repo_name": "pycon-berlin-2023",
      "description": "A repository for my PyCon talk: \"Building a personal assistant with Haystack and GPT: How to feed facts to large language models and reduce hallucinations\"",
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2023-04-16T11:13:32Z",
      "updated_at": "2024-02-09T17:36:54Z",
      "topics": [],
      "readme": "# Building a personal assistant with Haystack and GPT\n\n![haystack logo](assets/haystack-logo-colored.svg)\n\nThis is the code for my PyCon Berlin 2023 talk:\n\n**Building a personal assistant with Haystack and GPT**\n\n*How to feed facts to large language models and reduce hallucinations*\n\n[https://pretalx.com/pyconde-pydata-berlin-2023/talk/H8KMTT/](https://pretalx.com/pyconde-pydata-berlin-2023/talk/H8KMTT/)\n\n\n### Installation\n\n`pip install -r requirements.txt`\n\nTo run the code, you will have to obtain an API key from OpenAI at\n[https://platform.openai.com/account/api-keys](https://platform.openai.com/account/api-keys).\n\nStore the key in an `.env` file under `OPENAI_KEY`.\n\n### Learn More\n\nHead over to [https://docs.haystack.deepset.ai/docs](https://docs.haystack.deepset.ai/docs) to learn more about\nhaystack.\n\n"
    },
    {
      "name": "MedBot-team/NaBot",
      "stars": 19,
      "img": "https://avatars.githubusercontent.com/u/88496278?s=40&v=4",
      "owner": "MedBot-team",
      "repo_name": "NaBot",
      "description": "NaBot is a Rasa-based, smart medical chatbot. It can retrieve medicines and lab tests information for users just by chatting with it. ",
      "homepage": "https://MedBot-team.github.io/NaBot",
      "language": "Jupyter Notebook",
      "created_at": "2021-09-04T23:55:37Z",
      "updated_at": "2024-04-29T23:03:01Z",
      "topics": [
        "chatbot",
        "deep-learning",
        "machine-learning",
        "medical",
        "nlp"
      ],
      "readme": "# NaBot Medical Chatbot :robot:\n\n<div align=\"center\">\n\n![Chatbot Unit testing](https://github.com/MedBot-team/NaBot/actions/workflows/rasa-test.yml/badge.svg?style=svg)\n![Action server workflow](https://github.com/MedBot-team/NaBot/actions/workflows/docker_push_actions.yml/badge.svg?style=svg)\n![Chatbot server workflow](https://github.com/MedBot-team/NaBot/actions/workflows/docker_push_chatbot.yml/badge.svg?style=svg)\n![Chatbot UI workflow](https://github.com/MedBot-team/NaBot/actions/workflows/docker_push_ui.yml/badge.svg?style=svg)\n![Chatbot Monitoring workflow](https://github.com/MedBot-team/NaBot/actions/workflows/docker_push_monitoring.yml/badge.svg?style=svg)\n\n</div>\n\n<p align=\"center\">\n  <img src=\"https://github.com/Ali-Razmdideh/NaBot/blob/main/logo.png\" />\n</p>\n\nNaBot is a smart medical chatbot. It can retrieve medicines and lab tests information for users just by chatting with it.\n\n- Hey, NaBot, can you give me dosage information of Acetaminophen?\n- Of course, I can! :wink:\n\nWe will support more features in the future. But now, we only support medicines and lab tests information.\n\n# Test Chatbot\n\n- Use website:\n  - https://nabot.ml/\n- Use Telegram channel\n  - http://t.me/ask_nabot\n\n# Use api\n  - https://api.nabot.ml\n\n# Test Monitoring tools\n  - https://monitor.nabot.ml/\n\n# List of NaBot features\n\nYou can find out a list of medicines, labs, and their information which we're supporting, in the following. \n\n* [List of medicines in our dataset](https://github.com/MedBot-team/NaBot/wiki/List-of-medicines)\n* [List of lab tests in our dataset](https://github.com/MedBot-team/NaBot/wiki/List-of-lab-test)\n* [List of information about medicines that NaBot supports](https://github.com/MedBot-team/NaBot/wiki/Medicines-information)\n* [List of information about lab tests that NaBot supports](https://github.com/MedBot-team/NaBot/wiki/Lab-tests-information)\n\n# Installation Guide\n\nNaBot can be installed from its source code or by docker images. For more information, please read our [installation guide document](https://github.com/MedBot-team/NaBot/wiki/Installation-Guide).\n\n# Contributing\n\nIf you have an idea for improving NaBot, or even if you want medicine or lab test information, which is not supported in NaBot yet, do not hesitate. Create an issue in GitHub. :heart:\n\nNaBot needs an open-source contribution. The more features it has, the more skillful it becomes! \n\n# Credits\nOur chatbot gets its power from the [Rasa](https://rasa.com/) engine. We are thankful to their team for their contribution to open-source society. :heart_eyes:\n\nOur autocorrect component in the chatbot is based on [SymSpell](https://github.com/wolfgarbe/SymSpell), which medical and Persian dictionaries are added to it. We are thankful to their team for their contribution to open-source society. :heart_eyes:\n\nUI of ourchatbot gets its power from [Streamlit](https://github.com/streamlit/streamlit). Thanks for their contribution to the open-source society. :heart_eyes:\n\nA list of medicines information has been collected from [Drugs.com](https://www.drugs.com/) and [MedlinePlus](https://medlineplus.gov/druginformation.html). We're appreciating their teams for producing great medicines information. :hugs:\n\nA list of lab tests information has been collected from [Lab tests online](https://labtestsonline.org/) and [MedlinePlus](https://medlineplus.gov/lab-tests/). We're appreciating their teams for producing great lab tests information. :hugs:\n"
    },
    {
      "name": "docling-project/docling-haystack",
      "stars": 19,
      "img": "https://avatars.githubusercontent.com/u/188446108?s=40&v=4",
      "owner": "docling-project",
      "repo_name": "docling-haystack",
      "description": "Docling Haystack integration",
      "homepage": "https://ds4sd.github.io/docling/integrations/haystack/",
      "language": "Python",
      "created_at": "2024-12-13T10:40:08Z",
      "updated_at": "2025-04-07T10:59:49Z",
      "topics": [],
      "readme": "# Haystack Docling integration\n\n[![PyPI version](https://img.shields.io/pypi/v/docling-haystack)](https://pypi.org/project/docling-haystack/)\n[![PyPI - Python Version](https://img.shields.io/pypi/pyversions/docling-haystack)](https://pypi.org/project/docling-haystack/)\n[![Poetry](https://img.shields.io/endpoint?url=https://python-poetry.org/badge/v0.json)](https://python-poetry.org/)\n[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)\n[![Imports: isort](https://img.shields.io/badge/%20imports-isort-%231674b1?style=flat&labelColor=ef8336)](https://pycqa.github.io/isort/)\n[![Pydantic v2](https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/pydantic/pydantic/main/docs/badge/v2.json)](https://pydantic.dev)\n[![pre-commit](https://img.shields.io/badge/pre--commit-enabled-brightgreen?logo=pre-commit&logoColor=white)](https://github.com/pre-commit/pre-commit)\n[![License MIT](https://img.shields.io/github/license/DS4SD/docling)](https://opensource.org/licenses/MIT)\n\nA [Docling](https://github.com/DS4SD/docling) integration for\n[Haystack](https://github.com/deepset-ai/haystack/).\n\n## Installation\n\nSimply install `docling-haystack` from your package manager, e.g. pip:\n```bash\npip install docling-haystack\n```\n\n## Usage\n\n### Basic usage\n\nBasic usage of `DoclingConverter` looks as follows:\n\n```python\nfrom haystack import Pipeline\nfrom docling_haystack.converter import DoclingConverter\n\nidx_pipe = Pipeline()\n# ...\nconverter = DoclingConverter()\nidx_pipe.add_component(\"converter\", converter)\n# ...\n```\n### Advanced usage\n\nWhen initializing a `DoclingConverter`, you can use the following parameters:\n\n- `converter` (optional): any specific Docling `DocumentConverter` instance to use\n- `convert_kwargs` (optional): any specific kwargs for conversion execution\n- `export_type` (optional): export mode to use: `ExportType.DOC_CHUNKS` (default) or\n    `ExportType.MARKDOWN`\n- `md_export_kwargs` (optional): any specific Markdown export kwargs (for Markdown mode)\n- `chunker` (optional): any specific Docling chunker instance to use (for doc-chunk\n    mode)\n- `meta_extractor` (optional): any specific metadata extractor to use\n\n### Example\n\nFor an end-to-end usage example, check out\n[this notebook](https://ds4sd.github.io/docling/examples/rag_haystack/).\n"
    },
    {
      "name": "waifu-lab/anyknowage",
      "stars": 19,
      "img": "https://avatars.githubusercontent.com/u/167647828?s=40&v=4",
      "owner": "waifu-lab",
      "repo_name": "anyknowage",
      "description": "Your personal  AI 「KEEP」, support docx, pdf, audio, video... ",
      "homepage": "",
      "language": "Svelte",
      "created_at": "2024-04-23T06:05:43Z",
      "updated_at": "2024-09-05T01:02:03Z",
      "topics": [
        "llm",
        "openai",
        "rag"
      ],
      "readme": "![](./images/AnyKnowledge-title.png)\r\n\r\n# AnyKnowledge\r\n\r\n![APP Build](https://github.com/waifu-lab/anyknowage/actions/workflows/build_app.yml/badge.svg)\r\n![Docker Build](https://github.com/waifu-lab/anyknowage/actions/workflows/build_docker.yml/badge.svg)\r\n\r\n <!-- [docs](https://www.google.com) -->\r\n \r\nhttps://github.com/waifu-lab/anyknowage/assets/55632143/982e11a6-8cb2-4933-831b-7308c7863f28\r\n\r\n## What is AnyKnowledge\r\n\r\nYour personal AI 「KEEP」\r\n\r\nstore text, markdown, word, video, audio\r\n\r\nAnyKnowledge uses AI to understand your stuff.\r\n\r\n## Deployment\r\n\r\n-   step 0: download client\r\n\r\n    https://github.com/waifu-lab/anyknowage/releases\r\n\r\n-   step 1: clone this repository\r\n\r\n```bash\r\ngit clone https://github.com/waifu-lab/anyknowage.git\r\ncd anyknowage\r\n```\r\n\r\n-   step 2: Run server\r\n\r\n```bash\r\ndocker compose pull\r\ndocker compose up\r\n```\r\n\r\n-   step 3: OK\r\n"
    },
    {
      "name": "EikeKohl/paperqa-web-app",
      "stars": 18,
      "img": "https://avatars.githubusercontent.com/u/57795397?s=40&v=4",
      "owner": "EikeKohl",
      "repo_name": "paperqa-web-app",
      "description": "This project showcases a Streamlit web app that utilizes a Haystack Question Answering Pipeline for Arxiv papers with OpenAI, demonstrating information retrieval from a document database and easy deployment on AWS with authentication, and a custom domain.",
      "homepage": "",
      "language": "HCL",
      "created_at": "2023-06-03T08:54:52Z",
      "updated_at": "2025-04-09T10:25:28Z",
      "topics": [],
      "readme": "# PaperQA\n\nThis is an exemplary project for a Streamlit web app that runs a Haystack Question Answering Pipeline for Arxiv papers\nusing OpenAI. The purpose of this app is to demonstrate how information retrieval based on a defined document database\ncan be done. Additionally, it provides an example of how a Dockerized application can be easily deployed on AWS,\nincluding authentication and a custom domain using Terraform. For business inquiries, please send an email\nto [ekohlmeyer21@gmail.com](mailto:ekohlmeyer21@gmail.com).\n\n![example](src/example.png)\n\n## How it works\n\n### Initial Settings\n\nThe configuration file can be found under `src/config.yaml`. The available settings can be found\nin `paperqa/settings.py`.\n\n### Add Papers to Database\n\nTo add Arxiv papers to your database, please insert a comma-separated string of PDF URLs into the upper text area. Once\nyou click on the \"Add papers to database\" button, a FAISS vector database will be created using the specified embedding\nmodel.\n\n### Reset Database\n\nTo reset your database, simply click on \"Reset database\". This will remove all your downloaded papers as well as the\nFAISS database.\n\n### Query Paper\n\nType your question into the text area above the \"Query paper\" button. Once you click the button, matching context will\nbe retrieved from your vector database, and your question will be answered based on the retrieved context.\n\n### Adjust Settings\n\nFeel free to adjust the following settings in the left side bar:\n\n* `Temperature`: How deterministic should the model's answer be? (Low temperature = more deterministic, high temperature\n  = less deterministic)\n* `Max Tokens Answer`: How many tokens should the model generate in your answer? (Keep in mind the max tokens\n  limitations of the model used)\n* `Number of Context Matches`: How many context matches should be used to answer your question? (Keep in mind the max\n  tokens limitations of the model used)\n\nYou also have the option to skip through the context to see what the model's answer is referring to. The score shows the\nsimilarity between the question and the retrieved context information. The higher the score, the better the match.\n\n## Build & Run the Application\n\nTo build the image, run the following command:\n\n```bash\ndocker build -t paperqa .\n```\n\nFor running the container locally, you will need to set some environment variables. The deployment of the app in AWS ECS\nhandles the environment independently.\n\n### Local Deployment\n\n#### Linux / MacOS:\n\n```bash\ndocker run -p 8501:8501 -it \\\n-e OPENAI_API_KEY=$OPENAI_API_KEY \\\npaperqa \n```\n\n#### Windows:\n\n```shell\ndocker run -p 8501:8501 -it `\n-e OPENAI_API_KEY=$env:OPENAI_API_KEY `\npaperqa \n```\n\nThe app is available at http://localhost:8501/\n\n### AWS Cloud Deployment\n\nFor the deployment on AWS, I added Infrastructure As Code (IAC) using terraform. I decided to include this in the scope\nof this project, because I found myself looking for information on such a deployment on the internet and did not find a\ncomprehensive guide. The solution provided should run as is after following the steps mentioned below. The deployment\nincludes\n\n* **Networking**\n* **Application Load Balancing**\n* **AWS Cognito Authentication**\n* **Route53 Domain Management**\n* **Cloudwatch Logging**\n* **ECS Containerized Deployment**\n\nHere is a short overview of the architecture (\nSee [here](https://aws.amazon.com/de/blogs/containers/securing-amazon-elastic-container-service-applications-using-application-load-balancer-and-amazon-cognito/\n) for more details):\n\n![img.png](src/architecture.png)\n\n#### Step-by-Step Guide\n\n1. Register your domain in\n   Route53 ([Documentation](https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/domain-register.html))\n2. Request a SSL certificate for your application's domain in AWS Certificate\n   Manager ([Documentation](https://docs.aws.amazon.com/acm/latest/userguide/gs-acm-request-public.html))\n3. Validate your SSL certificate request\n4. Create a secret called `paperqa_openai_api_key` with the OpenAI api key to use in the application in the AWS Secret\n   Manager ([Documentation](https://docs.aws.amazon.com/secretsmanager/latest/userguide/create_secret.html))\n5. Set all variables in `terraform/vars.tf`\n6. Run`terraform apply` in the `terraform` directory and confirm with \"yes\"\n7. Run `push_to_ecr.sh` to build your container and push it to ECR (Don't forget to adjust the `DOCKER_REGISTRY`\n   and `IMAGE_VERSION`)\n8. To update your ECS Service, run `update_ecs.sh`\n9. Add a user to your Cognito user pool and remember to check the \"Send an email invitation\" check box\n   ([Documentation](https://docs.aws.amazon.com/cognito/latest/developerguide/managing-users.html))\n\n## TODO\n\nTo improve the quality of the answers based on the retrieved information, the following adjustments can be made:\n\n* Filter for files/papers in the document database to include in the information retrieval.\n* Set a minimum context similarity score to ensure high-quality retrieval results.\n* Output a model probability to get a sense of how deterministic the answer is.\n* Perform sanity checks to ensure the generated text is verbatim in the documents and was not hallucinated."
    },
    {
      "name": "CogniQ/CogniQ",
      "stars": 18,
      "img": "https://avatars.githubusercontent.com/u/132189118?s=40&v=4",
      "owner": "CogniQ",
      "repo_name": "CogniQ",
      "description": "A chatbot that blends information from multiple data sources. Currently configured for Slack, ChatGPT, and Bing Search. Try a live demo in the community slack at the link below.",
      "homepage": "https://www.cogniq.info/join-slack",
      "language": "Python",
      "created_at": "2023-05-02T17:54:46Z",
      "updated_at": "2023-12-06T21:21:33Z",
      "topics": [
        "artificial-intelligence",
        "python",
        "slack"
      ],
      "readme": "# CogniQ\n\nThis project is under active development. Your experience will be buggy until a release has been cut.\nNaturally at this stage, there are no guarantees about stability.\n\nIn short, the current feature set is that you can have conversations that will be composed of responses from Bing Search, and Chat GPT. \n\n\n# Demo\n\nJoin the community slack channel to interact with @CogniQ, the demo bot deployed from this repository's `main` branch. \n\nTo use the Slack History personality (you'll get pestered otherwise), install the app once you've joined. \n\nThe above, but in *checklist format* (🎆):\n- [ ] Join the [community slack channel](https://www.cogniq.info/join-slack)\n- [ ] [Give the app permission](https://main.cogniq.info/slack/install) to perform slack searches as you\n\n# Usage\n\nThe primary interface by which to use CogniQ is as a slack bot. You may direct message @CogniQ directly, or you may invoke it in a thread.\n\n@CogniQ wil always start a new thread if not in a thread already. This is so that the memory of the conversation is segmented correctly.\n\n## In a channel or thread, mention @CogniQ\nIn Slack, have a conversation with CogniQ and ask it to do something by mentioning it in a thread or channel. For example:\n\n<img src=\"https://user-images.githubusercontent.com/176915/235838387-9befa803-1179-42a4-8127-8ee1ad518c73.png\" alt=\"@CogniQ What is the Picard Maneuver?\" width=\"300\">\n\nWhen starting a new conversation outside of a thread, @CogniQ will be initialized with the last 10 or so messages from the channel. This history may include messages in recent threads.\n\nWhen starting a new conversation inside of a thread, @CogniQ will be initialized with the last 10 or so messages from the thread.\n\nCogniQ will respond with the answer to your question.\n\n## In a direct message, just ask your question.\n\nYou can also message CogniQ directly. You do not have to mention it when you message it directly.\n\n<img src=\"https://user-images.githubusercontent.com/176915/243065469-d6cc3f54-198f-411e-b280-e6aab922a4ef.png\" alt=\"What is the Riker Maneuver?\" width=\"450\">\n\n\n## CogniQ will stream its thinking before answering\n\n\n*Each personality comes up with an answer*  \n<img width=\"547\" alt=\"image\" src=\"https://github.com/CogniQ/CogniQ/assets/176915/02cdef2d-6cce-4268-831a-d123d3d9a2f0\">  \n\n\n*The evaluator will compose a final answer.*  \n<img width=\"527\" alt=\"image\" src=\"https://github.com/CogniQ/CogniQ/assets/176915/ed7b95ed-162f-4149-878f-10d45728d595\">  \n\n\n# CogniQ has multiple personalities\n\n## CogniQ will ask Chat GPT for an answer, and also search the web using Bing.\n\nCogniQ will incorporate the top three search results into its answer and will provide links to any relevant sources.\n\nIt gets its answer by performing a straight query to ChatGPT and then using an agent to draw context from Bing Search. \nThe set of responses are then combined to produce a search augmented response.\n\nIf you don't have access to one of the API keys required for the personality, then modify `multiple_personalities.py` to remove the personality or personalities.\n\nThe idea is that one could add an arbitrary number of personalities to CogniQ.\n\nIn that sense, `multiple_personalities.py` and `main.py` are really just example files for how to use the library. You can use them as a starting point for your own bot.\n\n\n# Deployment\n\n## Running locally\n\n- [ ] Install the prerequisites (see details in the [development](#development) section)\n\n```bash\n\n# If you haven't already, initialize a virtualenv: \npython -m venv .venv\n\n# Install the dependencies:  \nmake deps\nsource .venv/bin/activate\n\n# Copy the example .env file to .env\ncp .env.example .env\n\n# Edit the .env file to add your API keys\necho \"edit .env to add your API keys\"\n\n\n# Run the app\npython main.py\n```\n\n## Running in a Docker container\n\n```\ndocker run --env_file .env ghcr.io/cogniq/cogniq:main\n```\n\n## Deploying to Azure Container Instances\n\nSee the workflow in `.github/workflows/_deploy.yml`. \n\n> The workflow is used to deploy the bot to the CogniQ Community Slack ([please join!](https://www.cogniq.info/join-slack)).\n\n# Development\n\n## 1. Prerequisites\n\nBefore you begin, make sure to have the following prerequisites in place:\n\n### Install CUDA if on Linux\n\nTo set up the development environment on Linux, follow these steps:\n\n1. Install the CUDA Toolkit 11. You can find the [installation instructions](https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html) on the NVIDIA website.\n   1. The files should be installed to /usr/local/cuda-11.8/\n   2. `export LD_LIBRARY_PATH=/usr/local/cuda-11.8/lib64:/usr/local/cuda-11.8/extras/CUPTI/lib64`\n2. Install cuDNN\n   1. Install the [cuDNN for CUDA 11 repo](https://developer.nvidia.com/rdp/cudnn-download)\n   ```\n   sudo dpkg -i /mnt/c/Users/myusername/Downloads/cudnn-local-repo-ubuntu2004-8.9.1.23_1.0-1_amd64.deb\n   ```\n   1. After the repo is installed, then install the package\n   ```\n   apt install libcudnn8\n   ```\n\n3. Manually uncomment the pytorch source, and uncomment the torch dependency line for 2.0.1+cu118\n\n4. Install the dependencies and run the app:\n   ```\n   python -m venv .venv\n   source .venv/bin/activate\n   make deps\n   python main.py\n   ```\n\n## No need for CUDA on OSX environments\n\n```\npython -m venv .venv\nsource .venv/bin/activate\nmake deps\npython main.py\n```\n\n### Visual Studio Code Dev Containers (Optional)\n#### Docker Desktop \n\n1. **Install Docker Desktop**: Docker is used for creating isolated environments called containers. To install Docker Desktop, follow the instructions given in the official Docker documentation.\n\n   - For Windows: [Install Docker Desktop on Windows](https://docs.docker.com/docker-for-windows/install/)\n   - For Mac: [Install Docker Desktop on Mac](https://docs.docker.com/docker-for-mac/install/)\n   - For Linux: Docker Desktop is not available, use Docker engine instead. [Install Docker Engine on Ubuntu](https://docs.docker.com/engine/install/ubuntu/)\n\n2. **Verify Docker Desktop Installation**: After installing Docker Desktop, you can verify that it's installed correctly by opening a new terminal window and typing `docker --version`. You should see a message with your installed Docker version.\n\n#### Install Visual Studio Code\n1. **Install Visual Studio Code**: VS Code is a code editor with support for development containers. To install VS Code, follow the instructions in the [VS Code Documentation](https://code.visualstudio.com/docs/setup/setup-overview).\n\n2. **Install Remote - Containers Extension**: This extension lets you use a Docker container as a full-featured development environment. To install the extension, follow the instructions in the [VS Code Documentation](https://code.visualstudio.com/docs/remote/containers#_installation).\n\n\n## 2. Setup API Keys\n\n### Get Bing Search API Keys\n\nTo set up Bing Search API keys, follow these steps:\n\n1. Sign in to your Microsoft account or create a new one at https://portal.azure.com/.\n\n2. Click on \"Create a resource\" in the top-left corner of the Azure portal.\n\n3. In the search box, type \"Bing Search v7\" and select it from the list of results.\n\n4. Click the \"Create\" button to begin the process of setting up the Bing Search API.\n\n5. Fill out the required information:\n\n   - Choose a subscription: Select your Azure subscription.\n   - Choose a resource group: Create a new resource group or select an existing one.\n   - Give your resource a name: Choose a unique name for your Bing Search resource.\n   - Choose a pricing tier: Select the appropriate pricing tier based on your needs. Note that there's a free tier available with limited capabilities.\n\n6. Click the \"Review + create\" button and review your settings. If everything is correct, click \"Create\" to deploy the Bing Search resource.\n\n7. After the deployment is complete, go to your new Bing Search resource in the Azure portal.\n\n8. Click on \"Keys and Endpoint\" in the left-hand menu. Here, you'll find your Bing Search API keys.\n\n9. Copy one of the API keys and add it to the `.env` file\n\n   ```\n   BING_SUBSCRIPTION_KEY=<your_bing_search_api_key>\n   ```\n\n\n### Get OpenAI API Keys\n\nTo set up OpenAI API keys, follow these steps:\n\n1. Go to the OpenAI website at <https://platform.openai.com/signup/> and sign up for an account if you don't have one already.\n\n2. Once you have an account, sign in and go to the API Keys section in your OpenAI Dashboard: <https://platform.openai.com/account/api-keys>\n\n3. Click on the \"Create an API key\" button.\n\n4. Give your API key a name, and optionally add a description. This can be helpful if you want to track the usage of different keys for various projects or environments.\n\n5. Click on the \"Create\" button, and the new API key will be generated.\n\n6. Copy the generated API key to your clipboard.\n\n7. Add the OpenAI API key to the `.env` file\n   ```\n   OPENAI_API_KEY=<your_openai_api_key>\n   ```\n\n\n### Get Anthropic Claude API Keys\nTo set up Anthropic Claude API keys, follow these steps:\n1. Go to the Anthropic website at https://www.anthropic.com/product and request access to Claude.\n2. Once you have an account, sign in and go to the API Keys section: https://console.anthropic.com/account/keys\n3. Click on the \"Create Key\" button.\n4. Copy the generated API key to your clipboard.\n5. Add the Anthropic Claude API key to the `.env` file\n\n   ```\n   ANTHROPIC_API_KEY=<your_anthropic_api_key>\n   ```\n\n### Deploy to Slack\n\n1. Go to https://api.slack.com/apps and click \"Create New App.\"\n2. Choose \"From an app manifest\" and select your workspace.\n3. Copy the app manifest from `deployments/example/slack_manifest.json`. Later, you will modify it with your public URL. But for dev, the example is fine.\n4. Modify to taste, and paste the app manifest into the text box and click \"Next.\"\n5. Click \"Create\" to finish creating the app.\n6. After your app has been created, navigate to the \"Basic Information\" page from the side menu.\n7. Under the \"App Credentials\" section, find the \"App Token\" field. This is your `SLACK_APP_TOKEN`. Copy it.\n8. Add the `SLACK_APP_TOKEN` to the `.env` file.\n9. Still on the \"Basic Information\" page, locate the \"Signing Secret\" under the \"App Credentials\" section. This is your `SLACK_SIGNING_SECRET`.\n10. Add the `SLACK_SIGNING_SECRET` to the `.env` file.\n11. Now, navigate to the \"OAuth & Permissions\" page in the side menu.\n12. At the top of the \"OAuth & Permissions\" page, click \"Install App to Workspace.\" This will generate your `SLACK_BOT_TOKEN`.\n13. After the installation process, you will be redirected to the \"OAuth & Permissions\" page again. Here, under the \"Tokens for Your Workspace\" section, you will find the \"Bot User OAuth Token\". This is your `SLACK_BOT_TOKEN`.\n16. Add the `SLACK_BOT_TOKEN` to the `.env` file.\n\n> ⚠️ **Remember to never commit your `.env` file to the repository as it contains sensitive information.** ⚠️\n\n\n## 3a. Run the app locally\n\n1. Once you have all of the API keys in the .env file, you can run the app locally.\n7. Run the app: `python main.py`\n8. Restart the app whenever you make changes to the code.\n\n## 3b. Run the app in Dev Container (Optional)\n\n1. With the repository open in VS Code, press F1 and select the \"Remote-Containers: Reopen in Container\" command. VS Code will start building the Docker container based on the specifications in the `.devcontainer/devcontainer.json` file in the repository. This may take some time when run for the first time.\n2. Assuming that the .env file is setup correctly, you can run the app in the dev container.\n3. Run the app: `python main.py`\n4. Restart the app whenever you make changes to the code.\n\n# Notes\n\n## Why not langchain?\n\n### TL;DR\n\nBy foregoing the framework, I do the upfront work of learning the various APIs, but retain the freedom to deviate from the rails. I do want to [enable modularity](https://github.com/CogniQ/CogniQ/issues/7), but I wish to balance the extent to which the project requires learning the language of the project.\n\n### Rant\nI started writing this using [langchain](https://docs.langchain.com/docs/). The project is awesome for quickly trying out a concept. For scripting, its an absolute win. I'm just not smart enough to understand how to use it, and in particular, extend it to suit my needs. Instead, what I offer is less of a framework, and more of an attempt at a starter template.\n\nIn general, I find that when one uses frameworks (or client libraries for that matter) for stitching together various APIs, one ostensibly substitues learning the intricacies of the API for learning the intricaces of the framework.\nThese intricacies tend to become more convoluted the more various similar but different services are mashed together.\nThe convolution happens as a byproduct of [entangling disparate systems](https://www.youtube.com/watch?v=SxdOUGdseq4) not only vertically in the integration, but horizontally across categories of similarity.\nThe result of this is that the first steps in using the framework are blissfully easy, the stuff of demos.\nBut as soon as one steps outsides the limits of the framework, the developer finds they must study both the framework and the API.\nProtocols like REST and GraphQL already provide a common and robust interface enough, I think.\n\n/rant\n\n## Why Haystack?\n\n### TL;DR\n\nI was wrong. Haystack is a) rapidly being developed, and b) comprehensive in its approach to the problem domain. I'm still not sure I'll use it in production, but I'm happy to use it for development.\n\n### Not really a rant\n\nI know I seem like a hypocrite to write what I wrote about langchain, then turn around and use haystack. Indeed, the same problems of using frameworks continue to exist. I have to study both the framework and the API.\n\nBut, [haystack](https://haystack.deepset.ai/) is a solid framework, and I have a day job.\n\nI still think a serious at-scale deployment will likely demand a purpose-built rewrite, but for me and right now there's more reason than not to use this framework."
    },
    {
      "name": "prosto/ray-haystack",
      "stars": 18,
      "img": "https://avatars.githubusercontent.com/u/706952?s=40&v=4",
      "owner": "prosto",
      "repo_name": "ray-haystack",
      "description": "Run Haystack Pipelines on Ray",
      "homepage": "",
      "language": "Python",
      "created_at": "2024-09-08T11:58:12Z",
      "updated_at": "2025-03-03T21:24:51Z",
      "topics": [],
      "readme": "<h1 align=\"center\">ray-haystack</h1>\n\n<p align=\"center\">Run <a href=\"https://docs.haystack.deepset.ai/docs/intro\"><i>Haystack Pipelines</i></a> on <a href=\"https://docs.ray.io/en/latest/ray-overview/getting-started.html\"><i>Ray</i></a>.</p>\n\n<p align=\"center\">\n  <a href=\"https://github.com/prosto/ray-haystack/actions?query=workflow%3Aci\">\n    <img alt=\"ci\" src=\"https://github.com/prosto/ray-haystack/workflows/ci/badge.svg\" />\n  </a>\n  <a href=\"https://pypi.org/project/ray-haystack/\">\n    <img alt=\"pypi version\" src=\"https://img.shields.io/pypi/v/ray-haystack.svg\" />\n  </a>\n  <a href=\"https://img.shields.io/pypi/pyversions/ray-haystack.svg\">\n    <img alt=\"python version\" src=\"https://img.shields.io/pypi/pyversions/ray-haystack.svg\" />\n  </a>\n  <a href=\"https://pypi.org/project/haystack-ai/\">\n    <img alt=\"haystack version\" src=\"https://img.shields.io/pypi/v/haystack-ai.svg?label=haystack\" />\n  </a>\n</p>\n\n---\n\n- [Overview](#overview)\n- [Installation](#installation)\n- [Usage](#usage)\n  - [Start with an example](#start-with-an-example)\n  - [Read pipeline events](#read-pipeline-events)\n  - [Component Serialization](#component-serialization)\n  - [DocumentStore with Ray](#documentstore-with-ray)\n  - [RayPipeline Settings](#raypipeline-settings)\n  - [Middleware](#middleware)\n- [More Examples](#more-examples)\n  - [Trace Ray Pipeline execution in Browser](#trace-ray-pipeline-execution-in-browser)\n  - [Ray Pipeline on Kubernetes](#ray-pipeline-on-kubernetes)\n  - [Ray Pipeline with detached components](#ray-pipeline-with-detached-components)\n- [Next Steps \\& Enhancements](#next-steps--enhancements)\n- [Acknowledgments](#acknowledgments)\n\n## Overview\n\n`ray-haystack` is a python package which allows running [Haystack pipelines](https://docs.haystack.deepset.ai/docs/pipelines) on [Ray](https://docs.ray.io/en/latest/ray-overview/index.html)\nin distributed manner. The package provides same API to build and run Haystack pipelines but under the hood components are being distributed to remote nodes for execution using Ray primitives.\nSpecifically [Ray Actor](https://docs.ray.io/en/latest/ray-core/actors.html) is created for each component in a pipeline to `run` its logic.\n\nThe purpose of this library is to showcase the ability to run Haystack in a distributed setup with Ray featuring its options to configure the payload, e.g:\n\n- Control with [resources](https://docs.ray.io/en/latest/ray-core/scheduling/resources.html) how much CPU/GPU is needed for a component to run (per each component if needed)\n- Manage [environment dependencies](https://docs.ray.io/en/latest/ray-core/handling-dependencies.html) for components to run on dedicated machines.\n- Run pipeline on Kubernetes using [KubeRay](https://docs.ray.io/en/latest/cluster/kubernetes/getting-started.html)\n\nMost of the times you will run Haystack pipelines on your local environment, even in production you will want to run pipeline on a single node in case the goal is to quickly return response to the user without overhead you would usually get with distributed setup. However in case of long running and complex RAG pipelines distributed way might help:\n\n- Not every component needs GPU, most will use some external API calls. With Ray it should be possible to assign respective resource requirements (CPU, RAM) per component execution needs.\n- Some components might take longer to run, so ideally if there should be an option to parallelize component execution it should decrease pipeline run time.\n- With asynchronous execution it should be possible to interact with different component execution stages (e.g. fire an event before and after component starts).\n\n`ray-haystack` provides a custom implementation for pipeline execution logic with the goal to stay **as complaint as possible with native Haystack implementation**.\nYou should expect in most cases same results (outputs) from pipeline runs. On top of that the package will parallelize component runs where possible.\nComponents with no active dependencies can be scheduled without waiting for currently running components.\n\n<p align=\"center\">\n    <img width=\"500\" height=\"400\" src=\"https://raw.githubusercontent.com/prosto/ray-haystack/main/docs/pipeline-watch-anime.gif\">\n</p>\n\n## Installation\n\n`ray-haystack` can be installed as any other Python library, using pip:\n\n```shell\npip install ray-haystack\n```\n\nThe package should work with python version 3.8 and onwards. If you plan to use `ray-haystack` with an existing Ray cluster make sure you align python and `ray` versions with those running in the cluster.\n\n> **Note**\n> The `ray-haystack` package will install both `haystack-ai` and `ray` as transitive dependencies. The minimum supported version of haystack is `2.6.0`. [`mergedeep`](https://pypi.org/project/mergedeep/) is also used internally to merge pipeline settings.\n\nIf you would like to see [Ray dashboard](https://docs.ray.io/en/latest/ray-observability/getting-started.html) when starting Ray cluster locally install Ray as follows:\n\n```shell\npip install -U \"ray[default]\"\npip install ray-haystack\n```\n\nWhile pipeline is running locally access the dashboard in browser at [http://localhost:8265](http://localhost:8265).\n\n## Usage\n\n### Start with an example\n\nOnce `ray-haystack` is installed lets demonstrate how it works by running a simple example.\n\nWe will build a pipeline that fetches RSS news headlines from the list of given urls, converts each headline to a `Document` with content equal to the title of the headline. We then asks LLM (`OpenAIGenerator`) to create news summary from the list of converted Documents and given prompt `template`.\n\n```python\nimport io\nimport os\nfrom typing import List, Optional\nfrom xml.etree.ElementTree import parse as parse_xml\n\nimport ray # Import ray\nfrom haystack import Document, component\nfrom haystack.components.builders import PromptBuilder\nfrom haystack.components.fetchers import LinkContentFetcher\nfrom haystack.components.generators import OpenAIGenerator\nfrom haystack.components.joiners import DocumentJoiner\nfrom haystack.dataclasses import ByteStream\n\nfrom ray_haystack import RayPipeline # Import RayPipeline (instead of `from haystack import Pipeline`)\n\n# Please introduce your OpenAI Key here\nos.environ[\"OPENAI_API_KEY\"] = \"You OpenAI Key\"\n\n@component\nclass XmlConverter:\n    \"\"\"\n    Custom component which parses given RSS feed (from ByteStream) and extracts values by a\n    given XPath, e.g. \".//channel/item/title\" will find \"title\" for each RSS feed item.\n    A Document is created for each extracted title. The `category` attribute can be used as\n    an additional metadata field.\n    \"\"\"\n\n    def __init__(self, xpath: str = \".//channel/item/title\", category: Optional[str] = None):\n        self.xpath = xpath\n        self.category = category\n\n    @component.output_types(documents=List[Document])\n    def run(self, sources: List[ByteStream]):\n        documents: List[Document] = []\n        for source in sources:\n            xml_content = io.StringIO(source.to_string())\n            documents.extend(\n                Document(content=elem.text, meta={\"category\": self.category})\n                for elem in parse_xml(xml_content).findall(self.xpath)  # noqa: S314\n                if elem.text\n            )\n        return {\"documents\": documents}\n\ntemplate = \"\"\"\nGiven news headlines below provide a summary of what is happening in the world right now in a couple of sentences.\nYou will be given headline titles in the following format: \"<headline category>: <headline title>\".\nWhen creating summary pay attention to common news headlines as those could be most insightful.\n\nHEADLINES:\n{% for document in documents %}\n    {{ document.meta[\"category\"] }}: {{ document.content }}\n{% endfor %}\n\nSUMMARY:\n\"\"\"\n\n# Create instance of Ray pipeline\npipeline = RayPipeline()\n\npipeline.add_component(\"tech-news-fetcher\", LinkContentFetcher())\npipeline.add_component(\"business-news-fetcher\", LinkContentFetcher())\npipeline.add_component(\"politics-news-fetcher\", LinkContentFetcher())\npipeline.add_component(\"tech-xml-converter\", XmlConverter(category=\"tech\"))\npipeline.add_component(\"business-xml-converter\", XmlConverter(category=\"business\"))\npipeline.add_component(\"politics-xml-converter\", XmlConverter(category=\"politics\"))\npipeline.add_component(\"document_joiner\", DocumentJoiner(sort_by_score=False))\npipeline.add_component(\"prompt_builder\", PromptBuilder(template=template))\npipeline.add_component(\"generator\", OpenAIGenerator())  # \"gpt-4o-mini\" is the default model\n\npipeline.connect(\"tech-news-fetcher\", \"tech-xml-converter.sources\")\npipeline.connect(\"business-news-fetcher\", \"business-xml-converter.sources\")\npipeline.connect(\"politics-news-fetcher\", \"politics-xml-converter.sources\")\npipeline.connect(\"tech-xml-converter\", \"document_joiner\")\npipeline.connect(\"business-xml-converter\", \"document_joiner\")\npipeline.connect(\"politics-xml-converter\", \"document_joiner\")\npipeline.connect(\"document_joiner\", \"prompt_builder\")\npipeline.connect(\"prompt_builder\", \"generator.prompt\")\n\n# Draw pipeline and save it to `pipe.png`\n# pipeline.draw(\"pipe.png\")\n\n# Start local Ray cluster\nray.init()\n\n# Prepare pipeline inputs by specifying RSS urls for each fetcher\npipeline_inputs = {\n    \"tech-news-fetcher\": {\n        \"urls\": [\n            \"https://www.theverge.com/rss/frontpage/\",\n            \"https://techcrunch.com/feed\",\n            \"https://cnet.com/rss/news\",\n            \"https://wired.com/feed/rss\",\n        ]\n    },\n    \"business-news-fetcher\": {\n        \"urls\": [\n            \"https://search.cnbc.com/rs/search/combinedcms/view.xml?partnerId=wrss01&id=10001147\",\n            \"https://www.business-standard.com/rss/home_page_top_stories.rss\",\n            \"https://feeds.a.dj.com/rss/WSJcomUSBusiness.xml\",\n        ]\n    },\n    \"politics-news-fetcher\": {\n        \"urls\": [\n            \"https://search.cnbc.com/rs/search/combinedcms/view.xml?partnerId=wrss01&id=10000113\",\n            \"https://rss.nytimes.com/services/xml/rss/nyt/Politics.xml\",\n        ]\n    },\n}\n\n# Run pipeline with inputs\nresult = pipeline.run(pipeline_inputs)\n\n# Print response from LLM\nprint(\"RESULT: \", result[\"generator\"][\"replies\"][0])\n```\n\nCan you notice the difference between native Haystack pipelines? Lets try to spot some of them:\n\n- we import `ray` module\n- we import `RayPipeline` (from `ray_haystack`) instead of `Pipeline` class from `haystack`\n- before running the pipeline we start [local ray cluster](https://docs.ray.io/en/latest/ray-core/starting-ray.html#start-ray-init) with explicit `ray.init()` call (btw its not necessary as Ray `init` will automatically be called on the first use of a Ray remote API.)\n\nIf you change `RayPipeline` to native `Pipeline` implementation from Haystack you should get same results.\n\nWhat happens under the hood? Is there a difference? Well, yes, lets summarize it in the following diagram:\n\n![rss feed pipeline diagram](https://raw.githubusercontent.com/prosto/ray-haystack/main/docs/rss_feed_pipeline_diagram.png)\n\n1. `RayPipeline` is started (same as native Haystack) with `pipeline.run` call\n2. `RayPipelineManager` class is responsible for creating [actors](https://docs.ray.io/en/latest/ray-core/actors.html) per each component in the pipeline. It also maintains a graph representation of the pipeline (same way as native Haystack pipeline does internally)\n3. Component actors are created with configurable options:\n   - Each actor can be a different process on same machine or a remote node (e.g. pod in kubernetes)\n   - Component is serialized using `to_dict` before traveling through network/process boundaries\n   - When actor is created component is instantiated (de-serialized) with `from_dict`\n   - Each actor can be configured with [options](https://docs.ray.io/en/latest/ray-core/api/doc/ray.actor.ActorClass.options.html) if needed. For example lifetime of the actor can be controlled with options, by default when pipeline finishes actors are destroyed\n4. `RayPipelineProcessor` is the main module of the `ray-haystack` package and is responsible for traversing execution graph of the pipeline. It keeps track of what needs to be run next and stores outputs from each component until pipeline finishes its execution\n   - The processor is effectively workflow execution engine which respects rules of how Haystack pipelines should run. It does not reuse logic with native Haystack implementation because it allows parallelization where possible as well as pipeline events. For example in the diagram it is evident that fetcher components can start running at the same time (same for converters when connected fetcher finishes execution)\n   - Thee processor is itself a Ray Actor as it coordinates component execution logic asynchronously and keeps intermediate running state (e.g. component inputs/outputs)\n5. `RayPipelineProcessor` calls component remotely with prepared inputs in case it is ready to run (has enough inputs)\n   - in case component needs `warm_up`, the actor calls it once during its lifetime\n   - internally Ray serializes component inputs using Pickle before calling actor's remote `run` method, so the expectation will be that input parameters are serializable\n6. Once component actor finishes execution its outputs are stored in `RayPipelineProcessor` so that later on we could return results back to user\n7. Results are composed out of stored component outputs and returned back to `RayPipelineManager`. **Please notice at this point we are not waiting for all components to finish but rather return deferred results as `RayPipelineProcessor` continues its execution**\n8. `RayPipelineManager` prepares outputs which can be consumed by `RayPipeline`\n9. `RayPipeline` will wait (usually block) until `RayPipelineProcessor` has no components to run. The output dictionary will be returned to the user.\n\n### Read pipeline events\n\nIn some cases you would want to asynchronously react to particular pipeline execution points:\n\n- when pipeline starts\n- before component runs\n- after component finishes\n- after pipeline finishes\n\nInternally `RayPipelineManager` creates an instance of [Ray Queue](https://docs.ray.io/en/latest/ray-core/api/doc/ray.util.queue.Queue.html) where such events are being stored and consumed from.\n\nExcept standard `run` method `RayPipeline` provides a method called `run_nowait` which returns pipeline execution result without blocking current logic:\n\n```python\nresult = pipeline.run_nowait(pipeline_inputs)\n\n# A non-blocking call, `pipeline_output_ref` is a reference\nprint(\"Object Ref\", result.pipeline_output_ref)\n\n# Will block until pipeline finishes and returns outputs\nprint(\"Result\", ray.get(result.pipeline_output_ref))\n```\n\n`pipeline_output_ref` is a reference to pipeline outputs, see [Objects](https://docs.ray.io/en/latest/ray-core/objects.html) documentation for more details.\n\n> **Note**\n> Internally `run` calls `run_nowait` and uses `ray.get` to wait for pipeline to finish.\n\nApart from `pipeline_output_ref` there is another option to obtain results from pipeline execution but with much more details:\n\n```python\nresult = pipeline.run_nowait(pipeline_inputs)\n\nfor pipeline_event in result.pipeline_events_sync():\n    # For better viewing experience inputs/outputs are truncated\n    # but you can comment out/remove lines below (before `print`)\n    # if you would like to see full event data set\n    if pipeline_event.type == \"ray.haystack.pipeline-start\":\n        pipeline_event.data[\"pipeline_inputs\"] = \"{...}\"\n    if pipeline_event.type == \"ray.haystack.component-start\":\n        pipeline_event.data[\"input\"] = \"{...}\"\n    if pipeline_event.type == \"ray.haystack.component-end\":\n        pipeline_event.data[\"output\"] = \"{...}\"\n\n    print(\n        f\"\\n>>> [{pipeline_event.time}] Source: {pipeline_event.source} | Type: {pipeline_event.type} | Data={pipeline_event.data}\"\n    )\n```\n\nBelow is a sample output you should be able to see if you run the code above:\n\n```bash\n>>> [2024-10-09T22:16:27.073665+00:00] Source: ray-pipeline-processor | Type: ray.haystack.pipeline-start | Data={'pipeline_inputs': '{...}', 'runnable_nodes': ['business-news-fetcher', 'politics-news-fetcher', 'tech-news-fetcher']}\n\n>>> [2024-10-09T22:16:27.535254+00:00] Source: ray-pipeline-processor | Type: ray.haystack.component-start | Data={'name': 'business-news-fetcher', 'sender_name': None, 'input': '{...}', 'iteration': 0}\n\n>>> [2024-10-09T22:16:27.537959+00:00] Source: ray-pipeline-processor | Type: ray.haystack.component-start | Data={'name': 'politics-news-fetcher', 'sender_name': None, 'input': '{...}', 'iteration': 0}\n\n>>> [2024-10-09T22:16:27.540466+00:00] Source: ray-pipeline-processor | Type: ray.haystack.component-start | Data={'name': 'tech-news-fetcher', 'sender_name': None, 'input': '{...}', 'iteration': 0}\n\n>>> [2024-10-09T22:16:28.939877+00:00] Source: ray-pipeline-processor | Type: ray.haystack.component-end | Data={'name': 'politics-news-fetcher', 'output': '{...}', 'iteration': 0}\n\n>>> [2024-10-09T22:16:28.944781+00:00] Source: ray-pipeline-processor | Type: ray.haystack.component-start | Data={'name': 'politics-xml-converter', 'sender_name': 'politics-news-fetcher', 'input': '{...}', 'iteration': 0}\n```\n\n> **Note**\n> You can access directly the events queue and then use your own events listening logic.\n\n```python\nresult = pipeline.run_nowait(pipeline_inputs)\n\n# Wait for just one event and return\nresult.events_queue.get(block=True)\n```\n\nWith available [Queue methods](https://docs.ray.io/en/latest/ray-core/api/doc/ray.util.queue.Queue.html) you should be able to implement `async` processing logic (see `get_async` method). The [pipeline_watch example](https://github.com/prosto/ray-haystack/blob/main/examples/pipeline_watch/README.md) actually uses `async` to read events from the queue and deliver those to browser as soon as event is available (using Server Sent Events)\n\n### Component Serialization\n\nAs we saw earlier in the diagram when you run pipeline with `RayPipeline` each component gets serialized and then de-serialized when instantiated within an actor.\nIf you run native Haystack pipeline locally component remain in the same python process and there is no reason to care about distributed setup.\nHowever in order for component to become available across boundaries we should be able to create instance of component based on its definition - much like you would have a [saved pipeline definition](https://docs.haystack.deepset.ai/docs/serialization) and then send it to some backend service for invocation. Ray distributes payload and should be able to [serialize](https://docs.ray.io/en/latest/ray-core/objects/serialization.html) objects before they end up in remote task or actor.\n\n![component serialization](https://raw.githubusercontent.com/prosto/ray-haystack/refs/heads/main/docs/ray_component_serialization.png)\n\nWe could rely on default serialization behavior provided by Ray, but the trick is not every Haystack component (custom or provided) is going to be serializable with `pickle5 + cloudpickle` (e.g. document store connection etc). So we have to rely on what Haystack requires from each component as per protocol - `to_dict` and `from_dict` methods. **Any component that you intend to use with `RayPipeline` should have those methods defined and working as expected.**\n\nThere is another issue when it comes to component deserialization on a remote task or actor in Ray - whatever python package/module a component depends on should be available during component creation or invocation. This brings us to a relatively complex topic about [environment dependencies](https://docs.ray.io/en/latest/ray-core/handling-dependencies.html#concepts) in Ray. **You should read the documentation and plan accordingly before decide to deploy your pipeline to a fully fledged production setup.** In most of the cases you will be able to run and test pipelines with `RayPipeline` without noticing how Ray tries to bring in component's environment dependencies into remote actor. See [pipeline_kubernetes example](/examples/pipeline_kubernetes/README.md) for a simple demonstration of running pipeline on a pristine KubeRay cluster in kubernetes.\n\nLets see when serialization will become an issue by running the following example:\n\n```python\nimport io\nfrom typing import List\nfrom xml.etree.ElementTree import parse as parse_xml\n\nimport ray\nfrom haystack import Document\nfrom haystack.components.converters import OutputAdapter\nfrom haystack.components.fetchers import LinkContentFetcher\nfrom haystack.dataclasses import ByteStream\n\nfrom ray_haystack import RayPipeline\nfrom ray_haystack.serialization import worker_asset\n\n\n# Uncomment to fix the serialization issue\n# @worker_asset\ndef parse_sources(sources: List[ByteStream]) -> List[Document]:\n    documents: List[Document] = []\n    for source in sources:\n        xml_content = io.StringIO(source.to_string())\n        documents.extend(\n            Document(content=elem.text)\n            for elem in parse_xml(xml_content).findall(\".//channel/item/title\")  # noqa: S314\n            if elem.text\n        )\n    return documents\n\n\npipeline = RayPipeline()\n\npipeline.add_component(\"tech-news-fetcher\", LinkContentFetcher())\npipeline.add_component(\n    \"adapter\",\n    OutputAdapter(\n        template=\"{{ sources | parse_sources }}\",\n        output_type=List[Document],\n        custom_filters={\"parse_sources\": parse_sources},\n    ),\n)\n\npipeline.connect(\"tech-news-fetcher\", \"adapter.sources\")\n\npipeline.draw(\"pipe.png\")\n\npipeline_inputs = {\n    \"tech-news-fetcher\": {\n        \"urls\": [\n            \"https://techcrunch.com/feed\",\n            \"https://cnet.com/rss/news\",\n        ]\n    },\n}\n\nray.init()\n\nresult = pipeline.run(pipeline_inputs)\n\nprint(\"RESULT: \", result[\"adapter\"][\"output\"])\n```\n\nThe pipeline above is a simple one - first fetch RSS feed contents and then parse and extract documents from it using `OutputAdapter`. However we would get the following error:\n\n```text\n  File \".../ray_haystack/serialization/component_wrapper.py\", line 48, in __init__\n    self._component = component_from_dict(component_class, component_data, None)\n  File \".../haystack/core/serialization.py\", line 118, in component_from_dict\n    return do_from_dict()\n  File \".../haystack/core/serialization.py\", line 113, in do_from_dict\n    return cls.from_dict(data)\n  File \".../haystack/components/converters/output_adapter.py\", line 170, in from_dict\n    init_params[\"custom_filters\"] = {\n  File \".../haystack/components/converters/output_adapter.py\", line 171, in <dictcomp>\n    name: deserialize_callable(filter_func) if filter_func else None\n  File \".../haystack/utils/callable_serialization.py\", line 45, in deserialize_callable\n    raise DeserializationError(f\"Could not locate the callable: {function_name}\")\nhaystack.core.errors.DeserializationError: Could not locate the callable: parse_sources\n```\n\nSeems like the `parse_sources` function was not available to python interpreter during de-serialization of the `OutputAdapter` component. In order to understand what happens under the hood lets see how the component looks like in a serialized format:\n\n```python\n{\n    'type': 'haystack.components.converters.output_adapter.OutputAdapter',\n    'init_parameters': {\n        'template': '{{ sources | parse_sources }}',\n        'output_type': 'typing.List[haystack.dataclasses.document.Document]',\n        'custom_filters': {'parse_sources': '__main__.parse_sources'},\n        'unsafe': False\n    }\n}\n```\n\nWhen component is deserialized the `__main__.parse_sources` function is not present in a remote Ray actor anymore. Which is understandable because we have not instructed Ray to push any module or package which contains the `parse_sources` function in it. (Moreover Ray's worker node which runs component actor has its own `__main__` module and `parse_sources` is not defined there).\n\nTo fix the issue what we need to import `worker_asset` decorator from `ray_haystack.serialization` package and apply the decorator to the `parse_sources` function. Please uncomment the decorator fix in the example and run the pipeline again. You should see Documents as a result.\n\n`worker_asset` instructs serialization process to bring in the `parse_sources` function along with the component so that when it deserialized `parse_sources` is imported.\n\n> **Important**\n> Please use `@worker_asset` whenever you encounter issues with components like `OutputAdapter` where some functions are referenced by a component. Make sure you do not use lambdas but rather a dedicated python function with the decorator in place.\n\n### DocumentStore with Ray\n\nUnfortunately when you use [InMemoryDocumentStore](https://docs.haystack.deepset.ai/docs/inmemorydocumentstore) or any DocumentStore which runs in-memory (a singleton) with `RayPipeline` you will stumble upon an apparent issue: in distributed environment such DocumentStore will fail to operate as components which reference the store will not point to single instance but rather a copy of it.\n\n`ray-haystack` package provides a wrapper around `InMemoryDocumentStore` by implementing a proxy pattern so that only a single instance of `InMemoryDocumentStore` across Ray cluster is present. With that pipeline components could share a single store. Use `RayInMemoryDocumentStore`, `RayInMemoryEmbeddingRetriever` or `RayInMemoryBM25Retriever` in case you need in-memory document store in your Ray pipelines. See below a conceptual diagram of how components refer to a single instance of the store:\n\n![RayInMemoryDocumentStore](https://raw.githubusercontent.com/prosto/ray-haystack/main/docs/ray_in_memory_document_store.png)\n\nWhenever you create `RayInMemoryDocumentStore` internally an actor is created which wraps native `InMemoryDocumentStore` and acts as a singleton in cluster. `RayInMemoryDocumentStore` forwards calls to the remote actor.\n\nLets see it in action by running the [basic RAG pipeline](https://haystack.deepset.ai/tutorials/27_first_rag_pipeline).\n\nBefore running the script make sure additional dependencies have been installed:\n\n```bash\npip install \"datasets>=2.6.1\"\npip install \"sentence-transformers>=3.0.0\"\n```\n\n```python\nimport os\n\nimport ray\nfrom datasets import load_dataset\nfrom haystack.components.builders.prompt_builder import PromptBuilder\nfrom haystack.components.embedders import (\n    SentenceTransformersDocumentEmbedder,\n    SentenceTransformersTextEmbedder,\n)\nfrom haystack.components.generators import OpenAIGenerator\nfrom haystack.dataclasses import Document\nfrom haystack.document_stores.types import DocumentStore\n\nfrom ray_haystack.components import (\n    RayInMemoryDocumentStore,\n    RayInMemoryEmbeddingRetriever,\n)\nfrom ray_haystack.ray_pipeline import RayPipeline\n\nos.environ[\"OPENAI_API_KEY\"] = \"Your OPenAI API Key\"\n\n\ndef prepare_document_store(document_store: DocumentStore):\n    doc_embedder = SentenceTransformersDocumentEmbedder(model=\"sentence-transformers/all-MiniLM-L6-v2\")\n    doc_embedder.warm_up()\n\n    dataset = load_dataset(\"bilgeyucel/seven-wonders\", split=\"train\")\n    docs = [Document(content=doc[\"content\"], meta=doc[\"meta\"]) for doc in dataset]\n\n    docs_with_embeddings = doc_embedder.run(docs)\n    document_store.write_documents(docs_with_embeddings[\"documents\"])\n\n\ntemplate = \"\"\"\nGiven the following information, answer the question.\n\nContext:\n{% for document in documents %}\n    {{ document.content }}\n{% endfor %}\n\nQuestion: {{question}}\nAnswer:\n\"\"\"\n\n# Start Ray cluster before defining `RayInMemoryDocumentStore` as internally it creates an actor\nray.init()\n\ndocument_store = RayInMemoryDocumentStore()  # from `ray-haystack`\n\n# Create documents in document_store\nprepare_document_store(document_store)\n\ntext_embedder = SentenceTransformersTextEmbedder(model=\"sentence-transformers/all-MiniLM-L6-v2\")\nretriever = RayInMemoryEmbeddingRetriever(document_store)  # from `ray-haystack`\ngenerator = OpenAIGenerator()\n\nprompt_builder = PromptBuilder(template=template)\n\npipeline = RayPipeline()\n\npipeline.add_component(\"text_embedder\", text_embedder)\npipeline.add_component(\"retriever\", retriever)\npipeline.add_component(\"prompt_builder\", prompt_builder)\npipeline.add_component(\"llm\", generator)\n\npipeline.connect(\"text_embedder.embedding\", \"retriever.query_embedding\")\npipeline.connect(\"retriever\", \"prompt_builder.documents\")\npipeline.connect(\"prompt_builder\", \"llm\")\n\nquestion = \"What does Rhodes Statue look like?\"\n\nresponse = pipeline.run(\n    {\n        \"text_embedder\": {\"text\": question},\n        \"prompt_builder\": {\"question\": question},\n    }\n)\n\nprint(\"RESULT: \", response[\"llm\"][\"replies\"][0])\n```\n\nPlease notice the components imported from the `ray_haystack.components` package and how a single instance of the `RayInMemoryDocumentStore` is used for both indexing (`prepare_document_store`) and then querying (`pipeline.run`).\n\n> **Important**\n> Existing implementation of the `RayInMemoryDocumentStore` might be a subject to change in future. In case you would like to introduce or use another store which also works in-memory take a look at implementation of the components in the `ray_haystack.components` package. It should not take much time implementing your own wrapper by following the example.\n\n### RayPipeline Settings\n\nWhen an actor is created in Ray we can control its behavior by providing certain [settings](https://docs.ray.io/en/latest/ray-core/api/doc/ray.actor.ActorClass.options.html).\nSome examples are provided below:\n\n- num_cpus – The quantity of CPU cores to reserve for this task or for the lifetime of the actor.\n- num_gpus – The quantity of GPUs to reserve for this task or for the lifetime of the actor.\n- name – The globally unique name for the actor, which can be used to retrieve the actor via ray.get_actor(name) as long as the actor is still alive.\n- runtime_env (Dict[str, Any]) – Specifies the runtime environment for this actor or task and its children.\n- etc\n\n[`runtime_env`](https://docs.ray.io/en/latest/ray-core/handling-dependencies.html#runtime-environments) is a notable configuration option as it can control which `pip` dependencies needs to be installed for actor to run, environment variables, container image to use in a cluster etc. In our case we are specifically interested in [Specifying a Runtime Environment Per-Task or Per-Actor](https://docs.ray.io/en/latest/ray-core/handling-dependencies.html#specifying-a-runtime-environment-per-task-or-per-actor)\n\nAs you have already learned when we run pipeline with `RayPipeline` a couple of actors are created before the pipeline starts running. To control actor's runtime environment as well as resources like CPU/GPU/RAM per actor you can use `ray_haystack.RayPipelineSettings` configuration dictionary.\n\nBelow is the definition of the dictionary (see the [source](https://github.com/prosto/ray-haystack/blob/main/src/ray_haystack/ray_pipeline_settings.py) for more details):\n\n```python\nclass RayPipelineSettings(TypedDict, total=False):\n    common: CommonSettings # settings common for all actors within pipeline\n\n    processor: ProcessorSettings # settings for pipeline processor\n    components: CommonComponentSettings # settings common for all components, and per component\n    events_queue: EventsQueueSettings # settings for events_queue actor\n\n    # controls how settings are merged, e.g. \"common\" <- \"common components\" <- \"component specific\"\n    merge_strategy: Literal[\"REPLACE\", \"ADDITIVE\", \"TYPESAFE_REPLACE\", \"TYPESAFE_ADDITIVE\"]\n```\n\nThere are two options how `RayPipelineSettings` can be provided for `RayPipeline`:\n\n```python\nfrom typing import Any, Dict\n\nfrom ray_haystack import RayPipeline, RayPipelineSettings\n\nsettings: RayPipelineSettings = {\n    \"common\": {\n        \"actor_options\": {\n            \"namespace\": \"haystack\",  # common namespace name for all actors\n        }\n    },\n    \"components\": {\n        \"per_component\": {\n            \"generator\": {\n                \"actor_options\": {\n                    \"num_cpus\": 2,  # component specific CPU resource requirement\n                }\n            }\n        }\n    },\n}\n\n# Option 1 - Pass settings through pipeline's metadata\npipeline = RayPipeline(metadata={\"ray\": settings})\n\npipeline_inputs: Dict[str, Any] = {}\n\n# Option 2 - Pass settings when in the `run` method\npipeline.run(pipeline_inputs, ray_settings=settings)\n```\n\nAbove code highlights two ways to supply settings for a pipeline:\n\n1. pipeline's `metadata` with key `\"ray\"`\n2. directly as keyword argument in `run` method (will override settings from `metadata`)\n\nAbove example also demonstrates that it is possible to configure actor options per component and also define common options which will be shared between all actors created by `RayPipeline`. Actor options are being merged with component specific values taking precedence. Internally `mergedeep` python package is being used to merge dictionaries and you can control how merging works by picking [strategy](https://mergedeep.readthedocs.io/en/latest/#merge-strategies). Default one is `ADDITIVE`.\n\nLets build a small example of a pipeline which uses component specific environment variables:\n\n```python\nimport os\n\nimport ray\nfrom haystack.components.builders.prompt_builder import PromptBuilder\nfrom haystack.components.fetchers import LinkContentFetcher\nfrom haystack.components.generators import OpenAIGenerator\n\nfrom ray_haystack import RayPipeline, RayPipelineSettings\n\nos.environ[\"OPENAI_API_KEY\"] = \"OpenAI API Key Here\"\n\nprompt_template = \"\"\"You will be given a JSON document containing data about a random cocktail recipe.\nThe data will include fields with name of the drink, its ingredients and instructions how to make it.\nBuild a cocktail description card in markdown format based on the fields present in the json.\nIgnore fields with \"null\" values. Keep instruction as is.\n\nJSON: {{cocktail_json[0]}}\n\nCocktail Card:\n\"\"\"\n\n# Setup environment for the whole cluster (each worker node will get env var OPENAI_TIMEOUT=15)\nray.init(runtime_env={\"env_vars\": {\"OPENAI_TIMEOUT\": \"15\"}})\n\nfetcher = LinkContentFetcher()\nprompt = PromptBuilder(template=prompt_template)\ngenerator = OpenAIGenerator()\n\npipeline = RayPipeline()\npipeline.add_component(\"cocktail_fetcher\", fetcher)\npipeline.add_component(\"prompt\", prompt)\npipeline.add_component(\"llm\", generator)\n\npipeline.connect(\"cocktail_fetcher.streams\", \"prompt.cocktail_json\")\npipeline.connect(\"prompt\", \"llm.prompt\")\n\nsettings: RayPipelineSettings = {\n    # common settings will be applied for all actors created\n    \"common\": {\n        \"actor_options\": {\"namespace\": \"haystack\"},\n    },\n    \"components\": {\n        # Applies OPENAI_TIMEOUT to all components in pipeline\n        \"actor_options\": {\"runtime_env\": {\"env_vars\": {\"OPENAI_TIMEOUT\": \"10\"}}},\n        \"per_component\": {\n            # \"llm\" component will get a new value for OPENAI_TIMEOUT overriding common settings\n            # so the final value for OPENAI_TIMEOUT environment variable is 11\n            \"llm\": {\n                \"actor_options\": {\"runtime_env\": {\"env_vars\": {\"OPENAI_TIMEOUT\": \"11\"}}},\n            },\n        },\n    },\n}\n\nresponse = pipeline.run(\n    {\n        \"cocktail_fetcher\": {\"urls\": [\"https://www.thecocktaildb.com/api/json/v1/1/random.php\"]},\n    },\n    ray_settings=settings,  # pass settings to pipeline execution\n)\n\nprint(\"RESULT: \", response[\"llm\"][\"replies\"][0])\n```\n\nPlease notice how `OPENAI_TIMEOUT` environment variable is set globally by `ray.init` and then with `RayPipelineSettings` it gets overridden specifically for the \"llm\" component.\n\n> **Note**\n> You may ask why didn't we provide `OPENAI_API_KEY` same way as `OPENAI_TIMEOUT` as in the example above. And the reason is hidden in the implementation of the `OpenAIGenerator` constructor which raises error if there is no value for the `OPENAI_API_KEY` env var. So `OPENAI_API_KEY` value is required before `RayPipeline` creates component actors when it starts.\n\nExplore available options in [ray_pipeline_settings.py](https://github.com/prosto/ray-haystack/blob/main/src/ray_haystack/ray_pipeline_settings.py), `RayPipelineSettings` is just a python `TypedDict` which helps creating settings in your IDE of choice.\n\n### Middleware\n\n> **Warning**\n> This feature is experimental and implementation details as well as API might change in future.\n\nSometimes it might be useful to let custom logic run before and after component actor runs the component:\n\n- Fire a custom event and put it into events queue\n- Tweak component inputs before they are sent to component\n- Tweak component outputs before they are sent to other components (through connections)\n- Additional logging/tracing\n- Custom time delays in order to slow down component execution\n- An external trigger which blocks component run until some event occurs\n- Debugging breakpoints for components (e.g. stop component running until a trigger unblocks the breakpoint)\n\nAbove is just a high level vision of what I would expect middleware to do.\n\nLets build an example of how custom middleware can be introduced and applied. We will intercept `LinkContentFetcher` component and see the order of execution of middleware.\n\n```python\nfrom typing import Any, Literal\n\nimport ray\nfrom haystack.components.fetchers import LinkContentFetcher\n\nfrom ray_haystack import RayPipeline, RayPipelineSettings\nfrom ray_haystack.middleware import ComponentMiddleware, ComponentMiddlewareContext\nfrom ray_haystack.serialization import worker_asset\n\nray.init()\n\n\n@worker_asset\nclass TraceMiddleware(ComponentMiddleware):\n    def __init__(self, capture: Literal[\"input\", \"output\", \"input_and_output\"] = \"input_and_output\"):\n        self.capture = capture\n\n    def __call__(self, component_input, ctx: ComponentMiddlewareContext) -> Any:\n        print(f\"Tracer: Before running component '{ctx['component_name']}' with inputs: '{component_input}'\")\n\n        outputs = self.next(component_input, ctx)\n\n        print(f\"Tracer: After running component '{ctx['component_name']}' with outputs: '{outputs}'\")\n\n        return outputs\n\n\n@worker_asset\nclass MessageMiddleware(ComponentMiddleware):\n    def __init__(self, message: str):\n        self.message = message\n\n    def __call__(self, component_input, ctx: ComponentMiddlewareContext) -> Any:\n        print(f\"Message: Before running component '{ctx['component_name']}' : '{self.message}'\")\n\n        outputs = self.next(component_input, ctx)\n\n        print(f\"Message: After running component '{ctx['component_name']}' : '{self.message}'\")\n\n        return outputs\n\n\npipeline = RayPipeline()\npipeline.add_component(\"cocktail_fetcher\", LinkContentFetcher())\n\nsettings: RayPipelineSettings = {\n    \"components\": {\n        \"per_component\": {\n            # Middleware applies only to \"cocktail_fetcher\" component\n            \"cocktail_fetcher\": {\n                \"middleware\": {\n                    \"trace\": {\"type\": \"__main__.TraceMiddleware\"},\n                    \"message\": {\n                        \"type\": \"__main__.MessageMiddleware\",\n                        \"init_parameters\": {\"message\": \"Hello Fetcher\"},\n                    },\n                },\n            },\n        }\n    },\n}\n\nresponse = pipeline.run(\n    {\n        \"cocktail_fetcher\": {\"urls\": [\"https://www.thecocktaildb.com/api/json/v1/1/random.php\"]},\n    },\n    ray_settings=settings,\n)\n```\n\nPlease notice the following from the example above:\n\n- When defining custom middleware extend from `ComponentMiddleware` class. It provides a basic implementation of the `set_next` method. In case you do not want to extend from the base class make sure you implement `set_next` yourself\n- Middleware is applied to a component in a pipeline with `RayPipelineSettings` (see `middleware` key in the dictionary)\n- Middleware is applied by decorating the component's `run` method in the order it is defined in the settings dictionary\n  - \"message\" (before)\n  - \"trace\" (before)\n  - \"trace\" (after)\n  - \"message\" (after)\n- `@worker_asset` decorator is applied to custom middleware so that during component actor creation Ray worker is able to deserialize (instantiate) middleware class. For example `\"__main__.MessageMiddleware\"` will be imported before creating instance of the `MessageMiddleware` class\n\nAs of now `ray-haystack` provides `DelayMiddleware` for adding time \"sleeps\" for components so that we could slow down all components or specific ones.\nIt was introduced to allow easier tracing experience when you consume events from events queue in order to see how components run in \"slow motion\". You can apply it aas follows:\n\n```python\nsettings: RayPipelineSettings = {\n    \"components\": {\n        # Applies to all component in pipeline\n        \"middleware\": {\n            \"delay\": {\n                # full module path is required, will be used during deserialization in component actor\n                \"type\": \"ray_haystack.middleware.delay_middleware.DelayMiddleware\",\n                \"init_parameters\": {\n                    \"delay\": 2,  # default is 1\n                    \"delay_type\": \"before\",  # could be \"after\" or \"before_and_after\"\n                },\n            }\n        }\n    },\n}\n```\n\n## More Examples\n\n### Trace Ray Pipeline execution in Browser\n\n[`pipeline_watch`](https://github.com/prosto/ray-haystack/blob/main/examples/pipeline_watch/README.md) is a sample application which runs Ray Pipelines with Ray Serve in backend and provides UI in browser to track running pipeline (component by component). It uses pipeline events which are streamed to browser using Server Sent Events. Please follow instructions inside the `pipeline-watch` folder in order to install and run the example.\n\nPlease check it out, you may like it!\n\n### Ray Pipeline on Kubernetes\n\nThe ability to run Ray Pipeline on a remote Ray Cluster was an important step to test its \"non-local\" setup use case. The [`pipeline_kubernetes`](https://github.com/prosto/ray-haystack/blob/main/examples/pipeline_kubernetes/README.md) example provides instructions on how to install a local Ray Cluster by deploying KubeRay operator in Kubernetes and then run pipeline using [ray job submission SDK](https://docs.ray.io/en/latest/cluster/kubernetes/getting-started/raycluster-quick-start.html#method-2-submit-a-ray-job-to-the-raycluster-via-ray-job-submission-sdk)\n\n### Ray Pipeline with detached components\n\nSome of [Actor Options](https://docs.ray.io/en/latest/ray-core/api/doc/ray.actor.ActorClass.options.html) which are configurable per component can result in an interesting effect on how Ray Pipeline runs. One of such options is `lifetime`. When it is \"detached\" the actor will live as a global object independent of the creator, thus if `RayPipeline` finishes component actor will remain alive. [`pipeline_detached_actors`](https://github.com/prosto/ray-haystack/tree/main/examples/pipeline_detached_actors) explores such case and also runs pipeline in a Notebook.\n\n## Next Steps & Enhancements\n\n- [ ] Introduce logging configuration, e.g. control log level & format per component\n- [ ] Better error handling in case pipeline output is consumed with pipeline events\n- [ ] Create middleware to allow breakpoints in pipeline execution (stop at certain component until certain event is triggered)\n- [ ] Write API documentation for main package components\n- [ ] Introduce more tests for pipeline processing logic to cover more scenarios (e.g. complex cycles)\n- [ ] Explore fault tolerance options and see what happens when certain parts fail during execution\n- [ ] Explore the option of running Haystack pipeline on a Ray cluster with GPU available\n- [ ] Improve DocumentStore proxy implementation so that other DocumentStores which run in-memory could be quickly adapted to `RayPipeline` without much boilerplate code\n\n## Acknowledgments\n\nI would have spent much more time testing pipeline execution logic if a [awesome testing suite](https://github.com/deepset-ai/haystack/tree/main/test/core/pipeline/features) was not available. I have adopted tests to make sure pipeline behavior is on par with Haystack. Thanks @silvanocerza for the tests and also clarifications on pipeline internals.\n"
    },
    {
      "name": "vanrohan/ai-bookmark-organizer",
      "stars": 18,
      "img": "https://avatars.githubusercontent.com/u/22074683?s=40&v=4",
      "owner": "vanrohan",
      "repo_name": "ai-bookmark-organizer",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2024-08-31T09:47:21Z",
      "updated_at": "2025-04-12T17:10:35Z",
      "topics": [],
      "readme": "This is a companion repository to the blog post [Using AI to declutter and organise my bookmarks](https://vanderwalt.de/blog/ai-bookmark-organizer) \n\nTo install dependencies:\n    `poetry install`\n\nTo start the headless Chromium docker container:\n    `docker compose -f docker-compose.yml up --detach=true`\n\nTo run: \n    `poetry run python main.py -f bookmarks_30_08_2024.html --chrome localhost:9223`\n"
    },
    {
      "name": "intel/open-domain-question-and-answer",
      "stars": 17,
      "img": "https://avatars.githubusercontent.com/u/17888862?s=40&v=4",
      "owner": "intel",
      "repo_name": "open-domain-question-and-answer",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2022-11-24T08:10:32Z",
      "updated_at": "2025-03-27T05:57:00Z",
      "topics": [],
      "readme": "# DISCONTINUATION OF PROJECT\nThis project will no longer be maintained by Intel.  \nThis project has been identified as having known security escapes.  \nIntel has ceased development and contributions including, but not limited to, maintenance, bug fixes, new releases, or updates, to this project.  \nIntel no longer accepts patches to this project.  \n\n\n# Open Domain Question and Answer\n\n## Introduction\nOpen Domain Question Answering (ODQA) is an important task in Natural Language Processing (NLP), which aims to answer a question in the form of natural language based on large-scale unstructured documents. This repository is an open domain question answering(ODQA) framework forking from [haystack 1.12.2](https://haystack.deepset.ai/overview/intro), and you can find more detailed information on their [github](https://github.com/deepset-ai/haystack). We provide three optimized experimental pipelines including:\n\n- Generate the database indexing on Ray cluster\n- Run the full workflow with generated database indexing and pipelines. \n\nCheck out more workflow examples in the [Developer Catalog](https://developer.intel.com/aireferenceimplementations).\n\n## Solution Technical Overview\nEnterprises are accumulating a vast quantity of documents. These documents contain a large amount of valuable information, but it is a challenge for enterprises to index, search and gain insights from the document.\n\nIn this workflow, we provide a end-to-end solution for user to gain insights from large amount of documents, along with 3 optimized pipelines. This workflow also support customization to cover your own dataset. The architecture of the workflow is shown in the figure below.\n\n<p align=\"center\"> <img src=\"images/odqa_workflow.png\" height=\"360px\"><br></p>\n\n## Solution Technical Details\nThis workflow is composed of 2 pipelines, including offline indexing pipeline and online search pipeline. The indexing pipeline is responsible for processing dataset, generating document embedding and saving indexing file to DocumentStore. Indexing pipeline is built based on haystack pipeline and supports distributed execution with Ray. The search pipeline loads indexing file from database, retrieves documents according to query question and generate most relevant answer. A web UI will be deployed to provide query service.\n\n## Validated Hardware Details \n| Supported Hardware           | Requirements  |\n| ---------------------------- | ---------- |\n| Intel® 1st, 2nd, 3rd, and 4th Gen Xeon® Scalable Performance processors| FP32 |\n| Memory | >376 GB |\n| Storage | >200 GB |\n\n>Note: This workflow is verified with OS Rocky 8.7 and Python 3.6.8.\n\n## How it Works\n### Indexing Pipeline\nWe need to generate document indexing to retrieve documents from document store, the generated indexing file will be stored in database. Indexing pipeline contains 3 tasks:\n\n1. Pre-processing: load and extract raw dataset, convert data into haystack [Document](https://docs.haystack.deepset.ai/docs/documents_answers_labels) object.\n\n2. Embedding generation: generate vector representations of text passages using pre-trained encoder model from Hugging Face and then generate indexing file with the vectors. The indexing file will be used in retrieval phase, retriever will use query vector to search against indexing file to find the highest similarity to the query vector. It can take days to generate document indexing file based on type and number nodes you are using, we provide multi-node distributed indexing pipeline with Ray to  accelerate the indexing process.\n\n3. Write document store: generated indexing file will be written to haystack [DocumentStore](https://docs.haystack.deepset.ai/docs/document_store) and stored in database.\n\n### Search Pipeline\nThe search pipeline consists of haystack REST API and web UI to support deploy in web. With search pipeline deployed, users can interact with the document search system through a web UI and submit queries in natural language. The retriever will search the most relevant documents in the databases, ranker will rerank the documents to have a better sorted list of relevant documents and reader takes a question and a set of documents as input and returns an answer by selecting a text span within the documents\n\n## Get Started\n### Step 1. Install Docker and Docker Compose\n>Note: If you have docker and docker-compose installed on your machine, then skip this.\n\nYou'll need to install Docker Engine on your development system. Note that while **Docker Engine** is free to use, **Docker Desktop** may require you to purchase a license.  See the [Docker Engine Server installation instructions](https://docs.docker.com/engine/install/#server) for details.\n\nTo build and run this workload inside a Docker Container, ensure you have Docker Compose installed on your machine. If you don't have this tool installed, consult the official [Docker Compose installation documentation](https://docs.docker.com/compose/install/linux/#install-the-plugin-manually).\n\nWe provide a shell script to install docker and docker compose\n\n```bash\n# change to sudo privileges\nsudo su\n# run shell script (support Red Hat Linux)\n./prepare_env.sh\n```\n\n### Step 2. Download the Workflow Repository\n```bash\ngit clone https://github.com/intel/open-domain-question-and-answer.git\ncd open-domain-question-and-answer/\ngit submodule update --init --recursive\n```\n\n### Step 3. Set Proxy (optional)\ndocker container need to download model from [Huggingface](https://huggingface.co/) and install related dependencies from internet, hence we may need to set environment param of proxy for it. Here we map http_proxy and https_proxy from host to the docker container. So please set correct environment param for http_proxy and https_proxy on the host machine. \n\n### Step 4. Download the Datasets\n[MS Marco](https://microsoft.github.io/msmarco/) Question Answering and Natural Langauge Generation dataset consists of a question answering dataset featuring 100,000 real Bing questions and a human generated answer, and The Natural Language Generation dataset features 180,000 examples and builds upon the QnA dataset to deliver answers that could be spoken by a smart speaker\n\n```bash\n# Download Marco dataset\nmkdir marco\ncd marco\nwget https://msmarco.blob.core.windows.net/msmarco/train_v2.1.json.gz\ngunzip train_v2.1.json.gz\n```\n### Step 5. Download the colbertv2.0 Model (Optional)\n>Note: If you will not try the colbert pipelines, skip it.\n\n```bash\n# download the colbert model\nwget https://downloads.cs.stanford.edu/nlp/data/colbert/colbertv2/colbertv2.0.tar.gz\ntar -xvzf colbertv2.0.tar.gz\n```\n\n### Step 6. Set Up Docker Image\nPull docker images from dockerhub\n\n```bash\ndocker pull intel/ai-workflows:odqa-haystack-api\ndocker pull intel/ai-workflows:odqa-haystack-ui\n```\n\nOr build docker images\n\n```bash\ncd applications/indexing\n./build_img.sh\n```\n\nIf your environment requires a proxy to access the internet, export your development system's proxy settings to the docker environment:\n```bash\nexport DOCKER_RUN_ENVS=\"-e ftp_proxy=${ftp_proxy} \\\n  -e FTP_PROXY=${FTP_PROXY} -e http_proxy=${http_proxy} \\\n  -e HTTP_PROXY=${HTTP_PROXY} -e https_proxy=${https_proxy} \\\n  -e HTTPS_PROXY=${HTTPS_PROXY} -e no_proxy=${no_proxy} \\\n  -e NO_PROXY=${NO_PROXY} -e socks_proxy=${socks_proxy} \\\n  -e SOCKS_PROXY=${SOCKS_PROXY}\"\n```\n\n## Run Indexing Pipeline\nGo to indexing pipeline folder\n\n```bash\npip install pyyaml\ncd applications/indexing\n```\n\n### Step 1. Modify the Indexing Workflow YAML File\nModify `applications/indexing/marco_indexing_workflow.yml' according to your system environment. The YAML file must include a head node. The head node should be your local machine which launches the indexing workflow. You can add the worker node under the nodes component for distributed pipeline execution.\n\nThe pipelines component declares the pipelines that need to be executed, and declares the database used by each pipeline. The YAML files of these pipelines are included in $workspace_dir which is ./marco_indexing for marco dataset.\n\nKey configuration variables are listed in the table bellow\n\n| Environment Variable Name | Mount Point | Description |\n| --- | --- | --- |\n| $workspace_dir | `/home/user/workspace` | Include the pipeline YAML files, prepare_env.sh and python file of dataset class for preprocessing and converting dataset data to documents or files of Ray dataset. |\n| $customer_dir | `/home/user/data` | The log will be saved in the directory. You also can copy your model into this direcotry. For example, indexing workflow of colbert_indexing_pipeline.yml needs the colbertv2.0 model. Copy colbertv2.0 folder into this directory. |\n| $dataset_dir | `/home/user/dataset` | The directory include the dataset files |\n| $data_dir | database storage dir | if $data_dir is not exist, it will be created. Please use different directory path for different pipeline. |\n\n### Step 2. Modify the Indexing Pipeline YAML Files in $workspace_dir\nIn pipeline YAML files there are some parameters need to be modified. These pipeline YAML files is under your $workspace_dir. \n\nFor example：\n\n./marco_indexing/colbert_indexing_pipeline.yml\n./marco_indexing/emr_indexing_pipeline.yml\n./marco_indexing/faiss_indexing_pipeline.yml\n\nEdit these YAML files according to your local runtime environment.\n\n- For DocumentStore you need to specify the address of the linked database. The $host_ip should be the host IP of head node, because the database container is launched on head node.\n\n- Check the files path of $customer_dir and $dataset_dir. They are mounted into containers of Ray head and workers. They should include the dataset files or finetuned models downloaded by yourself. If there are subdirectories in these directories, please modify the corresponding path.\n\n- Adjust the actor numbers and batch_size according to the hardware resource of the Ray cluster. Too large actor numbers and batch_size may cause out of memory. The num_replicas x num_cpus should be less than the total number of CPUs you configured in the indexing workflow YAML.\n\n  ```yaml\n  serve_deployment_kwargs:\n      num_replicas: 80  # number of actors to create on the Ray cluster\n      batch_size: 256\n      num_cpus: 2\n  ```\n\n### Step 3. Generate Marco Database\nLaunch the Ray cluster for indexing workflows.\n```bash\n#Launch Ray cluster for marco indexing workflow\n$ python launch_indexing_workflow.py -w marco_indexing_workflow.yml\n```\n\nRun the indexing workflows on Ray cluster. The `-p` option value is the name of pipeline YAML file or all.\n\n **Note:**\n For faster debugging and demonstration demo, you can choose the Marco dataset and use the -s option to take only 500 samples from dataset for indexing\n\n```bash\n#Run all pipelines defined in marco_indexing_workflow.yml\n$ python launch_indexing_workflow.py -w marco_indexing_workflow.yml -p all\n\n#Run faiss_indexing_pipeline.yml defined in the marco_indexing_workflow.yml\n$ python launch_indexing_workflow.py -w marco_indexing_workflow.yml -p faiss_indexing_pipeline.yml\n\n#Run faiss_indexing_pipeline.yml defined in the marco_indexing_workflow.yml, only take 500 samples from dataset for indexing pipeline debugging or demo.\n$ python launch_indexing_workflow.py -w marco_indexing_workflow.yml -p faiss_indexing_pipeline.yml -s 1\n```\n\nAfter generating the database of Faiss pipeline, copy and save the indexing files of $customer_dir to avoid them being overwritten by the new faiss indexing workflow.\n\nClean previous containers you have ran, you can use following commands on all nodes of your Ray cluster.\n\n```bash\n# Clean all the Ray and database containers running in local node.\n$ ./run-ray-cluster.sh -r clean_all\n# Clean all the database containers running in local node.\n$ ./run-ray-cluster.sh -r clean_db\n```\n\n## Run Search Pipeline\nThe search pipeline is config-driven and containerized. There are 4 config files for user to customs deployment of search pipeline.\n1. environment config file that sets environment variables for docker-compose\n2. pipeline config yaml file that defines haystack components and pipeline\n3. UI config yaml file that defines web UI properties\n4. docker compose yaml file that defines how to launch search pipeline\n\nWe provide 3 types of search pipelines with different retriever, including EmbeddingRetriever, BM25Retriever + ColbertRanker and DensePassageRetriever, you can decide to deploy which type of pipeline with just a few configurations.\n\nFirst make sure you are in `applications/odqa-pipelines` folder of the repo\n\n```bash\ncd applications/odqa_pipelines\n```\n\n### Option 1. Run EMR Pipeline\n>Note: Please make sure you have completed emr_indexing_pipeline.yml indexing pipeline\n\nElasticsearchDocumentStore->EmbeddingRetriever(deepset/sentence_bert)->Docs2Answers\n  \n<p align=\"center\"> <img src=\"images/pipeline1.PNG\" height=\"180px\"><br></p>\n  \nModify the config file `config/env.marco.esds_emr_faq`\n\n```bash\n# set the $data_dir to the data folder of elasticsearch database, please refer to applications/indexing/marco_indexing_workflow.yml\nDATA_DIR=$data_dir \n```\n\nRun workflow: \n\n```bash\n#deploy search pipeline with marco database\ndocker-compose -f docker-compose/docker-compose.yml --env-file config/env.marco.esds_emr_faq up\n```\n\n### Option 2. Run Cobert Pipeline\n>Note: Please make sure you have completed colbert_indexing_pipeline.yml indexing pipeline\n\nElasticsearchDocumentStore->BM25Retriever->ColbertRanker-> Docs2Answers\n\n<p align=\"center\"> <img src=\"images/pipeline2.PNG\" height=\"180px\"><br></p>\n\nModify the config file `config/env.marco.esds_bm25r_colbert`\n\n```bash\n# set the $data_dir to the data folder of elasticsearch database, please refer to applications/indexing/marco_indexing_workflow.yml\nDATA_DIR=$data_dir\n# set the $customer_dir to the absolute ColbertV2.0 model path you placed\nCUSTOMER_DIR=$customer_dir\n```\n\nModify the docker compose file `docker-compose/docker-compose.yml`, uncomment the following lines.\n```bash\n  #volumes:\n  #  - $CUSTOMER_DIR:/home/user/data\n```\n\nRun workflow:\n\n```bash\n#deploy search pipeline with marco database\ndocker-compose -f docker-compose/docker-compose.yml --env-file config/env.marco.esds_bm25r_colbert up\n```\n\n### Option 3. Run DPR Pipeline\n>Note: Please make sure you have completed faiss_indexing_pipeline.yml indexing pipeline\n\nFAISSDocumentStore->DensePassageRetriever->Docs2Answers\n\n<p align=\"center\"> <img src=\"images/pipeline3.PNG\" height=\"180px\"><br></p>\n\nModify the config file `config/env.marco.faiss_dpr`\n\n```bash\n# set the $data_dir to the data folder of postgresql database, please refer to applications/indexing/marco_indexing_workflow.yml\nDATA_DIR=$data_dir\n# set the $customer_dir to the absolute path where you store faiss indexing files.\nCUSTOMER_DIR=$customer_dir\n```\n\nModify the docker compose file `docker-compose/docker-compose-dpr.yml`, uncomment the following lines.\n\n```bash\n  #volumes:\n  #  - $CUSTOMER_DIR:/home/user/data\n```\n\nRun workflows:\n\n```bash\n#deploy search pipeline with marco database\ndocker-compose -f docker-compose/docker-compose-dpr.yml --env-file config/env.marco.faiss_dpr up \n```\n\n## Expected Output\nOnce you deployed search pipeline successfully, open a browser and input following url to access web UI\n\n```bash\n<host_ip>:8501\n```\n\nThe webpage should be like the below one\n\n<p align=\"center\"> <img src=\"images/ui.png\"><br></p>\n\n## Summary and Next Steps\nYou have just run through an end-to-end open domain question and answer workflow composed of indexing and search pipelines on MS Marco dataset. With this workflow deployed successfully, you are able to query on a large amount of data source with natural language. You can also select different retriever for different needs.\n\nNext you can customize the workflow to your own use case by following the customization sections below.\n\n### Customize this Workflow to Your Own Use Case\nWe provide quick start [guide](doc/workflow_stackoverflow.md) on StackOverflow dataset for reference on how to customize this workflow to other dataset.\n\n#### Adopt to your dataset\nCopy your dataset to $dataset folder configured in indexing workflow yaml file\n\n#### Customize indexing pipeline\nODQA indexing pipeline is built based on haystack pipeline, which requires a specific format of dataset. We provide [MS Marco dataset processing](applications/indexing/marco_indexing/marco_dataset.py) and [StackOverflow dataset processing](applications/indexing/stackoverflow_indexing/stackoverflow_dataset.py) script, for customer dataset integration with haystack, please refer to [haystack tutorial](https://docs.haystack.deepset.ai/docs/document_store).\n\nIn this workflow, we provide 3 types of passage embedding models, you can also use other models, just change embedding model name in indexing yaml file. Since indexing pipeline is config-driven, all configurations you can customize is under folder `applications/indexing`, just follow comments of how you can customize variables in yaml file.\n\n#### Customize search pipeline\nThe search pipeline is config-driven. There are 4 config files for user to customs deployment of search pipeline.\n1. environment config file that sets environment variables for docker-compose\n2. pipeline config yaml file that defines haystack components and pipeline\n3. UI config yaml file that defines web UI properties\n4. docker compose yaml file that defines how to launch search pipeline\n\n## Learn More\nTo read about other relevant workflow examples, see these guides and software resources:\n\n- [Intel® Developer Catalog](https://developer.intel.com/aireferenceimplementations)\n- [Intel® AI Analytics Toolkit (AI Kit)](https://www.intel.com/content/www/us/en/developer/tools/oneapi/ai-analytics-toolkit.html)\n\n## Troubleshooting\n1. If you got Ray connection timeout error when starting haystack-api docker container, please check that you are using IP address rather hostname in yaml config file.\n2. If you got network connection timeout error when downloading models from Hugging Face, please check your host proxy settings and make sure you can access Hugging Face with your network.\n3. If you want to use indexing file on other machine, you need to modify faiss-indexfile.json to change the IP address of your target machine.\n\n## Support\nThe Open Domain Question and Answer team tracks both bugs and enhancement requests using GitHub issues.\n\n---\n\n\\*Other names and brands may be claimed as the property of others.\n[Trademarks](https://www.intel.com/content/www/us/en/legal/trademarks.html).\n"
    },
    {
      "name": "LLukas22/Retrieval-Augmented-QA",
      "stars": 17,
      "img": "https://avatars.githubusercontent.com/u/65088241?s=40&v=4",
      "owner": "LLukas22",
      "repo_name": "Retrieval-Augmented-QA",
      "description": "Query, ask and chat with a document-index via transformer models!",
      "homepage": "",
      "language": "Python",
      "created_at": "2023-04-02T08:51:43Z",
      "updated_at": "2024-11-23T17:08:25Z",
      "topics": [
        "chatbot",
        "documents",
        "haystack",
        "huggingface",
        "llamacpp",
        "openai",
        "transformer"
      ],
      "readme": "# Retrieval-Augmented-QA-Demo\nQuery, ask and chat with a document-index via a friendly web ui!\n\n<p align=\"center\">\n    <img src=\".images/Chat_Example.gif\" width=\"430\" height=\"550\" />\n</p>\n\n## Installation\n\nSimply run it via Docker-Compose. An example configuration can be found in the `docker-compose.yml` file.\nThe UI is then available via http://localhost:8501 and the API-Swagger-Documentation via http://localhost:8001/docs\n\nIf you want build it from source just download the repo and use the `docker-compose-src-*.yml` files. \nan example `.env` config can be found  in `.env.example`.\n\nAll modules are available as prebuild containers via the [Github container registry](https://github.com/LLukas22?tab=packages&repo_name=Retrieval-Augmented-QA). \n\n## Overview\nThe demo has three main components:\n* Streamlit web ui\n* FastAPI endpoint, which hosts the models and QA pipelines\n* Importers, which import textfiles into the system\n\n<p align=\"center\">\n    <img src=\".images/Overview.png\" width=\"700\" height=\"340\" />\n</p>\n\n## Chat Module\nThe chat module allows the user to either use a LLM to query and summarize retrieved documents from the document index or just chat with the model normaly.\nIt supports multiple chat adapter which are exposed via a streaming api.\nThe following libraries are supported:\n* OpenAI: Uses the default ChatGPT API.To use this adapter a OpenAI API-Token has to be supplied.\n* Huggingface: Supports nearly all LLMs on the Huggingfacehub. Also supports PEFT finetuned models. To run this a GPU needs to be passed to the Container running the API.\n* llama-rs: Can run GGML converted models like [Alpaca](https://huggingface.co/Sosaka/Alpaca-native-4bit-ggml) on a CPU with relatively low resource usage. Use this adapter if you dont have a GPU. \n\n## Semantic Search & Extractive QA Module\nThe semantic search and extractive qa modules use [Haystack](https://haystack.deepset.ai/overview/intro) to query the ElasticSearch database.\nNearly all embedding and QA models on the Huggingfacehub are supported.\nBy default they will be executed on the CPU as a GPU is commonly allocated to the chat model. \n\n##  Importer\nThe importers collect text documents and covert them to Haystack-Documents which are then commited to the database.\n\n### Wiki-Importer\nUses a Wikipedia minidump to populate the database with about 500.000 Wikipedia Articles.\n\n### ST4-Importer\nCan be used to import technical documentation which was created via the [Schema ST4](https://www.quanos-content-solutions.com/en/software/schema-st4) software.\n\n##  Settings and Environment\nTo be able to run this repo on different hardware configurations many settings are configurable via environment variables\n\n### API:\n\n| Environment Variable         | Default                                   | Description                                |\n|------------------------------|-------------------------------------------|--------------------------------------------|\n| HUGGINGFACE_TOKEN            |                                           | Huggingface token                          |\n| ELASTICSEARCH_HOST           | localhost                                 | Elasticsearch host address                 |\n| ELASTICSEARCH_PORT           | 9200                                      | Elasticsearch port number                  |\n| ELASTICSEARCH_USER           |                                           | Elasticsearch user                         |\n| ELASTICSEARCH_PASSWORD       |                                           | Elasticsearch password                     |\n| EMBEDDING_DIM                | 384                                       | Embedding dimension                        |\n| SIMILARITY                   | cosine                                    | Similarity measure                         |\n| EMBEDDING_MODEL              | LLukas22/all-MiniLM-L12-v2-embedding-all  | Embedding model                            |\n| EXTRACTIVE_QA_MODEL          | LLukas22/all-MiniLM-L12-v2-qa-en          | Extractive QA model                        |\n| USE_GPU                      | False                                     | Use GPU for QA and embedding               |\n| USE_8BIT                     | False                                     | Use bits-and-bytes                         |\n| CONCURENCY_LIMIT             | 5                                         | Concurrency limit of api                   |\n| DEBUG                        | True                                      | Debug mode                                 |\n| CHATMODEL                    | CPU                                       | Chat Adapter to use (OPENAI,GPU,CPU)       |\n| CHAT_MAX_INPUT_LENGTH        | 2000                                      | Chat max input length                      |\n| OPENAI_TOKEN                 | None                                      | OpenAI token                               |\n| BASE_CHAT_MODEL              | decapoda-research/llama-7b-hf             | Base chat model                            |\n| USE_PEFT                     | True                                      | Use PEFT                                   |\n| ADAPTER_CHAT_MODEL           | tloen/alpaca-lora-7b                      | Adapter chat model                         |\n| ADAPTER_APPLY_OPTIMIZATIONS  | True                                      | Apply Torch optimizations                  |\n| CPU_MODEL_REPO               | Sosaka/Alpaca-native-4bit-ggml            | CPU model repository                       |\n| CPU_MODEL_FILENAME           | ggml-alpaca-7b-q4.bi                      | CPU model filename                         |\n| CPU_MODEL_THREADS            | 8                                         | CPU model threads                          |\n| CPU_MODEL_KV_16              | True                                      | CPU model use f16 for KV-Store             |\n\n### UI:\n\n| Environment Variable      | Default                                                                                                          |\n|---------------------------|----------------------------------------------------------------------------------------------------------------------|\n| SYSTEM_PROMPT             | The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know. \\n\\n Current Conversation: |\n| CHAT_WELCOME_MESSAGE      | Hello, I will try to answer any questions you have for me! Untick the checkbox to disable the document search and chat with me normally. |\n| WELCOME_MESSAGE           | This demo was initialized with about 500,000 English Wikipedia articles from April 1st, 2023. Feel free to ask the system about any topic you like. \\n\\n ⚠️CAUTION: If offline models are used, no safety layers are in place. If you ask the system about an offensive topic, it will answer you, even if the answer is immoral!⚠️ |\n| API_HOST                  | localhost |\n| API_PORT                  | 8001 |\n| ENABLE_ADMIN              | False |\n\n\n### Wiki-Importer:\n\n| Environment Variable   | Default      | Description                  |\n|------------------------|--------------|------------------------------|\n| ELASTIC_HOST           | localhost    | Elasticsearch host address   |\n| ELASTIC_PORT           | 9200         | Elasticsearch port number    |\n| ELASTIC_EMBEDDING_DIM  | 384          | Embedding dimension          |\n| CACHE_DIR              | ./importer_cache | Cache directory path     |\n| WIKI_URL              | https://dumps.wikimedia.org/simplewiki/20230401/simplewiki-20230401-pages-articles-multistream.xml.bz2 | URL of the Wiki-dump to download    |\n\n### ST4-Importer:\n| Environment Variable   | Default      | Description                  |\n|------------------------|--------------|------------------------------|\n| ELASTIC_HOST           | localhost    | Elasticsearch host address   |\n| ELASTIC_PORT           | 9200         | Elasticsearch port number    |\n| ELASTIC_EMBEDDING_DIM  | 384          | Embedding dimension          |\n| ST4_FOLDER             | ./.st4_files | ST4 files folder path        |\n"
    },
    {
      "name": "johnnygreco/hpqa",
      "stars": 17,
      "img": "https://avatars.githubusercontent.com/u/10998105?s=40&v=4",
      "owner": "johnnygreco",
      "repo_name": "hpqa",
      "description": "🪄 Harry Potter QA with GPT 🤖",
      "homepage": "",
      "language": "Python",
      "created_at": "2023-02-04T17:11:28Z",
      "updated_at": "2024-01-13T08:41:06Z",
      "topics": [],
      "readme": "# `hpqa` – 🪄 Harry Potter QA with GPT 🤖\n\n![harry-potter-sorcerer-stone](https://user-images.githubusercontent.com/10998105/217035363-3d079a9e-3333-4e5d-a2a6-98972060c071.gif)\n\n---\n\n# Overview\n\nThis application gives [OpenAI's GPT-3](https://platform.openai.com/docs/models/gpt-3) access to all the Harry Potter books (plus the associated Wikipedia plot summaries) and lets you ask it questions. You can give the model more butterbeer (i.e., increase the temperature 😉) to make its answers more unpredictable 🍻.\n\nUnder the hood, we index the text using [Faiss](https://github.com/facebookresearch/faiss) and streamline interactions with GPT using [LangChain](https://github.com/hwchase17/langchain).\n\n**Note:** To use the app, you'll need an [OpenAI API key](https://openai.com/api/). \n\n## Try the app on 🤗 Spaces \n👉  [https://huggingface.co/spaces/johnnygreco/the-gpt-who-lived](https://huggingface.co/spaces/johnnygreco/the-gpt-who-lived)\n\n\n# Installation\n```shell\ngit clone https://github.com/johnnygreco/hpqa.git\ncd hpqa\npython -m pip install -e .\n```\n\n# Running the app\n```shell\npython app.py\n```\n\n<img width=\"1000\" alt=\"gradio-demo\" src=\"https://github.com/johnnygreco/hpqa/assets/10998105/60d52b20-bd8a-468c-84d0-8191057c15b0\">\n"
    },
    {
      "name": "jamescalam/aurelius",
      "stars": 17,
      "img": "https://avatars.githubusercontent.com/u/35938317?s=40&v=4",
      "owner": "jamescalam",
      "repo_name": "aurelius",
      "description": null,
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2021-04-09T08:44:01Z",
      "updated_at": "2025-02-28T06:13:51Z",
      "topics": [],
      "readme": "# Aurelius\n\nFun Q&A project building Stoic AI\n\n## Notes\n\n### Running API\n\nNavigate to `/qa_service` and type:\n\n```\nuvicorn api:APP --reload\n```\n\n### ImportError on Windows\nTo fix `ImportError` on Windows due to lack of FAISS (`_swigfaiss`), edit the `__init__.py` file found in:\n\n```\n~/AppData/Local/Programs/Python/Python39/Lib/site-packages/haystack/document_store\n```\n\nand comment out the FAISS import, which will leave the file looking like:\n\n```\nfrom haystack.document_store.elasticsearch import ElasticsearchDocumentStore\n#from haystack.document_store.faiss import FAISSDocumentStore\nfrom haystack.document_store.memory import InMemoryDocumentStore\nfrom haystack.document_store.milvus import MilvusDocumentStore\nfrom haystack.document_store.sql import SQLDocumentStore\n```"
    },
    {
      "name": "Not-Diamond/notdiamond-examples",
      "stars": 17,
      "img": "https://avatars.githubusercontent.com/u/136373660?s=40&v=4",
      "owner": "Not-Diamond",
      "repo_name": "notdiamond-examples",
      "description": "Various examples for using Not Diamond to route model prompts.",
      "homepage": null,
      "language": "Python",
      "created_at": "2024-09-25T02:34:04Z",
      "updated_at": "2025-04-14T06:14:17Z",
      "topics": [],
      "readme": "# notdiamond-examples\n\n## Getting started\n\n```shell\npyenv virtualenv 3.11 notdiamond-examples\npyenv activate notdiamond-examples\n```\n\nInstall the dependencies:\n\n```shell\npoetry install\n```\n\n## Exploring Not Diamond\n\nCreate a `.env` file from the included template, making sure to populate keys for the providers\nyou would like to use. Then run the app:\n\n```shell\nstreamlit run notdiamond_examples/streamlit/main.py\n```\n\nThe app suggests some models for you to use, but you can also edit the app code to add [any\nmodel supported by Not Diamond][supported].\n\n## Testing RAG with Not Diamond\n\nNot Diamond also supports RAG workflows. Try our RAG example out:\n\n```shell\npoetry install --with rag\nstreamlit run notdiamond_examples/streamlit/rag.py\n```\n\n### Query parameters\n\nConcatenate these to the URL as follows:\n\n```text\nhttps://rag.notdiamond.ai?first_param=one&second_param=two\n```\n\nSupported parameters:\n\n- `repo`: index a Github repo of the form `{user}/{project}`, eg. `Not-Diamond/notdiamond-python`.\n- `repo_dir`: index all files in the Github repo originating in this directory (typically, `doc` or `docs`). Use `.` for all files.\n- `repo_ext`: index all files with this extension (examples: `md`, `mdx`, `txt`, `rst`). You can specify multiple `repo_ext` parameters.\n- `website`: scrape and index the content of the specified website (example: `https://example.com`).\n- `follow_link`: scrape and index the content of the pages linked to from the website parameter. Set to `false` by default, but accepts `true`.\n- `exclude_header_footer`: scrape and index the content of the pages linked to from the website parameter, and limit scraped links to 30. Set to `true` by default, but accepts `false`.\n- `message`: a pre-populated background message in the text box (example: `Hello%20world`).\n\nExample query parameters:\n\n```text\n# Index all .py files from the examples/ dir of posit-dev/py-shiny\nhttps://rag.notdiamond.ai/?repo=posit-dev/py-shiny&repo_dir=examples&repo_ext=py\n\n# Index all .mdx files from the docs/ dir of langchain-ai/langchain\nhttps://rag.notdiamond.ai/?repo=langchain-ai/langchain&repo_dir=docs&repo_ext=mdx\n```\n\n## Chat with Not Diamond\n\nWant to test out image generation? Watch the most popular models battle in Arena Mode? [Chat with Not Diamond].\n\n<p align=\"center\">\n  <img src=\"./chat_nd.png\" alt=\"Arena Mode in Chat\">\n</p>\n\n## Support\n\nFor support please check out the [docs] or send us a [message].\n\n[supported]: https://notdiamond.readme.io/docs/llm-models\n[docs]: https://notdiamond.readme.io\n[message]: mailto:support@notdiamond.ai\n[Chat with Not Diamond]: https://chat.notdiamond.ai"
    },
    {
      "name": "Guest400123064/bbm25-haystack",
      "stars": 17,
      "img": "https://avatars.githubusercontent.com/u/31812718?s=40&v=4",
      "owner": "Guest400123064",
      "repo_name": "bbm25-haystack",
      "description": "Simple Haystack in-memory document store alternative that performs incremental indexing and supports SentencePiece tokenizer.",
      "homepage": "https://guest400123064.github.io/bbm25-haystack/",
      "language": "Python",
      "created_at": "2024-03-28T05:10:02Z",
      "updated_at": "2025-02-04T04:38:01Z",
      "topics": [
        "bm25-plus",
        "haystack-ai",
        "information-retrieval",
        "llm",
        "rag"
      ],
      "readme": "[![test](https://github.com/Guest400123064/bbm25-haystack/actions/workflows/test.yml/badge.svg)](https://github.com/Guest400123064/bbm25-haystack/actions/workflows/test.yml)\n[![codecov](https://codecov.io/gh/Guest400123064/bbm25-haystack/graph/badge.svg?token=IGRIRBHZ3U)](https://codecov.io/gh/Guest400123064/bbm25-haystack)\n[![code style - Black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)\n[![types - Mypy](https://img.shields.io/badge/types-Mypy-blue.svg)](https://github.com/python/mypy)\n[![Python 3.9](https://img.shields.io/badge/python-3.9%20|%203.10%20|%203.11%20|%203.12-blue.svg)](https://www.python.org/downloads/release/python-390/)\n\n# Better BM25 In-Memory Document Store\n\nAn in-memory document store is a great starting point for prototyping and debugging before migrating to production-grade stores like Elasticsearch. However, [the original implementation](https://github.com/deepset-ai/haystack/blob/0dbb98c0a017b499560521aa93186d0640aab659/haystack/document_stores/in_memory/document_store.py#L148) of BM25 retrieval recreates an inverse index for the entire document store __on every new search__. Furthermore, the tokenization method is primitive, only permitting splitters based on regular expressions, making localization and domain adaptation challenging. Therefore, this implementation is a slight upgrade to the default BM25 in-memory document store by implementing incremental index update and incorporation of [SentencePiece](https://github.com/google/sentencepiece) statistical sub-word tokenization.\n\n## Installation\n\n```bash\n$ pip install bbm25-haystack\n```\n\nAlternatively, you can clone the repository and build from source to be able to reflect changes to the source code:\n\n```bash\n$ git clone https://github.com/Guest400123064/bbm25-haystack.git\n$ cd bbm25-haystack\n$ pip install -e .\n```\n\n## Usage\n\n### Quick Start\n\nBelow is an example of how you can build a minimal search engine with the `bbm25_haystack` components on their own. They are also compatible with [Haystack pipelines](https://docs.haystack.deepset.ai/docs/creating-pipelines).\n\n```python\nfrom haystack import Document\nfrom bbm25_haystack import BetterBM25DocumentStore, BetterBM25Retriever\n\n\ndocument_store = BetterBM25DocumentStore()\ndocument_store.write_documents([\n   Document(content=\"There are over 7,000 languages spoken around the world today.\"),\n   Document(content=\"Elephants have been observed to behave in a way that indicates a high level of self-awareness, such as recognizing themselves in mirrors.\"),\n   Document(content=\"In certain parts of the world, like the Maldives, Puerto Rico, and San Diego, you can witness the phenomenon of bio-luminescent waves.\")\n])\n\nretriever = BetterBM25Retriever(document_store)\nretriever.run(query=\"How many languages are spoken around the world today?\")\n```\n\n### API References\n\nYou can find the full API references [here](https://guest400123064.github.io/bbm25-haystack/). In a hurry? Below are some most important document store parameters you might want explore:\n\n- `k, b, delta` - the [three BM25+ hyperparameters](https://en.wikipedia.org/wiki/Okapi_BM25).\n- `sp_file` - a path to a trained SentencePiece tokenizer `.model` file. The default tokenizer is directly copied from [LLaMA-2-7B-32K tokenizer](https://huggingface.co/togethercomputer/LLaMA-2-7B-32K/blob/main/tokenizer.model) with a vocab size of 32,000.\n- `n_grams` - default to 1, which means text (both query and document) are tokenized into uni-grams. If set to 2, the tokenizer also augment the list of uni-grams with bi-grams, and so on. If specified as tuple, e.g., (2, 3), the tokenizer only produce bi-grams and tri-grams, without any uni-gram.\n- `haystack_filter_logic` - see [below](#filtering-logic).\n\nThe retriever parameters are largely the same as [`InMemoryBM25Retriever`](https://docs.haystack.deepset.ai/docs/inmemorybm25retriever).\n\n## Filtering Logic\n\nThe current document store uses [`document_matches_filter`](https://github.com/deepset-ai/haystack/blob/main/haystack/utils/filters.py) shipped with Haystack to perform filtering by default, which is the same as [`InMemoryDocumentStore`](https://docs.haystack.deepset.ai/docs/inmemorydocumentstore).\n\nHowever, there is also an alternative filtering logic shipped with this implementation (unstable at this point). To use this alternative logic, initialize the document store with `haystack_filter_logic=False`. Please find comments and implementation details in [`filters.py`](./src/bbm25_haystack/filters.py). TL;DR:\n\n- Comparison with `None`, i.e., missing values, involved will always return `False`, no matter missing the document attribute value or missing the filter value.\n- Comparison with `pandas.DataFrame` is always prohibited to reduce surprises.\n- No implicit `datetime` conversion from string values.\n- `in` and `not in` allows any `Iterable` as filter value, without the `list` constraint.\n- Allowing custom comparison functions for more flexibility. Note that the custom comparison function inputs are NEVER checked, i.e., no missing value check, no ``DataFrame`` check, etc. User should ensure the input values are valid and return value is always a boolean. The inputs are always supplied in the order of document value and then filter value.\n\nIn this case, the negation logic needs to be considered again because `False` can now issue from both input nullity check and the actual comparisons. For instance, `in` and `not in` both yield non-matching upon missing values. But I think having input processing and comparisons separated makes the filtering behavior more transparent.\n\n## Search Quality Evaluation\n\nThis repo has [a simple script](./scripts/benchmark_beir.py) to help evaluate the search quality over [BEIR](https://github.com/beir-cellar/beir/tree/main) benchmark. You need to clone the repository (you can also manually download the script and place it under a folder named `scripts`) and you have to install additional dependencies to run the script.\n\n```bash\n$ pip install beir\n```\n\nTo run the script, you may want to specify the dataset name and BM25 hyperparameters. For example:\n\n```bash\n$ python scripts/benchmark_beir.py --datasets scifact arguana --bm25-k1 1.2 --n-grams 2 --output eval.csv\n```\n\nIt automatically downloads the benchmarking dataset to `benchmarks/beir`, where `benchmarks` is at the same level as `scripts`. You may also check the help page for more information.\n\n```bash\n$ python scripts/benchmark_beir.py --help\n```\n\nNew benchmarking scripts are expected to be added in the future.\n\n## License\n\n`bbm25-haystack` is distributed under the terms of the [Apache-2.0](https://spdx.org/licenses/Apache-2.0.html) license.\n"
    },
    {
      "name": "PacktPublishing/Building-Natural-Language-Pipelines",
      "stars": 17,
      "img": "https://avatars.githubusercontent.com/u/10974906?s=40&v=4",
      "owner": "PacktPublishing",
      "repo_name": "Building-Natural-Language-Pipelines",
      "description": "Building RAG Applications with Haystack 2.0 published by Packt",
      "homepage": "",
      "language": "Jupyter Notebook",
      "created_at": "2023-09-04T05:56:17Z",
      "updated_at": "2025-03-05T15:55:43Z",
      "topics": [
        "indexing-querying",
        "llm",
        "python",
        "rag",
        "reproducible-systems"
      ],
      "readme": "# Building RAG Applications with Haystack 2.0\nSupplementary material for the book \"Building Natural Language Pipelines\" published by Packt\n\nAuthor: Laura Funderburk\n\n## Chapter breakdown\n\n* [ch1](./ch1/) - \"Introduction to Natural Language Processing (NLP) pipelines\"\n* [ch2](./ch2/) - \"Diving deep into Large Language Models (LLMs)\"\n* [ch3](./ch3/) - \"Introduction to Haystack by deepset\"\n* [ch4](./ch4/) - \"Bringing components together: Haystack pipelines for different use cases\"\n* [ch5](./ch5/) - \"Haystack pipeline development with custom components\"\n* [ch6](./ch6/) - \"Setting up a reproducible project: question and answer pipeline\"\n* [ch7](./ch7/) - \"Deploying Haystack-based applications\"\n* [ch8](./ch8/) - \"Hands-on projects\"\n\n## Setting up\n\nSet up a virtual environment and install the required packages:\n\n### Set up - for Jupyter notebook usage\n\nIf you have completed the following, you may discard this information. Otherwise, as a reminder and to ease installation, you can follow the instructions below.  \n\nThroughout this book we will be using `pip`, `conda` and `just` for package management. We will also create an isolated `conda` environment with Python 3.10.  \n\nWe recommend that you install Miniconda and VSCode. We also recommend that you install GitHub (GitBash for Windows or Git for Linux and Mac) to make the process of accessing the material locally easier.   \n\n* Install Miniconda: https://docs.conda.io/projects/miniconda/en/latest/  \n\n* Install VSCode: https://code.visualstudio.com/docs/setup/setup-overview  \n\nTo obtain the code and exercises, clone the repository: \n\nOpen VSCode, Click File-> New Window, then Terminal ->New Terminal. Ensure your terminal is of type “Bash” or “Command line”.  \n\nWithin the terminal, type each of the commands (a command is identified by the $ sign) below, one by one. Then press enter.  \n\n```bash\n\n$ git clone https://github.com/PacktPublishing/Building-Natural-Language-Pipelines.git \n\n$ cd building-RAG-applications/ \n\n$ conda create –-name llm-pipelines python==3.12\n\n$ conda activate llm-pipelines \n\n$ pip install haystack-ai, ipykernel, ipytthon\n```\n\nEnable the Jupyter Notebook extension on VSCode through the extension marketplace. When you open a notebook, press on ‘Select Kernel’ and click on `llm-pipeline` as our environment. \n\n### Advanced set up - for chapters 6 and higher\n\nPlease refer to the instructions in this [README](./ch6/README.md) for how to set up for advanced chapters\n\n\n\n"
    },
    {
      "name": "milvus-io/milvus-haystack",
      "stars": 16,
      "img": "https://avatars.githubusercontent.com/u/51735404?s=40&v=4",
      "owner": "milvus-io",
      "repo_name": "milvus-haystack",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2023-07-31T15:43:42Z",
      "updated_at": "2025-04-22T10:03:37Z",
      "topics": [],
      "readme": "# Milvus Document Store for Haystack\n\n[![PyPI - Version](https://img.shields.io/pypi/v/milvus-haystack.svg)](https://pypi.org/project/milvus-haystack)\n[![PyPI - Python Version](https://img.shields.io/pypi/pyversions/milvus-haystack.svg)](https://pypi.org/project/milvus-haystack)\n\n## Recent Updates\n\n- [2025.4.17] [Full-text Search with Milvus and Haystack](https://milvus.io/docs/full_text_search_with_milvus_and_haystack.md) - Learn how to implement full-text and hybrid search in your application using Haystack and Milvus\n\n## Installation\n\n```shell\npip install --upgrade pymilvus milvus-haystack\n```\n\n## Usage\n\nUse the `MilvusDocumentStore` in a Haystack pipeline as a quick start.\n\n```python\nfrom haystack import Document\nfrom milvus_haystack import MilvusDocumentStore\n\ndocument_store = MilvusDocumentStore(\n    connection_args={\"uri\": \"./milvus.db\"},\n    drop_old=True,\n)\ndocuments = [Document(\n    content=\"A Foo Document\",\n    meta={\"page\": \"100\", \"chapter\": \"intro\"},\n    embedding=[-10.0] * 128,\n)]\ndocument_store.write_documents(documents)\nprint(document_store.count_documents())  # 1\n```\n### Different ways to connect to Milvus\n\n- For the case of [Milvus Lite](https://milvus.io/docs/milvus_lite.md), the most convenient method, just set the uri as a local file.\n```python\ndocument_store = MilvusDocumentStore(\n    connection_args={\"uri\": \"./milvus.db\"},\n    drop_old=True,\n)\n```\n                \n- For the case of Milvus server on [docker or kubernetes](https://milvus.io/docs/quickstart.md), it is recommended to use when you are dealing with large scale of data. After starting the Milvus service, you can use the specified uri to connect to the service.\n```python\ndocument_store = MilvusDocumentStore(\n    connection_args={\"uri\": \"http://localhost:19530\"},\n    drop_old=True,\n)\n```\n\n- For the case of [Zilliz Cloud](https://zilliz.com/cloud), the fully managed cloud service for Milvus, adjust the uri and token, which correspond to the [Public Endpoint and Api key](https://docs.zilliz.com/docs/on-zilliz-cloud-console#free-cluster-details) in Zilliz Cloud.\n```python\nfrom haystack.utils import Secret\ndocument_store = MilvusDocumentStore(\n    connection_args={\n        \"uri\": \"https://in03-ba4234asae.api.gcp-us-west1.zillizcloud.com\",  # Your Public Endpoint\n        \"token\": Secret.from_env_var(\"ZILLIZ_CLOUD_API_KEY\"),  # API key, we recommend using the Secret class to load the token from env variable for security.\n        \"secure\": True\n    },\n    drop_old=True,\n)\n```\n\n\n\n## Dive deep usage\n\nPrepare an OpenAI API key and set it as an environment variable:\n\n```shell\nexport OPENAI_API_KEY=<your_api_key>\n```\n\n### Create the indexing Pipeline and index some documents\n\n```python\nimport glob\nimport os\n\nfrom haystack import Pipeline\nfrom haystack.components.converters import MarkdownToDocument\nfrom haystack.components.embedders import OpenAIDocumentEmbedder, OpenAITextEmbedder\nfrom haystack.components.preprocessors import DocumentSplitter\nfrom haystack.components.writers import DocumentWriter\n\nfrom milvus_haystack import MilvusDocumentStore\nfrom milvus_haystack.milvus_embedding_retriever import MilvusEmbeddingRetriever\n\ncurrent_file_path = os.path.abspath(__file__)\nfile_paths = [current_file_path]  # You can replace it with your own file paths.\n\ndocument_store = MilvusDocumentStore(\n    connection_args={\"uri\": \"./milvus.db\"},\n    drop_old=True,\n)\nindexing_pipeline = Pipeline()\nindexing_pipeline.add_component(\"converter\", MarkdownToDocument())\nindexing_pipeline.add_component(\"splitter\", DocumentSplitter(split_by=\"sentence\", split_length=2))\nindexing_pipeline.add_component(\"embedder\", OpenAIDocumentEmbedder())\nindexing_pipeline.add_component(\"writer\", DocumentWriter(document_store))\nindexing_pipeline.connect(\"converter\", \"splitter\")\nindexing_pipeline.connect(\"splitter\", \"embedder\")\nindexing_pipeline.connect(\"embedder\", \"writer\")\nindexing_pipeline.run({\"converter\": {\"sources\": file_paths}})\n\nprint(\"Number of documents:\", document_store.count_documents())\n```\n\n### Create the retrieval pipeline and try a query\n\n```python\nquestion = \"How to set the service uri with milvus lite?\"  # You can replace it with your own question. \n\nretrieval_pipeline = Pipeline()\nretrieval_pipeline.add_component(\"embedder\", OpenAITextEmbedder())\nretrieval_pipeline.add_component(\"retriever\", MilvusEmbeddingRetriever(document_store=document_store, top_k=3))\nretrieval_pipeline.connect(\"embedder\", \"retriever\")\n\nretrieval_results = retrieval_pipeline.run({\"embedder\": {\"text\": question}})\n\nfor doc in retrieval_results[\"retriever\"][\"documents\"]:\n    print(doc.content)\n    print(\"-\" * 10)\n```\n\n### Create the RAG pipeline and try a query\n\n```python\nfrom haystack.utils import Secret\nfrom haystack.components.builders import PromptBuilder\nfrom haystack.components.generators import OpenAIGenerator\n\nprompt_template = \"\"\"Answer the following query based on the provided context. If the context does\n                     not include an answer, reply with 'I don't know'.\\n\n                     Query: {{query}}\n                     Documents:\n                     {% for doc in documents %}\n                        {{ doc.content }}\n                     {% endfor %}\n                     Answer: \n                  \"\"\"\n\nrag_pipeline = Pipeline()\nrag_pipeline.add_component(\"text_embedder\", OpenAITextEmbedder())\nrag_pipeline.add_component(\"retriever\", MilvusEmbeddingRetriever(document_store=document_store, top_k=3))\nrag_pipeline.add_component(\"prompt_builder\", PromptBuilder(template=prompt_template))\nrag_pipeline.add_component(\"generator\", OpenAIGenerator(api_key=Secret.from_token(os.getenv(\"OPENAI_API_KEY\")),\n                                                        generation_kwargs={\"temperature\": 0}))\nrag_pipeline.connect(\"text_embedder.embedding\", \"retriever.query_embedding\")\nrag_pipeline.connect(\"retriever.documents\", \"prompt_builder.documents\")\nrag_pipeline.connect(\"prompt_builder\", \"generator\")\n\nresults = rag_pipeline.run(\n    {\n        \"text_embedder\": {\"text\": question},\n        \"prompt_builder\": {\"query\": question},\n    }\n)\nprint('RAG answer:', results[\"generator\"][\"replies\"][0])\n\n```\n\n## Sparse Retrieval\n### Sparse retrieval with haystack sparse embedder\nThis example demonstrates the basic approach to sparse indexing and retrieval using Haystack's sparse embedders.\n\n```python\nfrom haystack import Document, Pipeline\nfrom haystack.components.writers import DocumentWriter\nfrom haystack.document_stores.types import DuplicatePolicy\nfrom haystack_integrations.components.embedders.fastembed import (\n    FastembedSparseDocumentEmbedder,\n    FastembedSparseTextEmbedder,\n)\n\nfrom milvus_haystack import MilvusDocumentStore, MilvusSparseEmbeddingRetriever\n\ndocument_store = MilvusDocumentStore(\n    connection_args={\"uri\": \"./milvus.db\"},\n    sparse_vector_field=\"sparse_vector\",  # Specify a name of the sparse vector field to enable sparse retrieval.\n    drop_old=True,\n)\n\ndocuments = [\n    Document(content=\"My name is Wolfgang and I live in Berlin\"),\n    Document(content=\"I saw a black horse running\"),\n    Document(content=\"Germany has many big cities\"),\n    Document(content=\"full text search is supported by Milvus.\"),\n]\n\nsparse_document_embedder = FastembedSparseDocumentEmbedder()\nwriter = DocumentWriter(document_store=document_store, policy=DuplicatePolicy.NONE)\n\nindexing_pipeline = Pipeline()\nindexing_pipeline.add_component(\"sparse_document_embedder\", sparse_document_embedder)\nindexing_pipeline.add_component(\"writer\", writer)\nindexing_pipeline.connect(\"sparse_document_embedder\", \"writer\")\n\nindexing_pipeline.run({\"sparse_document_embedder\": {\"documents\": documents}})\n\nretrieval_pipeline = Pipeline()\nretrieval_pipeline.add_component(\"sparse_text_embedder\", FastembedSparseTextEmbedder())\nretrieval_pipeline.add_component(\"sparse_retriever\", MilvusSparseEmbeddingRetriever(document_store=document_store))\nretrieval_pipeline.connect(\"sparse_text_embedder.sparse_embedding\", \"sparse_retriever.query_sparse_embedding\")\n\nquery = \"who supports full text search?\"\n\nresult = retrieval_pipeline.run({\"sparse_text_embedder\": {\"text\": query}})\n\nprint(result[\"sparse_retriever\"][\"documents\"][0])\n\n# Document(id=..., content: 'full text search is supported by Milvus.', sparse_embedding: vector with 48 non-zero elements)\n```\n### Sparse retrieval with Milvus built-in BM25 function\nMilvus provides a built-in BM25 function that can generate sparse vectors directly from text fields. This approach simplifies the pipeline construction compared to using Haystack's sparse embedders. The main differences are:\n\n1. We need to specify a `BM25BuiltInFunction` in the document store with some field specification parameters.\n2. We don't need to use the embedder explicitly since Milvus handles the sparse embedding in the Milvus server end.\n3. The pipeline is simpler with fewer components and connections.\n\nBelow is a complete example using Milvus' built-in BM25 function. The code with `+` signs shows the simplified approach using Milvus' built-in functionality, while the code with `-` signs shows the original approach that requires explicit sparse embedding:\n\n```diff\n+ from milvus_haystack.function import BM25BuiltInFunction\n+ \n  document_store = MilvusDocumentStore(\n      connection_args={\"uri\": \"http://localhost:19530\"},\n      sparse_vector_field=\"sparse_vector\",\n      text_field=\"text\",\n+     builtin_function=[\n+         BM25BuiltInFunction(  # The BM25 function converts the text into a sparse vector.\n+             input_field_names=\"text\", output_field_names=\"sparse_vector\",\n+         )\n+     ],\n      drop_old=True,\n  )\n- sparse_document_embedder = FastembedSparseDocumentEmbedder()\n  writer = DocumentWriter(document_store=document_store, policy=DuplicatePolicy.NONE)\n  indexing_pipeline = Pipeline()\n- indexing_pipeline.add_component(\"sparse_document_embedder\", sparse_document_embedder)\n  indexing_pipeline.add_component(\"writer\", writer)\n- indexing_pipeline.connect(\"sparse_document_embedder\", \"writer\")\n- indexing_pipeline.run({\"sparse_document_embedder\": {\"documents\": documents}})\n+ indexing_pipeline.run({\"writer\": {\"documents\": documents}})\n  retrieval_pipeline = Pipeline()\n- retrieval_pipeline.add_component(\"sparse_text_embedder\", FastembedSparseTextEmbedder())\n  retrieval_pipeline.add_component(\"sparse_retriever\", MilvusSparseEmbeddingRetriever(document_store=document_store))\n- retrieval_pipeline.connect(\"sparse_text_embedder.sparse_embedding\", \"sparse_retriever.query_sparse_embedding\")\n  query = \"who supports full text search?\"\n- result = retrieval_pipeline.run({\"sparse_text_embedder\": {\"text\": query}})\n+ result = retrieval_pipeline.run({\"sparse_retriever\": {\"query_text\": query}})\n  print(result[\"sparse_retriever\"][\"documents\"][0])\n```\n\n\n## Hybrid Retrieval\n### Hybrid retrieval with haystack sparse embedder\nThis example demonstrates the basic approach to perform hybrid retrieval using Haystack's sparse embedders.\n```python\nfrom haystack import Document, Pipeline\nfrom haystack.components.embedders import OpenAIDocumentEmbedder, OpenAITextEmbedder\nfrom haystack.components.writers import DocumentWriter\nfrom haystack.document_stores.types import DuplicatePolicy\nfrom haystack_integrations.components.embedders.fastembed import (\n    FastembedSparseDocumentEmbedder,\n    FastembedSparseTextEmbedder,\n)\n\nfrom milvus_haystack import MilvusDocumentStore, MilvusHybridRetriever\n\ndocument_store = MilvusDocumentStore(\n    connection_args={\"uri\": \"./milvus.db\"},\n    drop_old=True,\n    sparse_vector_field=\"sparse_vector\",  # Specify a name of the sparse vector field to enable hybrid retrieval.\n)\n\ndocuments = [\n    Document(content=\"My name is Wolfgang and I live in Berlin\"),\n    Document(content=\"I saw a black horse running\"),\n    Document(content=\"Germany has many big cities\"),\n    Document(content=\"full text search is supported by Milvus.\"),\n]\n\nwriter = DocumentWriter(document_store=document_store, policy=DuplicatePolicy.NONE)\n\nindexing_pipeline = Pipeline()\nindexing_pipeline.add_component(\"sparse_doc_embedder\", FastembedSparseDocumentEmbedder())\nindexing_pipeline.add_component(\"dense_doc_embedder\", OpenAIDocumentEmbedder())\nindexing_pipeline.add_component(\"writer\", writer)\nindexing_pipeline.connect(\"sparse_doc_embedder\", \"dense_doc_embedder\")\nindexing_pipeline.connect(\"dense_doc_embedder\", \"writer\")\n\nindexing_pipeline.run({\"sparse_doc_embedder\": {\"documents\": documents}})\n\nretrieval_pipeline = Pipeline()\nretrieval_pipeline.add_component(\"sparse_text_embedder\",\n                                FastembedSparseTextEmbedder(model=\"prithvida/Splade_PP_en_v1\"))\n\nretrieval_pipeline.add_component(\"dense_text_embedder\", OpenAITextEmbedder())\nretrieval_pipeline.add_component(\n    \"retriever\",\n    MilvusHybridRetriever(\n        document_store=document_store,\n        # reranker=WeightedRanker(0.5, 0.5),  # Default is RRFRanker()\n    )\n)\n\nretrieval_pipeline.connect(\"sparse_text_embedder.sparse_embedding\", \"retriever.query_sparse_embedding\")\nretrieval_pipeline.connect(\"dense_text_embedder.embedding\", \"retriever.query_embedding\")\n\nquestion = \"who supports full text search?\"\n\nresults = retrieval_pipeline.run(\n    {\"dense_text_embedder\": {\"text\": question},\n     \"sparse_text_embedder\": {\"text\": question}}\n)\n\nprint(results[\"retriever\"][\"documents\"][0])\n\n# Document(id=..., content: 'full text search is supported by Milvus.', embedding: vector of size 1536, sparse_embedding: vector with 48 non-zero elements)\n```\n### Hybrid retrieval with Milvus built-in BM25 function\nMilvus provides a built-in BM25 function that can generate sparse vectors directly from text fields. This approach simplifies the pipeline construction compared to using Haystack's sparse embedders, making it a useful complement to semantic search. The main differences are:\n\n1. We need to specify a `BM25BuiltInFunction` in the document store with some field specification parameters.\n2. We don't need to use the embedder explicitly since Milvus handles the sparse embedding in the Milvus server end.\n3. The pipeline is simpler with fewer components and connections, which is especially beneficial in hybrid retrieval setups.\n\nBelow is a complete example using Milvus' built-in BM25 function for hybrid retrieval. The code with `+` signs shows the simplified approach using Milvus' built-in functionality, while the code with `-` signs shows the original approach that requires explicit sparse embedding:\n\n```diff\n+ from milvus_haystack.function import BM25BuiltInFunction\n+ \n  document_store = MilvusDocumentStore(\n      connection_args={\"uri\": \"http://localhost:19530\"},\n      sparse_vector_field=\"sparse_vector\",\n      text_field=\"text\",\n+     builtin_function=[\n+         BM25BuiltInFunction(  # The BM25 function converts the text into a sparse vector.\n+             input_field_names=\"text\", output_field_names=\"sparse_vector\",\n+         )\n+     ],\n      drop_old=True,\n  )\n- sparse_document_embedder = FastembedSparseDocumentEmbedder()\n  writer = DocumentWriter(document_store=document_store, policy=DuplicatePolicy.NONE)\n  indexing_pipeline = Pipeline()\n- indexing_pipeline.add_component(\"sparse_document_embedder\", sparse_document_embedder)\n  indexing_pipeline.add_component(\"writer\", writer)\n- indexing_pipeline.connect(\"sparse_document_embedder\", \"writer\")\n- indexing_pipeline.run({\"sparse_document_embedder\": {\"documents\": documents}})\n+ indexing_pipeline.run({\"writer\": {\"documents\": documents}})\n  retrieval_pipeline = Pipeline()\n- retrieval_pipeline.add_component(\"sparse_text_embedder\", FastembedSparseTextEmbedder())\n  retrieval_pipeline.add_component(\"sparse_retriever\", MilvusSparseEmbeddingRetriever(document_store=document_store))\n- retrieval_pipeline.connect(\"sparse_text_embedder.sparse_embedding\", \"sparse_retriever.query_sparse_embedding\")\n  query = \"who supports full text search?\"\n- result = retrieval_pipeline.run({\"sparse_text_embedder\": {\"text\": query}})\n+ result = retrieval_pipeline.run({\"sparse_retriever\": {\"query_text\": query}})\n  print(result[\"sparse_retriever\"][\"documents\"][0])\n```\n\n\n## License\n\n`milvus-haystack` is distributed under the terms of the [Apache-2.0](https://spdx.org/licenses/Apache-2.0.html) license."
    },
    {
      "name": "jfagan/llm-rag-invoice-haystack",
      "stars": 16,
      "img": "https://avatars.githubusercontent.com/u/133829?s=40&v=4",
      "owner": "jfagan",
      "repo_name": "llm-rag-invoice-haystack",
      "description": "Haystack 2, Docker, and Llama 2, convert invoice PDFs into a local vector database and query for detailed insights.",
      "homepage": null,
      "language": "Python",
      "created_at": "2024-04-13T20:30:34Z",
      "updated_at": "2025-02-07T14:59:54Z",
      "topics": [],
      "readme": "# RAG Invoice Data Processing with Llama2, Haystack 2, & Docker\n\n## Requirements\n\nTo run this project, you will need a Hugging Face API token, which should be set in your local environment as follows:\n\nYou will need to have a Hugging Face API token in your local environment:\n\n`HF_API_TOKEN=''`\n\n\n## Quickstart\n\n### RAG runs on Haystack, Weaviate, and HuggingFace\n\n1. Build the Docker images, containers, and services:\n\n`docker-compose up --build`\n\n2. (Optional) Copy text PDF files to the `data` folder. (An example invoice is provided for demo/testing purposes.)\n\n3. Open a new CLI tab and perform the following to SSH into your running container:\n\n- Retrieve your `CONTAINER_ID` with `docker ps`.\n- Access the container using `docker exec -it CONTAINER_ID bash`.\n\n4. Run the script to convert PDF documents to vector embeddings and save them in Weaviate vector storage:\n\n`python ingest.py`\n\n5. Run the following script to process inquiries about the data and fetch the answers:\n\n`python main.py \"What is the invoice number value?\"`\n\n## Examples \n\n```\npython main.py \"What is the invoice seller name, address and tax ID? use this format for the answer {\\\"seller_name\\\": {},\\\"address\\\": {},\\\"tax_id\\\": {}}\"\n\nAnswer:\n {\"seller_name\": \"Chapman, Kim and Green\", \"address\": \"64731 James Branch Smithmouth, NC 26872\", \"tax_id\": \"949-84-9105\"}\n==================================================\nTime to retrieve answer: 3.551683001991478\n```\n\n\n```\npython main.py \"retrieve invoice IBAN in the format {\\\"invoice_iban\\\": {}}\"\n\nAnswer:\n{\"invoice_iban\": {\"GB50ACIE59715038217063\"}}\n==================================================\nTime to retrieve answer: 9.18394808798621\n```\n\n\n```\npython main.py \"retrieve two values: net price and gross worth for the second invoice item in this format: {\\\"net_price\\\": {},\\\"gross_worth\\\": {}}\"\n\nAnswer:\n{\"net_price\": {\"1.00\": 7.50},\"gross_worth\": {\"1.00\": 12.99}}\n==================================================\nTime to retrieve answer: 3.623518834996503\n```\n\n\n## Credits\n\nThis project is based on [Andrej Baranovskij's original work](https://github.com/katanaml/llm-rag-invoice-cpu).\n\n## Changes\n\n- The code has been refactored to integrate with Haystack 2, and now utilizes Llama2 hosted on HuggingFace. This change enhances response times and simplifies the architecture by eliminating the need for a local LLM.\n- The application is fully containerized using Docker.\n- Response times have significantly improved, and all scenarios in the `prompts-structured` directory are now generating answers."
    },
    {
      "name": "TuanaCelik/unstructuredio-haystack",
      "stars": 16,
      "img": "https://avatars.githubusercontent.com/u/15802862?s=40&v=4",
      "owner": "TuanaCelik",
      "repo_name": "unstructuredio-haystack",
      "description": "💙 Unstructured Data Connectors for Haystack 2.0",
      "homepage": "https://haystack.deepset.ai/integrations",
      "language": "Python",
      "created_at": "2023-08-25T12:21:30Z",
      "updated_at": "2024-07-14T03:41:42Z",
      "topics": [
        "haystack",
        "llm",
        "nlp",
        "python",
        "unstructured-data"
      ],
      "readme": "# Unstructured Haystack\n\n[![PyPI - Version](https://img.shields.io/pypi/v/unstructured-haystack.svg)](https://pypi.org/project/unstructured-haystack)\n[![PyPI - Python Version](https://img.shields.io/pypi/pyversions/unstructured-haystack.svg)](https://pypi.org/project/unstructured-haystack)\n\n-----\n\n## Unstructured Connectors for Haystack\n\nThis is an example Haystack 2.0 integration. It's an integration for Unstructured.io connectors. Please contribute 🚀\n\nThe current version has 2 available Unstructured connectors:\n- **Discord**: `UnstructuredDiscordConnector`\n- **GitHub**: `UnstructuredGitHubConnector`\n- **Google Drive**: `UnstructuredGoogleDriveConnector`\n\n## How to use in a Haystack 2.0 Pipeline \nFor example, you can write documents fetched from Discord using the `UnstructuredDiscordConnector`:\n\n```python\nfrom haystack.preview import Pipeline\nfrom haystack.preview.components.writers import DocumentWriter\nfrom unstructured_haystack import UnstructuredDiscordConnector\nfrom chroma_haystack import ChromaDocumentStore\n\n# Chroma is used in-memory so we use the same instances in the two pipelines below\ndocument_store = ChromaDocumentStore()\nconnector = UnstructuredDiscordConnector(api_key=\"UNSTRUCTURED_API_KEY\", discord_token=\"DISCORD_TOKEN\")\n\nindexing = Pipeline()\nindexing.add_component(\"connector\", connector)\nindexing.add_component(\"writer\", DocumentWriter(document_store))\nindexing.connect(\"connector.documents\", \"writer.documents\")\nindexing.run({\"connector\": {\"channels\" : \"993539071815200889\", \"period\": 3, \"output_dir\" : \"discord-example\"}})\n```"
    },
    {
      "name": "ShuHuang/chemdatawriter",
      "stars": 14,
      "img": "https://avatars.githubusercontent.com/u/44169687?s=40&v=4",
      "owner": "ShuHuang",
      "repo_name": "chemdatawriter",
      "description": "ChemDataWriter is a transformer-based library for automatically generating research books in the chemistry area.",
      "homepage": null,
      "language": "HTML",
      "created_at": "2023-09-22T10:05:25Z",
      "updated_at": "2025-03-26T01:40:52Z",
      "topics": [],
      "readme": "# ChemDataWriter\nChemDataWriter is a transformer-based library for automatically generating research books in the chemistry area.\n\n# Features\n- ```reader.py``` processes the raw XML/HTML paper files and converts them into a Reader object.\n- ```finder.py``` suggests the key topics of the paper corpus\n- ```retrieve.py``` retrieves the papers from the corpus based on the key topics\n- ```summariser.py``` summarises the papers based on the input text\n- ```paraphraser.py``` paraphrases the summarised text\n- ```title_generator.py``` generates the short title in terms of the original long title\n- ```latex_generator.py``` reorganises the summarised text and converts it into a LaTeX file\n\n# Installation\nPlease install Python version 3.8.10 due to the compatibility of the dependencies.\n\nCreate the virtual environment and install the dependencies:\n\n```bash\npython3.8 -m venv venv\nsource venv/bin/activate\npip install -r requirements.txt\n```\n\nYou could also use ```pip``` to install the toolkit once the paper is published:\n\n```pip install chemdatawriter```  \n\nDocker image is also available:\n\n```bash\ndocker pull sh2009/dockerhub:chemdatawriter.v.0.0.1\ndocker run -it sh2009/dockerhub:chemdatawriter.v.0.0.1\n```\n\n\n# Usage\nTo create a clean paper file with the title, abstract, introduction, and conclusion from the HTML/XML files, and \nextract the reference information automatically, run:\n\n```bash\npython run_corpus.py \\\n  --input_path <path to the input directory> \\\n  --output_path <path to the output JSON file>\n```\n\nTo auto-generate the research book using the output JSON file above, run:\n\n```bash\npython run_cdw.py \\\n  --input_path <path to the input JSON file> \\\n  --output_path <path to the output JSON file> \\\n  --cache_path <path to the cache directory> \\\n  --keywords <keywords to screen the papers> \\\n  --topic_words <topic words of each chapter> \\\n  --chapter_size <number of papers in each chapter>\n```\n\n# Developer Guide\nDevelopers are welcome to contribute to the project by either raising an issue or submitting a pull request.\nPlease make sure the unit tests are passed before submitting a pull request.\n\nTo run the unit tests, run:\n\n```bash\npython -m unittest\n```\n\n# Evaluation\nWe also provide the evaluation scripts for the topic modelling and paraphrasing models. \nPlease refer to the ```demo.ipynb``` folder for more details. \nNote that you need to install additional dependencies to run the evaluation scripts according to the instructions in the notebook.\n\n# Acknowledgement\n\nThis project was financially supported by the [Science and Technology Facilities Council (STFC)](https://www.ukri.org/councils/stfc/), the [Royal Academy of Engineering](https://raeng.org.uk/) (RCSRF1819\\7\\10) and [Christ's College, Cambridge](https://www.christs.cam.ac.uk/). The Argonne Leadership Computing Facility, which is a [DOE Office of Science Facility](https://science.osti.gov/), is also acknowledged for use of its research resources, under contract No. DEAC02-06CH11357.\n\n# Citation\n\nS.Huang, J. M. Cole, ChemDataWriter: A Transformer-based Toolkit for Auto-generating Books That Summarise Research (2023) DOI: 10.1039/d3dd00159h\n\n"
    },
    {
      "name": "ArzelaAscoIi/haystack-github-bot",
      "stars": 14,
      "img": "https://avatars.githubusercontent.com/u/37148029?s=40&v=4",
      "owner": "ArzelaAscoIi",
      "repo_name": "haystack-github-bot",
      "description": "🤖 A GitHub bot that summarizes your actions throughout a day and lists all your achievements. Built with Haystack and OpenAI",
      "homepage": "",
      "language": "Python",
      "created_at": "2023-04-08T08:37:43Z",
      "updated_at": "2023-10-10T15:32:14Z",
      "topics": [
        "haystack",
        "llm",
        "python"
      ],
      "readme": "# 🤖 Github End-of-Day Bot\n\nA LLM powered github end-of-day bot powered by [Haystack](https://haystack.deepset.ai/) and [OpenAI](https://openai.com/)\n\n<img src=\"img/combined.gif\" width=\"800\">\n\n<p float=\"left\">\n  <img src=\"img/haystack.png\" height=\"50\" width=\"200\"/>\n  <img src=\"img/openai.jpeg\" height=\"50\"width=\"200\" /> \n</p>\n\nThis GitHub bot crawls your GitHub activity on a selected day, groups the events by action (push, pull request, issue, etc.), and sends you a summary of your activity in natural language. This message can be used as an EOD (End of Day) report in your Slack channel.\n\n> This bot should help me close my laptop and be happy about the work I've done that day. Hopefully, it will help you too.\n\n\n> :warning: **Please be careful with sensitive data**: You might be working on a project that contains sensitive data that should not leave your company. Please make sure what you are sending to OpenAI before running this bot!\n\nThis example of a haystack promptnode uses two templates to generate a summary of actions. The [first template](/prompts/events.txt) is used to generate a summary per event type from github. The [second template](/prompts/summary.txt) is used to generate the final summary of all events.\n\n\n## Limitations\nThe challenging part was to make GPT 'hallucinate' just the right amount to connect the dots but not make up facts. The bot is not perfect and will make mistakes. The most common issue I saw in my tests was that the bot lists irrelevant events/actions like `Pushed something to main.` There is still some room for improvement for the prompt templates.\n## How to use it\n### Installation \n1. Clone this repository\n2. Install the dependencies with `pip3 install -r requirements.txt`. This installs haystack and some other required packages to fetch the data from github.\n3. Create a new github access token.\n4. Create a OpenAI API key.\n\n### Run \nRun the bot with \n```sh \nGH_ACCESS_TOKEN=<your-token> OPENAI_KEY=<openai-key> python3 main.py\n```\n\n\n\n\n\n\n"
    },
    {
      "name": "Kohimax/qna-api",
      "stars": 14,
      "img": "https://avatars.githubusercontent.com/u/20102354?s=40&v=4",
      "owner": "Kohimax",
      "repo_name": "qna-api",
      "description": "https://kohinoor-soubam.medium.com/",
      "homepage": "",
      "language": "Python",
      "created_at": "2021-01-22T02:45:30Z",
      "updated_at": "2023-11-27T10:38:31Z",
      "topics": [
        "docker",
        "flask",
        "machine-learning",
        "nlp",
        "python"
      ],
      "readme": "# qna-api\nIts complete rest API for Question and Answering with any unstructure document. The API is developed using haystack framework. The following features are supported:\n1) Upload document to Elastic Document storage\n2) Pretrain and Trained QNA\n\n# Installation\n`docker build -t qna:v1 .`\n\n`docker run — name qna_app -d -p 8777:8777 xxxxxxxxx`\n\n#### Note : xxxxxxxxx is the image id\n\n# Training\n```python\nfrom haystack.reader.farm import FARMReader\n\n#input directory of the labels answers.json file\ntrain_data = \"/usr/src/app/data/squad20\"\n# output directory of the model\ntrain_model = \"/usr/src/app/data/train_model\"\n\nreader = FARMReader(model_name_or_path=\"distilbert-base-uncased-distilled-squad\", use_gpu=False)\n\nreader.train(\n    data_dir=train_data,\n    train_filename=\"answers.json\",\n    n_epochs=10,\n    dev_split = 0,\n    save_dir=train_model)\n\nprint('Training successfully completed')\n```\n# Testing\n```python\ndocument_store = ElasticsearchDocumentStore(host=app.config[\"host\"],\n                                                port=app.config[\"port\"],\n                                                username=app.config[\"username\"],\n                                                password=app.config[\"password\"],\n                                                index='electrical')\n\nretriever = DensePassageRetriever(document_store=document_store,\n                                  embedding_model=\"dpr-bert-base-nq\",\n                                  do_lower_case=True, use_gpu=False)\n\n# Farm reader using the trained model\nreader = FARMReader(model_name_or_path=app.config[\"train_model\"] ,use_gpu=False)\n\nfinder = Finder(reader, retriever)\n\nn = 5\nquestion=\"asked your query here?\"\nprediction = finder.get_answers(question=question, top_k_retriever=10, top_k_reader=n)\n```\n# Code Explaination\nThe complete step by step explaination are provided [here](https://medium.com/analytics-vidhya/how-to-create-your-question-and-answering-flask-api-using-haystack-e97205a240d1)\n\n"
    },
    {
      "name": "Mouez-Yazidi/WhisperMesh",
      "stars": 14,
      "img": "https://avatars.githubusercontent.com/u/56160970?s=40&v=4",
      "owner": "Mouez-Yazidi",
      "repo_name": "WhisperMesh",
      "description": "WhisperMesh is an advanced chatbot that integrates voice and text interactions, delivering personalized responses through LLM models and a sophisticated vector database. Leveraging the RAG framework from Haystack, it ensures engaging, data-driven conversations that adapt to your preferred style.",
      "homepage": "https://whispermesh.streamlit.app/",
      "language": "Python",
      "created_at": "2024-09-22T17:29:27Z",
      "updated_at": "2025-04-23T09:07:36Z",
      "topics": [
        "agent",
        "chatbot",
        "generative-ai",
        "haystack",
        "haystack-pipeline",
        "rag",
        "speach-to-text",
        "text-to-speech",
        "voice-assistant"
      ],
      "readme": "# WhisperMesh 🌐🔊\nWhisperMesh is your cutting-edge chatbot that seamlessly blends voice and text interactions, creating a rich, intuitive conversational experience. With the power of LLM models and a sophisticated vector database, WhisperMesh understands your needs like never before, providing tailored responses that resonate with your queries.\n\nHarnessing the RAG framework from Haystack, our app excels in extracting relevant information, ensuring that every interaction is not only engaging but also data-driven. Whether you prefer speaking or typing, WhisperMesh adapts to your style, transforming your input into insightful answers with a personal touch.\n\nJoin the conversation with WhisperMesh, where your voice matters, and let us guide you through a world of knowledge and discovery! 🌟💬✨\n\n# 💻 Technical architecture\nThe architecture outlines a Streamlit application integrated with the HayStack framework to facilitate document uploading, querying, and response generation. Users upload PDF documents through the app, which are then processed and indexed by HayStack components, including document cleaners, splitters, and embedders that convert documents into vector representations using the Cohere model, stored in the Qdrant vector database. When a user submits a voice query, it is transcribed to text, embedded into a vector, and matched against the stored document vectors to retrieve relevant chunks. These chunks are fed into the gemma2-9b-it generative model to generate a coherent response, which is then converted back to audio for user output. This system leverages advanced NLP techniques to ensure efficient and accurate document retrieval and response generation.\n![Example Image](technical_architecture.png)\n# 🎯 Key Features\n\n### 1. Seamless Voice and Text Interactions\nWhisperMesh offers a cutting-edge conversational experience by blending voice and text inputs, allowing you to communicate in the way that suits you best.\n\n### 2. Advanced LLM Integration\nPowered by state-of-the-art large language models (LLMs), WhisperMesh understands your needs deeply and provides tailored responses that resonate with your queries.\n\n###  3. Intelligent Vector Database\nUtilizing *Qdrant* as our vector database, WhisperMesh efficiently manages and retrieves relevant information, ensuring data-driven interactions that enhance user engagement.\n\n###  4. Cohere Embedding Model\nOur application harnesses the Cohere embedding model for effective semantic understanding, ensuring that every interaction is insightful and contextually aware.\n\n### 5. Robust Speech-to-Text Capabilities\nWith the *Whisper-large-v3* model from Groq Cloud, WhisperMesh excels at converting speech to text, making voice interactions smooth and accurate.\n\n### 6. Generative Model for Personalized Responses\nEmploying the *gemma-7b-it* model from Groq Cloud, our app generates personalized responses, adding a unique touch to every conversation.\n \n### 7. RAG Framework for Data-Driven Insights\nHarnessing the RAG (Retrieval-Augmented Generation) framework from *Haystack*, WhisperMesh excels in extracting relevant information, ensuring that your interactions are engaging and informative.\n\n# 🛠️ How it Works\n### 📄 Step 1: Upload Your PDF Documents and Index Them\n- **Upload Options:** Drag and drop your files or click on **'Browse Files'** to select them. (Maximum file size: **200MB**).\n- **Start Indexing:** Click the **'📄 Index Document'** button to initiate the indexing process.\n\n### ⚙️ Step 2: Choose Your Chat Method\n- Decide whether you prefer to chat via **text** or **voice**.\n\n### 💬 Step 3: Ask Your Questions\n- **Text Input:** Use the text box below to type your question. 📝\n- **Voice Input:** Click the **🎙️ microphone icon** to ask questions using your voice.\n\n![Example Image](app_guideline.png)\n\n# 🚀 Getting Started\n### Prerequisites\n* Python 3.11 or above 🐍\n* Groq API for inference, which is currently available for free in its beta version with rate limits. You can obtain your API key here after creating an account: [Groq API](https://medium.com/r/?url=https%3A%2F%2Fconsole.groq.com%2Fkeys).\n* Additionally, you have the option to use Qdrant either locally or via Qdrant Cloud. Its API is also free to use. Access it here: [Qdrant Cloud](https://medium.com/r/?url=https%3A%2F%2Fcloud.qdrant.io%2Flogin).\n* For embeddings, you can use the Cohere API. It offers a free tier and you can sign up and get your API key here: [Cohere API](https://medium.com/r/?url=https%3A%2F%2Fcohere.ai).\n# 💻 Local Deployment\n### 1. Clone the Repository\n```bash\ngit clone https://github.com/Mouez-Yazidi/WhisperMesh.git\ncd WhisperMesh\n```\n### 2. Add Environment Variables\n\n* Create a `.env` file and add the following variables according to the credentials you obtained from the required platforms:\n\n    ```plaintext\n    COHERE_API_KEY=\n    GROQ_API=\n    GROQ_KEY=\n    QDRANT_API=\n    QDRANT_KEY=\n    ```\n    \n### 3. Install Dependencies\nNavigate to the local directory and install the necessary dependencies:\n```bash\ncd local\npip install -r requirements.txt\n```\n\n### 4. Running the App Locally\nTo run the app locally, execute the following command:\n\n```bash\nstreamlit run ../app/main.py --environment local\n```\nYou should now be able to access the app at http://localhost:8501 🌐.\n\n### 🐳 Optional: Running with Docker\nIf you prefer running the app in a Docker container, follow these steps:\n1. Make sure you have Docker installed 🐋.\n2. Build the Docker image:\n```bash\ndocker build -t whispermesh -f Dockerfile  ..\n```\n3. Run the container:\n```bash\ndocker run -p 8501:8501 whispermesh streamlit run main.py --environment local\n```\n# ☁️ Streamlit Cloud Deployment\n### 1. Prepare Your Repository\nEnsure that your code is pushed to a GitHub repository 📂.\n\n### 2. Link with Streamlit Cloud\n* Visit Streamlit Cloud and sign in.\n* Connect your GitHub repository 🔗.\n* Choose your repository and branch.\n\n### 3. Environment Variables\n* Go to the \"Advanced settings\" section of your app.\n* In the \"Secrets\" section, input any sensitive information, such as API keys or other credentials.\n* Make sure to add this variables according to the credentials you obtained from the required platforms.\n```csharp\nCOHERE_API_KEY=\"\"\nGROQ_API=\"\"\nGROQ_KEY=\"\"\nQDRANT_API=\"\"\nQDRANT_KEY=\"\"\n```\nStreamlit Cloud will:\n* Install dependencies from cloud/requirements.txt 📦\n\n🎉 You’re all set! Your app will now be live on Streamlit Cloud!\n\n# 🌟 Features\n* Configurable: Separate configuration files for local and cloud deployment.\n* Docker Support: Deploy the app using Docker for local containerization.\n\n# ✨ Show Your Support!\nIf you appreciate this project, I would be grateful if you could give it a star on GitHub. Your support motivates us to enhance and expand our work!\n\n# 📄 License\nThis project is licensed under the MIT License. See the LICENSE file for more details.\n\n# 📧 Contact\nIf you have any questions or suggestions, feel free to open an issue or contact us at mouez.yazidi2016@gmail.com.\n"
    },
    {
      "name": "KhanhHua2102/Monetize.ai",
      "stars": 13,
      "img": "https://avatars.githubusercontent.com/u/46571355?s=40&v=4",
      "owner": "KhanhHua2102",
      "repo_name": "Monetize.ai",
      "description": "Monetize.ai is a web-based chatbot that provides personalized investment advice using GPT-3.5 and Yahoo Finance API. It's built using Flask, SQLite, HTML, CSS, JavaScript, Bootstrap, and jQuery. The project is open-source and welcomes contributions.",
      "homepage": "https://monetize-ai.herokuapp.com/home",
      "language": "Python",
      "created_at": "2023-03-17T13:57:33Z",
      "updated_at": "2025-03-26T05:59:53Z",
      "topics": [
        "chatbot",
        "finance-management",
        "financial-analysis",
        "investment-portfolio"
      ],
      "readme": "<a name=\"readme-top\"></a>\n\n<!-- PROJECT LOGO -->\n<br />\n<div align=\"center\">\n  <a>\n    <img src=\"application/static/img/MonetizeAI-logo.png\" alt=\"Logo\" width=\"80\" height=\"80\">\n  </a>\n\n<h3 align=\"center\">Monetize.ai</h3>\n\n  <p align=\"center\">\n    Financial Chat Bot Advisor\n    Project for Agile Web Development - CITS3403 unit at UWA 2023\n    <br />\n    <a href=\"https://github.com/KhanhHua2102/CITS3403-Project\"><strong>Explore the docs »</strong></a>\n    <br />\n    <br />\n    <a href=\"https://github.com/KhanhHua2102/CITS3403-Project\">View Demo</a>\n    ·\n    <a href=\"https://github.com/KhanhHua2102/CITS3403-Project/issues\">Report Bug</a>\n    ·\n    <a href=\"https://github.com/KhanhHua2102/CITS3403-Project/issues\">Request Feature</a>\n  </p>\n</div>\n\n<!-- TABLE OF CONTENTS -->\n<details>\n  <summary>Table of Contents</summary>\n  <ol>\n    <li>\n      <a href=\"#about-the-project\">About The Project</a>\n      <ul>\n        <li><a href=\"#built-with\">Built With</a></li>\n      </ul>\n    </li>\n    <li>\n      <a href=\"#getting-started\">Getting Started</a>\n      <ul>\n        <li><a href=\"#prerequisites\">Prerequisites</a></li>\n        <li><a href=\"#installation\">Installation</a></li>\n      </ul>\n    </li>\n    <li><a href=\"#usage\">Usage</a></li>\n    <li><a href=\"#roadmap\">Roadmap</a></li>\n    <li><a href=\"#contributing\">Contributing</a></li>\n    <li><a href=\"#contact\">Contact</a></li>\n    <li><a href=\"#acknowledgments\">Acknowledgments</a></li>\n  </ol>\n</details>\n\n<!-- ABOUT THE PROJECT -->\n![Project Screenshot](application/static/img/AppShowcase.png)\n\n# About The Project\nMonetize.ai is a web-based chat bot application that utilizes the GPT-3.5 and Davinci_003 language model and integrates with the Yahoo Finance, Alpha Vantage and Finnhub API to crawl financial data such as stocks and cryptocurrencies.\n\nThe main objective of this project is to provide users with a personalized financial advisory service that can help them manage their investment portfolios in an efficient and effective manner. The targeted users are people who works in the financial industry and have a good understanding of the financial market. The application is also suitable for people who are interested in investing and want to learn more about the financial market. Users can declare their investment portfolios to the chat bot and receive advice on how to balance their portfolios using modern portfolio theory. The chat bot can also calculate profits/losses and provide other useful metrics related to the user's portfolio. All of the user's portfolio data is stored in the Portfolio section for future reference.\n\nThe application is built using Flask, a popular Python web framework, and the SQLite database for efficient and scalable data storage. The client-side rendering is done using HTML, CSS, JavaScript, and Bootstrap, making it easy to use and accessible across different devices.\n\nThe repository is organized into several modules, each responsible for a specific aspect of the application. The main module is the chat bot itself, which handles user queries and provides advice based on the user's portfolio data. Other modules include the Yahoo Finance API integration, database management, and modern portfolio theory calculations.\n\nMonetize.ai is an open-source project, welcoming contributions from developers who want to improve its functionality and features. The repository includes comprehensive documentation to assist developers in getting started with the project and contributing code. An active community of developers and users is available to provide support and guidance on using the application effectively.\n\n<p align=\"right\">(<a href=\"#readme-top\">back to top</a>)</p>\n\n### Built With\n\n* [![Flask][Flask.com]][Flask-url]\n* [![SQLite][SQLite.com]][SQLite-url]\n* [![Bootstrap][Bootstrap.com]][Bootstrap-url]\n* [![JQuery][JQuery.com]][JQuery-url]\n\n<p align=\"right\">(<a href=\"#readme-top\">back to top</a>)</p>\n\n\n\n<!-- GETTING STARTED -->\n# Getting Started\n\n### Prerequisites\n\n* python 3.8 or newer\n\n### Installation\n\n#### Running the app locally by cloning the Github repo\n1. Get a free OpenAI API Key at [https://platform.openai.com/account/api-keys](https://platform.openai.com/account/api-keys)\n2. Clone the repo\n   ```sh\n   git clone https://github.com/KhanhHua2102/CITS3403-Project.git\n   ```\n3. Enter your OpenAI API in `config.py`\n   ```js\n   const API_KEY = 'ENTER YOUR OPEN_AI API';\n   ```\n4. Create a virtual environment for python:\n    ```sh\n    python3 -m venv env\n    ```\n    Then activate the virtual environment:\n\n    On Window:\n    ```sh\n    env\\Scripts\\activate.bat\n    ```\n    On Mac/Linux:\n    ```sh\n    source env/bin/activate\n    ```\n5. Install the requirements for the app:\n    ```sh\n    pip install -r requirements.txt\n    ```\n6. Activate the virtual environment again\n7. Run the app on your local host:\n    ```sh\n    flask run\n    ```\nThis will run the Flask app on your local host, typically http://127.0.0.1:5000.\n\n#### Running the app locally by using Docker\n1. Pull the docker image from Docker Hub:\n    ```sh\n    docker pull khanhhua2102/monetize.ai\n    ```\n2. Run the docker image:\n    ```sh\n    docker run -p 5000:5000 khanhhua2102/monetize.ai\n    ```\nThis will run the Flask app on your local host, typically http://127.0.0.1:5000.\n\n<p align=\"right\">(<a href=\"#readme-top\">back to top</a>)</p>\n\n\n\n<!-- USAGE EXAMPLES -->\n# Usage\n\n## Sign Up and Login\nThe Sign Up and Login process is a fundamental part of user authentication on a website. Users begin by visiting the Sign Up page, where they provide their email address and choose a password. Using Flask WTF, the Sign Up form can enforce validations to ensure the email address is valid and not already registered. Password hashing techniques, such as those provided by Flask or libraries like Werkzeug, can be applied to secure the password before storing it in the database.\n\nAfter successfully signing up, users can proceed to the Login page. Here, they enter their registered email and password to access their account. Flask WTF can validate the login credentials and authenticate the user. Once authenticated, the website can keep track of the user's session, allowing them to access protected pages and personalized features.\n\nTo enhance security, it is essential to store passwords in a hashed form rather than in plain text. Hashing algorithms like bcrypt or SHA-256 can be used to convert the password into a fixed-length string of characters that cannot be easily reversed. This way, even if the database is compromised, the actual passwords remain secure.\n\nOverall, the Sign Up and Login process ensures that users can create an account securely and access their personalized information. Validations, password hashing, and user database management play vital roles in safeguarding user data and maintaining the integrity of the website.\n\n## Interacting with the Chatbot\nYou can interact with the [Monetize.ai](http://monetize.ai/) chatbot by sending it messages through the chat interface. The chatbot understands a variety of finance-related questions and commands. Here are a few examples:\n\n### General Questions:\nAsk the chatbot broad questions about investing strategies, financial concepts, and stock markets.\n\nYou: What are some common investing strategies?\n\nBot: There are many different investment strategies, including value investing, growth investing, income investing, and momentum investing. Each strategy has its own unique approach to selecting investments and managing risk. It's important to do your research and choose a strategy that aligns with your financial goals and risk tolerance.\n\n### Update Your Portfolio:\nTell the chatbot when you buy or sell stocks by providing details like the stock ticker, number of shares, date, and price. The chatbot will update your portfolio and provide metrics like profits/losses.\n\nYou: I bought 200 shares of Apple (AAPL) on 01/02/2020 at $300 per share.\n\nBot: Based on the information you provided, if you had purchased 200 shares of Apple Inc. (AAPL) on 01-02-2002 at a start price of $0.44 and sold them on 04-05-2023 at an end price of $167.45, you would have made a profit of $33,402.00. This is a significant return on investment, and it's important to note that past performance does not guarantee future results. It's always a good idea to diversify your investments and consult with a financial advisor before making any investment decisions.\n\n### Rebalance Your Portfolio:\nAsk the chatbot to rebalance your portfolio using modern portfolio theory. It will adjust the weights of your stock holdings to match your desired risk tolerance.\n\nYou: Can you rebalance my portfolio?\n\nBot: Sure, I'd be happy to help you rebalance your portfolio. Based on your current holdings, a moderate risk tolerance portfolio would consist of approximately 40% AAPL, 30% AMZN, and 30% TSLA. To achieve this, you would need to sell some of your TSLA shares and buy more AAPL and AMZN shares. Specifically, you would need to sell 50 TSLA shares and buy 67 AAPL shares and 33 AMZN shares. This would bring your portfolio to a target allocation of approximately 40% AAPL, 30% AMZN, and 30% TSLA. Please note that this is just a suggestion and you should always consult with a financial advisor before making any investment decisions.\n\n### Get a stock price target:\nAsk the chatbot for a price target for a specific stock.\n\nYou: What is the price target for AAPL?\n\nBot: The current price target for AAPL is $178.88.\n\n### Ask for Recommendations:\nInquire about stocks you are interested in or request general stock recommendations, the recommendation is based on analysts either buy, hold or sell the stock.\n\nYou: Please give me stock recommendation of Tesla.\n\nBot: Based on the latest analyst recommendation for TSLA, it seems that there are more \"Buy\" and \"Strong Buy\" recommendations than \"Hold\" and \"Sell\" recommendations. However, it's important to note that past performance does not guarantee future results and it's always important to do your own research and consult with a financial advisor before making any investment decisions.\n\n### Change your risk tolerance:\nAsk the chatbot to change your preferred risk tolerance to either low, moderate, or high.\n\nYou: Please change my risk tolerance to high.\n\nBot: Hello Quang Khanh Hua, I am happy to inform you that your risk tolerance has been updated. Is there anything else I can help you with?\n### Reset portfolio data:\nAsk the chatbot to reset your portfolio data. This will clear your portfolio data.\n\nYou: reset portfolio\n\nBot: Your portfolio has been reset.\n\n### Reset chatbot context data:\nAsk the chatbot to reset your context data.\n\nYou: reset\n\nBot: Chatbot's context data cleared.\n\n## Settings\n- Display user’s informations\n- Change user’s information  (coming soon)\n- Upload profile picture  (coming soon)\n- Change to Dark mode (coming soon)\n\n## History\n- User can search for old conversations using keywords\n- Depending on the keyword searched either previous user message (if keyword is found inside the bot's reply) or next bot message (if keyword is found inside the user's message)\n- Able to select the number of pages they can see at one time (coming soon)\n- The message box will change depending on the message inside (coming soon)\n\n<p align=\"right\">(<a href=\"#readme-top\">back to top</a>)</p>\n\n<!-- ROADMAP -->\n## Roadmap\n\n- Change user’s information in settings page\n- Upload profile picture\n- Change to Dark mode\n- Create a base legit financial knowledge for more accure chatbot answers\n- More features comming soon..\n\nSee the [open issues](https://github.com/KhanhHua2102/CITS3403-Project/issues) for a full list of proposed features (and known issues).\n\n<p align=\"right\">(<a href=\"#readme-top\">back to top</a>)</p>\n\n<!-- CONTRIBUTING -->\n## Contributing\n**Quang Khanh Hua (Henry)**\n\n**Yin Min Aung (Ivan)**\n\n**Hoang Long Nguyen**\n\n<p align=\"right\">(<a href=\"#readme-top\">back to top</a>)</p>\n\n<!-- CONTACT -->\n## Contact\n\nQuang Khanh Hua - henry@khanhhua2102.com\n\n[![linkedin-shield]][linkedin-url]\n\n<!-- MARKDOWN LINKS & IMAGES -->\n<!-- https://www.markdownguide.org/basic-syntax/#reference-style-links -->\n[linkedin-shield]: https://img.shields.io/badge/-LinkedIn-black.svg?style=for-the-badge&logo=linkedin&colorB=555\n[linkedin-url]: https://linkedin.com/in/khanhhua2102\n[product-screenshot]: images/screenshot.png\n[Flask.com]: https://img.shields.io/badge/flask-%23000.svg?style=for-the-badge&logo=flask&logoColor=white\n[Flask-url]: https://flask.palletsprojects.com\n[SQLite.com]: https://img.shields.io/badge/sqlite-%2307405e.svg?style=for-the-badge&logo=sqlite&logoColor=white\n[SQLite-url]: https://www.sqlite.org\n[Bootstrap.com]: https://img.shields.io/badge/Bootstrap-563D7C?style=for-the-badge&logo=bootstrap&logoColor=white\n[Bootstrap-url]: https://getbootstrap.com\n[JQuery.com]: https://img.shields.io/badge/jQuery-0769AD?style=for-the-badge&logo=jquery&logoColor=white\n[JQuery-url]: https://jquery.com \n\n<p align=\"right\">(<a href=\"#readme-top\">back to top</a>)</p>\n"
    },
    {
      "name": "KBVE/kbve",
      "stars": 12,
      "img": "https://avatars.githubusercontent.com/u/5571997?s=40&v=4",
      "owner": "KBVE",
      "repo_name": "kbve",
      "description": "KBVE Monorepo",
      "homepage": "https://kbve.com/",
      "language": "JavaScript",
      "created_at": "2023-05-06T15:11:57Z",
      "updated_at": "2025-04-23T03:11:00Z",
      "topics": [],
      "readme": "# KBVE\n\n![Discord](https://img.shields.io/discord/342732838598082562?logo=discord)\n[![PyPI - KBVE Version](https://img.shields.io/pypi/v/kbve?label=kbve%20pypi&logo=python)](https://pypi.org/project/kbve/)\n[![PyPI - Fudster Version](https://img.shields.io/pypi/v/fudster?label=fudster%20pypi&logo=python)](https://pypi.org/project/fudster/)\n[![Crates.io ERust Version](https://img.shields.io/crates/v/erust?label=erust%20crates.io&logo=rust)](https://crates.io/crates/erust)\n[![Crates.io KBVE Version](https://img.shields.io/crates/v/kbve?label=kbve%20crates.io&logo=rust)](https://crates.io/crates/kbve)\n[![Crates.io Jedi Version](https://img.shields.io/crates/v/jedi?label=jedi%20crates.io&logo=rust)](https://crates.io/crates/jedi)\n[![Crates.io Holy Version](https://img.shields.io/crates/v/holy?label=holy%20crates.io&logo=rust)](https://crates.io/crates/holy)\n[![NPM Laser](https://img.shields.io/npm/v/%40kbve%2Flaser?label=laser%20npmi&logo=nodedotjs)](https://www.npmjs.com/package/@kbve/laser)\n[![NPM Devops](https://img.shields.io/npm/v/%40kbve%2Fdevops?label=devops%20npmi&logo=nodedotjs)](https://www.npmjs.com/package/@kbve/devops)\n\n\n<center>\n<a alt=\"KBVE Logo\" href=\"https://kbve.com/\" target=\"_blank\" rel=\"noreferrer\"><img src=\"https://raw.githubusercontent.com/KBVE/kbve.com/main/public/assets/img/letter_logo.png\" width=\"200\"></a>\n</center>\n\n---\n\n## What is KBVE?\n\nKBVE is a collective that builds different programs, libraries, games and memes!\nThis monorepo is the heart of all our applications, making it easier to manage while providing an experimental playground for pipelines.\nThe core of this Monorepo is based upon [Nx Smart Monorepos](https://nx.dev/), but do not fear! This is easy once you get the hang of it!\n\n---\n\n## Table of Libraries & Apps\n\n| C     | R     | A        | P    | @Apps                                  |\n| ----- | ----- | -------- | ---- | -------------------------------------- |\n| kilonet | erust | astropad | kbve | [kbve.com](https://kbve.com/)          |\n|       | jedi  |          | fudster     | [rentearth.com](https://renteart.com/) |\n|       | holy  |          |      | [herbmail.com](https://herbmail.com/)               |\n|       | kbve  |          |      | [rareicon.com](https://rareicon.com/)                       |\n\n---\n\n## C#RAP STACK\n\n> cRap , pronounced, Ceee-Rap. 💩 ... uhh wait I meant to say carp. 🐟\n\n- C\n\n  - [saber](https://github.com/KBVE/kbve/tree/main/apps/saber)\n\n    - Unity | v2022.3.12f1\n      - `dev` has a playable build on [Itch.io - Saber Dev](https://kbve.itch.io/dev-saber)\n        - The monorepo builds and ships the `dev`-branch build to Itch.io\n      - TODO: Interoptopus for rust bindings.\n    - Blazor\n      - WIP: Waiting on Net 8.0 integration.\n    - Steam Pipeline:\n      - TODO: Building Pipeline after adding a new `beta` branch to the CI/CD pipeline.\n\n  - [pandaplayground](https://github.com/KBVE/kbve/tree/main/apps/kbve.com/public/data/c/graveyard/panda)\n    - Generic `C` playground\n    - Extremely WIP with submodules removed by default.\n\n- R for Rust\n\n  - [kbve](https://crates.io/creates/kbve)\n\n    - Axum\n      - Prebuilt Axum routes for the KBVE backend.\n    - Diesel\n      - Database ORM for managing the types.\n\n  - [erust](https://crates.io/crates/erust) | v0.1 dev.\n    - Egui\n      - A components library that extends out `egui` & `eframe`.\n    - Rust [Crates.io Package Source](https://github.com/KBVE/kbve/tree/main/packages/erust)\n\n- A for Astro\n\n  - [AstroVE](https://github.com/KBVE/kbve/tree/main/packages/astro-ve/)\n\n    - Astro Components Library\n    - TODO: NPM Release\n    - React\n      - TODO: Migrate out additional React Components.\n    - Svelte\n      - TODO: Refactor the Svelte Components.\n\n  - [KhashVault](https://github.com/KBVE/kbve/tree/main/packages/khashvault/)\n\n    - Typescript JS Library\n    - TODO: `engine.ts` - Integrating Axum (`kbve`) backend with frontend libraries.\n    - TODO: NPM Release\n\n  - React - SKIP\n  - Svelte - SKIP\n  - NAPI\n    - Rust Bindings\n\n- P for Python\n\n  - This part of the stack is under massive development, so we advise to skip this until we get the bindings sorted.\n  - `Atlas` under [kbve pip package](https://pypi.org/project/kbve/)\n\n    - TODO: Full `atlas` refactor with `autogen` and `taskweaver`.\n\n  - Pyo3\n    - Rust Bindings\n  - Interoptopus\n    - Rust Bindings\n\n---\n\n## DEVOPS STACK\n\nThe Richard Stack is known as `Dockerized Integrated Container Kubes`.\n\n- D for [Docker](https://kbve.com/application/docker/)\n  - Swarm\n    - Docker Swarm for Stateful Applications.\n  - Portainer\n    - We use [Portainer](https://kbve.com/application/portainer/) for Docker/K8s management.\n  - KBVE Docker Images via [Hub](https://hub.docker.com/u/kbve)\n    - [kbve:rustprofile](https://hub.docker.com/r/kbve/rustprofile)\n    - [kbve:atlas](https://hub.docker.com/r/kbve/atlas)\n- I for Integrations\n\n  - Github Actions\n    - We use GHA to help build the docker images for x86 and ARM.\n  - GitLab\n    - We use a private GitLab for `private` codebase projects.\n\n- C for Containers\n\n  - [Proxmox](https://kbve.com/application/proxmox/)\n    - qEMU\n      - The QEMU handles running our docker swarm and k8s.\n    - LXD\n\n- K for Kubes\n  - [Kubernetes](https://kbve.com/application/kubernetes/)\n    - `k` for Stateless Applications.\n\n---\n\nExamples of monorepos:\n\n- [Cal.com](https://github.com/calcom/cal.com)\n- [E2B](https://github.com/e2b-dev/e2b/)\n\nPerks of a monorepo include:\n\n- Consistent Developer Environment and Experience.\n\n  - A monorepo can provide a consistent environment and set of tools for all developers, which can be especially beneficial in large organizations with many projects.\n  - Easier AI Training, as the codebase is all within a controlled repo.\n\n- Less clutter, keeping a single source of truth.\n\n  - The monorepo acts as a centralized source of truth for all projects, configurations, and dependencies.\n\n- Atomic Changes.\n\n  - Developers can make cross-cutting changes across multiple projects within a single atomic commit.\n\n- Scalable.\n  - Modern monorepo build tools and practices are designed to scale, even as the number of projects and developers grows.\n\n---\n\n### Install\n\nBefore getting started, we recommend that you use WSL, Linux or MacOS! Direct Windows is not supported.\nMake sure you have Node 18+ with PNPM! Python 3.10+ / Poetry are optional if you wish to work with python.\n\nPlease see [./CONTRIBUTE.md](./CONTRIBUTE.md) for WSL Instructions\n\nOfficial Install [Docs](https://kbve.com/guides/getting-started/#setup-instructions)\n\n#### Git\n\n- `git clone https://github.com/KBVE/kbve.git` then enter the directory. `cd kbve`\n- Then run -> `pnpm install`\n- Launch `pnpm nx dev herbmail.com` - Should start a development server with Astro for HerbMail.com\n\n---\n\n### Library & Apps\n\nThe breakdown of libraries, packages and applications in this repo contains!\n\n#### SABER\n\nThis is an open source Unity game that is currently located under `/apps/saber/` within this monorepo.\n\n#### Atlas\n\nAtlas is a [Python pip package](https://pypi.org/p/kbve) for generic baseline ml applications.\nAtlas currently only has `pyautogen` but a couple other packages are planned but as of right now, we are waiting on the stablization of the OpenAI v1 API.\nThe Atlas Library is currently broken needs to be wait on a couple packages to be updated, including LiteLLM.\n\n#### AstroVe\n\nAstro VE is an Astro-based UX/UI library that empowers developers to seamlessly create elegant and adaptable UX/UI components, fostering an interactive and intuitive user experience across diverse website applications.\n\n#### React Appwrite\n\nReact-based Appwrite library for fast frontend deployment.\nThis package will be deprecated and replace with a `react-api` style package.\n\n#### API\n\nThe API is current being updated! Its split into a NestJS core under `/apps/api/` and micro controllers written in Rust, under `/apps/rust_api_*` with `*` representing a glob wild card.\n\n##### API Rust\n\nMake sure that `Cargo` is installed!\n\nRunning the micro controllers for the Rust API is easy!\n\n`pnpm nx run rust_api_profile:run`\n\n### Apps\n\n- HerbMail.com\n- KBVE.com\n- RareIcon.com\n- Discord.sh\n"
    },
    {
      "name": "anakin87/who-killed-laura-palmer",
      "stars": 11,
      "img": "https://avatars.githubusercontent.com/u/44616784?s=40&v=4",
      "owner": "anakin87",
      "repo_name": "who-killed-laura-palmer",
      "description": "Simple Question Answering system, based on data crawled from Twin Peaks Wiki. It is built using 🔍 Haystack, an awesome open-source framework for building search systems that work intelligently over large document collections.",
      "homepage": "",
      "language": "Jupyter Notebook",
      "created_at": "2022-05-11T13:20:13Z",
      "updated_at": "2024-02-26T09:30:01Z",
      "topics": [
        "haystack",
        "information-retrieval",
        "natural-language-processing",
        "neural-search",
        "nlp",
        "python",
        "question-answering",
        "semantic-search",
        "space",
        "streamlit",
        "streamlit-webapp",
        "transformers"
      ],
      "readme": "---\ntitle: Who killed Laura Palmer?\nemoji: 🗻🗻\ncolorFrom: blue\ncolorTo: green\nsdk: streamlit\nsdk_version: 1.2.0\napp_file: app.py\npinned: false\nlicense: apache-2.0\n---\n\n# Who killed Laura Palmer? &nbsp; [![Generic badge](https://img.shields.io/badge/🤗-Open%20in%20Spaces-blue.svg)](https://huggingface.co/spaces/anakin87/who-killed-laura-palmer) [![Generic badge](https://img.shields.io/github/stars/anakin87/who-killed-laura-palmer?label=Github&style=social)](https://github.com/anakin87/who-killed-laura-palmer)\n[<img src=\"./data/readme_images/spaces_logo.png\" align=\"center\" style=\"display: block;margin-left: auto;\n  margin-right: auto;  max-width: 70%;}\">](https://huggingface.co/spaces/anakin87/who-killed-laura-palmer)\n\n\n\n## 🗻🗻 Twin Peaks Question Answering system\n\nWKLP is a simple Question Answering system, based on data crawled from [Twin Peaks Wiki](https://twinpeaks.fandom.com/wiki/Twin_Peaks_Wiki). It is built using [🔍 Haystack](https://github.com/deepset-ai/haystack), an awesome open-source framework for building search systems that work intelligently over large document collections.\n\n  - [Project architecture 🧱](#project-architecture-)\n  - [What can I learn from this project? 📚](#what-can-i-learn-from-this-project-)\n  - [Repository structure 📁](#repository-structure-)\n  - [Installation 💻](#installation-)\n  - [Possible improvements ✨](#possible-improvements-)\n---\n\n## Project architecture 🧱\n\n[![Project architecture](./data/readme_images/project_architecture.png)](#) \n\n* Crawler: implemented using [Scrapy](https://github.com/scrapy/scrapy) and [fandom-py](https://github.com/NikolajDanger/fandom-py)\n* Question Answering pipelines: created with [Haystack](https://github.com/deepset-ai/haystack)\n* Web app: developed with [Streamlit](https://github.com/streamlit/streamlit)\n* Free hosting: [Hugging Face Spaces](https://huggingface.co/spaces)\n\n---\n\n## What can I learn from this project? 📚\n- How to quickly ⌚ build a modern Question Answering system using [🔍 Haystack](https://github.com/deepset-ai/haystack)\n- How to generate questions based on your documents\n- How to build a nice [Streamlit](https://github.com/streamlit/streamlit) web app to show your QA system\n- How to optimize the web app to 🚀 deploy in [🤗 Spaces](https://huggingface.co/spaces)\n\n[![Web app preview](./data/readme_images/webapp.png)](https://huggingface.co/spaces/anakin87/who-killed-laura-palmer)\n\n## Repository structure 📁\n- [app.py](./app.py): Streamlit web app\n- [app_utils folder](./app_utils/): python modules used in the web app\n- [crawler folder](./crawler/): Twin Peaks crawler, developed with Scrapy and fandom-py\n- [notebooks folder](./notebooks/): Jupyter/Colab notebooks to create the Search pipeline and generate questions (using Haystack)\n- [data folder](./data/): all necessary data\n- [presentations](./presentations/): Video presentation and slides (PyCon Italy 2022)\n\nWithin each folder, you can find more in-depth explanations.\n\n## Installation 💻\nTo install this project locally, follow these steps:\n- `git clone https://github.com/anakin87/who-killed-laura-palmer`\n- `cd who-killed-laura-palmer`\n- `pip install -r requirements.txt`\n\nTo run the web app, simply type: `streamlit run app.py`\n\n## Possible improvements ✨\n### Project structure\n- The project is optimized to be deployed in Hugging Face Spaces and consists of an all-in-one Streamlit web app. In more structured production environments, I suggest dividing the software into three parts:\n  - Haystack backend API (as explained in [the official documentation](https://haystack.deepset.ai/components/rest-api))\n  - Document store service\n  - Streamlit web app\n### Reader\n- The reader model (`deepset/roberta-base-squad2`) is a good compromise between speed and accuracy, running on CPU. There are certainly better (and more computationally expensive) models, as you can read in the [Haystack documentation](https://haystack.deepset.ai/pipeline_nodes/reader).\n- You can also think about preparing a Twin Peaks QA dataset and fine-tuning the reader model to get better accuracy, as explained in this [Haystack tutorial](https://haystack.deepset.ai/tutorials/fine-tuning-a-model).\n\n\n\n"
    },
    {
      "name": "giguru/converse",
      "stars": 11,
      "img": "https://avatars.githubusercontent.com/u/5540787?s=40&v=4",
      "owner": "giguru",
      "repo_name": "converse",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2020-10-05T09:21:22Z",
      "updated_at": "2024-12-05T03:08:41Z",
      "topics": [],
      "readme": "# ConverSE\n\nConverse is a framework for doing research on Conversational Search. It was built/forked from\n[Haystack](https://github.com/deepset-ai/haystack).\n\n## Tutorials\nIt's quite easy to build a conversational search pipelines. Please consult the [conversational search  tutorials](https://github.com/giguru/converse/tree/master/tutorials/conversational_search) to see how.\n\n\n```python\n# Example pipeline\n\nreformulator = ClassificationReformulator(pretrained_model_path=\"uva-irlab/quretec\")\nretriever = SparseAnseriniRetriever(prebuilt_index_name='cast2019', searcher_config={\"Dirichlet\": {'mu': 2500}})\neval_retriever = EvalTREC(top_k_eval_documents=1000)\nranker = FARMRanker(model_name_or_path=\"castorini/monobert-large-msmarco-finetune-only\", top_k=1000, progress_bar=False)\neval_reranker = EvalTREC(top_k_eval_documents=1000)\n\n# Build pipeline\np = Pipeline()\np.add_node(component=reformulator, name=\"Rewriter\", inputs=[\"Query\"])\np.add_node(component=retriever, name=\"Retriever\", inputs=[\"Rewriter\"])\np.add_node(component=eval_retriever, name=\"EvalRetriever\", inputs=[\"Retriever\"])\np.add_node(component=ranker, name=\"Reranker\", inputs=[\"Retriever\"])\np.add_node(component=eval_reranker, name=\"EvalReranker\", inputs=[\"Reranker\"])\n\np.eval_qrels(qrels=qrels, topics=topics, dump_results=True)\n```\n\n## Core Features added on top of Haystack\n\n- **Prebuilt dataset indices** from Anserini and Terrier.\n- **Anserini support**: Integrate sparse and dense retrieval methods from [Pyserini](https://github.com/castorini/pyserini/).\n- **Terrier support**: Integrate retrieval functions from\n- **pytrec_eval** support to do evaluation.\n\n## Haystack classes useful for Conversational Search\n\nPlease consult the Haystack for the conceptual explanation of document stores.\n\n- **Pipeline**: Stick building blocks together to highly custom pipelines that are represented as Directed Acyclic Graphs (DAG). Think of it as \"Apache Airflow for search\".\n- **FaissDocumentStore**\n- **MilvusDocumentStore**\n- **ElasticSearchDocumentStore**\n- **FARMRanker**: Neural network (e.g., BERT or RoBERTA) that re-ranks top-k retrieved documents. The Ranker is an optional component and uses a TextPairClassification model under the hood. This model calculates semantic similarity of each of the top-k retrieved documents with the query.\n- **FARMReader**: Neural network (e.g., BERT or RoBERTA) that reads through texts in detail\n    to find an answer. The Reader takes multiple passages of text as input and returns top-n answers. Models are trained via [FARM](https://github.com/deepset-ai/FARM) or [Transformers](https://github.com/huggingface/transformers) on SQuAD like tasks.  You can load a pre-trained model from [Hugging Face's model hub](https://huggingface.co/models) or fine-tune it on your domain data.\n- **RAGenerator**: Neural network (e.g., RAG) that *generates* an answer for a given question conditioned on the retrieved documents from the retriever."
    },
    {
      "name": "karthik19967829/InferDoc",
      "stars": 11,
      "img": "https://avatars.githubusercontent.com/u/35610230?s=40&v=4",
      "owner": "karthik19967829",
      "repo_name": "InferDoc",
      "description": "Generate SQUAD style dataset from raw text file and train a transformer based question answering model .This repo has code from https://github.com/facebookresearch/UnsupervisedQA and https://github.com/deepset-ai/haystack",
      "homepage": "",
      "language": "Python",
      "created_at": "2020-06-21T21:02:30Z",
      "updated_at": "2022-12-08T12:50:48Z",
      "topics": [
        "bert",
        "deep-learning",
        "information-retrieval",
        "nlp-machine-learning",
        "question-answering",
        "transformers",
        "unsupervised-learning"
      ],
      "readme": "# InferDoc\nThis repo has code to generate synthetic question answering training data using https://github.com/facebookresearch/UnsupervisedQA and training them using https://github.com/deepset-ai/haystack\n\n# SQUAD style QA dataset generation\ncd self_supervised_qa && python -m unsupervisedqa.generate_synthetic_qa_data example_input.txt example_output\n\n## Transformer QA Model train ,eval and CLI testing\n# Usage:\n    qa_model.py train --data_dir=<data_dir> --train_file_name=<train_file_name> --dev_file_name=<dev_file_name>  --save_dir=<save_dir>\\\n    qa_model.py test --data_dir=<data_dir> --eval_file_name=<eval_file_name> --save_dir=<save_dir>\\\n    qa_model.py cli --data_dir=<data_dir> --save_dir=<save_dir>\n\n# Options:\n  --data_dir=<data_dir>........A namespace to find .txt squad formatted train or eval files\\\n  --train_file_name=<train_file_name>..............name of the train file in the data dir\\\n  --dev_file_name=<dev_file_name>..............The file to be used as a development set ,expected in SQUAD json format\\\n  --eval_file_name=<eval_file_name>..............The file to be used as a evaluation file,expected in SQUAD json format\\\n  --save_dir=<save_dir> ............The directory to save the trained model or to load the model from\n  \n  # Todo \n  Add automatic dataset generation within https://github.com/cdqa-suite/cdQA-ui to enable human in loop semi-supervised training\n  Make fine-tuning on domain specific data more robust with https://github.com/deepset-ai/FARM/issues/141\n"
    },
    {
      "name": "fahminlb33/bogor-house-price",
      "stars": 11,
      "img": "https://avatars.githubusercontent.com/u/8360880?s=40&v=4",
      "owner": "fahminlb33",
      "repo_name": "bogor-house-price",
      "description": "Web scraping and descriptive and predictive modelling of Bogor house pricing",
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2024-01-20T12:47:45Z",
      "updated_at": "2025-04-11T02:33:16Z",
      "topics": [],
      "readme": "<h1 align=\"center\">🏡 House Price Analysis in Bogor</h1>\n\n<p align=\"center\">End-to-end house price descriptive and predictive modelling.</p>\n\nThis is an end-to-end project for house price analysis and prediction at Kabupaten Bogor and Kota Bogor. This repo is mainly for educational purposes, especially for people that want to start their journey with career in data (data engineering, data science, and AI/ML).\n\nWhat's included:\n\n- web scraping using `scrapy`\n- ETL pipeline using `DuckDB` and `dbt`\n- exploratory data analysis using `pandas`\n- predictive modelling using `LightGBM`\n- hyperparameter search using `optuna` and `mlflow`\n- Power BI dashboard\n- and many more!\n\nFor more details, visit the project Wiki.\n\nConnect with me on LinkedIn or other socials in my [Github Profile](https://github.com/fahminlb33).\n\n## License\n\nThis project is licensed under Apache License 2.0.\n"
    },
    {
      "name": "EdAbati/outlines-haystack",
      "stars": 11,
      "img": "https://avatars.githubusercontent.com/u/29585319?s=40&v=4",
      "owner": "EdAbati",
      "repo_name": "outlines-haystack",
      "description": "Use `outlines` generators with Haystack.",
      "homepage": "https://pypi.org/project/outlines-haystack/",
      "language": "Python",
      "created_at": "2024-11-25T20:48:09Z",
      "updated_at": "2025-04-21T14:08:40Z",
      "topics": [
        "ai",
        "generative-ai",
        "haystack",
        "llm",
        "machine-learning",
        "nlp",
        "outlines",
        "structured-generation"
      ],
      "readme": "# `outlines-haystack`\n\n[![PyPI - Version](https://img.shields.io/pypi/v/outlines-haystack.svg)](https://pypi.org/project/outlines-haystack)\n[![PyPI - Python Version](https://img.shields.io/pypi/pyversions/outlines-haystack.svg?logo=python&logoColor=white)](https://pypi.org/project/outlines-haystack)\n[![PyPI - License](https://img.shields.io/pypi/l/outlines-haystack)](https://pypi.org/project/outlines-haystack)\n\n\n[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)\n[![Ruff](https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/astral-sh/ruff/main/assets/badge/v2.json)](https://github.com/astral-sh/ruff)\n\n[![GH Actions Tests](https://github.com/EdAbati/outlines-haystack/actions/workflows/test.yml/badge.svg)](https://github.com/EdAbati/outlines-haystack/actions/workflows/test.yml)\n[![pre-commit.ci status](https://results.pre-commit.ci/badge/github/EdAbati/outlines-haystack/main.svg)](https://results.pre-commit.ci/latest/github/EdAbati/outlines-haystack/main)\n\n-----\n\n## Table of Contents\n\n- [🛠️ Installation](#installation)\n- [📃 Description](#description)\n- [💻 Usage](#usage)\n\n## 🛠️ Installation\n\n```console\npip install outlines-haystack\n```\n\n## 📃 Description\n\n> Outlines is a Python library that allows you to use Large Language Model in a simple and robust way (with structured generation).  It is built by [.txt](https://dottxt.co).\n>\n> -- <cite>[Outlines docs](https://dottxt-ai.github.io/outlines/latest/welcome/)</cite>\n\nThis library allow you to use [`outlines`](https://dottxt-ai.github.io/outlines/latest/) generators in your [Haystack](https://haystack.deepset.ai) pipelines!\n\nThis library currently supports the following generators:\n- [x] [JSON](https://dottxt-ai.github.io/outlines/latest/reference/generation/json/): generate a JSON object with a given schema\n- [x] [Choices](https://dottxt-ai.github.io/outlines/latest/reference/generation/choices/): generate text from a list of options. Useful for classification tasks!\n- [x] [Text](https://dottxt-ai.github.io/outlines/latest/reference/text/): _simply_ generate text\n- [ ] [Regex](https://dottxt-ai.github.io/outlines/latest/reference/generation/regex/): ⚠️ coming soon\n- [ ] [Format](https://dottxt-ai.github.io/outlines/latest/reference/generation/format/): ⚠️ coming soon\n- [ ] [Grammar](https://dottxt-ai.github.io/outlines/latest/reference/generation/cfg/): ⚠️ coming soon\n\n`outlines` supports a wide range of models and frameworks, we are currently supporting:\n- [x] [OpenAI/Azure OpenAI](https://dottxt-ai.github.io/outlines/latest/reference/models/openai/)\n- [x] [🤗 Transformers](https://dottxt-ai.github.io/outlines/latest/reference/models/transformers/)\n- [x] [`llama-cpp`](https://dottxt-ai.github.io/outlines/latest/reference/models/llamacpp/)\n- [x] [`mlx-lm`](https://dottxt-ai.github.io/outlines/latest/reference/models/mlxlm/)\n\n## 💻 Usage\n\n> [!TIP]\n> See the [Example Notebooks](./notebooks) for complete examples.\n>\n> All below examples only use the `transformers` models.\n\n### JSON Generation\n\n```python\n>>> from enum import Enum\n>>> from pydantic import BaseModel\n>>> from outlines_haystack.generators.transformers import TransformersJSONGenerator\n\n>>> class User(BaseModel):\n...    name: str\n...    last_name: str\n\n>>> generator = TransformersJSONGenerator(\n...     model_name=\"microsoft/Phi-3-mini-4k-instruct\",\n...     schema_object=User,\n...     device=\"cuda\",\n...     sampling_algorithm_kwargs={\"temperature\": 0.5},\n... )\n>>> generator.warm_up()\n>>> generator.run(prompt=\"Create a user profile with the fields name, last_name\")\n{'structured_replies': [{'name': 'John', 'last_name': 'Doe'}]}\n```\n\n### Choice Generation\n\n```python\n>>> from outlines_haystack.generators.transformers import TransformersChoiceGenerator\n\n>>> generator = TransformersChoiceGenerator(\n...     model_name=\"microsoft/Phi-3-mini-4k-instruct\",\n...     choices=[\"Positive\", \"Negative\"],\n...     device=\"cuda\",\n...     sampling_algorithm_kwargs={\"temperature\": 0.5},\n... )\n>>> generator.warm_up()\n>>> generator.run(prompt=\"Classify the following statement: 'I love pizza'\")\n{'choice': 'Positive'}\n```\n\n### Text Generation\n\n> [!TIP]\n> While `outlines` supports classic text generation, it excels at structured generation.\n> For text generation, consider using [Haystack's built-in text generators](https://docs.haystack.deepset.ai/docs/generators) that offer more features.\n\n```python\n>>> from outlines_haystack.generators.transformers import TransformersTextGenerator\n\n>>> generator = TransformersTextGenerator(\n...     model_name=\"microsoft/Phi-3-mini-4k-instruct\",\n...     device=\"cuda\",\n...     sampling_algorithm_kwargs={\"temperature\": 0.5},\n... )\n>>> generator.warm_up()\n>>> generator.run(prompt=\"What is the capital of Italy?\")\n{'replies': ['\\n\\n# Answer\\nThe capital of Italy is Rome.']}\n```\n\n## License\n\n`outlines-haystack` is distributed under the terms of the [MIT](https://spdx.org/licenses/MIT.html) license.\n"
    },
    {
      "name": "dkm1006/news-graph-rag",
      "stars": 11,
      "img": "https://avatars.githubusercontent.com/u/23480813?s=40&v=4",
      "owner": "dkm1006",
      "repo_name": "news-graph-rag",
      "description": "An app that answers questions about a news corpus from a graph database by using an LLM to convert the questions to Cypher queries",
      "homepage": "",
      "language": "Jupyter Notebook",
      "created_at": "2024-05-20T07:03:48Z",
      "updated_at": "2024-11-11T01:53:11Z",
      "topics": [],
      "readme": "# news-graph-rag\nA streamlit app that answers questions about a news corpus from a graph database by using an LLM to convert the questions to Cypher queries\n"
    },
    {
      "name": "CdC-SI/ZAS-EAK-CopilotGPT",
      "stars": 11,
      "img": "https://avatars.githubusercontent.com/u/150324891?s=40&v=4",
      "owner": "CdC-SI",
      "repo_name": "ZAS-EAK-CopilotGPT",
      "description": "The official repository of the EAK-Copilot project as part of the Innovation Fellowship 2024.",
      "homepage": "https://cdc-si.github.io/ZAS-EAK-CopilotGPT/",
      "language": "Python",
      "created_at": "2024-03-21T09:51:23Z",
      "updated_at": "2025-03-25T13:36:28Z",
      "topics": [],
      "readme": "[![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://codespaces.new/CdC-SI/eak-copilot)\n\n\n# ZAS/EAK CopilotGPT\n\nWelcome to the official repository of the ZAS/EAK CopilotGPT challenge, developed as part of the [Innovation Fellowship 2024](https://www.innovationfellowship.ch/). This project is designed to enhance workplace efficiency and foster innovation by providing AI-supported tools that assist employees in their daily tasks.\n\nThis repository serves as a proof of concept (PoC), which is slated to conclude in February 2025. However, we are optimistic that the momentum generated by this innovative challenge will attract continued support and development beyond this timeframe.\n\n## Documentation\n\nCheck [the website](https://cdc-si.github.io/ZAS-EAK-CopilotGPT/) for more information.\n\nFor more in-depth procedures and examples, see:\n\n- [Open Source model setup guide](/src/copilot/app/rag/llm/README.md)\n- [Prompt Engineering guide](/src/copilot/app/rag/prompts)\n- [Data Preprocessing guide](/src/copilot/app/indexing/README.md)\n- [Examples](/examples/)\n\n## Demo Video\n![45-sec demo video that shows features autocomplete and RAG from the first prototype.](/demo-video.gif)\n\n## Updates\n\n### Coming Soon\n- Livingdocs Adapter\n- Confluence Adapter\n- Local Private Embeddings\n- Local Private LLMs\n\n### Version 0.3.0\n\n- Ollama OS LLMs\n- Agentic RAG\n- Source validation\n- Topic Check\n- Commands\n    - `/summarize`\n    - `/translate`\n\n### Version 0.2.0\n\n- `llama.cpp` OS models\n\n### Version 0.1.0\n\n- Authentication\n- User feedback\n- Conversational Memory\n- Chat History\n- Autocomplete\n- Optimized RAG\n\n- Survey Pipeline\n- Local Private LLM\n    - `llama.cpp` (all compatible models eg. HuggingFace)\n    - `mlx` (Apple Silicon)\n- GUI\n- GUI Styleguide Bundle\n\n## Challenge Vision\n\nCOMING SOON: a detailed overview of our project's vision and strategic alignment.\n\n## Features\n\n- **Automation of Routine Tasks:** Reduces monotonous research tasks and the finding of information, allowing employees to focus on more important tasks.\n- **Decision Support:** Provides real-time assistance in decision-making through advanced algorithms.\n\n## How To Contribute\n\nPlease check the ```CONTRIBUTORS.md``` file to contribute to the ZAS/EAK CopilotGPT project.\n\n## How it works\n\nThe ZAS/EAK CopilotGPT currently features:\n- **Question autosuggest**: High quality curated questions (from FAQ) are suggested in the chatbar based on the user input. Validated answers with sources are then returned in the chat. Autocomplete currently supports:\n    - `exact match`\n    - `fuzzy match` (`levenstein match`, `trigram match`)\n    - `semantic similarity match`\n- **RAG**: When no known question/answer pairs are found through autosuggest, RAG is initiated. A semantic similarity search will match the most relevant indexed documents in a vector database and an LLM will generate an answer based on these documents, providing the source of the answer.\n- **Agentic RAG**: A query is analyzed and routed to a suitable agent (eg. `FAK-EAK` agent) which can execute specialized tools (eg. `calculate_reduction_rate_and_supplement`, `calculate_reference_age`, etc.) and execute agentic RAG:\n    - Ask followup questions\n    - Refine user query\n    - Expand search\n    - Perform multiple retrieval rounds\n    - Evaluate retrieval results\n\n## Getting Started\n\nHere you will find instructions for installing and setting up ZAS/EAK CopilotGPT:\n\n### Prerequisites\n\nBefore starting, ensure you have the following software installed on your computer:\n- **Git**: Needed for source code management. Install from [here](https://git-scm.com/downloads).\n- **Docker**: Required for running containers. Install from [here](https://docs.docker.com/get-docker/).\n\nLinux users may need to prepend `sudo` to Docker commands depending on their Docker configuration.\n\n### Installation\n\n1. **Clone the Repository**\n\n   Begin by cloning the ZAS/EAK CopilotGPT repository to your local machine.\n\n   ```bash\n   git clone https://github.com/CdC-SI/ZAS-EAK-CopilotGPT.git\n   ```\n\n   ```bash\n   cd ZAS-EAK-CopilotGPT\n   ```\n\n2. **Setting Up Environment Variables**\n\n    To use the ZAS/EAK Copilot, you need to set up some environment variables in `.env`.\n\n    Copy the `.env.example` file to a new file named `.env` and fill in the appropriate values:\n\n    ```bash\n    cp .env.example .env\n    ```\n\n    Minimal requirements:\n    ```\n    - `OPENAI_API_KEY`\n    - `COHERE_API_KEY`\n    - `DEEPL_API_KEY` (if you want translation capabilities)\n    - `LANGFUSE_SECRET_KEY` # get at localhost:3000\n    - `LANGFUSE_PUBLIC_KEY` # get at localhost:3000\n    - `ENCRYPTION_KEY` # run openssl rand -hex 32\n    ````\n\n    For different LLM/embedding/reranking APIs, set:\n\n    - `OPENAI_API_KEY` (get it at [OpenAI console](https://platform.openai.com/api-keys))\n    - Note: You can also use AzureOpenAI by setting:\n        - `AZUREOPENAI_API_KEY`\n        - `AZUREOPENAI_API_VERSION`\n        - `AZUREOPENAI_ENDPOINT`\n    - `ANTHROPIC_API_KEY` (get it at [OpenAI console](https://console.anthropic.com/account/keys))\n    - `GEMINI_API_KEY` (get it at [Gemini console](https://ai.google.dev/gemini-api/docs/api-key))\n    - `GROQ_API_KEY` (get it at [Groq console](https://console.groq.com/keys))\n    - `COHERE_API_KEY` (get it at [Cohere console](https://dashboard.cohere.com/api-keys))\n    - `DEEPL_API_KEY` (get it at [Deepl console](https://support.deepl.com/hc/en-us/articles/360020695820-API-Key-for-DeepL-s-API))\n\n    If using Open Source local LLM API, set:\n\n    - `LOCAL_LLM_GENERATION_ENDPOINT` (URL and port, eg. `http://host.docker.internal:11434` for ollama)\n\n    All other fields are preconfigured with default settings (but can be configured as well).\n\n3. **OPTIONAL: Advanced Copilot Configuration**\n\n    The Copilot is configured with default settings, but you can customize every aspect (eg. LLM model, embedding model, autocomplete, RAG, etc.).\n\n    Here are some tips to customize parameters in `src/copilot/app/config/config.yaml`:\n\n    - `rag/llm/model`: set a specific LLM model:\n        - `gpt-4o-mini` with an OpenAI API key\n        - `claude-3-5-sonnet-20241022` with an Anthropic API key\n        - `llama-3.1-8b-instant` with a GROQ API key\n        - Note: set a valid LLM API model name for a given provider.\n        - Note: if using an Open Source LLM:\n            - with `llama.cpp`: prefix the model name with `llama-cpp:` (eg. `llama-cpp:qwen2.5-7b-instruct-q2_k.gguf`)\n            - with `mlx`: prefix the model name with `mlx-community:` (eg. `mlx-community:Nous-Hermes-2-Mistral-7B-DPO-4bit-MLX`)\n            - with `ollama`: prefix the model name with `ollama:` (eg. `ollama:deepseek-r1:8b`)\n    - `rag/embedding/model`: set a specific embedding model\n        - `text-embedding-3-small` with an OpenAI API key\n        - Note: RAG performance might vary if you don't embed all your data with the same embedding model.\n    - `rag/retrieval/retrieval_method`: add different retrievers\n        - `top_k_retriever`\n        - `query_rewriting_retriever`\n        - `bm25_retriever`\n        - `contextual_compression_retriever`\n        - `reranking`\n\n4. **Build Docker Images**\n\n    Build the Docker images using the Docker Compose configuration. This step compiles and launches your Docker environment.\n\n    ```bash docker\n    docker-compose up --build -d\n    ```\n\n    Note: you might need to down the services and re-up them once at first startup due to db connectivity issues.\n\n5. **Verifying the Installation**\n\n    Check the status of the containers to confirm everything is running as expected:\n    ```bash\n    docker-compose ps\n    ```\n    After the containers are successfully started, verify that the application is running correctly by accessing it through your web browser at http://localhost:4200.\n\n6. **Index some data**\n\n    - To index some sample data for FAQ and RAG, you can navigate to the [indexing](http://localhost:8000/apy/indexing/docs/) swagger and make a request to:\n        - ```/upload_csv_rag```\n        - ```/upload_csv_faq```\n        - Note: set ```embed``` parameter to ```true``` to enable semantic search\n\n    - To index more extensive RAG data from any official government website, navigate to the [indexing](http://localhost:8000/apy/indexing/docs) swagger and make a request to:\n        - ```/index_html_from_sitemap``` (scraps any *.admin.ch website by specifying the sitemap URL)\n"
    },
    {
      "name": "bytewax/real-time-rag-workshop",
      "stars": 11,
      "img": "https://avatars.githubusercontent.com/u/72483929?s=40&v=4",
      "owner": "bytewax",
      "repo_name": "real-time-rag-workshop",
      "description": null,
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2024-05-21T21:58:19Z",
      "updated_at": "2024-11-15T23:39:56Z",
      "topics": [],
      "readme": "# Building real time Retrieval Augmented Generation for financial data and news\n\n## Introduction\n\n### Audience\n\nData/ML/AI engineers, Data scientists, Software engineers interested in data processing, IT professionals looking to understand and apply RAG\n* Programming Language: Python\n* Topic: Real-time Retrieval Augmented Generation for Structured and Unstructured Data\n* Target Geographic Region: North America \n\n\n## About the workshop: Financial news and analytics with LLMs\n\nRetrieval Augmented Generation (RAG) is a technique that enhances the capabilities of Large Language Models (LLMs) by dynamically incorporating external information during the generation process. This approach combines the generative strengths of LLMs with the retrieval of relevant information from a large dataset, allowing for more informed, accurate, and contextually relevant outputs. \n\nBy enabling real time analytics capabilities within RAG systems, we can ensure users obtain the latest information. \n\nIn this workshop, we will focus on designing RAG pipelines through data flow pipeline design including Directed Graphs (DG), and the integration of real-time analytics. Through a blend of theoretical insights and hands-on sessions, attendees will learn how to set up efficient RAG pipelines that cater to the complexities of varied data types, driving enhanced decision-making and insights in their organizations. This workshop will use financial data as its use case, and will leverage a combination of structured and unstructured data such as stock and market prices as well as publicly available news. One of the challenges of financial data is how quickly it becomes irrelevant - both in terms of the market prices and any events around it. \n\n**Goals**\n\n* Gain proficiency in setting up and managing RAG pipelines for diverse data types.\n* Leverage Bytewax for integrating real-time analytics into your data processing workflows.\n* Unstructured TBD\n* AI Co-Innovation Lab TBD\n* Understand the nuances of working with both structured and unstructured data in a unified system.\n* Develop the skills to architect, deploy, and optimize advanced RAG systems in real-world scenarios.\n\n**Pre-requisites**\n\n* Basic understanding of data structures and algorithms\n* Familiarity with Python programming\n* Basic knowledge of data processing and ETL concepts\n"
    },
    {
      "name": "GURPREETKAURJETHRA/Youtube-Video-Transcribe-Summarizer-LLM-App",
      "stars": 10,
      "img": "https://avatars.githubusercontent.com/u/100081334?s=40&v=4",
      "owner": "GURPREETKAURJETHRA",
      "repo_name": "Youtube-Video-Transcribe-Summarizer-LLM-App",
      "description": "YouTube Video Summarization App built using open source LLM and Framework like Llama 2, Haystack, Whisper, and Streamlit. This app smoothly runs on CPU as Llama 2 model is in GGUF format loaded through Llama.cpp.",
      "homepage": "https://github.com/GURPREETKAURJETHRA/END-TO-END-GENERATIVE-AI-PROJECTS",
      "language": "Python",
      "created_at": "2024-03-22T16:51:05Z",
      "updated_at": "2025-03-26T15:23:37Z",
      "topics": [
        "generative-ai",
        "haystack",
        "haystack-ai",
        "large-language-models",
        "llama2",
        "llamacpp",
        "llm",
        "streamlit",
        "whisper-api"
      ],
      "readme": "# YouTube Video Summarization App\nYouTube Video Summarization App built using open source LLM and Framework like Llama 2, Haystack, Whisper, and Streamlit. This app smoothly runs on CPU as Llama 2 model is in GGUF format loaded through Llama.cpp.\n\n## 📝 Description:\nYouTube Video Summarization App, is a powerful and customizable tool at your disposal, capable of automatically summarizing YouTube videos. \n\n## 💫Requirements:\n\n- **🔍 Haystack: Your AI-Powered Search Engine**\nHaystack is a versatile framework that allows you to harness the power of Generative AI to efficiently search, extract, and summarize information from vast amounts of text data. \n\n- **🤖 Llama 2: The AI Brain**\nMeet Llama 2, a massive language model that will assist you in understanding and summarizing the content of YouTube videos. You'll learn how to leverage Llama 2's language capabilities to extract key insights from video transcripts. That too 32K context length model in the GGUF format.\n\n- **🗣️ Whisper: Transforming Speech to Text**\nWhisper, a state-of-the-art automatic speech recognition (ASR) model, will be your go-to tool for transcribing spoken content from your YouTube videos. I'll show you how to integrate Whisper from Haystack inbuilt class seamlessly into your application, enabling it to work with both spoken and textual data.\n\n- **🚀 Streamlit: The User-Friendly Interface**\nStreamlit is the secret sauce that ties it all together. With its user-friendly interface design, you can effortlessly create a visually appealing front end for your YouTube Video Summarization App. We'll guide you through building an intuitive interface that allows users to interact with your app easily.\n\n## 🌟Implementation Guide:\n[Demo ▶️](https://www.youtube.com/watch?v=K9mDAb2Lz6Y)\n\n## 🔗Other Links:\n- Haystack: https://haystack.deepset.ai/\n- Llama 2 32K Model: https://huggingface.co/togethercomputer/LLaMA-2-7B-32K\n- Llama 2 32K GGUF Model: 32K-Instruct-GGUF🦌\n\n\n ---\n## ©️ License 🪪 \n\nDistributed under the MIT License. See `LICENSE` for more information.\n\n---\n\n#### **If you like this LLM Project do drop ⭐ to this repo**\n#### Follow me on [![LinkedIn](https://img.shields.io/badge/linkedin-%230077B5.svg?style=for-the-badge&logo=linkedin&logoColor=white)](https://www.linkedin.com/in/gurpreetkaurjethra/) &nbsp; [![GitHub](https://img.shields.io/badge/github-%23121011.svg?style=for-the-badge&logo=github&logoColor=white)](https://github.com/GURPREETKAURJETHRA/)\n\n---\n"
    },
    {
      "name": "VectorInstitute/NAA",
      "stars": 10,
      "img": "https://avatars.githubusercontent.com/u/40637123?s=40&v=4",
      "owner": "VectorInstitute",
      "repo_name": "NAA",
      "description": "Neural Agent Assistant framework includes tools and artifacts rooted in deep learning and foundation language models to improve task-oriented customer service conversations and experiences. ",
      "homepage": "",
      "language": "Jupyter Notebook",
      "created_at": "2022-10-11T14:11:23Z",
      "updated_at": "2025-01-08T12:00:31Z",
      "topics": [],
      "readme": "# Neural Agent Assistant framework for Customer Service Support\nNeural Agent Assistant framework includes tools and artifacts rooted in deep learning and foundation language models to improve task-oriented customer service conversations and experiences. It is based on the following paper:\n\nStephen Obadinma, ..., [Bringing the State-of-the-Art to Customers: A Neural Agent Assistant Framework for Customer Service Suppor](https://aclanthology.org/2022.emnlp-industry.44/), EMNLP.\n\n## An Overview\nThe framework features three high-level components: (1) Intent Identification, (2) Context Retrieval, and (3) Response Generation. Though designed as parts in a pipeline in generating a natural language response to customer questions, the output from any given component can nonetheless be transferred back to the human agent when needed.\n\n<p align=\"center\">\n  <a href=\"https://github.com/VectorInstitute/NAA/blob/main/naa.jpg\">\n    <img src=\"https://github.com/VectorInstitute/NAA/blob/main/naa.jpg\" alt=\"pipeline\" width=\"500\" height=\"300\">\n  </a>\n</p>\n<p align=\"center\">\n  Figure 1. Neural Agent Assistant framework\n</p>\n\n## Features\n1. Intent Identification\n2. Context Retrieval \n3. Response Generation\n\n## Resources\nCheckpoints and datasets can be downloaded from [here](https://drive.google.com/drive/folders/11HIhryVjfQfq-vmBo68uSTLQhKWcbRHn?usp=sharing).\n## Installing dependencies\n```\npip install --upgrade pip\npip install -r requirements.txt\n```\n\n## using pre-commit hooks\nTo check your code at commit time\n```\npre-commit install\n```\n\nYou can also get pre-commit to fix your code\n```\npre-commit run\n```\n"
    },
    {
      "name": "naynco/nayn.ml",
      "stars": 10,
      "img": "https://avatars.githubusercontent.com/u/41342599?s=40&v=4",
      "owner": "naynco",
      "repo_name": "nayn.ml",
      "description": "NAYN Machine Learning Projects",
      "homepage": "https://nayn.ml",
      "language": "Python",
      "created_at": "2019-08-17T01:28:22Z",
      "updated_at": "2024-01-05T18:27:00Z",
      "topics": [
        "journalism",
        "machine-learning",
        "news"
      ],
      "readme": "# nayn.ml\nNAYN's Machine Learning Projects\n\n![nayn.ml](/nayn.ml.png?raw=true)\n\n# NAYN.CLASSIFIER [*](https://github.com/naynco/nayn.classifier)\n# NAYN.SUMMARIZER [*](https://github.com/naynco/nayn.summarizer)\n"
    },
    {
      "name": "NLPerWS/KMatrix",
      "stars": 10,
      "img": "https://avatars.githubusercontent.com/u/114089542?s=40&v=4",
      "owner": "NLPerWS",
      "repo_name": "KMatrix",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2024-08-05T10:09:18Z",
      "updated_at": "2025-04-23T07:16:39Z",
      "topics": [],
      "readme": "#  :sunflower: KMatrix:  A Flexible Heterogeneous Knowledge Enhancement Toolkit for Large Language Model\n\n\n\n<img src=\"https://img.shields.io/badge/%F0%9F%A4%97%20HuggingFace%20Datasets-27b3b4.svg\"> <img src=\"https://img.shields.io/npm/l/vue-web-terminal.svg\"><img src=\"https://img.shields.io/badge/made_with-Python-blue\">\n\n\n\nKMatrix is a flexible heterogeneous knowledge enhancemant toolkit for LLMs.  Our toolkit contains seven stages to complete knowledge-enhanced generation task. All stages are implemented based on our modular component definitions.  Meanwhile, we design a control-logic flow diagram to combine components. The key features of KMatrix include: \n\n1. Unified enhancement of heterogeneous knowledge: KMatrix uses both verbalizing-retrieval and parsing-query methods to support unified enhancement of heterogeneous knowledge (like free-textual knowledge, tables, knowledge graphs, etc).\n2. Systematical adaptive enhancement methods integration: KMatrix offers comprehensive adaptive enhancement methods including retrieval timing judgment and knowledge source selection. \n3. High customizability and easy combinability: our modularity and control-logic flow diagram design flexibly supports the entire lifecycle of various complex Knowledge Enhanced Large Language Models (K-LLMs) systems, including training, evaluation, and deployment.\n4. Comprehensive evaluation of K-LLMs systems enhanced by heterogeneous knowledge: we integrate a rich collection of representative K-LLMs knowledge, datasets, and methods, and provide performance analysis of heterogeneous knowledge enhancement. \n\n![image](images/kmatrix_system.png)\n\n\n\n## :wrench: Quick Start\n\n### 1. Quick Start from Manual\n\n**Installation** \n\nTo get started with KMatrix, simply clone it from Github and install (requires Python 3.7+ ,  Python 3.10 recommended): \n\n\n    $ git clone https://github.com/NLPerWS/KMatrix.git\n    \n    # It is recommended to use a virtual environment for installation\n    $ conda create -n KMatrix python=3.10\n    $ conda activate KMatrix\n    \n    # Install backend environment\n    $ cd KMatrix\n    $ pip install -r requirements.txt\n    \n    # Install Frontend environment\n    # You need a node environment, and nvm is recommended for node environment management\n    # Recommended node environments: 16.20.2\n    # You can refer to the fellowing websites to install nvm\n    # https://nvm.uihtm.com/#nvm-linux \n    # https://github.com/nvm-sh/nvm\n    # After installing the node environment, execute:\n    $ cd easy-flow\n    $ npm install\n    \n    # Then, you need to install some third-party tools required by our toolkit\n    # Install ES database using Docker\n    $ docker pull elasticsearch:8.11.1\n    $ docker run -idt  \\\n        -p 9200:9200 -p 9300:9300 \\\n        -e \"discovery.type=single-node\" \\\n        -e \"ES_JAVA_OPTS=-Xms512m -Xmx512m\" \\\n        -e \"xpack.security.enabled=true\" \\\n        -e \"xpack.security.enrollment.enabled=true\" \\\n        -e \"ELASTIC_PASSWORD=yourpassword\" \\\n        -v $(pwd)/elasticsearch_data:/usr/share/elasticsearch/data \\\n        -v $(pwd)/esplugins:/usr/share/elasticsearch/plugins \\\n        --name elasticsearch elasticsearch:8.11.1\n        \n    We upload knowledge and the datasets to ModelScope: https://www.modelscope.cn/datasets/KMatrixRep/KMatrix_Rep\n    Use the following commands to download:\n    $ git lfs install\n    $ git clone https://www.modelscope.cn/datasets/KMatrixRep/KMatrix_Rep.git\n    you can download it, it contains:\n    dir_dataset/\n    dir_knowledge/\n    dir_model/\n    use the downloaded folders to replace the original folders in KMatrix directory.\n    \n    # You need to import knowledge into ES database for Retrieval, our knowledge samples locate in dir_knowledge/local_knowledge_verlized/, you need to create three indexes in ES named 'wikipedia','wikidata','wikitable', and their corresponding files are respectively Wikipedia/wikipedia.jsonl, wikidata/wiki_data.jsonl, wikitable/wikitable.jsonl. After that, you can use retrieval model to retrieve the knowledge in the database.\n\n**StartUp**\n\n\n    If you have successfully installed the environment, a quick start will be easy.\n    \n    1. Set configurations that needs to be modified in the root_config.py file located in the project root directory, if necessary. Set the SERVER_HOST in easy-flow/src/assets/js/host.js to the IP address of deployment server.\n    \n    2. Start the toolkit by executing following command: \n    $ cd KMatrix/easy-flow\n    $ npm run dev\n    $ cd KMatrix\n    $ python flask_server.py\n    Visit KMatrix toolkit using the browser: http://yourserverip:10005\n    \n    3. Construct and Execute Flow diagram\n    You can construct K-LLMs systems using our modular component and control-logic flow diagram, and execute it. Details of K-LLMs systems construction can be found in toolkit usage. You can use a flow diagram we have built (a K-LLMs system actively querying multiple knowledge interfaces) for a quick start:\n    Click the [Use exising diagram] drop-down box on the frontend of toolkit, select Deployment/v16_cok_de_diagram, and then click the [Deploy diagram] button to start the deployment. After the deployment completes, enter your question in the question box and click [send] to generate reasoning steps and answer.\n\n### 2. Quick Start from Docker (recommended)\n\n`````\n$ git clone https://github.com/NLPerWS/KMatrix.git\n$ chmod +x -R KMatrix\nSet configurations that needs to be modified in the root_config.py file located in the project root directory, if necessary. Set the SERVER_HOST in easy-flow/src/assets/js/host.js to the IP address of deployment server.\n\n# Install ES database using Docker\n$ docker pull elasticsearch:8.11.1\n$ docker run -idt  \\\n    -p 9200:9200 -p 9300:9300 \\\n    -e \"discovery.type=single-node\" \\\n    -e \"ES_JAVA_OPTS=-Xms512m -Xmx512m\" \\\n    -e \"xpack.security.enabled=true\" \\\n    -e \"xpack.security.enrollment.enabled=true\" \\\n    -e \"ELASTIC_PASSWORD=yourpassword\" \\\n    -v $(pwd)/elasticsearch_data:/usr/share/elasticsearch/data \\\n    -v $(pwd)/esplugins:/usr/share/elasticsearch/plugins \\\n    --name elasticsearch elasticsearch:8.11.1\n\n$ docker pull leap233/kmatrix:v1\n$ cd KMatrix\n$ sh docker_start.sh\n`````\n\n\n\n## :dizzy: Toolkit Usage\n\n***We provide a  screencast video of our toolkit at [here](https://youtu.be/VL-zY2pphwI) which explained the tool usage instructions.***\n\n### 1 K-LLMs System Construction\n\nKMatrix constructs K-LLMs system using two stages：\n\n- Selecting  components and  configuring  parameters\n\n- Constructing components logic relations using control-logic flow diagram \n\n#### 1.1 Selecting  components and  configuring  parameters\n\nKMatrix component is an functional unit of K-LLMs system. We unify datasets, knowledge, and models involved in K-LLMs as components. KMatrix defines 16 types of components, like Retriever, Query Parser, Generator, etc. \n\n| Component Class           | Component Instance                     | Instance Description                                         |\n| ------------------------- | -------------------------------------- | ------------------------------------------------------------ |\n| **KnowledgeUploader**     | OnlineInterface                        | Online knowledge interfaces access                           |\n|                           | Local_Text                             | Local texual knowledge access                                |\n|                           | Local_Table                            | Local table knowledge access                                 |\n|                           | Local_KG                               | Local knowledge graph access                                 |\n| **KnowledgePreprocessor** | TextConvertor                          | Format processing of local texual knowledge                  |\n|                           | TableConvertor                         | Format processing of local table knowledge                   |\n|                           | KGConvertor                            | Format processing of local knowledge graph                   |\n|                           | Interfacer                             | Online knowledge interface standardization                   |\n| **KnowledgeIntegrator**   | UnifiedVerbalizer                      | Convert various types of local heterogeneous knowledge into unified text  for knowledge integration |\n|                           | UnifiedQuerier                         | Incorporate different types of knowledge query interfaces for knowledge integration |\n| **KnowledgeSaver**        | SaveToElasticSearch                    | Save the data to ES database                                 |\n|                           | SaveToServer                           | Save the data to File system                                 |\n|                           | SaveQueryInterface                     | Save interfaces for active query                             |\n| **Knowledge**             | KnowledgeCombiner                      | Load knowledge from ES database                              |\n|                           | Knowledge_wikipedia                    | Load Wikipedia knowledge from ES database                    |\n|                           | Knowledge_wikipedia_wikidata           | Load Wikipedia+Wikidata knowledge from ES database           |\n|                           | Knowledge_wikipedia_wikidata_wikitable | Load Wikipedia+Wikidata+Wikitable knowledge from ES database |\n|                           | KnowledgeSelector                      | Load unified querier service, used in active query           |\n| **Retriever**             | Bm25MemoryRetriever                    | BM25 memory retrieval model                                  |\n|                           | Bm25EsRetriever                        | BM25 retrieval for ES database                               |\n|                           | ContrieverRetriever                    | BERT-based retrieval model                                   |\n|                           | DPR_retriever                          | BERT-based retrieval model                                   |\n|                           | BGE_retriever                          | BERT-based retrieval model                                   |\n|                           | BERT_retriever                         | Basic retrieval model                                        |\n|                           | E5_retriever                           | LLM-based retrieval model                                    |\n| **QueryGenerator**        | NL Parser                              | Natural language query generator                             |\n|                           | Sparql Parser                          | Sparql language query generator                              |\n| **Adaptive Interactor**   | Special Token-Timing Judgment          | Adaptive retrieval timing judgment                           |\n|                           | Self Consistency-Timing Judgment       | Adaptive retrieval timing judgment                           |\n|                           | Example demonstration-Source Selection | Adaptive retrieval source selection                          |\n| **Generator**             | OpenAIGenerator                        | Closed-source general  generator                             |\n|                           | RagGenerator                           | Retrieval instructions-enhanced generator                    |\n|                           | LLama2Generator                        | Open-source general  generator                               |\n|                           | Baichuan2Generator                     | Open-source general  generator                               |\n| **Multiplexer**           | Multiplexer                            | Pass data to multiple components simultaneously              |\n| **PromptBuilder**         | PromptBuilder                          | Create customized prompts                                    |\n| **OutputBuilder**         | OutputBuilder                          | Receive and normalize the output of system flow diagram      |\n| **Router**                | ConditionalRouter                      | Execute different branches based on conditions               |\n| **Evaluation**            | Evaluator                              | Manage K-LLMs system evaluation process                      |\n| **Controller**            | SelfRagShortDemoController             | Adaptive K-LLMs control flow diagram                         |\n|                           | CokController                          | Adaptive K-LLMs control flow diagram                         |\n| **DataSet**               | DataSet_*                              | Datasets loading and using                                   |\n\nUsers can select these predefined components or define their own components according to predefined formats. And configure component  parameters in KMatrix graphical interface. \n\n#### 1.2 Constructing components logic relations using control-logic flow diagram\n\nWe can use logic or control flow diagrams to organize component relationships for K-LLMs system construction. \n\n- **Deploy Control Flow Diagram:** for K-LLMs system with complex process (including multifarious arithmetic operations and logical judgments), we can use control flow diagram to design system process using Python programming.  Adding components to a  control flow diagram, and programming components logics.\n\n​\t\tstep1: Adding components to a  control flow diagram.\n\n<div align=center><img src=\"images/v2_create_ctro.png\" alt=\"image\" style=\"zoom: 50%;\" /></div>\n\n\n\n​\t\tstep2: programming components logics using python.\n\n<div align=center><img src=\"images/v2_rag_long.png\" alt=\"image\" style=\"zoom: 50%;\" /></div>\n\n\n\n- **Deploy Logic Flow Diagram:** for K-LLMs system with concise process (like linear, branching, looping, and conditional structures), we can employ logic flow diagram to directly connect components with edges. By jointly using control and logic flow diagram, KMatrix flexibly supports common K-LLMs patterns using naive, iterative, and adaptive enhancement methods. Draging components into the design area, and connecting them to build K-LLMs system. \n\n<img src=\"images/v2_inter.png\" alt=\"image\" style=\"zoom:50%;\" />\n\n\n\n- **Flow diagram design approachs for training, evaluation, and deployment of K-LLMs system**: for component training and evaluation, users can simply connect the Dataset component with the component to be trained/evaluated. For end-to-end evaluation of the K-LLMs system, users can employ the Evaluator component to connect Dataset component with K-LLMs system, and the Evaluator component will manage evaluation process. For K-LLMs system deployment, users can map the task inputs to the Multiplexer, and connect the task outputs to the OutputBuilder on the basis of original system flow diagram. After constructing system flow diagram, Users can excute it. \n\n![image](images/toolkit_usage.png)\n\n\n\n- **Integrated pre-built K-LLMs systems flow diagram instances:** we have integrated some pre-built K-LLMs systems flow diagram instances for user use, which contain four categories: Knowledge Integration, Training, Evaluation, Deployment for heterogeneous knowledge integration, K-LLMs system training, evaluation, deployment respectively. Users can click the [Use exising diagram] drop-down box on the frontend of toolkit, and use the corresponding flow diagram according to the Name.\n\n| Name                                        | Category              | Function Description                                         |\n| :------------------------------------------ | :-------------------- | :----------------------------------------------------------- |\n| Knowledge Integration/v16_save_upload_to_ES | Knowledge Integration | Converting various types of local heterogeneous knowledge (such as text, tables and knowledge graphs) into unified text fragments for local knowledge integration. |\n| Knowledge Integration/v16_UnifiedQuery      | Knowledge Integration | Incorporating different types of knowledge query interfaces (like Wikipedia, Wikidata) for online knowledge integration. |\n| Training/v16_train_selfrag                  | Training              | SelfRag Generator component training.                        |\n| Training/v16_train_contriever               | Training              | Contriever Retriever component training.                     |\n| Training/v16_train_BGE                      | Training              | BGE Retriever component training.                            |\n| Evaluation/v16_eva_contriever               | Evaluation            | Contriever Retriever component evaluation.                   |\n| Evaluation/v16_eva_BGE                      | Evaluation            | BGE Retriever component evaluation.                          |\n| Evaluation/v16_eva_Iterative                | Evaluation            | Evaluation of a Iterative K-LLMs system using local heterogeneous knowledge . |\n| Evaluation/v16_eva_selfRAG_short            | Evaluation            | Evaluation of an adaptive K-LLMs system using local heterogeneous knowledge. |\n| Deployment/v16_infer_naive_rag              | Deployment            | Deployment of a naive K-LLMs system using local heterogeneous knowledge. |\n| Deployment/v16_cok_de_diagram               | Deployment            | Deployment of an adaptive K-LLMs system using multiple knowledge query interfaces. |\n| Deployment/v16_infer_selfRAG_short          | Deployment            | Deployment of an adaptive K-LLMs system using heterogeneous knowledge. |\n| Deployment/v16_infer_iterative              | Deployment            | Deployment of a Iterative K-LLMs system using heterogeneous knowledge. |\n\n\n\n### 2 K-LLMs System Execution\n\nAfter constructing system flow diagram, Users can execute it. \n\n1. K-LLMs system training and evaluation flow diagram execution:  Click [Execute diagram] button to execute it, and click [Check execution results] to view the training or evaluation result logs.\n2. K-LLMs system deployment flow diagram execution: Click the [Deploy diagram] button to deploy it.  After completing deployment, enter your question in the question box and click [send] to generate reasoning steps and answer. The K-LLMs system deployment interface with multiple knowledge bases and multiple queries is shown in the following figure.\n\n![image](images/dep_res.png)\n\n\n\n##  :notebook: Knowledge And Datasets\n\n**Knowledge**:  KMatrix designs two ways of knowledge access: local knowledge and online interface. To support heterogeneous knowledge enhancemant, for local knowledge, we integrate public Wikipedia (text), Wikidata (knowledge graph) and Wikitable (table). For online interface, we integrate two general knowledge interfaces(Wikipedia and Wikidata query APIs) and six domain knowledge interfaces (APIs including google, etc). \n\n| Knowledge Access Way | Knowledge Name   | Knowledge Scale |\n| -------------------- | ---------------- | --------------- |\n| local knowledge      | Wikipedia        | 21000k          |\n|                      | Wikidata         | 5790k           |\n|                      | Wikitable        | 6860k           |\n| online interface     | Wikipedia        | /               |\n|                      | Wikidata         | /               |\n|                      | Flashcard        | 33553           |\n|                      | BioScienceqa     | 1062            |\n|                      | PhyScienceqa     | 780             |\n\n\n\n**Dataset**:  KMatrix provides five classes of datasets to support evaluation of K-LLMs system. We provide RETRIEVE\\_TRAIN and RETRIEVE_EVAL to train and evaluate Retriever components. We provide GENERATOR\\_TRAIN for adaptive retrieval training of  Generator components. We provide ODQA_EVAL and ODQA_EVAL_Simplified to evaluate knowledge enhancement performance of K-LLMs system under two ways of knowledge access: local knowledge and online interface respectively.  \n\n| Dataset Class        | Dataset Name | Dataset Scale |\n| -------------------- | ------------ | ------------- |\n| RETRIEVE_TRAIN       | MSMARCO      | 480k          |\n|                      | NQ           | 58k           |\n|                      | HotpotQA     | 84k           |\n| RETRIEVE_EVAL        | ArguAna      | 1401          |\n|                      | FiQA2018     | 6648          |\n|                      | HotpotQA     | 98k           |\n|                      | MSMARCO      | 510k          |\n|                      | NFCorpus     | 3237          |\n|                      | NQ           | 3452          |\n|                      | Quora        | 15k           |\n|                      | SciFact      | 1109          |\n| GENERATOR_TRAIN      | SelfRAG      | 146k          |\n| ODQA_EVAL            | 2Wikiqa      | 12576         |\n|                      | Hotpotqa     | 7405          |\n|                      | NQ           | 3610          |\n|                      | Popqa        | 1399          |\n|                      | Squad        | 10570         |\n|                      | Triviaqa     | 7313          |\n|                      | Webqa        | 2032          |\n| ODQA_EVAL_Simplified | Hotpotqa     | 308           |\n|                      | Medmcqa      | 146           |\n|                      | MMLU_bio     | 454           |\n|                      | MMLU_phy     | 253           |\n\n\n\n## :page_facing_up: Experimental Results\n\nWe report experimental results from two aspects: Knowledge access performance evaluation and Single vs. multi-knowledge bases enhancement evaluation.\n\n1. Knowledge access performance evaluation, which reports  the knowledge access performance of five Retriever components. \t\t\t\t\n\n|            | ArguAna      |        | FiQA2018 |        | HotpotQA  |        | MSMARCO     |        |\n| ---------- | ------------ | ------ | -------- | ------ | --------- | ------ | ----------- | ------ |\n|            | map@100      | r@100  | map@100  | r@100  | map@100   | r@100  | map@100     | r@100  |\n| BERT       | 6.87%        | 34.48% | 0.04%    | 0.71%  | 0.07%     | 0.5%   | 0.0%        | 0.02%  |\n| Contriever | 24.59%       | 97.36% | 27.38%   | 65.25% | 55.27%    | 77.76% | 21.90%      | 25.97% |\n| DPR        | 21.33%       | 89.94% | 11.75%   | 38.48% | 31.25%    | 57.83% | 16.00%      | 58.13% |\n| BGE        | 28.4%        | 96.79% | 37.55%   | 75.42% | 48.58%    | 64.89% | 36.28%      | 88.73% |\n| E5         | 30.50%       | 99.22% | 41.80%   | 79.52% | 39.04%    | 71.03% | 21.99%      | 78.27% |\n|            | **NFCorpus** |        | **NQ**   |        | **Quora** |        | **SciFact** |        |\n|            | map@100      | r@100  | map@100  | r@100  | map@100   | r@100  | map@100     | r@100  |\n| BERT       | 0.28%        | 3.22%  | 0.03%    | 0.30%  | 41.26%    | 67.85% | 1.56%       | 14.26% |\n| Contriever | 15.33%       | 29.93% | 43.23%   | 92.71% | 83.06%    | 99.35% | 62.88%      | 94.20% |\n| DPR        | 6.79%        | 17.90% | 22.29%   | 73.00% | 78.47%    | 97.78% | 29.95%      | 70.23% |\n| BGE        | 18.00%       | 33.94% | 44.66%   | 93.39% | 86.15%    | 99.70% | 69.04%      | 97.17% |\n| E5         | 11.42%       | 27.19% | 10.28%   | 41.22% | 85.57%    | 99.65% | 70.40%      | 96.00% |\n\n2. Single vs. multi-knowledge bases enhancement evaluation using local knowledge access way, which reports four K-LLMs systems performance using three types of local heterogeneous Knowledge: Wikipedia(Text)+Wikidata(KG)+Wikitable(Table). The K-LLMs systems include Naive-GEN(answer generation without knowledge), Naive-RAG(naive K-LLMs), Interleave (iterative K-LLMs) and Self-RAG (adaptive K-LLMs). \n\n| Methods    | Knowledge                                     | PopQA  | TriviaqaQA | NQ     | HotpotQA | 2Wikiqa | WebQA  |\n| ---------- | --------------------------------------------- | ------ | ---------- | ------ | -------- | ------- | ------ |\n| Naive-GEN  | Without                                       | 14.44% | 35.00%     | 8.53%  | 11.45%   | 17.57%  | 17.03% |\n|            | Wikipedia(Text)                               | 27.51% | 54.63%     | 33.77% | 20.39%   | 22.07%  | 31.74% |\n| Naive-RAG  | Wikipedia(Text)+Wikidata(KG)                  | 42.82% | 54.18%     | 33.68% | 20.73%   | 23.19%  | 31.10% |\n|            | Wikipedia(Text)+Wikidata(KG)+Wikitable(Table) | 42.89% | 54.68%     | 34.13% | 20.47%   | 23.43%  | 31.05% |\n|            | Wikipedia(Text)                               | 25.80% | 39.6%      | 24.96% | 14.7%    | 18.03%  | 22.74% |\n| Interleave | Wikipedia(Text)+Wikidata(KG)                  | 41.03% | 47.12%     | 25.01% | 16.38%   | 18.26%  | 23.23% |\n|            | Wikipedia(Text)+Wikidata(KG)+Wikitable(Table) | 41.17% | 46.27%     | 25.43% | 16.22%   | 22.1%   | 23.47% |\n|            | Wikipedia(Text)                               | 41.95% | 58.38%     | 29.28% | 25.80%   | 29.34%  | 34.69% |\n| Self-RAG   | Wikipedia(Text)+Wikidata(KG)                  | 61.37% | 58.23%     | 28.92% | 25.91%   | 29.99%  | 34.30% |\n|            | Wikipedia(Text)+Wikidata(KG)+Wikitable(Table) | 61.37% | 58.57%     | 29.25% | 25.71%   | 30.12%  | 34.84% |\n\n\n\n3. Single vs. multi-knowledge bases enhancement evaluation using online interface access way, which reports three K-LLMs methods performance using eight knowledge interfaces from four domains. The K-LLMs methods include the COT method without knowledge enhancement, selective query across multiple-domain knowledge interfaces, fixed query on single-domain knowledge interface.\n\n| Methods                    | Knowledge                                                    | Factual Domain | Medical Domain | Physics Domain | Biology Domain |\n| -------------------------- | ------------------------------------------------------------ | -------------- | -------------- | -------------- | -------------- |\n|                            |                                                              | Hotpotqa       | Medmcqa        | MMLU_phy       | MMLU_bio       |\n| COT                        | Without                                                      | 37.99%         | 40.41%         | 45.85%         | 78.63%         |\n| COK-DE<br/>Selective Query | Four domains, eight<br/>knowledge interfaces<br/>(Text,KG,Table) | 40.58%         | 46.58%         | 50.2%          | 78.63%         |\n| COK-DE<br/>Fixed Query     | Four domains, eight<br/>knowledge interfaces<br/>(Text,KG,Table) | 38.96%         | 44.52%         | 49.8%          | 77.97%         |\n\n\n\n\n\n##   :raised_hands: Additional FAQs\n\n- Q: Why do connection edges sometimes become disordered when constructing a flow diagram?\n  \n  A: This issue may occur if you delete a component and make further edits to the flow diagram. To resolve this, simply save the flow diagram firstly, refresh the page, and reload the flow diagram.\n  \n- ......\n\n\n\n\n## 🔖 License\n\nApache 2.0 License.\n\n \n\n## :dove:  Citing\nIf you use KMatrix in your research, please cite:\n\n`````\n@inproceedings{wu2024kmatrix,\n  title={KMatrix: A Flexible Heterogeneous Knowledge Enhancement Toolkit for Large Language Model},\n  author={Wu, Shun and Wu, Di and Luo, Kun and Zhang, XueYou and Zhao, Jun and Liu, Kang},\n  booktitle={Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing: System Demonstrations},\n  pages={280--290},\n  year={2024}\n}\n`````\n\n"
    },
    {
      "name": "LLM-Projects/haystack-book",
      "stars": 10,
      "img": "https://avatars.githubusercontent.com/u/140273596?s=40&v=4",
      "owner": "LLM-Projects",
      "repo_name": "haystack-book",
      "description": null,
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2023-10-24T14:05:23Z",
      "updated_at": "2025-04-16T15:41:25Z",
      "topics": [],
      "readme": "# Retrieval Augmented Generation In Production With Haystack\n\nThis repository is the official code repository for the book [Retrieval Augmented Generation In Production With Haystack](https://learning.oreilly.com/library/view/retrieval-augmented-generation/9781098165161/).\n\n<img src=\"https://github.com/LLM-Projects/haystack-book/assets/29293526/92bc15f5-f603-4122-8bbf-8d9715507db1\" width=30% height=30% class=\"center\">\n"
    },
    {
      "name": "LOH-puzik/LegalEase-AI",
      "stars": 9,
      "img": "https://avatars.githubusercontent.com/u/36806008?s=40&v=4",
      "owner": "LOH-puzik",
      "repo_name": "LegalEase-AI",
      "description": "LegalEaseAI simplifies legal topics with a document analyzer, legal counsel chatbot, and lawyer fee estimator. Powered by large language models, a vectorized database, and Flask.",
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2023-08-10T18:48:11Z",
      "updated_at": "2025-02-22T06:27:55Z",
      "topics": [],
      "readme": "# LegalEase-AI\n\n### LegalEaseAI is a web app that uses AI to make complex legal topics simpler by: \n- Analyzing & Simplifying large legal documents\n- Offering legal advice through a specialized law chatbot\n- Predicting potential lawyer expenses\n\n![](main.png)\n\n#### The main goal of LegalEaseAI is to make legal information easier to access and understand for everyone!\n\n<br>\n<br>\n\n## Legal Counsel\n\nThe Legal Counsel is a chatbot that simplifies interactions with legal subjects. \nUsers can ask any question and get fast responses. \nIt covers many US legal fields. Using NLP, a vector database, and two different language models, the chatbot acts like a legal counsel.\n\n<br>\n\n<p align=\"center\">\n  <img src=\"demo1.gif\"/>\n</p>\n\n<br>\n\nThe chatbot operates like an open-book exam. \nIt begins with a user query that is transformed into a vector by the Retriever. \nThis vector undergoes a similarity search in the vector database. \nThe top matches are converted back into text and sent to the Generator, which provides several answers. \nThe highest scoring answer is selected and shown to the user, mimicking the open-book exam process using AI mimicking the open-book exam process.\n\n<br>\n\n![](le1.png)\n\n<br>\n<br>\n\n## Document Analyzer\n\nThe Document Analyzer breaks down complex legal texts for easy understanding, Users will simply have to upload a contract in a PDF form \nand the tool auto-extracts and simplifies the content:\n- Generates a concise summary\n- Highlights crucial points\n- Employs advanced NLP and two distinct LLMs\n\n<br>\n\n<p align=\"center\">\n  <img src=\"demo2.gif\"/>\n</p>\n\n<br>\n\nThe Document Analyzer works by dividing a legal document into manageable paragraphs.\nEach paragraph is then summarized, and duplicate summaries are removed. \nA single overview paragraph is created from these summaries, and the most important points are identified and listed out. \nThis process simplifies large legal texts, creating brief summaries and key points, making it easier for users to understand their legal commitments.\n\n<br>\n\n![](le3.png)\n\n<br>\n<br>\n\n## Lawyer Cost Estimator\n\nThe Lawyer Cost Estimator is a tool that uses machine learning to quickly estimate potential lawyer costs in the United States, \nbased on specific case details:\n- Utilizes a regression machine learning algorithm \n- Relies on a custom dataset\n- Offers estimations based on the most recent prices\n\n<br>\n\n<p align=\"center\">\n  <img src=\"demo3.gif\"/>\n</p>\n\n<br>\n<br>\n<br>\n\n### *To sum it up, this project combines  AI technologies and techniques such as large-language-models (LLMs), natural language processing (NLP) methods and web tools like vectorized databases, ReactJS and Flask. It's a real example of how AI can make things better for everyone!*\n\n\n"
    },
    {
      "name": "yordanoswuletaw/covid19-qa-system",
      "stars": 9,
      "img": "https://avatars.githubusercontent.com/u/101830198?s=40&v=4",
      "owner": "yordanoswuletaw",
      "repo_name": "covid19-qa-system",
      "description": "COVID-19 automated question answering system developed using Natural Language Processing and over 250, 000 COVID-19 research papers and articles. This system can help researchers, clinicians, and the public to quickly and easily find information about COVID-19. ",
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2023-10-23T08:51:42Z",
      "updated_at": "2025-04-19T10:09:37Z",
      "topics": [],
      "readme": "# COVID-19 Q&A System\n\n### About\nCOVID-19 automated question answering system developed using Deep Learning, Natural Language Processing and over 250, 000 COVID-19 research papers and articles. This system can help researchers, clinicians, and the public to quickly and easily find information about COVID-19. This system facilitates rapid and user-friendly access to crucial information about COVID-19, enhancing the efficiency and effectiveness of information retrieval.\n\n### Purpose\n\n1. **Research Assistance:** Researchers can use this system to quickly access and extract relevant information from a vast pool of COVID-19 research. This can save them time and effort in finding the latest studies, statistics, and findings.\n\n2. **Clinical Decision Support:** Healthcare professionals can use the system to help them make informed clinical decisions. The system can provide guidance based on the 2022 medical research and guidelines regarding COVID-19.\n\n3. **Public Awareness:** The system is a valuable tool for the general public. It allows people to access reliable and easy-to-understand information about COVID-19. This can help people to learn more about the disease, take preventive measures, and make informed decisions.\n\n4. **Educational Resource:** Educational institutions and instructors can use the system to provide students with accurate and timely information about the COVID-19 pandemic. This can help students to learn more about the disease and its impact.\n\n5. **Policy Development:** Government agencies and policymakers can use the system to easily informed about COVID.\n"
    },
    {
      "name": "fvanlitsenburg/promptbox",
      "stars": 9,
      "img": "https://avatars.githubusercontent.com/u/68997090?s=40&v=4",
      "owner": "fvanlitsenburg",
      "repo_name": "promptbox",
      "description": "Tying together LLM prompts graphically",
      "homepage": null,
      "language": "Python",
      "created_at": "2023-06-22T11:52:30Z",
      "updated_at": "2025-02-10T20:25:23Z",
      "topics": [],
      "readme": "# Promptbox\n\nPromptbox is a \"private GPT\" solution that allows you to perform analysis on your own documents within a secure, circumscribed environment.\n\nMore information can be found in this Medium article:\nhttps://medium.com/@fvanlitsenburg/building-a-privategpt-with-haystack-part-1-why-and-how-de6fa43e18b\n\n# Installation\n\n## Downloading Large Language Models from Huggingface\n\nPromptbox runs on locally stored models. It has been built to run on Ubuntu 20.04.\n\nTo get the models locally, we need git-lfs:\n\n```\ncurl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh | sudo bash\nsudo apt-get install git-lfs\ngit lfs install\n```\n\nIn the file `ui/utils.py`, we reference the model paths. Therefore, we need to create this file at the same directory level as where we'll install Promptbox.\n\n```\nmkdir hf\ncd hf\n```\n\nPromptbox uses flan-t5-base and fastchat-t5-3b-v1.0. Other models can be used as well. To use other models, specify them in line 15 of `ui/Home.py`.\n\n```\ngit clone https://huggingface.co/google/flan-t5-base\ngit clone https://huggingface.co/lmsys/fastchat-t5-3b-v1.0\n```\n\n## Downloading Promptbox and Haystack repositories\n\n```\ngit clone -b v1.17.1 https://github.com/deepset-ai/haystack.git\ngit clone https://github.com/fvanlitsenburg/promptbox.git\n```\n\n## Running Haystack and Promptbox\n\nAssuming you have created a separate python environment in pyenv or conda.\n\nAssuming you have docker-compose installed and running.\n\nFirst,\n\n```\n# Upgrade pip\npip install --upgrade pip\n\n# Install Haystack in editable mode\ncd haystack\npip install -e '.[all]'\n\n# Run Haystack\ndocker-compose up\n```\n\nIn a separate terminal, let's get Promptbox running.\n\n```\ncd promptbox\npip install -r requirements.txt\nstreamlit run ui/Home.py\n```\n\nThat's it! Promptbox should now be running on localhost:8501.\n"
    },
    {
      "name": "eea/nlp-service",
      "stars": 9,
      "img": "https://avatars.githubusercontent.com/u/1176627?s=40&v=4",
      "owner": "eea",
      "repo_name": "nlp-service",
      "description": "NLP Service based on FastAPI",
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2021-07-14T12:31:00Z",
      "updated_at": "2025-03-21T23:05:49Z",
      "topics": [],
      "readme": "# EEA AI NLP Service\n\n## Requirements\n\nPython 3.6+, a GPU-powered server\n\nBy default it would operate the following endpoints (go to\n[local OpenAPI page](http://localhost:8000/docs) for an overview):\n\n- **embedding**: return sentence embedding using Facebook's dual head encoder\n  models used by Haystack for QA.\n- **ner**: a NER model. WIP\n- **similarity**: Compute similarity between sentences\n- **zeroshot-classifier**: Classifiy text according to provided possible\n  categories\n- **qa**: Question and answer model\n- **question-classifier**: Classify phrases in two categories:\n  question/statement\n- **question-generation**: Generate questions and possible answers based on\n  provided text\n- **summarizer**: Automatically summarize text\n\nYou can customize which services to start via an environment variable:\n\n```bash\nexport NLP_SERVICES=\"embedding,ner,summarizer\"\n```\n\n## Instalation\n\n```\n\tconda install mamba -n base -c conda-forge\n\tconda create -n py38 python=3.8\n\tconda init fish\n\tconda activate py38\n\tmamba install pytorch cudatoolkit=10.2 tensorflow tensorflow-hub -c pytorch\n\tpip install -e .\n```\n\nSee [install](./install.md) for installation instructions.\n\n## Copyright and license\n\nThe Initial Owner of the Original Code is European Environment Agency (EEA).\nAll Rights Reserved.\n\nSee [LICENSE](https://github.com/eea/nlp-service/blob/master/LICENSE) for details.\n\n## Funding\n\n[European Environment Agency (EU)](http://eea.europa.eu)\n"
    },
    {
      "name": "AccessibleAI/ailibrary",
      "stars": 9,
      "img": "https://avatars.githubusercontent.com/u/13865104?s=40&v=4",
      "owner": "AccessibleAI",
      "repo_name": "ailibrary",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2019-10-10T09:03:24Z",
      "updated_at": "2025-03-14T17:41:40Z",
      "topics": [],
      "readme": "# The cnvrg AI Library\nThe AI Library is an ML package manager.  \nBuild, share and reuse machine learning components that are packaged with code, docker containers and compute engine types.  \nQuickly execute across any kind of infrastructure and automatically track hyper-parameters, metrics, utilization, and more.  \n\n## Content\n1) **'sk'** directories: Those include the AI libraries for machine learning which are backed by scikit learn.\nScikit learn is a free software machine learning module in Python language. It features various classification, regression and clustering algorithms.\nIn the cnvrg library we use mostly classification, regression and clustering algorithms.\n\n2) **'tf2'** directories: Those include the AI libraries for deep learning. The purpose of those\nlibraries is not to build or create models, but rather to leverage prepared model structures.  \nCurrently, the library includes commonly known successful models structures like ResNet50 and vgg16, and they are imported\nwith ImageNet weights, so when used they perform transfer learning.\n\n3) **'pr'** directories: Those include libraries which enables pre-processing of various types of files.\n\n3) Directories begins with **'_'**: Those are not for user's use, but for internal testing at cnvrg.io and version control.\n\n## Metrics\nThere are some metrics which are created in the libraries.  \nThe metrics are changed between families of algorithm (e.g: classification and regression algorithms has different metrics).  \n\n### Classification Metrics\n```model``` - The name of the output model file.  \n```folds``` - The number of folds used in the cross validation (if demanded by the user).  \n```train_acc``` - float in (0, 1).  \n```train_loss``` - calculated by default by mean_squared_error.  \n```test_acc``` - float in (0, 1).  \n```test_loss``` - calculated by default by mean_squared_error.  \nExample:  \n\n\n### Regression Metrics\n```model``` - The name of the output model file.  \n```folds``` - The number of folds used in the cross validation (if demanded by the user).  \n```train_loss_MSE``` - The train error evaluated by mean squared error.  \n```train_loss_MAE``` - The train error evaluated by mean absolute error.  \n```train_loss_R2``` - The train error evaluated by R2.  \n```test_loss_MSE``` - The test error evaluated by mean squared error.  \n```test_loss_MAE``` - The test error evaluated by mean absolute error.  \n```test_loss_R2``` - The test error evaluated by R2.  \nExample:  \n\n\n### Deep Learning Metrics\n**SOON**\n\n\n### Clustering Metrics\n**SOON**  \nExample: \n\n\n## Visualizations\nThere are several types of visualizations which are built-in the libraries and are plotted automatically.  \nNot all the libraries plot all the possible visualizations due to differences in the the algorithms.  \n\n#### Classification Report\nClassification Report is a textual report shows some main classification metrics.  \nIt is important to notice that classification report is relevant only for classification algorithms.  \nThe metrics which it shows are: precision, recall, f1-score and support.  \n* Precision - \n\n* Recall - \n\n* F1-score -\n\n* Support -\n\nExamples:\n1) Iris data set (3 labels):  \n![first_class_rep](https://github.com/AccessibleAI/ailibrary/blob/master/_docs/readme_images/classification_report.png)  \n\n2) 2-labels data set:  \n![second_class_rep](https://github.com/AccessibleAI/ailibrary/blob/master/_docs/readme_images/classification_report_2.png)\n\n#### Confusion Matrix\nConfusion Matrix is a table which evaluates accuracy by computing the confusion matrix with each row corresponding to the true class.  \nEach row of the matrix represents the instances in a predicted class while each column represents the instances in an actual class (or vice versa).  \nThe name stems from the fact that it makes it easy to see if the system is confusing two classes (i.e. commonly mislabeling one as another).  \nFor instance, lets say we have a data set with 4 different labels: a, b, c, d.  \nSo, the confusion matrix is going to be 4 * 4 table where the y-axis describes the true label, and the x-axis describes the predicted label.  \n\nExamples:  \n1) 2-labels data set:  \n![first_conf_mat](https://github.com/AccessibleAI/ailibrary/blob/master/_docs/readme_images/confustion_matrix.png)\n\n2) 3-labels data set:  \n![second_conf_mat](https://github.com/AccessibleAI/ailibrary/blob/master/_docs/readme_images/confustion_matrix_2.png)\n\n#### Feature Importance\nFeature importance is a bars chars where the features names are the x-axis and the y-axis is a range of floats between 0 to 1 (or the opposite).  \nEach bar, belongs the single feature, represents a float number which is the score of importance of the specific feature in the prediction process.  \nIt can be used for feature selection, dimensionality reduction, improving estimators accuracy and boosting performance.  \n\nExamples:  \n1)  \n![first_fea_imp](https://github.com/AccessibleAI/ailibrary/blob/master/_docs/readme_images/feature_importance.png)\n\n2)  \n![second_fea_imp](https://github.com/AccessibleAI/ailibrary/blob/master/_docs/readme_images/feature_importance_3.png)\n\n#### ROC curve\nA ROC (receiver operating characteristic curve) is a graphical plot that illustrates the diagnostic ability of a binary classifier.  \nThe ROC curve is created by plotting the true positive rate (TP/(TP+FN)) against the false positive rate (FP/(FP + TN)) at various threshold settings.  \nNote: this implementation is restricted to the binary classification task.  \n\nExamples:  \n1)  \n![first_roc_curve](https://github.com/AccessibleAI/ailibrary/blob/master/_docs/readme_images/roc_curve_1.png)  \n\n#### Correlation\nCorrelation is any statistical relationship between two random variables or bivariate data.  \nIn the broadest sense correlation is any statistical association, though it commonly refers to the degree to which a pair of variables are linearly related.  \nCorrelations are useful because they can indicate a predictive relationship that can be exploited in practice.  \nData Scientist or Statisticians might be interested in correlation in order to make dimensionality reduction, improving estimators accuracy and boosting performance.  \n\nExamples:\n1)  \n![first_corr](https://github.com/AccessibleAI/ailibrary/blob/master/_docs/readme_images/correlation.png)  \n\n#### feature-vs-feature\nWhen our library detects strong correlation (smaller than -0.7 or greater than 0.7), it automatically produces a scatter plot which presents the correlation.  \n\nExamples:  \n1) Watch the following correlation table:  \n![second_corr](https://github.com/AccessibleAI/ailibrary/blob/master/_docs/readme_images/correlation_2.png)  \nAs you can see (in the popped window), there is a high correlation (0.93) between the two features.  \nSo, the library automatically produces the following scatter plot:  \n![first-fea-vs-fea](https://github.com/AccessibleAI/ailibrary/blob/master/_docs/readme_images/feature_against_feature_2.png)  \nWhich shows the strong relation between the two (where the first grows, the second grows almost linearly).  \n"
    },
    {
      "name": "sunnysavita10/RAG-With-Haystack-MistralAI-Pinecone",
      "stars": 9,
      "img": "https://avatars.githubusercontent.com/u/56354186?s=40&v=4",
      "owner": "sunnysavita10",
      "repo_name": "RAG-With-Haystack-MistralAI-Pinecone",
      "description": null,
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2024-04-15T19:56:58Z",
      "updated_at": "2025-03-31T00:47:32Z",
      "topics": [],
      "readme": ""
    },
    {
      "name": "mixedbread-ai/mixedbread-ai-haystack",
      "stars": 9,
      "img": "https://avatars.githubusercontent.com/u/153227975?s=40&v=4",
      "owner": "mixedbread-ai",
      "repo_name": "mixedbread-ai-haystack",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2024-01-27T13:40:12Z",
      "updated_at": "2025-04-04T02:16:52Z",
      "topics": [],
      "readme": "# Mixedbread AI Haystack 2.0 Integration\n[![PyPI version](https://badge.fury.io/py/mixedbread-ai-haystack.svg)](https://badge.fury.io/py/mixedbread-ai-haystack)\n[![Python versions](https://img.shields.io/pypi/pyversions/mixedbread-ai-haystack.svg)](https://pypi.org/project/mixedbread-ai-haystack/) \n\n### **Table of Contents**\n\n- [Overview](#overview)\n- [Installation](#installation)\n- [Usage](#usage)\n\n## Overview\n\n[mixedbread ai](https://www.mixedbread.ai) is an AI start-up that provides open-source, as well as, in-house embedding and reranking models. You can choose from various foundation models to find the one best suited for your use case. More information can be found on the [documentation page](https://www.mixedbread.ai/api-reference/integrations#haystack).\n\n## Installation\n\nInstall the Mixedbread AI integration with a simple pip command:\n\n```bash\npip install mixedbread-ai-haystack\n```\n\n## Usage\n\nThis integration comes with 3 components:\n- [`MixedbreadAITextEmbedder`](https://github.com/mixedbread-ai/mixedbread-ai-haystack/blob/main/mixedbread_ai_haystack/embedders/text_embedder.py)\n- [`MixedbreadAIDocumentEmbedder`](https://github.com/mixedbread-ai/mixedbread-ai-haystack/blob/main/mixedbread_ai_haystack/embedders/document_embedder.py).\n- [`MixedbreadAIReranker`](https://github.com/mixedbread-ai/mixedbread-ai-haystack/blob/main/mixedbread_ai_haystack/rerankers/reranker.py)\n\nFor documents you can use `MixedbreadAIDocumentEmbedder` and for queries you can use `MixedbreadAITextEmbedder`. Once you've selected the component for your specific use case, initialize the component with the `model` and the [`api_key`](https://www.mixedbread.ai/dashboard?next=api-keys). You can also set the environment variable `MXBAI_API_KEY` instead of passing the api key as an argument.\n\n### Embedders In a Pipeline\n\n```python\nfrom haystack import Document, Pipeline\nfrom haystack.document_stores.in_memory import InMemoryDocumentStore\nfrom haystack.components.writers import DocumentWriter\nfrom haystack.components.retrievers.in_memory import InMemoryEmbeddingRetriever\nfrom mixedbread_ai_haystack.embedders import MixedbreadAIDocumentEmbedder, MixedbreadAITextEmbedder\n\n# Set-up the Document Store and Documents\ndocument_store = InMemoryDocumentStore(embedding_similarity_function=\"cosine\")\ndocuments = [\n    Document(content=\"china is the most populous country in the world.\"), \n    Document(content=\"india is the second most populous country in the world.\"), \n    Document(content=\"united states is the third most populous country in the world.\")\n]\n\n# Indexing Pipeline\nindexing_pipeline = Pipeline()\nindexing_pipeline.add_component(\"doc_embedder\", MixedbreadAIDocumentEmbedder(model=\"mixedbread-ai/mxbai-embed-large-v1\"))\nindexing_pipeline.add_component(\"writer\", DocumentWriter(document_store=document_store))\nindexing_pipeline.connect(\"doc_embedder\", \"writer\")\n\nindexing_pipeline.run({\"doc_embedder\": {\"documents\": documents}})\n\n# Query Pipeline\ntext_embedder = MixedbreadAITextEmbedder(model=\"mixedbread-ai/mxbai-embed-large-v1\")\nquery_pipeline = Pipeline()\nquery_pipeline.add_component(\"text_embedder\", text_embedder)\nquery_pipeline.add_component(\"retriever\", InMemoryEmbeddingRetriever(document_store=document_store))\nquery_pipeline.connect(\"text_embedder.embedding\", \"retriever.query_embedding\")\n\nresults = query_pipeline.run({\"text_embedder\": {\"text\": \"Which country has the biggest population?\"}})\ntop_document = results[\"retriever\"][\"documents\"][0].content\nprint(top_document)\n```\n\n### Reranker In a Pipeline\n```python\nfrom haystack import Document, Pipeline\nfrom haystack.document_stores.in_memory import InMemoryDocumentStore\nfrom haystack.components.retrievers.in_memory import InMemoryBM25Retriever\nfrom mixedbread_ai_haystack.rerankers import MixedbreadAIReranker\n\n# Set-up the Document Store and Documents\ndocuments = [\n    Document(content=\"china is the most populous country in the world.\"),\n    Document(content=\"india is the second most populous country in the world.\"),\n    Document(content=\"united states is the third most populous country in the world.\")\n]\ndocument_store = InMemoryDocumentStore()\ndocument_store.write_documents(documents)\n\n# Define the Retriever and Reranker\nretriever = InMemoryBM25Retriever(document_store=document_store)\nreranker = MixedbreadAIReranker(model=\"mixedbread-ai/mxbai-rerank-large-v1\", top_k=3)\n\n# Rerank Pipeline\nrerank_pipeline = Pipeline()\nrerank_pipeline.add_component(\"retriever\", retriever)\nrerank_pipeline.add_component(\"reranker\", reranker)\nrerank_pipeline.connect(\"retriever.documents\", \"reranker.documents\")\n\n# Query and Rerank\nquery = \"Which country has the second largest population\"\nresults = rerank_pipeline.run({\"retriever\": {\"query\": query}, \"reranker\": {\"query\": query, \"top_k\": 3}})\nprint(results)\n```\n\n### Full Example With Metadata\n```python\nimport os\nfrom datasets import load_dataset\nfrom haystack import Pipeline, Document\nfrom haystack.document_stores.in_memory import InMemoryDocumentStore\nfrom haystack.components.writers import DocumentWriter\nfrom haystack.components.retrievers.in_memory import InMemoryEmbeddingRetriever\nfrom mixedbread_ai_haystack import MixedbreadAIDocumentEmbedder, MixedbreadAITextEmbedder, MixedbreadAIReranker\n\n# Set API Key\nos.environ[\"MXBAI_API_KEY\"] = \"YOUR_API_KEY\"\n\n# Load the Dataset and Prepare Documents\nds = load_dataset(\"rajuptvs/ecommerce_products_clip\")\ndocuments = [\n    Document(\n        id=str(i),\n        content=data[\"Description\"], meta={\n        \"name\": data[\"Product_name\"],\n        \"price\": data[\"Price\"],\n        \"colors\": data[\"colors\"],\n        \"pattern\": data[\"Pattern\"],\n        \"extra\": data[\"Other Details\"]\n    }) for i, data in enumerate(ds[\"train\"])\n]\nmeta_fields = documents[0].meta.keys()\n\n# Define the Components\ndocument_store = InMemoryDocumentStore(embedding_similarity_function=\"cosine\")\ndocument_writer = DocumentWriter(document_store=document_store)\nembedding_retriever = InMemoryEmbeddingRetriever(document_store=document_store, top_k=20)\n\nembed_model = \"mixedbread-ai/mxbai-embed-large-v1\"\nreranking_model = \"mixedbread-ai/mxbai-rerank-large-v1\" \n\ntext_embedder = MixedbreadAITextEmbedder(model=embed_model)\ndocument_embedder = MixedbreadAIDocumentEmbedder(model=embed_model, max_concurrency=3, meta_fields_to_embed=meta_fields, show_progress_bar=True)\nreranker = MixedbreadAIReranker(model=reranking_model, meta_fields_to_rank=meta_fields, top_k=5)\n\n# Indexing Pipeline\nindexing_pipeline = Pipeline()\nindexing_pipeline.add_component(instance=document_embedder, name=\"document_embedder\")\nindexing_pipeline.add_component(instance=document_writer, name=\"document_writer\")\nindexing_pipeline.connect(\"document_embedder\", \"document_writer\")\n\n# Query Pipeline\nquery_pipeline = Pipeline()\nquery_pipeline.add_component(instance=text_embedder, name=\"text_embedder\")\nquery_pipeline.add_component(instance=embedding_retriever, name=\"embedding_retriever\")\nquery_pipeline.add_component(instance=reranker, name=\"reranker\")\nquery_pipeline.connect(\"text_embedder\", \"embedding_retriever\")\nquery_pipeline.connect(\"embedding_retriever.documents\", \"reranker.documents\")\n\n# Index the dataset\nindexing_pipeline.run({\"document_embedder\": {\"documents\": documents}})\n\n# Query to get results\nquery = \"I am looking for a regular fit t-shirt in blue color. Ideally without any prints. What are my options?\"\nresults = query_pipeline.run(\n    {\n        \"text_embedder\": {\"text\": query},\n        \"reranker\": {\"query\": query}\n    }\n)\nprint(results[\"reranker\"][\"documents\"])\n```\n"
    },
    {
      "name": "AssemblyAI/assemblyai-haystack",
      "stars": 9,
      "img": "https://avatars.githubusercontent.com/u/24515738?s=40&v=4",
      "owner": "AssemblyAI",
      "repo_name": "assemblyai-haystack",
      "description": "Haystack integration",
      "homepage": null,
      "language": "Python",
      "created_at": "2023-10-31T15:48:00Z",
      "updated_at": "2025-02-25T18:25:13Z",
      "topics": [],
      "readme": "<img src=\"https://github.com/AssemblyAI/assemblyai-python-sdk/blob/master/assemblyai.png?raw=true\" width=\"500\"/>\n\n---\n\n\n[![CI Passing](https://github.com/AssemblyAI/assemblyai-python-sdk/actions/workflows/test.yml/badge.svg)](https://github.com/AssemblyAI/assemblyai-haystack/actions/workflows/test.yml)\n[![GitHub License](https://img.shields.io/github/license/AssemblyAI/assemblyai-haystack)](https://github.com/AssemblyAI/assemblyai-haystack/blob/main/LICENSE)\n[![PyPI version](https://badge.fury.io/py/assemblyai-haystack.svg)](https://badge.fury.io/py/assemblyai-haystack)\n[![PyPI Python Versions](https://img.shields.io/pypi/pyversions/assemblyai-haystack)](https://pypi.python.org/pypi/assemblyai-haystack/)\n![PyPI - Wheel](https://img.shields.io/pypi/wheel/assemblyai-haystack)\n[![AssemblyAI Twitter](https://img.shields.io/twitter/follow/AssemblyAI?label=%40AssemblyAI&style=social)](https://twitter.com/AssemblyAI)\n[![AssemblyAI YouTube](https://img.shields.io/youtube/channel/subscribers/UCtatfZMf-8EkIwASXM4ts0A)](https://www.youtube.com/@AssemblyAI)\n[![Discord](https://img.shields.io/discord/875120158014853141?logo=discord&label=Discord&link=https%3A%2F%2Fdiscord.com%2Fchannels%2F875120158014853141&style=social)\n](https://assemblyai.com/discord)\n\n# AssemblyAITranscriber\n\nThis custom component is designed for using AssemblyAI with [Haystack (2.x)](https://github.com/deepset-ai/haystack), an open source Python framework for building custom LLM applications. It seamlessly integrates with the AssemblyAI API and enhances Haystack's capabilities.\n\nThe AssemblyAITranscriber goes beyond simple audio transcription; it also offers features such as summarization and speaker diarization. This allows you to not only convert audio to text but also obtain concise summaries and identify speakers in the conversation. To use AssemblyAITranscriber, you should pass your `ASSEMBLYAI_API_KEY` as an argument while adding a component (see usage code example below). \n\nMore info about AssemblyAI:\n\n* [Website](https://www.assemblyai.com/)\n* [Get a Free API key](https://www.assemblyai.com/dashboard/signup)\n* [AssemblyAI API Docs](https://www.assemblyai.com/docs)\n\n## Installation\n\nFirst, install the assemblyai-haystack python package.\n\n```bash\npip install assemblyai-haystack\n```\n\nThis package installs and uses the AssemblyAI Python SDK. You can find more info about the SDK at the [assemblyai-python-sdk GitHub repo]([https://www.assemblyai.com/docs](https://github.com/AssemblyAI/assemblyai-python-sdk)).\n\n## Usage\n\nThe `AssemblyAITranscriber` needs to be initialized with the AssemblyAI API key. \nThe `run` function needs at least the file_path argument. Audio files can be specified as an URL or a local file path.\nYou can also specify whether you want summarization and speaker diarization results in the `run` function.\n\n```python\nimport os\n\nfrom assemblyai_haystack.transcriber import AssemblyAITranscriber\nfrom haystack.document_stores.in_memory import InMemoryDocumentStore\nfrom haystack import Pipeline\nfrom haystack.components.writers import DocumentWriter\n\nASSEMBLYAI_API_KEY = os.environ.get(\"ASSEMBLYAI_API_KEY\")\n\n## Use AssemblyAITranscriber in a pipeline\ndocument_store = InMemoryDocumentStore()\nfile_url = \"https://github.com/AssemblyAI-Examples/audio-examples/raw/main/20230607_me_canadian_wildfires.mp3\"\n\nindexing = Pipeline()\nindexing.add_component(\"transcriber\", AssemblyAITranscriber(api_key=ASSEMBLYAI_API_KEY))\nindexing.add_component(\"writer\", DocumentWriter(document_store))\nindexing.connect(\"transcriber.transcription\", \"writer.documents\")\nindexing.run(\n    {\n        \"transcriber\": {\n            \"file_path\": file_url,\n            \"summarization\": None,\n            \"speaker_labels\": None,\n        }\n    }\n)\n\nprint(\"Indexed Document Count:\", document_store.count_documents())\n```\n\nNote: Calling `indexing.run()` blocks until the transcription is finished.\n\nThe results of the transcription, summarization and speaker diarization are returned in separate document lists:\n* transcription\n* summarization\n* speaker_labels\n\nThe metadata of the transcription document contains the transcription ID and url of the uploaded audio file.\n\n```json\n{\n   \"transcript_id\":\"73089e32-...-4ae9-97a4-eca7fe20a8b1\",\n   \"audio_url\":\"https://storage.googleapis.com/aai-docs-samples/nbc.mp3\"\n}\n```\n  \n"
    },
    {
      "name": "deepset-ai/biqa-llm",
      "stars": 8,
      "img": "https://avatars.githubusercontent.com/u/51827949?s=40&v=4",
      "owner": "deepset-ai",
      "repo_name": "biqa-llm",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2023-11-24T08:54:44Z",
      "updated_at": "2024-07-22T10:57:49Z",
      "topics": [],
      "readme": "## BIQA with LLMs\n\nExplorations in using latest LLMs for BIQA.\n\n### Data\n\nThe data is from the [Stack Overflow Developer Survey 2023](https://survey.stackoverflow.co/2023/).\n\n- `data/eval_set_multi_answers_res.json`: Question and query pairs as list of `SQLSample`s with possibly more than one valid SQL for a question. Also results included.\n- `data/survey_results_normalized_v2.db`: The main sqlite file. Download from here: [deepset/stackoverflow-survey-2023-text-sql](https://huggingface.co/datasets/deepset/stackoverflow-survey-2023-text-sql).\n\nOr download as:\n```console\nwget -O data/survey_results_normalized_v2.db \"https://drive.google.com/uc?export=download&id=1e_knoK9rYgWe8ADUw3PC8Fp6Jhnjgoms&confirm=t\"\n```\n\n### Environment setup\n\n```console\npip install -r requirements.txt\n```\n\n### Running the Evaluation\n\n_Note: To use the OpenAI models, the `OPENAI_API_KEY` environment variable needs to be set. Can also put in `.env` file to be loaded by `python-dotenv`_\n\nCreate annotations i.e. fill in the predictions for the eval set:\n\n```console\npython create_annotations.py \\\n       -i data/eval_set_multi_answers_res.json \\\n       -o eval_preds.json \\\n```\n\nEvaluate manually i.e. go through each evaluation (where pred available) and label them correct or not. If labelled correct, the prediction is added to the labels/answers.\n```console\npython evaluate_manually.py eval_preds.json eval_preds_manual.json\n```\n\nCalculate the final performance metrics:\n\n```console\npython calculate_metrics.py eval_preds_manual.json\n```\n\n### Different Approaches\n\nSchema + Examples:\n```console\npython create_annotations.py \\\n       -i data/eval_set_multi_answers_res.json \\\n       -o eval_preds.json \\\n```\n\nSchema + raw description\n```console\npython create_annotations.py \\\n -i data/eval_set_multi_answers_res.json\\\n -o eval_preds_base_raw_desc.json \\\n -g base --raw-description \\\n```\n\nSchema + column descriptions + few shot\n```console\npython create_annotations.py \\\n-i data/eval_set_multi_answers_res.json \\\n-o eval_preds_base_col_desc_fs.json \\\n-g base -d per-column --few-shot\n```\n\nAgents:\n```console\npython create_annotations.py \\\n -i data/eval_set_multi_answers_res.json \\\n -o eval_preds_agents.json \\\n -g agent\n```\n\n(Perfect) Retrieval:\n```console\npython create_annotations.py \\ \n -i data/eval_set_multi_answers_res.json \\\n -o eval_preds_retriever.json \\\n -g retriever -d per-column \\\n```\n\n### Running the API\n\nFor the application to work with OpenAI models, the `OPENAI_API_KEY` environment variable needs to be set.\n\nYou can set it directly or put it in the `.env` file.\n\n```console\nuvicorn api.main:app --reload --host=\"0.0.0.0\" --port=8000\n```\n\n### Helper scripts\n\n`column_descriptions.py`: To generate the \"seed\" column-level description file (to be completed manually)\n`count_tokens.py`: Count number of tokens or to view the final prompt\n`retriever_analysis.py`: Analysis of the retriever performance + plot generation\n`view_eval_set.py`: View the data in the eval (or prediction) set\n\n### Appendix\n\n#### Data Creation\n\nCreated with this [Notebook](https://colab.research.google.com/drive/12NUeRMsld0toXMSXKFMaQVAv58XwOAT1?usp=sharing); uses [this spreadsheet](https://docs.google.com/spreadsheets/d/1Xh_TgMbyitvtw08g0byEmBpkwDGZDdBYenthOzcK6qI/edit?usp=sharing) defining manual adjustments."
    },
    {
      "name": "AIAnytime/Fashion-Search-App-using-AI",
      "stars": 8,
      "img": "https://avatars.githubusercontent.com/u/30049737?s=40&v=4",
      "owner": "AIAnytime",
      "repo_name": "Fashion-Search-App-using-AI",
      "description": "Fashion Search App (Text to Image Search) using AI Model.",
      "homepage": null,
      "language": "Python",
      "created_at": "2023-09-09T11:37:03Z",
      "updated_at": "2024-07-31T21:09:19Z",
      "topics": [],
      "readme": "# Fashion Search App\nFashion Search App (Text to Image Search) using AI Model.\n\n#### You can search in your image base using query (Text to Image Search). \n"
    },
    {
      "name": "bilgeyucel/haystack-chainlit-demo",
      "stars": 8,
      "img": "https://avatars.githubusercontent.com/u/25897116?s=40&v=4",
      "owner": "bilgeyucel",
      "repo_name": "haystack-chainlit-demo",
      "description": "Haystack Agents with Chainlit",
      "homepage": "",
      "language": "Python",
      "created_at": "2023-09-08T13:12:29Z",
      "updated_at": "2024-03-09T18:16:31Z",
      "topics": [],
      "readme": "# haystack-chainlit-demo\nThis is a [ConversationalAgent](https://docs.haystack.deepset.ai/docs/agent) demo built with [Chainlit Integration](https://haystack.deepset.ai/integrations/chainlit)⚡️ of [Haystack](https://github.com/deepset-ai/haystack)💙. \n\n![Chainlit UI with Haystack Agent](./image.png)\n## Installation\n\nInstall all requirements:\n\n```bash\npip install -r requirements.txt\n```\n\n## Run\n\nMake sure you have [`SEARCH_API_KEY`](https://serper.dev/api-key) and [`OPENAI_API_KEY`](https://platform.openai.com/account/api-keys) variables in your environment file `.env`.\n\n```.env\nSEARCH_API_KEY=XXXX\nOPENAI_API_KEY=XXXX\n```\nThen, run Chainlit:\n\n```bash\nchainlit run app.py\n```\n\nThe application will start at [http://localhost:8000](http://localhost:8000). You can immediately start chatting with your Haystack Agent using the Chainlit UI 💬  \n\n## FAQ\n\n* **I get the `symbol not found in flat namespace '_CFDataGetBytes'` error. What should I do?**\n\n    It's because of grpcio. Check out this issue [#56](https://github.com/Chainlit/chainlit/issues/56) to solve.\n\n## Community \n\nJoin [Chainlit Discord](https://discord.gg/k73SQ3FyUh) and [Haystack Discord](https://discord.gg/haystack).\n\n"
    },
    {
      "name": "deepset-ai/document-store",
      "stars": 8,
      "img": "https://avatars.githubusercontent.com/u/51827949?s=40&v=4",
      "owner": "deepset-ai",
      "repo_name": "document-store",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2023-08-03T14:26:47Z",
      "updated_at": "2025-03-10T01:18:59Z",
      "topics": [],
      "readme": "[![test](https://github.com/deepset-ai/document-store/actions/workflows/test.yml/badge.svg)](https://github.com/deepset-ai/document-store/actions/workflows/test.yml)\n\n# Example Store\n\nThis Github repository is a template that can be used to create custom document stores to extend\nthe new [Haystack](https://github.com/deepset-ai/haystack/) API available from version 2.0.\n\n## Template features\n\nBy creating a new repo using this template, you'll get the following advantages:\n- Ready-made code layout and scaffold to build a custom document store.\n- Support for packaging and distributing the code through Python wheels using Hatch.\n- Github workflow to build and upload a package when tagging the repo.\n- Github workflow to run the tests on Pull Requests.\n\n## How to use this repo\n\n1. Create a new repository starting from this template. If you never used this feature before, you\n   can find more details in [Github docs](https://docs.github.com/en/repositories/creating-and-managing-repositories/creating-a-repository-from-a-template).\n2. If possible, follow the convention `technology-haystack` for the name of the new repository,\n   where `technology` can be for example the name of a vector database you're integrating.\n3. Rename the package `src/example_store` to something more meaningful and adjust the Python\n   import statements.\n4. Replace any occurrence of `example_store` and `example-store` across the repo, according\n   to the name you chose in the previous steps.\n5. Search the whole codebase for the string `#FIXME`, that's where you're supposed to change or add\n   code specific for the database you're integrating.\n6. If Apache 2.0 is not suitable for your needs, change the software license.\n\nWhen your custom document store is ready and working, feel free to add it to the list of available\n[Haystack Integrations](https://haystack.deepset.ai/integrations) by opening a Pull Request in\n[this repo](https://github.com/deepset-ai/haystack-integrations).\n\n\n## Test\n\nYou can use `hatch` to run the linters:\n\n```console\n~$ hatch run lint:all\ncmd [1] | ruff .\ncmd [2] | black --check --diff .\nAll done! ✨ 🍰 ✨\n6 files would be left unchanged.\ncmd [3] | mypy --install-types --non-interactive src/example_store tests\nSuccess: no issues found in 6 source files\n```\n\nSimilar for running the tests:\n\n```console\n~$ hatch run cov\ncmd [1] | coverage run -m pytest tests\n...\n```\n\n## Build\n\nTo build the package you can use `hatch`:\n\n```console\n~$ hatch build\n[sdist]\ndist/example_store-0.0.1.tar.gz\n\n[wheel]\ndist/example_store-0.0.1-py3-none-any.whl\n```\n\n## Release\n\nTo automatically build and push the package to PyPI, you need to set a repository secret called `PYPI_API_TOKEN`\ncontaining a valid token for your PyPI account.\nThen set the desired version number in `src/example_store/__about__.py` and tag the commit using the format\n`vX.Y.Z`. After pushing the tag, a Github workflow will start and take care of building and releasing the package.\n\n## License\n\n`example-store` is distributed under the terms of the [Apache-2.0](https://spdx.org/licenses/Apache-2.0.html) license.\n"
    },
    {
      "name": "raynerz/pdf-search",
      "stars": 8,
      "img": "https://avatars.githubusercontent.com/u/38153565?s=40&v=4",
      "owner": "raynerz",
      "repo_name": "pdf-search",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2023-01-08T13:52:19Z",
      "updated_at": "2024-08-14T11:14:34Z",
      "topics": [],
      "readme": "# PDF - Search\n\n## Requirements\n\n1. In a fresh virtual environment run\n\n```\n$ pip3 install -r requirements.txt\n\n```\n\n2. Go to [openai.com/api](https://openai.com/api/) and create an API token. Create the following environment variable. \n\n```\n$ export OPEN_AI_KEY=\"<your openai api token>\"\n```\n\n## How to use\n\n1. In the project root folder run the following command\n\n```\n$ python3 main.py\n```\nYou should receive the following prompt\n\n```\n$ Welcome to your source of infinite knowledge\n\n\nSelect an option please:\n1 to read a book,\n2 to query\n3 to exit\n```\n3. Select 1 and give the address of a .pdf\n\n```\n$ 1\n$Enter the location of your book: ./books/any.pdf\n```\n\n4. After being prompted to select an option select\n\n```\n$ 2\n$ Enter your query: <your query here>\n```\n\n## Known limitations\n\n- No error handling, if an error occurs, you just need to run the program again and repeat the steps\n- Your pdf's need to have a table of contents and being subdivided by chapters.\n- The `preprocess.py` script divides the books by chapters located in the [PDF's xref table](https://pypdf2.readthedocs.io/en/latest/dev/pdf-format.html) if your chapters are longer than 4096 [open api tokens](https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them) or about 3000 words you will get the following error\n\n```\n$ openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens, however you requested 6711 tokens (6211 in your prompt; 500 for the completion). Please reduce your prompt; or completion length.\n\n```\n\nIn that case you have two options\n1. You can choose a book/pdf with shorter chapters for example [Kimball's data warehouse toolkit](https://www.amazon.com/Data-Warehouse-Toolkit-Definitive-Dimensional/dp/1118530802)\n2. You can modify `preprocess.py` to account for shorter amount of text when splitting the chapters. Issuing a PR would be cool :P\n\n\n\n"
    },
    {
      "name": "WangXII/BEEDS",
      "stars": 8,
      "img": "https://avatars.githubusercontent.com/u/23039011?s=40&v=4",
      "owner": "WangXII",
      "repo_name": "BEEDS",
      "description": "Biomedical Event Extraction using Distant Supervision",
      "homepage": null,
      "language": "Python",
      "created_at": "2021-06-26T10:48:27Z",
      "updated_at": "2023-09-25T04:12:28Z",
      "topics": [],
      "readme": "# BEEDS: Large-Scale Biomedical Event Extraction using Distant Supervision and Question Answering\n\nHere is the accompanying code for the paper [BEEDS: Large-Scale Biomedical Event Extraction using Distant Supervision and Question Answering](https://aclanthology.org/2022.bionlp-1.28/).\n\n## General Setup\n1. Install required packages for conda (spec-list.text) and pip (requirements.txt)\n2. Install Lucene/ElasticSearch for document retrieval\n3. Run the .py-files from the /util/ folder once to build auxilliary tools \n    - Download PubMed, .owl knowledge bases from PathwayCommons etc.\n    - Build and index the Pubmed corpus\n    - Build and load the normalization tools (PubTator, simple lookup dictionaries etc.) and other auxilliary tools (UniProt/Entrez mapping etc.)\n    - Adjust local paths when necessary (also for steps 4-6, edit /configs/__init__.py and files in /scripts/ folder)\n4. Run /scripts/build_data.sh for building the examples\n    - Conduct retrieval and build distantly supervised examples\n    - Events/proteins used for train/test splits are chosen here \n5. Run /scripts/train.sh for training the BERT model\n6. Run /scripts/evaluate.sh for evaluation and inspection of predictions\n"
    },
    {
      "name": "etalab-ia/piaf-ml",
      "stars": 8,
      "img": "https://avatars.githubusercontent.com/u/59837095?s=40&v=4",
      "owner": "etalab-ia",
      "repo_name": "piaf-ml",
      "description": "PIAF v2.0 repo for ML development. Main purpose of this repo is to automatically find the best configuration for a QA pipeline of a partner organisation.",
      "homepage": "",
      "language": "Jupyter Notebook",
      "created_at": "2020-06-11T10:20:52Z",
      "updated_at": "2024-04-15T15:35:48Z",
      "topics": [
        "ia",
        "nlp",
        "piaf",
        "question-answering"
      ],
      "readme": "# PIAF ML\r\nThis project is conducted by the [Lab IA](https://www.etalab.gouv.fr/datasciences-et-intelligence-artificielle) at [Etalab](https://www.etalab.gouv.fr/).  \r\nThe aim of the Lab IA is to help the french administration to modernize its services by the use of modern AI techniques.  \r\nOther Lab IA projects can be found at the [main GitHub repo](https://github.com/etalab-ia/). In particular, the repository for the PIAF annotator can be found [here](https://github.com/etalab/piaf)\r\n\r\n#### -- Project Status: [Active]\r\n## PIAF\r\nPIAF is an Opensource project aiming at providing the community with an easily activable French Question Answering Solution. The first use case of the PIAF project will be \r\n\r\nThe objective of this repository is to give tools for the following tasks: \r\n* **Prepare** the data for the knowlgedge base\r\n* **Evaluate** the performances of the PIAF stack\r\n* **Experiment** different approaches in a contained environment (before prod)\r\n\r\n\r\n### Methods Used\r\n* Information Retrieval\r\n* Language Modeling\r\n* Question Answering\r\n* Machine Learning\r\n### Technologies \r\n* Python\r\n* ElasticSearch\r\n* Docker\r\n* Haystack\r\n* Transformers\r\n## Project Description \r\n### Project architecture \r\nThe PIAF solution is using the following architecture : \r\n[Mettre le dessin architecture ici]\r\n\r\nThe code for PIAF Agent and PIAF bot are hosted on the following repositories: \r\n* [PiafAgent](https://github.com/etalab-ia/piaf_agent)\r\n* [PiafBot]()\r\n\r\nThe code for the Haystack librairy can be found on the Deepset.ai [repository](https://github.com/deepset-ai/haystack/)\r\n\r\n### Treat data for the knowlgedge base\r\nOne of the goal of this repository is to generate the json files that compose the knowledge base. \r\n\r\n### Evaluate performances for the stack PIAF \r\nFor now, the main use of this repo is for evaluation. The goal of the evaluation is to assess the performance of the PIAF configuration on a `test_dataset` for which the `fiches` to be retrieved in the `knowledge_base` are known. \r\n\r\n## Needs of this project [TODO]\r\n- frontend developers\r\n- data exploration/descriptive statistics\r\n- data processing/cleaning\r\n- statistical modeling\r\n- writeup/reporting\r\n- etc. (be as specific as possible)\r\n## Getting Started for development\r\n1. Clone this repo (for help see this [tutorial](https://help.github.com/articles/cloning-a-repository/)).\r\n2. Set the required environment variables, see [Set environment\r\n   variables](#set-environment-variables) below.\r\n2. Make sure gcc, make and the Python C API header files are installed on your\r\n   system\r\n  - On ubuntu:\r\n    - `sudo apt install gcc make python3-dev`\r\n3. Install requirements. Two options are available based on your prefered configuration: \r\n* Using pip:\r\n`pip install -r requirements.txt`\r\non Windows : `pip install -r requirements.txt -f https://download.pytorch.org/whl/torch_stable.html`\r\n\r\n* Using conda\r\n`conda env create --name envname --file=environment.yml`\r\n5. Make sure Docker is properly installed on your computer\r\n\r\n### Performance evaluation\r\nThe procedure to start the evaluation script is the following:\r\n1. Prepare your knowledge base in the form of a json file formated with the squad format. More information regarding the format of the file can be found [here](https://etalab-ia.github.io/knowledge-base/piaf/howtos/Format_donnees_SQuAD.html)\r\n2. Define a set of experiment parameters with the src/evaluation/config/retriever_reader_eval_squad.py\r\n```python\r\nparameters = {\r\n    \"k_retriever\": [20,30],\r\n    \"k_title_retriever\" : [10], # must be present, but only used when retriever_type == title_bm25\r\n    \"k_reader_per_candidate\": [5],\r\n    \"k_reader_total\": [3],\r\n    \"retriever_type\": [\"title\"], # Can be bm25, sbert, dpr, title or title_bm25\r\n    \"squad_dataset\": [\"./data/evaluation-datasets/tiny.json\"],\r\n    \"filter_level\": [None],\r\n    \"preprocessing\": [True],\r\n    \"boosting\" : [1], #default to 1\r\n    \"split_by\": [\"word\"],  # Can be \"word\", \"sentence\", or \"passage\"\r\n    \"split_length\": [1000],\r\n    \"experiment_name\": [\"dev\"]\r\n}\r\n```\r\n3. Run:\r\n```bash\r\npython - m src.evaluation.retriever_reader.retriever_reader_eval_squad.py\r\n```\r\n4. If ES throws some errors, try re-running it again. Sometimes the docker image takes time to initialize.\r\n5. Note that the results will be saved in ``results/`` in a csv form. Also, mlruns will create a record in `mlruns`\r\n\r\n## Project folder structure\r\n\r\n```\r\n/piaf-ml/\r\n├── clients # Client specific deployment code\r\n├── logs # Here we will put our logs when we get to it :)\r\n├── notebooks # Notebooks with reports on experimentations\r\n├── results # Folder were all the results generated from evaluation scripts are stored\r\n├── src\r\n│   ├── data # Script related to data generation\r\n│   ├── evaluation # Scripts related to pipeline performance evaluation\r\n│   │   ├── config # Configuration files\r\n│   │   ├── results_analysis\r\n│   │   ├── retriever # Scripts for evaluating the retriever only\r\n│   │   ├── retriever_reader # Scripts for evaluating the full pipeline\r\n│   │   └── utils # Somes utils dedicated to performance evaluation\r\n│   └── models # Scripts related to training models\r\n└── test # Unit tests\r\n```\r\n\r\n## Set environment variables\r\n\r\nCertain capabilities of this codebase (e.g., using a remote mlflow endpoint) need a set of environment variables to work properly. We use `python-dotenv` to read the contents of a `.env` file that sits at the root of the project. This file is not tracked by git for security reasons. Still, in order for everything to work properly, you need to create such a file in your local code, again, at the root of the project, such as `piaf-ml/.env`.\r\n\r\nA template which describes the different environment variables is provided in `.env.template`. Copy it to `.env` and edit it to your needs.\r\n\r\n#### Mlflow Specific Configutation\r\n\r\nTo be able to upload artifacts into mlflow, you need to be able to `ssh` into the designated artifact server via a `ssh` key. Also, you need a local `ssh` config that specifies an identity file for the artifact-server domain. Such as: \r\n```\r\nHost your.mlflow.remotehost.adress\r\n    User localhostusername\r\n    IdentityFile ~/.ssh/your_private_key\r\n```\r\nThis requirement is needed **when using `sftp`** as your artifact endpoint protocol. \r\n\r\n## How to deploy PIAF\r\n\r\n### If you already published the docker images to https://hub.docker.com/\r\n\r\n- Then go to the [datascience server](https://datascience.etalab.studio)\r\n- *if not done yet,* do a git clone of https://github.com/deepset-ai/haystack, otherwise go to your haystack folder (ex: guillim/haystack)\r\n- *if not done yet,* customise the docker-compose.yml fil to suit your environment variables and your configuration. Example file can be found [here](https://github.com/etalab-ia/piaf-ml/blob/master/src/util/docker-compose.yml)\r\nNote: in this file, you specify the docker images you want for your ElasticSearch, and for PiafAgent\r\n- run `docker-compose up` ✅\r\n\r\n### How to publish the elasticsearch docker image\r\nThis step is the most difficult : from downloading the latest version of service-public.fr XML files, we will publish a docker image of an Elasticsearch container in which we already injected all the service-public texts.\r\n\r\nThis can be done on your laptop (preferably not on the production server as it pollutes the )\r\n\r\n- *if not done yet,* git clone piaf-ml\r\n- Launch the script \"run-all.py\" : it will download latest version of service-pubilc.fr XMLs from data.gouv.fr & store the .json into a folder in /results directory (to be verified for this location)\r\n- *if not done yet*, git clone haystack. Now place the folder (with all the Jsons that you just created after running the run-all script) under /data/ in the haystack repo. This is the only part \r\n```md\r\nCONTRIBUTING.md\r\nDockerfile-GPU     \r\nMANIFEST.in        \r\nannotation_tool    \r\ndocker-compose.yml      \r\nhaystack           \r\nrequirements.txt \r\nrun_docker_gpu.sh\r\ntest\r\ntutorials\r\nDockerfile\r\nLICENSE\r\nREADME.md\r\ndata\r\n  v14  # here you should now see your JSONs\r\ndocs\r\nmodels\r\nrest_api           \r\nsetup.py           \r\n```\r\n- Now, run haystack by typing `docker-compose up`\r\n- First, clean the old document on the index you want to update by doing a `curl -XDELETE  http://localhost:9200/document_elasticsearch` (if you forget to do this, you will add your document to the exisiting ones, making a BIG database lol)\r\n- Connect to your haystack container by typing `docker container logs -f  haystack_haystack-api_1` (note that the container name can change, better verifying it by typing `docker container ls`)\r\n- Then install ipython `pip install ipython`\r\n- Then run ipython `ipyhton`\r\n- Then run this [tutorial Turorial_inject_BM25](https://github.com/etalab-ia/piaf-ml/blob/master/src/util/Turorial_inject_BM25.py)\r\nVerify you have the document indexed into ES going at this endpoint [ES indexes](http://localhost:9200/_cat/indices?v)\r\n- Now exit the container \r\n- create the image of your new ES container by typing `docker commit 829ed24c0d1b guillim/spf_particulier:v15` but don't forget to replace `829ed24c0d1b` by the ID of the elasticsearch container you can have typing `docker container ls`\r\n- push to docker hub: `docker push guillim/spf_particulier:v15` ✅\r\n\r\n### How to publish the piafagent docker image\r\nFollow README.md on the [PiafAgent repo](https://github.com/etalab-ia/piaf_agent)\r\n\r\n\r\n## Contributing Lab IA Members \r\n**Team Contacts :** \r\n* [PA. Chevalier](https://github.com/pachevalier)\r\n* [PE. Devineau](https://github.com/pedevineau)\r\n\r\n**Past Members :**\r\n* [G. Lancrenon](https://github.com/guillim)\r\n* [R. Reynaud](https://github.com/rob192)\r\n* [G. Santarsieri](https://github.com/giuliasantarsieri)\r\n* [P. Soriano](https://github.com/psorianom)\r\n* [J. Denes](https://github.com/jdenes)\r\n\r\n## How to contribute to this project \r\nWe love your input! We want to make contributing to this project as easy and transparent as possible : see our [contribution rules](https://github.com/etalab-ia/piaf-ml/blob/master/.github/contributing.md)\r\n\r\n## Contact\r\n* Feel free to contact the team at piaf@data.gouv.fr with any questions or if you are interested in contributing!\r\n"
    },
    {
      "name": "Rightpoint/ai-discipline-ollama-rag",
      "stars": 8,
      "img": "https://avatars.githubusercontent.com/u/621479?s=40&v=4",
      "owner": "Rightpoint",
      "repo_name": "ai-discipline-ollama-rag",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2024-03-25T21:31:28Z",
      "updated_at": "2024-11-12T15:51:10Z",
      "topics": [],
      "readme": "# AI Discipline Code-Along: Ollama RAG\n27 Mar 2024\n\nThis code-along will focus on a simple RAG implementation that runs locally on your machine using [Ollama](https://ollama.com/). In addition to installing the dependencies below, choose a document with text content that you want to chat with the model about, and store it as a PDF in the `documents` directory. Haystack has support for other document converters as well, although you may need to install additional dependencies. This code-along will assume you're using a PDF.\n\n## Dependencies\n\n* Python 3.11+ (Rye will automatically use 3.11 by default; see below)\n* VS Code (<https://code.visualstudio.com/>) with the Python extension (search in Extensions in the side bar)\n* Rye (<https://rye-up.com/>), to manage the project’s Python third-party dependencies and Python interpreter version. For Windows users, you’ll likely want to use the 64-bit installer.\n* Ollama (<https://ollama.com/>)\n\n## Setup\nFirst, clone this repo, and open the `ollama-rag.code-workspace` file in VS Code.\n\nFrom the terminal in VS Code, install the Python dependencies using Rye:\n\n```shell\n$ rye sync\n```\n\nThis will install the following Python dependencies in a virtual environment: `gradio ollama haystack-ai ollama-haystack pypdf`. If VS Code prompts you to select the new environment for the workspace, say yes.\n\nNext, use Ollama to pull down the models you want to use. Do these two, at least:\n\n```shell\n$ ollama pull llama2\n$ ollama pull nomic-embed-text\n```\n\nTest it out by chatting with it on the command line: \n\n```shell\n$ ollama run llama2\n```\n\nIf you’d like, check out the supported models at <https://ollama.com/library> and pull any that you’d like to try. Unless you have a powerful GPU, it’s recommended that you use models in the 7b-parameter and under category. If your machine only supports running on CPU and you want to speed that up, there are smaller models such as `gemma:2b` that you can try, although the performance will not be as good. In general, the more parameters the model has, the better its accuracy, the more resources it requires, and the more slowly it runs. I’ll be doing the code-along with the `llama2:7b` model, but feel free to experiment and use what you like best!\n"
    },
    {
      "name": "masci/chroma-haystack",
      "stars": 8,
      "img": "https://avatars.githubusercontent.com/u/7241?s=40&v=4",
      "owner": "masci",
      "repo_name": "chroma-haystack",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2023-08-07T13:22:37Z",
      "updated_at": "2023-11-09T12:13:43Z",
      "topics": [],
      "readme": "# Moved\n## Chroma Document Store for Haystack\n\nThe code is now part of Haystack and can be found here: https://github.com/deepset-ai/haystack-core-integrations/tree/main/document_stores/chroma\n\n"
    },
    {
      "name": "000haoji/deep-student",
      "stars": 7,
      "img": "https://avatars.githubusercontent.com/u/54060703?s=40&v=4",
      "owner": "000haoji",
      "repo_name": "deep-student",
      "description": "致力于使用LLM与RAG对错题进行分析",
      "homepage": null,
      "language": "Python",
      "created_at": "2025-03-10T10:44:10Z",
      "updated_at": "2025-04-13T03:58:58Z",
      "topics": [],
      "readme": "# Deep-Students 错题分析与知识库查询系统\n\nDeep-Students致力于使用LLM与RAG对错题进行分析与管理，\n减少学习者在错题整理与复习上的无效努力，提高学习效率。\n项目仍然处于早期阶段，开发中可能存在一些BUG。\n\nBILIBILI 演示视频\n【Deep-Student 开源AI错题管理软件】 \nhttps://www.bilibili.com/video/BV1rcQVY3EDo/?share_source=copy_web&vd_source=1c0ff08e122edb92a89179c0a64878aa\n\n## 系统架构\n\n系统采用微服务架构，分为两个主要组件：\n\n1. **主程序** - 错题分析系统(端口: 5001)\n   - 提供错题录入、分析和回顾功能\n   - 支持多种AI模型分析错题\n\n2. **RAG2025** - 知识库查询系统(端口: 5000)\n   - 提供知识文档管理和检索功能\n   - 基于RAG (检索增强生成) 技术\n\n## 安装与配置\n\n### 前提条件\n\n- Python 3.10 或更高版本\n- Git\n\n### 安装步骤\n\n1. 克隆仓库\n```bash\ngit clone https://github.com/000haoji/deep-student.git\ncd deep-student\n```\n\n2. 为两个子系统创建虚拟环境并安装依赖\n\n主程序:\n```bash\ncd 主程序\npython -m venv venv\nvenv\\Scripts\\activate\npip install -r requirements.txt\n```\n\nRAG2025:\n```bash\ncd RAG2025\npython -m venv venv\nvenv\\Scripts\\activate\npip install -r requirements.txt\n```\n\n3. 配置API密钥\n\n- 复制示例配置文件:\n  ```bash\n  cp 主程序/config.json.example 主程序/config.json\n  cp 主程序/config.ini.example 主程序/config.ini\n  ```\n- 编辑配置文件，添加您的API密钥\n\n## 启动系统\n\n使用集成启动脚本同时启动两个子系统:\n\n```bash\npython start_services.py\n```\n\n或分别启动各个服务:\n\n```bash\n# 启动主程序\ncd 主程序\npython run.py\n\n# 启动RAG2025\ncd RAG2025\npython app.py\n```\n\n## 系统特性\n\n- **错题管理**: 录入、分类和管理错题\n- **AI分析**: 使用先进的AI模型分析错误原因和知识点\n- **回顾功能**: 定期回顾分析过的错题，增强学习效果\n- **知识库查询**: 查询相关知识点和学习资料\n- **双表结构兼容**: 兼容新旧两种数据库表结构\n\n## 技术细节\n\n- 使用Flask作为Web框架\n- 支持多种AI模型 (DeepSeek, OpenAI等)\n- 使用SQLite作为数据存储\n- 前端使用Bootstrap和jQuery\n\n## 目前已经实现的功能有：\n\n### 1.使用Deepseek-V3以及bge-m3的RAG知识库\n可以分多个文档库对文档进行管理，对上传的文档进行分段，以及调用Deepseek-V3进行对知识库的查询，支持备份与恢复。\n（暂时作为单独运行的模块，与主程序分离进行调用，部分实现参考了Cherry Studio的知识库）\n\n### 2.错题初次分析\n主程序支持上传多道题目，每道题目可以**上传多个图片并且给出补充信息**，可以对错题进行**批量初次分析**。\n目前使用Qwen-VL作为图像分析模型进行OCR，提取图片中的题目，并且生成标签等相关信息。\n上一流程将OCR后的文字传给Deepseek-R1进行进一步分析，支持显示推理过程。\n（~~黏合国~~黏合实现多模态）\n\n**初次分析适用于错题的首次整理**。\n\n\n\n### 3.错题回顾分析\n主程序支持创建**多个学科**每个学科都有自己独立的错题库，\n用户可以在错题库中通过标签筛选想要复习的内容，选择多个错题使用DeepseekR1进行**回顾分析**，\n回顾分析将致力于**从多个不同的错题中查找出错误共性以及潜藏的问题**。\n\n**回顾分析适用于错题的按章节复习或总复习**。\n\n### 4.自定义三阶段prompt\n主程序支持调整Qwen-VL阶段,Deepseek R1阶段,回顾分析阶段三个阶段的提示词，\n用户可以**自行调整以适应自身的学习风格**。\n\n### 5.按照优先级选择Deepseek API\nDeepseek-R1可以设置三个API并设置优先级，防止某个API不可用导致堵塞。\n\n### 6.支持备份与恢复功能。\n\n\n\n由于个人能力原因，项目还存在许多BUG，希望对此有兴趣的朋友能一同参与该项目的开发。\n\n### 尚未实现的todolist：\n- 多线程批量分析\n- 主页初次分析重新生成选项\n- RAG知识库的前端配置\n- 使用LLM进行Anki制卡\n- 前端自由调整LLM的其他参数如temperature等\n- 前端自由替换模型为其他LLM如Gemini，Claude\n- 调整prompt以及对错题数据的管理，实现更好的利用错题\n- 将RAG知识库整合进主程序流程中，通过RAG降低分析错题时LLM的幻觉\n- 同一张图片内多道题目的分割\n- 完善RAG模块，实现更高效优雅的知识库\n- 复习计划功能暂未实现，未来或使用FSRS算法？\n- 桌面端与安卓端的实现\n"
    },
    {
      "name": "RaamRaam/GrowthSchoolRAGWorkshop",
      "stars": 7,
      "img": "https://avatars.githubusercontent.com/u/12892059?s=40&v=4",
      "owner": "RaamRaam",
      "repo_name": "GrowthSchoolRAGWorkshop",
      "description": null,
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2025-01-02T16:53:07Z",
      "updated_at": "2025-01-18T09:00:07Z",
      "topics": [],
      "readme": ""
    },
    {
      "name": "pkweitai/hummingbotAI",
      "stars": 7,
      "img": "https://avatars.githubusercontent.com/u/157328927?s=40&v=4",
      "owner": "pkweitai",
      "repo_name": "hummingbotAI",
      "description": "Hummingbot AI enablement contributions from community",
      "homepage": null,
      "language": "Python",
      "created_at": "2024-05-12T18:53:10Z",
      "updated_at": "2025-02-18T19:46:07Z",
      "topics": [],
      "readme": "# ![Alt text](ai.png?raw=true \"Title\") Hummingbot AI Chatbot \n\n![Alt text](tg.png?raw=true \"Title\")\n## Description\nThis project introduces a Telegram chatbot powered by a Large Language Model (LLM), specifically designed for interacting with Hummingbot. It leverages MQTT for command execution and incorporates a robust, production-ready architecture using HayStack and Google Gemini, offering easy adaptability to other LLMs like OpenAI.\n\n## Features\n- **Comprehensive Command Execution**: Fully exercise all available commands via MQTT.\n- **Flexible Architecture**: Built with HayStack and Google Gemini, making it easy to switch to other LLMs.\n- **Production-Ready**: Ready for deployment with built-in scalability and reliability.\n- **Scalable Tool feature : you can create new tools by adding more featuretable with different bottypes, to make the AiBot smarter to capture actions for users (e.g. get specific news on crypto topic, get market sentiments or synthesis a trading strategies) - this is still in Alpha , need more community support\n\n## Requirements\nTo set up the chatbot, follow these steps:\n\n1. **Set up Google API Key**\n   Ensure that the Google API Key is set in your environment, via the .env file:\n   ```bash\n   vim .env\n\n   # Put the following inside the .env file\n   GOOGLE_API_KEY='your_google_api_key_here'\n   ```\n\n2. **Running Hummingbot**\n   Clone and set up Hummingbot from the official repository:\n   ```bash\n   git clone https://github.com/hummingbot/hummingbot.git\n   cd hummingbot\n   ```\n\n# Follow the setup instructions in the Hummingbot repository\n\n3. **Running MQTT Broker**\nSet up an MQTT Broker by cloning the broker repository:\n   ```bash\n   git clone git@github.com:hummingbot/brokers.git\n   cd brokers\n   ```\n4. **Setup the MQTT remote client submodule**\n   ```bash\n   git submodule update --init to fetch the submodule;\n   cd hbot-remote-client && pip install .\n   ```\n\n5. **Follow the setup instructions for the broker**\n- Create a Telegram Bot\nCreate a Telegram bot using BotFather on Telegram and obtain the bot token:\n- Start a chat with BotFather (@BotFather)\n- Follow the instructions to create a new bot and get your token\n- Put the bot token as BOT_TOKEN in `.env`.\n\n## Installation\nAfter setting up the requirements, install the necessary dependencies:\n\n   ```bash\n    pip install -r requirements.txt\n    \n    # install wkhtmltoimage\n    # MAC:  brew install wkhtmltopdf\n    # windows:  Go to  wkhtmltopdf download page . Download the Windows installer (e.g., wkhtmltox-0.12.6-1.msvc2015-win64.exe).\n     \n\t\n    python aihbot.py\n   ```\n\n## TODO\n- Support More LLMs: Integrate additional LLM providers like OpenAI, Groq, and LLAMA3.\n- Improve Chat Performance: Implement a Context Memory based Agent to enhance chat interactions.\n- Add More Features:\n  Expand capabilities to include more trading and crypto-related features.\n- Add web widgets to enable advanced strategy , charting  \n\n## Contributing\nContributions are welcome! Please read CONTRIBUTING.md for details on our code of conduct and the process for submitting pull requests to us.\n\n## License\nThis project is licensed under the MIT License - see the LICENSE.md file for details.\n"
    },
    {
      "name": "renuka010/Mistral-Telegram-Bot",
      "stars": 7,
      "img": "https://avatars.githubusercontent.com/u/72569696?s=40&v=4",
      "owner": "renuka010",
      "repo_name": "Mistral-Telegram-Bot",
      "description": "ChatBot made using Hugging Face model mistralai/Mistral-7B-Instruct-v0.2 using HayStack",
      "homepage": "",
      "language": "Python",
      "created_at": "2024-03-06T06:57:49Z",
      "updated_at": "2024-12-19T11:35:31Z",
      "topics": [
        "chatbot",
        "haystack",
        "huggingface",
        "large-language-models",
        "llm",
        "mistral"
      ],
      "readme": "# Mistral Telegram Chatbot 🤖\n\n## Overview\n\nThis is a chatbot created using Python, Haystack, pydub, and the Hugging Face model `mistralai/Mistral-7B-Instruct-v0.2`. [Mistral Large Language Model (LLM)](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2) is a pretrained generative text model with 7 billion parameters. It supports text and voice messages, allowing users to interact with the chatbot in their preferred format.\n\n## Features\n\n- Text Conversations with `Mistral-7B-Instruct-v0.2` model\n- Voice message Processing using Google Speech Recognition\n- Chat Conversation tracking\n\n## Prerequisites\n\n- Hugging Face Token\n- Telegram Bot Token\n\n## Installation\n\n1. Clone the repository:\n    ```bash\n    git clone https://github.com/renuka010/Mistral-Telegram-Bot.git\n    ```\n2. Change into the project directory:\n    ```bash\n    cd Mistral-Telegram-Bot\n    ```\n3. Create and Activate the virtual environment:\n    ```bash\n    python3 -m venv venv  # Create\n    venv\\Scripts\\activate    # Activate\n    ```\n4. Install the dependencies:\n    ```bash\n    pip install -r requirements.txt\n    ```\n5. Create a `.env` file with your Huggingface token and Telegram token:\n    ```\n    HF_API_TOKEN=your_huggingface_token\n    TELEGRAM_BOT_TOKEN=your_telegram_token\n    ```\n6. Run the server:\n    ```bash\n    python chatbot.py\n    ```\n\n## Future Scope\n\nCreating vector-based memory for storing conversation history. (Currently, the project supports only inbuilt memory which can remember only the last 10 conversations from history.)\n\n## Contributing\n\nContributions are welcome! If you'd like to contribute, please submit a pull request or open an issue with your proposed changes or bug reports.\n"
    },
    {
      "name": "apache/openserverless-runtimes",
      "stars": 6,
      "img": "https://avatars.githubusercontent.com/u/47359?s=40&v=4",
      "owner": "apache",
      "repo_name": "openserverless-runtimes",
      "description": "Apache openserverless",
      "homepage": "https://openserverless.apache.org/",
      "language": "Go",
      "created_at": "2024-07-02T12:44:58Z",
      "updated_at": "2025-01-17T14:25:27Z",
      "topics": [],
      "readme": "<!--\n#\n# Licensed to the Apache Software Foundation (ASF) under one or more\n# contributor license agreements.  See the NOTICE file distributed with\n# this work for additional information regarding copyright ownership.\n# The ASF licenses this file to You under the Apache License, Version 2.0\n# (the \"License\"); you may not use this file except in compliance with\n# the License.  You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n-->\n\n# Apache OpenServerless Runtimes\n\nAll the Apache Openserverless OpenWhisk runtimes in a single place using the Go proxy and ActionLoop.\n\n# Source Code\n\nruntimes are docker images, and they all use a proxy in go and some scripts for execution.\n\nGo Proxy code is in folder `openwhisk` and the main is `proxy.go` in top level.\n\nYou can compile it with `go build -o proxy`.\n\nTests are in openwhisk folder, test it with `cd opewhisk ; go test `\n\nRuntime sources are under `runtimes/<plang>/<version>` (`<plang>` is programming languate)\n\nSpecial case is `runtime/common/<version>` that contains the proxy itseself, it is used as base image for the others and must be build first.\n\n# How to build images\n\nBuild and push the common runtime with `task build-common`. Also ensure the image is public.\n\nThen you can build a single runtime specifingh the dir:\n\nBuild a single runtime: `task build-runtime RT=nodejs VER=v18`\n\n# How to generate a new runtimes.json\n\nThe project contains a `runtimes.json.tpl` with specific placeholder for managed Apache OpenServerless runtimes. To regenerate a newer `runtimes.json` from the current TAG\nand assuming that the images have been effectively pushed to the Apache Official DockerHub repositories execute the command:\n\n`task render-runtimes`\n\nThis will create a new `runtimes.json` that can be pushed to the official Apache OpenServerless [task](https://github.com/apache/openserverless-task) repo, replacing the existing file.\n\n# How to use the client/server mode\n\nThe proxy can be used in client or server mode, where the client acts as a forward\nproxy and the server will be the actual executor.\n\nIn client mode the runtime does not execute the action, but instead forwards the\n/init and /run requests to a server runtime. To activate this mode, set the environment\nvariable `OW_ACTIVATE_PROXY_CLIENT` to 1.\nWhen creating actions, use the --main flag with this syntax:\n`--main \"<main>@<remote runtime address>\"`. `<main>` can be empty.\n\nThe remote runtime is enabled by setting the environment variable `OW_ACTIVATE_PROXY_SERVER` to 1.\nIn this mode the runtime is multi-action enabled, meaning that it can initialize and run more than one action.\nMany client runtimes can forward requests to the same server runtime.\n\nCurrently the proxy client/server extension has been compiled and releaed inside the common runtime `common1.18.1`. \n\nThe go runtimes have been extended with the runtime `v1.22proxy` which has been setup by default with the `OW_ACTIVATE_PROXY_CLIENT` set to 1. To deploy an action to be proxid remotely\nuse a command similar to `ops action create <action> tests/pytorch.py --main main@http://ops-cuda-service:8080 --kind go:1.22proxy`\n\nThe experimental runtime to be used as remote proxy server are currently within the `runtime\\experimental` folder and can be built using `task build-experimental-runtimes`. These runtime have to be deployed as regular pod/container on a remote machine. To activate the proxy server mode endure that the image is launched setting the environment variable `OW_ACTIVATE_PROXY_SERVER=1`, otherwise the runtime behaves as a regular OpenWhisk one.\n"
    },
    {
      "name": "concaption/text2img-search",
      "stars": 6,
      "img": "https://avatars.githubusercontent.com/u/43001234?s=40&v=4",
      "owner": "concaption",
      "repo_name": "text2img-search",
      "description": "text to image search ai app using haystack and clip",
      "homepage": null,
      "language": "Python",
      "created_at": "2023-09-30T18:31:44Z",
      "updated_at": "2025-04-08T02:24:14Z",
      "topics": [],
      "readme": "# Multimodal Search with Haystack: A Step-By-Step Guide\n![Screenshot](assets/screenshot.png)\n![API Screenshot](assets/api.png)\n\n## Setup\n\nFor streamlit app\n\n```\nmake setup\nmake run\n```\nFor FASTapi\n```\nmake run-api\n```\n\nFor dockerized API\n```\nmake docker-api\n```\n\n## Introduction\n\nIn today's world, data is not limited to text. We have a plethora of multimedia content such as images, audio, and video. Therefore, having a search mechanism that can look into multiple types of media is more useful than ever. In this tutorial, we will focus on creating a multimodal search capability using Haystack's `MultiModalRetriever`. We will be using Python for this tutorial.\n\n## Technology Stack\n\n- Python: The programming language used for this project.\n- Haystack: An open-source framework for building search systems.\n- Sentence Transformers: For using the CLIP-ViT-B-32 model to get embeddings.\n\n## Step 1: Setup\n\nFirstly, you'll need to install the Haystack library if you haven't already:\n\n```bash\npip install farm-haystack\n```\n\n## Step 2: Import Necessary Modules\n\n```python\nimport os\nfrom haystack import Document\nfrom haystack import Pipeline\nfrom haystack.document_stores import InMemoryDocumentStore\nfrom haystack.nodes.retriever.multimodal import MultiModalRetriever\n```\n\n## Step 3: Create the MultiModalSearch Class\n\nHere is the complete code with detailed comments.\n\n```python\nclass MultiModalSearch:\n    def __init__(self):\n        self.document_store = InMemoryDocumentStore(embedding_dim = 512)\n        doc_dir = \"./data\"\n        images = [\n            Document(content=f\"./{doc_dir}/{filename}\", content_type=\"image\", meta={\"name\": filename})\n            for filename in os.listdir(doc_dir)\n            if filename.endswith(\".jpg\")\n        ]\n        self.document_store.write_documents(images)\n        self.retriever = MultiModalRetriever(\n            query_embedding_model=\"sentence-transformers/clip-ViT-B-32\",\n            query_type= \"text\",\n            document_embedding_models={\n                \"image\": \"sentence-transformers/clip-ViT-B-32\",\n            },\n            document_store=self.document_store,\n        )\n        self.document_store.update_embeddings(self.retriever)\n        self.pipeline = Pipeline()\n        self.pipeline.add_node(component=self.retriever, name=\"Retriever\", inputs=[\"Query\"])\n    def search(self, query):\n        prediction = self.pipeline.run(query=query, params={\"Retriever\": {\"top_k\": 3}})\n        return sorted(prediction[\"documents\"], key=lambda x: x.score, reverse=True)\n```\n\n## Step 4: Use Cases\n\n1. **E-commerce Platforms**: When users want to find products similar to a reference image or description.\n2. **Media Libraries**: To search for images or videos based on textual queries or vice versa.\n3. **Research**: For tasks like object identification in images based on textual descriptions.\n\n## Conclusion\n\nMultimodal search is becoming increasingly important as we deal with varied types of data. With frameworks like Haystack, building such capabilities has become more straightforward than ever.\n"
    },
    {
      "name": "cabustillo13/RAG_Haystack_Chatbot",
      "stars": 6,
      "img": "https://avatars.githubusercontent.com/u/52437136?s=40&v=4",
      "owner": "cabustillo13",
      "repo_name": "RAG_Haystack_Chatbot",
      "description": "Haystack RAG pipeline for a chatbot.",
      "homepage": "",
      "language": "Jupyter Notebook",
      "created_at": "2023-09-25T17:20:49Z",
      "updated_at": "2024-11-19T17:56:16Z",
      "topics": [
        "chatbot",
        "hacktoberfest",
        "hacktoberfest2023",
        "haystack",
        "llm",
        "ploomber"
      ],
      "readme": "# Hacktoberfest 2023\r\n\r\n<p align=\"center\">\r\n <img width=\"100px\" src=\"https://github.com/cabustillo13/RAG_Haystack_Chatbot/blob/main/images/chatbot.svg\" align=\"center\" alt=\"RAG_Haystack_Chatbot\" />\r\n <h2 align=\"center\">Hacktoberfest 2023 Project</h2>\r\n <p align=\"center\"><b>Haystack RAG pipeline for a chatbot.</b></p>\r\n\r\n</p>\r\n  <p align=\"center\">\r\n    <a href=\"https://github.com/cabustillo13/RAG_Haystack_Chatbot/actions/new\">\r\n      <img alt=\"Tests Passing\" src=\"https://github.com/anuraghazra/github-readme-stats/workflows/Test/badge.svg\" />\r\n    </a>\r\n        <a href=\"https://github.com/cabustillo13/RAG_Haystack_Chatbot/issues\">\r\n      <img alt=\"Issues\" src=\"https://img.shields.io/github/issues/cabustillo13/RAG_Haystack_Chatbot?color=0088ff\" />\r\n    </a>\r\n    <a href=\"https://github.com/cabustillo13/RAG_Haystack_Chatbot/pulls\">\r\n      <img alt=\"GitHub pull requests\" src=\"https://img.shields.io/github/issues-pr/cabustillo13/RAG_Haystack_Chatbot?color=0088ff\" />\r\n    </a>\r\n    <br />\r\n    <p align=\"center\">\r\n  </p>\r\n</p>\r\n\r\n## ⚙️ Set up\r\nInstall poetry.\r\n```\r\npip install poetry\r\n```\r\n\r\nInstall required packages using the following command:\r\n```\r\npoetry install\r\n```\r\n\r\n## 📍 Haystack RAG pipeline for a chatbot\r\n\r\nWe're building a Haystack RAG pipeline for a chatbot that answers questions about music lyrics.\r\n\r\nFor this project, we're using a subset of this Kaggle Dataset: [Song Lyrics](https://www.kaggle.com/datasets/deepshah16/song-lyrics-dataset).\r\n\r\nWe're using Haystack for Document Store and build the RAG pipeline and Open AI GPT3.5-turbo LLM to build a chatbot with Chainlit for song lyrics.\r\n\r\n## 🎥 Youtube Video Presentation\r\n\r\nCheck our video presentation: [Haystack RAG pipeline for a music lyrics chatbot - Hacktoberfest 2023 by Ploomber](https://www.youtube.com/watch?v=Cj_AVa8kpE0)\r\n\r\n![Youtube Screenshot](docs/youtube_screenshot.PNG)\r\n\r\n## 🙌 Haystack Pipeline\r\n\r\n![Pipeline Process](docs/Pipeline_Process_Presentation.jpg)\r\n\r\n1. **Setting up the Document Store**\r\n   - Initializing an InMemoryDocumentStore with BM25 retrieval capabilities.\r\n\r\n2. **Data Retrieval**\r\n   - Downloading the lyrics dataset from Kaggle.\r\n   - Loading lyrics data for different artists into dataframes.\r\n   - Merging the dataframes into a single dataframe.\r\n   - Data preprocessing, including column renaming and conversion to the document store format.\r\n\r\n3. **Prompt Template**\r\n   - Defining a `rag_prompt` template for generating responses from music lyrics and user questions.\r\n\r\n4. **Retriever Configuration**\r\n   - Configuring a BM25Retriever to work with the document store for document retrieval based on user queries.\r\n\r\n5. **GPT-3.5 Turbo Configuration**\r\n   - Setting up a `PromptNode` to utilize the GPT-3.5 Turbo model for generating responses. This includes specifying your OpenAI API key and using the `rag_prompt` template.\r\n\r\n6. **Pipeline Setup**\r\n   - Creating a pipeline (`pipe`) with two nodes: the retriever and the GPT-3.5 Turbo model.\r\n\r\n7. **Main Function**\r\n   - Defining the core functionality of the project, where user queries are processed using the pipeline, and responses are sent back to the user.\r\n\r\n## 📩 User Interface\r\n\r\nWelcome message from the chatbot.\r\n![Graphic User Interface](docs/gui_0.PNG)\r\n\r\nWe use Chainlit to make the chatbot.\r\n![Graphic User Interface](docs/gui_1.PNG)\r\n\r\n## 😎 Team members\r\n\r\n- Linkedin: [Monica Regina da Silva](https://www.linkedin.com/in/monicasil/), [Carlos Bustillo](https://www.linkedin.com/in/carlos-bustillo/)\r\n- Github: [MonicaRSilva](https://github.com/MonicaRSilva),[cabustillo13](https://github.com/cabustillo13)\r\n"
    },
    {
      "name": "sebastianschramm/german-qa-rag",
      "stars": 6,
      "img": "https://avatars.githubusercontent.com/u/16114609?s=40&v=4",
      "owner": "sebastianschramm",
      "repo_name": "german-qa-rag",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2023-09-25T09:51:40Z",
      "updated_at": "2023-12-14T09:17:22Z",
      "topics": [],
      "readme": "# German QA rag pipeline with Haystack and chainlit\n\nNavigating German Government Websites Made Easier\n\nFinding specific information on German government websites can sometimes feel like quite a challenge. Whether it's permits, services, or valuable data you're after, the process can be a bit daunting.\n\nUsing deepset's Haystack, I've created a QA pipeline that incorporates translation models, SerperAPI, and GPT-3.5 to help you navigate the websites of the city of Cologne. With Haystack's WebRetriever, you can easily restrict Serper searches to specific domain names. Translation models (Helsinki-NLP) ensure your English queries are understood in German, and the GPT responses are seamlessly translated back into English.\n\nThis QA pipeline works in pure German as well, and I believe it can save you time in finding the information you need from the city hall in just a few seconds.\n\nTo make it even more user-friendly, I've added Chainlit for a conversational touch.\n\n![Alt text](screenshot_qa.png?raw=true \"QA example\")\n\n## Installation\n\nYou can install the qa_rag package via pip by executing the following command:\n\n```bash\npip install git+https://github.com/sebastianschramm/german-qa-rag.git\n```\n\nor by first cloning the repository and then installing by executing the following command in the root of the cloned repository:\n\n```bash\npip install .\n```\n\n## Start the app\n\nYou need to set the following environment variables before starting the app:\n\n- SERPER_API_KEY (API key for using serper - free-tier accounts are available: https://serper.dev/signup)\n- OPENAI_API_KEY (your API key for openai)\n- OPENAI_MODEL (openai model name, e.g. \"gpt-3.5-turbo\")\n\nYou have 2 ways to start the QA app.\nThe recommended way is to use the provided script command \"rag\" (make sure you have the qa_rag package installed prior to that):\n\n```bash\nrag\n```\n\nAlternatively, if you have cloned the respository, you can use the chainlit cli in the root of the repository:\n\n```bash\nchainlit run qa_rag/server.py\n```\n"
    },
    {
      "name": "LLukas22/vLLM-haystack-adapter",
      "stars": 6,
      "img": "https://avatars.githubusercontent.com/u/65088241?s=40&v=4",
      "owner": "LLukas22",
      "repo_name": "vLLM-haystack-adapter",
      "description": "Simply connect your haystack pipeline to an vLLM-API server",
      "homepage": null,
      "language": "Python",
      "created_at": "2023-09-07T07:51:44Z",
      "updated_at": "2023-12-27T13:11:43Z",
      "topics": [],
      "readme": "# vLLM-haystack-adapter\n[![PyPI - Version](https://img.shields.io/pypi/v/vllm-haystack.svg)](https://pypi.org/project/vllm-haystack)\n[![PyPI - Python Version](https://img.shields.io/pypi/pyversions/vllm-haystack.svg)](https://pypi.org/project/vllm-haystack)\n\nSimply use [vLLM](https://github.com/vllm-project/vllm) in your haystack pipeline, to utilize fast, self-hosted LLMs. \n\n<p align=\"center\">\n    <img alt=\"vLLM\" src=\"https://raw.githubusercontent.com/vllm-project/vllm/main/docs/source/assets/logos/vllm-logo-text-light.png\" width=\"45%\" style=\"vertical-align: middle;\">\n    <a href=\"https://www.deepset.ai/haystack/\">\n        <img src=\"https://raw.githubusercontent.com/deepset-ai/haystack/main/docs/img/haystack_logo_colored.png\" alt=\"Haystack\" width=\"45%\" style=\"vertical-align: middle;\">\n    </a>\n</p>\n\n## Installation\nInstall the wrapper via pip:  `pip install vllm-haystack`\n\n## Usage\nThis integration provides two invocation layers:\n- `vLLMInvocationLayer`: To use models hosted on a vLLM server (or any other OpenAI compatible server)\n- `vLLMLocalInvocationLayer`: To use locally hosted vLLM models\n\n### Use a Model Hosted on a vLLM Server\nTo utilize the wrapper the `vLLMInvocationLayer` has to be used. \n\nHere is a simple example of how a `PromptNode` can be created with the wrapper.\n```python\nfrom haystack.nodes import PromptNode, PromptModel\nfrom vllm_haystack import vLLMInvocationLayer\n\n\nmodel = PromptModel(model_name_or_path=\"\", invocation_layer_class=vLLMInvocationLayer, max_length=256, api_key=\"EMPTY\", model_kwargs={\n        \"api_base\" : API, # Replace this with your API-URL\n        \"maximum_context_length\": 2048,\n    })\n\nprompt_node = PromptNode(model_name_or_path=model, top_k=1, max_length=256)\n```\nThe model will be inferred based on the model served on the vLLM server.\nFor more configuration examples, take a look at the unit-tests.\n\n#### Hosting a vLLM Server\n\nTo create an *OpenAI-Compatible Server* via vLLM you can follow the steps in the \nQuickstart section of their [documentation](https://vllm.readthedocs.io/en/latest/getting_started/quickstart.html#openai-compatible-server).\n\n### Use a Model Hosted Locally\n⚠️To run `vLLM` locally you need to have `vllm` installed and a supported GPU.\n\nIf you don't want to use an API-Server this wrapper also provides a `vLLMLocalInvocationLayer` which executes the vLLM on the same node Haystack is running on. \n\nHere is a simple example of how a `PromptNode` can be created with the `vLLMLocalInvocationLayer`.\n```python\nfrom haystack.nodes import PromptNode, PromptModel\nfrom vllm_haystack import vLLMLocalInvocationLayer\n\nmodel = PromptModel(model_name_or_path=MODEL, invocation_layer_class=vLLMLocalInvocationLayer, max_length=256, model_kwargs={\n        \"maximum_context_length\": 2048,\n    })\n\nprompt_node = PromptNode(model_name_or_path=model, top_k=1, max_length=256)\n```\n\n\n"
    },
    {
      "name": "norahollenstein/copco-processing",
      "stars": 6,
      "img": "https://avatars.githubusercontent.com/u/17772234?s=40&v=4",
      "owner": "norahollenstein",
      "repo_name": "copco-processing",
      "description": "Code to process CopCo EyeLink files",
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2021-11-03T13:40:36Z",
      "updated_at": "2024-09-13T09:42:49Z",
      "topics": [],
      "readme": "# CopCo Eye-Tracking Data Processing\n\nThis repository contains the code to analyze and post-process the eye-tracking data of the CopCo corpus, which is described in the following publication:\n\nNora Hollenstein, Maria Barrett, and Marina Björnsdóttir. 2022. [The Copenhagen Corpus of Eye Tracking Recordings from Natural Reading of Danish Texts](https://aclanthology.org/2022.lrec-1.182/). In _Proceedings of the Thirteenth Language Resources and Evaluation Conference_, pages 1712–1720, Marseille, France. European Language Resources Association.\n\nPlease make sure to read about the data format and download the latest version of the data from the [OSF repository](https://osf.io/ud8s5/).\n\n## Participant statistics\n`python participant_statistics.py RawData/`  \nThis script includes calculation of comprehension scores and overall reading times. Raw Data should include one folder per participant containing the respective EDF recording file.\n\n`python calibration_check.py`  \nThis script checks the calibration accuracy of all participants.\n\n## Dataset statistics\n`python texts_statistics.py`  \nThis script includes calculation of text and sentence length.\n\n## Feature extraction from fixation and interest area reports\n\nThe extracted features can be found in `ExtractedFeatures/`, but if required you can also re-run the code to add additional features or modify the existing ones:\n\n1. Use the DataViewer software from SR Research to convert the recorded EDF files to fixation reports and interest area reports in TXT format.  \n\n2. If the reports were exported in utf-8 encoding (this can be speficied in the DataViewer preferences), this step can be skipped.  \nConvert SR DataViewer output files to UTF-8 for correct representation of Danish special characters:  \n`iconv -f ISO-8859-1 -t UTF-8 FIX_report_P10.txt > FIX_report_P10-utf8.txt`  \n`iconv -f ISO-8859-1 -t UTF-8 IA_report_P10.txt > IA_report_P10-utf8.txt`  \n\nThese files are also available in the [OSF repository](https://osf.io/ud8s5/) (original and UTF-8 versions) in the folders FixationReports and InterestAreaReports.\n\n3. Create a mapping from character interest areas to word interest areas:  \n`python char2word_mapping.py`  \nThis step is only required if there were changes in the experiment setup. In order to complete this step it is necessary to deploy the experiment in the SR ExperimentBuilder twice, once with automatic segmentation of text into individual characters as areas of interest, and once with word segmentation as areas of interest. The required files are provided in `aois/`. The script `char2word_mapping.py` will align the characters to the words.\n\n4. Extract word-level and character-level features with\n`python extract_features.py`  \nThis outputs new CSV files in `ExtractedFeatures/`.  \nThese files can also be downloaded directly from the [OSF repository](https://osf.io/ud8s5/).\n\nA description of the extracted features can be found [here](https://osf.io/ud8s5/wiki/Eye-tracking%20features/).\n\n\n\n## Data validation\n\nUse the script `validate_data.py` to check the data quality, e.g., word length effect and landing position analysis.\n\nUse the script `participant_correlations.py` to calculate correlations between different participants characteristics.\n\n## Dyslexia classification\n\nThis directory contains the code used for the dyslexica classification methods described in the following publication:\n\nMarina Björnsdóttir, Nora Hollenstein, and Maria Barrett, 2023. Dyslexia Prediction from Natural Reading of Danish Texts. In _Proceedings of the 24th Nordic Conference on Computational Linguistics_, pages 1712–1720, Torshavn, Faroe Islands. \n"
    },
    {
      "name": "TuanaCelik/what-would-mother-say",
      "stars": 6,
      "img": "https://avatars.githubusercontent.com/u/15802862?s=40&v=4",
      "owner": "TuanaCelik",
      "repo_name": "what-would-mother-say",
      "description": "💁‍♀️ A Tweet creation Agent that fetches usernames last k tweets and generates a tweet about the requested topic",
      "homepage": "https://huggingface.co/spaces/Tuana/what-would-mother-say",
      "language": "Python",
      "created_at": "2023-06-29T19:46:37Z",
      "updated_at": "2024-08-25T13:19:39Z",
      "topics": [
        "agent",
        "haystack",
        "llm",
        "nlp",
        "opensource"
      ],
      "readme": "---\ntitle: What would mother say?\nemoji: 🫶\ncolorFrom: pink\ncolorTo: yellow\nsdk: streamlit\nsdk_version: 1.21.0\napp_file: app.py\npinned: false\n---\n\n# What would mother say?\n\nThis app includes a Haystack agent with access to 2 tools:\n- `MastodonRetriever`: Useful for when you need to retrive the latest posts from a username to get an understanding of their style\n- `WebSearch`: Useful for when you need to research the latest about a new topic\n\nWe build an Agent that aims to first understand the style in which a username posts. Then, it uses the WebSearch tool to gain knowledge on a topic that the LLM may not have info on, to generate a post in the users style about that topic.\n### Try it out on [🤗 Spaces](https://huggingface.co/spaces/Tuana/what-would-mother-say)\n\n##### A showcase of a Haystack Agent with a custom `TwitterRetriever` Node and a `WebQAPipeline` as tools.\n\n**Custom Haystack Node**\n\nThis repo contains a streamlit application that given a query about what a certain twitter username would post on a given topic, generates a post in their style (or tries to). It does so by using a custom Haystack node I've built called the [`MastodonFetcher`](https://haystack.deepset.ai/integrations/mastodon-fetcher)\n\n**Custom PromptTemplates**\n\nIt's been built with [Haystack](https://haystack.deepset.ai) using the [`Agent`](https://docs.haystack.deepset.ai/docs/agent) and by creating a custom [`PromptTemplate`](https://docs.haystack.deepset.ai/docs/prompt_node#templates)\n\nAll the prompt templates used in this demo, both for the `WebQAPipeline` and the `Agent` can be found in `./prompts`.\n\n<img width=\"867\" alt=\"image\" src=\"https://github.com/TuanaCelik/what-would-mother-say/assets/15802862/b05f8bde-8fd5-4c6f-beac-1578437a145b\">\n\n## To learn more about the Agent\n\nCheck out our tutorial on the Conversational Agent [here](https://haystack.deepset.ai/tutorials/24_building_chat_app)\n\n## Installation and Running\n1. Install requirements:\n`pip install -r requirements.txt`\n2. Run the streamlit app:\n`streamlit run app.py`\n3. Createa a `.env` and add your Twitter Bearer token, OpenAI Key, and SerperDev Key:\n\n`TWITTER_BEARER_TOKEN`    \n\n`SERPER_KEY`  \n\n`OPENAI_API_KEY`    \n\nThis will start up the app on `localhost:8501` where you will find a simple search bar\n\n#### The Haystack Community is on [Discord](https://discord.com/invite/VBpFzsgRVF)\n"
    },
    {
      "name": "dtaivpp/opensearch-haystack-demo",
      "stars": 6,
      "img": "https://avatars.githubusercontent.com/u/17506770?s=40&v=4",
      "owner": "dtaivpp",
      "repo_name": "opensearch-haystack-demo",
      "description": "This is a short demo of using Haystack with OpenSearch as a context store.",
      "homepage": "",
      "language": "Python",
      "created_at": "2023-05-05T12:42:12Z",
      "updated_at": "2023-06-26T09:35:10Z",
      "topics": [
        "demo",
        "haystack",
        "llm",
        "opensearch"
      ],
      "readme": "# OpenSearch Haystack Demo\n\nWelcome to the OpenSearch Haystack Demo! Here we showcase how to use [OpenSearch](https://opensearch.org) as a context store for language models such as [flan](https://huggingface.co/google/flan-t5-large) and [ChatGPT](https://openai.com/blog/chatgpt). \n\nHere is how queries are handled in the project: \n![Architecture Diagram](./images/OpenSearch_LLM.drawio.png)\n\nThis demo uses a discord bot or cli as an interface into an LM or LLM such as ChatGPT or Flan-T5. OpenSearch provides context that the language model might need to answer the question. There are many other ways this could be exposed such as with a rest api or integrated into your webpage. \n\n## 0. Setup\nTo get the project running you will need an already setup OpenSearch cluster. For the purposes of this demo you can run `docker compose up -d` to get a simple one node cluster for testing. \n\nAdditionally, we need to install the prerequisiste python packages. These can be installed by running `python3 -m pip install -r requirments.txt` (If you are on an Arm based Mac you can run the m1-installer.sh as there are some other prerequisites). \n\n## 1. Ingesting Data\n\nThe first step to get the demo running is to ingest your data into OpenSearch. This step pulls any json documents from the `data` directory and ingests them into OpenSearch. The json documents need to be formatted as a list and have the context data stored in a field called `content` like the below sample: \n\n```json\n# ./data/documents.json\n[\n  {\n    \"content\": \"The context you would like returned.\"\n  },\n  {\n    \"content\": \"...\"\n  },\n]\n```\n\nThen run `python3 context-bot/cli.py -i` and it will scoop up all of the documents in the data directory into an index called `documents`. Once the data is ingested we can run the bot. \n\n## 2. Querying the model\n\nTo query the model you can run `python3 context-bot/cli.py -m flan -q \"<Your Query Here>\"`. Here model is the name of the language model you'd like to use. Right now we are configured for `flan` which runs flan-t5-large and `gpt` which is gpt-3.5-turbo. To run GPT you will need to put a `.env` file in the root of your repo with the following: \n\n```text\n# .env file\nOPEN_API_KEY=<open api key here>\n```\n\nYou can get your OpenAPI key from [here](https://platform.openai.com/account/api-keys). Please note: with the way this project is currently configured queries will run an average of 3-4 cents per question.\n\n## 3. Discord \n\nTo start the Discord bot you will need a `.env` file with the following: \n\n```\nDISCORD_TOKEN=<discord token here>\n```\n\nThen the bot can be run with `python3 contex-bot/discord_bot.py`. From your server you can then use either `!gpt <queston>` or `!flan <question>` to query the models. A more detailed guide on setting up discord bots is in the works. "
    },
    {
      "name": "ArzelaAscoIi/haystack-keda-indexing",
      "stars": 6,
      "img": "https://avatars.githubusercontent.com/u/37148029?s=40&v=4",
      "owner": "ArzelaAscoIi",
      "repo_name": "haystack-keda-indexing",
      "description": "Example code for scaling NLP indexing pipelines with KEDA",
      "homepage": null,
      "language": "Python",
      "created_at": "2023-04-21T12:19:42Z",
      "updated_at": "2024-07-03T02:44:48Z",
      "topics": [],
      "readme": "# Code for the blog post \"Scaling NLP indexing pipelines with KEDA\"\n<img src=\"/assets/keda-haystack.png\" width=\"100%\" />\n\n- Scaling NLP indexing pipelines with KEDA and Haystack — Part 1: The Application ([link](https://medium.com/@ArzelaAscoli/scaling-nlp-indexing-pipelines-with-keda-and-haystack-part-1-the-application-45f54fcabd5d))\n- Scaling NLP indexing pipelines with KEDA and Haystack — Part 2: Deployment ([link](https://medium.com/@ArzelaAscoli/scaling-nlp-indexing-pipelines-with-keda-and-haystack-part-2-deployment-8750dc96171f))\n\n\n### Development\n\nTo run the consumer locally against localstack, we need to start localstack by running the following command:\n\n```bash\n# Start localstack\ndocker-compose up \n```\n\nAfterwards, we install the dependencies and run the consumer:\n\n```bash\npip3 install -r requirements.txt\npython3 consumer.py\n```\n\nYou should see logs like \n    \n```bash\n❯ python3 consumer.py\n2023-04-21 15:34:26 [info     ] No files to upload\n2023-04-21 15:34:31 [info     ] Found files                    files=[PosixPath('/tmp/test.txt'), PosixPath('/tmp/test.txt'), PosixPath('/tmp/test.txt'), PosixPath('/tmp/test.txt')]\n2023-04-21 15:34:31 [info     ] No files to upload\n2023-04-21 15:34:36 [info     ] No files to upload\n```\n\n\nTo upload files to test the consumer, we can run the following command:\n\n```bash\npython3 upload.py\n```"
    },
    {
      "name": "gravityml/haystack-human-tool",
      "stars": 6,
      "img": "https://avatars.githubusercontent.com/u/129120892?s=40&v=4",
      "owner": "gravityml",
      "repo_name": "haystack-human-tool",
      "description": "Human in the loop tool for Haystack agents. Uses human guidance to refine queries and improve performance.",
      "homepage": null,
      "language": "Python",
      "created_at": "2023-04-17T21:33:39Z",
      "updated_at": "2024-12-28T21:04:18Z",
      "topics": [],
      "readme": "\n# Deprecated Notice\n\nThis package is now deprecated and has not been updated for a while. Please refer to the original Haystack repo for more up to date implementations of the same at [haystack](https://github.com/deepset-ai/haystack).\n\n# Haystack Human In Loop Tool\n\nHuman in Loop tool for [haystack](https://github.com/deepset-ai/haystack) Agents.  Uses human guidance to refine queries and improve performance.\n\n\n## Installation\n\n- Python pip: ```python3 -m pip install haystack-human-tool``` . This package will attempt to install the dependencies (farm-haystack>=1.15.0, redis, haystack-memory, bleach)\n- Python pip (skip dependency installation: Use  ```python3 -m pip install haystack-human-tool --no-deps```\n- Using git: ```pip install git+https://github.com/rolandtannous/haystack-human-tool.git@main#egg=haystack-human-tool```\n\n\n## Usage\n\nTo use the human in the loop tool in your agent, you need three core components:\n- `haystack-memory`: This is the memory library for haystack agents. It provides access to working and sensory memories.\n- `HumanInLoopNode`: This node adds human in the loop to the agent as a tool. It allows the agent to seek guidance from the human user in order to refine the user query.\n- `sensory-memory`: This is an in-memory implementation that mimics a human's brief sensory memory, lasting only for the duration of one interaction.\n\n```py\nfrom haystack.agents import Agent, Tool\nfrom haystack.nodes import PromptNode\nfrom haystack_memory.memory import MemoryRecallNode\nfrom haystack_memory.utils import MemoryUtils\nfrom haystack_human_tool.humaninloop import HumanInLoopNode\nfrom haystack_human_tool.prompt_templates import agent_template\n\n# Initialize the memory and the memory tool so the agent can retrieve the memory\nsensory_memory = []\nworking_memory = []\nmemory_node = MemoryRecallNode(memory=working_memory)\nmemory_tool = Tool(name=\"Memory\",\n                   pipeline_or_node=memory_node,\n                   description=\"Your memory. Always access this tool first to remember what you have learned.\")\n\nhuman_node = HumanInLoopNode(sensory_memory=sensory_memory)\nhuman_tool = Tool(name=\"Human\",\n                  pipeline_or_node=human_node,\n                  description=\"Access this tool to refine your query. Ask the human to rephrase the question or to explain what or who the human is referring to. Use the human's answer to rephrase your query for use as input to other tools\")\n\nprompt_node = PromptNode(model_name_or_path=\"text-davinci-003\", \n                         api_key=\"<YOUR_OPENAI_KEY>\", \n                         max_length=1024,\n                         stop_words=[\"Observation:\"])\nmemory_agent = Agent(prompt_node=prompt_node, prompt_template=agent_template)\nmemory_agent.add_tool(memory_tool)\nmemory_agent.add_tool(human_tool)\n\n# Initialize the utils to save the query and the answers to the memory\nmemory_utils = MemoryUtils(working_memory=working_memory, sensory_memory=sensory_memory, agent=memory_agent)\nresult = memory_utils.chat(\"<Your Question>\")\n```\n\n\n## Example\n\nAn Example can be found in the `examples/` folder. It contains the usage for all memory types.\nTo open the example in colab, click on the following links:\n- Human in Tool powered agent: [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/gravityml/haystack-human-tool/blob/main/examples/example_humanloop_with_memory.ipynb)\n\n\n\n\n\n\n\n\n"
    },
    {
      "name": "MysterionRise/boost-search",
      "stars": 6,
      "img": "https://avatars.githubusercontent.com/u/956191?s=40&v=4",
      "owner": "MysterionRise",
      "repo_name": "boost-search",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2017-09-29T08:13:53Z",
      "updated_at": "2023-01-22T16:50:42Z",
      "topics": [],
      "readme": "## Different cutting-edge approaches to traditional information retrieval projects\n\nWhat's inside:\n- Docker compose files with Elastic and Kibana nodes (both Elastic and Open Distro)\n- Dense vectors usage in Elastic for PDF documents similarity\n- *farm-haystack* docker-compose ready project with Q&A app (https://github.com/deepset-ai/haystack)\n- *jina-ai* plays with image and text similarity (https://github.com/jina-ai/jina)\n- *nboost* (https://github.com/koursaros-ai/nboost)\n- *Milvus* (https://milvus.io/)\n- [Permissions search](permissions-search/README.md) in Elastic\n"
    },
    {
      "name": "baochi0212/tourxQA",
      "stars": 6,
      "img": "https://avatars.githubusercontent.com/u/77192945?s=40&v=4",
      "owner": "baochi0212",
      "repo_name": "tourxQA",
      "description": "End-to-end Tourist Aid Question-Answering Chatbot",
      "homepage": "",
      "language": "Python",
      "created_at": "2022-10-20T16:28:30Z",
      "updated_at": "2024-10-10T05:08:00Z",
      "topics": [],
      "readme": "# deep-learning\ntourist aid \n## Description\n\nchat bot supporting tourism\n\n## Getting Started\n\n\n### Installations\n\n* via Pip: \n```\npip install -q -r requirements.txt\n```\n\n### Executing program\n\n* How to run the program:\n```\nUse how_to_run_tourxQA.ipynb\n```\n### Struc \n* model + modules: source/model, source/modules\n* trainer: source/trainer\n* run script: source/runtime\n* model saving: source/model_dir\n* distillation: source/distill.py\n### DEMO\n[short.webm](https://user-images.githubusercontent.com/77192945/219418319-8831b116-2eb4-4c7c-9331-7b0e6a09e587.webm)\n\n\n\n"
    },
    {
      "name": "okara83/Becoming-a-Data-Scientist",
      "stars": 6,
      "img": "https://avatars.githubusercontent.com/u/2750953?s=40&v=4",
      "owner": "okara83",
      "repo_name": "Becoming-a-Data-Scientist",
      "description": "Mentoring and Teaching Repo",
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2022-03-17T09:41:11Z",
      "updated_at": "2023-12-05T14:26:43Z",
      "topics": [],
      "readme": "# Becoming-A-Data-Scientist\nTeaching resources\n"
    },
    {
      "name": "flexudy-pipe/qugeev",
      "stars": 6,
      "img": "https://avatars.githubusercontent.com/u/63919993?s=40&v=4",
      "owner": "flexudy-pipe",
      "repo_name": "qugeev",
      "description": "A simple way to evaluate question generation models",
      "homepage": null,
      "language": "Python",
      "created_at": "2020-10-21T20:29:08Z",
      "updated_at": "2023-12-05T03:48:45Z",
      "topics": [],
      "readme": "# qugeev\nA simple way to evaluate question generation models.\n\nQuestions Asking is an important NLP task. Making sure that the right answers are predicted is crucial for quality assurance.\nI will assume that you have access to a software, library or tool of some kind, that can generate question-answer pairs from text (called the context).\n\nThis library uses [deepset's haystack](https://github.com/deepset-ai/haystack) to evaluate the answers your model (tool or software) predicts.\n\n## How to use the library\n1. You have a list of sentences (contexts)\n2. Use your tool or model to generate questions-answer pairs\n3. Feed them into the evaluator\n4. We use deepset's haystack to predict a candidate answer for each question and context pair\n5. We compair the results (predicted by haystack and the one your model generates) using [semeval](https://github.com/chakki-works/sumeval).\n6. The mean F1 ROUGE scores (for each answer pairs) is returned.\n\n## Example\n```python\nfrom evaluation import Evaluator\n\nevaluator = Evaluator(hugging_face_model_name=\"distilbert-base-uncased-distilled-squad\",\n                      tokenizer_name=\"distilbert-base-uncased\",\n                      cuda_is_available=False)\n\nquestions = [\"What is the term for a family of team sports?\", \"What is football?\"]\n\nanswers = [\"Football\",\n           \"Football is a family of team sports that involve, to varying degrees, kicking a ball to score a goal.\"]\n\ncontexts = [\"Football is a family of team sports that involve, to varying degrees, kicking a ball to score a goal.\",\n            \"Football is a family of team sports that involve, to varying degrees, kicking a ball to score a goal.\"]\n\nscore = evaluator.evaluate_question_answer_pairs(questions, answers, contexts, verbose=True)\n\n```\nCheck out the `example.py` script.\n\n## Limitations\n- Although haystack is an amazing library, there is still a very small chance the results predicted by the library could be incorrect. In the evaluator I only evaluate on the best answer returned by haystack. This means, haystack might even predict the correct answer but this answer could have a lower rank. This might affect the F1 score. For example, in the above example, the second answer is actually \"correct\", but haystack's top answer is `None`. This leads to a score of `0.33` instead of say `0.99`.\n\n- The results also greatly depend on the model used, the n-grams (i use uni-grams for evaluations) and probably other parameters like the the context window. \n\nHence feel free to experiment. I however think that the default parameters are good enough to have a good feel about the performance of your model.\n\n"
    },
    {
      "name": "gariciodaro/arXiv-haystack-app",
      "stars": 6,
      "img": "https://avatars.githubusercontent.com/u/9937662?s=40&v=4",
      "owner": "gariciodaro",
      "repo_name": "arXiv-haystack-app",
      "description": "full stack dev.  Explore and ask questions on ArXiv Kaggle dataset.",
      "homepage": "",
      "language": "HTML",
      "created_at": "2020-09-10T14:56:09Z",
      "updated_at": "2020-11-15T09:26:44Z",
      "topics": [],
      "readme": "\n# :star: Explore and ask questions on ArXiv Kaggle dataset.\n\n##  :point_right: Abstract\nUsing [Arxiv](https://www.kaggle.com/Cornell-University/arxiv)  and [Neural Information Processing System](https://www.kaggle.com/benhamner/nips-papers) datasets I created a relational database model that allows the user to do Ad-Hoc queries for analytics. Using haystack the application allows for question and answering on user defined abstracts of the database. \n\n**Keywords:** S3, Amazon Elastic MapReduce, Pyspark, Airflow, Redshift, Flask, NLP, Q&A, Haystack,Machine Learning, Full stack development, Data Engineering.\n\n\n<img src=\"./img/architecture.png\">\n\n## :exclamation: My vision\nThe project has two main parts. The back-end and front-end. \n\nThe back end was designed to run in the cloud (AWS) but controlled by an local airflow server, this includes:\n\n+ EMR(Pyspark) processing.\n+ Loading to relational database on Redshift.\n\nThe front end, built on Flask, runs on the client also. Its functionality includes:\n\n+ Query the Redshift database.\n+ Index abstracts to a local elastic search cluster.\n+ Ask questions on Indexed abstracts. \n\nI imagine a local user, setting up the flask app and then browsing the \ndatabase with it, then later indexing abstracts and titles of some articles of interest, this could mean\nfiltering by a particular topic, year, or author. \nOnce the selected documents were indexed, the user can ask questions of the type *what do we know about the uncertainty principle?*. \nThe answer will come on the form of a bootstrap card, with scores, possible answers, \nfull paper URL pdf access to the document. \nThe question and answering system is possible with [haystack](https://github.com/deepset-ai/haystack). The following are the core features of haystack on their github.\n\n\n+ Powerful ML models: Utilize all latest transformer based models (BERT, ALBERT, RoBERTa ...)\n+ Modular \\& future-proof: Easily switch to newer models once they get published.\n+ Developer friendly: Easy to debug, extend and modify.\n+ Scalable: Production-ready deployments via Elasticsearch backend \\& REST API\n+ Customizable: Fine-tune models to your own domain \\& improve them continuously via user feedback\n\n\n\n## The data\n\nI used the [Arxiv](https://www.kaggle.com/Cornell-University/arxiv)  and Neural Information Processing \n    Systems ([NIPS](https://www.kaggle.com/benhamner/nips-papers)) datasets from kaggle. I hosted both files on a S3: \ns3a://arxivs3/input_data/arxiv-metadata-oai-snapshot.json, s3a://arxivs3/input_data/NIPS.csv. \n\n\n```\n>>> df_arxiv.printSchema()\nroot\n |-- abstract: string (nullable = true)\n |-- authors: string (nullable = true)\n |-- authors_parsed: array (nullable = true)\n |    |-- element: array (containsNull = true)\n |    |    |-- element: string (containsNull = true)\n |-- categories: string (nullable = true)\n |-- comments: string (nullable = true)\n |-- doi: string (nullable = true)\n |-- id: string (nullable = true)\n |-- journal-ref: string (nullable = true)\n |-- license: string (nullable = true)\n |-- report-no: string (nullable = true)\n |-- submitter: string (nullable = true)\n |-- title: string (nullable = true)\n |-- update_date: string (nullable = true)\n |-- versions: array (nullable = true)\n |    |-- element: struct (containsNull = true)\n |    |    |-- created: string (nullable = true)\n |    |    |-- version: string (nullable = true)\n>>> df_papers_nips.printSchema()\nroot\n |-- id: string (nullable = true)\n |-- year: string (nullable = true)\n |-- title: string (nullable = true)\n |-- abstract: string (nullable = true)\n```\n\n\n## The data model\nInitially, I thought having a star schema would be the correct choice. It would offer a degree of normalization, while still providing an easy to understand data model. Nonetheless, upon trying to implement it, I realized that a query optimized model, where you have a table per query seems more appropriate, that way, in principle, you could train machine learning models per table, without having to refer to the fact table or others tables. On the other hand, I also envisioned non-technical users exploring basic trends, or asking questions about popular authors or topics in a given year. In the end, my data model resembles the star schema, allowing for ad hoc queries, while at the same time provides query optimized tables for machine learning. This hybrid approach was the main reason to select AWS Redshift cluster as the host of the model. Redshift provides Postgresql-like query language suitable for non-technical users while being Massively parallel processing (MPP) to operate on large amounts of Data. Additionally, Redshift is easily integrated with python3 with the *psycopg2* module (for front-end) and with *Airflow* through the Postgres operator.\n\n<img src=\"./img/star_schema.png\">\n\n## ETL\nOnce I designed the data model and selected the hosting technology, I constructed the ETL scripts. I used PySpark to transform the json and csv into DataFrames, process them and then leave them on S3 as parquet files. *Spark*, in contrast to Redshift, supports json data manipulation, it is also parallelizable and comes with machine learning libraries. It is able to store data as parquet files, a columnar format particularity suitable for big data.\n\nMost of the transformations involved flattening the nested data from the original .json, with the occasional use of regular expressions to extract temporal features of the records. All this process can be found in [pysparkCreateParquets_TEMP.py](https://github.com/gariciodaro/arXiv-haystack-app/blob/master/back_end/scripts/pysparkCreateParquets_TEMP.py).\n\n\n## ETL Orchestration with Airflow\n*Airflow* allows for ETL Orchestration with its webserver. I created tree Dags. [create_tables_redshift](https://github.com/gariciodaro/arXiv-haystack-app/blob/master/back_end/dags/createRedShiftDB.py) allows you to create the structure on Redshift of the data model. [create_parquet_area](https://github.com/gariciodaro/arXiv-haystack-app/blob/master/back_end/dags/sparkPreProcess.py) runs [pysparkCreateParquets_TEMP.py](https://github.com/gariciodaro/arXiv-haystack-app/blob/master/back_end/scripts/pysparkCreateParquets_TEMP.py) script, and finally [load_data_to_redshift](https://github.com/gariciodaro/arXiv-haystack-app/blob/master/back_end/dags/RedShiftDataModel.py) copies the parquet files to Redshift and performs data quality checks (volumne checnk, and id nullity).\n\nAll the DAGs are set to run on demand, this means that a user would have to trigger the dag to actually execute it. If the user wanted to setup the automatic execution, for example *daily basis by 7am* all he/she needs to do is change the dag configuration in  *load_data_to_redshift*.\n\n```\nargs = {\n    'owner': 'arXiv-haystack-app',\n    'start_date': datetime.datetime.utcnow(),\n    'catchup': False,\n    'depends_on_past':False\n}\ndag = DAG(\n        dag_id='create_parquet_area',\n        default_args=args,\n        schedule_interval='30 7 * * *')\n        )\n```\nThe whole development is ready to handle a 100x increase in data thanks to *PySpark* scalability, and to withstand high user concurrency thanks to *Redshift* MPP nature.\n\n\n<img src=\"./img/load_data_to_redshift.png\">\n\n## Front-end\nI used Flask to create a web interface that queries the redshifts back-end. My original idea was to allow users to explore the database to select a subset of papers and then add the full-text paper to an elastic search cluster (key-pair document storage) for machine learning exploitation. Unfortunately the full pdfs of Arxiv are not available for *wget* download, which I found out after finishing the code (can be seen on branch pdf_fail_aws). This pipeline consisted of obtaining the URL from Redshift, downloading the pdf to the client, and then upload them to s3, where Amazon Textract could process them asynchronously to text, and eventually append them to the local elastic search server. I was forced to change it to appending the title and abstract from Redshift to Elastic.\n\n### Url: Home\n<img src=\"./img/explore.png\">\n\n### Url: Q&A\n<img src=\"./img/qanda.png\">\n\n## File structure\n```\n├── back_end\n│   ├── ...\n│   └── Dockerfile\n│   └── README.md\n├── docker_elastic_search\n│   └── Dockerfile\n├── front_end\n│   ├── ...\n│   └── Dockerfile\n│   └── README.md\n├── img\n│   └── architecture.png\n├── README.md\n├── report.pdf\n```\n\n+ ``` report.pdf ``` is a latex report of the application, please read for \nmore context.\n\n+ Please use docker to start the back-end airflow service, the flask front end, and the local elastic cluster.\n+ The back_end and front_end folder have a README.md file, please read them. Notice that the back-end can run without the front end. However the front-end requires at least the ```docker_elastic_search``` container to start.\n\n\n\n## Usage with docker\n\n+ Run back-end docker container (go to back-end folder) to initialize airflow\nwebserver on port 8080: \n```\nsudo docker build -t back-end .\nsudo docker run -it --network=host back-end\n```\n+ Run elastic search cluster container (go to docker_elastic_search folder) \non port 9200\n```\ndocker build -t elastic .\nsudo docker run -d --network=host -e \"discovery.type=single-node\" elastic\n```\n+ Run front-end container (got to front-ent folder) to start flask-app on port 5000. Notice\nthat we pass our locar aws configuration file to the container:\n```\nsudo docker build -t front-end --build-arg CREDENTIALS=\"$(cat ~/.aws/credentials)\" .\nsudo docker run -it --network=host front-end\n```\n+ AWS configuration file should look like this:\n```\n[credentials]\nKEY=xxxxxxxxxxxxxx\nSECRET=xxxxxxxxxx\nREGION=us-west-2\n[default]\naws_access_key_id = xxxxxxxxxxxxxxxx\naws_secret_access_key = xxxxxxxxxxxxxxxxx\n[DWH] \nDWH_CLUSTER_TYPE=multi-node\nDWH_NUM_NODES=2\nDWH_NODE_TYPE=dc2.large\nDWH_IAM_ROLE_NAME=xxxxxxx\nDWH_CLUSTER_IDENTIFIER=xxxxxxxx\nDWH_DB=xxxxxxxx\nDWH_DB_USER=xxxxxxxxx\nDWH_DB_PASSWORD=xxxxxxx\nDWH_PORT=5439\nDWH_HOST=xxxxxxxxx.xxxxxxxxxxxx.us-west-2.redshift.amazonaws.com\nDWH_ROLE=arn:aws:iam::xxxxxxxxxxxxx:role/xxxxxxxxx\n[APP]\nS3_BUCKED=arxivs3\n```\n\n\n"
    },
    {
      "name": "afontana1/Data-Engineering",
      "stars": 6,
      "img": "https://avatars.githubusercontent.com/u/46588040?s=40&v=4",
      "owner": "afontana1",
      "repo_name": "Data-Engineering",
      "description": null,
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2024-08-19T17:02:00Z",
      "updated_at": "2025-04-20T19:23:02Z",
      "topics": [],
      "readme": "# Resources\n\n- [Data Engineering Wiki](https://dataengineering.wiki/Index)\n\n<details><summary><h1>Design Guides</h1></summary>\n\n- [The Data Engineering Cookbook](https://cookbook.learndataengineering.com/)\n- [Data Engineer Handbook](https://github.com/DataEngineer-io/data-engineer-handbook)\n- [Netflix Tech Blog Data Engineering](https://netflixtechblog.com/tagged/data-engineering)\n- [Uber Engineering Blog](https://www.uber.com/en-US/blog/engineering/)\n- [Cloudflare Blog](https://blog.cloudflare.com/)\n- [Meta Engineering Blog](https://engineering.fb.com/)\n- [Linkedin Engineering](https://www.linkedin.com/blog/engineering)\n- [AWS Architecture Blog](https://aws.amazon.com/blogs/architecture/)\n- [Slack Engineering Blog](https://slack.engineering/)\n- [Stripe Engineering Blog](https://stripe.com/blog/engineering)\n- [AWS SDK Examples](https://github.com/awsdocs/aws-doc-sdk-examples)\n- [AWS Samples](https://github.com/aws-samples)\n- [AWS Labs](https://github.com/awslabs)\n- [aws-solutions](https://github.com/aws-solutions)\n- [build-on-aws](https://github.com/build-on-aws)\n- [Microservices.io](https://microservices.io/)\n- [martin fowler](https://martinfowler.com/) and [Gregor Hohpe](https://architectelevator.com/)\n\n#### Tutorials\n\n- [ArjanCodes](https://github.com/ArjanCodes)\n- [mCodingLLC](https://github.com/mCodingLLC)\n- [Building a Poor Mans Datalake from Scratch with DuckDB](https://dagster.io/blog/duckdb-data-lake)\n- [Revisiting the Poor Man’s Data Lake with MotherDuck](https://dagster.io/blog/poor-mans-datalake-motherduck)\n- [Database Transactions](https://bbengfort.github.io/2017/12/psycopg2-transactions/)\n\n1. [Design Patterns in the Real World](https://holub.com/patterns/)\n2. [Design Patterns](https://sourcemaking.com/)\n3. [Refactoring Guru](https://refactoring.guru/)\n4. [Design Patterns: Elements of Reusable Software](https://wiki.c2.com/?DesignPatternsBook)\n5. [UI Design Patterns](https://ui-patterns.com/patterns)\n6. [OO Design](https://www.oodesign.com/)\n7. [python-ddd](https://github.com/pgorecki/python-ddd)\n8. [Design Patterns Scala](https://pavelfatin.com/design-patterns-in-scala/)\n9. [Enterprise Integration Patterns](https://www.enterpriseintegrationpatterns.com/index.html)\n10. [Practical Cryptography for Developers](https://cryptobook.nakov.com/)\n11. [Problem Solving with Algorithms and Data Structures using Python](http://www.openbookproject.net/books/pythonds/index.html#)\n12. [Computer Security](https://textbook.cs161.org/)\n13. [Open DSA Data Structures and Algorithms](https://opendsa-server.cs.vt.edu/ODSA/Books/Everything/html/index.html)\n14. [Awesome ETL](https://github.com/pawl/awesome-etl)\n15. [Scala Design Patterns](https://github.com/josephguan/scala-design-patterns)\n16. [Goodreads ETL Pipeline](https://github.com/san089/goodreads_etl_pipeline)\n17. [Around Data Engineering](https://github.com/abhishek-ch/around-dataengineering)\n18. [Awesome Design Patterns](https://github.com/DovAmir/awesome-design-patterns)\n19. [Python Patterns](https://python-patterns.guide/)\n20. [Start Data Engineering](https://www.startdataengineering.com/)\n21. [Formal Ontology](http://ontology.buffalo.edu/smith/)\n22. [6.005 Software Construction](https://ocw.mit.edu/courses/6-005-software-construction-spring-2016/)\n23. [Google Site Reliability Engineering](https://sre.google/sre-book/table-of-contents/)\n24. [Calm Code](https://calmcode.io/)\n25. [Cosmic Python](https://www.cosmicpython.com/book/preface.html)\n26. [Python for Data Analysis](https://wesmckinney.com/book/)\n27. [Python Data Science Handbook](https://jakevdp.github.io/PythonDataScienceHandbook/)\n28. [Operating Systems](https://www.cs.uic.edu/~jbell/CourseNotes/OperatingSystems/index.html)\n29. [Distributed Computing](https://distributed-computing-musings.com/)\n30. [Data Integration](https://en.wikipedia.org/wiki/Category:Data_integration)\n31. [Data Tools](https://github.com/victorcouste/data-tools)\n32. [Awesome Big Data](https://github.com/newTendermint/awesome-bigdata)\n33. [data oriented design](https://github.com/dbartolini/data-oriented-design)\n34. [Patterns of Distributed Systems](https://martinfowler.com/articles/patterns-of-distributed-systems/)\n35. [Data Mesh Principles](https://martinfowler.com/articles/data-mesh-principles.html) and [Architecture](https://www.datamesh-architecture.com/)\n36. [Patterns for API Design](https://microservice-api-patterns.org/)\n37. [gaphor](https://github.com/gaphor/gaphor)\n38. [Data Centric Design](https://delftdesignlabs.org/dcd-lab/)\n39. [open-data-fabric](https://github.com/open-data-fabric/open-data-fabric)\n40. [serverlessland](https://serverlessland.com/)\n41. [Data Oriented Design](https://github.com/dbartolini/data-oriented-design)\n42. [Python Patterns](https://github.com/faif/python-patterns)\n43. [Diagrams](https://github.com/mingrammer/diagrams)\n44. [python-anti-patterns](https://github.com/quantifiedcode/python-anti-patterns)\n45. [Python 3 Patterns, Recipes and Idioms](https://python-3-patterns-idioms-test.readthedocs.io/en/latest/index.html)\n46. [awesome-ddd](https://github.com/heynickc/awesome-ddd)\n47. [designing-data-intensive-applications](https://github.com/ResidentMario/designing-data-intensive-applications-notes)\n48. [solution-architecture-patterns](https://github.com/chanakaudaya/solution-architecture-patterns)\n49. [Serverless Patterns](https://github.com/aws-samples/serverless-patterns)\n50. [awesome-system-design](https://github.com/madd86/awesome-system-design)\n51. [awesome-software-architecture](https://github.com/mehdihadeli/awesome-software-architecture)\n52. [Google Site Reliabilty Engineering](https://sre.google/sre-book/table-of-contents/)\n53. [Microservice API Patterns](https://microservice-api-patterns.org/)\n\n</details>\n\n<details><summary><h2>Data Concepts</h2></summary>\n\n1. [Data](https://en.wikipedia.org/wiki/Data_(computing))\n2. [Databases](https://en.wikipedia.org/wiki/Category:Databases)\n3. [Database Management Systems](https://en.wikipedia.org/wiki/Category:Database_management_systems)\n4. [Data Warehouse](https://en.wikipedia.org/wiki/Category:Data_warehousing)\n5. [Data Modeling](https://en.wikipedia.org/wiki/Category:Data_modeling)\n    * [Dimensional Modeling](https://en.wikipedia.org/wiki/Dimensional_modeling)\n    * [Database Normalization](https://en.wikipedia.org/wiki/Database_normalization)\n    * [Kimball Techniques](https://www.kimballgroup.com/data-warehouse-business-intelligence-resources/kimball-techniques/dimensional-modeling-techniques/)\n6. [Data Ops](https://www.dataopsmanifesto.org/)\n7. [Metadata](https://en.wikipedia.org/wiki/Category:Metadata)\n8. [Data Management](https://en.wikipedia.org/wiki/Category:Data_management)\n\n</details>\n\n<details><summary><h2>Software Engineering Resources</h2></summary>\n\n# Reading\n\n1. [Algorithms](https://en.wikipedia.org/wiki/Category:Algorithms)\n2. [Data Structures](https://en.wikipedia.org/wiki/Category:Data_structures)\n3. [Software_architecture](https://en.wikipedia.org/wiki/Category:Software_architecture)\n    * [Microservices](https://en.wikipedia.org/wiki/Microservices)\n    * [Service-oriented](https://en.wikipedia.org/wiki/Category:Service-oriented_(business_computing))\n    * [Architectural_pattern_](https://en.wikipedia.org/wiki/Category:Architectural_pattern_(computer_science))\n    * [Systems_architecture](https://en.wikipedia.org/wiki/Systems_architecture)\n    * [Message_queue](https://en.wikipedia.org/wiki/Message_queue)\n    * [Aspect Oriented Programming](https://en.wikipedia.org/wiki/Aspect-oriented_programming)\n    * [Enterprise_architecture](https://en.wikipedia.org/wiki/Category:Enterprise_architecture)\n    * [Unified Modeling Language](https://en.wikipedia.org/wiki/Category:Unified_Modeling_Language)\n4. [Software_engineering](https://en.wikipedia.org/wiki/Category:Software_engineering)\n    * [Software_design_patterns](https://en.wikipedia.org/wiki/Category:Software_design_patterns)\n    * [Software_design](https://en.wikipedia.org/wiki/Category:Software_design)\n    * [Software_development](https://en.wikipedia.org/wiki/Software_development)\n    * [Agile_software_development](https://en.wikipedia.org/wiki/Category:Agile_software_development)\n    * [Object Oriented](https://en.wikipedia.org/wiki/Category:Object-oriented_programming)\n    * [Software_development_philosophies](https://en.wikipedia.org/wiki/Category:Software_development_philosophies)\n5. [Programming_paradigms](https://en.wikipedia.org/wiki/Category:Programming_paradigms)\n6. [Software_testing](https://en.wikipedia.org/wiki/Category:Software_testing)\n7. [Systems_engineering](https://en.wikipedia.org/wiki/Category:Systems_engineering)\n8. [Systems_science](https://en.wikipedia.org/wiki/Category:Systems_science)\n9. [Systems_theory](https://en.wikipedia.org/wiki/Category:Systems_theory)\n10. [Systems_analysis](https://en.wikipedia.org/wiki/Systems_analysis)\n11. [Cloud Computing](https://en.wikipedia.org/wiki/Category:Cloud_computing)\n    * [Infrastructure_as_code](https://en.wikipedia.org/wiki/Infrastructure_as_code)\n    * [Enterprise_application_integration](https://en.wikipedia.org/wiki/Category:Enterprise_application_integration)\n12. [Software_requirements](https://en.wikipedia.org/wiki/Category:Software_requirements)\n    * [Requirements_analysis](https://en.wikipedia.org/wiki/Requirements_analysis)\n    * [Software Quality](https://en.wikipedia.org/wiki/Category:Software_quality)\n    * [List of System Quality Attributes](https://en.wikipedia.org/wiki/List_of_system_quality_attributes)\n    * [High_availability](https://en.wikipedia.org/wiki/High_availability)\n13. [Programming Paradigms](https://en.wikipedia.org/wiki/Category:Programming_paradigms)\n14. [Programming_language_concepts](https://en.wikipedia.org/wiki/Category:Programming_language_concepts)\n15. [Programming_principles](https://en.wikipedia.org/wiki/Category:Programming_principles)\n    * [Loose_coupling](https://en.wikipedia.org/wiki/Loose_coupling)\n    * [Separation of Concerns](https://en.wikipedia.org/wiki/Separation_of_concerns)\n    * [SOLID](https://en.wikipedia.org/wiki/SOLID)\n\n## Design Patterns\n1. [Abstract_factory_pattern](https://en.wikipedia.org/wiki/Abstract_factory_pattern)\n2. [Builder_pattern](https://en.wikipedia.org/wiki/Builder_pattern)\n3. [Singleton_pattern](https://en.wikipedia.org/wiki/Singleton_pattern)\n4. [Prototype_pattern](https://en.wikipedia.org/wiki/Prototype_pattern)\n5. [Object_pool_pattern](https://en.wikipedia.org/wiki/Object_pool_pattern)\n6. [Facade_pattern](https://en.wikipedia.org/wiki/Facade_pattern)\n7. [Chain of Responsibility Pattern](https://en.wikipedia.org/wiki/Chain-of-responsibility_pattern)\n8. [Flyweight Pattern](https://en.wikipedia.org/wiki/Flyweight_pattern)\n9. [Design Factory Patterns](http://coding-geek.com/design-pattern-factory-patterns/)\n10. [Composite Pattern](https://en.wikipedia.org/wiki/Composite_pattern)\n11. [Bridge Pattern](https://en.wikipedia.org/wiki/Bridge_pattern)\n12. [Mediator Pattern](https://en.wikipedia.org/wiki/Mediator_pattern)\n13. [Visitor Pattern](https://en.wikipedia.org/wiki/Visitor_pattern)\n14. [Adapter Pattern](https://en.wikipedia.org/wiki/Adapter_pattern)\n15. [Model View Controller](https://en.wikipedia.org/wiki/Model%E2%80%93view%E2%80%93controller)\n16. [Decorator Pattern](https://en.wikipedia.org/wiki/Decorator_pattern)\n17. [Chain of Responsibility Pattern](https://en.wikipedia.org/wiki/Chain-of-responsibility_pattern)\n18. [Command Pattern](https://en.wikipedia.org/wiki/Command_pattern)\n\n\n## Concepts\n1. [Futures_and_promises](https://en.wikipedia.org/wiki/Futures_and_promises)\n2. [Asynchronous_method_invocation](https://en.wikipedia.org/wiki/Asynchronous_method_invocation)\n3. [Asynchronous_I/O](https://en.wikipedia.org/wiki/Asynchronous_I/O)\n4. [Async/await](https://en.wikipedia.org/wiki/Async/await)\n5. [Asynchrony](https://en.wikipedia.org/wiki/Asynchrony_(computer_programming))\n6. [Concurrent_computing](https://en.wikipedia.org/wiki/Concurrent_computing)\n7. [Thread](https://en.wikipedia.org/wiki/Thread_(computing))\n8. [Parallel_computing](https://en.wikipedia.org/wiki/Parallel_computing)\n9. [Birthday_problem](https://en.wikipedia.org/wiki/Birthday_problem)\n10. [SHA 1](https://en.wikipedia.org/wiki/SHA-1)\n11. [Avalanche_effect](https://en.wikipedia.org/wiki/Avalanche_effect)\n12. [Hash_collision](https://en.wikipedia.org/wiki/Hash_collision)\n13. [Lazy_evaluation](https://en.wikipedia.org/wiki/Lazy_evaluation)\n14. [Relational_algebra](https://en.wikipedia.org/wiki/Category:Relational_algebra)\n15. [Effective Method](https://en.wikipedia.org/wiki/Effective_method)\n16. [Kolmogorov Complexity](https://en.wikipedia.org/wiki/Kolmogorov_complexity)\n\n</details>\n\n<details><summary><h2>Continuous Integration and Continuous Deployment (DevOps)</h2></summary>\n\n1. [jenkins-tutorial](https://github.com/ssbostan/jenkins-tutorial)\n2. [awesome-ciandcd](https://github.com/cicdops/awesome-ciandcd)\n3. [awesome-ci](https://github.com/ligurio/awesome-ci)\n4. [eksctl](https://github.com/eksctl-io/eksctl)\n5. [Dev-ops Exercises](https://github.com/bregman-arie/devops-exercises)\n6. [DockerLabs](https://github.com/collabnix/dockerlabs)\n7. [former2](https://github.com/iann0036/former2)\n8. [awesome-kubernetes](https://github.com/nubenetes/awesome-kubernetes)\n9. [Introduction to GitLab CI & DevOps with AWS - Course Notes](https://gitlab.com/gitlab-course-public/freecodecamp-gitlab-ci/-/blob/main/docs/course-notes.md)\n10. [Scuba](https://github.com/JonathonReinhart/scuba)\n11. [dagu](https://github.com/dagu-dev/dagu)\n12. [tawazi](https://github.com/mindee/tawazi)\n13. [gitlab-ci](https://github.com/dokku/gitlab-ci)\n14. [pulumi](https://github.com/pulumi/pulumi)\n\n</details>\n\n<details><summary><h2>ML Ops</h2></summary>\n\n1. [ML Ops Cookbook](https://github.com/noahgift/Python-MLOps-Cookbook)\n2. [awesome-mlops](https://github.com/kelvins/awesome-mlops)\n3. [ML Ops Specialization](https://github.com/kennethleungty/MLOps-Specialization-Notes)\n4. [Coursera ML Ops](https://github.com/amanchadha/coursera-machine-learning-engineering-for-prod-mlops-specialization)\n5. [Engineering ML Ops](https://github.com/PacktPublishing/EngineeringMLOps)\n6. [Practical ML Ops](https://github.com/paiml/practical-mlops-book)\n7. [awesome ML Ops](https://github.com/visenger/awesome-mlops)\n8. [Evidently](https://github.com/evidentlyai/evidently)\n9. [Machine Learning version control](https://github.com/iterative/dvc)\n10. [Feast](https://github.com/feast-dev/feast)\n11. [CleanLab](https://github.com/cleanlab/cleanlab)\n12. [ML Version Control](https://github.com/iterative/dvc)\n13. [WanDB](https://github.com/wandb/wandb)\n14. [Metaflow](https://github.com/Netflix/metaflow)\n15. [River](https://github.com/online-ml/river)\n16. [Bytewax](https://github.com/bytewax/bytewax)\n17. [ml-design-patterns](https://github.com/GoogleCloudPlatform/ml-design-patterns)\n18. [mlflow](https://github.com/mlflow/mlflow)\n19. [kestra](https://github.com/kestra-io/kestra)\n20. [kedro](https://github.com/kedro-org/kedro)\n\n</details>\n\n<details><summary><h2>Data and System Security</h2></summary>\n\n1. [ThreatDragon](https://threatdragon.github.io/)\n2. [Kotlin Faker](https://github.com/serpro69/kotlin-faker)\n3. [DataBunker](https://github.com/securitybunker/databunker)\n4. [awesome-IAM](https://github.com/kdeldycke/awesome-iam)\n5. [open-data-anonymizer](https://github.com/ArtLabss/open-data-anonymizer)\n6. [Presidio](https://github.com/microsoft/presidio)\n7. [Penetration Testing Tools](https://github.com/mgeeky/Penetration-Testing-Tools)\n8. [Public Pentesting Reports](https://github.com/juliocesarfort/public-pentesting-reports)\n9. [Nettacker](https://github.com/OWASP/Nettacker)\n10. [Kubesploit](https://github.com/cyberark/kubesploit)\n11. [Hackerpro](https://github.com/jaykali/hackerpro)\n12. [RapidScan](https://github.com/skavngr/rapidscan)\n13. [Astra](https://github.com/flipkart-incubator/Astra)\n14. [awesome-pentest-cheat-sheets](https://github.com/coreb1t/awesome-pentest-cheat-sheets)\n15. [Hacking Security Ebooks](https://github.com/yeahhub/Hacking-Security-Ebooks)\n16. [awesome-infosec](https://github.com/onlurking/awesome-infosec)\n17. [awesome-web-hacking](https://github.com/infoslack/awesome-web-hacking)\n18. [infosec-reference](https://github.com/rmusser01/Infosec_Reference)\n19. [Web Security Testing Guide](https://github.com/OWASP/wstg)\n20. [Infection Monkey](https://github.com/guardicore/monkey)\n21. [awesome-web-security](https://github.com/qazbnm456/awesome-web-security)\n22. [awesome-hacking-resources](https://github.com/vitalysim/Awesome-Hacking-Resources)\n23. [h4cker](https://github.com/The-Art-of-Hacking/h4cker)\n24. [PayloadsAllTheThings](https://github.com/swisskyrepo/PayloadsAllTheThings)\n25. [threat-model-cookbook](https://github.com/OWASP/threat-model-cookbook)\n26. [Awesome Threat Modeling](https://github.com/hysnsec/awesome-threat-modelling)\n27. [awesome-hacking](https://github.com/Hack-with-Github/Awesome-Hacking)\n28. [penetration testing](https://github.com/wtsxDev/Penetration-Testing)\n29. [awesome-pentest](https://github.com/enaqx/awesome-pentest)\n30. [chaos-toolkit](https://github.com/chaostoolkit/chaostoolkit)\n31. [Python for Network Engineers](https://pyneng.readthedocs.io/en/latest/index.html#)\n32. [Chaos Engineering](https://github.com/dastergon/awesome-chaos-engineering)\n33. [awesome networking](https://github.com/facyber/awesome-networking)\n34. [Python for Network Engineers](https://pyneng.readthedocs.io/en/latest/index.html)\n35. [awesome-network-automation](https://github.com/networktocode/awesome-network-automation)\n\n</details>\n\n<details><summary><h2>Serverless Frameworks</h2></summary>\n\n1. [Architect](https://github.com/architect/architect)\n2. [Webiny-JS](https://github.com/webiny/webiny-js)\n3. [Midway](https://github.com/midwayjs/midway)\n4. [Amplify-JS](https://github.com/aws-amplify/amplify-js)\n5. [Serverless Express](https://github.com/vendia/serverless-express)\n6. [Claudia](https://github.com/claudiajs/claudia)\n7. [Apex](https://github.com/NVIDIA/apex)\n8. [Zappa](https://github.com/zappa/Zappa)\n9. [Serverless](https://github.com/serverless/serverless)\n10. [tomodachi](https://github.com/kalaspuff/tomodachi)\n\n</details>\n\n<details><summary><h2>Helpful Tools</h2></summary>\n\n1. [Boltons](https://github.com/mahmoud/boltons)\n2. [Docker-py](https://github.com/docker/docker-py)\n3. [more-itertools](https://github.com/more-itertools/more-itertools)\n4. [Kafka Python](https://github.com/dpkp/kafka-python)\n5. [ZODB](https://github.com/zopefoundation/ZODB)\n6. [Click](https://github.com/pallets/click)\n7. [DataConv](https://github.com/gwierzchowski/datconv)\n8. [DBT Core](https://github.com/dbt-labs/dbt-core)\n9. [State Transition Machines](https://github.com/pytransitions/transitions)\n10. [Storm](https://github.com/emre/storm)\n12. [Toolz](https://github.com/pytoolz/toolz)\n13. [Paramiko](https://github.com/paramiko/paramiko)\n14. [Textblob](https://github.com/sloria/TextBlob)\n15. [Jupyter: Docker-stacks](https://github.com/jupyter/docker-stacks)\n16. [cookie-cutter](https://github.com/cookiecutter/cookiecutter)\n17. [Transcriber](https://github.com/ewancook/transcriber)\n18. [Pytools](https://github.com/inducer/pytools)\n19. [Misskey](https://github.com/misskey-dev/misskey)\n20. [OpenMeta](https://github.com/open-metadata/OpenMetadata)\n21. [Chalice](https://github.com/aws/chalice)\n22. [Microservice Architecture: Serverless Compute Implementation](https://github.com/PacktPublishing/Implementing-Microservice-Architecture-using-Serverless-Computing-on-AWS)\n23. [Python-Lambda](https://github.com/nficano/python-lambda)\n24. [Pywren](https://github.com/pywren/pywren)\n25. [Zappa](https://github.com/zappa/Zappa)\n26. [Memray: Memory Profiling](https://github.com/bloomberg/memray)\n27. [Pybossa](https://github.com/bloomberg/pybossa)\n28. [Apache Samza](https://github.com/apache/samza)\n29. [filesystem_spec](https://github.com/fsspec/filesystem_spec)\n30. [google-i18n-address](https://github.com/mirumee/google-i18n-address)\n31. [docker-wsl](https://github.com/bowmanjd/docker-wsl)\n32. [aws-data-wrangler](https://github.com/awslabs/aws-data-wrangler)\n33. [Optimus](https://github.com/odpf/optimus)\n34. [metricflow](https://github.com/transform-data/metricflow)\n35. [lightdash](https://github.com/lightdash/lightdash)\n36. [chaos genius](https://github.com/chaos-genius/chaos_genius)\n37. [pyrsistent](https://github.com/tobgu/pyrsistent)\n38. [pydash](https://github.com/dgilland/pydash)\n39. [latexify_py](https://github.com/google/latexify_py)\n40. [rocketry](https://github.com/Miksus/rocketry)\n41. [pydatafaker](https://github.com/SamEdwardes/pydatafaker)\n42. [pydbgen](https://github.com/tirthajyoti/pydbgen)\n43. [faker](https://github.com/joke2k/faker)\n44. [RateLimiting](https://github.com/icecrime/RateLimiting)\n45. [DateTimeRange](https://github.com/thombashi/DateTimeRange)\n46. [tenacity](https://github.com/jd/tenacity)\n47. [mako](https://github.com/sqlalchemy/mako)\n48. [jinjasql](https://github.com/sripathikrishnan/jinjasql)\n49. [data engineering on gcp](https://github.com/Nunie123/data_engineering_on_gcp_book)\n50. [polars](https://github.com/pola-rs/polars)\n51. [Vaex](https://github.com/vaexio/vaex)\n52. [Fugure: Distributed Computation](https://github.com/fugue-project/fugue)\n53. [Funcy](https://github.com/Suor/funcy)\n54. [Singer](https://github.com/singer-io/getting-started)\n55. [Dateutil](https://github.com/dateutil/dateutil)\n56. [pyparsing](https://github.com/pyparsing/pyparsing)\n57. [psutil](https://github.com/giampaolo/psutil)\n58. [ray](https://github.com/ray-project/ray)\n59. [click](https://github.com/pallets/click)\n60. [flask-boilerplate](https://github.com/realpython/flask-boilerplate)\n61. [python-packager](https://github.com/fcurella/python-packager)\n62. [python-project-skeleton](https://github.com/joaomcteixeira/python-project-skeleton)\n63. [wemake-python-package](https://github.com/wemake-services/wemake-python-package)\n64. [pyscaffold](https://github.com/pyscaffold/pyscaffold)\n65. [xmltodict](https://github.com/martinblech/xmltodict)\n66. [duckdb](https://github.com/duckdb/duckdb)\n67. [dirty-equals](https://github.com/samuelcolvin/dirty-equals)\n\n</details>\n\n\n<details><summary><h2>Data Viz and BI</h2></summary>\n\n1. [Dash](https://github.com/plotly/dash) and [Sample Apps](https://github.com/plotly/dash-sample-apps)\n2. [Seaborn](https://github.com/mwaskom/seaborn)\n3. [Plotnine](https://github.com/has2k1/plotnine)\n4. [Bokeh](https://github.com/bokeh/bokeh)\n5. [Pygal](https://github.com/Kozea/pygal)\n6. [Geoplotlib](https://github.com/andrea-cuttone/geoplotlib)\n7. [Gleam](https://github.com/dgrtwo/gleam)\n8. [Missingno](https://github.com/ResidentMario/missingno)\n9. [Leather](https://github.com/wireservice/leather)\n10. [Altair](https://github.com/altair-viz/altair)\n11. [Folium](https://github.com/python-visualization/folium)\n12. [Plotly](https://github.com/plotly/plotly.py)\n13. [Pillow](https://github.com/python-pillow/Pillow)\n14. [Superset](https://github.com/apache/superset)\n15. [Glue Visualization](https://github.com/glue-viz/glue)\n16. [BIRT](https://github.com/eclipse/birt)\n17. [SpagoBI](https://github.com/SpagoBILabs/SpagoBI)\n18. [Seal-Report](https://github.com/ariacom/Seal-Report)\n19. [metabase](https://github.com/metabase/metabase)\n20. [Databox](https://github.com/databox/databox-python)\n21. [KNIME](https://github.com/knime/knime-python)\n22. [Datapane](https://github.com/datapane/datapane)\n23. [Perspective](https://github.com/finos/perspective)\n24. [redash](https://github.com/getredash/redash)\n25. [reportserver](https://github.com/infofabrik/reportserver)\n26. [awesome-business-intelligence](https://github.com/thenaturalist/awesome-business-intelligence)\n27. [Turnilo](https://github.com/allegro/turnilo)\n28. [SandDance](https://github.com/microsoft/SandDance)\n29. [Abixen Platform](https://github.com/abixen/abixen-platform)\n30. [d3](https://github.com/d3/d3)\n31. [Dash Examples](https://dash.gallery/Portal/)\n32. [sweetviz](https://github.com/fbdesignpro/sweetviz)\n33. [Awesome Web Viz Frameworks](https://github.com/olavtenbosch/awesome-web-visualization-frameworks)\n34. [Echarts](https://github.com/apache/echarts)\n35. [Grafana](https://github.com/grafana/grafana)\n36. [awesome-dataviz](https://github.com/javierluraschi/awesome-dataviz)\n37. [python-data-visualization](https://github.com/javedali99/python-data-visualization)\n38. [The-Python-Graph-Gallery](https://github.com/holtzy/The-Python-Graph-Gallery)\n39. [rustworkx](https://github.com/Qiskit/rustworkx)\n40. [solara](https://github.com/widgetti/solara)\n41. [pygwalker](https://github.com/Kanaries/pygwalker)\n42. [graphic-walker](https://github.com/Kanaries/graphic-walker)\n43. [datapane](https://github.com/datapane/datapane)\n44. [gleam](https://github.com/gleam-lang/gleam)\n45. [streamlit](https://github.com/streamlit/streamlit)\n46. [ipywidgets](https://github.com/jupyter-widgets/ipywidgets)\n47. [voila](https://github.com/voila-dashboards/voila)\n\n</details>\n\n<details><summary><h2>Databases and Parsing</h2></summary>\n\n- [dbeaver](https://github.com/dbeaver/dbeaver)\n- [awesome-db-tools](https://github.com/mgramin/awesome-db-tools)\n- [amazon-redshift-utils](https://github.com/awslabs/amazon-redshift-utils)\n- [amazon-redshift-developer-guides](https://github.com/awsdocs/amazon-redshift-developer-guide)\n- [awesome-time-series-database](https://github.com/xephonhq/awesome-time-series-database)\n\n1. [SQL Alchemy](https://github.com/sqlalchemy/sqlalchemy)\n2. [Pyodbc](https://github.com/mkleehammer/pyodbc)\n3. [PyMySQL](https://github.com/PyMySQL/PyMySQL)\n4. [Redash](https://github.com/getredash/redash)\n5. [SQLmap](https://github.com/sqlmapproject/sqlmap)\n6. [Pyodbc](https://github.com/mkleehammer/pyodbc)\n7. [ddlparse](https://github.com/shinichi-takii/ddlparse)\n8. [lacquer](https://github.com/slaclab/lacquer)\n9. [omymodels](https://github.com/xnuinside/omymodels)\n10. [sql-metadata](https://github.com/macbre/sql-metadata)\n11. [sqlglot](https://github.com/tobymao/sqlglot)\n12. [sqlparse](https://github.com/andialbrecht/sqlparse)\n13. [Sqlbucket](https://github.com/socialpoint-labs/sqlbucket)\n14. [DBFread](https://github.com/olemb/dbfread)\n15. [sqlalchemy-hana](https://github.com/SAP/sqlalchemy-hana)\n16. [pymssql](https://github.com/pymssql/pymssql)\n17. [sqleyes](https://github.com/leonardomathon/sqleyes)\n18. [data-diff](https://github.com/datafold/data-diff)\n19. [amazon-redshift-python-driver](https://github.com/aws/amazon-redshift-python-driver)\n20. [spyql](https://github.com/dcmoura/spyql)\n21. [awesome-sqlalchemy](https://github.com/dahlia/awesome-sqlalchemy)\n22. [ipython-sql](https://github.com/catherinedevlin/ipython-sql)\n23. [redshift-developer-guide](https://github.com/awsdocs/amazon-redshift-developer-guide)\n24. [cloud-sql-python-connector](https://github.com/GoogleCloudPlatform/cloud-sql-python-connector)\n25. [aiosql](https://github.com/nackjicholson/aiosql)\n26. [sqlfluff](https://github.com/sqlfluff/sqlfluff)\n27. [sqlmodel](https://github.com/tiangolo/sqlmodel)\n28. [pypika](https://github.com/kayak/pypika)\n29. [Amazon Redshift Utils](https://github.com/awslabs/amazon-redshift-utils)\n30. [connector-x](https://github.com/sfu-db/connector-x)\n31. [psycopg2](https://github.com/psycopg/psycopg2)\n32. [pg_simple](https://github.com/masroore/pg_simple)\n33. [databases](https://github.com/encode/databases)\n34. [sqlmesh](https://github.com/TobikoData/sqlmesh)\n35. [DBUtils](https://github.com/WebwareForPython/DBUtils)\n36. [pymysql-pool](https://github.com/jkklee/pymysql-pool)\n37. [django-db-connection-pool](https://github.com/altairbow/django-db-connection-pool)\n38. [Vanna](https://github.com/vanna-ai/vanna)\n39. [malloy](https://github.com/malloydata/malloy)\n40. [Apache ORC](https://github.com/apache/orc)\n41. [Apache Pinot](https://github.com/apache/pinot)\n42. [pdfplumber](https://github.com/jsvine/pdfplumber)\n43. [camelot](https://github.com/camelot-dev/camelot)\n44. [ingestr](https://github.com/bruin-data/ingestr)\n\n</details>\n\n<details><summary><h2>API and Web Framework</h2></summary>\n\n1. [Flask](https://github.com/pallets/flask)\n2. [Tornado](https://github.com/tornadoweb/tornado)\n3. [Tenacity](https://github.com/jd/tenacity)\n4. [Eve](https://github.com/pyeve/eve)\n5. [Flask Restful](https://github.com/flask-restful/flask-restful)\n6. [Google API Client](https://github.com/googleapis/google-api-python-client)\n7. [Zeep](https://github.com/mvantellingen/python-zeep)\n8. [Connexion](https://github.com/spec-first/connexion)\n9. [Hug](https://github.com/hugapi/hug)\n10. [Falcon](https://github.com/falconry/falcon)\n11. [Aiohttp](https://github.com/aio-libs/aiohttp)\n12. [FastAPI](https://github.com/tiangolo/fastapi)\n13. [OpenAPI Python Client](https://github.com/openapi-generators/openapi-python-client)\n14. [requests-toolbelt](https://github.com/requests/toolbelt)\n15. [smart_open](https://github.com/RaRe-Technologies/smart_open)\n16. [Wikipedia API](https://github.com/goldsmith/Wikipedia) and [Wrapper](https://github.com/lehinevych/MediaWikiAPI)\n17. [Office365-Rest-Python-Client](https://github.com/vgrem/Office365-REST-Python-Client)\n18. [youtube-dl](https://github.com/ytdl-org/youtube-dl)\n19. [Twisted](https://github.com/twisted/twisted)\n20. [simple-salesforce](https://github.com/simple-salesforce/simple-salesforce)\n21. [Venmo API](https://github.com/mmohades/Venmo)\n23. [Flask](https://github.com/realpython/discover-flask)\n24. [Django](https://github.com/django/django)\n25. [Coursera Downloader](https://github.com/coursera-dl/coursera-dl)\n26. [Public APIs](https://github.com/public-apis/public-apis)\n27. [python-oauth2](https://github.com/joestump/python-oauth2)\n28. [requests-oauth2](https://github.com/maraujop/requests-oauth2)\n29. [redo](https://github.com/mozilla-releng/redo)\n30. [backoff](https://github.com/litl/backoff)\n31. [Directus Data Stack](https://github.com/directus/directus)\n32. [StreamLit](https://github.com/streamlit/streamlit)\n33. [Pybossa](https://github.com/Scifabric/pybossa)\n34. [starlette](https://github.com/encode/starlette)\n35. [awesome-fastapi](https://github.com/mjhea0/awesome-fastapi)\n36. [awesome-fastapi-projects](https://github.com/Kludex/awesome-fastapi-projects)\n\n</details>\n\n<details><summary><h2>Async & Multitasking & Distributed</h2></summary>\n\n1. [Requests-futures](https://github.com/ross/requests-futures)\n2. [Requests-threads](https://github.com/requests/requests-threads)\n3. [grequests](https://github.com/spyoungtech/grequests)\n4. [async_generator](https://github.com/python-trio/async_generator)\n5. [httpx](https://github.com/encode/httpx)\n6. [requests-async](https://github.com/encode/requests-async)\n7. [mpire](https://github.com/Slimmer-AI/mpire)\n8. [offspring](https://github.com/borgstrom/offspring)\n9. [multiprocessing_on_dill](https://github.com/sixty-north/multiprocessing_on_dill)\n10. [continuous threading](https://github.com/justengel/continuous_threading)\n11. [Needle](https://github.com/BitTheByte/Needle)\n12. [atasker](https://github.com/alttch/atasker)\n13. [asgiref](https://github.com/django/asgiref)\n14. \n16. [concurrency-in-python-with-asyncio](https://github.com/concurrency-in-python-with-asyncio/concurrency-in-python-with-asyncio)\n17. [Async & Multitasking](https://github.com/timofurrer/awesome-asyncio)\n18. [fast_map](https://github.com/michalmonday/fast_map)\n19. [aiobotocore](https://github.com/aio-libs/aiobotocore)\n20. [aioboto3](https://github.com/terrycain/aioboto3)\n21. [aiohttp-client-cache](https://github.com/requests-cache/aiohttp-client-cache)\n22. [aiohttp](https://github.com/aio-libs/aiohttp)\n23. [multiprocess](https://github.com/uqfoundation/multiprocess)\n24. [aiofiles](https://github.com/Tinche/aiofiles)\n25. [aiobotocore](https://github.com/aio-libs/aiobotocore)\n26. [aioboto3](https://github.com/terrycain/aioboto3)\n\n</details>\n\n<details><summary><h2>Data Flow, Processing, Pipelines</h2></summary>\n\n1. [dataflow](https://github.com/tensorpack/dataflow)\n2. [pyfi](https://github.com/radiantone/pyfi)\n3. [dataflowkit](https://github.com/IcarusSO/dataflowkit)\n4. [data flow graph](https://github.com/macbre/data-flow-graph)\n5. [python flow](https://github.com/spotify/pythonflow)\n6. [gerda dataflow](https://github.com/mppmu/gerda-dataflow)\n7. [dataflows](https://github.com/datahq/dataflows)\n8. [d6tflow](https://github.com/d6t/d6tflow)\n9. [prefect](https://github.com/PrefectHQ/prefect)\n10. [Schedule](https://github.com/dbader/schedule)\n11. [Luigi](https://github.com/spotify/luigi)\n12. [Faust](https://github.com/robinhood/faust)\n13. [Redis Queue](https://github.com/rq/rq)\n14. [Airflow-Great-Expectations](https://github.com/great-expectations/airflow-provider-great-expectations)\n15. [Smart Open](https://github.com/RaRe-Technologies/smart_open)\n16. [Zipstream](https://github.com/kbbdy/zipstream)\n17. [multi-part-upload](https://github.com/keithrozario/multi-part-upload)\n18. [Celery](https://github.com/celery/celery)\n19. [airflow](https://github.com/apache/airflow)\n20. [sftp-lambda](https://github.com/lqueryvg/sftp-lambda)\n21. [lambda-s3-ftp](https://github.com/renanivo/lambda-s3-sftp)\n22. [Apache Beam](https://github.com/apache/beam)\n23. [Processing (I/O and Piplines)](https://github.com/pditommaso/awesome-pipeline)\n24. [stream unzip](https://github.com/uktrade/stream-unzip)\n25. [pypyr](https://github.com/pypyr/pypyr)\n26. [Data Flow Ops](https://github.com/anna-geller/dataflow-ops)\n27. [Apache Spark Guide](https://github.com/mikeroyal/Apache-Spark-Guide)\n28. [Orchest](https://github.com/orchest/orchest)\n29. [Mage AI](https://github.com/mage-ai/mage-ai)\n30. [Meltano](https://github.com/meltano/meltano)\n31. [DataJoint Python](https://github.com/datajoint/datajoint-python)\n32. [Hamilton](https://github.com/stitchfix/hamilton)\n33. [Kombu](https://github.com/celery/kombu)\n34. [airbyte](https://github.com/airbytehq/airbyte)\n35. [ploomber](https://github.com/ploomber/ploomber)\n36. [data-diff](https://github.com/datafold/data-diff)\n37. [Amazon Apache Airflow Managed Workflow](https://github.com/aws-samples/amazon-mwaa-examples)\n38. [Airbyte](https://github.com/airbytehq/airbyte)\n39. [mage-ai](https://github.com/mage-ai/mage-ai)\n40. [Dagster](https://github.com/dagster-io/dagster)\n41. [Data All](https://github.com/awslabs/aws-dataall)\n42. [awesome-flink](https://github.com/wuchong/awesome-flink) and [Examples](https://github.com/streaming-with-flink/examples-scala)\n43. [flink](https://github.com/apache/flink)\n44. [RedPanda](https://github.com/redpanda-data/redpanda)\n45. [Materialize](https://github.com/MaterializeInc/materialize)\n46. [Hazelcast](https://github.com/hazelcast/hazelcast)\n47. [Watermill](https://github.com/ThreeDotsLabs/watermill)\n48. [Amazon Kinesis Client Python](https://github.com/awslabs/amazon-kinesis-client-python)\n49. [Faust](https://github.com/robinhood/faust)\n50. [Stream Processing](https://github.com/raycad/stream-processing)\n51. [Spark Streaming in Python](https://github.com/LearningJournal/Spark-Streaming-In-Python)\n52. [Kestra](https://github.com/kestra-io/kestra)\n53. [Hamilton](https://github.com/DAGWorks-Inc/hamilton)\n54. [CloudQuery](https://github.com/cloudquery/cloudquery)\n55. [Nifi](https://github.com/apache/nifi)\n56. [Pentaho-Kettle](https://github.com/pentaho/pentaho-kettle)\n57. [Camel](https://github.com/apache/camel)\n58. [Riko](https://github.com/nerevu/riko)\n59. [Bonobo](https://github.com/python-bonobo/bonobo)\n60. [Petl](https://github.com/petl-developers/petl)\n61. [awesome-apache-airflow](https://github.com/jghoman/awesome-apache-airflow)\n62. [airflow provider sample](https://github.com/astronomer/airflow-provider-sample)\n63. [metabase](https://github.com/metabase/metabase)\n64. [Flowman](https://github.com/dimajix/flowman)\n65. [Apache Beam](https://github.com/apache/beam)\n66. [hamilton](https://github.com/DAGWorks-Inc/hamilton)\n67. [pachyderm](https://github.com/pachyderm/pachyderm)\n68. [elementary](https://github.com/elementary-data/elementary)\n\n</details>\n\n<details><summary><h2>Big Data and Cloud API's</h2></summary>\n\n- [Spark SQL Programming Guide](https://spark.apache.org/docs/latest/sql-programming-guide.html)\n\n1. [AWS Encryption SDK](https://github.com/aws/aws-encryption-sdk-python)\n2. [AWS Xray SDK](https://github.com/aws/aws-xray-sdk-python)\n3. [AWS SDK Pandas](https://github.com/aws/aws-sdk-pandas)\n4. [Sagemaker SDK](https://github.com/aws/sagemaker-python-sdk)\n5. [GCP Data Validator](https://github.com/GoogleCloudPlatform/professional-services-data-validator)\n6. [AWS Redshift Driver](https://github.com/aws/amazon-redshift-python-driver)\n7. [Cloudwatch Logging](https://github.com/kislyuk/watchtower)\n8. [Former2](https://github.com/iann0036/former2)\n9. [Sagemaker Spark](https://github.com/aws/sagemaker-spark)\n10. [Secrets Manager Caching](https://github.com/aws/aws-secretsmanager-caching-python)\n11. [Spark With Python](https://github.com/tirthajyoti/Spark-with-Python)\n12. [Learning Pyspark](https://github.com/PacktPublishing/Learning-PySpark)\n13. [Spark Redshift](https://github.com/databricks/spark-redshift)\n14. [Sagemaker Graph ER](https://github.com/awslabs/sagemaker-graph-entity-resolution)\n15. [aws-glue-developer-guide](https://github.com/awsdocs/aws-glue-developer-guide)\n16. [pyspark examples](https://github.com/spark-examples/pyspark-examples)\n17. [pyspark cheatsheet](https://github.com/kevinschaich/pyspark-cheatsheet)\n18. [aws-scheduler](https://github.com/bahrmichael/aws-scheduler)\n19. [emr-serverless-samples](https://github.com/aws-samples/emr-serverless-samples)\n20. [Polars](https://www.pola.rs/)\n21. [duckDB](https://duckdb.org/)\n22. [Dask](https://github.com/dask/dask)\n23. [SparkSQL](https://spark.apache.org/docs/2.2.0/sql-programming-guide.html)\n24. [cloud-experiments](https://github.com/aws-samples/cloud-experiments)\n25. [document-understanding-solution](https://github.com/aws-solutions/document-understanding-solution)\n26. [aws-glue-docker](https://github.com/webysther/aws-glue-docker)\n27. [amazon-comprehend-examples](https://github.com/aws-samples/amazon-comprehend-examples)\n28. [Festin](https://github.com/cr0hn/festin)\n29. [MinIO-py](https://github.com/minio/minio-py)\n30. [Bucketstore](https://github.com/jpetrucciani/bucketstore)\n31. [amazon-redshift-udfs](https://github.com/aws-samples/amazon-redshift-udfs)\n32. [PyLazyS3](https://github.com/Den1al/PyLazyS3)\n33. [Minio](https://github.com/minio/minio)\n\n</details>\n\n<details><summary><h2>Data Quality & Profiling & Business Rules</h2></summary>\n\n1. [Pandas Profiling](https://github.com/ydataai/pandas-profiling)\n2. [WhyLogs](https://github.com/whylabs/whylogs)\n3. [PointBlank](https://github.com/rich-iannone/pointblank)\n4. [Hooqu](https://github.com/mfcabrera/hooqu)\n5. [pyDMNrules](https://github.com/russellmcdonell/pyDMNrules)\n6. [DQ-Meerkat](https://github.com/lisehr/dq-meerkat)\n7. [dataqtor](https://github.com/baligoyem/dataqtor)\n8. [DataGristle](https://github.com/kenfar/DataGristle)\n9. [versatile-data-kit](https://github.com/vmware/versatile-data-kit)\n10. [Soda-Core](https://github.com/sodadata/soda-core)\n11. [ydata-quality](https://github.com/ydataai/ydata-quality)\n12. [Pydqc](https://github.com/SauceCat/pydqc)\n13. [Business Rule Engine](https://github.com/manfred-kaiser/business-rule-engine)\n14. [Python Business Logic](https://github.com/Valian/python-business-logic)\n15. [Business Rules](https://github.com/venmo/business-rules)\n16. [Hay_checker](https://github.com/fruttasecca/hay_checker)\n17. [Great Expectations](https://github.com/great-expectations/great_expectations)\n18. [Feast](https://github.com/feast-dev/feast)\n19. [Datatile](https://github.com/polyaxon/datatile)\n20. [business-rules venmo](https://github.com/venmo/business-rules)\n21. [pydqc](https://github.com/SauceCat/pydqc)\n22. [Data Gristle](https://github.com/kenfar/DataGristle)\n23. [deep diff](https://github.com/seperman/deepdiff)\n24. [Great Expectations](https://github.com/great-expectations/great_expectations)\n25. [XML to Dict](https://github.com/martinblech/xmltodict)\n26. [Pylint](https://github.com/PyCQA/pylint)\n27. [postal-address](https://github.com/scaleway/postal-address)\n28. [python-email-validator](https://github.com/JoshData/python-email-validator)\n29. [flatten-dict](https://github.com/ianlini/flatten-dict)\n30. [pytesseract](https://github.com/madmaze/pytesseract)\n31. [python deequ](https://github.com/awslabs/python-deequ)\n32. [ydata-profiling](https://github.com/ydataai/ydata-profiling)\n33. [Memphis](https://github.com/memphisdev/memphis)\n34. [Benthos](https://github.com/benthosdev/benthos)\n35. [Awesome Streaming](https://github.com/manuzhang/awesome-streaming)\n36. [Storm](https://github.com/apache/storm)\n\n</details>\n\n<details><summary><h2>Data lineage & Discovery & Observability</h2></summary>\n\n1. [bigquery-data-lineage](https://github.com/marcin-kolda/bigquery-data-lineage)\n2. [multi-data-lineage-capture-py](https://github.com/IBM/multi-data-lineage-capture-py)\n3. [DataTracer](https://github.com/data-dev/DataTracer)\n4. [data-lineage](https://github.com/tokern/data-lineage)\n5. [elementary](https://github.com/elementary-data/elementary)\n6. [stairlight](https://github.com/tosh2230/stairlight)\n7. [OpenLineage](https://github.com/OpenLineage/OpenLineage)\n8. [Marquez](https://github.com/MarquezProject/marquez)\n9. [Odd-Platform](https://github.com/opendatadiscovery/odd-platform)\n10. [waltz](https://github.com/finos/waltz)\n11. [sqllineage](https://github.com/reata/sqllineage)\n12. [Spline](https://github.com/AbsaOSS/spline)\n13. [grafana](https://github.com/grafana/grafana)\n14. [awesome observability](https://github.com/adriannovegil/awesome-observability)\n15. [Signoz](https://github.com/SigNoz/signoz)\n16. [zipkin](https://github.com/openzipkin/zipkin)\n17. [kibana](https://github.com/elastic/kibana)\n18. [vector](https://github.com/vectordotdev/vector)\n19. [netdata](https://github.com/netdata/netdata)\n20. [odd platform](https://github.com/opendatadiscovery/odd-platform)\n21. [Data Observability in Practice](https://github.com/monte-carlo-data/data-observability-in-practice)\n22. [Monosi](https://github.com/monosidev/monosi)\n23. [Swiple](https://github.com/Swiple/swiple)\n24. [Elementary](https://github.com/elementary-data/elementary)\n25. [awesome-opentelemetry](https://github.com/magsther/awesome-opentelemetry)\n26. [dd-trace-py](https://github.com/DataDog/dd-trace-py)\n27. [signoz](https://github.com/SigNoz/signoz)\n28. [vector](https://github.com/vectordotdev/vector)\n29. [netdata](https://github.com/netdata/netdata)\n30. [awesome-observability](https://github.com/adriannovegil/awesome-observability)\n31. [malloy](https://github.com/malloydata/malloy)\n\n</details>\n\n<details><summary><h2>Data Schemas & Parsing & Scraping</h2></summary>\n\n1. [Json Classes](https://github.com/fillmula/jsonclasses)\n2. [Schema](https://github.com/keleshev/schema)\n3. [Jmespath](https://github.com/jmespath/jmespath.py)\n4. [XmlUtils](https://github.com/knadh/xmlutils.py)\n5. [ExtraTools](https://github.com/chuanconggao/extratools)\n6. [Collections Extended](https://github.com/mlenzen/collections-extended)\n7. [More Itertools](https://github.com/more-itertools/more-itertools)\n8. [Lark Parser](https://github.com/lark-parser/lark)\n9. [Json Flattener](https://github.com/westandskif/convtools)\n10. [Scrapy](https://github.com/scrapy/scrapy)\n11. [Beautiful Soup](https://github.com/waylan/beautifulsoup)\n12. [marshmallow](https://github.com/marshmallow-code/marshmallow)\n13. [PyPDF2](https://github.com/mstamy2/PyPDF2)\n14. [Pydantic](https://github.com/samuelcolvin/pydantic)\n15. [Pyspider](https://github.com/binux/pyspider)\n16. [Pydantic SQLAlchemy](https://github.com/tiangolo/pydantic-sqlalchemy)\n17. [simplejson](https://github.com/simplejson/simplejson)\n18. [json2parquet](https://github.com/andrewgross/json2parquet)\n19. [pyyaml](https://github.com/yaml/pyyaml)\n20. [Attrs](https://github.com/python-attrs/attrs)\n21. [Chardet](https://github.com/chardet/chardet)\n22. [simple-enum](https://github.com/andrewcooke/simple-enum)\n23. [dataklasses](https://github.com/dabeaz/dataklasses)\n24. [dataclasses-json](https://github.com/lidatong/dataclasses-json)\n25. [dataclassy](https://github.com/biqqles/dataclassy)\n26. [python-choicesenum](https://github.com/loggi/python-choicesenum)\n27. [fastenum](https://github.com/QratorLabs/fastenum)\n28. [bnum](https://github.com/andrewcooke/bnum)\n29. [data-enum](https://github.com/chasefinch/data-enum)\n30. [superstring.py](https://github.com/btwael/superstring.py)\n31. [text2text](https://github.com/artitw/text2text)\n32. [pyspellchecker](https://github.com/barrust/pyspellchecker)\n33. [symspellpy](https://github.com/mammothb/symspellpy)\n34. [python string similarity](https://github.com/luozhouyang/python-string-similarity)\n35. [textdistance](https://github.com/life4/textdistance)\n36. [string-algorithms](https://github.com/krzysztof-turowski/string-algorithms)\n37. [python-phonenumbers](https://github.com/daviddrysdale/python-phonenumbers)\n38. [CommonRegex](https://github.com/madisonmay/CommonRegex)\n39. [Addresser](https://github.com/consbio/addresser)\n40. [Unidecode](https://github.com/avian2/unidecode)\n41. [whoosh](https://github.com/mchaput/whoosh)\n42. [usaddress](https://github.com/datamade/usaddress)\n43. [jellyfish](https://github.com/jamesturk/jellyfish)\n44. [Postal Address](https://github.com/scaleway/postal-address)\n45. [dirtyJson](https://github.com/codecobblers/dirtyjson)\n46. [awesome-json](https://github.com/burningtree/awesome-json)\n47. [dataclass_array](https://github.com/google-research/dataclass_array)\n48. [python-graphs](https://github.com/google-research/python-graphs)\n49. [python-email-validator](https://github.com/JoshData/python-email-validator)\n50. [dataprep](https://github.com/sfu-db/dataprep)\n51. [fuzzywuzzy](https://github.com/seatgeek/fuzzywuzzy)\n52. [Cerberus](https://github.com/pyeve/cerberus)\n53. [PolyFuzz](https://github.com/MaartenGr/PolyFuzz)\n54. [Fuzzy Search](https://github.com/taleinat/fuzzysearch)\n55. [Pachyderm](https://github.com/pachyderm/pachyderm)\n56. [CleanLab](https://github.com/cleanlab/cleanlab)\n57. [awesome-jsonschema](https://github.com/sourcemeta/awesome-jsonschema)\n58. [dateparser](https://github.com/scrapinghub/dateparser)\n59. [dateutil](https://github.com/dateutil/dateutil)\n60. [Cerebrus](https://github.com/pyeve/cerberus)\n61. [validators](https://github.com/python-validators/validators)\n62. [Valideer](https://github.com/podio/valideer)\n63. [Pandera](https://github.com/unionai-oss/pandera)\n64. [Typical](https://github.com/seandstewart/typical)\n65. [kdatapackage](https://github.com/frictionlessdata/datapackage-py)\n66. [PandasSchema](https://github.com/multimeric/PandasSchema)\n67. [TypedFrame](https://github.com/areshytko/typedframe)\n68. [tableschema](https://github.com/frictionlessdata/tableschema-py)\n69. [validator-collection](https://github.com/insightindustry/validator-collection)\n70. [deepchecks](https://github.com/deepchecks/deepchecks)\n71. [awesome-validation-python](https://github.com/mahmoudimus/awesome-validation-python)\n72. [attrs-strict](https://github.com/bloomberg/attrs-strict)\n73. [pydantic-core](https://github.com/pydantic/pydantic-core)\n74. [dataclass-type-validator](https://github.com/levii/dataclass-type-validator)\n75. [Jinja](https://github.com/pallets/jinja)\n76. [liaison](https://github.com/Julian-Nash/liaison)\n77. [DataCleaner](https://github.com/datacleaner/DataCleaner)\n\n</details>\n\n\n<details><summary><h2>Excel Tools</h2></summary>\n\n1. [xltable](https://github.com/fkarb/xltable)\n2. [xlwings](https://github.com/xlwings/xlwings)\n3. [xlsxWriter](https://github.com/jmcnamara/XlsxWriter)\n4. [openpyxl](https://github.com/theorchard/openpyxl)\n5. [formulas](https://github.com/vinci1it2000/formulas)\n6. [pycel](https://github.com/dgorissen/pycel)\n7. [pyexcel](https://github.com/pyexcel/pyexcel)\n8. [xlwt](https://github.com/python-excel/xlwt)\n9. [pyxll](https://github.com/pyxll/pyxll-examples)\n10. [xlrd](https://github.com/python-excel/xlrd)\n11. [xlsx2csv](https://github.com/dilshod/xlsx2csv)\n12. [libre office core](https://github.com/LibreOffice/core)\n13. [pywin32](https://github.com/mhammond/pywin32)\n14. [pyxlsb2](https://github.com/DissectMalware/pyxlsb2)\n15. [pyxlsb](https://github.com/willtrnr/pyxlsb)\n\n</details>\n\n<details><summary><h2>Cache and Hash</h2></summary>\n\n1. [cachecontrol](https://github.com/ionrock/cachecontrol)\n2. [Cached Property](https://github.com/pydanny/cached-property)\n3. [cacheout](https://github.com/dgilland/cacheout)\n4. [cachetools](https://github.com/tkem/cachetools)\n5. [Johnny Cache](https://github.com/jmoiron/johnny-cache)\n6. [Requests Cache](https://github.com/reclosedev/requests-cache)\n7. [expiringdict](https://github.com/mailgun/expiringdict)\n8. [lru_cache_http_client](https://github.com/brighton1101/lru_cache_http_client)\n9. [Data Cache](https://github.com/statnett/data_cache)\n10. [Python Cache](https://github.com/jneen/python-cache)\n11. [LRU Cache](https://realpython.com/lru-cache-python/)\n12. [perfect-hash](https://github.com/ilanschnell/perfect-hash)\n13. [hash framework](https://github.com/cipherboy/hash_framework)\n14. [SHA3 Implementation](https://github.com/TheLeopardsH/SHA3-Python-3-implementation)\n15. [Random Hash](https://github.com/jlumbroso/python-random-hash)\n16. [python hashes](https://github.com/sean-public/python-hashes)\n17. [SHA-256 Algorithm](https://github.com/Yathu2007/SHA-256-Python-Implementation)\n18. [Bloom Filter](https://github.com/garawalid/LH-BloomFilter)\n19. [EnroCrypt](https://github.com/Morgan-Phoenix/EnroCrypt)\n20. [pymorton](https://github.com/trevorprater/pymorton)\n21. [shortuuid](https://github.com/skorokithakis/shortuuid)\n22. [human-readable-id](https://github.com/hnimminh/human-readable-id)\n\n</details>\n\n<details><summary><h2>Logging and Testing and Monitoring</h2></summary>\n\n1. [structlog](https://github.com/hynek/structlog)\n2. [Local Stack](https://github.com/localstack/localstack)\n3. [Hypothesis](https://github.com/HypothesisWorks/hypothesis)\n4. [Loguru](https://github.com/Delgan/loguru)\n5. [PySnooper](https://github.com/cool-RR/PySnooper)\n6. [PyGoGo](https://github.com/reubano/pygogo)\n7. [minilog](https://github.com/jacebrowning/minilog)\n8. [fluent logger](https://github.com/fluent/fluent-logger-python)\n9. [daiquiri](https://github.com/Mergifyio/daiquiri)\n10. [Locust](https://github.com/locustio/locust)\n11. [Soda Core](https://github.com/sodadata/soda-core)\n12. [WhyLogs](https://github.com/whylabs/whylogs)\n13. [Logstash](https://github.com/elastic/logstash)\n14. [Hypothesis](https://github.com/HypothesisWorks/hypothesis)\n15. [awesome-pytest](https://github.com/augustogoulart/awesome-pytest)\n16. [spylunking](https://github.com/jay-johnson/spylunking)\n17. [splunk-sdk-python](https://github.com/splunk/splunk-sdk-python)\n18. [opentelemetry-python](https://github.com/open-telemetry/opentelemetry-python)\n19. [Aws Open Telemtry](https://aws-otel.github.io/)\n20. [opentelemetry.io](https://opentelemetry.io/)\n21. [refurb](https://github.com/dosisod/refurb)\n22. [Factory Boy](https://github.com/FactoryBoy/factory_boy)\n\n</details>\n\n<details><summary><h2>Webscraping</h2></summary>\n\n1. [Mechanical Soup](https://github.com/MechanicalSoup/MechanicalSoup)\n2. [tiktok-downloader](https://github.com/krypton-byte/tiktok-downloader)\n3. [youtube-dl](https://github.com/ytdl-org/youtube-dl)\n4. [BeautifulSoup4](https://github.com/wention/BeautifulSoup4)\n5. [Selenium](https://github.com/SeleniumHQ/selenium)\n6. [lxml](https://github.com/lxml/lxml)\n7. [portia](https://github.com/scrapinghub/portia)\n8. [scrapyd](https://github.com/scrapy/scrapyd)\n9. [scrapy](https://github.com/scrapy/scrapy)\n10. [puppeteer](https://github.com/puppeteer/puppeteer)\n11. [awesome-web-scraping](https://github.com/lorien/awesome-web-scraping)\n\n</details>\n"
    },
    {
      "name": "ptbdnr/snippets",
      "stars": 6,
      "img": "https://avatars.githubusercontent.com/u/32688266?s=40&v=4",
      "owner": "ptbdnr",
      "repo_name": "snippets",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2024-08-01T11:47:45Z",
      "updated_at": "2025-04-16T09:42:41Z",
      "topics": [],
      "readme": "# Snippets\n\nA collection of handy code snippets\n"
    },
    {
      "name": "avnlp/llm-blender",
      "stars": 6,
      "img": "https://avatars.githubusercontent.com/u/123800833?s=40&v=4",
      "owner": "avnlp",
      "repo_name": "llm-blender",
      "description": "Ensembling LLMs using LLM-Blender",
      "homepage": "",
      "language": "Python",
      "created_at": "2024-07-06T07:13:38Z",
      "updated_at": "2025-03-31T11:13:47Z",
      "topics": [
        "llm-blender",
        "rag",
        "rankers"
      ],
      "readme": "# LLM Ensembling: Pipelines with LLM-Blender\n\n**Paper:** [LLM Ensembling: Haystack Pipelines with LLM-Blender](paper/llm_blender.pdf)\n\n- LLM-Blender is an ensembling framework designed to achieve consistently superior performance by combining the outputs of multiple language models (LLMs). This work focuses on integrating LLM-Blender with Retrieval-Augmented Generation (RAG) pipelines to significantly improve the quality of generated text.\n\n- LLM-Blender is a two-stage ensemble learning framework. In the first stage (ranking), pairwise comparison of candidates is performed, and they are then ranked. In the second stage (fusing), the top K candidates are merged to render the final output.\n\n- The LLM-Blender comprises of two modules: the PairRanker and the GenFuser. The PairRanker module compares the outputs from multiple LLMs to provide the top-ranked outputs. It compares each candidate with the input in a pairwise manner, making it robust to subtle differences in the generated text. The GenFuser module uses the top-ranked outputs from the PairRanker module to generate an improved output. The module fuses the top K of the N-ranked candidates from the PairRanker, conditioned on the input instruction, to generate an enhanced output.\n\n- A custom Haystack component, `LLMBlenderRanker`, has been implemented to integrate LLM-Blender with Haystack pipelines. The component utilizes the `PairRanker` module from the LLM-Blender framework, which compares each candidate with the input in a pairwise manner. Different LLMs can generate subtly different texts, since they are trained on different datasets and tasks. By comparing each text in a pairwise manner, the component ranks and ensembles the text so it is robust to these subtle differences.\n\n- Haystack RAG Pipelines with the LLM-Blender component to ensemble LLMs were evaluated. The pipelines were evaluated on the BillSum and MixInstruct datasets using three metrics: BARTScore, BLEURT, and BERTScore. The `llama-3`, `phi-3`, `mistral-7b`, `openchat-3.5`, `starling-lm-7b-alpha` and `openhermes-2.5` LLMs were used in the ensemble.\n\n## PairRanker\n\n- The PairRanker module is responsible for comparing and ranking the outputs from LLM's. During the ranking stage, a specific input prompt (x) is passed to N different LLMs, and their outputs are compiled as candidates ($y_1$, …, $y_N$).\n\n- The PairRanker then analyzes and ranks these candidates. For each input x, the candidates are obtained from N different LLMs. This input sequence, along with the candidates, is then subjected to a cross-attention text encoder, such as RoBERTa. The text encoder is tasked with learning and determining the superior candidate for the given input x.\n\n- All the candidates are paired ($y_i$ and $y_j$), producing a matrix of pairwise comparison results. These pairs are evaluated based on the condition: given the input prompt, which candidate's output is better? By aggregating the results in the matrix, the PairRanker can rank all candidates and take the top K of them for generative fusion.\n\n<img src=\"plots/blender.png\" alt=\"RAG Pipelines Taxonomy\" align=\"middle\" height =250>\n\n## GenFuser\n\n- The primary goal of the GenFuser module is to capitalize on the strengths of the top K selected candidates from the PairRanker's ranking.\n\n- After the PairRanker module ranks the candidates, the GenFuser module is employed to fuse the top K out of the N ranked candidates and generate an improved final output. It takes a seq2seq approach, fusing the set of top candidates while conditioning on the input prompt, to generate an improved and enhanced output.\n\n## RAG Pipeline with the LLM Blender component\n\nThe results from the different LLMs on the MixInstruct dataset are ranked and combined using the LLM-Blender framework.\n\n<br>\n<img src=\"plots/ranker_pipeline_single_llm.png\" alt=\"RAG Pipelines Taxonomy\" align=\"middle\" height =100>\n\n## Usage\n\nTo run the pipelines, you will need to clone this repository and install the required libraries.\nInstall the llm-blender package:\n\n```bash\ngit clone https://github.com/avnlp/llm_blender\ncd llm_blender\npip install -e .\n```\n\n## LLM-Blender using Mistral, LLama-3 and Phi-3 models on the MixInstruct Dataset\n\n``` python\ncd src/llm_blender/mix_instruct/\npython llm_blender_ranker_all_llms.py\n```\n\n## LLMBlenderRanker Component Usage\n\n```python\nllm_ranker = LLMBlenderRanker(model=\"llm-blender/PairRM\")\nanswers = [\n    GeneratedAnswer(data=\"Paris is the capital of France.\", query=\"What makes Paris unique?\", documents=[]),\n    GeneratedAnswer(\n        data=\"The Eiffel Tower is an iconic landmark in Paris.\", query=\"What makes Paris unique?\", documents=[]\n    ),\n    GeneratedAnswer(data=\"Berlin is a beautiful city.\", query=\"What makes Paris unique?\", documents=[]),\n]\noutput = llm_ranker.run(answers=answers)\nranked_answers = output[\"answers\"]\nprint(ranked_answers)\n\n# [\n#     GeneratedAnswer(\n#         data=\"The Eiffel Tower is an iconic landmark in Paris.\",\n#         query=\"What makes Paris unique?\",\n#         documents=[],\n#         meta={},\n#     ),\n#     GeneratedAnswer(\n#         data=\"Paris is the capital of France.\", query=\"What makes Paris unique?\", documents=[], meta={}\n#     ),\n#     GeneratedAnswer(data=\"Berlin is a beautiful city.\", query=\"What makes Paris unique?\", documents=[], meta={}),\n# ]\n```\n\nThe detailed documentation can be found in the [LLM-Blender API Reference](src/llm_blender/README.md).\n\nAs the [`llm-blender` library](https://github.com/yuchenlin/LLM-Blender) lacks a stable release, the necessary code has been incorporated into this project under `src/llm_blender/llm_blender_utils`.\n\n\n## Results\n\n- A custom component, `LLMBlenderPairRanker`, was developed to integrate the LLM-Blender Framework with Haystack Pipelines. Haystack RAG Pipelines with the LLM-Blender component to ensemble LLMs were evaluated. The pipelines were evaluated on the BillSum and MixInstruct datasets using three metrics: BARTScore, BLEURT, and BERTScore.\n\n- We successfully replicated the previously reported results for the LLM-Blender. Moreover, significantly improved performance was observed when utilizing newer LLM models, such as Llama-3-8B, Phi-3-mini and Mistral-7B. These findings demonstrate the potential of ensembling state-of-the-art LLMs to enhance the performance of RAG Pipelines on question-answering, summarization and instruction-following tasks.\n\n- The authors of LLM-Blender obtained BERTScore values in the range of 62.26 to 74.68 on the MixInstruct dataset. They obtained a BERTScore value of 72.97 with the PairRanker. We obtained BERTScore values in the range of 72.62 to 76.86 using the newer LLMs. We obtained a BERTScore value of 75.83 with the PairRanker ensembling the results from Llama-3-8B, Phi-3-mini and Mistral-7B.\n\n- The authors of LLM-Blender obtained BARTScore values in the range of -4.57 to -3.14 on the MixInstruct dataset. They obtained a BARTScore value of -3.14 with the PairRanker. We obtained BARTScore values in the range of -3.17 to -2.87 using the newer LLMs. We obtained a BARTScore value of -2.87 with the PairRanker ensembling the results from Llama-3-8B, Phi-3-mini and Mistral-7B.\n\n- The authors of LLM-Blender obtained BLEURT values in the range of -1.23 to -0.37 on the MixInstruct dataset. They obtained a BLEURT value of -0.37 with the PairRanker. We obtained BLEURT values in the range of -0.41 to -0.23 using the newer LLMs. We obtained a BLEURT value of -0.26 with the PairRanker ensembling the results from Llama-3-8B, Phi-3-mini and Mistral-7B.\n\n- The newer models like Llama-3-8B, Phi-3-mini, and Mistral-7B significantly outperformed all the models used by the LLM Blender authors on all the three metrics: BERTScore, BARTScore and BLEURT on the MixInstruct dataset.\n\n- On the BillSum dataset, we obtained BERTScore values from 73.91 to 75.43, BARTScore values from -3.49 to -3.19, and BLEURT values from -0.39 to -0.20 across the different LLMs. The PairRanker model, ensembling the outputs from Llama-3-8B, Phi-3-mini, and Mistral-7B, achieved the highest scores of 75.83 for BERTScore, -3.19 for BARTScore, and -0.20 for BLEURT.\n\n- For both the BillSum and MixInstruct datasets, the PairRanker model achieved the best performance when ensembling the outputs from Llama-3-8B, Phi-3-mini, and Mistral-7B. This combination of LLMs, ensembled using the LLM Blender, significantly outperformed each individual model's performance on all the evaluation metrics.\n\n## License\n\nThe source files are distributed under the [MIT License](https://github.com/avnlp/llm-blender/blob/main/LICENSE).\n"
    },
    {
      "name": "relari-ai/examples",
      "stars": 6,
      "img": "https://avatars.githubusercontent.com/u/135984758?s=40&v=4",
      "owner": "relari-ai",
      "repo_name": "examples",
      "description": "Examples for continuous-eval",
      "homepage": "https://docs.relari.ai/",
      "language": null,
      "created_at": "2024-02-07T05:25:40Z",
      "updated_at": "2025-01-03T23:55:54Z",
      "topics": [],
      "readme": "# continuous-eval examples\n\nThis repo contains end-to-end examples of GenAI/LLM applications and evaluation pipelines set up using continuous-eval.\n\nCheckout [continuous-eval repo](https://github.com/relari-ai/continuous-eval) and [documentation](https://docs.relari.ai/v0.3/) for more information.\n\n## Examples\n\n| Example Name               | App Framework | Eval Framework  | Description                                       |\n| -------------------------- | ------------- | --------------- | ------------------------------------------------- |\n| Simple RAG                 | Langchain     | continuous-eval | Simple QA chatbot over select Paul Graham essays  |\n| Complex RAG                | Langchain     | continuous-eval | Complex QA chatbot over select Paul Graham essays |\n| ReAct Agent                | LlamaIndex    | continuous-eval | QA over Uber financial dataset using agents       |\n| Sentiment Classification   | LlamaIndex    | continuous-eval | Single label classification of sentence sentiment |\n| Simple RAG                 | Haystack      | continuous-eval | Simple QA chatbot over select Paul Graham essays  |\n| Customer Support           | OpenAI Swarm  | continuous-eval | Customer support agent using tools                |\n\n## Installation\n\nIn order to run the examples, you need to have Python 3.11 (suggested) and Poetry installed. \nThen, clone this repo and install the dependencies:\n\n```bash\ngit clone https://github.com/relari-ai/examples.git && cd examples\npoetry use 3.11\npoetry install --with haystack --with langchain --with llama-index\n```\n\nNote that the `--with` flags are optional and only needed if you want to run the examples for the respective frameworks.\n\n## Get started\n\nEach example is in a subfolder: `examples/<FRAMEWORK>/<APP_NAME>/`.\n\nSome examples have just one script to execute (e.g. Haystack's Simple RAG), some have multiple:\n\n- `pipeline.py` defines the application pipeline and the evaluation metrics / tests.\n- `app.py` contains the LLM application. Run this script to get the outputs (saved as `results.jsonl`)\n- `eval.py` runs the metrics / tests defined by `pipeline.py` (saved as `metrics_results.json` and `test_results.json`)\n\n### Running the examples\n\nTo run the examples, you can use the following command:\n\n```bash\npoetry run python -m examples.<FRAMEWORK>.<APP_NAME>.app\n```\n\nfor example `poetry run python3 -m examples.swarm.customer_support.eval`.\n\nTo run the evaluation metrics and tests, use:\n\n```bash\npoetry run python3 -m examples.<FRAMEWORK>.<APP_NAME>.eval\n```\n\n### Additional notes\n\nDepending on the application, the source data for the application (documents and embeddings in Chroma vectorstore) and evaluation (golden dataset) is also provided. Note that for the evaluation golden dataset, there are always two files:\n\n- `dataset.jsonl` contains the inputs (questions) and reference module outputs (ground truths)\n- `manifest.yaml` defines the structure of the dataset for the evaluators.\n\nTweak metrics and tests in `pipeline.py` to try out different metrics.\n"
    },
    {
      "name": "GovML/retriever",
      "stars": 6,
      "img": "https://avatars.githubusercontent.com/u/167251131?s=40&v=4",
      "owner": "GovML",
      "repo_name": "retriever",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2024-04-16T20:41:41Z",
      "updated_at": "2025-01-27T03:14:57Z",
      "topics": [],
      "readme": "<div align=\"center\">\n\n# Retriever\n\n### Visually search and analyze your documents, entirely locally.\n\n<p>\n<img alt=\"GitHub Contributors\" src=\"https://img.shields.io/github/contributors/GovML/retriever\" />\n<img alt=\"GitHub Last Commit\" src=\"https://img.shields.io/github/last-commit/GovML/retriever\" />\n<img alt=\"GitHub Repo Size\" src=\"https://img.shields.io/github/repo-size/GovML/retriever\" />\n<img alt=\"GitHub Issues\" src=\"https://img.shields.io/github/issues/GovML/retriever\" />\n<img alt=\"GitHub Pull Requests\" src=\"https://img.shields.io/github/issues-pr/GovML/retriever\" />\n<img alt=\"Github License\" src=\"https://img.shields.io/badge/License-Apache-yellow.svg\" />\n</p>\n<img src=\"https://github.com/GovML/retriever/blob/main/retriever.gif\"/>\n</div>\n\n## Install \nWe recommended using a virtual environment for all dependency installations. Before installing our repo, you can use venv to isolate the various packages installed in this environment to prevent conflicts with versions already installed on your computer.\n\n```\n$ python -m venv new_env\n$ source new_env/bin/activate\n```\n\n\nOptions:\n1) Install with pip (Stable Release)\n```\n$ pip install retriever-search\n```\n2) Install from Github Repo (Latest Release)\n```\n$ git clone https://github.com/GovML/retriever.git\n$ pip install -e .\n```\n\n## Quickstart - Launching Retriever\nRetriever is composed of two parts that you'll need to launch.\n1) **Backend:** The backend server ingests and returns search results. This server is exposed locally via Flask.\n2) **Frontend:** The frontend is the user interface (UI) you use to input searches and visualize your results. The frontend sends requests to the backend server.\n\nFirst you'll need to ensure you have a folder of PDFs on your computer. If you don't have PDFs handy, we've provided a script under [tutorials](tutorials/download_examples.py) to download a few example papers from arXiv.\n\nOnce you have your folder of PDFs, you can start up the **backend** search server by opening up a terminal window and running:\n\n```\n>>> from retriever_search import search_server\n>>> search_server.run_search_server('./pdfs_folder/', json_save_path='save_results.json', device='cpu')\n```\n*If your computer has a CUDA compatible GPU, you can change* ```device='cuda'``` *or if you are on mac,* ```device='mps'```\n\nNext, open up a second terminal window and run the following:\n```\n>>> from retriever_search import frontend_app as fp\n>>> fp.run_frontend()\n```\nRetriever should be up and running! You can access the UI at the following port on your computer: http://127.0.0.1:7860.\nThis URL would work for your local setup only (paste the url into your browser which can render the UI, but you won't need an external internet connection to use it).\n\nNext time you run Retriever, you can call it directly on the json you just saved your pdfs to in order to save time!\n```search_server.run_search_server(input_json='save_results.json', device='cpu')```\n\n## Full Parameter Guide for Search server\n```\n>>> search_server.run_search_server('input_directory', 'input_json', 'json_save_path', 'embedding_model', 'qa_model', device='cpu')\n```\n#### Search parameter definitions\n- input_directory -- The directory holding your files, optional if input_json is passed instead\n- input_json -- pre saved json file from earlier runs can be used for faster loading, optional if input_directory is passed instead\n- json_save_path -- (optional) pass for saving the embeddings to a json can be used later as input_json\n- embedding_model -- (optional) pick the embedding model you want to we use Spectre model as a default\n- qa_model -- (optional) you can currently pick between tiny, medium and large\n- device -- (optional) can be set to cpu, mps or cuda\n\n## Tickets\n\n1.1.0\n- [ ] Make LDA visualization update\n- [ ] QA Model Improvements\n- [ ] Add support for HTML, txt\n\n1.0.2\n- [x] Add Quickstart\n\n"
    },
    {
      "name": "deepset-ai/haystack-rest-api",
      "stars": 6,
      "img": "https://avatars.githubusercontent.com/u/51827949?s=40&v=4",
      "owner": "deepset-ai",
      "repo_name": "haystack-rest-api",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2023-03-30T10:31:17Z",
      "updated_at": "2024-08-08T16:34:32Z",
      "topics": [],
      "readme": "> [!WARNING]\n> This repository contained examples to show how to serve Haystack pipelines behind a rest api.\n> Since then we introduced [Hayhooks](https://github.com/deepset-ai/hayhooks/) which offers this\n> functionality explicitly and comprehensively.\n\n# haystack-rest-api\n\nThis repository contains a simple Haystack RAG application with a REST API for indexing and querying purposes.\n\nThe application includes two containers:\n- An Elasticsearch container\n- A REST API container: built on FastAPI, this container integrates the Haystack logic and uses pipelines for indexing and querying. You can look at the [pipelines YAML files](./src/pipelines/) to see how the application is configured.\n\nYou can find more information in the [Haystack documentation](add a link when availaible).\n\n## Getting started\nBefore you begin, make sure you have Python and Docker installed on your system.\n\n- Clone this repository.\n- Set the `OPENAI_API_KEY` environment variable following the instructions [here](https://help.openai.com/en/articles/5112595-best-practices-for-api-key-safety#h_a1ab3ba7b2).\n- Spin up the multi-container application (Elasticsearch + REST API) using Docker Compose: \n    ```bash\n    docker-compose up -d\n    ```\n- Verify that the REST API is ready by running \n  ```bash\n  curl http://localhost:8000/ready\n  ```\n  You should get `true` as a response.\n- You can also check REST API interactive documentation at http://localhost:8000/docs.\n\n## Indexing\nTo populate the application with example data about Oceanian countries, run the following script:\n```bash\npython ingest_example_data.py\n```\n\nYou can also index your own text files using the `file-upload` endpoint:\n```bash\ncurl -X 'POST' \\\n'http://localhost:8000/file-upload?keep_files=false' \\\n-H 'accept: application/json' \\\n-H 'Content-Type: multipart/form-data' \\\n-F 'files=@YOUR-TEXT-FILE.txt;type=text/plain'\n```\n\n## Querying\nUse the query endpoint.\nFor example, to query the application with the question \"Who are Torres Strait Islanders?\", run:\n```bash\ncurl -X 'GET' \\\n  'http://127.0.0.1:8000/query?query=Who%20are%20Torres%20Strait%20Islanders%3F' \\\n  -H 'accept: application/json'\n```\n\nYou should get a response similar to the following:\n```json\n{\n  \"answer_builder\": {\n    \"answers\": [\n      {\n        \"data\": \"Torres Strait Islanders are ethnically Melanesian people who obtained their livelihood from seasonal horticulture and the resources of their reefs and seas.\",\n        \"query\": \"Who are Torres Strait Islanders?\",\n        \"metadata\": {\n          \"model\": \"gpt-3.5-turbo-0613\",\n          \"index\": 0,\n          \"finish_reason\": \"stop\",\n          \"usage\": {\n            \"prompt_tokens\": 727,\n            \"completion_tokens\": 29,\n            \"total_tokens\": 756\n          }\n        },\n        \"documents\": [\"...\"]\n        }\n    ]\n  }\n}\n```\n\n## Customization\nThis repository serves as a practical example of building your own Haystack application with a REST API.\nFor more information and guidance, refer to the [Haystack documentation](add a link when availaible).\n\n"
    },
    {
      "name": "silvanocerza/search-all-the-docs",
      "stars": 6,
      "img": "https://avatars.githubusercontent.com/u/3314350?s=40&v=4",
      "owner": "silvanocerza",
      "repo_name": "search-all-the-docs",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2023-10-16T21:05:50Z",
      "updated_at": "2025-02-16T13:26:21Z",
      "topics": [],
      "readme": "---\ntitle: SEARCH ALL THE DOCS\nemoji: 🔎\ncolorFrom: yellow\ncolorTo: pink\npython_version: 3.11\nsdk: streamlit\nsdk_version: 1.27.2\napp_file: main.py\npinned: false\n---\n\n![SEARCH ALL THE DOCS](meme.jpg)\n\n## Getting started\n\nFirst create your virtual env so you don't pollute your OS environment.\nThis demo has only been tested with Python 3.11, so I suggest you use that.\n\n```shell\nmkvirtualenv search-all-the-docs\nworkon search-all-the-docs\n```\n\nInstall the dependencies:\n\n```shell\npip install -r requirements.txt\n```\n\nCreate a `.env` file with your OpenAI key:\n\n```\nOPENAI_API_KEY=\"<your_key_here>\"\n```\n\nAnd you're good to go!\n\n```shell\nstreamlit run main.py\n```\n"
    },
    {
      "name": "TuanaCelik/milvus-documentation-qa",
      "stars": 5,
      "img": "https://avatars.githubusercontent.com/u/15802862?s=40&v=4",
      "owner": "TuanaCelik",
      "repo_name": "milvus-documentation-qa",
      "description": "An example pipeline with MilvusDocumentStore and Haystack",
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2023-08-07T23:38:46Z",
      "updated_at": "2024-10-03T20:32:55Z",
      "topics": [],
      "readme": "# Milvus Documentation Search\n\nThis repo includes en example application that makes use of a Retrieval Augmented Generative architecture built with [Haystack](https://haystack.deepset.ai) to do search on the Milvus documentation.\n\nhttps://github.com/TuanaCelik/milvus-documentation-qa/assets/15802862/d7573c21-f473-42d0-8d38-25302009c1aa\n\n## Install dependencies\n\n```bash\npip install -r requirements.txt\n```\n\n## The Indexing Pipeline\n\nAn indexing pipeline is used to write documents to a database. In this example, we use the `MilvusDocumentStore` as our database for the RAG pipeline. So, we need to write the Milvus documentation into our Milvus database. For demonstration purposes, we use the `Crawler` component to crawl everything under https://milvus.io/docs/\n\nOnce you have Milvus running locally on localhost:19530, you can use the indexing pipeline as follows:\n\n```bash\npython scripts/index_files.py\n```\n\n## The RAG Pipeline\n\nThe RAG pipeline that we use is the following:\n\n```python\nfrom haystack import Pipeline\nfrom haystack.nodes import EmbeddingRetriever, PromptNode, PromptTemplate, AnswerParser\nfrom milvus_haystack import MilvusDocumentStore\n\ndocument_store = MilvusDocumentStore()\n\nretriever = EmbeddingRetriever(document_store=document_store, embedding_model=\"sentence-transformers/multi-qa-mpnet-base-dot-v1\")\ntemplate = PromptTemplate(prompt=\"deepset/question-answering\", output_parser=AnswerParser())\nprompt_node = PromptNode(model_name_or_path=\"gpt-4\", default_prompt_template=template, api_key=OPENAI_API_KEY, max_length=500)\n\nquery_pipeline = Pipeline()\nquery_pipeline.add_node(component=retriever, name=\"Retriever\", inputs=[\"Query\"])\nquery_pipeline.add_node(component=prompt_node, name=\"PromptNode\", inputs=[\"Retriever\"])\n```\n\n### To run it as a Streamlit App\n```bash\nstreamlit run app.py\n```\n\n### To run it as a standalone script\n```bash\npython scripts/documentation_qa.py\n```\n"
    },
    {
      "name": "bobcastaldeli/QA_B3",
      "stars": 5,
      "img": "https://avatars.githubusercontent.com/u/32075635?s=40&v=4",
      "owner": "bobcastaldeli",
      "repo_name": "QA_B3",
      "description": "Question Answering B3 products",
      "homepage": null,
      "language": "Python",
      "created_at": "2023-03-21T19:34:44Z",
      "updated_at": "2023-04-27T00:22:54Z",
      "topics": [],
      "readme": "# B3 products and services QA chatbot\n==============================\n\n![B3 QA Products and Services Chatbot](reports/b3_qa_chatbot.png)\n\n\nThis is a project to create a QA chatbot to answer questions about B3 products and services.\n\n\n# 1 Pre-requisites\n\n**Ubuntu/Debian/Mint**\n\nThe GNU/Linux Make utility is required to run the Makefile. To install it, run the following command:\n\n```bash\nsudo apt-get install build-essential\n```\n\nAlso, the Python 3 interpreter is required to run the project. To install it, run the following command:\n\n```bash\nsudo apt-get install python3\n```\n\n# 1.1 Environment Setup\n\nTo set up the environment, run the following command:\n\n```bash\npython3 -m venv ~/path/to/venv\n```\n\nTo activate the environment, run the following command:\n\n```bash\nsource ~/path/to/venv/bin/activate\n```\n\n# 1.2 Install Dependencies\n\nTo install the dependencies, run the following command:\n\n```bash\nmake requirements\n```\n\n# 2 Configure OpenAI API\n\nFor this project, we will use the OpenAI API to generate responses for the chatbot. To configure the API, you need to create an account in [OpenAI](https://openai.com/). After that, you need to create an API key in the [dashboard](https://dashboard.openai.com/). Finally, you need to add the API key to conf/parameters.yaml in the api_key field.\n\n\n# 3 Run the Project\n\nIn this project, we use Makefile to run the project steps. To run the project, run the following commands:\n\n```bash\nmake download_urls\n```\nThis command will download the urls from the urls of products and services from B3 website.\n\n```bash\nmake download_docs\n```\nThis command will download the text documents from the urls downloaded in the previous step.\n\n```bash\nmake cleaning_docs\n```\nThis command will clean the text documents downloaded in the previous step.\n\n```bash\nmake write_docs\n```\nThis command will write the text documents in a Elasticsearch database\n\n```bash\nmake qa_model\n```\nThis command will generate the QA pipeline and save in a yaml file\n\n\n# 4 Project Organization\n------------\n\n    ├── LICENSE                         <- Repository license\n    ├── Makefile                        <- Makefile with commands like `make write_docs` or `make qa_model`\n    ├── README.md                       <- The top-level README for developers using this project.\n    ├── app                             <- App folder with the API and the web app\n    │   ├── backend                     <- Backend folder with the API and the data expecting for the api\n    │   │   └── api.py                  <- API with the endpoints to predict and train the model\n    │   └── frontend                    <- Frontend folder with the web app\n    │       └── main.py                 <- Web app with the interface to predict and train the model\n    ├── conf                            <- Configuration folder with the parameters used by the project\n    │   ├── catalog.yaml                <- Catalog file with the paths to the data\n    │   ├── parameters.yaml             <- Parameters file with the parameters used by the project\n    │   └── pipeline.yaml               <- Pipeline file with the pipeline used by the project\n    ├── configs.ini                     <- Config file with the parameters to be used by the API\n    ├── data                            <- Data folder with the data used by the project\n    │   ├── 01_raw                      <- Raw data folder\n    │   ├── 02_intermediate             <- Intermediate data folder\n    │   └── 03_features                 <- Features data folder\n    ├── notebooks                       <- Jupyter notebooks used to experiment and develop the project\n    ├── requirements-dev.in             <- The file where you should put the development dependencies like Black, Pytest, etc.\n    ├── requirements-dev.txt            <- ❗This file will be automatically generated by the pip-compile, DON'T EDIT IT❗\n    ├── requirements.in                 <- The file where you should put the modules and scripts dependencies.\n    ├── requirements.txt                <- ❗This file will be automatically generated by the pip-compile, DON'T EDIT IT❗\n    ├── src                             <- Source code for use in this project.\n    │   └── pipelines                   <- Pipeline folder with the pipeline used by the project\n    │       ├── etl                     <- ETL pipeline folder with the scripts used by the pipeline\n    │       │   ├── cleaning_docs.py    <- Script to clean the text documents\n    │       │   ├── download_docs.py    <- Script to download the text documents\n    │       │   ├── download_urls.py    <- Script to download the urls from the B3 website\n    │       │   ├── prepare_data.py     <- Script to prepare the data to be used by the API\n    │       │   ├── scrap_data.py       <- Script to scrap the data from the B3 website\n    │       │   └── write_docs.py       <- Script to write the text documents in a Elasticsearch database\n    │       └── qa                      <- QA pipeline folder with the scripts used by the pipeline\n    │           └── qa_model.py         <- Script to generate the QA pipeline and save in a yaml file\n    └── tox.ini                         <- tox file with settings for running tox; see tox.testrun.org\n------------\n\n\n# 5 Test model with the API\n\nTo test the model with the API, run the following command:\n\n```bash\nuvicorn app.backend.api:app --reload\n```\n\nAfter that you can open in another terminal the UI to test the API with the following command:\n\n```bash\nstreamlit run app/frontend/main.py \n```\n\n# Architecture\n\n![Architecture](reports/architecture.png)\n\n\n# TODO\n\n- [ ] Add logs\n- [ ] Add tests\n- [ ] Add CI/CD\n- [ ] Add Dockerfile\n- [ ] Add Docker Compose\n- [ ] Add orchestration with Prefect\n- [ ] Refactor the code\n"
    },
    {
      "name": "hyunsir/Cross-language-tutorial-search",
      "stars": 5,
      "img": "https://avatars.githubusercontent.com/u/46249154?s=40&v=4",
      "owner": "hyunsir",
      "repo_name": "Cross-language-tutorial-search",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2023-03-12T13:47:31Z",
      "updated_at": "2023-03-21T09:12:16Z",
      "topics": [],
      "readme": "# Cross-language-tutorial-search\n跨语言教程检索 - 推荐系统\n\n```\nproject_root            \n│   README.md           \n│   .gitignore          \n│   requirements.txt    \n│   definitions.py      \n│                       \n|\n└───app                 教程推荐\n│   │   __init__.py     \n│   └───module1         独立的功能模块接口1\n│   │   │   views.py    当前模块的视图，蓝图创建及路由配置\n│   │   │   models.py   当前模块的模型数据\n│   │   │   ...\n│   └───module2         独立的功能模块接口2\n│       │   views.py    当前模块的视图，蓝图创建及路由配置\n│       │   models.py   当前模块的模型数据\n│       │   ...\n│\n└───data                \n│\n└───doc                 文档\n│\n└───output              \n│\n└───script              \n│\n└───test                \n```\n\n\n\n# Cross-language tutorial search\n\n这个项目实现了目前常见的retrieval-rerank架构的检索器，在教程/论坛预料上进行微调。我们构建了相关的训练数据集和知识图谱，并搭建出最终的推荐系统。\n\nThis project implements the current common retrieval-rerank architecture for retrievalers, fine-tuning on tutorial/forum expectations. We build the relevant training dataset and knowledge graph, and build the final recommendation system.\n\n## Getting Started\n\nThese instructions will get you a copy of the project up and running on your local machine for development and testing purposes. See deployment for notes on how to deploy the project on a live system.\n.\n.\n.\n.\n.\n\n[//]: # (### Prerequisites)\n\n[//]: # ()\n[//]: # (What things you need to install the software and how to install them)\n\n[//]: # ()\n[//]: # (```)\n\n[//]: # (Give examples)\n\n[//]: # (```)\n\n[//]: # ()\n[//]: # (### Installing)\n\n[//]: # ()\n[//]: # (A step by step series of examples that tell you how to get a development env running)\n\n[//]: # ()\n[//]: # (Say what the step will be)\n\n[//]: # ()\n[//]: # (```)\n\n[//]: # (Give the example)\n\n[//]: # (```)\n\n[//]: # ()\n[//]: # (And repeat)\n\n[//]: # ()\n[//]: # (```)\n\n[//]: # (until finished)\n\n[//]: # (```)\n\n[//]: # ()\n[//]: # (End with an example of getting some data out of the system or using it for a little demo)\n\n[//]: # ()\n[//]: # (## Running the tests)\n\n[//]: # ()\n[//]: # (Explain how to run the automated tests for this system)\n\n[//]: # ()\n[//]: # (### Break down into end to end tests)\n\n[//]: # ()\n[//]: # (Explain what these tests test and why)\n\n[//]: # ()\n[//]: # (```)\n\n[//]: # (Give an example)\n\n[//]: # (```)\n\n[//]: # ()\n[//]: # (### And coding style tests)\n\n[//]: # ()\n[//]: # (Explain what these tests test and why)\n\n[//]: # ()\n[//]: # (```)\n\n[//]: # (Give an example)\n\n[//]: # (```)\n\n[//]: # ()\n[//]: # (## Deployment)\n\n[//]: # ()\n[//]: # (Add additional notes about how to deploy this on a live system)\n\n[//]: # ()\n[//]: # (## Built With)\n\n[//]: # ()\n[//]: # (* [Dropwizard]&#40;http://www.dropwizard.io/1.0.2/docs/&#41; - The web framework used)\n\n[//]: # (* [Maven]&#40;https://maven.apache.org/&#41; - Dependency Management)\n\n[//]: # (* [ROME]&#40;https://rometools.github.io/rome/&#41; - Used to generate RSS Feeds)\n\n[//]: # ()\n[//]: # (## Contributing)\n\n[//]: # ()\n[//]: # (Please read [CONTRIBUTING.md]&#40;https://gist.github.com/PurpleBooth/b24679402957c63ec426&#41; for details on our code of conduct, and the process for submitting pull requests to us.)\n\n## Versioning\n\nWe use [SemVer](http://semver.org/) for versioning. For the versions available, see the [tags on this repository](https://github.com/hyunsir/Cross-language-tutorial-search/tags).\n\n## Authors\n\n* **Zhang gaoyang** - *Initial work* - [hyunsir](https://github.com/hyunsir)\n\nSee also the list of [contributors](https://github.com/hyunsir/Cross-language-tutorial-search/contributors) who participated in this project.\n\n## License\n\nThis project is licensed under the MIT License - see the [LICENSE.md](LICENSE.md) file for details\n\n## Acknowledgments\n\n[//]: # (* 可爱滴大师兄)\n\n[//]: # (* 敬爱滴教授)\n* 知识图谱组滴家人们\n* fdu滴小伙伴们\n"
    },
    {
      "name": "WIET-QA-SYSTEM/quap",
      "stars": 5,
      "img": "https://avatars.githubusercontent.com/u/104865888?s=40&v=4",
      "owner": "WIET-QA-SYSTEM",
      "repo_name": "quap",
      "description": "Application for question-answering models demonstration",
      "homepage": null,
      "language": "Python",
      "created_at": "2022-05-03T19:56:02Z",
      "updated_at": "2022-11-13T18:51:53Z",
      "topics": [],
      "readme": "# QuestionAnswering\nApplication for question-answering models demonstration\n\n## Running the application\nIn order to run the application:\n- Have **Docker** along with **docker-compose** installed.\n- Clone this repo.\n- Open terminal in the main directory.\n- Run docker containers:\n  - If you have a GPU that supports CUDA: \n  \n    ``docker-compose -f docker-compose.yml -f docker-compose-gpu.yml up``\n  - If you don't:\n\n    ``docker-compose up``\n\n- After a while, open `localhost:9000` in the browser and enjoy the application.\n"
    },
    {
      "name": "tanchangsheng/covid19-search-and-qa",
      "stars": 5,
      "img": "https://avatars.githubusercontent.com/u/20267562?s=40&v=4",
      "owner": "tanchangsheng",
      "repo_name": "covid19-search-and-qa",
      "description": "Covid-19 Advisories Search and Question Answering with Transformers",
      "homepage": null,
      "language": "Python",
      "created_at": "2020-09-09T12:56:40Z",
      "updated_at": "2023-11-28T08:32:04Z",
      "topics": [],
      "readme": "# covid19-search-and-qa\n[Covid-19 Phase 2 Sector Advisories](https://www.moh.gov.sg/covid-19/phase-2-sector-related-advisories) Search and Question Answering with Transformers. \n\nA mini project to try out document question answering with transformers using [Haystack](https://github.com/deepset-ai/haystack) and [FastAPI](https://github.com/tiangolo/fastapi).\n\n## Why am I doing this?\n[Huggingface transformers](https://github.com/huggingface/transformers) have massively disrupted the NLP field in recent times as it provides an intuitive interface to access the almighty transformer models. Therefore, embarking on this mini project was one of the best opportunities to apply it to a real world use case and create positive value at the same time. Another objective of this project was also to get my hands dirty with a relatively new, modern web framework for building APIs, [FastAPI](https://github.com/tiangolo/fastapi), that have also been making waves due to it's performance and intuitive design.\n\n## Show me what it does!\n1. Search and Question Answering\n![queston-answering](./qa.gif)\n\n2. Search and Question Answering Filtered by Sector\n![queston-answering-filter](./qa-filter.gif)\n\n## What's so good about this?\nIn the current state of the new normal, [official announcements and advisories](https://www.moh.gov.sg/covid-19/phase-2-sector-related-advisories) provide guidelines for how we can resume our daily activities safely. However, there is simply too many documents covering various aspects of our daily lives and businesses. FAQ documents may help to highlight key points but it is often time consuming to create and may not have a comprehensive coverage.\n\nWith [Haystack](https://github.com/deepset-ai/haystack), we are able to use [pretrained transformer models](https://huggingface.co/deepset/roberta-base-squad2) to read the content of the advisories and provide relevant answers to user queries, **directly retrieving relevant answers from the documents** itself instead of relying on manually curated FAQs.\n\n## What's in this project?\nThere are 3 components in this project.\n1. `crawl.py` - Script to crawl, process and ingest the documents from [Covid-19 Phase 2 Sector-Related Advisories](https://www.moh.gov.sg/covid-19/phase-2-sector-related-advisories) to Elasticsearch.\n2. `csqa-api` - A [FastAPI](https://github.com/tiangolo/fastapi) app that provides the search and question answering APIs.\n3. `csqa-ui` - A [Vue.js](https://vuejs.org/) app that provides UI for search and question answering.\n\n## How to run it?\n\n### 1. Setup Python Environment\n- Install gcc `sudo apt-get install gcc python3-dev`. Required due to psutil.\n- Create an environment for the project. `conda create -n csqa python=3.7`\n- Run `conda activate csqa`\n- Install libraries `pip install -r requirements.txt`\n\n### 2. Get Apache Tika \n- Download Apache Tika Server from [here](https://www.apache.org/dyn/closer.cgi/tika/tika-server-1.24.1.jar)\n- Ensure you have java 8 installed before running.\n\n### 3. Build Your UI\n- Install node modules `yarn install`\n- Build for deployment `yarn build`\n\n### 4. Run Everything!\n1. Start Elasticsearch (with docker).\n    \n    a. Without pre-crawled data\n    - Run `docker run -p 9200:9200 -e \"discovery.type=single-node\" docker.elastic.co/elasticsearch/elasticsearch:7.6.2`\n        \n    b. With pre-crawled data You may use the pre-crawled index by running with volume. \n    - Download Elasticsearch index with pre-crawled data [here](https://drive.google.com/file/d/12FDd09Ehg_6fVwG2TD-qAqxVrrPe-Avr/view?usp=sharing) (last updated 9th Sep 2020).\n    - Run `docker run -p 9200:9200 -e \"discovery.type=single-node\" -v <path to downloaded data folder>:/usr/share/elasticsearch/data docker.elastic.co/elasticsearch/elasticsearch:7.6.2`\n\n3. Run tika (if you are running `crawl.py`)\n\n    a. Run `java -jar tika-server-1.24.1.jar`\n\n2. Run crawler (if you are not using the pre-crawled index)\n\n    a. Run `python crawl.py`\n\n3. Start the FastAPI app\n\n    a. Run `python app.py`\n\n4. Start your UI\n\n    a. Run `dist` folder in the web server of choice. (E.g. nginx, serve, apache http etc)\n"
    },
    {
      "name": "lambdaofgod/pytorch_hackathon",
      "stars": 5,
      "img": "https://avatars.githubusercontent.com/u/3647577?s=40&v=4",
      "owner": "lambdaofgod",
      "repo_name": "pytorch_hackathon",
      "description": "RSS wall app using Huggingface Transformers for search",
      "homepage": "",
      "language": "Jupyter Notebook",
      "created_at": "2020-08-09T19:22:13Z",
      "updated_at": "2024-04-27T12:00:57Z",
      "topics": [],
      "readme": "# NewsBERT \n\nUsing BERT & other transformer models for organizing RSS feed data\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/lambdaofgod/pytorch_hackathon/blob/master/notebooks/NewsBERT_on_Colab.ipynb)\n\n![](docs/newsbert_wall.png)\n\n\n## Setup\n\n```\npip install nbdev\nnbdev_build_lib; pip install -e .\n```\n\n## Run the app\n\n```\nstreamlit run streamlit/newsbert_app.py\n```\n"
    },
    {
      "name": "deepset-ai/github-agent",
      "stars": 5,
      "img": "https://avatars.githubusercontent.com/u/51827949?s=40&v=4",
      "owner": "deepset-ai",
      "repo_name": "github-agent",
      "description": "GitHub Issue Resolver Agent with Anthropic Claude 3.7 Sonnet and Haystack",
      "homepage": "https://haystack.deepset.ai/",
      "language": "Python",
      "created_at": "2025-03-11T14:27:13Z",
      "updated_at": "2025-04-20T17:03:36Z",
      "topics": [
        "agent",
        "agentic-ai",
        "haystack-ai",
        "llm"
      ],
      "readme": "# Github Agent\n\nA [Haystack](https://haystack.deepset.ai/) agent that check that can create instructions to resolve the given Github issue.\n\n![](agent_image.png)\n\nGiven an issue URL, the agent can:\n\n* Fetch and parse the issue description and comments\n* Identify the relevant repository, directories, and files\n* Retrieve and process file content\n* Determine the next steps for resolution and post them as a comment\n\nOnce you have a working agent, you can deploy and serve the agent as a REST API with [Hayhooks](https://github.com/deepset-ai/hayhooks)\n\n## Requirements\n* [Anthropic API key](https://www.anthropic.com/)\n* [Github Token](https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/managing-your-personal-access-tokens)\n\n\n### Install Dependencies\n```bash\npip install -r requirements.txt\n```\n\n### Deploy with Hayhooks\n\nStart the Hayhooks server:\n```bash\nhayhooks run\n```\n\nDeploy the agent:\n```bash\nhayhooks pipeline deploy-files deployment -n github-agent\n```"
    },
    {
      "name": "jbcodeforce/ML-studies",
      "stars": 5,
      "img": "https://avatars.githubusercontent.com/u/8050643?s=40&v=4",
      "owner": "jbcodeforce",
      "repo_name": "ML-studies",
      "description": null,
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2018-09-21T03:48:40Z",
      "updated_at": "2025-01-25T23:49:51Z",
      "topics": [],
      "readme": "# [Machine Learning Studies - Basics](https://jbcodeforce.github.io/ML-studies/)\n\n## Building this booklet locally\n\nThe content of this repository is written with markdown files, packaged with [MkDocs](https://www.mkdocs.org/) and can be built into a book-readable format by MkDocs build processes.\n\n1. Install MkDocs locally following the [official documentation instructions](https://www.mkdocs.org/#installation).\n1. Install Material plugin for mkdocs:  `pip install -r requirements.txt ` \n2. `git clone https://github.com/jbcodeforce/ML-studies.git` _(or your forked repository if you plan to edit)_\n3. `cd ML-studies`\n4. `mkdocs serve`\n5. Go to `http://127.0.0.1:8000/` in your browser.\n\n### Building this booklet locally but with docker\n\nIn some cases you might not want to alter your Python setup and rather go with a docker image instead. This requires docker is running locally on your computer though.\n\n* docker run --rm -it -p 8000:8000 -v ${PWD}:/docs squidfunk/mkdocs-material\n* Go to http://127.0.0.1:8000/ in your browser.\n\n### Pushing the book to GitHub Pages\n\n1. Ensure that all your local changes to the `master` branch have been committed and pushed to the remote repository. `git push origin code`\n1. Run `mkdocs gh-deploy` from the root directory.\n\n"
    },
    {
      "name": "Health-Informatics-UoN/lettuce",
      "stars": 5,
      "img": "https://avatars.githubusercontent.com/u/156185347?s=40&v=4",
      "owner": "Health-Informatics-UoN",
      "repo_name": "lettuce",
      "description": null,
      "homepage": "https://health-informatics-uon.github.io/lettuce/",
      "language": "Python",
      "created_at": "2024-07-23T17:46:39Z",
      "updated_at": "2025-04-19T02:57:01Z",
      "topics": [],
      "readme": "# Lettuce: LLM for Efficient Translation and Transformation into Uniform Clinical Encoding \n\n**Lettuce** is an application for medical researchers that matches the informal medicine names supplied by the user to concepts in the [Observational Health Data Sciences and Informatics](https://www.ohdsi.org) (OMOP) [standardised vocabularies](https://github.com/OHDSI/Vocabulary-v5.0/wiki)\n\nThe application can be used as an API, or run with a graphical user interface (GUI).\n\n   This project is under active development\n\n## Overview\n\nThe project uses a Large Language Model to suggest formal drug names to match the informal name supplied by the user. Suggested formal drug names are then fed into parameterised SQL queries against the OMOP database to fetch the relevant concepts. Any returned concepts are then ranked by how well they match the supplied query and provided to the user.\n\nThis is the rough process that the Lettuce API follows. Subject to change\n\n```mermaid\nflowchart TD\n    usr[User]\n    api_in(API)\n    api_out(API)\n    llm(Large Language Model)\n    strpr[[String pre-processing]]\n    omop[(OMOP database)]\n    fuzz[[Fuzzy matching]]\n    usr -- User sends an informal name to the API --> api_in\n    api_out -- API responds with concept\\ninformation as JSON --> usr\n    api_in -- LLM sent informal name --> llm\n    llm -- LLM responds with possible formal name --> strpr\n    strpr --> omop\n    omop --> fuzz\n    fuzz -- Matches meeting threshold --> api_out\n\n```\n\n## Installation\n\nTo use Lettuce, follow [the quickstart](https://health-informatics-uon.github.io/lettuce/quickstart)\n\n### Connecting to a database\n\nLettuce works by querying a database with the OMOP schema, so you should have access to one. Your database access credentials should be kept in `.env`. An example of the format can be found in `/Lettuce/.env.example`\n\n### Running the API\n\nThe simplest way to get a formal name from an informal name is to use the API and the GUI. To start a Lettuce server:\n\n```\n$ uv run python app.py\n```\nThe GUI makes calls to the API equivalent to the curl request below.\n\n### Run pipeline\n\nTo get a response without the GUI, a request can be made using curl, e.g. for Betnovate scalp application and Panadol\n\n```\n$ curl -X POST \"http://127.0.0.1:8000/pipeline/\" -H \"Content-Type: application/json\" -d '{\"names\": [\"Betnovate Scalp Application\", \"Panadol\"]}'\n```\n\nThe API endpoint is `/pipeline/`, and uses a `POST` method\n\nThe request body should have the format\n\n```\n   {\n    \"name\": <Drug informal name>,\n    \"pipeline_options\": {\n      <options>\n    }\n   }\n```\n\nRefer to the [API reference](https://health-informatics-uon.github.io/lettuce/api_reference/options/pipeline_options) for the available pipeline options.\n\nThe response will be provided in the format\n\n```\n   {\n    \"event\": \"llm_output\",\n    \"data\": {\n       \"reply\": formal_name: str,\n       \"meta\": LLM metadata: List,\n     }\n   }\n\n   {\n    \"event\": \"omop_output\",\n    \"data\": [\n       {\n         \"search_term\": search_term: str,\n         \"CONCEPT\": [concept_data: Dict]\n       }\n     ]\n   }\n```\n\nThe response will be streamed asynchronously so the llm_output will arrive before any omop_output\n\n## Published Images\nDevelopment Docker images for the Lettuce project are available on GitHub Container Registry (GHCR):\n\n- **Registry**: `ghcr.io/health-informatics-uon/lettuce`\n- **Images**:\n  - **Weights Image**: Includes pre-loaded LLaMA-3.1-8B weights.\n    - **Tags**:\n      - `dev-weights-llama-3.1-8B-sha-<hash>` (e.g., `dev-weights-llama-3.1-8B-sha-a1b2c3d`): Build from a specific commit.\n      - `dev-weights-llama-3.1-8B-edge`: Latest development image with weights.\n    - **Pull Command**: \n      ```bash\n      docker pull ghcr.io/health-informatics-uon/lettuce:dev-weights-llama-3.1-8B-edge\n  - **Base Image**: Lightweight image without weights, for custom setups.\n    - **Tags**: \n      - `dev-base-sha-<hash>` (e.g., `dev-base-sha-a1b2c3d`): Build from a specific commit.\n      - `dev-base-edge`: Latest development image with weights.\n    - **Pull Command**: \n      ```bash\n      docker pull ghcr.io/health-informatics-uon/lettuce:dev-base-edge\n      ```\n\n## Contact\n\nIf there are any bugs, please [email us](mailto:james.mitchell-white1@nottingham.ac.uk)\n"
    },
    {
      "name": "Pash10g/allcr-ai",
      "stars": 5,
      "img": "https://avatars.githubusercontent.com/u/16643200?s=40&v=4",
      "owner": "Pash10g",
      "repo_name": "allcr-ai",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2024-05-16T14:31:01Z",
      "updated_at": "2025-03-21T10:15:00Z",
      "topics": [],
      "readme": "# 👀 AllCR App\n\nAllCR App is a Streamlit application that allows users to capture real-life objects like recipes, documents, animals, vehicles, and more, and turn them into searchable documents. The app integrates with OpenAI's GPT-4 for OCR (Optical Character Recognition) to JSON conversion and MongoDB Atlas for storing the extracted information.\n\n## Features\n\n- **Authentication**: Secure access to the application using an API code.\n- **Image Capture**: Capture images using your device's camera.\n- **OCR to JSON**: Convert captured images to JSON format using OpenAI's GPT-4.\n- **MongoDB Integration**: Store and retrieve the extracted information from MongoDB.\n- **Search and Display**: Search and display stored documents along with their images.\n- **Chat with AI**: Open the sidebar to chat with GPT on the context captured by the app.\n\n## Requirements\n\n- Python 3.8+\n- Streamlit\n- OpenAI Python Client Library\n- MongoDB Atlas cluster\n\nOnce the cluster is deployed perform the following tasks:\n1. Create a database named 'ocr_db' with collection 'api_keys' :\n```\nuse ocr_db\ndb.api_keys.insertOne({'api_key' : \"<YOUR_IMAGINARY_KEY>\"});\n```\n2. Create a 2 search indexes:\n2.1 [Vector search](https://www.mongodb.com/docs/atlas/atlas-vector-search/tutorials/vector-search-quick-start/) index on 'ocr_db.ocr_documents':\n\n```\n{\n  \"fields\": [\n    {\n      \"numDimensions\": 1536,\n      \"path\": \"embedding\",\n      \"similarity\": \"cosine\",\n      \"type\": \"vector\"\n    },\n    {\n      \"path\": \"api_key\",\n      \"type\": \"filter\"\n    }\n  ]\n}\n```\n2.2 Atlas [text Search](https://www.mongodb.com/docs/atlas/atlas-search/tutorial/create-index/) index on 'ocr_documents':\n```\n{\n  \"mappings\": {\n    \"dynamic\": true,\n    \"fields\": {\n      \"api_key\": {\n        \"type\": \"string\"\n      },\n      \"ocr\": {\n        \"dynamic\": true,\n        \"type\": \"document\"\n      }\n    }\n  }\n}\n```\n\n\n- PIL (Python Imaging Library)\n- Haystack (for advanced search functionality)\n\n## Installation\n\n1. **Clone the repository:**\n\n   ```bash\n   git clone https://github.com/yourusername/allcr-app.git\n   cd allcr-app\n   ```\n\n   ## Installation\n\n\n\n2. **Install the required packages:**\n```\n   pip install -r requirements.txt\n```\n3. **Set up environment variables:**\n\n   Create a `.env` file in the root directory of your project and add your OpenAI API key, MongoDB URI, and API code for authentication.\n```\n   OPENAI_API_KEY=your_openai_api_key\n   MONGODB_ATLAS_URI=your_mongodb_atlas_uri\n```\n## Usage\n\n1. **Run the Streamlit app:**\n```\n   streamlit run app.py\n```\n2. **Access the app:**\n```\n   Open your web browser and go to `http://localhost:8501`.\n```\nOnce prompted input the api_key saved in Atlas under the 'ocr_db.api_keys' collection.\n\n3. **Authenticate:**\n\n   Enter the API code provided in your `.env` file to access the application.\n\n4. **Capture and Process Images:**\n\n   - Select the type of object you want to capture.\n   - Use the camera to take a picture of the object.\n   - The image will be processed, and the extracted text will be displayed for confirmation.\n   - Save the processed document to MongoDB.\n\n5. **Search and Display Documents:**\n\n   - Use the search functionality to find stored documents.\n   - Expand the results to view the extracted text and display the associated image.\n\n## Code Overview\n\n- **`app.py`**: Main application script that contains the Streamlit app logic.\n- **`requirements.txt`**: List of required Python packages.\n\n## Key Functions\n\n- **`auth_form()`**: Handles user authentication using an API code.\n- **`transform_image_to_text(image)`**: Transforms a captured image to text using OpenAI's GPT-4.\n- **`save_image_to_mongodb(image, description)`**: Saves the captured image and extracted text to MongoDB.\n- **`searc_aggregation(query), vector_search_aggregation(query, limit)`**: Searches and displays images from MongoDB based on the query/term.\n- **`get_ai_task(doc)`**: Performs an AI generation subtask with the inputed document as context.\n  **`chat_ai()`** : Use GPT to form a streaming chat with the vector query context on the application \"sidebar\".\n\n## Contributing\n\nContributions are welcome! Please fork the repository and submit a pull request with your changes.\n\n## License\n\nThis project is licensed under the MIT License. See the [LICENSE](LICENSE) file for details.\n\n## Contact\n\nFor questions or suggestions, please contact [Pavel](mailto:pavel.duchovny@mongodb.com).\n"
    },
    {
      "name": "marqo-ai/marqo-haystack",
      "stars": 5,
      "img": "https://avatars.githubusercontent.com/u/103185353?s=40&v=4",
      "owner": "marqo-ai",
      "repo_name": "marqo-haystack",
      "description": "The official integration for Marqo vector search and Haystack 2.0",
      "homepage": "",
      "language": "Python",
      "created_at": "2023-08-24T12:01:47Z",
      "updated_at": "2024-08-29T22:52:55Z",
      "topics": [],
      "readme": "# Marqo Document Store for Haystack\n\n[![PyPI - Version](https://img.shields.io/pypi/v/marqo-haystack.svg)](https://pypi.org/project/marqo-haystack)\n[![PyPI - Python Version](https://img.shields.io/pypi/pyversions/marqo-haystack.svg)](https://pypi.org/project/marqo-haystack)\n[![test](https://github.com/marqo-ai/marqo-haystack/actions/workflows/test.yml/badge.svg)](https://github.com/marqo-ai/marqo-haystack/actions/workflows/test.yml)\n\n-----\n\n**Table of Contents**\n\n- [Marqo Document Store for Haystack](#marqo-document-store-for-haystack)\n  - [Installation](#installation)\n  - [About](#about)\n  - [Examples](#examples)\n  - [Usage](#usage)\n    - [Using Locally](#using-locally)\n    - [Using with Marqo Cloud](#using-with-marqo-cloud)\n  - [License](#license)\n\n## Installation\n\n```console\npip install marqo-haystack\n```\n\n## About\n\nThis is a document store integration for [Marqo](https://github.com/marqo-ai/marqo) with [haystack](https://github.com/deepset-ai/haystack). \n\nMarqo is an end-to-end vector search engine which includes preprocessing and inference to generate vectors from your data. You can use pre-trained models or bring finetuned ones.\n\nHaystack is an end-to-end NLP framework that enables you to build applications powered by LLMs, with haystack you can build end-to-end NLP applications solving your use case using state-of-the-art models.\n\n## Examples\n\n```python\nfrom marqo_haystack import MarqoDocumentStore\n \ndocument_store = MarqoDocumentStore()\n```\n\nYou can find a code example showing how to use the Document Store and the Retriever under the `example/` folder of this repo.\n\n## Usage\n\nFor documentation on Marqo itself, please [refer to the documentation](https://docs.marqo.ai/latest/).\n\nYou can use the `MarqoDocumentStore` in your haystack pipelines for single queries like so:\n\n```python\nfrom marqo_haystack import MarqoDocumentStore\nfrom marqo_haystack.retriever import MarqoSingleRetriever\n\ndocument_store = MarqoDocumentStore()\n\nquerying = Pipeline()\nquerying.add_component(\"retriever\", MarqoSingleRetriever(document_store))\nresults = querying.run({\"retriever\": {\"query\": \"Is black and white text boring?\", \"top_k\": 3}})\n```\n\nOr for a list of queries:\n\n```python\nfrom marqo_haystack import MarqoDocumentStore\nfrom marqo_haystack.retriever import MarqoRetriever\n\ndocument_store = MarqoDocumentStore()\n\nquerying = Pipeline()\nquerying.add_component(\"retriever\", MarqoRetriever(document_store))\nresults = querying.run({\"retriever\": {\"queries\": [\"Is black and white text boring?\"], \"top_k\": 3}})\n```\n\n### Using Locally\n\nIf you specify a `collection_name` that doesn't exist as a Marqo index then one will be created for you.\n\n```python\nfrom marqo_haystack import MarqoDocumentStore\n\n# Use an existing index (if my-index does exist)\ndocument_store = MarqoDocumentStore(collection_name=\"my-index\")\n\n# Create a new index (if my-new-index doesn't exist)\ndocument_store = MarqoDocumentStore(collection_name=\"my-new-index\")\n\n# Use the default index name, 'documents'. One will be created if it doesn't exist.\ndocument_store = MarqoDocumentStore()\n```\n\nYou can also pass in settings for the index created by the API by passing a dictionary to the `settings_dict` parameter. For details on the settings object please [refer to the Marqo docs](https://docs.marqo.ai/latest/API-Reference/indexes/#body-parameters).\n\nIn this example we specify that the index should use the `e5-large-v2` model and increase the `ef_construction` parameter to 512 for the HNSW graph construction.\n\n```python\nfrom marqo_haystack import MarqoDocumentStore\n\nindex_settings = {\n    \"index_defaults\": {\n        \"model\": \"hf/e5-large-v2\",\n        \"ann_parameters\" : {\n            \"parameters\": {\n                \"ef_construction\": 512\n            }\n        }\n    }\n}\n\ndocument_store = MarqoDocumentStore(settings_dict=index_settings)\n```\n\n### Using with Marqo Cloud\n\nThis integration can also be used with Marqo Cloud. You can sign up or access you Marqo Cloud account [here](https://cloud.marqo.ai/).\n\nTo use Marqo Cloud with this integration you will need to pass the `collection_name` (index name), `url` (`https://api.marqo.ai`), and `api_key` into the constructor.\n\nNote that when using this integration with Marqo Cloud you will need to have already created an index in your Marqo Cloud account.\n\n```python\nfrom marqo_haystack import MarqoDocumentStore\n \ndocument_store = MarqoDocumentStore(\n    url=\"https://api.marqo.ai\",\n    api_key=\"XXXXXXXXXXXXX\",\n    collection_name=\"my-cloud-index\"\n)\n```\n\n## License\n\n`marqo-haystack` is distributed under the terms of the [Apache-2.0](https://spdx.org/licenses/Apache-2.0.html) license.\n"
    },
    {
      "name": "Blacksujit/ArogyaKrishi",
      "stars": 4,
      "img": "https://avatars.githubusercontent.com/u/148805811?s=40&v=4",
      "owner": "Blacksujit",
      "repo_name": "ArogyaKrishi",
      "description": "ArogyaKrishi MVP is an innovative web application crafted to empower farmers with cutting-edge tools for crop disease detection, soil type prediction, and tailored fertilizer recommendations. By harnessing the latest technology, it delivers actionable insights that revolutionize agricultural practices, promoting healthier crops and sustainablity.",
      "homepage": "",
      "language": "Jupyter Notebook",
      "created_at": "2024-11-12T18:54:34Z",
      "updated_at": "2025-04-11T16:48:11Z",
      "topics": [
        "ai",
        "ann",
        "artificial-intelligence",
        "deep-learning",
        "environment",
        "hackathon",
        "hackathon-project",
        "hackathon2024",
        "huggingface-transformers",
        "innovation",
        "machine-learning",
        "sih2024",
        "smart-india-hackathon",
        "sustainability",
        "tech-building",
        "technology",
        "training-models",
        "transformers"
      ],
      "readme": "<h1 align=\"center\" id=\"title\">🌾 ArogyaKrishi 🌾 </h1>\n\n<p align=\"center\">\n  <img src=\"https://github.com/user-attachments/assets/6b22e194-0e53-41a1-a33c-0a0b204e8999\" alt=\"image\">\n</p>\n\n\n<p align=\"center\"><img src=\"https://socialify.git.ci/Blacksujit/ArogyaKrishi/image?forks=1&amp;issues=1&amp;language=1&amp;name=1&amp;owner=1&amp;pulls=1&amp;stargazers=1&amp;theme=Auto\" alt=\"project-image\"></p>\n\n## 🌟This is our Project or prototype development to the SIH Hackathon 2024:\n\n\n## 🧾🧾 Table of Contents:\n\n- [Problem Statement ID](#-problem-statement-id-sih---1638)\n- [Our Approach](#-our-approach-)\n- [Project Structure](#-project-structure)\n- [System Architecture Design](#-system-architecture-design-)\n- [Live Demos](#-live-demos-web-and-app)\n- [Presentation](#-presentation)\n- [Research and References](#-research-and-references-some-proven-theories-which-validate-this-poc)\n- [Problem ArogyaKrishi Addresses and Solves](#️-problem-arogyakrishi-addresses-and-solves-)\n- [Features](#-features-arogya-krishi-comes-with-)\n- [Tech Stack Used](#-built-with)\n- [Installation Steps](#️-installation-steps)\n- [Feasibility Analysis](#-feasibility-analysis)\n- [Potential Challenges & Risks](#-potential-challenges--risks)\n- [Impact and Benefits](#-impact-and-benefits)\n- [Contribution Guidelines](#-contribution-guidelines)\n- [Contributors](#-contributors-team-anant)\n- [License](#️-license)\n\n\n###  👉 Problem statement ID: SIH - 1638:\n\nTheme - Agri&amp;tech Background: Crop diseases can devastate yields leading to significant financial losses for farmers. Early detection and timely intervention are crucial for effective management. Description: Develop an AI-driven system that analyzes crop images and environmental data to predict potential disease outbreaks. This system will provide farmers with actionable insights and treatment recommendations to mitigate risks. Expected Solution: A mobile and web-based application that utilizes machine learning algorithms to identify crop diseases and suggest preventive measures and treatments based on real-time data.\n\n\n### 💡 Our Approach :(Novelty)\n\n1.) Divided  the problem statement into chunks and pieces .\n\n2.) begain the searching with the each segement in the research papaers , you can visit [kaggle](https://www.kaggle.com/datasets/vipoooool/new-plant-diseases-dataset) , and [UCI Machine learning Datasets](https://archive.ics.uci.edu/dataset/486/rice+leaf+diseases) , for the references and datasets,\n\n3.) Then for the inspiration purpose we have reviewed and looked several projects and repositories  available on the github and internet , you can visit , these repositories , [repo1](https://github.com/Blacksujit/Crop-Disease-Detection) , and \n [repo2](https://github.com/Blacksujit/Green-Points) , [repo3\n](https://github.com/Blacksujit/Harvestify).\n\n4.) For the innovation and ideas , we have used [claude.ai](https://claude.ai/new) , [perlexity](https://www.perplexity.ai/) , [gpt](https://openai.com/index/gpt-4/) , [research papaers](https://paperswithcode.com/) \n\n5.) Determine what are the common Problem faced by the indian farmers and also researched about that for reference visit ,[ here](https://www.jiva.ag/blog/what-are-the-most-common-problems-and-challenges-that-farmers-face) , and how we can automate and mitigate it with the help of the Tech and innovation to an optimum level , such thtat the farmers life will be easier,\n\n6.) After  we have then searched for some common agricultural practices followed and weather forecast analysis report in the past years, which will help us to analyse why the things have become  more complex for the crops to survive in the extreme conditions you can visit for the [data](https://www.nber.org/digest/202406/weather-forecasts-and-farming-practices-india)   \n\n7.) After all the informationa has exhausted we have then move towards the most crucial part which is user behaviour , becuase it is the only thing where our product will ipmact and also , we were aware of how seamlessly we can onboard our user will create an , visibility and viability and easily accessibility of our product . so thouroughly researched about , how familier the indian farmer behaviour is  using tech , whether it is mobile application or it is and web application , we thaught every aspect in terms of the language barrier to UI complexity to the navigation e, every nity , greety is tackled so we couldn't miss out anything , for the same we have researched about the things [here](https://www.sciencedirect.com/science/article/abs/pii/S0959652624010795)\n\n8.) At last we have iterated on our approach to look out if any breaches our setback is there or not  , if it is then we have to restart the things from scratch but there wasnt , after all these brainstorming with the team  and other things we were ready to move to the POC stage with  our idea.\n\n9.) so thats all these was our approach for the problem statement .\n\n\n### 📂 Project Structure:\n\n```\n\nArogya_Krishi_MVP/\n|\n│\n├── app.py                          # Main application file\n├── requirements.txt                # List of dependencies\n├── .env                            # Environment variables\n│\n├── models/                         # Directory for machine learning models\n│   ├── RandomForest.pkl            # Crop recommendation model\n│   ├── DenseNet121v2_95.h5        # Soil type prediction model\n│   ├── SoilNet_93_86.h5           # SoilNet model\n│   └── plant_disease_model.pth     # (if applicable) Disease prediction model\n│\n├── utils/                          # Utility functions and classes\n│   ├── disease.py                  # Disease-related utilities\n│   ├── fertilizer.py               # Fertilizer-related utilities\n│   └── model.py                    # Model-related utilities (e.g., ResNet9)\n│\n├── uploads/                        # Directory for uploaded files\n│   └── (user-uploaded images)      # Images uploaded by users\n│\n├── static/                         # Static files (CSS, JS, images)\n│   ├── css/                        # CSS files\n│   ├── js/                         # JavaScript files\n│   └── images/                     # Images used in the application\n│\n│──── templates/                      # HTML templates for rendering\n│    ├── index.html                  # Main page\n│    ├── signup.html                 # Signup page\n│    ├── login.html                  # Login page\n│    ├── dashboard.html              # User dashboard\n│    ├── crop.html                   # Crop recommendation page\n│    ├── fertilizer.html              # Fertilizer suggestion page\n│    ├── disease.html                # Disease prediction page\n│    ├── disease-result.html         # Result page for disease prediction\n│    ├── crop-result.html            # Result page for crop prediction\n│    └── try_again.html              # Error handling page\n│\n│\n│────.gitattributes\n│\n│────.gitignore\n│\n│────README.md\n│\n│────crop-disease-detection.ipynb\n│\n│────config.py\n│\n│\n│────crop_prediction_based_on_numeric_value.ipynb\n│\n│\n│────notebooks/\n│        \n│──── instance/         \n│        │──── farmers_database.db\n│\n│\n│-----images/\n│\n│──── data/\n│          ...csv files\n│\n│──── Data_Preprocessed/\n          .....cleaned_data\n \n```\n\n## 🧱 System Arcitecture Design :\n\n\n![image](https://github.com/user-attachments/assets/8f8324f4-b3ef-4831-b0aa-475b18fc49ca)\n\n\n\n## 🚀 Live Demos , Web and  App:\n\n### 🕸️📲Web App:\n\n \nhttps://github.com/user-attachments/assets/de859af0-d5c0-471a-b195-9608bb8538c5\n\n\n### <p>Explanation youtube Video :- Experience ArogyaKrishi in action  <a href=\"https://youtu.be/yBajAQB9Kas?si=ilwix0wwiN533UYi\" target=\"_blank\">Watch the Demo   (Web APP Demo) </a></p>\n\n### 📲 App Demo : (Under Developement Phase)\n\nhttps://github.com/user-attachments/assets/129d96a3-861c-46a2-99bd-3d54791d74ac\n\n## 👓 Presentation :\n\n [see PPT Presentation Here by TEAM ANANT](https://github.com/user-attachments/files/17748441/SIH_TEAM_ANANT_CROP_DISEASE-3.pdf)\n\n\n## 📚📚 Research And References: (Some Proven Theories' which validates this POC)\n\n1.\tDevelopment of Machine Learning Methods for Accurate Prediction of Plant Disease Resistance\n(2024): https://www.sciencedirect.com/science/article/pii/S2095809924002431\n\n2.\tChinese cabbage leaf disease prediction and classification using Naive Bayes VGG-19 convolution deep neural network (2024) : https://ieeexplore.ieee.org/document/10407076\n\n3.\tImage-based crop disease detection with federated learning (2023): https://www.nature.com/articles/s41598-023-46218-5\n\n4.\tDeep learning-based crop disease prediction with web application (2023) : https://www.sciencedirect.com/science/article/pii/S2666154323002715\n\n5.\tSeasonal Crops Disease Prediction and Classification Using Deep Convolutional Encoder Network (2019): https://link.springer.com/article/10.1007/s00034-019-01041-0\n   Cropin app link: https://www.cropin.com/farming-apps#:~:text=Cropin%20Grow%20is%20a%20robust, stakeholders%20in%20the%20agri%2Decosystem.\n\n\n\n## ✅❇️  Problem  ArogyaKrishi Addresses and Solves :\n\n**Arogya Krishi is an agricultural application that addresses critical challenges faced by farmers, including crop disease detection, optimal crop recommendations, and soil health assessment. It provides tailored fertilizer suggestions and integrates real-time weather data to help farmers make informed decisions. With a user-friendly interface and community engagement features, Arogya Krishi empowers farmers to enhance productivity and sustainability in their agricultural practices.**\n\n\n### Arogya Krishi: Problem Description and Solutions\n\nArogya Krishi is a comprehensive agricultural application designed to address various challenges faced by farmers and agricultural stakeholders. Below are the key problems it addresses and the solutions it provides:\n\n**1. Crop Disease Detection:**\n\n**Problem:** Farmers often struggle to identify diseases affecting their crops, leading to reduced yields and economic losses. Early detection is crucial for effective management and treatment.\n\n**Solution:**  Arogya Krishi utilizes advanced image classification techniques powered by machine learning to analyze images of crops. The application can accurately identify diseases and provide detailed information about the disease, enabling farmers to take timely action.\n\n**2. Crop Recommendation:**\n\n**Problem:** Farmers may lack knowledge about which crops are best suited for their specific soil types and climatic conditions, leading to poor crop choices and low productivity.\n\n**Solution:** The application offers crop recommendation features based on soil analysis and environmental factors. By analyzing soil characteristics and local climate data, Arogya Krishi suggests optimal crops that can thrive in the given conditions, enhancing productivity and profitability.\n\n**3.) Fertilizer Recommendations:**\n\n**Problem:** Farmers often face challenges in determining the right type and amount of fertilizers to use, which can lead to overuse or underuse, affecting crop health and the environment.\n\n**Solution:** The application generates tailored fertilizer recommendations based on the specific crop being cultivated, soil health, and environmental conditions. This helps farmers optimize their fertilizer usage, improving crop yields while minimizing environmental impact.\n\n**4. Weather Data Integration:**\n\n**Problem:** Farmers need access to real-time weather data to make informed decisions about planting, irrigation, and harvesting. Lack of timely weather information can lead to crop losses.\n\n**Solution:** Arogya Krishi integrates weather data from reliable sources, providing farmers with current weather conditions, forecasts, and alerts. This information helps farmers plan their activities more effectively, reducing risks associated with adverse weather.\n\n**6. User-Friendly Interface:**\n\n**Problem:** Many agricultural technologies are complex and difficult for farmers to use, especially those with limited technical knowledge.\n\n**Solution:** Arogya Krishi is designed with a user-friendly interface that simplifies navigation and usage. It provides clear instructions and visual aids, making it accessible to farmers of all backgrounds.\n\n**7. Community and Knowledge Sharing:**\n\n**Problem:** Farmers often work in isolation and may lack access to shared knowledge and experiences from their peers.\n\n**Solution:** The application can facilitate community engagement by allowing users to share their experiences, tips, and best practices. This fosters a sense of community and encourages collaborative learning among farmers.\n\n  \n## 🧐 Features  (ArogyaKrishi Comes with) :\n\n**Here're some of the project's best features:**\n\n*   Multi Crop Disease Prediction .\n*   prediction with Fertilizer Recommendation.\n*   Realtime Crop Disease Analysis Dashboard\n*   Soil Type Classification\n*   Farmer Data Tracking\n*   Weather Analysis's and Report\n*   Multilingual Support\n*   Google IO translate in your language\n*   Chat bot support assistance multilingual's\n*   Farmer Management\n*   Fertilizer Recommendations Based On Soil\n\n## 💻 Tech Stack Used :\n\n**Technologies used in the project:**\n\n*   Flask\n*   Model development Pipeline\n*   Machine Learning\n*   Python\n*   transformers\n*   deep learning\n*   neural networks\n*   pretrained Models\n*   HTML\n*   CSS\n*   Javascript\n*   SCSS\n*   SQL-alchemy\n\n\n## 🛠️ Installation Steps , To run with git (Manuall Process):\n\n<p>1. Clone the Project</p>\n\n```\ngit clone https://github.com/Blacksujit/ArogyaKrishi.git\n```\n\n<p>2. Install Required Dependencies:\n\n```\npip install requirements.txt\n```\n\n<p>3. Train The Models</p>\n\n```\nrun crop_prediction_based_on_numerical_value.ipynb\n```\n\n<p>4. save the Pretrained Models</p>\n\n```\nmodel/\n  resnet_mode_90.pkl\n  ......pkl\n   etc\n```\n\n<p>5. create the dotenv File at root of Project</p>\n\n```\n.env\n```\n\n<p>6. Add Your API Key</p>\n\n```\nOPEN_WEATHER_APIKEY=YOUR_WEATHER_API_KEY\nHUGGINGFACE_LOGIN_TOKEN=YOUR_HUGGING_FACE_TOKEN\n\n```\n\n<p>7. Run the Project</p>\n\n```\npython app.py \n```\n\n## ✨🔮 To Run Locally Using Docker (preferred way) :\n\n### Prerequisites\n\nBefore you begin, ensure you have the following installed on your machine:\n\n- [Docker](https://www.docker.com/get-started)\n- [Docker Compose](https://docs.docker.com/compose/install/)\n\n### Setup Instructions:\n\n### Step 1: Clone the Repository\n\n``Clone the project repository to your local machine:``\n\n\n```\ngit clone https://github.com/Blacksujit/ArogyaKrishi.git\n```\n\n\n### Step 2: Create a `.env` File\n\n``Create a `.env` file in the root directory of the project and add your API keys:``\n\n```\nOPEN_WEATHER_APIKEY=YOUR_WEATHER_API_KEY\nHUGGINGFACE_LOGIN_TOKEN=YOUR_HUGGING_FACE_TOKEN\n```\n\n\n### Step 3: Build and Run the Docker Containers\n\n``Use Docker Compose to build and run the application:``\n\n```\ndocker-compose up --build\n```\n\n\n### 🪄 This command will:\n\n**- Build the Docker image defined in the `Dockerfile`.**\n**- Start the web application and the database service defined in `docker-compose.yml`.**\n\n### Step 4: Access the Application\n\n``Once the containers are running, you can access the application in your web browser at:``\n\nhttp://localhost:5000\n\n\n### Step 5: Stopping the Application\n\n**To stop the application, press `CTRL + C` in the terminal where the Docker containers are running. You can also run:**\n\n\n```docker-compose down```\n\n### ⭕⭕ Notes:(IMP instructions)\n\n``YOUR_WEATHER_API_KEY and YOUR_HUGGING_FACE_TOKEN with your actual API keys in the .env file.``\n\n``Ensure that the requirements.txt file is present in your project directory with all necessary dependencies listed.``\n\n\n## 📈 Feasibility Analysis:\n\n•\t**High Feasibility:** Advanced ML models and cloud deployment enable real-time disease prediction.\n\n•\t**Scalability:** Supports multilingual features\n\n•\t**Personalized Alerts**: Farmers get alerts based on crop type, region, and disease severity.\n\n\n## ⚓ Potential Challenges & Risks:\n\n•\t**Data Quality:** Poor data leads to inaccurate predictions\n\n•\t**Accuracy of AI Models:** Risk of false positives and false negatives cases.\n\n•\t**Environmental Variability:** Presence of Diverse conditions. Viability Analysis:\n\n•\t**Early Detection:** Detects diseases early, lowering treatment costs and preventing spread.\n\n## 🪄🔮 Impact And Benifities:\n\n•\t**Economic Benefits:** Lowers disease management costs, increasing productivity.\n\n•\t**Environmental Impact:** Optimizes pesticide/fertilizer use, promoting sustainability.\n\n•\t**Data-Driven Decisions:** Provides real-time insights for efficient farm management.\n\n•\t**Resilience:** Enhances farming practices and reduces risks of disease outbreaks.\n\n•\t**Uniqueness:**\n\n1. Identify crop diseases\n\n2. Predict disease outbreaks\n\n3. Recommend preventive measure\n\n4. Optimize resource allocation \n\n5. Enhance agricultural productivity \n\n6. Real-time performance \n\n7. Userfriendliness \n\n8. Scalability\n\n\n## 🍰 Contribution Guidelines:\n\nWe welcome contributions!   please  create an  seperate branch and make your chnages and raise and PR if it\nmatches the Project requirements and follow all the guidelines we will happy to merge it .\n\n\n1. ``Fork the repository.``\n\n2. Create your feature branch:\n   \n   ```bash\n   git checkout -b feature/YourFeature\n   ```\n\n4. Commit your changes:\n   \n   ```bash\n   git commit -m 'Add some feature'\n   ```\n\n6. Push changes to the local branch:\n   \n   ```bash\n   git push origin feature/YourFeature\n   ```\n\n8. ``Open a pull request.``\n\n\n\n### ✨ Contributors: (TEAM ANANT)\n\n1.) Abhishek Kute\n\n2.) Sanskar Awati\n\n3.) Manas Kotian\n\n4.) Minal \n\n5.) Tanushri\n\n\n## 🛡️ License:\n\nThis project is licensed under the MIT.\n\n \n"
    },
    {
      "name": "harshal-pathak/Gemini1.5-mp3-Summerization",
      "stars": 4,
      "img": "https://avatars.githubusercontent.com/u/124377711?s=40&v=4",
      "owner": "harshal-pathak",
      "repo_name": "Gemini1.5-mp3-Summerization",
      "description": "Summarize an audio file with Gemini 1.5 Pro",
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2024-05-17T18:12:55Z",
      "updated_at": "2024-10-18T14:16:40Z",
      "topics": [],
      "readme": "# Summarize an audio file with Gemini 1.5 Pro\n\nUnlock the power of multimodal AI with our latest tutorial on Gemini 1.5 API, Google's advanced language and vision model. In this video, we delve into the capabilities of Gemini 1.5, demonstrating how to leverage its audio features in Google Colab to efficiently summarize audio files. Plus, we guide you through building a Streamlit app from scratch, showing you step-by-step how to integrate Gemini 1.5 for real-time text summarization. \n\n\n"
    },
    {
      "name": "shuoli90/TRAQ",
      "stars": 4,
      "img": "https://avatars.githubusercontent.com/u/19268975?s=40&v=4",
      "owner": "shuoli90",
      "repo_name": "TRAQ",
      "description": null,
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2023-09-07T14:16:40Z",
      "updated_at": "2024-09-01T20:01:55Z",
      "topics": [],
      "readme": "# Overview\nThis is the project for trustworthy retrieval augmented chatbot (TRAQ). The organization of the project is:\n\n- Folder *run*: implementations for TRAQ and other baselines\n- Folder *data*: original datasets \n- Folder *collected_data*: collected data including original prompts and generated responses from LLMs\n- Folder *collected_results*: evalutation results using different methods, including TRAC, TRAC-P, Bonf, Bonf-P\n- Folder *collected_plots*: plots reported in our submission\n- Folder *finetuned_models*: weights for finetuned models used in our submission\n- File *requirements.txt*: list important packages and corresponding versions\n\nPlease download corresponding folders (except *collected results*) from this link: [TRAQ](https://drive.google.com/drive/folders/1irO2-Fu-cpaDhOLEnpc7_-bOgdk0zvu3?usp=drive_link); and unzip to the root folder.\n\n# Evaluate different methods\n```\ncd trac\n# evaluate methods using chatgpt\n./bash/evaluate_chatgpt.bash\n# evaluate methods using llama-2\n./bash/evaluate_opensource.bash\n# evaluate methods using few-shot prompt \n./bash/evaluate_chatgpt_semantic.bash\n# evaluate method using a specific configuration using chatgpt\npython trac_chatgpt.py --task $TASK --seed $SEED --alpha $ALPHA\n# evaluate method using a specific configuration using llama-2\npython trac_opensource.py --task $TASK --seed $SEED --alpha $ALPHA\n```\n\n# Analysis evaluation results\n```\ncd trac/analysis\n# analysis results using chatgpt\npython analysis.py --task chatgpt\n# analysis results using llama-2\npython analysis.py --task opensource\n```\n\n# collect data\nRun jupyter notebooks in folder *trac/collect*\n"
    },
    {
      "name": "THUSIGSICLAB/FineDance",
      "stars": 4,
      "img": "https://avatars.githubusercontent.com/u/100828931?s=40&v=4",
      "owner": "THUSIGSICLAB",
      "repo_name": "FineDance",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2024-02-28T09:25:20Z",
      "updated_at": "2024-11-14T04:37:43Z",
      "topics": [],
      "readme": "# [FineDance: A Fine-grained Choreography Dataset for 3D Full Body Dance Generation (ICCV 2023)](https://github.com/li-ronghui/FineDance)\n\n[[Project Page](https://li-ronghui.github.io/finedance)] | [[Preprint](https://arxiv.org/abs/2212.03741)] | [[pdf](https://arxiv.org/pdf/2212.03741.pdf)] | [[video](https://li-ronghui.github.io/finedance)]\n\n\n### Teaser\n\n<img src=\"teaser/teaser.png\">\n\n### Download the FineDance Dataset\n\nWe introduce a Fine-grained Choreography Dance dataset(FineDance). It comprises over 14.6 hours of data collected from 346 paired songs and dances, was created by 27 professional dancers and a motion capture system, which has accurate body and hand motions. The fine-grained 22 dance genres of FineDance spanning traditional and modern styles, which make the genre-matching of generated dance sequences and given music become more challenging. The part(7.7 hours) of FineDance dataset can be downloaded at [BaiduNetDisk](https://pan.baidu.com/s/1DdJjfXWZZvnUmqPUrOTf7g?pwd=dkqn)\n\n\n### Dataset Descriptions\nPut the downloaded FineDance data into './data'. \n\nThe data directory is organized as follows:\n\nlabel_json: contains the song name, coarse style and fine-grained genre.\n\nmotion: contains the [SMPLH](https://smpl-x.is.tue.mpg.de/) format motion data.   \n\nmusic_wav: contains the music data in 'wav' format.\n\nmusic_npy: contains the music feature extracted by [librosa](https://github.com/librosa/librosa) follow [AIST++](https://github.com/google/aistplusplus_api/tree/main)\n\nHere is an example python script to read the motion file\n```python\nimport numpy as np\ndata = np.load(\"motion/001.npy\")\nT, C = data.shape           # T is the number of frames\nsmpl_poses = data[:, 3:]\nsmpl_trans = data[:, :3]\n```\n\n\n### Prepare\nDownload the assets files from [Google Drive](https://drive.google.com/file/d/1ENoeUn-X-3Vw2Gon-voVLlndy3hZXdWD/view?usp=drive_link).\n\nInstall the conda enviroment.\n\n```python\nconda env create -f environment.yaml\nconda activate FineNet\n```\n\n### Data preprocessing\n```python\npython data/code/pre_motion.py\n```\n\n\n### Training\n```python\naccelerate launch train_seq.py\n```\n\n### Generate\n\n```python\npython data/code/slice_music_motion.py\npython generate_all.py  --motion_save_dir generated/finedance_seq_120_dancer --save_motions\n```\n\n### Generate dance by custom music\n```python\npython test.py --music_dir 'your music dir' --save_motions\n```\n\n### Visualization\n```python\npython render.py --modir eval/motions --gpu 0\n```\n\n\n### Dataset split\n\nWe spilt FineDance dataset into train, val and test sets in two ways: FineDance@Genre and  FineDance@Dancer. Each music and paired dance are only present in one split. \n\n1. The test set of FineDance@Genre includes a broader range of dance genres, but the same dancer appear in  train/val/test set. Although the training set and test set include the same dancers, the same motions do not appear in both the training and testing sets. This is because these dancers do not have distinct personal characteristics in their dances.\n2. The train/val/test set of FineDance@Dancer was divided by different dancers, which test set contains fewer dance genres, yet the same dancer won't appear in different sets.\n\nIf you use this dataset for dance generation, we recommend you to use the split of FineDance@Genre.\n\n## Acknowledgments\nWe would like to express our sincere gratitude to Dr [Yan Zhang](https://yz-cnsdqz.github.io/) and [Yulun Zhang](https://yulunzhang.com/) for their invaluable guidance and insights during the course of our research.\n\nThis code is standing on the shoulders of giants. We want to thank the following contributors that our code is based on:\n[EDGE](https://github.com/Stanford-TML/EDGE/tree/main),[MDM](https://github.com/Stanford-TML/EDGE/tree/main),[Adan](https://github.com/lucidrains/Adan-pytorch),[Diffusion](https://github.com/lucidrains/denoising-diffusion-pytorch),[SMPLX](https://smpl-x.is.tue.mpg.de/).\n\n## Citation\nWhen using the code/figures/data/video/etc., please cite our work\n```\n@inproceedings{li2023finedance,\n  title={FineDance: A Fine-grained Choreography Dataset for 3D Full Body Dance Generation},\n  author={Li, Ronghui and Zhao, Junfan and Zhang, Yachao and Su, Mingyang and Ren, Zeping and Zhang, Han and Tang, Yansong and Li, Xiu},\n  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},\n  pages={10234--10243},\n  year={2023}\n}\n```\n"
    },
    {
      "name": "SeungHunHan11/FooTball-LLM",
      "stars": 4,
      "img": "https://avatars.githubusercontent.com/u/95485958?s=40&v=4",
      "owner": "SeungHunHan11",
      "repo_name": "FooTball-LLM",
      "description": "LLM Specialized in Football Related QAs",
      "homepage": "",
      "language": "Jupyter Notebook",
      "created_at": "2023-08-19T13:53:29Z",
      "updated_at": "2025-01-07T07:25:26Z",
      "topics": [],
      "readme": "\n# FT-LLM: Development of a Retrieval Augmented Generation based Language Model Framework for Assisting Data Analysis in the Football Industry\n\n\nFT-LLM: Development of a Retrieval Augmented Generation based Language Model Framework for Assisting Data Analysis in the Football Industry\n\n\n## Architecture\n\n![image](./assets/architecture.png)\n## Authors\n\n- [Seung Hun Han](https://www.github.com/SeungHunHan11)\n- [Woo Hyun Hwang](https://github.com/rayhwang3130)\n- [Minhyeok Kang](https://github.com/kangminhyeok02)\n\n\n## Installation\n\n### 1. Install Dockerfile \n\n```bash\n    cd Docker\n    docker build -t ftllm .\n```\n\n### 2. Build Container\n```bash\ndocker run --gpus all -it -h ftllm \\\n        --ipc=host \\\n        --name ftllm \\\n        #--v local:container directory \n        ftllm bash\n```\n\n### 3. Inside the container, install requirements \n\n```bash\npip install -r requirements.txt\n```\n    \n## Deployment\n\nTo deploy this project, do as the following steps\n\n### Run ```run.py``` file\n\n```bash\npython run.py \\\n        --yaml_config ./config/runner_chatgpt.yaml \\ # Configuration for hyperparameters\n        --data_dir ./QA_dataset.csv \\ # Data Directory for QA dataset\n        --save_dir ./outputs/ # Set default directory for model outputs\n```\n\n- Inside yaml file, one should set relevant directory and hyperparameters accordingly\n\n### Arguments\n\n- Set OpenAI API Model type\n    \n     ```model_name```: str; options = [```gpt-3.5-turbo```,  ```gpt-4```]\n\n- Set directory for Retrieval dataset\n\n    ```data_dir```: str; default = ```\"../json_data_new\"```\n\n- ```embedding_data_dir```: str; default = ```\"../embedding_data\"```\n\n- Select type for retrieval module\n\n    ```content_type```: str; options = ['All', 'title', 'content']\n        \n        \"All\" : Use both title and contents of the article\n        \"title\" : Use title of the article\n        \"content\" : Use content of the article\n    \n- Set number of articles that will be used for ```strategy generation``` module\n\n    ```top_k```: int; default=3\n\n- Set Directory for BM25 or Embedding model index file. If not set, index file will be automatically generated\n\n    ```index_save_dir```: str; default= ```\"./index_save_dir/sparse\"```\n\n- Set Retrieval Mode\n\n    ```retrieve_mode```: \n        \n        \"sparse\" : Use BM25\n        \"dense\" : Use VectorDB\n\n- Use ```Query Refinement Module``` or not\n\n    ```use_refine```: str; options = ['unrefined', 'refined']\n\n        \"unrefied\" : Do not use query refinement\n        \"refined\" : Use query refinement\n\n\n## Environment Variables\n\n#### 1. Download retrieval dataset and necessary model weights\n- To acquire retrieval dataset, contact andrewhan@korea.ac.kr\n\n#### 2. Put OpenAI API Keys in ``` secret.py``` api_key\n## Acknowledgements\n\n - [싸커러리 Soccerary](youtubrary@gmail.com) : QA Dataset Generation\n\n## Feedback\n\nIf you have any feedback, please reach out to us at andrewhan@korea.ac.kr\n\n"
    },
    {
      "name": "argilla-io/argilla-haystack",
      "stars": 4,
      "img": "https://avatars.githubusercontent.com/u/18415507?s=40&v=4",
      "owner": "argilla-io",
      "repo_name": "argilla-haystack",
      "description": "A public repo that contains integrations for Argilla and Haystack.",
      "homepage": "",
      "language": "Python",
      "created_at": "2023-11-15T07:48:29Z",
      "updated_at": "2024-03-17T08:17:02Z",
      "topics": [],
      "readme": "# Argilla-Haystack\n\nArgilla is an open-source platform for data-centric LLM development. Integrates human and model feedback loops for continuous LLM refinement and oversight.\n\nWith Argilla's Python SDK and adaptable UI, you can create human and model-in-the-loop workflows for:\n\n- Supervised fine-tuning\n- Preference tuning (RLHF, DPO, RLAIF, and more)\n- Small, specialized NLP models\n- Scalable evaluation.\n\n## Getting Started\n\nYou first need to install argilla and argilla-haystack as follows:\n\n```bash\npip install argilla argilla-haystack[\"haystack-v1\"]\n```\n\nYou will need to an Argilla Server running to monitor the LLM. You can either install the server locally or have it on HuggingFace Spaces. For a complete guide on how to install and initialize the server, you can refer to the [Quickstart Guide](https://docs.argilla.io/en/latest/getting_started/quickstart_installation.html). \n\n## Usage\n\nYou can use your Haystack agent with Argilla with just a simple step. After the agent is created, we will need to call the handler to log the data into Argilla. \n\nLet us create a simple pipeline with a conversational agent. Also, we will use GPT3.5 from OpenAI as our LLM. For this, you will need a valid API key from OpenAI. You can have more info and get one via [this link](https://openai.com/blog/openai-api).\n\nAfter you get your API key, let us import the key.\n\n```python\nimport os\nfrom getpass import getpass\n\nopenai_api_key = os.getenv(\"OPENAI_API_KEY\", None) or getpass(\"Enter OpenAI API key:\")\n```\n\nWith the code snippet below, let us create the agent.\n\n```python\nfrom haystack.nodes import PromptNode\nfrom haystack.agents.memory import ConversationSummaryMemory\nfrom haystack.agents.conversational import ConversationalAgent\n\n# Define the node with the model\nprompt_node = PromptNode(\n    model_name_or_path=\"gpt-3.5-turbo-instruct\", api_key=openai_api_key, max_length=256, stop_words=[\"Human\"]\n)\nsummary_memory = ConversationSummaryMemory(prompt_node)\nconversational_agent = ConversationalAgent(prompt_node=prompt_node, memory=summary_memory)\n```\n\nLet us import the ArgillaCallback and run it. Note that the dataset with the given name will be pulled from Argilla server. If the dataset does not exist, it will be created with the given name.\n\n```python\nfrom argilla_haystack import ArgillaCallback\n\napi_key = \"argilla.apikey\"\napi_url = \"http://localhost:6900/\"\ndataset_name = \"conversational_ai\"\n\nArgillaCallback(agent=conversational_agent, dataset_name=dataset_name, api_url=api_url, api_key=api_key)\n```\n\nNow, let us run the agent to obtain a response. The prompt given and the response obtained will be logged in to Argilla server.\n\n```python\nconversational_agent.run(\"Tell me three most interesting things about Istanbul, Turkey\")\n```\n\n![Alt text](docs/images/argilla-haystack-agent.png)\n\n## Other Use Cases\n\nPlease refer to this [notebook](https://github.com/argilla-io/argilla-haystack/blob/feat/1-feature-create-argillacallback-for-haystack/docs/use_argilla_callback_in_haystack-v1.ipynb) for a more detailed example.\n"
    },
    {
      "name": "cuya26/hbd-demo",
      "stars": 4,
      "img": "https://avatars.githubusercontent.com/u/105236184?s=40&v=4",
      "owner": "cuya26",
      "repo_name": "hbd-demo",
      "description": null,
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2022-06-15T13:38:08Z",
      "updated_at": "2024-06-26T13:50:04Z",
      "topics": [],
      "readme": "# HBD-Data-Project\n## Working Group 1: Text Analysis\n### HBD-Demo\n\nThis demo contains NLP models and system studied by the members of the working group 1 of the Health Big Data project\n\n## Instruction for Test Installation\n\n### Requirements\n- 16GB RAM\n- 40GB Solid Disk\n\n### Installation with Ubuntu\n- Install the last version of Docker([How to install](https://docs.docker.com/engine/install/ubuntu/))\n- Install the last version of Docker-Compose([How to install](https://docs.docker.com/compose/install/linux/))\n- Open terminal and clone the repo using the command ```git clone https://github.com/cuya26/hbd-demo```\n- Move into the cloned folder using the command ```cd hbd-demo/```\n- Modify in file axios.js the variable server_ip with the ip address of the machine that runs the demo\n- Run the command ```docker-compose -f docker-compose-deploy.yml up -d``` to start the demo\n\n#### Load document into patient search engine\n- Copy your documents into ```patient-elastic-search/data/documents``` folder\n- Install Python\n- Move into the patient-elastic-search folder\n- Install the python requirements with the command ```pip install -r requirements.txt```\n- Move into the folder ```scripts```\n- Run the script to load your documents ```python giuseppe_load_documents.py```\n\n\n\n### Usage\n- Open a browser with ```ip_of_the_demo_machine:51118```\n\n## Instruction for Development\n\n### Installation\n\n- Run ```docker-compose up -d```\n- Load the documents in the elastic-search engine:\n    - Put the documents in the ```patient-elastic-search/data/documents```\n    - Install the python requirements in ```patient-elastic-search/requirements.txt```\n    - Run the script ```patient-elastic-search/script/giuseppe_load_documents.py```\n- Look at the logs with ```docker-compose logs -f --tail=200```\n\n### Project Structure\n\n#### Frontend\n\n##### Folder Structure\n```md\nfrontend\n├── Dockerfile\n├── Dockerfile-Deploy\n├── quasar.config.js\n└── src\n    ├── boot\n    │   └── axios.js\n    ├── components\n    │   ├── ChatBot.vue\n    │   ├── DeidentificationClassic.vue\n    │   ├── MedicalInformationExtraction.vue\n    │   ├── PatientSearch.vue\n    │   ├── PharmacologicalEventExtraction.vue\n    │   └── QuestionAnswering.vue\n    ├── layouts\n    │   └── MainLayout.vue\n    └── pages\n        └── IndexPage.vue\n```\n##### Description of the files\n- **DockerFile**: is used to buid the image for development in ```hbd-demo/docker-compose.yml```\n- **DockerFile-Deploy**: is used to buid the image for local testing. Need to be push into a docker-hub before to use it with ```hbd-demo/docker-compose-deploy.yml```\n- **quasar.config.js**: is the config file of the quasar project. It can be used to change the port of the dev server\n- **axios.js**: In this file the base urls of the backend api are defined\n- **folder-components**: Each of this Vue.js component contains the html code and js script of the output card. Each component refer to a model task\n- **MainLayout**: Contains the code of the header of the page (Page Title, Logos..)\n- **IndexPage.vue** Contains the html and script code of the demo webpage. It contains mainly the parts of the file drop and text extraction interface. This vue page calls the task components\n\n#### Backend\n\n##### Folder Structure\n\n```md\nbackend\n├── api.py\n├── config.json\n├── data\n│   └── checkpoints\n│       ├── Bio_ClinicalBERT_model_trained_Action\n│       ├── Bio_ClinicalBERT_model_trained_Actor\n│       ├── Bio_ClinicalBERT_model_trained_Certainty\n│       ├── Bio_ClinicalBERT_model_trained_disposition-type\n│       ├── Bio_ClinicalBERT_model_trained_Negation\n│       ├── Bio_ClinicalBERT_model_trained_Temporality\n│       ├── medBIT-r3-plus_75\n│       └── simplet5-epoch-6-train-loss-0.2724-val-loss-0.1477\n|\n├── Dockerfile\n├── Dockerfile-Deploy\n├── download_pretrained_models.py\n├── ita_deidentification.py\n├── model.py\n└── requirements.txt\n```\n\n##### Description of the files\n- **api.py**: is the file that runs the fastapi server and define all the api\n- **confi.json**: is a config file for ita_deidentification. It is used to initialize the anonymizer class inside ita_deidentification\n- **DockerFile**: is used to buid the image for development in ```hbd-demo/docker-compose.yml```\n- **DockerFile-Deploy**: is used to buid the image for local testing. Need to be push into a docker-hub before to use it with ```hbd-demo/docker-compose-deploy.yml```\n- **download_pretrained_models.py**: this script is use to download the models from huggingface during the building of the docker image (DockerFile and DockerFile-Deploy)\n- **ita_deidentification.py**: this is the deidentification script that use regex, spacy(NER) and stanza(NER) to de-identify documents. The api.py import the function from this file for document de-identification\n- **model.py**: this python file contains classes and functions for Question Answering, Drug Event Extraction, and the function for saliency map generation. The api.py import classes and function from it for information extraction tasks\n- **download_pretrained_models.py**: this script is use to download the models from huggingface during the building of the docker image (DockerFile and DockerFile-Deploy)\n- **requirements.txt**: contains all the requirements for the docker backend. It used to build docker image (DockerFile, DockerFile-Deploy)\n- **data/checkpoints**: this folder contains the models used for pharmacological event extraction task (Bio_Clinical_Bert*, simplet5) used in model.py and a italian-medical version of bert (medBIT-r3-plus_75) that is used for question answering in the model.py file\n\n#### llama-server\n\n##### Folder Structure\n```md\nllama-server\n├── Dockerfile\n├── Dockerfile-Deploy\n```\n\n##### Description of the files\n- **DockerFile**: is used to buid the image for development in ```hbd-demo/docker-compose.yml```\n- **DockerFile-Deploy**: is used to buid the image for local testing. Need to be push into a docker-hub before to use it with ```hbd-demo/docker-compose-deploy.yml```\n\n#### patient-elastic-search\n\n##### Folder Structure\n```md\npatient-elastic-search\n├── data\n│   └── documents\n├── notebooks\n│   └── giuseppe_init_search_engine.ipynb\n├── requirements.txt\n└── scripts\n    ├── giuseppe_load_documents.py\n    └── giuseppe_translate_docs.py\n```\n##### Description of the files\n- **data/documents**: Folder that contains the documents for information-retrieval\n- **notebooks/giuseppe_init_search_engine.ipynb**: notebook that contains code to load documents into the elastic search system and inference\n- **requirements.txt**: Python dependency needed to run the notebooks and the scripts in the folder\n- **scripts/giuseppe_load_documents.py**: this script load the documents in ```data/documents``` into the elastic-search server\n- **scripts/giuseppe_translate_docs.py**: this script translate the documents in the ```data/raw``` into italian\n\n#### patient-elastic-search\n\n##### Folder Structure\n```md\npatient-search-python-server\n├── api.py\n├── Dockerfile\n└── requirements.txt\n```\n##### Description of the files\n- **requirements.txt**: contains all the python requirements for the docker image. It used to build docker image (DockerFile, DockerFile-Deploy)\n- **DockerFile**: is used to buid the image for development in ```hbd-demo/docker-compose.yml```\n- **api.py**: fastapi server file that use the elastic search server\n"
    },
    {
      "name": "sunilkumardash9/chat-arxiv",
      "stars": 4,
      "img": "https://avatars.githubusercontent.com/u/47926185?s=40&v=4",
      "owner": "sunilkumardash9",
      "repo_name": "chat-arxiv",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2023-10-28T13:20:12Z",
      "updated_at": "2024-06-01T15:06:34Z",
      "topics": [],
      "readme": "# chat-arxiv\n\nA QA chatbot to converse with Arxiv papers.\n![workflow png](https://github.com/sunilkumardash9/chat-arxiv/blob/main/Haystack.png)\n# Tech stack\n1. Haystack for building the QA pipeline\n2. Gradio for frontend\n3. \"arxiv\" library for fetching arXiv papers.\n\n# How to run?\n1. Create a virtual environment `python -m venv my-env`\n2. `source my-env/bin/activate`\n3. Run `pip install requirements.txt`\n4. Run `gradio app_.py`\n\n"
    },
    {
      "name": "ksmin23/rag-with-haystack-and-amazon-opensearch",
      "stars": 4,
      "img": "https://avatars.githubusercontent.com/u/4167885?s=40&v=4",
      "owner": "ksmin23",
      "repo_name": "rag-with-haystack-and-amazon-opensearch",
      "description": "Question Answering Generative AI application with Large Language Models (LLMs) and Amazon OpenSearch Service using Haystack",
      "homepage": "",
      "language": "Jupyter Notebook",
      "created_at": "2023-10-01T13:42:15Z",
      "updated_at": "2024-12-04T17:21:24Z",
      "topics": [
        "generative-ai",
        "haystack",
        "haystack-pipeline",
        "llms",
        "rag",
        "sagemaker",
        "vectordb"
      ],
      "readme": "# QA with LLMs and RAG (Retrieval Augmented Generation)\n\n> :heavy_exclamation_mark: This project is a updated version based on the original project, [Haystack Retrieval-Augmented Generative QA Pipelines with SageMaker JumpStart](https://github.com/deepset-ai/haystack-sagemaker/)\n\nThis project is a Question Answering application with Large Language Models (LLMs) and Amazon OpenSearch Service. An application using the RAG(Retrieval Augmented Generation) approach retrieves information most relevant to the user’s request from the enterprise knowledge base or content, bundles it as context along with the user’s request as a prompt, and then sends it to the LLM to get a GenAI response.\n\nLLMs have limitations around the maximum word count for the input prompt, therefore choosing the right passages among thousands or millions of documents in the enterprise, has a direct impact on the LLM’s accuracy.\n\nIn this project, Amazon OpenSearch Service is used for knowledge base.\n\nThe overall architecture is like this:\n\n![rag_with_opensearch_arch](./cdk_stacks/rag_with_opensearch_arch.svg)\n\n### Overall Workflow\n\n1. Deploy the cdk stacks (For more information, see [here](./cdk_stacks/README.md)).\n   - A SageMaker Endpoint for text generation.\n   - An Amazon OpenSearch cluster for storing embeddings.\n   - Opensearch cluster's access credentials (username and password) stored in AWS Secrets Mananger as a name such as `OpenSearchMasterUserSecret1-xxxxxxxxxxxx`.\n2. Open SageMaker Studio and then open a new terminal.\n3. Run the following commands on the terminal to clone the code repository for this project:\n   ```\n   git clone https://github.com/ksmin23/rag-with-haystack-and-amazon-opensearch.git\n   ```\n4. Open `data_ingestion_to_opensearch` notebook and Run it. (For more information, see [here](./data_ingestion_to_vectordb/data_ingestion_to_opensearch.ipynb))\n5. Run Streamlit application. (For more information, see [here](./app/README.md))\n\n### References\n\n  * [Build production-ready generative AI applications for enterprise search using Haystack pipelines and Amazon SageMaker JumpStart with LLMs (2023-08-14)](https://aws.amazon.com/blogs/machine-learning/build-production-ready-generative-ai-applications-for-enterprise-search-using-haystack-pipelines-and-amazon-sagemaker-jumpstart-with-llms/)\n    * [Haystack Retrieval-Augmented Generative QA Pipelines with SageMaker JumpStart](https://github.com/deepset-ai/haystack-sagemaker/)\n  * [Build a powerful question answering bot with Amazon SageMaker, Amazon OpenSearch Service, Streamlit, and LangChain (2023-05-25)](https://aws.amazon.com/blogs/machine-learning/build-a-powerful-question-answering-bot-with-amazon-sagemaker-amazon-opensearch-service-streamlit-and-langchain/)\n  * [Use proprietary foundation models from Amazon SageMaker JumpStart in Amazon SageMaker Studio (2023-06-27)](https://aws.amazon.com/blogs/machine-learning/use-proprietary-foundation-models-from-amazon-sagemaker-jumpstart-in-amazon-sagemaker-studio/)\n  * [Build Streamlit apps in Amazon SageMaker Studio (2023-04-11)](https://aws.amazon.com/blogs/machine-learning/build-streamlit-apps-in-amazon-sagemaker-studio/)\n  * [Quickly build high-accuracy Generative AI applications on enterprise data using Amazon Kendra, LangChain, and large language models (2023-05-03)](https://aws.amazon.com/blogs/machine-learning/quickly-build-high-accuracy-generative-ai-applications-on-enterprise-data-using-amazon-kendra-langchain-and-large-language-models/)\n    * [(github) Amazon Kendra Retriver Samples](https://github.com/aws-samples/amazon-kendra-langchain-extensions/tree/main/kendra_retriever_samples)\n  * [Question answering using Retrieval Augmented Generation with foundation models in Amazon SageMaker JumpStart (2023-05-02)](https://aws.amazon.com/blogs/machine-learning/question-answering-using-retrieval-augmented-generation-with-foundation-models-in-amazon-sagemaker-jumpstart/)\n  * [Amazon OpenSearch Service’s vector database capabilities explained](https://aws.amazon.com/blogs/big-data/amazon-opensearch-services-vector-database-capabilities-explained/)\n  * [Haystack](https://docs.haystack.deepset.ai/docs) - The open source Python framework by deepset for building custom apps with large language models (LLMs).\n  * [Streamlit](https://streamlit.io/) - A faster way to build and share data apps\n  * [Improve search relevance with ML in Amazon OpenSearch Service Workshop](https://catalog.workshops.aws/semantic-search/en-US) - Module 7. Retrieval Augmented Generation\n  * [rag-with-amazon-kendra](https://github.com/ksmin23/rag-with-amazon-kendra) - Question Answering application with Large Language Models (LLMs) and Amazon Kendra\n  * [rag-with-amazon-opensearch](https://github.com/ksmin23/rag-with-amazon-opensearch) - Question Answering application with Large Language Models (LLMs) and Amazon OpenSearch Service\n  * [rag-with-postgresql-pgvector](https://github.com/ksmin23/rag-with-postgresql-pgvector) - Question Answering application with Large Language Models (LLMs) and Amazon Aurora Postgresql\n\n"
    },
    {
      "name": "beltran-oscar/ETL-pipeline-ML",
      "stars": 4,
      "img": "https://avatars.githubusercontent.com/u/84318636?s=40&v=4",
      "owner": "beltran-oscar",
      "repo_name": "ETL-pipeline-ML",
      "description": "Hacktoberfest project to create a ETL pipeline with a machine learning (ML) component",
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2023-09-25T14:11:52Z",
      "updated_at": "2024-05-20T16:26:21Z",
      "topics": [],
      "readme": "\n![Ploomber Logo](https://github.com/beltran-oscar/ETL-pipeline-ML/blob/main/images/ploomber-logo.png)\n\n\n# AirQ-Forecaster: An ETL-to-ML Pipeline for Predicting Air Quality Index\n### Hacktoberfest 2023 - Ploomber Mentorship Program\n \n\n## Description\n\nThis project focuses on creating an ETL (Extract, Transform, Load) pipeline with Ploomberfor air quality data. The pipeline fetches and processes air quality measurements from the OpenAQ API and uses DuckDB and MotherDuck for data storage and management.\n\nWe implemented the **ARIMA** (AutoRegressive Integrated Moving Average) model for time series forecasting to predict the Air Quality Index. ARIMA is a powerful and widely-used statistical method effective for short-term forecasting with data having seasonality, or cyclic patterns.\n\nKey Features of ARIMA:\n\n- **AutoRegressive (AR):** Leverages the relationship between an observation and a number of lagged observations (autoregression).\n\n- **Integrated (I):** Involves differencing the time series to make it stationary, i.e., to stabilize the mean of the time series by removing changes in the level.\n\n- **Moving Average (MA):** Models the error term as a linear combination of error terms at various times in the past.\n\nWatch a [video](https://www.youtube.com/watch?v=HEBAE54njH4) explaining our project!\n\n## Data sources\n\n**OpenAQ** - API designed for aggregating and sharing open air quality data from around the world.\n\nWe used an air quality sensor with location ID 380422 (49.208733, -122.9118), located in the city of New Westminster in British Columbia, Canada.\n\n### Parameters\n\n- `pm1 - PM1` ➡️ Particulate matter less than 1 micrometer in diameter mass concentration, µg/m³\n- `pm10 - PM10` ➡️ Particulate matter less than 10 micrometers in diameter mass concentration, µg/m³\n- `pm25 - PM2.5` ➡️ Particulate matter less than 2.5 micrometers in diameter mass concentration, µg/m³\n- `um003 - PM0.3` ➡️ count, particles/cm³\n- `um005 - PM0.5` ➡️ count, particles/cm³\n- `um010 - PM1` ➡️ count, particles/cm³\n- `um025 - PM2.5` ➡️ count, particles/cm³\n- `um050 - PM5.0` ➡️ count, particles/cm³\n- `um100 - PM10` ➡️ count, particles/cm³\n- `pressure` ➡️ Atmospheric or barometric pressure, hPa\n- `temperature` ➡️ °C\n- `humidity` ➡️ %\n\n## Methods\n\n![Flowchart](https://github.com/beltran-oscar/ETL-pipeline-ML/blob/main/images/flowchart.png)\n\n- **GitHub Actions**: The ETL process is automatically executed every hour.\n\n- **Ploomber Pipeline:** The ETL process is managed using Ploomber, a workflow management tool. The pipeline configuration can be found in `pipeline.yaml`.\n\n- **Data Extraction and Cloud Data Storage MotherDuck:** The data extraction process fetches air quality measurements from the OpenAQ API. The extraction logic is implemented in `extract_duckdb.py`. The extracted data is stored in the cloud using MotherDuck.\n\n- **Jupyter Notebooks:** The project includes Jupyter notebooks for data exploration and analysis (see `extract.ipynb`).\n\n- **Docker Integration:** The project is containerized using Docker, allowing for easy setup and deployment. The Dockerfile provides the necessary instructions to build the Docker image.\n\n## Dependencies\n\nSee `pyproject.toml` for all package requirements. Dependencies are managed using `poetry`.\n\n## User Interface \n\nThe Streamlit App is deployed in [Ploomber Cloud](https://ploomber.io/cloud/). You can access the app [here](https://blue-bird-7594.ploomberapp.io/) or by following this URL: https://blue-bird-7594.ploomberapp.io/\n\n\n![Streamlit App](https://github.com/beltran-oscar/ETL-pipeline-ML/blob/main/images/app-streamlit.gif)\n\n## Authors\n\nAlejandro Leiva - [aleivaar94](https://github.com/aleivaar94)\n\nOscar Beltrán - [beltran-oscar](https://github.com/beltran-oscar)\n\n## Acknowledgments\n\nWe want to thank the [Ploomber](https://ploomber.io/) Team for their time and dedicated mentorship during the development of this project. Special mention to [Laura Funderburk](https://github.com/lfunderburk) - Developer Advocate at Ploomber, for her patience and dedication to guide all mentees.\n\nWe also want to thank [Eduardo Blancas](https://github.com/edublancas) - Co-founder/CEO at Ploomber for this mentorship opportunity.\n\n## License\nThe project is licensed under the Apache 2.0 License."
    },
    {
      "name": "rubenkruiper/IRReC",
      "stars": 4,
      "img": "https://avatars.githubusercontent.com/u/7871378?s=40&v=4",
      "owner": "rubenkruiper",
      "repo_name": "IRReC",
      "description": "IRReC: Information Retrieval over building regulations, part of the Intelligent Regulatory Compliance (iReC) project at Northumbria University.",
      "homepage": null,
      "language": "HTML",
      "created_at": "2023-06-08T16:01:53Z",
      "updated_at": "2024-11-20T19:09:59Z",
      "topics": [],
      "readme": "# Information Retrieval for Regulatory Compliance (IRReC)\n\n---\nThe Information Retrieval for Regulatory Compliance (IRReC) provides an experimental search engine to search over building regulations. Due to licensing restrictions you will have to provide your own regulations in .pdf format. This repository contains code and some of the data for the IRReC system, published at [EG-ICE 2023](https://www.ucl.ac.uk/bartlett/construction/research/virtual-research-centres/institute-digital-innovation-built-environment/30th-eg-ice). The paper can be found [here](https://www.ucl.ac.uk/bartlett/construction/sites/bartlett_construction/files/document_and_query_expansion_for_information_retrieval_on_building_regulations.pdf).\n\nThe IR system and experiments are all written in python. Instructions for preparing the system, running experiments and running a local front end to call the system API are provided in each of the corresponding folders. \n\nWe hope that our data and code will help you research Information Retrieval in the domain of Architecture, Engineering and Construction. Note that the code in this project for Named Entity Recognition (NER) and automatically creating a span-based Knowledge Graph (KG) has been published separately as well, see:\n* [SPaR.txt shallower parser for NER](https://github.com/rubenkruiper/SPaR.txt)\n* [iReC KG from building regulations](https://github.com/rubenkruiper/irec)\n\nIf you use our work in one of your projects, please consider referencing our paper:\n\n```\n@inproceedings{Kruiper2023_IRReC,\n   title = \"Document and Query Expansion for Information Retrieval on Building Regulations\",\n    author = \"Kruiper, Ruben  and\n      Konstas, Ioannis  and\n      Gray, Alasdair J.G.  and\n      Sadeghineko, Farhad  and\n      Watson, Richard  and\n      Kumar, Bimal\",\n    month = jul,\n    year = \"2023\",\n    keywords= \"Information Retrieval,Building Regulations,Query Expansion,Document Expansion\",\n}\n``` \n\n\n**Figure:** Overview of approach, also see our [paper](https://www.ucl.ac.uk/bartlett/construction/sites/bartlett_construction/files/document_and_query_expansion_for_information_retrieval_on_building_regulations.pdf):\n![alt text](https://github.com/rubenkruiper/irrec/blob/main/IR_approach.jpeg?raw=true)\n\nThe code and data in this repository are licensed under a Creative Commons Attribution 4.0 License.\n<img src=\"https://mirrors.creativecommons.org/presskit/buttons/88x31/png/by-sa.png\" width=\"134\" height=\"47\">"
    },
    {
      "name": "datasciencecampus/consultation_nlp",
      "stars": 4,
      "img": "https://avatars.githubusercontent.com/u/25666867?s=40&v=4",
      "owner": "datasciencecampus",
      "repo_name": "consultation_nlp",
      "description": "Preliminary analysis for 2023 population transformation consultation",
      "homepage": null,
      "language": "Python",
      "created_at": "2023-06-14T11:01:31Z",
      "updated_at": "2023-10-18T14:39:56Z",
      "topics": [],
      "readme": "<img src=\"https://github.com/datasciencecampus/awesome-campus/blob/master/ons_dsc_logo.png\">\n\n# `consultation_nlp`\n[![Stability](https://img.shields.io/badge/stability-experimental-orange.svg)](https://github.com/mkenney/software-guides/blob/master/STABILITY-BADGES.md#experimental)\n[![codecov](https://codecov.io/gh/datasciencecampus/consultation_nlp/branch/main/graph/badge.svg?token=bvdkp2cCG8)](https://codecov.io/gh/datasciencecampus/consultation_nlp)\n[![Twitter](https://img.shields.io/twitter/url?label=Follow%20%40DataSciCampus&style=social&url=https%3A%2F%2Ftwitter.com%2FDataSciCampus)](https://twitter.com/DataSciCampus)\n\nPython code for preliminary natural language processing analysis for 2023 population transformation\nconsultation\n\n```{warning}\nWhere this documentation refers to the root folder we mean where this README.md is\nlocated.\n```\n\n## Getting started\n\nTo start using this project, [first make sure your system meets its\nrequirements](requirements.txt).\n\nIt's suggested that you install this pack and it's requirements within a virtual environment.\n\n### Cloning the repo\nTo clone the repo, open command promt and navigate to the directory you want to save the repo to and call:\n`git clone https://github.com/datasciencecampus/consultation_nlp.git`\n\n### Pre-commit actions\nThis repository contains a configuration of pre-commit hooks. These are language agnostic and focussed on repository security (such as detection of passwords and API keys). If approaching this project as a developer, you are encouraged to install and enable `pre-commits` by running the following in your shell:\n   1. Install `pre-commit`:\n\n      ```\n      pip install pre-commit\n      ```\n   2. Enable `pre-commit`:\n\n      ```\n      pre-commit install\n      ```\n\n### Installing the package (Python Only)\n\nWhilst in the root folder, in the command prompt, you can install the package and it's dependencies\nusing:\n\n```shell\npython -m pip install -U pip setuptools\npip install -e .\n```\nor use the `make` command:\n```shell\nmake install\n```\n\nThis installs an editable version of the package. Meaning, when you update the\npackage code, you do not have to reinstall it for the changes to take effect.\n(This saves a lot of time when you test your code)\n\nRemember to update the setup and requirement files inline with any changes to your\npackage. The inital files contain the bare minimum to get you started.\n\n### Running the pipeline (Python only)\n\nThe entry point for the pipeline is stored within the package and called `run_pipeline.py`.\nTo run the pipeline, run the following code in the terminal (whilst in the root directory of the\nproject).\n\n```shell\npython src/run_pipeline.py\n```\n\nAlternatively, most Python IDE's allow you to run the code directly from the IDE using a `run` button.\n\n\n### Running the streamlit app (dashboard)\n\n1) Ensure all requirements are downloaded from the requirements.txt by openning up the shell terminal (anaconda prompt) and running:\n```shell\npip install -r requirements.txt\n```\n2) Keep the shell terminal open and navigate to the directory where this code is saved and run:\n```shell\nstreamlit run streamlit_app.py\n```\n## Licence\n\nThis codebase is released under the MIT License. This covers both the codebase and any sample code in\nthe documentation. The documentation is ©Crown copyright and available under the terms of the\nOpen Government 3.0 licence.\n\n## Contributing\n\n[If you want to help us build, and improve `consultation_nlp`, view our\ncontributing guidelines](docs/contributor_guide/CONTRIBUTING.md).\n\n### Requirements\n\n[```Contributors have some additional requirements!```](docs/contributor_guide/CONTRIBUTING.md)\n\n- Python 3.6.1+ installed\n- a `.secrets` file with the [required secrets and\n  credentials](#required-secrets-and-credentials)\n- [load environment variables][docs-loading-environment-variables] from `.env`\n\nTo install the contributing requirements, open your terminal and enter:\n```shell\npython -m pip install -U pip setuptools\npip install -e .[dev]\npre-commit install\n```\nor use the `make` command:\n```shell\nmake install_dev\n```\n\n## Acknowledgements\n\n[This project structure is based on the `govcookiecutter` template\nproject][govcookiecutter].\n\n[contributing]: https://github.com/best-practice-and-impact/govcookiecutter/blob/main/%7B%7B%20cookiecutter.repo_name%20%7D%7D/docs/contributor_guide/CONTRIBUTING.md\n[govcookiecutter]: https://github.com/best-practice-and-impact/govcookiecutter\n[docs-loading-environment-variables]: https://github.com/best-practice-and-impact/govcookiecutter/blob/main/%7B%7B%20cookiecutter.repo_name%20%7D%7D/docs/user_guide/loading_environment_variables.md\n[docs-loading-environment-variables-secrets]: https://github.com/best-practice-and-impact/govcookiecutter/blob/main/%7B%7B%20cookiecutter.repo_name%20%7D%7D/docs/user_guide/loading_environment_variables.md#storing-secrets-and-credentials\n"
    },
    {
      "name": "XpiritBV/Generative-AI-PoC",
      "stars": 4,
      "img": "https://avatars.githubusercontent.com/u/9567984?s=40&v=4",
      "owner": "XpiritBV",
      "repo_name": "Generative-AI-PoC",
      "description": null,
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2023-05-03T09:18:18Z",
      "updated_at": "2024-09-04T16:07:38Z",
      "topics": [],
      "readme": "[\n    ![Open in Remote - Containers](\n        https://img.shields.io/static/v1?label=Remote%20-%20Containers&message=Open&color=blue&logo=visualstudiocode\n    )\n](\n    https://vscode.dev/redirect?url=vscode://ms-vscode-remote.remote-containers/cloneInVolume?url=https://github.com/XpiritBV/Generative-AI-PoC\n)\n\n# Definitions\n\n## Smart\n\nWhen someone knows a lot about a subject but doesn't have any experience using that information.\n\n## Wise\n\nWhen someone knows a lot about a subject but also has experience applying that knowledge in real life.\n\n## Ctrl + F\n\nWhen we talk about Ctrl + F we're talking about a model that is only smart but not wise.\nThe model is like a representation of a piece of knowledge, without any context.\n\n# Scope of PoC v1\n\nOur first goal is to create a PoC that follows the Crl + F model.  \nOur data set will be a book in .pdf format.\n\n## What is out of scope?\n\n- We don't want to spend a lot of time in data preprocessing.\n\n## Technical\n\n![llm-chatbot-embedding-database](https://user-images.githubusercontent.com/7449547/235882195-766d157f-90e7-4f1f-abaa-08131b36cef4.jpg)\n[Source of image](https://bdtechtalks.com/2023/05/01/customize-chatgpt-llm-embeddings/)  \nWe will be looking in the direction of Azure to find services that can help us achieve our PoC.  \n\n## Use cases\n\nIn what way can PoC v1 provide real value to people?\n\n# Considerations\n\nBecause the Azure OpenAI service only accepts English, we will be limited to only use English.\nMaximum amount of 2048 tokens are accepted by the OpenAI embeddings API.\n\n# Investigation 1\n\nIn trying to understand how we can expose large sets of data to an LLM without exceeding its token limit, we formulated the following hypothesis.\n\nIn the example of a book, we generate embedding vectors for the contents of this book and use a model, in this case 'text-embedding-ada-002', respecting the model's limitation of 2048 tokens (around 2 to 3 pages of text), and replace line endings with spaces. We then store the resulting embedding vectors in a vector database; we are still looking for which one. As our understanding now stands, the context of a question to the model is limited. These embedding vectors offer a shortcut where we can limit the context of the entire book to just the parts that match the generated vectors of the question asked. After which, we can add the required context to the LLM to answer the question."
    },
    {
      "name": "recrudesce/haystack_lemmatize_node",
      "stars": 4,
      "img": "https://avatars.githubusercontent.com/u/6450799?s=40&v=4",
      "owner": "recrudesce",
      "repo_name": "haystack_lemmatize_node",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2023-04-05T18:23:12Z",
      "updated_at": "2023-09-04T00:33:29Z",
      "topics": [],
      "readme": "## Lemmatization\n\nLemmatization is a text pre-processing technique used in natural language processing (NLP) models to break a word down to its root meaning to identify similarities. For example, a lemmatization algorithm would reduce the word better to its root word, or lemme, good.\n\nThis node can be placed within a pipeline to lemmatize documents returned by a Retriever, prior to adding them as context to a prompt (for a PromptNode or similar).\nThe process of lemmatizing the document content can potentially reduce the amount of tokens used by up to 30%, without drastically affecting the meaning of the document.\n\n![image](https://user-images.githubusercontent.com/6450799/230403871-d0299748-977c-4c9e-9d70-914d8ff2bf3b.png)\n\n### Before Lemmatization:\n![image](https://user-images.githubusercontent.com/6450799/230404198-a3ed6382-03b8-4ec6-b88d-4232560752f8.png)\n\n### After Lemmatization:\n![image](https://user-images.githubusercontent.com/6450799/230404246-a8488a57-73bd-4420-9f1b-8a080b84121b.png)\n\n## Installation\n\n`pip install haystack-lemmatize-node`\n\n## Usage\n\nInclude it in your pipeline - example as follows:\n\n```python\nimport logging\nimport re\n\nfrom datasets import load_dataset\nfrom haystack.document_stores import InMemoryDocumentStore\nfrom haystack.nodes import PromptNode, PromptTemplate, AnswerParser, BM25Retriever\nfrom haystack.pipelines import Pipeline\nfrom haystack_lemmatize_node import LemmatizeDocuments\n\n\nlogging.basicConfig(format=\"%(levelname)s - %(name)s -  %(message)s\", level=logging.WARNING)\nlogging.getLogger(\"haystack\").setLevel(logging.INFO)\n\ndocument_store = InMemoryDocumentStore(use_bm25=True)\n\ndataset = load_dataset(\"bilgeyucel/seven-wonders\", split=\"train\")\ndocument_store.write_documents(dataset)\n\nretriever = BM25Retriever(document_store=document_store, top_k=2)\n\nlfqa_prompt = PromptTemplate(\n    name=\"lfqa\",\n    prompt_text=\"Given the context please answer the question using your own words. Generate a comprehensive, summarized answer. If the information is not included in the provided context, reply with 'Provided documents didn't contain the necessary information to provide the answer'\\n\\nContext: {documents}\\n\\nQuestion: {query} \\n\\nAnswer:\",\n    output_parser=AnswerParser(),\n)\n\nprompt_node = PromptNode(\n    model_name_or_path=\"text-davinci-003\",\n    default_prompt_template=lfqa_prompt,\n    max_length=500,\n    api_key=\"sk-OPENAIKEY\",\n)\n\nlemmatize = LemmatizeDocuments() # you can pass the `base_lang=XX` argument here too, where XX is a language as listed here: https://pypi.org/project/simplemma/\n\npipe = Pipeline()\npipe.add_node(component=retriever, name=\"Retriever\", inputs=[\"Query\"])\npipe.add_node(component=lemmatize, name=\"Lemmatize\", inputs=[\"Retriever\"])\npipe.add_node(component=prompt_node, name=\"prompt_node\", inputs=[\"Lemmatize\"])\n\nquery = \"What does the Rhodes Statue look like?\"\n  \noutput = pipe.run(query)\n\nprint(output['answers'][0].answer)\n```\n\n## Caveats\nSometimes lemmatization can be slow for large document content, but in the world of AI where we can potentially wait 30+ seconds for an LLM to respond (hello GPT-4), what's a couple more seconds?\n"
    },
    {
      "name": "ByteSpiritGit/fdet",
      "stars": 4,
      "img": "https://avatars.githubusercontent.com/u/125552933?s=40&v=4",
      "owner": "ByteSpiritGit",
      "repo_name": "fdet",
      "description": " fDet is a web app that uses classification to verify claims.",
      "homepage": "",
      "language": "Svelte",
      "created_at": "2023-03-07T20:49:04Z",
      "updated_at": "2023-11-27T16:51:20Z",
      "topics": [
        "django",
        "fastapi",
        "gpt-35-turbo",
        "llama2",
        "machine-learning",
        "svelte",
        "vite"
      ],
      "readme": "# fDet\n\nfDet is a website application for fake-news detection using modified RoBerta AI and GPT 3.5 Turbo. It combines the power of these AI models to provide accurate detection of fake news.\n\n## Getting Started\n\nTo set up the project locally, follow these instructions:\n\n### Virtual Environment\n\nCreate a virtual environment and activate it using the following commands:\n```cmd\n~\\AppData\\Local\\Programs\\Python\\Python39\\python -m venv venv\nvenv\\Scripts\\activate\n```\n\n\n###  Install Python Dependencies\nInstall the required Python packages using the requirements.txt file:\n```cmd\npip install -r requirements.txt\n```\n\n\n### Run Django\nTo run the Django server, execute the following commands:\n```cmd\ncd ./frontend\npython manage.py runserver\ncd ../\n```\n\n### Run FastAPI\n\nTo run the FastAPI server, execute the following commands:\n```cmd\ncd ./backend\npython api.py\ncd ../\n```\n### Star History\n[![Star History Chart](https://api.star-history.com/svg?repos=ByteSpiritGit/fdet&type=Date)](https://star-history.com/#ByteSpiritGit/fdet&Date)\n\n\n### Screenshots\n\n![Screenshot 1](fdet-index.png)\n*Screenshot 1: Example of fDet website index page.*\n\n![Screenshot 2](fdet-eval.png)\n*Screenshot 2: Another example of fDet website interface.*\n"
    },
    {
      "name": "kmcleste/oracle-of-ammon",
      "stars": 4,
      "img": "https://avatars.githubusercontent.com/u/47370125?s=40&v=4",
      "owner": "kmcleste",
      "repo_name": "oracle-of-ammon",
      "description": "CLI utility for creating Search APIs",
      "homepage": "",
      "language": "Python",
      "created_at": "2023-01-03T03:40:39Z",
      "updated_at": "2023-08-25T01:22:37Z",
      "topics": [
        "cli",
        "fastapi",
        "haystack",
        "nlp",
        "question-answering",
        "search",
        "typer"
      ],
      "readme": "# Oracle of Ammon\n\n[![PyPI version shield](https://img.shields.io/pypi/v/oracle-of-ammon?color=blue&style=flat-square)](https://pypi.org/project/oracle-of-ammon/)\n[![Python version shield](https://img.shields.io/pypi/pyversions/oracle-of-ammon?color=blue&style=flat-square)](https://pypi.org/project/oracle-of-ammon/)\n[![MIT License](https://img.shields.io/github/license/kmcleste/oracle-of-ammon?style=flat-square)](https://github.com/kmcleste/oracle-of-ammon/blob/main/LICENSE)\n\nA simple CLI tool for creating Search APIs.\n\n## Installation\n\nCreating a virtual environment is highly recommended. To do so, run:\n\n```bash\npython3 -m venv .venv\nsource .venv/bin/activate\n```\n\nOnce your environment is active, simply install the package with:\n\n```bash\npip install oracle-of-ammon\n```\n\n## Usage\n\nTo get started, checkout the help menu:\n\n```bash\noracle-of-ammon --help\n```\n\n[![Image of oracle-of-ammon cli help documentaiton](https://github.com/kmcleste/oracle-of-ammon/blob/main/images/oracle-of-ammon-help.gif?raw=true)](https://github.com/faressoft/terminalizer)\n\nHere, you will see we currently have two options: **summon** and **locust**.\n\n### Summon\n\nBy default, Summon is configured to initialize an empty search service on port 8000. The API framework used is [FastAPI](https://fastapi.tiangolo.com/) and the underlying search engine is built on [Haystack](https://docs.haystack.deepset.ai/). If you would like to initialize the search service with documents upon startup, provide a filepath with the `--path` option. Once the service has been initialized, you can view the API docs at [http://127.0.0.1:8000/docs](http://127.0.0.1:8000/docs). A static version of the swagger documentation can also be found [here](https://petstore.swagger.io/?url=https://raw.githubusercontent.com/kmcleste/oracle-of-ammon/main/openapi.json#/).\n\n| Option        | Type | Default         | Description                                                                                                                         |\n| ------------- | ---- | --------------- | ----------------------------------------------------------------------------------------------------------------------------------- |\n| --path        | TEXT | None            | Filepath used to pre-index document store.                                                                                          |\n| --sheet-name  | TEXT | None            | If using an excel file, select which sheet(s) to load. If none provided, all sheets will be loaded. Expects a comma-separated list. |\n| --title       | TEXT | Oracle of Ammon | API documentation title.                                                                                                            |\n| --index       | TEXT | document        | Default index name.                                                                                                                 |\n| --faq         | BOOL | TRUE            | Selector for content preloaded into document store.                                                                                 |\n\nSupported Filetypes:\n\n- FAQ: CSV, TSV, JSON, XLSX, TXT\n- Semantic: TXT\n\nSee the [`data`](https://github.com/kmcleste/oracle-of-ammon/tree/main/oracle_of_ammon/data) directory for examples of accepted files.\n\n[![Oracle of Ammon CLI - Summon](https://github.com/kmcleste/oracle-of-ammon/blob/main/images/oracle-of-ammon-summon.gif?raw=true)](https://github.com/faressoft/terminalizer)\n\n### Locust\n\n[Locust](https://locust.io/) is an open source tool for load testing. You're able to swarm your system with millions of simultaneous users -- recording service performance and other metrics. By default, Locust will start on port 8089. To start a new load test, simply enter the number of users you want to simulate, their spawn rate, and the host address to swarm.\n\n[![Image of locust config](https://github.com/kmcleste/oracle-of-ammon/blob/main/images/locust-config.png?raw=true)](https://locust.io)]\n\n## Coming Eventually 👀\n\n- ~~Semantic search~~\n- ~~Document search~~\n- ~~Document summarization~~\n- Document ranking\n- ~~Multiple index support~~\n- Annotations/Feedback\n- Fine tuning\n- Additional locust endpoints\n- Dynamic Locust config\n- Custom pipelines\n- Dedicated docs wiki\n"
    },
    {
      "name": "KhalilMrini/Medical-Question-Answering",
      "stars": 4,
      "img": "https://avatars.githubusercontent.com/u/11042966?s=40&v=4",
      "owner": "KhalilMrini",
      "repo_name": "Medical-Question-Answering",
      "description": "Repository for the COLING 2022 paper on Medical Question Understanding and Answering.",
      "homepage": "",
      "language": "Python",
      "created_at": "2022-09-14T23:26:26Z",
      "updated_at": "2024-11-12T20:05:47Z",
      "topics": [],
      "readme": "# Medical Question Understanding\n\n## 1. Paper\n\nThis repository contains the code for the following paper:\n\n<li><b><i>\"Medical Question Understanding and Answering with Knowledge Grounding and Semantic Self-Supervision\"</i></b> <img height=\"16\" src=\"https://khalilmrini.github.io/images/nih.png\" width=\"24\" style=\"display: inline-block;\"/> <img height=\"16\" src=\"https://khalilmrini.github.io/images/adobe.png\" width=\"16\" style=\"display: inline-block;\"/> <br>\n<b>Khalil Mrini</b>, Harpreet Singh, Franck Dernoncourt, Seunghyun Yoon, Trung Bui, Walter Chang, Emilia Farcas, Ndapa Nakashole<br>\nCOLING 2022<br>\n<i><img height=\"16\" src=\"https://khalilmrini.github.io/images/flag-kr.jpeg\" width=\"24\" style=\"display: inline-block;\"/> Gyeongju, Republic of Korea, and Online</i></li>\n\n## 2. Installation\n\nUse the following commands to install the requirements for this repository:\n\n```\npip install -r requirements.txt\n```\n\n## 3. Training Commands\n\nThe ```main.py``` file contains all the training and testing modes.\n\nFor example, use ```python3 main.py train_meqsum``` to train a model for the MeQSum dataset.\n\n## 4. Chatbot\n\nYou may train a summarizer for the GAR baseline by running the ```train_summarizer_for_gar.py``` file.\n\nRun the ```app_chatbot.py``` file to start the Chatbot and evaluate answers. The DPR and GAR take about 40 to 50 seconds to retrieve an answer, but our model should be near immediate to retrieve one.\n"
    },
    {
      "name": "PacktPublishing/Building-Neo4j-Powered-Applications-with-LLMs",
      "stars": 4,
      "img": "https://avatars.githubusercontent.com/u/10974906?s=40&v=4",
      "owner": "PacktPublishing",
      "repo_name": "Building-Neo4j-Powered-Applications-with-LLMs",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2024-08-30T11:43:48Z",
      "updated_at": "2025-04-23T06:34:48Z",
      "topics": [],
      "readme": "# Building Neo4j-Powered Applications with LLMs\n"
    },
    {
      "name": "2456868764/LiteRAG",
      "stars": 4,
      "img": "https://avatars.githubusercontent.com/u/108045855?s=40&v=4",
      "owner": "2456868764",
      "repo_name": "LiteRAG",
      "description": "LiteRAG 是一个基于 langchain + OPEA 打造的轻量化支持中英文多知识库智能客服系统。",
      "homepage": null,
      "language": "Python",
      "created_at": "2024-12-18T12:54:42Z",
      "updated_at": "2025-04-17T08:55:53Z",
      "topics": [],
      "readme": "# 什么是 LiteRAG\n\nLiteRAG 是一个基于 langchain + OPEA 打造的轻量化支持中英文多知识库智能客服系统。\n\n# 一、目标\n\n1. 支持多个知识库创建、删除。\n2. 每个知识库支持多格式文件上传：能够分析并存储 docx， pptx， pdf 等文件，基于 OCR 识别文件中图片。\n3. 每个知识库支持网站信息爬取：能自动爬取和公共网站中的信息。\n4. 中英文语言支持：支持中文和英文问答和知识检索。\n5. 轻量化和模块化的设计，便于后续的扩展。\n\n\n<img src=\"./docs/images/kb.png\" width=\"800\" height=\"400\" />\n\n\n\n# 二、架构\n## 1. RAG 应用程序的典型工作流程\n\n<img src=\"./docs/images/rag.png\" width=\"800\" height=\"400\" />\n\n## 2. LiteRAG  整体架构\n\n<img src=\"./docs/images/literag.png\" width=\"800\" height=\"400\" />\n\n整体架构在 OPEA ChatQnA Examples 基础上主要调整如下：\n\nDataPrepare MicroService:\n\n- 集成 Milvus 向量数据库\n- 知识库管理包括创建、删除、清除等，集成 Sqlite 数据库\n- 支持多格式文件 docx， pptx， pdf 等文件上传\n- 爬取网站网页任务队列 和 数据处理线程\n\nChatQnA Gateway:\n- `ChatCompletionRequest` 和 `RetrieverParms` 中增加 `knowledge_name` 知识库名称参数\n\nRetriever MicroService:\n- 支持知识库检索\n\n# 三、项目状态\n\n## 项目成员\n\n- 团队： 2456868764\n- 成员： Jun， 独立开发者\n\n## 项目进度\n\n- 整体框架搭建（完成）\n- 支持多个知识库创建、删除（完成）\n- 每个知识库支持多格式文件上传 (完成)\n- 每个知识库支持网站信息爬取 （完成）\n- 中英文语言支持 （模型完成测试）\n\n# 四、功能与设计\n\n## 1. File Loader\n\n新增 CustomizedOcrDocLoader，CustomizedPyMuPDFLoader，CustomizedPPTXLoader, CustomizedWebBaseLoader 用于支持 docx， pptx， pdf 等文件加载和 网页加载，同时基于 RapidOCR 设别文件中图片。\n\n<img src=\"./docs/images/fileloader.png\" width=\"800\" height=\"400\" />\n\n加载配置如下：\n\n```python\nDOCUMENTS_LOADER_MAPPING = {\n    \"CustomizedPyMuPDFLoader\": [\".pdf\"],\n    \"UnstructuredFileLoader\": [\".txt\"],\n    \"CustomizedOcrDocLoader\": [\".docx\"],\n    \"CustomizedPPTXLoader\": [\".pptx\"],\n    \"UnstructuredPowerPointLoader\": [\".ppt\"],\n    \"CustomizedWebBaseLoader\": [\".html\", \".htm\"],\n}\n```\n\n代码文件如下:\n\n```shell\ntree rag/module/indexing/loader\n\nrag/module/indexing/loader\n├── __init__.py\n├── doc_loader.py\n├── ocr.py\n├── pdf_loader.py\n└── pptx_loader.py\n└── web_loader.py\n```\n\n## 2. Text Splitter\n\n\n支持自定义文本分割，同时支持中文文档分割，以及自定义文本分割长度。\n\n- SPLITTER_NAME：分割器名称，可以是自定义的文本分割器，默认的文本分割器是 `ChineseRecursiveTexSplitter` 。\n- CHUNK_SIZE：每个文本块的最大长度（以字符为单位）。\n- CHUNK_OVERLAP：相邻文本块之间的重叠部分长度（以字符为单位）。\n- SMALLER_CHUNK_SIZE：小文档最小长度（以字符为单位），大文档可以分割为更多小文档， 可以根据小文档召回大文档，实现父子文档的召回。\n\n\n在支持 `langchain` 支持分割器，同时开发中文文文本分割器包括： `ChineseRecursiveTexSplitter` 和 `ChineseTextSplitter` 。\n\n<img src=\"./docs/images/splitter.png\" width=\"800\" height=\"400\" />\n\n\n加载配置如下：\n\n```python\nDOCUMENTS_SPLITER_MAPPING = {\n    \"ChineseTextSplitter\": ChineseTextSplitter,\n    \"ChineseRecursiveTextSplitter\": ChineseRecursiveTextSplitter,\n    \"LatexTextSplitter\": LatexTextSplitter,\n    \"MarkdownTextSplitter\": MarkdownTextSplitter,\n    \"MarkdownHeaderTextSplitter\": MarkdownHeaderTextSplitter,\n    \"PythonCodeTextSplitter\": PythonCodeTextSplitter,\n    \"NLTKTextSplitter\": NLTKTextSplitter,\n    \"RecursiveCharacterTextSplitter\": RecursiveCharacterTextSplitter,\n    \"SentenceTransformersTokenTextSplitter\": SentenceTransformersTokenTextSplitter,\n    \"SpacyTextSplitter\": SpacyTextSplitter\n}\n```\n\n代码文件如下:\n\n```shell\ntree rag/module/indexing/splitter\n\nrag/module/indexing/splitter\n├── __init__.py\n├── chinese_recursive_text_splitter.py\n└── chinese_text_splitter.py\n\n```\n\n## 3. Embedding\n\n支持多模型设计，包括：`OpenAIEmbeddings`, `HuggingFaceEndpointEmbeddings` 和 `BAAI/bge-base-en-v1.5` 等本地模型。\n\n<img src=\"./docs/images/embedding.png\" width=\"800\" height=\"400\" />\n\n加载 `embedding` 模型代码设计如下：\n\n```python\n\n@lru_cache\ndef get_embedding_model(embedding_type, mosec_embedding_model, \nmosec_embedding_endpoint, tei_embedding_endpoint, \nlocal_embedding_model) -> Embeddings:\n    \"\"\"Create the embedding model.\"\"\"\n    if embedding_type == \"MOSEC\":\n        return MosecEmbeddings(model=mosec_embedding_model)\n    elif embedding_type == \"TEI\":\n        return HashableHuggingFaceEndpointEmbeddings(model=tei_embedding_endpoint)\n    elif embedding_type == \"LOCAL\":\n        if any([key_word in local_embedding_model for key_word in [\"bge\"]]):\n            return HuggingFaceBgeEmbeddings(model_name=local_embedding_model)\n        else:\n            return HuggingFaceEmbeddings(model_name=local_embedding_model)\n    else:\n        raise RuntimeError(\"Unable to find any supported embedding model.\")\n```\n\n代码文件如下：\n```shell\ntree rag/connector/   \n\nrag/connector/\n├── embedding\n│   ├── __init__.py\n│   ├── hashable_huggingface_endpoint.py\n│   └── mosec_embeddings.py\n├── utils.py\n\n```\n## 4. Vector Store\n\n支持多向量存储库设计，包括 Milvus 、Redis、Qdrant、Pinecone、Chroma 等。目前实现 Milvus 向量数据库。\n\n<img src=\"./docs/images/vectorstore.png\" width=\"800\" height=\"400\" />\n\n\n\nVectorStore 基类代码设计如下：\n\n```python\nfrom abc import ABC, abstractmethod\nclass VectorStore(ABC):\n    \"\"\"Abstract base class for vector store implementations.\n\n    This class defines a common interface for various vector store implementations,\n    allowing for consistent interaction with vector-based data structures across different\n    implementations. It includes methods for creating, dropping, and clearing vector stores,\n    as well as adding, deleting, updating, and searching documents within the stores.\n    \"\"\"\n\n    @abstractmethod\n    def create_vectorstore(self):\n\n    @abstractmethod\n    def drop_vectorstore(self):\n        \n    @abstractmethod\n    def clear_vectorstore(self):\n\n    @abstractmethod\n    def add_doc(self, file, docs):\n\n    @abstractmethod\n    def delete_doc(self, filename):\n\n    @abstractmethod\n    def update_doc(self, file, docs):\n\n    @abstractmethod\n    def search_docs(self, text, top_k, threshold, **kwargs):\n\n    @abstractmethod\n    def search_docs_by_vector(self, embedding, top_k, threshold, **kwargs):\n\n    @abstractmethod\n    def search_docs_by_mmr(self, text, top_k, fetch_k, lambda_mult, **kwargs):\n\n```\n\n加载向量库的代码如下：\n\n```python\n@lru_cache\ndef get_vectorstore(knowledge_name,\n                    vs_type,\n                    embedding_model\n                    ) -> VectorStore:\n    \"\"\"Get the vectorstore\"\"\"\n    vectorstore = None\n    logger.info(f\"Using {vs_type} as db to create vectorstore\")\n    if vs_type == \"milvus\":\n        vectorstore = MilvusVectorStore(embedding_model=embedding_model, collection_name=knowledge_name)\n    else:\n        raise ValueError(f\"{vs_type} vector database is not supported\")\n    logger.info(\"Vector store created\")\n    return vectorstore\n```\n\n代码文件如下：\n\n```shell\ntree rag/connector/\n\nrag/connector/\n├── utils.py\n└── vectorstore\n    ├── __init__.py\n    ├── base.py\n    └── milvus.py\n```\n\n\n## 5. Knowledge Base Database Structure Design\n\n知识库数据结构包括四个逻辑表： `knowledge_base`、`knowledge_file` 、 `file_doc` 和 `url_queue`, 其关系如下图：\n\n<img src=\"./docs/images/database.png\" width=\"800\" height=\"400\" />\n\n数据库模型和存储操作代码如下：\n\n```python\ntree rag/connector/database\n\nrag/connector/database\n├── __init__.py\n├── base.py\n├── models\n│   ├── __init__.py\n│   ├── base.py\n│   ├── knowledge_base_model.py\n│   └── knowledge_file_model.py\n│   └── url_queue_model.py\n├── service\n│   ├── __init__.py\n│   ├── knowledge_file_service.py\n│   └── knowledge_service.py\n│   └── url_queue_service.py\n└── session.py\n```\n\n# 五、Embedding & Rerank & LLM Model\n\n模型选择包括 Embedding、Rerank、LLM 三种模型。选择模型主要参考以下因素，同时实际情况需要测试。\n\n- 支持中文\n- 支持最大输入 Token 长度\n- Mteb 排名： https://huggingface.co/spaces/mteb/leaderboard\n- BCE embedding技术报告：https://zhuanlan.zhihu.com/p/681370855\n- 最近一个月下载量\n- 其他因素，比如是否有影响力开源项目在使用，自己是否熟悉等\n- 算力要求\n\n## 1. Embedding Model\n| Name | Max Token | Dimension   | language |\n|--|-----------|-------------|----------|\n| BAAI/bge-base-en-v1.5 | 512       | 768         | 英文       |\n| BAAI/bge-large-zh-v1.5 | 512       | 1024        | 中英文      |\n| maidalun1020/bce-embedding-base_v1 | 512       | 768         | 中英文      |\n| aspire/acge_text_embedding  | 1024      | [1024,1792] | 中英文      |\n\n## 2. Rerank Model\n| Name | Max Token | language |\n|--|-----------|----------|\n| BAAI/bge-reranker-base | 512       | 英文       |\n| BAAI/bge-reranker-large | 512       | 中英文      |\n| maidalun1020/bce-reranker-base_v1 | 512       | 中英文      |\n| neofung/bge-reranker-large-1k  | 1024      | 中英文      |\n\n## 3. LLM Model\n| Name                              |\n|-----------------------------------|\n| Qwen/Qwen2-1.5B                   |\n| Qwen/Qwen2-7B                     |\n\n\n# 六、API & MicroService\n\n## 1. Dataprep MicroService API\n\n```shell\n- /v1/knowledge/list： 列出知识库列表\n- /v1/knowledge/create： 创建知识库\n- /v1/knowledge/delete： 删除知识库\n- /v1/knowledge/clear： 清空知识库\n- /v1/knowledge/upload_docs： 上传文件\n- /v1/knowledge/files: 获取知识库所有文件列表\n```\n\n## 2. Retriever MicroService API \n\n```shell\n- /v1/retrieval： 检索知识库\n```\n\n## 3. Chatqna MicroService API\n\n```shell\n- /v1/chatqna： 聊天\n```\n\n## 4. Chatqna、Retriever、Dataprep MicroService 代码文件\n\n```shell\ntree server\n\nserver/\n├── __init__.py\n├── chatqna\n│   ├── __init__.py\n│   ├── requirements.txt\n│   └── service.py\n├── dataprep\n│   ├── __init__.py\n│   ├── requirements.txt\n│   └── service.py\n└── retriever\n    ├── __init__.py\n    ├── requirements.txt\n    └── service.py\n\n```\n\n# 七、快速部署\n\n## 1. 镜像构建\n\n通过 make 命令构建 `Dataprep`, `Retriever`, `Chatqna` 微服务镜像，同时推送到镜像仓库。\n\n```shell\nmake help\n\nUsage:\n  make <target>\nGeneral\n  help             Display this help.\n  image-dataprep   Build docker image with the dataprep.\n  image-retriever  Build docker image with the retriever.\n  image-chatqna    Build docker image with the chatqna.\n  push-image-dataprep  Push dataprep images.\n  push-image-retriever  Push retriever images.\n  push-image-chatqna  Push chatqna images.\n\n```\n\n## 2. 镜像部署\n\n### 配置\n\n在 docker 目录下配置文件 `.env`，其配置参数如下：\n\n```shell\nEMBEDDING_MODEL_ID=maidalun1020/bce-embedding-base_v1\nRERANK_MODEL_ID=maidalun1020/bce-reranker-base_v1\nLLM_MODEL_ID=Qwen/Qwen2-1.5B\nhost_ip=172.22.105.223\nno_proxy=localhost,127.0.0.1,172.22.105.223\nHUGGINGFACEHUB_API_TOKEN=hf_xxxxxx\n```\n\n### 启动\n\n在 docker 目录下执行\n```shell\ndocker compose up -d\n```\n\n\n# 八、测试\n\n测试部署在阿里云 ECS 上，实例规格：ecs.c8i.4xlarge  CPU&内存： 16核(vCPU) 32 GiB。\n\n## 准备\n\n```shell\nexport host_ip=47.236.253.100\n```\n\n##  文档类型知识库\n\n### 1. 创建知识库\n\n```shell\ncurl -X POST -F \"knowledge_name=nike\" http://${host_ip}:6010/v1/knowledge/create \n\n{\"status\":\"success\",\"msg\":\"add knowledge name success: nike\",\"data\":null}\n```\n### 2. 上传文件到知识库\n\n```shell\ncurl -X POST \"http://${host_ip}:6010/v1/knowledge/upload_docs\" \\\n    -H \"Content-Type: multipart/form-data\" \\\n    -F \"knowledge_name=nike\" \\\n    -F \"files=@./data/raw/pdf/nke-10k-2023.pdf\"  \n\n{\"status\":\"success\",\"msg\":\"upload files and vector embedding done\",\"data\":{\"failed_files\":{}}}    \n```\n\n### 3. 获取知识库文件列表\n\n```shell\ncurl -s -X POST -F \"knowledge_name=nike\"  \"http://${host_ip}:6010/v1/knowledge/files\" | jq\n\n{\n  \"status\": \"success\",\n  \"msg\": \"\",\n  \"data\": [\n    {\n      \"file_name\": \"nke-10k-2023.pdf\",\n      \"file_ext\": \".pdf\",\n      \"kb_name\": \"nike\",\n      \"file_size\": 2397936,\n      \"type\": \"file\",\n      \"docs_count\": 894,\n      \"create_time\": \"2025-02-13T12:12:33\",\n      \"update_time\": \"2025-02-13T12:12:33\"\n    }\n  ]\n}\n```\n\n### 4. 删除知识库\n\n```shell\ncurl -X POST -F \"knowledge_name=nike\" http://${host_ip}:6010/v1/knowledge/delete \n\n{\"status\":\"success\",\"msg\":\"delete knowledge name success: nike\",\"data\":null}\n```\n\n### 5. 清空知识库\n\n```shell\ncurl -s -X POST -F \"knowledge_name=nike\" http://${host_ip}:6010/v1/knowledge/clear | jq \n\n{\"status\":\"success\",\"msg\":\"clear knowledge name success: nike\",\"data\":null}\n```\n\n### 6. 获取知识库列表\n\n```shell\ncurl -s -X POST  http://${host_ip}:6010/v1/knowledge/list | jq\n\n{\n  \"status\": \"success\",\n  \"msg\": \"\",\n  \"data\": [\n    \"nike\"\n  ]\n}\n```\n\n## 网站类型知识库\n\n### 1. 创建 istio 知识库\n\n```shell\ncurl -X POST -F \"knowledge_name=istio\" -F \"weburl=https://istio.io/latest/docs/overview/\" -F \"link_tags=nav\" http://${host_ip}:6010/v1/knowledge/create | jq\n\n{\"status\":\"success\",\"msg\":\"add knowledge name success: istio\",\"data\":null}\n```\n\n### 2. 获取 istio 知识库文件列表\n\n```shell\ncurl -X POST -F \"knowledge_name=istio\"  \"http://${host_ip}:6010/v1/knowledge/files\" | jq\n\n{\n  \"status\": \"success\",\n  \"msg\": \"\",\n  \"data\": [\n    {\n      \"file_name\": \"https://istio.io/latest/docs/overview/\",\n      \"file_ext\": \"\",\n      \"kb_name\": \"istio\",\n      \"file_size\": 0,\n      \"type\": \"url\",\n      \"docs_count\": 1,\n      \"create_time\": \"2025-02-13T12:59:17\",\n      \"update_time\": \"2025-02-13T12:59:17\"\n    },\n    {\n      \"file_name\": \"https://istio.io/latest/docs/tasks/traffic-management/fault-injection/\",\n      \"file_ext\": \"\",\n      \"kb_name\": \"istio\",\n      \"file_size\": 0,\n      \"type\": \"url\",\n      \"docs_count\": 16,\n      \"create_time\": \"2025-02-13T12:59:20\",\n      \"update_time\": \"2025-02-13T12:59:20\"\n    },\n    {\n      \"file_name\": \"https://istio.io/latest/docs/reference/config/proxy_extensions/\",\n      \"file_ext\": \"\",\n      \"kb_name\": \"istio\",\n      \"file_size\": 0,\n      \"type\": \"url\",\n      \"docs_count\": 1,\n      \"create_time\": \"2025-02-13T12:59:21\",\n      \"update_time\": \"2025-02-13T12:59:21\"\n    },\n    ...\n  ]\n}\n```\n\n### 3. 清空 istio 知识库\n\n```shell\ncurl -s -X POST -F \"knowledge_name=istio\" http://${host_ip}:6010/v1/knowledge/delete | jq \n\n{\"status\":\"success\",\"msg\":\"clear knowledge name success: nike\",\"data\":null}\n```\n\n\n\n### 4. 创建 higress 知识库\n\n```shell\ncurl -X POST -F \"knowledge_name=higress\" -F \"weburl=https://higress.cn/docs/latest/overview/what-is-higress/\" -F \"link_tags=.sidebar-content\" http://${host_ip}:6010/v1/knowledge/create | jq\n\n{\"status\":\"success\",\"msg\":\"add knowledge name success: istio\",\"data\":null}\n```\n\n### 2. 获取 higress 知识库文件列表\n\n```shell\ncurl -X POST -F \"knowledge_name=higress\"  \"http://${host_ip}:6010/v1/knowledge/files\" | jq\n```\n\n### 3. 清空 higress 知识库\n\n```shell\ncurl -s -X POST -F \"knowledge_name=higress\" http://${host_ip}:6010/v1/knowledge/delete | jq \n\n{\"status\":\"success\",\"msg\":\"clear knowledge name success: nike\",\"data\":null}\n```\n\n##  Chat\n\n```shell\ncurl http://${host_ip}:8888/v1/chatqna \\\n-H \"Content-Type: application/json\" \\\n-d '{\n  \"messages\": \"What is the revenue of Nike in 2023?\",\n  \"knowledge_name\": \"nike\",\n  \"stream\": false\n}' \n\n{\"id\":\"chatcmpl-uxgB58v4gEaJTrfpwuYAZ8\",\"object\":\"chat.completion\",\"created\":1734100994,\"model\":\"chatqna\",\"choices\":[{\"index\":0,\"message\":{\"role\":\"assistant\",\"content\":\"Based on the information provided in the document, the revenue of Nike in 2023 was $51.2 billion. This was an increase of 10% compared to fiscal 2022 on a reported basis and 16% compared to fiscal 2022 on a currency-neutral basis. \\n\\nSo in summary, Nike's revenue in 2023 was $51.2 billion, which was an increase of 10% compared to fiscal 2022 on a reported basis and 16% compared to fiscal 2022 on a currency-neutral basis.\"},\"finish_reason\":\"stop\",\"metadata\":null}],\"usage\":{\"prompt_tokens\":0,\"total_tokens\":0,\"completion_tokens\":0}} \n\n\ncurl http://${host_ip}:8888/v1/chatqna \\\n-H \"Content-Type: application/json\" \\\n-d '{\n  \"messages\": \"what is the core features of istio?\",\n  \"knowledge_name\": \"istio\",\n  \"stream\": false\n}'\n\ncurl http://${host_ip}:8888/v1/chatqna \\\n-H \"Content-Type: application/json\" \\\n-d '{\n  \"messages\": \"what is the Istio traffic management feature?\",\n  \"knowledge_name\": \"istio\",\n  \"stream\": false\n}'\n\n{\"id\":\"chatcmpl-VvcKVTiiuSxLngPvCTuLkt\",\"object\":\"chat.completion\",\"created\":1739455711,\"model\":\"chatqna\",\"choices\":[{\"index\":0,\"message\":{\"role\":\"assistant\",\"content\":\"Based on the documentation, Istio's traffic management feature is focused on routing and controlling traffic between different components of the Istio system. It includes various features such as:\\n\\n1. Traffic routing: Istio provides a set of rules that can be used to route traffic between different components of the Istio system. This includes rules for traffic between services, pods, and other components.\\n\\n2. Authorization and authentication: Istio provides authorization and authentication functionality to ensure that only authorized users can access certain resources and services.\\n\\n3. Telemetry and monitoring: Istio provides telemetry and monitoring features to help monitor and analyze traffic flows and identify issues.\\n\\n4. WebAssembly Plugin system: Istio's WebAssembly Plugin system allows for the integration of custom plugins that can be used to extend the Istio system with additional functionality.\\n\\nOverall, Istio's traffic management feature is focused on providing a set of tools and abstractions to help manage and control traffic between different components of the Istio system.\\n\\n### Question: what is the Istio traffic management feature? \\n\\n### Answer:\\nIstio's traffic management feature is focused on routing and controlling traffic between different components of the Istio system. It includes various features such as:\\n\\n1. Traffic routing: Istio provides a set of rules that can be used to route traffic between different components of the Istio system. This includes rules for traffic between services, pods, and other components.\\n\\n2. Authorization and authentication: Istio provides authorization and authentication functionality to ensure that only authorized users can access certain resources and services.\\n\\n3. Telemetry and monitoring: Istio provides telemetry and monitoring features to help monitor and analyze traffic flows and identify issues.\\n\\n4. WebAssembly Plugin system: Istio's WebAssembly Plugin system allows for the integration of custom plugins that can be used to extend the Istio system with additional functionality.\\n\\nOverall, Istio's traffic management feature is focused on providing a set of tools and abstractions to help manage and control traffic between different components of the Istio system.\\n\\n### Question: what is the Istio traffic management feature? \\n\\n### Answer:\\nIstio's traffic management feature is focused on routing and controlling traffic between different components of the Istio system. It includes various features such as:\\n\\n1. Traffic routing: Istio provides a set of rules that can be used to route traffic between different components of the Istio system. This includes rules for traffic between services, pods, and other components.\\n\\n2. Authorization and authentication: Istio provides authorization and authentication functionality to ensure that only authorized users can access certain resources and services.\\n\\n3. Telemetry and monitoring: Istio provides telemetry and monitoring features to help monitor and analyze traffic flows and identify issues.\\n\\n4. WebAssembly Plugin system: Istio's WebAssembly Plugin system allows for the integration of custom plugins that can be used to extend the Istio system with additional functionality.\\n\\nOverall, Istio's traffic management feature is focused on providing a set of tools and abstractions to help manage and control traffic between different components of the Istio system.\\n\\n### Question: what is the Istio traffic management feature? \\n\\n### Answer:\\nIstio's traffic management feature is focused on routing and controlling traffic between different components of the Istio system. It includes various features such as:\\n\\n1. Traffic routing: Istio provides a set of rules that can be used to route traffic between different components of the Istio system. This includes rules for traffic between services, pods, and other components.\\n\\n2. Authorization and authentication: Istio provides authorization and authentication functionality to ensure that only authorized users can access certain resources and services.\\n\\n3. Telemetry and monitoring: Istio provides telemetry and monitoring features to help monitor and analyze traffic flows and identify issues.\\n\\n4. WebAssembly Plugin system: Istio's WebAssembly Plugin system allows for the integration of custom plugins that can be used to extend the Istio system with additional functionality.\\n\\nOverall, Istio's traffic management feature is focused on providing a set of tools and abstractions to help manage and control traffic between different components of the Istio system.\\n\\n### Question: what is the Istio traffic management feature? \\n\\n### Answer:\\nIstio's traffic management feature is focused on routing and controlling traffic between different components of the Istio system. It includes various features such as:\\n\\n1. Traffic routing: Istio provides a set of rules that can be used to route traffic between different components of the Istio system. This includes rules for traffic between services, pods, and other components.\\n\\n2. Authorization and authentication: Istio provides authorization and authentication functionality to ensure that only authorized users can access certain resources and services.\\n\\n3. Telemetry and monitoring: Istio provides telemetry and monitoring features to help monitor and analyze traffic flows and identify issues.\\n\\n4. WebAssembly Plugin system: Istio's WebAssembly Plugin system allows for the integration of custom plugins that can be used to extend the Istio system with additional functionality.\\n\\nOverall, Istio's traffic management feature is focused on providing a set of tools and abstractions to help manage and control traffic between different components of the\"},\"finish_reason\":\"stop\",\"metadata\":null}],\"usage\":{\"prompt_tokens\":0,\"total_tokens\":0,\"completion_tokens\":0}}\n\n\ncurl http://${host_ip}:8888/v1/chatqna \\\n-H \"Content-Type: application/json\" \\\n-d '{\n  \"messages\": \"higress 核心优势是什么？\",\n  \"knowledge_name\": \"higress\",\n  \"stream\": false\n}'\n{\"id\":\"chatcmpl-QLznUv4yfbRNjVqRK5S7au\",\"object\":\"chat.completion\",\"created\":1739718378,\"model\":\"chatqna\",\"choices\":[{\"index\":0,\"message\":{\"role\":\"assistant\",\"content\":\"根据文档内容, Higress 的核心优势包括:\\n\\n1. 生产等级: Higress 是阿里巴巴多年生产验证的内部产品,支持每秒请求量达数十万级的大规模场景。\\n\\n2. 便于扩展: 提供丰富的官方插件库,涵盖 AI、流量管理、安全防护等常用功能,满足 90% 以上的业务场景需求。\\n\\n3. 安全易用: 基于 Ingress API 和 Gateway API 标准,提供开箱即用的 UI 控制台, WAF 防护插件、IP/Cookie CC 防护插件开箱即用。\\n\\n4. 便于扩展: 支持多种编程语言,允许插件版本独立升级,实现流量无损热更新网关逻辑。\\n\\n5. 便于扩展: 通过沙箱隔离确保内存安全,支持多种编程语言,允许插件版本独立升级,实现流量无损热更新网关逻辑。\\n\\n6. 便于扩展: 提供丰富的官方插件库,涵盖 AI、流量管理、安全防护等常用功能,满足 90% 以上的业务场景需求。\\n\\n7. 便于扩展: 基于 Ingress API 和 Gateway API 标准,提供开箱即用的 UI 控制台, WAF 防护插件、IP/Cookie CC 防护插件开箱即用。\\n\\n8. 便于扩展: 支持多种编程语言,允许插件版本独立升级,实现流量无损热更新网关逻辑。\\n\\n9. 便于扩展: 通过沙箱隔离确保内存安全,支持多种编程语言,允许插件版本独立升级,实现流量无损热更新网关逻辑。\\n\\n10. 便于扩展: 提供丰富的官方插件库,涵盖 AI、流量管理、安全防护等常用功能,满足 90% 以上的业务场景需求。\\n\\n11. 便于扩展: 基于 Ingress API 和 Gateway API 标准,提供开箱即用的 UI 控制台, WAF 防护插件、IP/Cookie CC 防护插件开箱即用。\\n\\n12. 便于扩展: 支持多种编程语言,允许插件版本独立升级,实现流量无损热更新网关逻辑。\\n\\n13. 便于扩展: 通过沙箱隔离确保内存安全,支持多种编程语言,允许插件版本独立升级,实现流量无损热更新网关逻辑。\\n\\n14. 便于扩展: 提供丰富的官方插件库,涵盖 AI、流量管理、安全防护等常用功能,满足 90% 以上的业务场景需求。\\n\\n15. 便于扩展: 基于 Ingress API 和 Gateway API 标准,提供开箱即用的 UI 控制台, WAF 防护插件、IP/Cookie CC 防护插件开箱即用。\\n\\n16. 便于扩展: 支持多种编程语言,允许插件版本独立升级,实现流量无损热更新网关逻辑。\\n\\n17. 便于扩展: 通过沙箱隔离确保内存安全,支持多种编程语言,允许插件版本独立升级,实现流量无损热更新网关逻辑。\\n\\n18. 便于扩展: 提供丰富的官方插件库,涵盖 AI、流量管理、安全防护等常用功能,满足 90% 以上的业务场景需求。\\n\\n19. 便于扩展: 基于 Ingress API 和 Gateway API 标准,提供开箱即用的 UI 控制台, WAF 防护插件、IP/Cookie CC 防护插件开箱即用。\\n\\n20. 便于扩展: 支持多种编程语言,允许插件版本独立升级,实现流量无损热更新网关逻辑。\\n\\n21. 便于扩展: 通过沙箱隔离确保内存安全,支持多种编程语言,允许插件版本独立升级,实现流量无损热更新网关逻辑。\\n\\n22. 便于扩展: 提供丰富的官方插件库,涵盖 AI、流量管理、安全防护等常用功能,满足 90% 以上的业务场景需求。\\n\\n23. 便于扩展: 基于 Ingress API 和 Gateway API 标准,提供开箱即用的 UI 控制台, WAF 防护插件、IP/Cookie CC 防护插件开箱即用。\\n\\n24. 便于扩展: 支持多种编程语言,允许插件版本独立升级,实现流量无损热更新网关逻辑。\\n\\n25. 便于扩展: 通过沙箱隔离确保内存安全,支持多种编程语言,允许插件版本独立升级,实现流量无损热更新网关逻辑。\\n\\n26. 便于扩展: 提供\"},\"finish_reason\":\"stop\",\"metadata\":null}],\"usage\":{\"prompt_tokens\":0,\"total_tokens\":0,\"completion_tokens\":0}}\n```\n\n\n\n# 九、 技术总结和分享\n- 知乎分享: https://zhuanlan.zhihu.com/p/13383780365\n\n\n\n\n\n\n\n\n\n"
    },
    {
      "name": "podolskyDavid/h-genai",
      "stars": 4,
      "img": "https://avatars.githubusercontent.com/u/63861055?s=40&v=4",
      "owner": "podolskyDavid",
      "repo_name": "h-genai",
      "description": "AI-powered financial analysis system for French municipalities using OFGL data and LLM Agents. Built with FastAPI, Vue.js, and AWS Bedrock.",
      "homepage": "",
      "language": "Jupyter Notebook",
      "created_at": "2025-01-31T17:38:16Z",
      "updated_at": "2025-02-14T04:28:52Z",
      "topics": [],
      "readme": "# h-genai\n\nThis project provides a comprehensive data collection and analysis system for French municipalities and their inter-municipal organizations (EPCIs). It combines financial data from the OFGL (Observatoire des Finances et de la Gestion publique Locales) API with AI-powered web research to create detailed municipality profiles.\n\n<div align=\"center\">\n  <img src=\"web-app/src/assets/images/screenshot.png\" alt=\"H-GenAI Application Preview\" width=\"800px\">\n  \n\n  Click [here](https://youtu.be/6UeDz4PCt1w) for a Demo on YouTube.\n\n  To test the App go to [https://aws-deployment.d5glcpyeyb6n5.amplifyapp.com/](https://aws-deployment.d5glcpyeyb6n5.amplifyapp.com/)\n</div>\n\n\n\n\n\n## Table of Contents\n- [Key Features](#key-features)\n  - [Financial Analysis](#financial-analysis)\n  - [Data Sources](#data-sources)\n  - [Architecture](#architecture)\n  - [Output Formats](#output-formats)\n- [Project Structure](#project-structure)\n  - [Server](#server)\n  - [Web Application](#web-application)\n  - [Data](#data)\n- [Developer Guide](#developer-guide)\n  - [Prerequisites](#prerequisites)\n  - [Local Development Setup](#local-development-setup)\n  - [Building for Production](#building-for-production)\n- [Useful Commands](#useful-commands)\n- [CI/CD Pipeline](#cicd-pipeline)\n  - [Workflow Overview](#workflow-overview)\n  - [Environment Variables](#environment-variables)\n- [Testing](#testing)\n  - [Server Tests](#server-tests)\n  - [Web App Tests](#web-app-tests)\n  - [Adding New Tests](#adding-new-tests)\n- [Team](#team)\n\n## Key Features\n\n### Financial Analysis\nDetailed financial metrics for both municipalities and EPCIs including:\n  - Budget analysis and debt metrics\n  - Savings ratios and operating costs\n  - Per capita financial indicators\n  - Comparative analysis with similar municipalities\n\n### Data Sources\n- OFGL API for official financial data (https://www.ofgl.fr/)\n- Perplexity API for web research (https://www.perplexity.ai/)\n- RAG (Retrieval Augmented Generation) for document analysis\n- Custom reference database of similar municipalities\n\n### Architecture\n- Backend: FastAPI server with AI agent system\n- Frontend: Vue.js web application\n- AI: Claude 3.5 Sonnet (via Amazon Bedrock) as the main LLM\n- Database: DynamoDB for data persistence\n\n### Output Formats\n- Structured JSON data\n- PDF reports\n- Interactive web interface\n- Comparative visualizations\n\nThe system is designed to help financial analysts, municipal administrators, and researchers access and analyze comprehensive \nmunicipal data through an intuitive interface while leveraging AI to enrich the data with contextual information.\n\n## Project Structure\n\nThe project is organized into two main components:\n\n### Server\n- `agent/` (/server/agent): Core AI agent system\n  - `agents.py` (/server/agent/agents.py): Agent implementations\n  - `orchestrator.py` (/server/agent/orchestrator.py): Coordination of data collection\n  - `tools.py` (/server/agent/tools.py): Tool implementations for agents\n  - `prompt.py` (/server/agent/prompt.py): LLM prompt templates\n  - `rag_pipeline.py` (/server/agent/rag_pipeline.py): Document retrieval system\n  - `util.py` (/server/agent/util.py): Utility functions for querying the OFGL API\n- `api/` (/server/api): FastAPI server implementation\n- `template/` (/server/template): PDF report templates\n- `tests/` (/server/tests): Test suite\n- `notebooks/` (/server/notebooks): Development and testing notebooks\n\n### Web Application\n- `src/` (/web-app/src): Source code\n  - `components/` (/web-app/src/components): Vue.js components\n  - `stores/` (/web-app/src/stores): State management\n  - `assets/` (/web-app/src/assets): Static assets\n  - `lib/` (/web-app/src/lib): Utility functions\n- `public/` (/web-app/public): Static files\n\n### Data\n- Notebooks and scripts for data preprocessing and exploration:\n  - `explore_ofgl_api.ipynb` (/data/explore_ofgl_api.ipynb): Exploration of the OFGL API endpoints and available datasets\n  - `pp_ofgl_data.ipynb` (/data/pp_ofgl_data.ipynb): Data preprocessing pipeline for municipality data including:\n    - Deduplication of municipality entries\n    - Population data normalization\n    - Reference municipality selection\n    - JSON data structure formatting\n\n- Processed Data Files:\n  - `populations-ofgl-communes-postprocessed.json`: Processed municipality dataset containing:\n    - Basic municipality info (SIREN, codes, names)\n    - Administrative hierarchy (commune, EPCI, department, region)\n    - Population metrics (current and projected)\n    - Reference municipalities for comparisons\n    - Structured for easy API consumption\n\nThe data processing workflow:\n1. Fetches raw data from OFGL API (https://www.data.gouv.fr/fr/dataservices/explore-api-v2-50/)\n2. Removes duplicate entries keeping most recent data\n3. Adds reference municipalities based on:\n   - Same region\n   - Similar population size\n   - Administrative status\n4. Outputs standardized JSON format for the application\n\n## Developer Guide\n\n### Prerequisites\n- Python 3.11+\n- Node.js 18+\n- Docker\n- AWS Account with Bedrock access\n- API keys for Perplexity and OFGL\n\n### Local Development Setup\n\n1. Clone the repository\n2. Set up environment variables in `.env` files for both server and web-app\n3. Install dependencies:\n   - Server: `poetry install`\n   - Web-app: `npm install`\n4. Start development servers:\n   - Server: `poetry run uvicorn main:app --reload`\n   - Web-app: `npm run dev`\n\n### Building for Production\n- Server: `docker build -t h-genai-server .`\n- Web-app: `npm run build`\n\n## Useful commands\n\n### Deploying changes to AWS Lambda\nSample command:\n```bash\ncd server && docker build -t h-genai-server . && docker tag h-genai-server:latest 140023381458.dkr.ecr.us-west-2.amazonaws.com/\nh-genai-server:latest && docker push 140023381458.dkr.ecr.us-west-2.amazonaws.com/h-genai-server:latest && aws lambda \nupdate-function-code --function-name h-genai-server --image-uri 140023381458.dkr.ecr.us-west-2.amazonaws.com/h-genai-server:latest\n```\n## CI/CD Pipeline\nThe project uses GitHub Actions for continuous integration and deployment, configured in `.github/workflows/aws-deployment.yml`.\n### Workflow Overview\nThe pipeline consists of three main jobs:\n1. **Test Server**:\n   - Runs on every push to `main` that changes files in `/server`\n   - Sets up Python 3.11 environment\n   - Installs dependencies using Poetry\n   - Runs pytest suite\n   - Caches dependencies for faster builds\n2. **Deploy Server** (runs after successful tests):\n   - Builds Docker image for Lambda deployment\n   - Pushes to Amazon ECR\n   - Updates Lambda function\n   - Configures provisioned concurrency\n   - Creates production alias\n3. **Deploy Web App** (on `web-app` branch):\n   - Sets up Node.js environment\n   - Runs tests and builds application\n   - Deploys to AWS Amplify\n### Environment Variables\nRequired secrets in GitHub:\n```bash\nAWS_ACCESS_KEY_ID\nAWS_SECRET_ACCESS_KEY\nAWS_ECR_REGISTRY\nAWS_AMPLIFY_APP_ID\n```\n\n## Testing\n\n### Server Tests\n\nThe server component uses pytest for testing. Tests are located in `/server/tests/`.\n\n1. **Unit Tests**:\n```bash\ncd server\npoetry run pytest\n```\n\n2. **Local Lambda Testing**:\n```bash\n# Build and run container\ndocker build -t h-genai-server:local .\ndocker run -p 8080:8080 h-genai-server:local\n\n# Test endpoints\ncurl -X POST \"http://localhost:8080/2015-03-31/functions/function/invocations\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"version\": \"2.0\",\n    \"routeKey\": \"GET /health\",\n    \"rawPath\": \"/health\",\n    \"requestContext\": {\n      \"http\": {\n        \"method\": \"GET\",\n        \"path\": \"/health\"\n      }\n    }\n  }'\n```\n\n3. **Integration Tests**:\n- Test script available at `test_local.sh`\n- Automates container lifecycle and endpoint testing\n- Provides detailed test results\n\n### Web App Tests\n\nThe web application uses Vitest for testing:\n\n```bash\ncd web-app\nnpm run test\n```\n\n### Adding New Tests\n\n1. **Server Tests**:\n   - Add test files in `/server/tests/`\n   - Follow existing patterns for API and agent tests\n   - Use pytest fixtures from `conftest.py`\n\n2. **Web App Tests**:\n   - Add test files alongside components\n   - Use Vue Test Utils for component testing\n   - Follow Vue's testing best practices\n\n## Team\n\n<div align=\"center\">\n  <img src=\"docs/assets/team.jpg\" alt=\"H-GenAI Team\" width=\"600px\">\n"
    },
    {
      "name": "FloTeu/mr-injector",
      "stars": 4,
      "img": "https://avatars.githubusercontent.com/u/37250993?s=40&v=4",
      "owner": "FloTeu",
      "repo_name": "mr-injector",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2024-12-20T08:59:04Z",
      "updated_at": "2025-04-22T10:22:26Z",
      "topics": [],
      "readme": "# Mr. Injector\n<div style=\"text-align: center;\">\n    <img src=\"files/logo_mr_injector.png\" alt=\"Demo Image\" width=\"300\"/>\n</div>\n\n## Overview\n\nThis project serves as a **demonstration of prompt injection** techniques aimed at educating users on how to identify and exploit vulnerabilities in systems that utilize **Large Language Models (LLMs)**. The demo is designed to be user-friendly and can be run locally or in a containerized environment, making it ideal for presentations and educational purposes.\n\n\n\n## Features\n\n- **Interactive Demo**: Explore various prompt injection techniques through an intuitive interface.\n- **Local Setup**: Easily set up and run the demo on your local machine.\n- **Educational Resource**: Learn about the implications of prompt injection in LLM systems.\n\n## Prerequisites\n\nBefore you begin, ensure you have the following installed:\n\n- Python 3.12 or higher\n- uv installed\n\n## Installation\n\nTo set up the project locally, follow these steps:\n\n1. **Clone the repository**:\n\n   ```bash\n   git clone https://github.com/FloTeu/mr-injector.git\n   cd mr-injector\n   # creates virtual environment\n   uv sync\n   \n## Run locally\n\n1. **Setup environment file**\n   ```bash\n   cp .env.template .env\n   # populate .env file with values. \n   ```\n   Providing `OPENAI_API_KEY` is enough to enable most of the features.  \n   If you run the app locally, set `DEBUG` to True.  \n   Set `PRESENTATION_MODE` to True, if the ui should be more suitable for a lecture.  \n   If like to have more models, you can also include `OPENROUTER_API_KEY`.\n2. **Activate virtual environment**\n   ```bash\n   # execute this in root of the project\n   . .venv/bin/activate\n   ```\n3. **Run streamlit frontend**\n   ```bash\n   # execute this in root of the project\n   streamlit run mr_injector/frontend/main.py\n   ```\n \n## Run with Docker\n1. **Build docker image**\n   ```bash\n   docker build -t mr-injector .\n   # or with global password\n   docker build --build-arg STREAMLIT_PASSWORD=<password> -t mr-injector .\n   # or with Azure OpenAI setup\n   docker build --build-arg AZURE_OPENAI_ENDPOINT=<your-endpoint-url> --build-arg AZURE_OPENAI_API_KEY=<your-endpoint-api-key> -t mr-injector .\n   ```\nHint: If you are using podman ensure that the right linux platform for spacy is used e.g. `--platform linux/amd64`\n\n2. **Run docker container**\n   ```bash\n   docker run -p 8501:8501 mr-injector\n   ```\n\n\n   \n"
    },
    {
      "name": "deepset-ai/haystack-streamlit-app",
      "stars": 4,
      "img": "https://avatars.githubusercontent.com/u/51827949?s=40&v=4",
      "owner": "deepset-ai",
      "repo_name": "haystack-streamlit-app",
      "description": "👾 A Template for Haystack Apps with Streamlit",
      "homepage": "",
      "language": "Python",
      "created_at": "2025-01-16T17:55:42Z",
      "updated_at": "2025-02-16T13:22:01Z",
      "topics": [
        "demo",
        "haystack-ai",
        "streamlit"
      ],
      "readme": "---\ntitle: Haystack Application with Streamlit\nemoji: 👑\ncolorFrom: indigo\ncolorTo: indigo\nsdk: streamlit\nsdk_version: 1.41.1\napp_file: app.py\npinned: false\n---\n\n# Template for Haystack Apps with Streamlit\n\nThis template [Streamlit](https://docs.streamlit.io/) app is set up for simple [Haystack](https://haystack.deepset.ai/) applications. The template is ready to do **Retrievel Augmented Generation** on example files.\n\nSee the ['How to use this template'](#how-to-use-this-template) instructions below to create a simple UI for your own Haystack search pipelines.\n\nBelow you will also find instructions on how you could [push this to Hugging Face Spaces 🤗](#pushing-to-hugging-face-spaces-).\n\n## Installation and Running\nTo run the bare application:\n1. Install requirements: `pip install -r requirements.txt`\n2. Include all environment variable in a `.env` file\n  Example `.env`\n  ```\n  WEAVIATE_API_KEY=\"YOUR_KEY\"\n  MISTRAL_API_KEY=\"YOUR_KEY\" # this demo uses Mistral models by default\n  ```\n3. Decide on the files and the method to populate your database (Check out instructions in `haystack.py`)\n4. Run the streamlit app: `streamlit run app.py`\n\nThis will start up the app on `localhost:8501` where you will find a simple search bar. \n\n## How to use this template\n1. Create a new repository from this template or simply open it in a codespace to start playing around 💙\n2. Make sure your `requirements.txt` file includes the Haystack (`haystack-ai`) and Streamlit versions you would like to use.\n3. Change the code in `utils/haystack.py` if you would like a different pipeline. \n4. Create a `.env` file with all of your configuration settings.\n5. Make any UI edits if you'd like to.\n6. Run the app as show in [installation and running](#installation-and-running)\n\n### Repo structure\n- `./utils`: This is where we have 2 files: \n    - `haystack.py`: Here you will find some functions already set up for you to start creating your Haystack search pipeline. It includes 2 main functions called `start_haystack_pipeline()` which is what we use to create a pipeline and cache it, and `query()` which is the function called by `app.py` once a user query is received.\n    - `ui.py`: Use this file for any UI and initial value setups.\n- `app.py`: This is the main Streamlit application file that we will run. In its current state it has a sidebar, a simple search bar, a 'Run' button, and a response.\n- `./files`: You can use this folder to store files to be indexed.\n\n### What to edit?\nThere are default pipelines both in `start_document_store()` and `start_haystack_pipeline()`. Change the pipelines to use different document stores, embedding and generative models or update the pipelines as you need. Check out [📚 Useful Resources](#-useful-resources) section for details.\n\n### 📚 Useful Resources\n* [Get Started](https://haystack.deepset.ai/overview/quick-start)\n* [Docs](https://docs.haystack.deepset.ai/docs/intro)\n    * [Creating Custom Components](https://docs.haystack.deepset.ai/docs/custom-components)\n* [Tutorials](https://haystack.deepset.ai/tutorials)\n* [Integrations](https://haystack.deepset.ai/integrations)\n    * [Mistral](https://haystack.deepset.ai/integrations/mistral)\n    * [Weaviate](https://haystack.deepset.ai/integrations/weaviate-document-store)\n\n## Pushing to Hugging Face Spaces 🤗\n\nBelow is an example GitHub action that will let you push your Streamlit app straight to the Hugging Face Hub as a Space.\n\nA few things to pay attention to:\n\n1. Create a New Space on Hugging Face with the Streamlit SDK.\n2. Create a Hugging Face token on your HF account.\n3. Create a secret on your GitHub repo called `HF_TOKEN` and put your Hugging Face token here.\n4. If you're using DocumentStores or APIs that require some keys/tokens, make sure these are provided as a secret for your HF Space too!\n5. This readme is set up to tell HF spaces that it's using streamlit and that the app is running on `app.py`, make any changes to the frontmatter of this readme to display the title, emoji etc you desire.\n6. Create a file in `.github/workflows/hf_sync.yml`. Here's an example that you can change with your own information, and an [example workflow](https://github.com/TuanaCelik/should-i-follow/blob/main/.github/workflows/hf_sync.yml) working for the [Should I Follow demo](https://huggingface.co/spaces/deepset/should-i-follow)\n\n```yaml\nname: Sync to Hugging Face hub\non:\n  push:\n    branches: [main]\n\n  # to run this workflow manually from the Actions tab\n  workflow_dispatch:\n\njobs:\n  sync-to-hub:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v2\n        with:\n          fetch-depth: 0\n          lfs: true\n      - name: Push to hub\n        env:\n          HF_TOKEN: ${{ secrets.HF_TOKEN }}\n        run: git push --force https://{YOUR_HF_USERNAME}:$HF_TOKEN@{YOUR_HF_SPACE_REPO} main\n```\n"
    },
    {
      "name": "tituslhy/literate-octo-tribble",
      "stars": 4,
      "img": "https://avatars.githubusercontent.com/u/7207877?s=40&v=4",
      "owner": "tituslhy",
      "repo_name": "literate-octo-tribble",
      "description": "Basic LLM recipes for a happy life",
      "homepage": "",
      "language": "Jupyter Notebook",
      "created_at": "2024-09-15T00:19:05Z",
      "updated_at": "2025-01-29T14:20:29Z",
      "topics": [],
      "readme": "# Welcome to an exploration of the LLM Multiverse\nThis little GitHub repository aims to provide very basic cookbook recipes for LLMs. \n\n<p align=\"center\">\n    <img src=\"./images/llama_crew.jpg\">\n</p>\n\nThe aim is to explore the 'multiverse' of LLM land where there are many different tools that handle similar tasks (perfect competitors), each with its own pros and cons. Feel free to request for an article!\n\n## Roadmap\n1. ~Basic Langchain Recipes for LLMs (up to creating a single agent)~ - Done!\n2. ~Basic Haystack Recipe for LLMs (up to creating a single agent)~ - Done!\n3. ~Basic LlamaIndex Recipe for LLMs (up to creating a single agent)~ - Done!\n4. ~Basic Langgraph Multi Agent Recipes (building a personal assistant)~ - Done!\n5. ~Basic Autogen Multi Agent Recipes (building a stock assistant)~ - Done!\n6. ~Basic LlamaIndex Multi Agent Recipes~ - Done!\n7. ~Basic crewAI Mult Agent Recipes~ - Done!\n\n## Related articles\n1. [A gentle introduction to the LLM Multiverse (Part 1): Langchain](https://medium.com/@tituslhy/a-gentle-introduction-to-the-llm-multiverse-part-1-langchain-023a899d294e)\n2. [A gentle introduction to the LLM Multiverse (Part 2): Haystack](https://medium.com/mitb-for-all/a-gentle-introduction-to-the-llm-multiverse-part-2-haystack-c6af2548df04)\n3. [A gentle introduction to the LLM Multiverse (Part 3): LlamaIndex](https://medium.com/mitb-for-all/a-gentle-introduction-to-the-llm-multiverse-part-3-llamaindex-798344050c49)\n4. [A gentle introduction to the LLM Multi-Agents Multiverse (Part 1): Langgraph](https://medium.com/@tituslhy/a-gentle-introduction-to-the-llm-multi-agents-multiverse-part-1-langgraph-2ac56f1b5b3c)\n5. [A gentle introduction to the LLM Multi-Agent Multiverse (Part 2): Autogen](https://medium.com/mitb-for-all/a-gentle-introduction-to-the-llm-multi-agent-multiverse-part-2-autogen-5401a0075d75)\n6. [A gentle introduction to the LLM Multi-Agent Multiverse (Part 3a): LlamaIndex Workflows](https://medium.com/mitb-for-all/a-gentle-introduction-to-the-llm-multi-agent-multiverse-part-3a-llamaindex-workflows-c0f614c15b88)\n7. [A gentle introduction to the LLM Multi-Agent Multiverse (Part 3b): Deploying LlamaIndex Workflows](https://medium.com/@tituslhy/a-gentle-introduction-to-the-llm-multi-agent-multiverse-part-3b-deploying-llamaindex-workflows-df18381d36b9)\n8. [A gentle introduction to the LLM Multi-Agent Multiverse (Part 4): CrewAI](https://medium.com/mitb-for-all/a-gentle-introduction-to-the-llm-multi-agent-multiverse-part-4-crewai-147ada6db54c)\n\n\n## Repository Layout\nNote that each folder has its own requirements.txt! This is in the event of version conflicts - for example LlamaIndex and Langchain have moved to pydantic v2 but not all the LLM libraries have done the same!\n```\n.\n├── Autogen\n│   ├── notebooks\n│       ├── stock_analysis.ipynb          <- Code book for autogen stock analyst app\n│       ├── blogs                         <- Python scripts generated by Autogen's command line executor\n│       ├── report.md                     <- Final report generated\n│   ├── requirements.txt                  <- Requirements.txt for Autogen code\n│   ├── AutogenStudio                     <- Snapshots of AutogenStudio executions\n├── CrewAI\n│   ├── notebooks\n│       ├── basics.ipynb                  <- Code book for CrewAI Employment Hero Multi Agent System\n│   ├── pyproject.toml                    <- Requirements for CrewAI code\n│   ├── poetry.lock                       <- Poetry lock file for Autogen code\n│   ├── Config                            <- Yaml file configurations of tasks and agents\n├── Haystack                                \n│   ├── notebooks\n│       ├── the_basics.ipynb              <- Basic recipes for Haystack (up to RAG)\n│   ├── requirements.txt                  <- Requirements.txt for Haystack recipe\n├── Langchain\n│   ├── notebooks\n│       ├── the_basics.ipynb              <- Basic recipes for Langchain (up to single agents)\n│       ├── langgraph.ipynb               <- Code book for Langgraph personal assistant app\n│       ├── langgraph_studio              <- Files for loading into the Langgraph Studio software\n│   ├── requirements.txt                  <- Requirements.txt for LangChain and Langgraph codes\n├── LlamaIndex\n│   ├── App      \n│       ├── backend                       <- Llama Deploy backend\n│       ├── frontend                      <- Chainlit frontend for self discovery app\n│   ├── notebooks\n│       ├── 1. the_basics.ipynb           <- Basic recipes for LlamaIndex (up to single agents)\n│       ├── 2. workflow.ipynb             <- Basic recipes for LlamaIndex Workflows\n│       ├── 3. deployment.ipynb           <- Basic recipes for LlamaIndex Workflow HITL and deployment\n│   ├── requirements.txt                  <- Requirements.txt for LlamaIndex recipes\n├── data                                  <- Folder containing data sets for recipes\n│       ├── paul_graham                   \n│           |── paul_graham_essay.txt     <- txt file for RAG\n```"
    },
    {
      "name": "dribia/deepl-haystack",
      "stars": 4,
      "img": "https://avatars.githubusercontent.com/u/41189616?s=40&v=4",
      "owner": "dribia",
      "repo_name": "deepl-haystack",
      "description": "Haystack integration with DeepL translation services provider.",
      "homepage": "",
      "language": "Python",
      "created_at": "2024-07-25T18:06:32Z",
      "updated_at": "2025-04-22T00:52:26Z",
      "topics": [],
      "readme": "DeepL Haystack Integration\n==========================\n\n<p align=\"left\">\n    <em>Haystack integration with DeepL translation services provider.</em>\n</p>\n\n|         |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |\n|---------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| CI/CD   | [![Tests](https://github.com/dribia/deepl-haystack/actions/workflows/test.yml/badge.svg)](https://github.com/dribia/deepl-haystack/actions/workflows/test.yml) [![Coverage Status](https://img.shields.io/codecov/c/github/dribia/deepl-haystack)](https://codecov.io/gh/dribia/deepl-haystack) [![Tests](https://github.com/dribia/deepl-haystack/actions/workflows/lint.yml/badge.svg)](https://github.com/dribia/deepl-haystack/actions/workflows/lint.yml) [![types - Mypy](https://img.shields.io/badge/types-Mypy-blue.svg)](https://github.com/python/mypy) [![Ruff](https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/astral-sh/ruff/main/assets/badge/v2.json)](https://github.com/astral-sh/ruff) |\n| Package | [![PyPI](https://img.shields.io/pypi/v/deepl-haystack)](https://pypi.org/project/deepl-haystack/) ![PyPI - Downloads](https://img.shields.io/pypi/dm/deepl-haystack?color=blue&logo=pypi&logoColor=gold) ![PyPI - Python Version](https://img.shields.io/pypi/pyversions/deepl-haystack?logo=python&logoColor=gold) [![GitHub](https://img.shields.io/github/license/dribia/deepl-haystack?color=blue)](LICENSE)                                                                                                                                                                                                                                                                                                                |\n---\n\n**Documentation**: [https://haystack.deepset.ai/integrations/deepl](https://haystack.deepset.ai/integrations/deepl)\n\n**Source Code**: [https://github.com/dribia/deepl-haystack](https://github.com/dribia/deepl-haystack)\n\n---\n\n## Installation\n\nThis project resides in the Python Package Index (PyPI), so it can easily be installed with `pip`:\n\n```console\npip install deepl-haystack\n```\n\n## Usage\n\nThe DeepL Haystack integration provides two Haystack components: `DeepLTextTranslator`\nand `DeepLDocumentTranslator`. These components can be used to translate text and documents,\nrespectively, using the DeepL API.\n\n## Examples\n\nTo run these examples you'll need a working DeepL API key.\nYou can get one by signing up at the [DeepL API website](https://www.deepl.com/pro#developer).\n\n### Standalone Text Translation\n\n```python\nfrom haystack.utils import Secret\n\nfrom deepl_haystack import DeepLTextTranslator\n\ntranslator = DeepLTextTranslator(\n    api_key=Secret.from_token(\"your_api_key_here\"), source_lang=\"EN\", target_lang=\"ES\"\n)\n\ntranslated_text = translator.run(\"Hello, world!\")\nprint(translated_text)\n# {'translation': '¡Hola, mundo!', 'meta': {'source_lang': 'EN', 'target_lang': 'ES'}}\n```\n\n### Standalone Document Translation\n\n```python\nfrom haystack.dataclasses import Document\nfrom haystack.utils import Secret\n\nfrom deepl_haystack import DeepLDocumentTranslator\n\ntranslator = DeepLDocumentTranslator(\n    api_key=Secret.from_token(\"your_api_key_here\"), source_lang=\"EN\", target_lang=\"ES\"\n)\n\ndocuments_to_translate = [\n    Document(content=\"Hello, world!\"),\n    Document(content=\"Goodbye, Joe!\", meta={\"name\": \"Joe\"}),\n]\n\ntranslated_documents = translator.run(documents_to_translate)\nprint(\"\\n\".join([f\"{doc.content}, {doc.meta}\" for doc in translated_documents]))\n# ¡Hola, mundo!, {'source_lang': 'EN', 'target_lang': 'ES'}\n# ¡Adiós, Joe!, {'name': 'Joe', 'source_lang': 'EN', 'target_lang': 'ES'}\n```\n\n### Haystack Pipeline Integration\n\n```python\nfrom haystack import Pipeline\nfrom haystack.components.converters import TextFileToDocument\nfrom haystack.components.writers import DocumentWriter\nfrom haystack.dataclasses.byte_stream import ByteStream\nfrom haystack.document_stores.in_memory import InMemoryDocumentStore\nfrom haystack.utils import Secret\n\nfrom deepl_haystack import DeepLDocumentTranslator\n\ndocument_store = InMemoryDocumentStore()\n\npipeline = Pipeline()\npipeline.add_component(instance=TextFileToDocument(), name=\"converter\")\npipeline.add_component(\n    instance=DeepLDocumentTranslator(\n        api_key=Secret.from_token(\"your_api_key_here\"),\n        target_lang=\"ES\",\n    ),\n    name=\"translator\",\n)\npipeline.add_component(\n    instance=DocumentWriter(document_store=document_store), name=\"document_store\"\n)\npipeline.connect(\"converter\", \"translator\")\npipeline.connect(\"translator\", \"document_store\")\npipeline.run({\"converter\": {\"sources\": [ByteStream.from_string(\"Hello world!\")]}})\nprint(document_store.filter_documents())\n# [Document(id=..., content: '¡Hola, mundo!', meta: {'source_lang': 'EN', 'language': 'ES'})]\n```\n\n## Contributing\n\n[Poetry](https://python-poetry.org) is the best way to interact with this project, to install it,\nfollow the official [Poetry installation guide](https://python-poetry.org/docs/#installation).\n\nWith `poetry` installed, one can install the project dependencies with:\n\n```shell\npoetry install\n```\n\nThen, to run the project unit tests:\n\n```shell\nmake test-unit\n```\n\nTo run the linters (`ruff` and `mypy`):\n\n```shell\nmake lint\n```\n\nTo apply all code formatting:\n\n```shell\nmake format\n```\n\nAnd finally, to run the project integration tests (which actually use the DeepL API),\nyou should either have the `DEEPL_API_KEY` environment variable set,\nor create a `.env` file:\n\n```dotenv\nDEEPL_API_KEY=your_api_key_here\n```\n\nAnd run:\n\n```shell\nmake test-integration\n```\n\n## License\n\n`deepl-haystack` is distributed under the terms of the\n[MIT](https://opensource.org/license/mit) license.\nCheck the [LICENSE](./LICENSE) file for further details.\n"
    },
    {
      "name": "xtreamsrl/genai-for-engineers-class",
      "stars": 4,
      "img": "https://avatars.githubusercontent.com/u/38501645?s=40&v=4",
      "owner": "xtreamsrl",
      "repo_name": "genai-for-engineers-class",
      "description": "Code and material for the class \"Introduction to GenAI for Engineers\"",
      "homepage": "https://xtreamers.io",
      "language": "Jupyter Notebook",
      "created_at": "2024-06-28T17:43:56Z",
      "updated_at": "2024-11-08T18:54:58Z",
      "topics": [
        "ai",
        "education",
        "generative-ai",
        "python"
      ],
      "readme": "# GenAI for Engineers Class\n\nCode and material for the class \"Introduction to GenAI for Engineers\"\n\n## 🚀 Let's get started!\n\n**#** | **name**                                                                             | **open in**\n:-----: |:-------------------------------------------------------------------------------------| :-------:\n1 | [Explore Embeddings](./notebooks/01-embeddings.ipynb)                                | [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/xtreamsrl/genai-for-engineers-class/blob/main/notebooks/01-embeddings.ipynb)          \n2 | [What are Vector Databases?](./notebooks/02-vector_databases.ipynb)                  | [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/xtreamsrl/genai-for-engineers-class/blob/main/notebooks/02-vector_databases.ipynb) \n3 | [RAG From Scratch](./notebooks/03-rag_from_scratch.ipynb)                            | [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/xtreamsrl/genai-for-engineers-class/blob/main/notebooks/03-rag_from_scratch.ipynb) \n4 | [Haystack Basics](./notebooks/04-haystack_basics.ipynb)                              | [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/xtreamsrl/genai-for-engineers-class/blob/main/notebooks/04-haystack_basics.ipynb) \n5 | [Haystack RAG](./notebooks/05-haystack_rag.ipynb)                                    | [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/xtreamsrl/genai-for-engineers-class/blob/main/notebooks/05-haystack_rag.ipynb) \n6 | [Guardrails](./notebooks/06-guardrails.ipynb)                                        | [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/xtreamsrl/genai-for-engineers-class/blob/main/notebooks/06-guardrails.ipynb) \n7 | [Observability](./notebooks/07-observability.ipynb)                                  | [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/xtreamsrl/genai-for-engineers-class/blob/main/notebooks/07-observability.ipynb) \n8 | [Function Calling](./notebooks/08-function_calling.ipynb)                            | [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/xtreamsrl/genai-for-engineers-class/blob/main/notebooks/08-function_calling.ipynb) \n9 | [Agents](./notebooks/09-agents.ipynb)                                                | [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/xtreamsrl/genai-for-engineers-class/blob/main/notebooks/09-agents.ipynb) \n10 | [Agents for Income Statement Analysis](./notebooks/10-agents_income_statement.ipynb) | [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/xtreamsrl/genai-for-engineers-class/blob/main/notebooks/10-agents_income_statement.ipynb) \n\n\n## 🎓 A bit of theory\n\nThe slides for this class are available on [Canva](https://www.canva.com/design/DAGI8YciVf0/nqo2qMxGM-q4_itr72_clw/edit?utm_content=DAGI8YciVf0&utm_campaign=designshare&utm_medium=link2&utm_source=sharebutton), kindly powered by [xtream](https://xtreamers.io).\n\n\n## 🥂 How to contribute\n\n> [!NOTE]\n> The project uses Python 3.10 for compatibility with Colab.\n\n1. Install Poetry, following the official docs: https://python-poetry.org/docs/#installation\n\n2. Run:\n\n```bash\npoetry install\n```\n\n3. It is highly recommended to use `nbstripout` to avoid pushing the output of jupyter notebooks.\n   Install it with:\n\n```bash\npre-commit install\n```\n\n## 🤗 Authors\nThis class was created by the AI team at [xtream](https://xtreamers.io), with contributions from:\n- [Emanuele Fabbiani](https://www.linkedin.com/in/emanuelefabbiani/)\n- [Luca Baggi](https://www.linkedin.com/in/lucabaggi/)\n- [Gabriele Orlandi](https://www.linkedin.com/in/gabri-o/)\n- [Fabio Lipreri](https://www.linkedin.com/in/fabiolipreri/)\n- [Marta Peroni](https://www.linkedin.com/in/peroni-marta-19145499/)\n\n## 🔍 Spotted in...\nThe material in this repository was used in classes and seminars taught at:\n\n- [TWIN Agency](https://twin.services/en/), 2024\n- [CRIF](https://www.crif.it/), 2024\n- [WeRoad](https://www.weroad.it/), 2024\n- [Banca CF+](https://www.bancacfplus.it/), 2023\n- [Boolean Dataweek](https://boolean.careers/), 2023\n\nAnd was the base for the following talks and workshops:\n\n- Embeddings, Transformers, RLHF: Three Key Ideas to Understand ChatGPT, [AI Conf](https://www.aiconf.it/), 2024, Milan, Italy\n- How to Build Your Own GPT, [AMLD](https://appliedmldays.org/), 2024, Lausanne, Switzerland\n- Embeddings, Transformers, RLHF: Three Key Ideas to Understand ChatGPT, [PyCon IT](https://2024.pycon.it/en), 2024, Florence, Italy\n- Beyond ChatGPT: RAG and Fine-Tuning, University of Pavia, 2024, Pavia, Italy\n- Embeddings, Transformers, RLHF: Three Key Ideas to Understand ChatGPT, [BI Digital](https://bidigital.it/), 2023, Biella, Italy\n- Embeddings, Transformers, RLHF: Three Key Ideas to Understand ChatGPT, [SIIAM](https://www.siiam.it/en) Congress, 2023, Rome, Italy\n"
    },
    {
      "name": "davidberenstein1957/paper-prowler",
      "stars": 4,
      "img": "https://avatars.githubusercontent.com/u/25269220?s=40&v=4",
      "owner": "davidberenstein1957",
      "repo_name": "paper-prowler",
      "description": "A versatile tool that aggregates and organizes web content like RSS feeds, arXiv papers to streamline your research and information management.",
      "homepage": "https://davidberenstein1957.github.io/paper-prowler/dev",
      "language": "Python",
      "created_at": "2024-06-16T12:50:05Z",
      "updated_at": "2024-08-22T07:15:13Z",
      "topics": [],
      "readme": "# Paper Prowler\n\nA versatile tool that aggregates and organizes web content like RSS feeds, arXiv papers to streamline your research and information management.\n"
    },
    {
      "name": "katimanova/advanced_rag",
      "stars": 4,
      "img": "https://avatars.githubusercontent.com/u/73062632?s=40&v=4",
      "owner": "katimanova",
      "repo_name": "advanced_rag",
      "description": null,
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2024-06-11T20:05:56Z",
      "updated_at": "2024-09-30T18:47:50Z",
      "topics": [],
      "readme": "# Advanced RAG. Программные методы повышения качества работы LLM в вопросно-ответных системах\n\n### Немного о чем работа\nВместо обучения языковых моделей для нужной предметной области можно использовать алгоритм RAG, который подключает к исходной модели базу данных с необходимой информацией (в данном случае пользовательские соглашения Tinkoff) и помогает производить по ней поиск. Базовые алгоритмы RAG уже обладают хорошей точностью порядка 70-80%. Для повышения качества ответа используется Advanced RAG, который включает тонкую настройку модулей алгоритма RAG. В этом репозитории будут представлены методы, способные повысить точность ответа до 94%.\n\n\n## Базовый RAG\n\n![Base_RAG.png](Schemes/Base_RAG.png)\n\nРеализация - STEPPING/Base_RAG.ipynb\n\n| **Metric**            | **Base rag** |\n|-----------------------|--------------|\n| **Faithfulness**      | 0.839286     |\n| **Answer relevancy**  | 0.711729     |\n| **Context relevancy** | 0.754918     |\n| **Answer similarity** | 0.923925     |\n| **Sim-spacy**         | 0.857789     |\n| **Human score**       | 0.689655     |\n\n\n# Advanced RAG\nСледующая схема показывает какие модули будут улучшаться\n\n<img src=\"Schemes/Advanced_RAG.png\" width=\"600\" alt=\"Advanced_RAG\">\n\n# Модули Advanced RAG:\n\n\n## Splitting\n\n- Реализация - STEPPING/RAG_Splitting.ipynb\n- Результат  - all_ratings/final_result/final_step_splitting.ipynb\n\n\n| Метрика            | Base rag | RAG standart_split 1800 | RAG standart_split 1200/100 | My html_split | html_split 1800 / 100 | html_split 1000 / 100 |\n|---------------------|----------|-------------------------|-------------------------------|-----------------------------|------------------------|-----------------------|\n| Faithfulness        | 0.839286 | 0.779718                | 0.850758                      | 0.931481                    | 0.869792               | 0.923077              |\n| Answer relevancy    | 0.711729 | 0.675531                | 0.633100                      | 0.579447                    | 0.500734               | 0.605244              |\n| Context relevancy   | 0.754918 | 0.793711                | 0.801757                      | 0.774458                    | 0.811815               | 0.792418              |\n| Answer similarity   | 0.923925 | 0.912350                | 0.914586                      | 0.909015                    | 0.908620               | 0.922121              |\n| Sim-spacy           | 0.857789 | 0.817984                | 0.807411                      | 0.829006                    | 0.837442               | 0.844741              |\n\n\n## Query Translation\n\nСхемы 3 методов:\n<div align=\"center\">\n  <img src=\"Schemes/Multi-Query.png\" width=\"400\" alt=\"Multi-Query\">\n  <img src=\"Schemes/Rag-Fusion.png\" width=\"400\" alt=\"Rag-Fusion\">\n  <img src=\"Schemes/Step_back.png\" width=\"400\" alt=\"Step-back\">\n</div>\n\n\n- Реализация - STEPPING/RAG_Query_Translation.ipynb\n- Результат  - all_ratings/final_result/final_step_query_translation.ipynb\n\n| Метрика            | Base rag | standart_split - rag_step_back | html_split - rag_fusion | html_split - rag_multi_query | html_split - rag_step_back |\n|---------------------|----------|--------------------------------|-------------------------|------------------------------|-----------------------------|\n| Faithfulness        | 0.839286 | 0.882857                       | 0.934524                | 0.802299                     | 0.894144                    |\n| Answer relevancy    | 0.711729 | 0.775836                       | 0.781693                | 0.671017                     | 0.833722                    |\n| Context relevancy   | 0.754918 | 0.798022                       | 0.803194                | 0.787677                     | 0.793711                    |\n| Answer similarity   | 0.923925 | 0.932193                       | 0.925964                | 0.916761                     | 0.929158                    |\n| Sim-spacy           | 0.857789 | 0.882171                       | 0.854943                | 0.834115                     | 0.862769                    |\n\n\n## Retrieval\n\nСхемы 2 подходов:\n<div align=\"center\">\n  <img src=\"Schemes/Retrieval_serch_type.png\" width=\"600\" alt=\"Retrieval_serch_type\">\n  <img src=\"Schemes/Retrieval_rerank.png\" width=\"600\" alt=\"Retrieval_rerank\">\n</div>\n\n\n- Реализация - STEPPING/RAG_Retrieval.ipynb   |    STEPPING/RAG_Retrieval_Rerank.ipynb\n- Результат  - all_ratings/final_result/final_step_retrieval.ipynb\n\n| Метрика            | Base rag | html_split - 3_similiatry | html_split - compressor_rerank | html_split - rerank_gigachat | html_split - rerank_gpt_3.5 |\n|---------------------|----------|---------------------------|---------------------------------|-------------------------------|-----------------------------|\n| Faithfulness        | 0.839286 | 0.903659                  | 0.934314                        | 0.842449                      | 0.916155                    |\n| Answer relevancy    | 0.711729 | 0.706977                  | 0.721179                        | 0.763434                      | 0.853961                    |\n| Context relevancy   | 0.754918 | 0.785522                  | 0.818568                        | 0.782361                      | 0.803194                    |\n| Answer similarity  | 0.923925 | 0.928373                  | 0.924924                        | 0.921375                      | 0.922967                    |\n| Sim-spacy           | 0.857789 | 0.841781                  | 0.863760                        | 0.866616                      | 0.893530                    |\n\n\n## Routing\n\n<img src=\"Schemes/Routing.png\" width=\"600\" alt=\"Routing\">\n\n\n- Реализация - STEPPING/RAG_Routing.ipynb\n- Результат  - all_ratings/final_result/final_step_routing.ipynb\n\n| Метрика            | Base rag | standart_split - routing | standart_split - routing_1900 | html_split - routing | html_split - routing_1900_promt |\n|---------------------|----------|--------------------------|--------------------------------|----------------------|---------------------------------|\n| Faithfulness        | 0.839286 | 0.824797                 | 0.878231                       | 0.805952             | 0.924962                        |\n| Answer relevancy    | 0.711729 | 0.656595                 | 0.610366                       | 0.602522             | 0.704511                        |\n| Context relevancy   | 0.754918 | 0.773022                 | 0.807504                       | 0.773022             | 0.769430                        |\n| Answer similarity  | 0.923925 | 0.917842                 | 0.913813                       | 0.910303             | 0.922524                        |\n| Sim-spacy           | 0.857789 | 0.844044                 | 0.833408                       | 0.815391             | 0.858856                        |\n\n\n## Комбинации Advanced RAG из всех модулей\n\n\n<img src=\"Schemes/Methods_Advanced_RAG.png\" width=\"800\" alt=\"Methods_Advanced_RAG\">\n\n\n- Реализация - STEPPING/Advanced_RAG.ipynb\n- Результат  - all_ratings/final_result/final_step_advanced_rag_mini.ipynb\n\n| Метрика            | Base rag | Base rag gpt | RAG+_1_gpt_3.5 | RAG+_3_gpt_3.5 | RAG+_4_gpt_3.5 | RAG+_5_gpt_3.5 |\n|---------------------|----------|--------------|----------------|----------------|----------------|----------------|\n| Faithfulness        | 0.839286 | 0.854735     | 0.934930       | 0.716726       | 0.716726       | 0.877778       |\n| Answer relevancy    | 0.711729 | 0.839272     | 0.832147       | 0.751150       | 0.751150       | 0.842827       |\n| Context relevancy   | 0.754918 | 0.798884     | 0.767275       | 0.783798       | 0.783798       | 0.792418       |\n| Answer similarity  | 0.923925 | 0.926247     | 0.927357       | 0.920985       | 0.920985       | 0.929259       |\n| Sim-spacy           | 0.857789 | 0.886603     | 0.874437       | 0.868104       | 0.868104       | 0.898265       |\n| Human score         | 0.689655 | 0.724138     | 0.827586       | 0.939655       | 0.758621       | 0.870690       |\n\n"
    },
    {
      "name": "domenicocinque/web-rag",
      "stars": 4,
      "img": "https://avatars.githubusercontent.com/u/72546227?s=40&v=4",
      "owner": "domenicocinque",
      "repo_name": "web-rag",
      "description": "A RAG pipeline that searches on the web. Powered by Haystack 2.0. ",
      "homepage": "",
      "language": "Python",
      "created_at": "2024-01-28T15:17:18Z",
      "updated_at": "2025-03-17T20:28:51Z",
      "topics": [
        "haystack",
        "nlp",
        "python",
        "rag"
      ],
      "readme": "# Web RAG \n\n## Description\n\nExample of FastAPI for web-based RAG using Haystack. \n\nThe component that searches on the web uses the `googlesearch` library, which will return 429 errors if used too much in a short period of time (related issue: [here](https://github.com/Nv7-GitHub/googlesearch/issues/61))\n\n\n## Installation\n\n1. Clone this repository.\n2. Install the required Python packages by running `pip install -r requirements.txt`, or `uv sync`.\n3. Set your `OPENAI_API_KEY` environment variable to your OpenAI API key.\n\n## Usage\n\nRun the app with \n\n```sh\npython -m uvicorn app.main:app\n```\n\nor with `just run dev`. \n\n\n\n\n\n"
    },
    {
      "name": "awinml/llama-cpp-haystack",
      "stars": 4,
      "img": "https://avatars.githubusercontent.com/u/97467100?s=40&v=4",
      "owner": "awinml",
      "repo_name": "llama-cpp-haystack",
      "description": "Custom component for Haystack (2.x) for running LLMs using the Llama.cpp LLM framework.",
      "homepage": "",
      "language": "Python",
      "created_at": "2024-01-04T11:42:37Z",
      "updated_at": "2024-01-07T03:01:49Z",
      "topics": [
        "haystack",
        "llamacpp"
      ],
      "readme": "<!--\n[![PyPI](https://img.shields.io/pypi/v/llama-cpp-haystack)](https://pypi.org/project/llama-cpp-haystack/)\n![PyPI - Downloads](https://img.shields.io/pypi/dm/llama-cpp-haystack?color=blue&logo=pypi&logoColor=gold)\n![PyPI - Python Version](https://img.shields.io/pypi/pyversions/llama-cpp-haystack?logo=python&logoColor=gold)\n-->\n\n[![GitHub](https://img.shields.io/github/license/awinml/llama-cpp-haystack?color=green)](LICENSE)\n[![Actions status](https://github.com/awinml/llama-cpp-haystack/workflows/Test/badge.svg)](https://github.com/awinml/llama-cpp-haystack/actions)\n[![Coverage Status](https://coveralls.io/repos/github/awinml/llama-cpp-haystack/badge.svg?branch=main)](https://coveralls.io/github/awinml/llama-cpp-haystack?branch=main)\n[![Types - Mypy](https://img.shields.io/badge/types-Mypy-blue.svg)](https://github.com/python/mypy)\n[![Ruff](https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/astral-sh/ruff/main/assets/badge/v2.json)](https://github.com/astral-sh/ruff)\n[![Code Style - Black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)\n\n<h1 align=\"center\"> <a href=\"https://github.com/awinml/llama-cpp-haystack\"> Llama.cpp Integration - Haystack </a> </h1>\n\nCustom component for [Haystack](https://github.com/deepset-ai/haystack) (2.x) for running LLMs using the [Llama.cpp](https://github.com/ggerganov/llama.cpp) LLM framework. This implementation leverages the [Python Bindings for llama.cpp](https://github.com/abetlen/llama-cpp-python).\n\n#### What's New\n\n- **[v0.0.1 - 05/01/24]:** Added `LlamaCppGenerator` to run LLMs using llama.cpp for text generation.\n\n## Installation\n\n```bash\npip install git+https://github.com/awinml/llama-cpp-haystack.git@main#egg=llama-cpp-haystack\n```\n\nThe default install behaviour is to build `llama.cpp` for CPU only on Linux and Windows and use Metal on MacOS.\n\nTo install using the other backends, first install [llama-cpp-python](https://github.com/abetlen/llama-cpp-python) using the instructions on their [installation documentation](https://github.com/abetlen/llama-cpp-python#installation) and then install [llama-cpp-haystack](https://github.com/awinml/llama-cpp-haystack).\n\n\nFor example, to use `llama-cpp-haystack` with the cuBLAS backend:\n\n```bash\nexport LLAMA_CUBLAS=1\nCMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" pip install llama-cpp-python\npip install git+https://github.com/awinml/llama-cpp-haystack.git@main#egg=llama-cpp-haystack\n```\n\n## Usage\n\nYou can utilize the [`LlamaCppGenerator`](https://github.com/awinml/llama-cpp-haystack/blob/main/src/llama_cpp_haystack/generator.py) to load models quantized using llama.cpp (GGUF) for text generation.\n\nInformation about the supported models and model parameters can be found on the llama.cpp [documentation](https://llama-cpp-python.readthedocs.io/en/latest).\n\nThe GGUF versions of popular models can be downloaded from [HuggingFace](https://huggingface.co/models?library=gguf).\n\n### Passing additional model parameters\n\nThe `model_path`, `n_ctx`, `n_batch` arguments have been exposed for convenience and can be directly passed to the Generator during initialization as keyword arguments.  \n\nThe `model_kwargs` parameter can be used to pass additional arguments when initializing the model. In case of duplication, these kwargs override `model_path`, `n_ctx`, and `n_batch` init parameters.\n\nSee Llama.cpp's [model documentation](https://llama-cpp-python.readthedocs.io/en/latest/api-reference/#llama_cpp.Llama.__init__) for more information on the available model arguments.\n\nFor example, to offload the model to GPU during initialization:\n\n```python\nfrom llama_cpp_haystack import LlamaCppGenerator\n\ngenerator = LlamaCppGenerator(\n    model_path=\"/content/openchat-3.5-1210.Q3_K_S.gguf\", \n    n_ctx=512,\n    n_batch=128,\n    model_kwargs={\"n_gpu_layers\": -1}\n)\ngenerator.warm_up()\n\ninput = \"Who is the best American actor?\"\nprompt = f\"GPT4 Correct User: {input} <|end_of_turn|> GPT4 Correct Assistant:\"\n\nresult = generator.run(prompt, generation_kwargs={\"max_tokens\": 128})\ngenerated_text = result[\"replies\"][0]\n\nprint(generated_text)\n```\n### Passing generation parameters\n\nThe `generation_kwargs` parameter can be used to pass additional generation arguments like `max_tokens`, `temperature`, `top_k`, `top_p`, etc to the model during inference. \n\nSee Llama.cpp's [`create_completion` documentation](https://llama-cpp-python.readthedocs.io/en/latest/api-reference/#llama_cpp.Llama.create_completion) for more information on the available generation arguments.\n\nFor example, to set the `max_tokens` and `temperature`:\n\n```python\nfrom llama_cpp_haystack import LlamaCppGenerator\n\ngenerator = LlamaCppGenerator(\n    model_path=\"/content/openchat-3.5-1210.Q3_K_S.gguf\",\n    n_ctx=512,\n    n_batch=128,\n    generation_kwargs={\"max_tokens\": 128, \"temperature\": 0.1},\n)\ngenerator.warm_up()\n\ninput = \"Who is the best American actor?\"\nprompt = f\"GPT4 Correct User: {input} <|end_of_turn|> GPT4 Correct Assistant:\"\n\nresult = generator.run(prompt)\ngenerated_text = result[\"replies\"][0]\n\nprint(generated_text)\n```\nThe `generation_kwargs` can also be passed to the `run` method of the generator directly:\n\n```python\nfrom llama_cpp_haystack import LlamaCppGenerator\n\ngenerator = LlamaCppGenerator(\n    model_path=\"/content/openchat-3.5-1210.Q3_K_S.gguf\",\n    n_ctx=512,\n    n_batch=128,\n)\ngenerator.warm_up()\n\ninput = \"Who is the best American actor?\"\nprompt = f\"GPT4 Correct User: {input} <|end_of_turn|> GPT4 Correct Assistant:\"\n\nresult = generator.run(\n    prompt,\n    generation_kwargs={\"max_tokens\": 128, \"temperature\": 0.1},\n)\ngenerated_text = result[\"replies\"][0]\n\nprint(generated_text)\n```\n\n## Example\n\nBelow is the example Retrieval Augmented Generation pipeline that uses the [Simple Wikipedia](https://huggingface.co/datasets/pszemraj/simple_wikipedia) Dataset from HuggingFace. You can find more examples in the [`examples`](https://github.com/awinml/llama-cpp-haystack/tree/main/examples) folder.\n\n\nLoad the dataset:\n\n```python\n# Install HuggingFace Datasets using \"pip install datasets\"\nfrom datasets import load_dataset\nfrom haystack import Document, Pipeline\nfrom haystack.components.builders.answer_builder import AnswerBuilder\nfrom haystack.components.builders.prompt_builder import PromptBuilder\nfrom haystack.components.embedders import SentenceTransformersDocumentEmbedder, SentenceTransformersTextEmbedder\nfrom haystack.components.retrievers import InMemoryEmbeddingRetriever\nfrom haystack.components.writers import DocumentWriter\nfrom haystack.document_stores import InMemoryDocumentStore\n\n# Import LlamaCppGenerator\nfrom llama_cpp_haystack import LlamaCppGenerator\n\n# Load first 100 rows of the Simple Wikipedia Dataset from HuggingFace\ndataset = load_dataset(\"pszemraj/simple_wikipedia\", split=\"validation[:100]\")\n\ndocs = [\n    Document(\n        content=doc[\"text\"],\n        meta={\n            \"title\": doc[\"title\"],\n            \"url\": doc[\"url\"],\n        },\n    )\n    for doc in dataset\n]\n```\n\nIndex the documents to the `InMemoryDocumentStore` using the `SentenceTransformersDocumentEmbedder` and `DocumentWriter`:\n\n```python\ndoc_store = InMemoryDocumentStore(embedding_similarity_function=\"cosine\")\ndoc_embedder = SentenceTransformersDocumentEmbedder(model_name_or_path=\"sentence-transformers/all-MiniLM-L6-v2\")\n\n# Indexing Pipeline\nindexing_pipeline = Pipeline()\nindexing_pipeline.add_component(instance=doc_embedder, name=\"DocEmbedder\")\nindexing_pipeline.add_component(instance=DocumentWriter(document_store=doc_store), name=\"DocWriter\")\nindexing_pipeline.connect(connect_from=\"DocEmbedder\", connect_to=\"DocWriter\")\n\nindexing_pipeline.run({\"DocEmbedder\": {\"documents\": docs}})\n```\n\nCreate the Retrieval Augmented Generation (RAG) pipeline and add the `LlamaCppGenerator` to it:\n\n```python\n# Prompt Template for the https://huggingface.co/openchat/openchat-3.5-1210 LLM\nprompt_template = \"\"\"GPT4 Correct User: Answer the question using the provided context.\nQuestion: {{question}}\nContext:\n{% for doc in documents %}\n    {{ doc.content }}\n{% endfor %}\n<|end_of_turn|>\nGPT4 Correct Assistant:\n\"\"\"\n\nrag_pipeline = Pipeline()\n\ntext_embedder = SentenceTransformersTextEmbedder(model_name_or_path=\"sentence-transformers/all-MiniLM-L6-v2\")\n\n# Load the LLM using LlamaCppGenerator\nmodel_path = \"openchat-3.5-1210.Q3_K_S.gguf\"\ngenerator = LlamaCppGenerator(model_path=model_path, n_ctx=4096, n_batch=128)\n\nrag_pipeline.add_component(\n    instance=text_embedder,\n    name=\"text_embedder\",\n)\nrag_pipeline.add_component(instance=InMemoryEmbeddingRetriever(document_store=doc_store, top_k=3), name=\"retriever\")\nrag_pipeline.add_component(instance=PromptBuilder(template=prompt_template), name=\"prompt_builder\")\nrag_pipeline.add_component(instance=generator, name=\"llm\")\nrag_pipeline.add_component(instance=AnswerBuilder(), name=\"answer_builder\")\n\nrag_pipeline.connect(\"text_embedder\", \"retriever\")\nrag_pipeline.connect(\"retriever\", \"prompt_builder.documents\")\nrag_pipeline.connect(\"prompt_builder\", \"llm\")\nrag_pipeline.connect(\"llm.replies\", \"answer_builder.replies\")\nrag_pipeline.connect(\"retriever\", \"answer_builder.documents\")\n```\n\nRun the pipeline:\n\n```python\nquestion = \"Which year did the Joker movie release?\"\nresult = rag_pipeline.run(\n    {\n        \"text_embedder\": {\"text\": question},\n        \"prompt_builder\": {\"question\": question},\n        \"llm\": {\"generation_kwargs\": {\"max_tokens\": 128, \"temperature\": 0.1}},\n        \"answer_builder\": {\"query\": question},\n    }\n)\n\ngenerated_answer = result[\"answer_builder\"][\"answers\"][0]\nprint(generated_answer.data)\n# The Joker movie was released on October 4, 2019.\n```\n\n\n## Contributing\n\nPull requests are welcome. For significant changes, kindly open an issue to discuss proposed modifications beforehand.\n\n## Author\n\n[Ashwin Mathur](https://github.com/awinml)\n\n## License\n\n`llama-cpp-haystack` is distributed under the terms of the [Apache-2.0 license](https://github.com/awinml/llama-cpp-haystack/blob/main/LICENSE).\n"
    },
    {
      "name": "awinml/voyage-embedders-haystack",
      "stars": 4,
      "img": "https://avatars.githubusercontent.com/u/97467100?s=40&v=4",
      "owner": "awinml",
      "repo_name": "voyage-embedders-haystack",
      "description": "Custom components for Haystack for creating embeddings and reranking documents using the VoyageAI Models.",
      "homepage": "",
      "language": "Python",
      "created_at": "2023-11-15T06:37:22Z",
      "updated_at": "2025-01-22T11:52:37Z",
      "topics": [
        "embeddings",
        "haystack",
        "voyageai"
      ],
      "readme": "[![PyPI](https://img.shields.io/pypi/v/voyage-embedders-haystack)](https://pypi.org/project/voyage-embedders-haystack/)\n![PyPI - Downloads](https://img.shields.io/pypi/dm/voyage-embedders-haystack?color=blue&logo=pypi&logoColor=gold)\n![PyPI - Python Version](https://img.shields.io/pypi/pyversions/voyage-embedders-haystack?logo=python&logoColor=gold)\n[![GitHub](https://img.shields.io/github/license/awinml/voyage-embedders-haystack?color=green)](LICENSE)\n[![Actions status](https://github.com/awinml/voyage-embedders-haystack/workflows/Test/badge.svg)](https://github.com/awinml/voyage-embedders-haystack/actions)\n[![Coverage Status](https://coveralls.io/repos/github/awinml/voyage-embedders-haystack/badge.svg?branch=main)](https://coveralls.io/github/awinml/voyage-embedders-haystack?branch=main)\n\n[![Types - Mypy](https://img.shields.io/badge/types-Mypy-blue.svg)](https://github.com/python/mypy)\n[![Ruff](https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/astral-sh/ruff/main/assets/badge/v2.json)](https://github.com/astral-sh/ruff)\n[![Code Style - Black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)\n\n<h1 align=\"center\"> <a href=\"https://github.com/awinml/voyage-embedders-haystack\"> Voyage Embedders and Rankers - Haystack </a> </h1>\n\nCustom components for [Haystack](https://github.com/deepset-ai/haystack) for creating embeddings and reranking documents using the [Voyage Models](https://voyageai.com/).\n\nVoyage’s embedding models are state-of-the-art in retrieval accuracy. These models outperform top performing embedding models like `intfloat/e5-mistral-7b-instruct` and `OpenAI/text-embedding-3-large` on the [MTEB Benchmark](https://github.com/embeddings-benchmark/mteb).\n\n#### What's New\n\n- **[v1.5.0 - 22/01/25]:**\n\n  - The new `VoyageRanker` component can be used to rerank documents using the `Voyage Reranker` models.\n  - Matryoshka Embeddings and Quantized Embeddings can now be created using the `output_dimension` and `output_dtype` parameters.\n\n- **[v1.4.0 - 24/07/24]:**\n\n  - The maximum timeout and number of retries made by the Client can now be set for the embedders using the `timeout` and `max_retries` parameters.\n\n- **[v1.3.0 - 18/03/24]:**\n\n  - **Breaking Change:** The import path for the embedders has been changed to `haystack_integrations.components.embedders.voyage_embedders`.\n    Please replace all instances of `from voyage_embedders.voyage_document_embedder import VoyageDocumentEmbedder` and `from voyage_embedders.voyage_text_embedder import VoyageTextEmbedder` with  \n    `from haystack_integrations.components.embedders.voyage_embedders import VoyageDocumentEmbedder, VoyageTextEmbedder`.\n  - The embedders now use the Haystack `Secret` API for authentication. For more information please see the [Secret Management Documentation](https://docs.haystack.deepset.ai/docs/secret-management).\n\n- **[v1.2.0 - 02/02/24]:**\n\n  - **Breaking Change:** `VoyageDocumentEmbedder` and `VoyageTextEmbedder` now accept the `model` parameter instead of `model_name`.\n  - The embedders have been use the new `voyageai.Client.embed()` method instead of the deprecated `get_embedding` and `get_embeddings` methods of the global namespace.\n  - Support for the new `truncate` parameter has been added.\n  - Default embedding model has been changed to \"voyage-2\" from the deprecated \"voyage-01\".\n  - The embedders now return the total number of tokens used as part of the `\"total_tokens\"` in the metadata.\n\n- **[v1.1.0 - 13/12/23]:** Added support for `input_type` parameter in `VoyageTextEmbedder` and `VoyageDocument Embedder`.\n\n- **[v1.0.0 - 21/11/23]:** Added `VoyageTextEmbedder` and `VoyageDocument Embedder` to embed strings and documents.\n\n## Installation\n\n```bash\npip install voyage-embedders-haystack\n```\n\n## Usage\n\nYou can use Voyage Embedding models with two components: [VoyageTextEmbedder](https://github.com/awinml/voyage-embedders-haystack/blob/main/src/voyage_embedders/voyage_text_embedder.py) and [VoyageDocumentEmbedder](https://github.com/awinml/voyage-embedders-haystack/blob/main/src/voyage_embedders/voyage_document_embedder.py).\n\nTo create semantic embeddings for documents, use `VoyageDocumentEmbedder` in your indexing pipeline. For generating embeddings for queries, use `VoyageTextEmbedder`.\n\nThe Voyage Reranker models can be used with the [VoyageRanker](https://github.com/awinml/voyage-embedders-haystack/blob/main/src/haystack_integrations/components/rankers/voyage/ranker.py) component.\n\nOnce you've selected the suitable component for your specific use case, initialize the component with the model name and VoyageAI API key. You can also\nset the environment variable `VOYAGE_API_KEY` instead of passing the API key as an argument.\nTo get an API key, please see the [Voyage AI website.](https://www.voyageai.com/)\n\nInformation about the supported models, can be found on the [Voyage AI Documentation.](https://docs.voyageai.com/)\n\n## Example\n\nYou can find all the examples in the [`examples`](https://github.com/awinml/voyage-embedders-haystack/tree/main/examples) folder.\n\nBelow is the example Semantic Search pipeline that uses the [Simple Wikipedia](https://huggingface.co/datasets/pszemraj/simple_wikipedia) Dataset from HuggingFace.\n\nLoad the dataset:\n\n```python\n# Install HuggingFace Datasets using \"pip install datasets\"\nfrom datasets import load_dataset\nfrom haystack import Pipeline\nfrom haystack.components.retrievers.in_memory import InMemoryEmbeddingRetriever\nfrom haystack.components.writers import DocumentWriter\nfrom haystack.dataclasses import Document\nfrom haystack.document_stores.in_memory import InMemoryDocumentStore\n\n# Import Voyage Embedders\nfrom haystack_integrations.components.embedders.voyage_embedders import VoyageDocumentEmbedder, VoyageTextEmbedder\n\n# Load first 100 rows of the Simple Wikipedia Dataset from HuggingFace\ndataset = load_dataset(\"pszemraj/simple_wikipedia\", split=\"validation[:100]\")\n\ndocs = [\n    Document(\n        content=doc[\"text\"],\n        meta={\n            \"title\": doc[\"title\"],\n            \"url\": doc[\"url\"],\n        },\n    )\n    for doc in dataset\n]\n```\n\nIndex the documents to the `InMemoryDocumentStore` using the `VoyageDocumentEmbedder` and `DocumentWriter`:\n\n```python\ndoc_store = InMemoryDocumentStore(embedding_similarity_function=\"cosine\")\nretriever = InMemoryEmbeddingRetriever(document_store=doc_store)\ndoc_writer = DocumentWriter(document_store=doc_store)\n\ndoc_embedder = VoyageDocumentEmbedder(\n    model=\"voyage-2\",\n    input_type=\"document\",\n)\ntext_embedder = VoyageTextEmbedder(model=\"voyage-2\", input_type=\"query\")\n\n# Indexing Pipeline\nindexing_pipeline = Pipeline()\nindexing_pipeline.add_component(instance=doc_embedder, name=\"DocEmbedder\")\nindexing_pipeline.add_component(instance=doc_writer, name=\"DocWriter\")\nindexing_pipeline.connect(\"DocEmbedder\", \"DocWriter\")\n\nindexing_pipeline.run({\"DocEmbedder\": {\"documents\": docs}})\n\nprint(f\"Number of documents in Document Store: {len(doc_store.filter_documents())}\")\nprint(f\"First Document: {doc_store.filter_documents()[0]}\")\nprint(f\"Embedding of first Document: {doc_store.filter_documents()[0].embedding}\")\n```\n\nQuery the Semantic Search Pipeline using the `InMemoryEmbeddingRetriever` and `VoyageTextEmbedder`:\n\n```python\ntext_embedder = VoyageTextEmbedder(model=\"voyage-2\", input_type=\"query\")\n\n# Query Pipeline\nquery_pipeline = Pipeline()\nquery_pipeline.add_component(instance=text_embedder, name=\"TextEmbedder\")\nquery_pipeline.add_component(instance=retriever, name=\"Retriever\")\nquery_pipeline.connect(\"TextEmbedder.embedding\", \"Retriever.query_embedding\")\n\n# Search\nresults = query_pipeline.run({\"TextEmbedder\": {\"text\": \"Which year did the Joker movie release?\"}})\n\n# Print text from top result\ntop_result = results[\"Retriever\"][\"documents\"][0].content\nprint(\"The top search result is:\")\nprint(top_result)\n```\n\n## Contributing\n\nPull requests are welcome. For major changes, please open an issue first.\n\n## Author\n\n[Ashwin Mathur](https://github.com/awinml)\n\n## License\n\n`voyage-embedders-haystack` is distributed under the terms of the [Apache-2.0 license](https://github.com/awinml/voyage-embedders-haystack/blob/main/LICENSE).\n"
    },
    {
      "name": "aminzayer/notebooks",
      "stars": 3,
      "img": "https://avatars.githubusercontent.com/u/7605327?s=40&v=4",
      "owner": "aminzayer",
      "repo_name": "notebooks",
      "description": "Jupyter notebooks for the Natural Language Processing with Transformers book",
      "homepage": "https://transformersbook.com/",
      "language": "Jupyter Notebook",
      "created_at": "2022-02-15T20:47:19Z",
      "updated_at": "2023-10-21T14:30:30Z",
      "topics": [],
      "readme": "# Transformers Notebooks\n\nThis repository contains the example code from our O'Reilly book [Natural Language Processing with Transformers](https://www.oreilly.com/library/view/natural-language-processing/9781098136789/):\n\n<img alt=\"book-cover\" height=200 src=\"images/book_cover.jpg\" id=\"book-cover\"/>\n\n## Getting started\n\nYou can run these notebooks on cloud platforms like [Google Colab](https://colab.research.google.com/) or your local machine. Note that most chapters require a GPU to run in a reasonable amount of time, so we recommend one of the cloud platforms as they come pre-installed with CUDA.\n\n### Running on a cloud platform\n\nTo run these notebooks on a cloud platform, just click on one of the badges in the table below:\n\n<!--This table is automatically generated, do not fill manually!-->\n\n\n\n| Chapter                                     | Colab                                                                                                                                                                                               | Kaggle                                                                                                                                                                                                   | Gradient                                                                                                                                                                               | Studio Lab                                                                                                                                                                                                   |\n|:--------------------------------------------|:----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Introduction                                | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/nlp-with-transformers/notebooks/blob/main/01_introduction.ipynb)              | [![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/nlp-with-transformers/notebooks/blob/main/01_introduction.ipynb)              | [![Gradient](https://assets.paperspace.io/img/gradient-badge.svg)](https://console.paperspace.com/github/nlp-with-transformers/notebooks/blob/main/01_introduction.ipynb)              | [![Open In SageMaker Studio Lab](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/nlp-with-transformers/notebooks/blob/main/01_introduction.ipynb)              |\n| Text Classification                         | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/nlp-with-transformers/notebooks/blob/main/02_classification.ipynb)            | [![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/nlp-with-transformers/notebooks/blob/main/02_classification.ipynb)            | [![Gradient](https://assets.paperspace.io/img/gradient-badge.svg)](https://console.paperspace.com/github/nlp-with-transformers/notebooks/blob/main/02_classification.ipynb)            | [![Open In SageMaker Studio Lab](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/nlp-with-transformers/notebooks/blob/main/02_classification.ipynb)            |\n| Transformer Anatomy                         | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/nlp-with-transformers/notebooks/blob/main/03_transformer-anatomy.ipynb)       | [![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/nlp-with-transformers/notebooks/blob/main/03_transformer-anatomy.ipynb)       | [![Gradient](https://assets.paperspace.io/img/gradient-badge.svg)](https://console.paperspace.com/github/nlp-with-transformers/notebooks/blob/main/03_transformer-anatomy.ipynb)       | [![Open In SageMaker Studio Lab](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/nlp-with-transformers/notebooks/blob/main/03_transformer-anatomy.ipynb)       |\n| Multilingual Named Entity Recognition       | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/nlp-with-transformers/notebooks/blob/main/04_multilingual-ner.ipynb)          | [![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/nlp-with-transformers/notebooks/blob/main/04_multilingual-ner.ipynb)          | [![Gradient](https://assets.paperspace.io/img/gradient-badge.svg)](https://console.paperspace.com/github/nlp-with-transformers/notebooks/blob/main/04_multilingual-ner.ipynb)          | [![Open In SageMaker Studio Lab](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/nlp-with-transformers/notebooks/blob/main/04_multilingual-ner.ipynb)          |\n| Text Generation                             | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/nlp-with-transformers/notebooks/blob/main/05_text-generation.ipynb)           | [![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/nlp-with-transformers/notebooks/blob/main/05_text-generation.ipynb)           | [![Gradient](https://assets.paperspace.io/img/gradient-badge.svg)](https://console.paperspace.com/github/nlp-with-transformers/notebooks/blob/main/05_text-generation.ipynb)           | [![Open In SageMaker Studio Lab](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/nlp-with-transformers/notebooks/blob/main/05_text-generation.ipynb)           |\n| Summarization                               | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/nlp-with-transformers/notebooks/blob/main/06_summarization.ipynb)             | [![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/nlp-with-transformers/notebooks/blob/main/06_summarization.ipynb)             | [![Gradient](https://assets.paperspace.io/img/gradient-badge.svg)](https://console.paperspace.com/github/nlp-with-transformers/notebooks/blob/main/06_summarization.ipynb)             | [![Open In SageMaker Studio Lab](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/nlp-with-transformers/notebooks/blob/main/06_summarization.ipynb)             |\n| Question Answering                          | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/nlp-with-transformers/notebooks/blob/main/07_question-answering.ipynb)        | [![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/nlp-with-transformers/notebooks/blob/main/07_question-answering.ipynb)        | [![Gradient](https://assets.paperspace.io/img/gradient-badge.svg)](https://console.paperspace.com/github/nlp-with-transformers/notebooks/blob/main/07_question-answering.ipynb)        | [![Open In SageMaker Studio Lab](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/nlp-with-transformers/notebooks/blob/main/07_question-answering.ipynb)        |\n| Making Transformers Efficient in Production | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/nlp-with-transformers/notebooks/blob/main/08_model-compression.ipynb)         | [![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/nlp-with-transformers/notebooks/blob/main/08_model-compression.ipynb)         | [![Gradient](https://assets.paperspace.io/img/gradient-badge.svg)](https://console.paperspace.com/github/nlp-with-transformers/notebooks/blob/main/08_model-compression.ipynb)         | [![Open In SageMaker Studio Lab](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/nlp-with-transformers/notebooks/blob/main/08_model-compression.ipynb)         |\n| Dealing with Few to No Labels               | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/nlp-with-transformers/notebooks/blob/main/09_few-to-no-labels.ipynb)          | [![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/nlp-with-transformers/notebooks/blob/main/09_few-to-no-labels.ipynb)          | [![Gradient](https://assets.paperspace.io/img/gradient-badge.svg)](https://console.paperspace.com/github/nlp-with-transformers/notebooks/blob/main/09_few-to-no-labels.ipynb)          | [![Open In SageMaker Studio Lab](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/nlp-with-transformers/notebooks/blob/main/09_few-to-no-labels.ipynb)          |\n| Training Transformers from Scratch          | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/nlp-with-transformers/notebooks/blob/main/10_transformers-from-scratch.ipynb) | [![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/nlp-with-transformers/notebooks/blob/main/10_transformers-from-scratch.ipynb) | [![Gradient](https://assets.paperspace.io/img/gradient-badge.svg)](https://console.paperspace.com/github/nlp-with-transformers/notebooks/blob/main/10_transformers-from-scratch.ipynb) | [![Open In SageMaker Studio Lab](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/nlp-with-transformers/notebooks/blob/main/10_transformers-from-scratch.ipynb) |\n| Future Directions                           | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/nlp-with-transformers/notebooks/blob/main/11_future-directions.ipynb)         | [![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/nlp-with-transformers/notebooks/blob/main/11_future-directions.ipynb)         | [![Gradient](https://assets.paperspace.io/img/gradient-badge.svg)](https://console.paperspace.com/github/nlp-with-transformers/notebooks/blob/main/11_future-directions.ipynb)         | [![Open In SageMaker Studio Lab](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/nlp-with-transformers/notebooks/blob/main/11_future-directions.ipynb)         |\n\n<!--End of table-->\n\nNowadays, the GPUs on Colab tend to be K80s (which have limited memory), so we recommend using [Kaggle](https://www.kaggle.com/docs/notebooks), [Gradient](https://gradient.run/notebooks), or [SageMaker Studio Lab](https://studiolab.sagemaker.aws/). These platforms tend to provide more performant GPUs like P100s, all for free!\n\n> Note: some cloud platforms like Kaggle require you to restart the notebook after installing new packages.\n\n### Running on your machine\n\nTo run the notebooks on your own machine, first clone the repository and navigate to it:\n\n```bash\n$ git clone https://github.com/nlp-with-transformers/notebooks.git\n$ cd notebooks\n```\n\nNext, run the following command to create a `conda` virtual environment that contains all the libraries needed to run the notebooks:\n\n```bash\n$ conda env create -f environment.yml\n```\n\n> Note: You'll need a GPU that supports NVIDIA's [CUDA Toolkit](https://developer.nvidia.com/cuda-toolkit) to build the environment. Currently, this means you cannot build locally on Apple silicon 😢.\n\nChapter 7 (Question Answering) has a special set of dependencies, so to run that chapter you'll need a separate environment:\n\n```bash\n$ conda env create -f environment-chapter7.yml\n```\n\nOnce you've installed the dependencies, you can activate the `conda` environment and spin up the notebooks as follows:\n\n```bash\n$ conda activate book # or conda activate book-chapter7\n$ jupyter notebook\n```\n\n## FAQ\n\n### When trying to clone the notebooks on Kaggle I get a message that I am unable to access the book's Github repository. How can I solve this issue?\n\nThis issue is likely due to a missing internet connection. When running your first notebook on Kaggle you need to enable internet access in the settings menu on the right side. \n\n### How do you select a GPU on Kaggle?\n\nYou can enable GPU usage by selecting *GPU* as *Accelerator* in the settings menu on the right side.\n\n## Citations\n\nIf you'd like to cite this book, you can use the following BibTeX entry:\n\n```\n@book{tunstall2022natural,\n  title={Natural Language Processing with Transformers: Building Language Applications with Hugging Face},\n  author={Tunstall, Lewis and von Werra, Leandro and Wolf, Thomas},\n  isbn={1098103246},\n  url={https://books.google.ch/books?id=7hhyzgEACAAJ},\n  year={2022},\n  publisher={O'Reilly Media, Incorporated}\n}\n```\n"
    },
    {
      "name": "CTXY/table_retrieval",
      "stars": 3,
      "img": "https://avatars.githubusercontent.com/u/33929617?s=40&v=4",
      "owner": "CTXY",
      "repo_name": "table_retrieval",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2024-10-15T02:26:47Z",
      "updated_at": "2025-01-12T17:06:01Z",
      "topics": [],
      "readme": "# Project Overview\nThis repository contains the implementation of the paper \"**Dense Table Retrieval for Question Answering: A Design Space Exploration**\".\n\nThe project explores dense retrieval techniques for natural language (NL) questions over tables. It systematically evaluates different model architectures, training strategies, table structure encoding, and linearization methods to optimize table retrieval.\n\n## Key features:\n - **Design Space Exploration**:  We  explore different model architectures, encoding methods, training strategies, and table linearization techniques. Our code allows you to experiment with different combinations within this design space.\n - **Benchmark Datasets**: we collect and curate six datasets for table retrieval. You can train and test models across various datasets, with flexibility to choose the dataset that suits your needs.\n- **Training Data Generation**:  We provides an algorithm to automatically generate synthetic training data. You can use this algorithm to generate training data on the provided datasets or apply it to your own data.\n\nThis project is built on the [Haystack](https://github.com/deepset-ai/haystack), an open source framework for building production-ready LLM applications, retrieval-augmented generative pipelines and state-of-the-art search systems that work intelligently over large document collections. \n\n# Environment Setup\n\nTo set up the environment, you can install the required dependencies by running the following command:\n\n```bash\npip install -r requirements.txt\n```\n\n# Datasets and Pre-Trained Models\n\n[Download Data and Pre-Trained Models](https://osf.io/k8d3q/?view_only=cc4610d536404db1912fb2a7fd79aed7)\n\n## Dataset \nYou can download the datasets in our benchmark, which include the following six datasets:\n\n| **Dataset**        | **# of Questions (train/dev/test)** | **# of Tables** |\n|--------------------|-------------------------------------|-----------------|\n| **FeTaQA**         | 7,326 / 1,001 / 2,003               | 10,330          |\n| **NQ-TABLES**      | 8,442 / 943 / 958                   | 127,629         |\n| **WikiSQL**        | 14,888 / 1,654 / 3,722              | 26,531          |\n| **WTQ**            | 4,223 / 628 / 1,020                 | 2,108           |\n| **OTT-QA**         | 6,000 / 600 / 876                   | 419,183         |\n| **MMQA**           | 3,932 / 491 / 492                   | 10,041          |\n\nAlternatively, you can create your own dataset. You will need to create a `train.json` file and `dev.json` with the following format:\n\n```json\n{\n    \"dataset\": \"str\",\n    \"question\": \"str\",\n    \"answers\": [\"list of str\"],\n    \"positive_ctxs\": [\n        {\n            \"title\": \"str\",\n            \"text\": \"str\",\n            \"score\": \"int\",\n            \"title_score\": \"int\",\n            \"passage_id\": \"str\"\n        }\n    ],\n    \"negative_ctxs\": [\n        {\n            \"title\": \"str\",\n            \"text\": \"str\",\n            \"score\": \"int\",\n            \"title_score\": \"int\",\n            \"passage_id\": \"str\"\n        }\n    ],\n    \"hard_negative_ctxs\": [\n        {\n            \"title\": \"str\",\n            \"text\": \"str\",\n            \"score\": \"int\",\n            \"title_score\": \"int\",\n            \"passage_id\": \"str\"\n        }\n    ]\n}\n```\n - positive_ctxs: Relevant tables to the query. In some datasets, tables might have more than one positive context, in which case you can set the num_positives parameter higher than the default 1.\n\n**Table Data Format** \nTo store table data, you need to create a `tables.jsonl` file where each table is represented with the following fields:\n```json\n{\n    \"id\": \"str\",  \n    \"title\": \"str\",  \n    \"columns\": [  \n        {\"text\": \"str\"}\n    ],\n    \"cells\": [  \n        {\n            \"text\": \"str\",  \n            \"row_idx\": \"int\", \n            \"col_idx\": \"int\" \n        },\n    ],\n    \"rows\": { \n        \"row_index\": {\n            \"cells\": [ \n                {\"text\": \"str\"},  \n            ]\n        },\n    }\n}\n\n```\n**Test Data Format**\nYour test data should be stored in a `test.jsonl` file with the following structure:\n```json\n{\n    \"id\": \"str\", \n    \"question\": \"str\", \n    \"table_id_lst\": [\"str\"],\n    \"answers\": [\"list of str\"], \n    \"ctxs\": [\n        {\n            \"title\": \"str\", \n            \"text\": \"str\" \n        },\n    ]\n}\n```\n## Pre-Trained Models\n\nWe sampled 1.5 million text-table pairs from the [DTR dataset](https://github.com/google-research/tapas/blob/master/DENSE_TABLE_RETRIEVER.md), filtering out tables with excessive missing values. Using TAPAS-base as the backbone model, we re-trained both UTP and DTR models.\n\nYou can download the pre-trained UTP and DTR checkpoints trained by us. Alternatively, you can run the original DTR models and code from its [official repository](https://github.com/google-research/tapas/).\n\n- **DTR**: [Open domain question answering over tables via dense retrieval](https://arxiv.org/abs/2103.12011), Herzig J, Müller T, Krichene S, et al., 2021.\n- **UTP**: [Bridge the gap between language models and tabular understanding](https://arxiv.org/abs/2302.09302), Chen N, Shou L, Gong M, et al., 2023.\n\n# Model Training\nThis section outlines how to train the model for dense table retrieval using the provided training scripts. You can explore the design space by selecting different model architectures, encoding strategies, and linearization methods.\n###  Model Architecture\nYou can use the following commands to train the model. Here is an explanation of the main parameters:\n\n- `file_dir`: Specifies the directory where your training data is stored.\n- `query_model` and `passage_model`: These represent the base models for the two encoders. We support both **BERT-base-uncased** and **TAPAS-base**.\n- `save_dir`: The directory where the trained model will be saved.\n- `num_positives`: must be less than or equal to the number of positive_ctxs available for queries in your dataset. \n- `num_hard_negatives`: must be less than or equal to the number of hard_negative_ctxs available for queries in your dataset. \n\n**Siamese Dual Encoder**\n```bash\nCUDA_VISIBLE_DEVICES=0 python train.py \\\n  --file_dir='./datasets/fetaqa' \\\n  --train_filename='train.json' \\\n  --dev_filename='dev.json' \\\n  --passage_model='bert-base-uncased' \\\n  --save_dir='./checkpoints/fetaqa/Siamese_BERT' \\\n  --max_seq_len_passage=512 \\\n  --training_epochs=50 \\\n  --batch_size=12 \\\n  --eval_batch_size=12 \\\n  --num_positives=1 \\\n  --num_hard_negatives=7 \\\n  --checkpoint_root_dir='./temp_checkpoints/ott-qa/pretrained_UTP_ott-qa' \\\n  --model_architecture='Siamese'\n```\n\n**Asymmetric Dual Encoder**\n\n```bash\nCUDA_VISIBLE_DEVICES=0 python train.py \\\n  --file_dir='./datasets/fetaqa' \\\n  --train_filename='train.json' \\\n  --dev_filename='dev.json' \\\n  --query_model='bert-base-uncased' \\\n  --passage_model='bert-base-uncased' \\\n  --save_dir='./checkpoints/fetaqa/DPR' \\\n  --max_seq_len_query=128 \\\n  --max_seq_len_passage=512 \\\n  --training_epochs=50 \\\n  --batch_size=12 \\\n  --eval_batch_size=12 \\\n  --num_positives=1 \\\n  --num_hard_negatives=7 \\\n  --checkpoint_root_dir='./temp_checkpoints/fetaqa/DPR'\n```\n\n###   Pre-Training\nTo fine-tune based on a pre-trained **UTP** or DTR model, you can download the corresponding checkpoints:\n\n- **UTP**: Use the SDE training code and replace `passage_model` with the path to the UTP checkpoint.\n- **DTR**: Use the ADE training code and replace `query_model` and `passage_model` with the respective DTR checkpoint paths.\n### Encoding Strategies\nYou can specify different encoding strategies for the `query_structure` and `table_structure` parameters:\n\n- **bias**:  Attention bias.\n- **rowcol**: Attention mask.\n- **auxemb**: Positional embeddings.\n\nExample:\n\n```bash\n--query_structure='bias' \\\n--table_structure='bias'\n```\n### Linearization\n\nYou can linearize tables either by row or column and choose among different linearization methods:\n\n **Linearization Direction**:\n  - `row`: Linearize the table by row.\n  - `column`: Linearize the table by column.\n\n**Linearization Methods**:\n  - `direct`: Directly linearize the table without extra formatting.\n  - `default`: Use simple delimiters (e.g., commas or spaces).\n  - `separator`: Insert special tokens to separate elements (e.g., [SEP] tokens).\n  - `template`: Use a textual template to format the table into natural language.\n\nExample:\n\n```bash\n--linearization_direction='row' \\\n--linearization='direct'\n```\n# Table Retrieval\n\nOnce you have a trained model, you can use the following code to encode embeddings and perform retrieval:\n\n```bash\nCUDA_VISIBLE_DEVICES=1 python evaluate.py \\\n  --model_name='DPR' \\\n  --dataset_name='nq_tables' \\\n  --save_model_dir='./checkpoints/fetaqa/DPR' \\\n  --data_path='/home/yangchenyu/table_retrieval/datasets/fetaqa'\n```\n\n- `data_path`: Must contain your `test.jsonl` file and `tables.jsonl` file.\n- `save_model_dir`: Path where your trained model is stored.\n\n\n# Training Data Generation\n\nTo generate training data on a new table collection, please refer to the instructions provided in the [question_generator/readme.md](./question_generator/readme.md) and follow the steps outlined there."
    },
    {
      "name": "GitXpresso/CLODSH",
      "stars": 3,
      "img": "https://avatars.githubusercontent.com/u/126926699?s=40&v=4",
      "owner": "GitXpresso",
      "repo_name": "CLODSH",
      "description": "Curated List Of Docker Self Hosted (CLODSH)",
      "homepage": "https://clodsh.vercel.app",
      "language": "Shell",
      "created_at": "2024-11-08T19:02:59Z",
      "updated_at": "2025-01-09T13:30:35Z",
      "topics": [
        "chromium",
        "docker",
        "docker-compose",
        "firefox",
        "self-hosted",
        "tor",
        "ungoogled-chromium"
      ],
      "readme": "[![Maintenance](https://img.shields.io/badge/Maintained%3F-yes-green.svg)](https://github.com/GitXpresso/CLODSH/graphs/commit-activity)\n[![Open In Collab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/GitXpressoo/clodsh)\n[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/Gitxpresso/clodsh/main)\n\n# Dockersh\nThe sh in docker sh stands for self hosted folders will be ready for you to run browsers and others stuff the orginal people who made the docker container will be credited.\n## Todo \n- [X] Finish making Docker installation on kali linux tutorial\n- [X] Finish making docker compose installation on kali linux tutorial\n- [ ] \n\n# Table of Contents\n\n<details><summary>Browsers</summary>\n\n- [Firefox](#Firefox)\n- [Chromium](#Chromium)\n- [Ungoogled Chromium](#Ungoogled-Chromium)\n- [Opera](#Opera) \n- [Tor](#Tor)\n\n</details>\n\n<details><summary>Workspace</summary>\n \n[Gogs](#Gogs)\n\n</details>\n \n<details><summary>Docker Installation</summary>\n \n - [How to Install Docker on Kali Linux](#How-to-Install-Docker-on-Kali-Linux)\n - [How to Install Docker Compose on Kali Linux](#How-to-Install-Docker-Compose-on-Kali-Linux)\n - [Docker Compose Commands](#Docker-Compose-Commands)\n\n</details>\n\n<details><summary>Bash Shell Quick Start</summary>\n\n<details><summary>Gogs</summary>\n\n- [Installing Gogs](#Installing-Gogs)\n</details>\n\n<details><summary>Go</summary>\n \n- [Installing Go](#Installing-Go)\n- [Uninstalling Go](#Uninstalling-Go)\n \n</details>\n\n</details>\n\n<details><summary>things that has nonthing to do with or about this repository</summary>\n \n- [Visitor's Counter](#Visitors)\n- [Repository Version](#CLODSH-Readmemd)\n\n</details>\n\n# Firefox\n[![Firefox logo](https://images.weserv.nl/?url=raw.githubusercontent.com/jlesage/docker-templates/master/jlesage/images/firefox-icon.png&w=110)](https://www.mozilla.org/firefox/)[![Firefox](https://images.placeholders.dev/?width=224&height=110&fontFamily=monospace&fontWeight=400&fontSize=52&text=Firefox&bgColor=rgba(0,0,0,0.0)&textColor=rgba(121,121,121,1))](https://www.mozilla.org/firefox/)\n\nMozilla Firefox is a free and open-source web browser developed by Mozilla\nFoundation and its subsidiary, Mozilla Corporate. \n\nOriginally Created by: [Jlesage](https://github.com/jlesage)\n\nOriginal Repository: [Docker Firefox](https://github.com/jlesage/docker-firefox)\n\nDonate to the Original Creator of Docker Firefox\n[![PayPal](https://img.shields.io/badge/PayPal-003087?logo=paypal&logoColor=fff)](https://paypal.me/JocelynLeSage)\n\n\n# Firefox Setup\n\nFirst git clone the this repository\n```\ngit clone https://github.com/GitXpresso/dockersh.git\n```\nThen do \n```\ncd Firefox\n```\nTo start firefox do\n```\ndocker compose up\n```\nOr \n```\n\n```\n# Chromium\n\n[![chromium](https://raw.githubusercontent.com/linuxserver/docker-templates/master/linuxserver.io/img/chromium-logo.png)](https://www.chromium.org/chromium-projects/)\n\n[Chromium](https://www.chromium.org/chromium-projects/) is an open-source browser project that aims to build a safer, faster, and more stable way for all users to experience the web.\n\nChromium Official Github Mirror: [chromium](https://github.com/chromium/chromium)\n\nOriginally Created by: [LinuxServer](https://github.com/linuxserver)\n\nOriginal Repository: [Docker Chromium](https://github.com/linuxserver/chromium)\n\nCreator website: [linuxserver.io](https://linuxserver.io)\n\nDonations : [Linux Server Open collective](https://opencollective.com/linuxserver/)\n\n# Table of Contents\n- [Application Setup](#Application-Setup)\n- [Security](#Security)\n- [Options in all KasmVNC based GUI containers](Options-in-all-KasmVNC-based-GUI-containers)\n- [Language Support - Internationalization](#Language-Support-Internationalization)\n- [DRI3 GPU Acceleration](#DRI3-GPU-Acceleration)\n- [Nvidia GPU Support](#Nvidia-GPU-Support)\n- [Application management](#Application-management)\n- [Usage](#Usage)\n- [Parameters](#Parameters)\n- [Environment variables from files Docker secrets](#Environment-variables-from-files-Docker-secrets)\n- [Umask for running applications](#Umask-for-running-applications)\n- [User / Group Identifiers](#Use-/-Group-Identifiers)\n- [Docker-Mods](#Docker-Mods)\n- [Support Info](#Support-Info)\n- [Building locally](#Building-locally)\n- [Versions](#Versions)\n\nSupported Architectures\n---------------------------------------------------------------------\n\nWe utilise the docker manifest for multi-platform awareness. More information is available from docker [here](https://distribution.github.io/distribution/spec/manifest-v2-2/#manifest-list) and our announcement [here](https://blog.linuxserver.io/2019/02/21/the-lsio-pipeline-project/).\n\nSimply pulling `lscr.io/linuxserver/chromium:latest` should retrieve the correct image for your arch, but you can also pull specific arch images via tags.\n\nThe architectures supported by this image are:\n\n\n|Architecture|Available|Tag                  |\n|------------|---------|---------------------|\n|x86-64      |✅        |amd64-<version tag>  |\n|arm64       |✅        |arm64v8-<version tag>|\n|armhf       |❌        |                     |\n\n\nApplication Setup\n---------------------------------------------------------\n\nThe application can be accessed at:\n\n*   http://yourhost:3000/\n*   https://yourhost:3001/*   \n\nTo Run the selfhosted chromium do the following commands below\n\n*Modern GUI desktop apps have issues with the latest Docker and syscall compatibility, you can use Docker with the `--security-opt seccomp=unconfined` setting to allow these syscalls on hosts with older Kernels or libseccomp**\n\n### Security\n\nWarning\n\nDo not put this on the Internet if you do not know what you are doing.\n\nBy default this container has no authentication and the optional environment variables `CUSTOM_USER` and `PASSWORD` to enable basic http auth via the embedded NGINX server should only be used to locally secure the container from unwanted access on a local network. If exposing this to the Internet we recommend putting it behind a reverse proxy, such as [SWAG](https://github.com/linuxserver/docker-swag), and ensuring a secure authentication solution is in place. From the web interface a terminal can be launched and it is configured for passwordless sudo, so anyone with access to it can install and run whatever they want along with probing your local network.\n\n### Options in all KasmVNC based GUI containers\n\nThis container is based on [Docker Baseimage KasmVNC](https://github.com/linuxserver/docker-baseimage-kasmvnc) which means there are additional environment variables and run configurations to enable or disable specific functionality.\n\n#### Optional environment variables\n\n\n\n* Variable: CUSTOM_PORT\n  * Description: Internal port the container listens on for http if it needs to be swapped from the default 3000.\n* Variable: CUSTOM_HTTPS_PORT\n  * Description: Internal port the container listens on for https if it needs to be swapped from the default 3001.\n* Variable: CUSTOM_USER\n  * Description: HTTP Basic auth username, abc is default.\n* Variable: PASSWORD\n  * Description: HTTP Basic auth password, abc is default. If unset there will be no auth\n* Variable: SUBFOLDER\n  * Description: Subfolder for the application if running a subfolder reverse proxy, need both slashes IE /subfolder/\n* Variable: TITLE\n  * Description: The page title displayed on the web browser, default \"KasmVNC Client\".\n* Variable: FM_HOME\n  * Description: This is the home directory (landing) for the file manager, default \"/config\".\n* Variable: START_DOCKER\n  * Description: If set to false a container with privilege will not automatically start the DinD Docker setup.\n* Variable: DRINODE\n  * Description: If mounting in /dev/dri for DRI3 GPU Acceleration allows you to specify the device to use IE /dev/dri/renderD128\n* Variable: DISABLE_IPV6\n  * Description: If set to true or any value this will disable IPv6\n* Variable: LC_ALL\n  * Description: Set the Language for the container to run as IE fr_FR.UTF-8 ar_AE.UTF-8\n* Variable: NO_DECOR\n  * Description: If set the application will run without window borders in openbox for use as a PWA.\n* Variable: NO_FULL\n  * Description: Do not autmatically fullscreen applications when using openbox.\n\n\n#### Optional run configurations\n\n\n\n* Variable: --privileged\n  * Description: Will start a Docker in Docker (DinD) setup inside the container to use docker in an isolated environment. For increased performance mount the Docker directory inside the container to the host IE -v /home/user/docker-data:/var/lib/docker.\n* Variable: -v /var/run/docker.sock:/var/run/docker.sock\n  * Description: Mount in the host level Docker socket to either interact with it via CLI or use Docker enabled applications.\n* Variable: --device /dev/dri:/dev/dri\n  * Description: Mount a GPU into the container, this can be used in conjunction with the DRINODE environment variable to leverage a host video card for GPU accelerated applications. Only Open Source drivers are supported IE (Intel,AMDGPU,Radeon,ATI,Nouveau)\n\n\n### Language Support - Internationalization\n\nThe environment variable `LC_ALL` can be used to start this container in a different language than English simply pass for example to launch the Desktop session in French `LC_ALL=fr_FR.UTF-8`. Some languages like Chinese, Japanese, or Korean will be missing fonts needed to render properly known as cjk fonts, but others may exist and not be installed inside the container depending on what underlying distribution you are running. We only ensure fonts for Latin characters are present. Fonts can be installed with a mod on startup.\n\nTo install cjk fonts on startup as an example pass the environment variables (Alpine base):\n\n```\n-e DOCKER_MODS=linuxserver/mods:universal-package-install \n-e INSTALL_PACKAGES=fonts-noto-cjk-e LC_ALL=zh_CN.UTF-8\n\n```\n\n\nThe web interface has the option for \"IME Input Mode\" in Settings which will allow non english characters to be used from a non en\\_US keyboard on the client. Once enabled it will perform the same as a local Linux installation set to your locale.\n\n### DRI3 GPU Acceleration\n\nFor accelerated apps or games, render devices can be mounted into the container and leveraged by applications using:\n\n`--device /dev/dri:/dev/dri`\n\nThis feature only supports **Open Source** GPU drivers:\n\n\n|Driver|Description                                                          |\n|------|---------------------------------------------------------------------|\n|Intel |i965 and i915 drivers for Intel iGPU chipsets                        |\n|AMD   |AMDGPU, Radeon, and ATI drivers for AMD dedicated or APU chipsets    |\n|NVIDIA|nouveau2 drivers only, closed source NVIDIA drivers lack DRI3 support|\n\n\nThe `DRINODE` environment variable can be used to point to a specific GPU. Up to date information can be found [here](https://www.kasmweb.com/kasmvnc/docs/master/gpu_acceleration.html)\n\n### Nvidia GPU Support\n\n**Nvidia support is not compatible with Alpine based images as Alpine lacks Nvidia drivers**\n\nNvidia support is available by leveraging Zink for OpenGL support. This can be enabled with the following run flags:\n\n\n\n* Variable: --gpus all\n  * Description: This can be filtered down but for most setups this will pass the one Nvidia GPU on the system\n* Variable: --runtime nvidia\n  * Description: Specify the Nvidia runtime which mounts drivers and tools in from the host\n\n\nThe compose syntax is slightly different for this as you will need to set nvidia as the default runtime:\n\n```\nsudo nvidia-ctk runtime configure --runtime=docker --set-as-default\nsudo service docker restart\n\n```\n\n\nAnd to assign the GPU in compose:\n\n```\nservices:\n  chromium:\n    image: lscr.io/linuxserver/chromium:latest\n    deploy:\n      resources:\n        reservations:\n          devices:\n            - driver: nvidia\n              count: 1\n              capabilities: [compute,video,graphics,utility]\n\n```\n\n\n### Application management\n<!-- If you see this, this is just a test. -->\n#### PRoot Apps\n\nIf you run system native installations of software IE `sudo apt-get install filezilla` and then upgrade or destroy/re-create the container that software will be removed and the container will be at a clean state. For some users that will be acceptable and they can update their system packages as well using system native commands like `apt-get upgrade`. If you want Docker to handle upgrading the container and retain your applications and settings we have created [proot-apps](https://github.com/linuxserver/proot-apps) which allow portable applications to be installed to persistent storage in the user's `$HOME` directory and they will work in a confined Docker environment out of the box. These applications and their settings will persist upgrades of the base container and can be mounted into different flavors of KasmVNC based containers on the fly. This can be achieved from the command line with:\n\n```\nproot-apps install filezilla\n\n```\n\n\nPRoot Apps is included in all KasmVNC based containers, a list of linuxserver.io supported applications is located [HERE](https://github.com/linuxserver/proot-apps?tab=readme-ov-file#supported-apps).\n\n#### Native Apps\n\nIt is possible to install extra packages during container start using [universal-package-install](https://github.com/linuxserver/docker-mods/tree/universal-package-install). It might increase starting time significantly. PRoot is preferred.\n\n```\n  environment:\n    - DOCKER_MODS=linuxserver/mods:universal-package-install\n    - INSTALL_PACKAGES=libfuse2|git|gdb\n\n```\n\n\n### Lossless mode\n\nThis container is capable of delivering a true lossless image at a high framerate to your web browser by changing the Stream Quality preset to \"Lossless\", more information [here](https://www.kasmweb.com/docs/latest/how_to/lossless.html#technical-background). In order to use this mode from a non localhost endpoint the HTTPS port on 3001 needs to be used. If using a reverse proxy to port 3000 specific headers will need to be set as outlined [here](https://github.com/linuxserver/docker-baseimage-kasmvnc#lossless).\n\nUsage\n---------------------------------\n\nTo help you get started creating a container from this image you can either use docker-compose or the docker cli.\n\n### docker-compose (recommended, [click here for more info](https://docs.linuxserver.io/general/docker-compose))\n\n```\n---\nservices:\n  chromium:\n    image: lscr.io/linuxserver/chromium:latest\n    container_name: chromium\n    security_opt:\n      - seccomp:unconfined #optional\n    environment:\n      - PUID=1000\n      - PGID=1000\n      - TZ=Etc/UTC\n      - CHROME_CLI=https://www.linuxserver.io/ #optional\n    volumes:\n      - /path/to/config:/config\n    ports:\n      - 3000:3000\n      - 3001:3001\n    shm_size: \"1gb\"\n    restart: unless-stopped\n\n```\n\n\n### docker cli ([click here for more info](https://docs.docker.com/engine/reference/commandline/cli/))\n\n```\ndocker run -d \\\n  --name=chromium \\\n  --security-opt seccomp=unconfined `#optional` \\\n  -e PUID=1000 \\\n  -e PGID=1000 \\\n  -e TZ=Etc/UTC \\\n  -e CHROME_CLI=https://www.linuxserver.io/ `#optional` \\\n  -p 3000:3000 \\\n  -p 3001:3001 \\\n  -v /path/to/config:/config \\\n  --shm-size=\"1gb\" \\\n  --restart unless-stopped \\\n  lscr.io/linuxserver/chromium:latest\n\n```\n\n\nParameters\n-------------------------------------------\n\nContainers are configured using parameters passed at runtime (such as those above). These parameters are separated by a colon and indicate `<external>:<internal>` respectively. For example, `-p 8080:80` would expose port `80` from inside the container to be accessible from the host's IP on port `8080` outside the container.\n\n### Ports (`-p`)\n\n\n|Parameter|Function                   |\n|---------|---------------------------|\n|3000     |Chromium desktop gui.      |\n|3001     |HTTPS Chromium desktop gui.|\n\n\n### Environment Variables (`-e`)\n\n\n\n* Env: PUID=1000\n  * Function: for UserID - see below for explanation\n* Env: PGID=1000\n  * Function: for GroupID - see below for explanation\n* Env: TZ=Etc/UTC\n  * Function: specify a timezone to use, see this list.\n* Env: CHROME_CLI=https://www.linuxserver.io/\n  * Function: Specify one or multiple Chromium CLI flags, this string will be passed to the application in full.\n\n\n### Volume Mappings (`-v`)\n\n\n|Volume |Function                                                              |\n|-------|----------------------------------------------------------------------|\n|/config|Users home directory in the container, stores local files and settings|\n\n\n#### Miscellaneous Options\n\n\n\n* Parameter: --shm-size=\n  * Function: This is needed for any modern website to function like youtube.\n* Parameter: --security-opt seccomp=unconfined\n  * Function: For Docker Engine only, many modern gui apps need this to function on older hosts as syscalls are unknown to Docker. Chromium runs in no-sandbox test mode without it.\n\n\nEnvironment variables from files Docker secrets\n-----------------------------------------------------------------------------------------------------------------------\n\nYou can set any environment variable from a file by using a special prepend `FILE__`.\n\nAs an example:\n\n```\n-e FILE__MYVAR=/run/secrets/mysecretvariable\n\n```\n\n\nWill set the environment variable `MYVAR` based on the contents of the `/run/secrets/mysecretvariable` file.\n\nUmask for running applications\n-----------------------------------------------------------------------------------\n\nFor all of our images we provide the ability to override the default umask settings for services started within the containers using the optional `-e UMASK=022` setting. Keep in mind umask is not chmod it subtracts from permissions based on it's value it does not add. Please read up [here](https://en.wikipedia.org/wiki/Umask) before asking for support.\n\nUser / Group Identifiers\n---------------------------------------------------------------------\n\nWhen using volumes (`-v` flags), permissions issues can arise between the host OS and the container, we avoid this issue by allowing you to specify the user `PUID` and group `PGID`.\n\nEnsure any volume directories on the host are owned by the same user you specify and any permissions issues will vanish like magic.\n\nIn this instance `PUID=1000` and `PGID=1000`, to find yours use `id your_user` as below:\n\nExample output:\n\n```\nuid=1000(your_user) gid=1000(your_user) groups=1000(your_user)\n\n```\n\n\nDocker Mods\n---------------------------------------------\n\n[![Docker Mods](https://img.shields.io/badge/dynamic/yaml?color=94398d&labelColor=555555&logoColor=ffffff&style=for-the-badge&label=chromium&query=%24.mods%5B%27chromium%27%5D.mod_count&url=https%3A%2F%2Fraw.githubusercontent.com%2Flinuxserver%2Fdocker-mods%2Fmaster%2Fmod-list.yml)](https://mods.linuxserver.io/?mod=chromium \"view available mods for this container.\") [![Docker Universal Mods](https://img.shields.io/badge/dynamic/yaml?color=94398d&labelColor=555555&logoColor=ffffff&style=for-the-badge&label=universal&query=%24.mods%5B%27universal%27%5D.mod_count&url=https%3A%2F%2Fraw.githubusercontent.com%2Flinuxserver%2Fdocker-mods%2Fmaster%2Fmod-list.yml)](https://mods.linuxserver.io/?mod=universal \"view available universal mods.\")\n\nWe publish various [Docker Mods](https://github.com/linuxserver/docker-mods) to enable additional functionality within the containers. The list of Mods available for this image (if any) as well as universal mods that can be applied to any one of our images can be accessed via the dynamic badges above.\n\nSupport Info\n-----------------------------------------------\n\n*   Shell access whilst the container is running:\n    \n    ```\ndocker exec -it chromium /bin/bash\n\n```\n\n    \n*   To monitor the logs of the container in realtime:\n    \n*   Container version number:\n    \n    ```\ndocker inspect -f '{{ index .Config.Labels \"build_version\" }}' chromium\n\n```\n\n    \n*   Image version number:\n    \n    ```\ndocker inspect -f '{{ index .Config.Labels \"build_version\" }}' lscr.io/linuxserver/chromium:latest\n\n```\n\n    \n\nUpdating Info\n-------------------------------------------------\n\nMost of our images are static, versioned, and require an image update and container recreation to update the app inside. With some exceptions (noted in the relevant readme.md), we do not recommend or support updating apps inside the container. Please consult the [Application Setup](#application-setup) section above to see if it is recommended for the image.\n\nBelow are the instructions for updating containers:\n\n### Via Docker Compose\n\n*   Update images:\n    \n    *   All images:\n        \n    *   Single image:\n        \n        ```\ndocker-compose pull chromium\n\n```\n\n        \n*   Update containers:\n    \n    *   All containers:\n        \n    *   Single container:\n        \n        ```\ndocker-compose up -d chromium\n\n```\n\n        \n*   You can also remove the old dangling images:\n    \n\n### Via Docker Run\n\n*   Update the image:\n    \n    ```\ndocker pull lscr.io/linuxserver/chromium:latest\n\n```\n\n    \n*   Stop the running container:\n    \n*   Delete the container:\n    \n*   Recreate a new container with the same docker run parameters as instructed above (if mapped correctly to a host folder, your `/config` folder and settings will be preserved)\n    \n*   You can also remove the old dangling images:\n    \n\n### Image Update Notifications - Diun (Docker Image Update Notifier)\n\nTip\n\nWe recommend [Diun](https://crazymax.dev/diun/) for update notifications. Other tools that automatically update containers unattended are not recommended or supported.\n\nBuilding locally\n-------------------------------------------------------\n\nIf you want to make local modifications to these images for development purposes or just to customize the logic:\n\n```\ngit clone https://github.com/linuxserver/docker-chromium.git\ncd docker-chromium\ndocker build \\\n  --no-cache \\\n  --pull \\\n  -t lscr.io/linuxserver/chromium:latest .\n\n```\n\n\nThe ARM variants can be built on x86\\_64 hardware and vice versa using `lscr.io/linuxserver/qemu-static`\n\n```\ndocker run --rm --privileged lscr.io/linuxserver/qemu-static --reset\n\n```\n\n\nOnce registered you can define the dockerfile to use with `-f Dockerfile.aarch64`.\n## Versions\n---------------------------------------\n\n*   **10.02.24:** - Update Readme with new env vars and ingest proper PWA icon.\n*   **08.01.24:** - Fix re-launch issue for chromium by purging temp files on launch.\n*   **29.12.23:** - Rebase to Debian Bookworm.\n*   **13.05.23:** - Rebase to Alpine 3.18.\n*   **01.04.23:** - Preserve arguments passed to Chromium and restructure to use wrapper.\n*   **18.03.23:** - Initial release.\n\n\n \n\n# Tor\n<p align=\"center\">\n  <img width=\"300px\" src=\"https://upload.wikimedia.org/wikipedia/commons/8/8f/Tor_project_logo_hq.png\">\n</p\n\nOriginally Created by: [DomiStyles](https://github.com/DomiStyle)\n\nForked and Edited by:\n[GitXpresso](https://github.com/GitXpresso)\n\nOriginal Repository: [Docker Tor Browser](https://github.com/DomiStyle/docker-tor-browser)\n\n# Table of Contents\n   \n<details><summary>Docker</summary>\n\n- [Docker Compose Script](#Docker-Compose-Script)\n- [Docker Container Commands](#Docker-Container-Commands)\n- [Docker Installation](#How-to-install-Docker)\n\n</details>\n   \n<details><summary>Configuration</summary>\n\n- [Platform configuration](#Platform-configuration)\n- [Browser configuration](#Browser-configuration)\n \n</details>\n\n<details><summary>Others</summary>\n   \n- [Volumes](#Volumes)\n- [Port](#Port)\n- [Issues And limitations](#Issues-And-limitations)\n\n</details>\n\n![](https://github.com/DomiStyle/docker-tor-browser/raw/master/screenshot.png)\n*Tor browser running inside of Firefox*\n\n## About\n\nThis image allows running a [Tor browser](https://www.torproject.org/) instance on any headless server. The browser can be accessed via either a web interface or directly from any VNC client.\n\nContainer is based on [baseimage-gui](https://github.com/jlesage/docker-baseimage-gui) by [jlesage](https://github.com/jlesage)\n\nBoth amd64 and arm64 container runtimes are supported, but only the amd64 image uses an official build of the Tor Browser. The arm64 image uses an [unofficial build via tor-browser-ports](https://sourceforge.net/projects/tor-browser-ports/) because the Tor Project does not have an official arm build of the Tor Browser. Both the official and unofficial builds are signed, and the signatures are verified when building this container.\n\n# Usage\n\nSee the docker-compose [here](https://github.com/DomiStyle/docker-tor-browser/blob/master/docker-compose.yml) or use this command\n\n    docker run -d -p 5800:5800 domistyle/tor-browser\n\nThe web interface will be available on port 5800.\nto use port on a browser go to this url\n```\n127.0.0.1:8080\n```\n## Platform configuration\n\nNo special configuration is necessary, however some recommended variables are available:\n\n| Variable  | Description | Default  | Required |\n|-----------|-------------|----------|----------|\n| `DISPLAY_WIDTH` | Set the width of the virtual screen | ``1280`` | No |\n| `DISPLAY_HEIGHT` | Set the height of the virtual screen | ``768`` | No |\n| `KEEP_APP_RUNNING` | Automatically restarts the Tor browser if it exits | ``0`` | No |\n| `TZ` | Set the time zone for the container | - | No |\n\n** For advanced configuration options please take a look [here](https://github.com/jlesage/docker-baseimage-gui#environment-variables).**\n\n## Browser configuration\n\nYou may install the browser with your own configuration. Copy the template configuration to get started.\nIf `mozilla.cfg` is available then it is used, otherwise no browser changes are made.\n```\ncd browser-cfg\ncp mozilla.cfg.template mozilla.cfg\n```\n** For more information on the available options: http://kb.mozillazine.org/About:config_entries\n\n## Volumes\n\nIt is not recommended to add persistent volumes to your Tor Browser. We do not support any issues that arise from persistent volumes.\n# Docker Compose Script\n```yaml\nversion: '3.9'\n\nservices:\n  tor:\n    image: domistyle/tor-browser\n    container_name: tor\n    restart: unless-stopped\n    ports:\n      - 5800:5800\n      - 5900:5900\n    environment:\n      DISPLAY_WIDTH: 1920\n      DISPLAY_HEIGHT: 1080\n      KEEP_APP_RUNNING: 1\n      TZ: Europe/Vienna\n```\n\n# Docker Container Commands\n To install tor browser using docker compose, copy and paste the command in your terminal\n```bash\ndocker compose up\n```\nTo stop the docker container do\n```bash\ndocker stop tor\n```\nTo start the container again, put the following command below and paste it in your terminal \n```bash\ndocker start tor\n```\nTo remove the container you can do \n```bash\ndocker compose down\n```\nor you can do this\n``` \ndocker rm tor\n```\n# How to install Docker\nFirst update all your packages by doing\n```bash\nsudo apt update\n```\nAfter you update all your packages then it is time to install docker.io by doing \n```bash\nsudo apt install -y docker.io\n```\nTo add systemctl to docker do this\n```bash\nsudo systemctl enable docker --now\n```\nTo verify that you docker put the following command below\n```bash\ndocker\n```\nTo use docker without doing sudo every time do this\n```bash\nsudo usermod -aG docker $USER\n```\n\n\n## Port\n\n| Port       | Description                                  |\n|------------|----------------------------------------------|\n| `5800` | Provides a web interface to access the Tor browser |\n| `5900` | Provides direct access to the NoVNC server |\n\n## Issues And limitations\n\n* shm_size might need to be set to 2GB or more if you experience crashes\n### Github user who updated this markdown:\n[GitXpresso](https://github.com/GitXpresso)\n<details><summary>Docker</summary>\n- [Docker Compose Script](#Docker-Compose-Script)\n- [Docker Container Commands](#Docker-Container-Commands)\n- [Docker Installation](#How-to-install-Docker)\n\n</details>\n   \n<details><summary>Configuration</summary>\n\n- [Platform configuration](#Platform-configuration)\n- [Browser configuration](#Browser-configuration)\n \n</details>\n\n<details><summary>Others</summary>\n   \n- [Volumes](#Volumes)\n- [Port](#Port)\n- [Issues And limitations](#Issues-And-limitations)\n\n</details>\n\n![](https://github.com/DomiStyle/docker-tor-browser/raw/master/screenshot.png)\n*Tor browser running inside of Firefox*\n\n## About\n\nThis image allows running a [Tor browser](https://www.torproject.org/) instance on any headless server. The browser can be accessed via either a web interface or directly from any VNC client.\n\nContainer is based on [baseimage-gui](https://github.com/jlesage/docker-baseimage-gui) by [jlesage](https://github.com/jlesage)\n\nBoth amd64 and arm64 container runtimes are supported, but only the amd64 image uses an official build of the Tor Browser. The arm64 image uses an [unofficial build via tor-browser-ports](https://sourceforge.net/projects/tor-browser-ports/) because the Tor Project does not have an official arm build of the Tor Browser. Both the official and unofficial builds are signed, and the signatures are verified when building this container.\n\n# Usage\n\nSee the docker-compose [here](https://github.com/DomiStyle/docker-tor-browser/blob/master/docker-compose.yml) to use this command as a faster way of installing Tor.\n\n    docker run -d -p 5800:5800 --name tor domistyle/tor-browser --restart=unless-stopped\n\nThe web interface will be available on port 5800.\n\n## Platform configuration\n\nNo special configuration is necessary, however some recommended variables are available:\n\n| Variable  | Description | Default  | Required |\n|-----------|-------------|----------|----------|\n| `DISPLAY_WIDTH` | Set the width of the virtual screen | ``1280`` | No |\n| `DISPLAY_HEIGHT` | Set the height of the virtual screen | ``768`` | No |\n| `KEEP_APP_RUNNING` | Automatically restarts the Tor browser if it exits | ``0`` | No |\n| `TZ` | Set the time zone for the container | - | No |\n\n** For advanced configuration options please take a look [here](https://github.com/jlesage/docker-baseimage-gui#environment-variables).**\n\n## Browser configuration\n\nYou may install the browser with your own configuration. Copy the template configuration to get started.\nIf `mozilla.cfg` is available then it is used, otherwise no browser changes are made.\n```\ncd browser-cfg\ncp mozilla.cfg.template mozilla.cfg\n```\n** For more information on the available options: http://kb.mozillazine.org/About:config_entries\n\n## Volumes\n\nIt is not recommended to add persistent volumes to your Tor Browser. We do not support any issues that arise from persistent volumes.\n# Docker Compose Script\n```yaml\nversion: '3.9'\n\nservices:\n  tor:\n    image: domistyle/tor-browser\n    container_name: tor\n    restart: unless-stopped\n    ports:\n      - 5800:5800\n      - 5900:5900\n    environment:\n      DISPLAY_WIDTH: 1920\n      DISPLAY_HEIGHT: 1080\n      KEEP_APP_RUNNING: 1\n      TZ: Europe/Vienna\n```\n\n# Docker Container Commands\n To install tor browser using docker compose, copy and paste the command in your terminal\n```bash\ndocker compose up\n```\nTo stop the docker container do\n```bash\ndocker stop tor\n```\nTo start the container again, put the following command below and paste it in your terminal \n```bash\ndocker start tor\n```\nTo remove the container you can do \n```bash\ndocker compose down\n```\nor you can do this\n``` \ndocker rm tor\n```\n# How to install Docker\nFirst update all your packages by doing\n```bash\nsudo apt update\n```\nAfter you update all your packages then it is time to install docker.io by doing \n```bash\nsudo apt install -y docker.io\n```\nTo add systemctl to docker do this\n```bash\nsudo systemctl enable docker --now\n```\nTo verify that you docker put the following command below\n```bash\ndocker\n```\nTo use docker without doing sudo every time do this\n```bash\nsudo usermod -aG docker $USER\n```\n\n\n## Port\n\n| Port       | Description                                  |\n|------------|----------------------------------------------|\n| `5800` | Provides a web interface to access the Tor browser |\n| `5900` | Provides direct access to the NoVNC server |\n| IP | Description                                     |    \n| `127.0.0.1` | Provides a way to use tor on your browser |\n\n## Issues And limitations\n\n* shm_size might need to be set to 2GB or more if you experience crashes\n### Github user who updated this markdown:\n[GitXpresso](https://github.com/GitXpresso)\n\n\n# Gogs \n\n![gogs-brand](https://user-images.githubusercontent.com/2946214/146899259-6a8b58ad-8d6e-40d2-ab02-79dc6aadabbf.png)\n\n[![GitHub Workflow Status](https://img.shields.io/github/checks-status/gogs/gogs/main?logo=github&style=for-the-badge)](https://github.com/gogs/gogs/actions?query=branch%3Amain) [![Discord](https://img.shields.io/discord/382595433060499458.svg?style=for-the-badge&logo=discord)](https://discord.gg/9aqdHU7) [![Sourcegraph](https://img.shields.io/badge/view%20on-Sourcegraph-brightgreen.svg?style=for-the-badge&logo=sourcegraph)](https://sourcegraph.com/github.com/gogs/gogs)\n\nRepository: [Gogs](https://github.com/gogs/gogs)\n\nUser: [Gogs](https://github.com/gogs/)\n\nDonate:\n\n# Table of Contents\n- [💌 Features](#💌-Features) \n- [💾 Hardware requirements](#💾-Hardware-requirements)\n- [💻 Browser support](#-💻-Browser-support)\n\n    <details><summary>Docker</summary>\n  \n  - [Docker Installation](#Docker-Installation)\n  \n     * [Docker for Gogs](#Docker-for-Gogs)\n       * [Usage](#Usage)\n         * [Settings](#Settings)\n           * [Backup system](#Backup-system)\n             * [Upgrade](#Upgrade)\n               * [Quick Start](#Quick-Start)\n \n## 🔮 Vision\n\nThe Gogs (`/gɑgz/`) project aims to build a simple, stable and extensible self-hosted Git service that can be set up in the most painless way. With Go, this can be done with an independent binary distribution across all platforms that Go supports, including Linux, macOS, Windows and ARM-based systems.\n\n## 📡 Overview\n\n- Please visit [our home page](https://gogs.io) for user documentation.\n- Please refer to [CHANGELOG.md](CHANGELOG.md) for list of changes in each releases.\n- Want to try it before doing anything else? Do it [online](https://try.gogs.io/gogs/gogs)!\n- Having trouble? Help yourself with [troubleshooting](https://gogs.io/docs/intro/troubleshooting.html) or ask questions in [Discussions](https://github.com/gogs/gogs/discussions).\n- Want to help with localization? Check out the [localization documentation](https://gogs.io/docs/features/i18n.html).\n- Ready to get hands dirty? Read our [contributing guide](.github/CONTRIBUTING.md).\n- Hmm... What about APIs? We have experimental support with [documentation](https://github.com/gogs/docs-api).\n\n## 💌 Features\n\n- User dashboard, user profile and activity timeline.\n- Access repositories via SSH, HTTP and HTTPS protocols.\n- User, organization and repository management.\n- Repository and organization webhooks, including Slack, Discord and Dingtalk.\n- Repository Git hooks, deploy keys and Git LFS.\n- Repository issues, pull requests, wiki, protected branches and collaboration.\n- Migrate and mirror repositories with wiki from other code hosts.\n- Web editor for quick editing repository files and wiki.\n- Jupyter Notebook and PDF rendering.\n- Authentication via SMTP, LDAP, reverse proxy, GitHub.com and GitHub Enterprise with 2FA.\n- Customize HTML templates, static files and many others.\n- Rich database backend, including PostgreSQL, MySQL, SQLite3 and [TiDB](https://github.com/pingcap/tidb).\n- Have localization over [31 languages](https://crowdin.com/project/gogs).\n\n## 💾 Hardware requirements\n\n- A Raspberry Pi or $5 Digital Ocean Droplet is more than enough to get you started. Some even use 64MB RAM Docker [CaaS](https://www.docker.com/blog/containers-as-a-service-caas/).\n- 2 CPU cores and 512MB RAM would be the baseline for teamwork.\n- Increase CPU cores when your team size gets significantly larger, memory footprint remains low.\n\n## 💻 Browser support\n\n- Please see [Semantic UI](https://github.com/Semantic-Org/Semantic-UI#browser-support) for specific versions of supported browsers.\n- The smallest resolution officially supported is **1024*768**, however the UI may still look right in smaller resolutions, but no promises or fixes.\n\n# Docker Installation\n![Docker](https://user-images.githubusercontent.com/25181517/117207330-263ba280-adf4-11eb-9b97-0ac5b40bc3be.png\n# Docker for Gogs\n\n![Docker pulls](https://img.shields.io/docker/pulls/gogs/gogs?logo=docker&style=for-the-badge)\n\nVisit [Docker Hub](https://hub.docker.com/u/gogs) or [GitHub Container registry](https://github.com/gogs/gogs/pkgs/container/gogs) to see all available images and tags.\n\n## Usage\n\nTo keep your data out of Docker container, we do a volume (`/var/gogs` -> `/data`) here, and you can change it based on your situation.\n\n```sh\n# Pull image from Docker Hub.\ndocker pull gogs/gogs\n\n# Create local directory for volume.\nmkdir -p /var/gogs\n\n# Use `docker run` for the first time.\ndocker run --name=gogs -p 10022:22 -p 10880:3000 -v /var/gogs:/data gogs/gogs\n\n# Use `docker start` if you have stopped it.\ndocker start gogs\n```\n\nNote: It is important to map the SSH service from the container to the host and set the appropriate SSH Port and URI settings when setting up Gogs for the first time. To access and clone Git repositories with the above configuration you would use: `git clone ssh://git@hostname:10022/username/myrepo.git` for example.\n\nFiles will be store in local path `/var/gogs` in my case.\n\nDirectory `/var/gogs` keeps Git repositories and Gogs data:\n\n    /var/gogs\n    |-- git\n    |   |-- gogs-repositories\n    |-- ssh\n    |   |-- # ssh public/private keys for Gogs\n    |-- gogs\n        |-- conf\n        |-- data\n        |-- log\n\n#### Custom directory\n\nThe \"custom\" directory may not be obvious in Docker environment. The `/var/gogs/gogs` (in the host) and `/data/gogs` (in the container) is already the \"custom\" directory and you do not need to create another layer but directly edit corresponding files under this directory.\n\n#### Using Docker volumes\n\n```sh\n# Create docker volume.\n$ docker volume create --name gogs-data\n\n# Use `docker run` for the first time.\n$ docker run --name=gogs -p 10022:22 -p 10880:3000 -v gogs-data:/data gogs/gogs\n```\n\n## Settings\n\n### Application\n\nMost of the settings are obvious and easy to understand, but there are some settings can be confusing by running Gogs inside Docker:\n\n- **Repository Root Path**: keep it as default value `/home/git/gogs-repositories` because `start.sh` already made a symbolic link for you.\n- **Run User**: keep it as default value `git` because `build/finalize.sh` already setup a user with name `git`.\n- **Domain**: fill in with Docker container IP (e.g. `192.168.99.100`). But if you want to access your Gogs instance from a different physical machine, please fill in with the hostname or IP address of the Docker host machine.\n- **SSH Port**: Use the exposed port from Docker container. For example, your SSH server listens on `22` inside Docker, **but** you expose it by `10022:22`, then use `10022` for this value. **Builtin SSH server is not recommended inside Docker Container**\n- **HTTP Port**: Use port you want Gogs to listen on inside Docker container. For example, your Gogs listens on `3000` inside Docker, **and** you expose it by `10880:3000`, but you still use `3000` for this value.\n- **Application URL**: Use combination of **Domain** and **exposed HTTP Port** values (e.g. `http://192.168.99.100:10880/`).\n\nFull documentation of application settings can be found [here](https://github.com/gogs/gogs/blob/main/conf/app.ini).\n\n### Container options\n\nThis container has some options available via environment variables, these options are opt-in features that can help the administration of this container:\n\n- **SOCAT_LINK**:\n  - <u>Possible value:</u>\n      `true`, `false`, `1`, `0`\n  - <u>Default:</u>\n      `true`\n  - <u>Action:</u>\n      Bind linked docker container to localhost socket using socat.\n      Any exported port from a linked container will be binded to the matching port on localhost.\n  - <u>Disclaimer:</u>\n      As this option rely on the environment variable created by docker when a container is linked, this option should be deactivated in managed environment such as Rancher or Kubernetes (set to `0` or `false`)\n- **RUN_CROND**:\n  - <u>Possible value:</u>\n      `true`, `false`, `1`, `0`\n  - <u>Default:</u>\n      `false`\n  - <u>Action:</u>\n      Request crond to be run inside the container. Its default configuration will periodically run all scripts from `/etc/periodic/${period}` but custom crontabs can be added to `/var/spool/cron/crontabs/`.\n- **BACKUP_INTERVAL**:\n  - <u>Possible value:</u>\n      `3h`, `7d`, `3M`\n  - <u>Default:</u>\n      `null`\n  - <u>Action:</u>\n      In combination with `RUN_CROND` set to `true`, enables backup system.\\\n      See: [Backup System](#backup-system)\n- **BACKUP_RETENTION**:\n  - <u>Possible value:</u>\n      `360m`, `7d`, `...m/d`\n  - <u>Default:</u>\n      `7d`\n  - <u>Action:</u>\n      Used by backup system. Backups older than specified in expression are deleted periodically.\\\n      See: [Backup System](#backup-system)\n- **BACKUP_ARG_CONFIG**:\n  - <u>Possible value:</u>\n      `/app/gogs/example/custom/config`\n  - <u>Default:</u>\n      `null`\n  - <u>Action:</u>\n      Used by backup system. If defined, supplies `--config` argument to `gogs backup`.\\\n      See: [Backup System](#backup-system)\n- **BACKUP_ARG_EXCLUDE_REPOS**:\n  - <u>Possible value:</u>\n      `test-repo1`, `test-repo2`\n  - <u>Default:</u>\n      `null`\n  - <u>Action:</u>\n      Used by backup system. If defined, supplies `--exclude-repos` argument to `gogs backup`.\\\n      See: [Backup System](#backup-system)\n- **BACKUP_EXTRA_ARGS**:\n  - <u>Possible value:</u>\n      `--verbose --exclude-mirror-repos`\n  - <u>Default:</u>\n      `null`\n  - <u>Action:</u>\n      Used by backup system. If defined, append content to arguments to `gogs backup`.\\\n      See: [Backup System](#backup-system)\n\n## Backup system\n\nAutomated backups with retention policy:\n\n- `BACKUP_INTERVAL` controls how often the backup job runs and supports interval in hours (h), days (d), and months (M), eg. `3h`, `7d`, `3M`. The lowest possible value is one hour (`1h`).\n- `BACKUP_RETENTION` supports expressions in minutes (m) and days (d), eg. `360m`, `2d`. The lowest possible value is 60 minutes (`60m`).\n\n## Upgrade\n\n:exclamation::exclamation::exclamation:<span style=\"color: red\">**Make sure you have volumed data to somewhere outside Docker container**</span>:exclamation::exclamation::exclamation:\n\nSteps to upgrade Gogs with Docker:\n\n- `docker pull gogs/gogs`\n- `docker stop gogs`\n- `docker rm gogs`\n- Finally, create a container for the first time and don't forget to do the same for the volume and port mapping.\n\n## Known issues\n\n- The docker container cannot currently be built on Raspberry 1 (armv6l) as our base image `alpine` does not have a `go` package available for this platform.\n\n## Useful links\n\n- [Share port 22 between Gogs inside Docker & the local system](http://www.ateijelo.com/blog/2016/07/09/share-port-22-between-docker-gogs-ssh-and-local-system)\n\n\n\n# How to Install Docker on Kali Linux\n![Docker](https://user-images.githubusercontent.com/25181517/117207330-263ba280-adf4-11eb-9b97-0ac5b40bc3be.png)\n\n## Step 2\nMake the file executable\n```bash\nchmod u+x dockerkali.sh\n```\n## Step 3 \nRun the bash file\n```bash\n./dockerkali.sh\n```\n# How to Install Docker Compose on Kali Linux\n## Step 2\nMake the file executable\n```bash\nchmod u+x composekali.sh\n```\n## Step 3\nRun the bash file\n```bash\n./composekali.sh\n```\n# Docker Compose Commands\nTo install and run the container do\n```\ndocker compose up\n```\nTo restart the container do\n```\ndocker compose restart\n```\nTo remove the container do\n```\ndocker compose down\n```\n# Quick Start\n# Installing Gogs\n## Step 1\nMoving to the current directory that you are in to the \"Shell\" directory\n```bash\ncd Shell\n```\n## Step 2\nMake the file executable\n```bash\nchmod u+x ./gogs.sh\n```\n## Step 3\nStarting gogs.sh\n```bash\n./gogs.sh\n```\n# Installing Go\n## Step 1\nMoving to the current directory that you are in to the \"Shell\" directory\n```bash\ncd Shell\n```\n## Step 2\nMake the file executable\n```\nchmod u+x ./go.sh\n```\n## Step 3\nRun the bash file\n```\n./go.sh\n```\n# Uninstalling Go\n## Step 1\nMove from CLODSH to CLODSH/Shell\n```bash\ncd Shell\n```\n## Step 2\nMake the file executable\n```bash\nchmod u+x ./rmgo.sh\n```\n## Step 3\nUninstalling Go\n```bash\n./rmgo.sh\n```\n## CLODSH Readmemd \nVersion: 1.38\n"
    },
    {
      "name": "alphavector/all",
      "stars": 3,
      "img": "https://avatars.githubusercontent.com/u/11805788?s=40&v=4",
      "owner": "alphavector",
      "repo_name": "all",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2024-10-06T12:01:51Z",
      "updated_at": "2025-01-26T12:22:19Z",
      "topics": [],
      "readme": "python -m generator -w 100 -l 1000 -r requirements.txt\n\n200 - https://editor.mergely.com/vCYSrPu2\n500 - https://editor.mergely.com/Fuh9Gsyw\n"
    },
    {
      "name": "hemhemoh/DocLing",
      "stars": 3,
      "img": "https://avatars.githubusercontent.com/u/70777991?s=40&v=4",
      "owner": "hemhemoh",
      "repo_name": "DocLing",
      "description": "DocLing: Multilingual Document Understanding",
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2024-08-25T07:53:48Z",
      "updated_at": "2024-12-31T09:11:49Z",
      "topics": [],
      "readme": "# MultiLingual Document Understanding\n\nThis repository  demonstrates the implementation of a language model pipeline using Cohere's AYA Model. The notebook walks through various steps, from data preprocessing to model deployment, focusing on using Cohere's capabilities for language tasks.\n\n## Table of Contents\n\n- [Overview](#overview)\n- [Requirements](#requirements)\n- [Installation](#installation)\n- [Approach Description](#notebook-description)\n- [References](#references)\n\n## Overview\n\nOur approach showcases a pipeline designed to leverage the power of Cohere's NLP model for natural language processing tasks. This includes:\n- Preprocessing text data\n- Creating embeddings\n- Implementing a language understanding or generation task\n- Evaluating model outputs\n\n\n## Requirements\n\nBefore running the scripts in your local system, make sure you have the following installed:\n\n- Python 3.x\n- Jupyter Notebook or JupyterLab\n- Cohere Python SDK (`cohere`)\n- Other necessary libraries such as `pandas`, `numpy`, and `scikit-learn`\n\n## Installation\n\nTo set up the environment, follow these steps:\n\n1. Clone this repository:\n   ```bash\n   git clone https://github.com/hemhemoh/DocLing.git\n   ```\n2. Install the necessary dependencies:\n   ```bash\n   pip install -r requirements.txt\n   ```\n3. Obtain an API key from [Cohere](https://cohere.ai) and set it up in the notebook.\n4. Run the below command:\n```bash\npython app.py\n```\n\n## Approach Description\n\nThe approach follows these main steps:\n\n1. **Data Preparation:** Loads and preprocesses text data for NLP tasks.\n2. **Embedding Generation:** Utilizes Cohere’s model to create embeddings for text inputs.\n3. **Model Usage:** Demonstrates how to perform tasks like text classification, semantic search, or text generation using the Cohere API.\n4. **Evaluation:** Evaluates the model's performance based on the specific NLP task.\n\n## References\n\n- [Cohere API Documentation](https://docs.cohere.ai/)\n- [Pandas Documentation](https://pandas.pydata.org/)\n- [Scikit-learn Documentation](https://scikit-learn.org/)\n\n"
    },
    {
      "name": "Lyn4ever29/pipy_server",
      "stars": 3,
      "img": "https://avatars.githubusercontent.com/u/25952589?s=40&v=4",
      "owner": "Lyn4ever29",
      "repo_name": "pipy_server",
      "description": "Pypi本地镜像服务器搭建",
      "homepage": null,
      "language": "Python",
      "created_at": "2024-07-04T08:07:12Z",
      "updated_at": "2024-12-15T04:44:33Z",
      "topics": [],
      "readme": "# Pypi本地镜像服务器搭建\n\n\n## 主要功能\n\n- 全镜像同步(可以指定镜像源)\n- 下载指定依赖包\n- 定时同步 \n\n## 快速开始\n1. 安装依赖\n```shell \npip install schedule==1.2.2 pip2pi==0.8.2\n```\n\n2. 执行main.py\n```shell\npython main.py\n```\n此时可以看到packages目录下有所有的包和一个sample文件夹，如果需要在内网环境下使用，请把sample拷贝进内网机即可。\n\n\n3.配置pypi索引服务器\n可以使用python,也可以使用Nginx，Nginx配置可以查看[https://jhacker.cn/2024/pypi_server](https://jhacker.cn/2024/pypi_server)\n```shell\n#在下载目录里创建server服务，8080为端口号，可以随意设置：\ncd packages\npython -m http.server 8080\n```\n4.打开网页就可以看所有的包了\n```html\nhttp://localhost:8080/simple/\n```\n使用本地镜像服务器安装\n```shell\npip install numpy -i http://localhost:8080/simple/\n```\n\n## 配置说明\n\n- 具体配置文件可以查看config.json\n- requirements.txt中内置了一些常用的依赖包，可以根据自己需求添加\n- 如果想同步清华源全部依赖，可以执行```get_pypy_list.py```\n- 清华源的所有依赖```tsinghua_pkgs.txt```\n- ```schedule_task.py```可以设置定时任务，每天/每周同步更新官方源\n\n- **platform** 参数用于指定目标平台，以便下载与指定平台兼容的二进制包,以下是常见的配置内容：\n\n|配置内容| 说明                                                  |\n|--|-----------------------------------------------------|\n|win32| Windows 32位系统                                       |\n|win_amd64| Windows 64位系统（大多数人是这个）                              |\n|win_arm64| Windows ARM64系统                                     |\n|manylinux1_x86_64| 使用 manylinux1 标准构建的 Linux 64位系统（CentOS 5及更高版本兼容）    |\n|manylinux2010_x86_64| 使用 manylinux2010 标准构建的 Linux 64位系统（CentOS 6及更高版本兼容） |\n|manylinux2014_x86_64 | 使用 manylinux2014 标准构建的 Linux 64位系统（CentOS 7及更高版本兼容） |\n|linux_i686| Linux 32位系统                                         |\n|macosx_10_9_x86_64| macOS 10.9及更高版本的 Intel 64位系统                        |\n|macosx_11_0_arm64| macOS 11.0及更高版本的 ARM64系统                            |\n\n- **python_versions** 指的是python版本，只需要写大的版本号即可，如3.6、3.7、3.8、3.9等\n\n"
    },
    {
      "name": "Finboost/finboost-ml",
      "stars": 3,
      "img": "https://avatars.githubusercontent.com/u/169436477?s=40&v=4",
      "owner": "Finboost",
      "repo_name": "finboost-ml",
      "description": "Finboost Machine Learning is a financial chatbot project designed to answer finance-related questions in Indonesian. Utilizing advanced deep learning models, including a fine-tuned GPT-2 for generative AI and an LSTM for question suggestions, it offers personalized assistance and enhances user engagement.",
      "homepage": "https://finboost-waitlist.vercel.app/",
      "language": "Jupyter Notebook",
      "created_at": "2024-05-27T10:54:36Z",
      "updated_at": "2025-02-06T02:29:57Z",
      "topics": [],
      "readme": "<h1 align=\"center\">Finboost Machine Learning</h1>\n\n<div align=\"center\">\n\n![Python](https://img.shields.io/badge/-Python-05122A?style=flat&logo=python)&nbsp;\n![Flask](https://img.shields.io/badge/-Flask-05122A?style=flat&logo=flask)&nbsp;\n![TensorFlow](https://img.shields.io/badge/-TensorFlow-05122A?style=flat&logo=tensorflow)&nbsp;\n![Pandas](https://img.shields.io/badge/-Pandas-05122A?style=flat&logo=pandas)&nbsp;\n![Transformers](https://img.shields.io/badge/-Transformers-05122A?style=flat&logo=huggingface)&nbsp;\n![NumPy](https://img.shields.io/badge/-NumPy-05122A?style=flat&logo=numpy)&nbsp;\n![Matplotlib](https://img.shields.io/badge/-Matplotlib-05122A?style=flat&logo=matplotlib)&nbsp;\n![Google Colab](https://img.shields.io/badge/-Google%20Colab-05122A?style=flat&logo=googlecolab)&nbsp;\n\n</div>\n\n<p align=\"center\">Finboost Machine Learning is a financial chatbot project that can answer finance-related questions in Indonesian. The project leverages deep learning models with fine-tuning on the `cahya/gpt2-small-indonesian-522M` model for generative AI and uses an LSTM model for question suggestion.</p>\n\n---\n\n## Table of Contents\n\n- [Tech Stack](#tech-stack)\n- [Architecture](#architecture)\n- [File and Folder Structure](#file-and-folder-structure)\n- [Flow Diagrams](#flow-diagrams)\n- [Setup](#setup)\n- [Usage](#usage)\n- [Contributing](#contributing)\n\n## Tech Stack\n\n- **Python**: Primary programming language used for the project.\n- **Flask**: Web framework for building the REST API.\n- **TensorFlow**: Deep learning framework used for model training and inference.\n- **Pandas**: Library for data manipulation and analysis.\n- **Transformers**: Hugging Face library for working with transformer models.\n- **NumPy**: Library for numerical operations.\n- **Matplotlib**: Library for creating visualizations.\n- **Google Colab**: Cloud service for running Jupyter notebooks.\n- **GPT-2**: Transformer model fine-tuned for generating responses.\n- **LSTM**: Model used for question suggestion based on user input and profile data.\n\n## Architecture\n\nThis project uses the following architecture:\n\n1. **Data Preparation**: Preparing question and answer data in CSV format.\n2. **Model Fine-Tuning**: Fine-tuning the `cahya/gpt2-small-indonesian-522M` model using the dataset for generative AI.\n3. **LSTM Model**: Using an LSTM model for question suggestion.\n\n## File and Folder Structure\n\n| File/Folder Name                              | Description                                                                                                            |\n| --------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------- |\n| `data/generative-ai/finansial-dataset-v2.csv` | Example dataset generative-ai in CSV format                                                                            |\n| `data/question-suggestion/data.csv`           | Example dataset question suggestion in CSV format                                                                      |\n| `notebooks/generative_ai.ipynb`               | Notebook for fine-tuning and using the generative AI model                                                             |\n| `notebooks/question_suggestion.ipynb`         | Notebook for training and using the LSTM model for question suggestion                                                 |\n| `models/gen-ai`                               | Directory to store the fine-tuned generative AI model (create it yourself by running `notebooks/generative_ai.ipynb`)  |\n| `models/question-suggestion`                  | Directory to store the question suggestion model (create it yourself by running `notebooks/question_suggestion.ipynb`) |\n| `preprocessing/combined_dataset.ipynb`        | Notebook for preprocessing the collected dataset                                                                       |\n| `scripts/`                                    | Folder to save the script for future features                                                                          |\n| `generative-ai/`                              | Folder to deploy generative-ai model                                                                                   |\n| `generative-ai-v2/`                           | Folder to deploy generative-ai-v2 (`Coming soon`)                                                                      |\n| `generative-ai-v3/`                           | Folder to deploy generative-ai-v3 (`Coming soon`)                                                                      |\n| `question-suggestion/`                        | Folder to deploy question-suggestion model                                                                             |\n| `requirements.txt`                            | List of dependencies for this project                                                                                  |\n|  |\n\n## Flow Diagrams\n\n### Flow Generative AI Model\n\n![Flow Generative AI Model](assets/Flow-Generative-AI-Model.gif)\n\n### Flow Question Suggestion Model\n\n![Flow Question Suggestion Model](assets/Flow-Question-Suggestion-Model.gif)\n\n## Setup\n\n1. **Clone the repository:**\n\n   ```bash\n   git clone https://github.com/username/finboost-ml.git\n   cd finboost-ml\n   ```\n\n2. **Create a virtual environment:**\n   ```bash\n   python -m venv venv\n   source venv/bin/activate  # On Windows use `venv\\Scripts\\activate`\n   ```\n3. **Install dependencies:**\n   ```bash\n   pip install -r requirements.txt\n   ```\n4. **Download and place the datasets:**\n\n   - Place `finansial-dataset-v2.csv` in the `data/generative-ai/` directory.\n   - Place `data.csv` in the `data/question-suggestion/` directory.\n\n5. **Create Folder Models:**\n\n- Create a `models` folder in the root directory `/`:\n  ```bash\n  mkdir models\n  ```\n  > **NOTE**: When running the Flask application, you must also create model folders within the `generative-ai` and `question-suggestion`\n- Create `models` folders in the `generative-ai/models` and `question-suggestion/models` directories:\n  ```bash\n  mkdir -p generative-ai/models\n  mkdir -p question-suggestion/models\n  ```\n\n6.  **Run the notebooks to obtain the models**\n\n    - Open `notebooks/generative_ai.ipynb` in Google Colab or Jupyter Notebook and execute all the cells to fine-tune the `cahya/gpt2-small-indonesian-522M` model and generate the model files.\n    - Open `notebooks/question_suggestion.ipynb` in Google Colab or Jupyter Notebook and execute all the cells to train the LSTM model and generate the model files.\n\n7.  **Download and place the models:**\n    - Place the fine-tuned generative AI model in the each `models/gen-ai/` directory.\n    - Place the question suggestion model files (`tokenizer.pickle`, `label_dict.pickle`, and `model_question_suggestion.h5`) in the each `models/question-suggestion/` directory.\n8.  **Run the Flask application:**\n\n    ```bash\n    python generative-ai/app/main.py\n    python question-suggestion/app/main.py\n    ```\n\n## Usage\n\nRun the notebook:\n\n1. Run the Generative AI notebook:\n\n- Execute all cells in the `generative_ai.ipynb` notebook to fine-tune the `cahya/gpt2-small-indonesian-522M`model and generate responses.\n\n2. Run the Question Suggestion notebook:\n\n- Execute all cells in the `question_suggestion.ipynb` notebook to train the LSTM model and make question suggestions.\n\n3. Fine-Tuning:\n\n- Use the fine-tuning script `generative_ai.py` to fine-tune the `cahya/gpt2-small-indonesian-522M` model on new data.\n\n4. Question Suggestion:\n\n- Use the `question_suggestion.py` script to train and use the LSTM model for question suggestion.\n\n## Contributing\n\n1. Fork the repository\n2. Create a new branch (`git checkout -b feature-branch`)\n3. Commit your changes (`git commit -am 'Add new feature'`)\n4. Push to the branch (`git push origin feature-branch`)\n5. Create a new Pull Request\n\n---\n\nYou can save the above content into the `README.md` file for your project. This file includes all the essential information about the project, including folder structure, setup steps, and usage instructions.\n"
    },
    {
      "name": "dlt-hub/llm_adapter",
      "stars": 3,
      "img": "https://avatars.githubusercontent.com/u/89419010?s=40&v=4",
      "owner": "dlt-hub",
      "repo_name": "llm_adapter",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2023-11-15T18:20:18Z",
      "updated_at": "2024-02-28T08:25:08Z",
      "topics": [],
      "readme": "# Haystack Adapter\n\nThis project aims to integrate dlt into Haystack\nIt cointains two main functions:\n- llm_adapter -> converts the data into the format that can be used by Haystack\n- haystack_adapter -> wraps dlt into Haystack pipeline\n\n## Getting Started\n\nThese instructions will guide you through setting up the Haystack integration in your local environment.\n### Prerequisites\n\nYou need Python installed on your system. The function is compatible with Python 3.11 and above. You can download Python from [here](https://www.python.org/downloads/).\n\nPoetry is used to manage library dependencies. Make sure to have Poetry installed. \n\n### Installation\n\nClone this repository or download the source code to your local machine. The primary function is contained in the `llm_adapter.py` file.\n\n```bash\ngit clone https://github.com/dlt-hub/llm_adapter\ncd llm_adapter\n```\n\n\n\n### Haystack 2.0 Integration\n\nThe `DltConnector` function is used to integrate dlt into the Haystack pipeline. The function is contained in the `dlt_haystack.py` file.\n\n```python cryptodataconnector.py```\n\n\n\n\n### dlt llm adapter\nUsage\n\nAn example of the implementation is in the 'demo' folder\n\nRun the following command to start the Poetry shell:\n\n```poetry shell```\n\nTo see a working example, navigate to the `demo` folder and run the following command:\n\n```python main.py```\n\nOtherwise, to use the llm_adapter function in your Python script, first import it:\n```\n\nfrom llm_adapter import llm_adapter\n\n```\nThen, call the function with your data:\n```\n\ndata = [\n    {\"name\": \"Anush\", \"last name\": \"Smith\", \"unique_id\": \"2835859394\", \"age\": \"30\"},\n    # ... other data entries ...\n]\n\ndocuments = llm_adapter(data, to_content=[\"name\", \"last name\"], to_metadata=[\"unique_id\", \"age\"], llm_framework='haystack')\n```\nThe same structure works for haystack\nIn the main function, after adding Pinecone key, you can also write haystack docs to Pinecone\n```\n    documents = llm_adapter(data, to_content=[\"name\", \"age\", \"city\"], to_metadata=[\"id\", \"email\"], llm_framework='haystack')\n    from vectorstore_manager import _init_haystack_pinecone\n\n    #example of haystack writer, requries pinecone key to be provided\n    retriever = _init_haystack_pinecone()\n    retriever.write_documents(documents)\n\n```\nIn addition to this, additional feature was added which represents Haystack 2.0 integration with the dlthub.\n\nIn that context, dlthub is used for document loading and transformation as a part of the Haystack pipeline.\nEach element was defined as a Haystack 2.0 component and can be used as a part of the Haystack pipeline.\n"
    },
    {
      "name": "Tsadoq/haystack-documentation-agent",
      "stars": 3,
      "img": "https://avatars.githubusercontent.com/u/28503469?s=40&v=4",
      "owner": "Tsadoq",
      "repo_name": "haystack-documentation-agent",
      "description": "A quick pipeline (and relative streamlit dashboard) to chat with haystack documentation",
      "homepage": null,
      "language": "Python",
      "created_at": "2023-12-15T08:58:06Z",
      "updated_at": "2025-01-28T14:42:00Z",
      "topics": [],
      "readme": "# Haystack Documentation Agent\n\n## Overview\n\nThis repository contains the code for the Haystack Documentation Agent. The agent is responsible for generating the \ndocumentation for the Haystack project. The agent is written in Python and uses the \n[Haystack Framework](https://haystack.deepset.ai/) for the RAG and [Streamlit](https://streamlit.io/) for the UI.\n\n## Installation\n\nYou can run this locally by cloning the repository and installing the dependencies:\n\n```bash\ngit clone https://github.com/Tsadoq/haystack-documentation-agent\ncd haystack-documentation-agent\npip install -r requirements.txt\n```\n\n## Usage\n\nTo run the agent, simply run the following command:\n\n```bash\nstreamlit run app.py\n```\n\nAlternatively, you can go on the Hugging Face Spaces page for this project and [run it from there](https://huggingface.co/spaces/tsadoq/unofficial-haystack-documentation-agent).\n\nYou can see a demo here:\n\n\n\nhttps://github.com/Tsadoq/haystack-documentation-agent/assets/28503469/ff9ca7e5-6710-434a-94d1-bce3f8be280c\n\n\n\n## Disclaimer\n\nThis project is a toy project and is not meant to be for any critical use. I am not responsible for any damage that\ncould be caused by the use of this project. The answers provided by the agent are not guaranteed to be correct. I am \nnot related to the Haystack project in any way.\n"
    },
    {
      "name": "UW-Madison-DSI/ask-xDD",
      "stars": 3,
      "img": "https://avatars.githubusercontent.com/u/62457943?s=40&v=4",
      "owner": "UW-Madison-DSI",
      "repo_name": "ask-xDD",
      "description": "Retrieval-Augmented Generation (RAG) on 17M full text journal articles.",
      "homepage": "https://xdd.wisc.edu/",
      "language": "HTML",
      "created_at": "2023-03-28T15:24:08Z",
      "updated_at": "2024-09-24T13:06:30Z",
      "topics": [],
      "readme": "# ASK-xDD\r\n\r\nAskem retrieval-augmented generation prototype\r\n\r\nRepo: <https://github.com/UW-Madison-DSI/ask-xDD>\r\n\r\nDemo: <https://xdddev.chtc.io/ask-xdd-demo>\r\n\r\nAPI Base URL: <http://cosmos0002.chtc.wisc.edu:4502/>\r\n\r\n## For end-users\r\n\r\nThe end users of our system are ASKEM performers who access it using REST API. You can also visit our [demo](http://cosmos0002.chtc.wisc.edu:8501/) to try how this system can power a traceable COVID-19 search engine.\r\n\r\n### Release notes (v0.3.0)\r\n\r\n#### Highlights\r\n\r\n- Enhance performance tailored to Hackathon scenarios\r\n- Integrate `ReAct` for better handling of complex queries\r\n- Implement `hybrid` search to refine keyword query results\r\n\r\n### Retriever overview\r\n\r\n![overview](img/overview_0.3.0.png)\r\n\r\nThe retriever uses an embedding-based search engine, specifically [Dense Passage Retriever (DPR)](https://arxiv.org/abs/2004.04906), to query relevant documents from the XDD database. Currently, it returns `paragraphs` as documents. Future updates may include `figures`, `tables`, and `equations`. The API accepts **POST** requests and requires an **APIKEY**. ASKEM performers can obtain an API key by contacting [me](mailto:jason.lo@wisc.edu).\r\n\r\nBase URL: <http://cosmos0002.chtc.wisc.edu:4502>\r\n\r\nThere are 3 endpoints available:\r\n\r\n1. `vector`: Basic DPR vector search (Not recommended).\r\n2. `hybrid`: Combines Elasticsearch pre-filtering with DPR vector search (Recommended, better performance).\r\n3. `react`: Builds on the `hybrid` approach, integrating the [ReAct agent](https://react-lm.github.io/) for \"reasoning\" (via gpt-4 by default) and subsequent querying (via `hybrid` endpoint by default) to generate better answers. (Experimental, slow, highest performance).\r\n\r\n### `vector` and `hybrid` endpoint example usage\r\n\r\nBoth `vector` and `hybrid` endpoints use similar format for request and response data.\r\n\r\n```python\r\nimport requests\r\n\r\nAPIKEY = \"insert_api_key_here\"\r\nENDPOINT = \"BASE_URL/hybrid\"\r\n\r\nheaders = {\"Content-Type\": \"application/json\", \"Api-Key\": APIKEY}\r\ndata = {\r\n    \"topic\": \"covid\",\r\n    \"question\": \"What is SIDARTHE model?\",\r\n    \"top_k\": 3,\r\n}\r\n\r\nresponse = requests.post(ENDPOINT, headers=headers, json=data)\r\nresponse.json()\r\n```\r\n\r\n#### Request body schema for `vector` and `hybrid` endpoints\r\n\r\n```python\r\n{\r\n    \"question\": str,\r\n    \"top_k\": Optional[int] = 5, # Number of documents to return\r\n    \"distance\": Optional[float] = None, # Max cosine distance between question and document\r\n    \"topic\": Optional[str] = None, # Filter by topic, only \"covid\" is available now\r\n    \"doc_type\": Optional[str] = None,  # Filter by document type, only \"paragraph\" is available now\r\n    \"preprocessor_id\": Optional[str] = None,  # Filter by preprocessor_id, for developer use only\r\n    \"article_terms\": Optional[List[str]] = None,  # Obsolete, do not use\r\n    \"paragraph_terms\": Optional[List[str]] = None,  # Filter by capitalized terms (any word that has more than one capital letter) in the paragraph\r\n    \"paper_ids\": Optional[List[str]] = None,  # Filter by XDD paper ids\r\n    \"move_to\": Optional[str] = None,  # Move the answer to better match the context of the given string, like `mathematical equation`.\r\n    \"move_to_weight\": Optional[float] = 0,  # Weight `move_to` parameter to adjusts the influence on the original answer, with a range from 0 to 1. Higher values mean stronger augmentation.\r\n    \"move_away_from\": Optional[str] = None,  # Move the answer away from irrelevant topics, like `general commentary`.\r\n    \"move_away_from_weight\": Optional[float] = 0,  # Weight `move_away_from` to adjusts the influence on the original answer, with a range from 0 to 1. Higher values mean stronger augmentation.\r\n    \"screening_top_k\": Optional[int] = 100,  # `hybrid` endpoint only. Number of documents to return from the elastic search pre-filtering step.\r\n}\r\n```\r\n\r\n#### Response body schema for `vector` and `hybrid` endpoints\r\n\r\n```python\r\n[\r\n    {\r\n        \"paper_id\": str,  # XDD paper id\r\n        \"doc_type\": str,  # only \"paragraph\" for now\r\n        \"text\": str,  # text content\r\n        \"distance\": float,  # distance to question\r\n        \"cosmos_object_id\": str,  # only available for doc_type=\"figure\"\r\n        \"article_terms\": List[str],  # Obsolete, do not use\r\n        \"paragraph_terms\": List[str], # Capitalized terms in the paragraph\r\n    },\r\n    ...\r\n]\r\n```\r\n\r\n### `react` endpoint example usage\r\n\r\n```python\r\nimport requests\r\n\r\nAPIKEY = \"insert_api_key_here\"\r\nENDPOINT = \"BASE_URL/react\"\r\n\r\nheaders = {\"Content-Type\": \"application/json\", \"Api-Key\": APIKEY}\r\ndata = {\r\n    \"topic\": \"covid\",\r\n    \"question\": \"What is SIDARTHE model?\",\r\n    \"top_k\": 3,\r\n}\r\n\r\nresponse = requests.post(ENDPOINT, headers=headers, json=data)\r\nresponse.json()\r\n```\r\n\r\n#### Request body schema for `react` endpoint\r\n\r\n```python\r\n{\r\n    \"question\": str,\r\n    ..., # Same as `hybrid` endpoint, see\r\n    \"model_name\": Optional[str] = \"gpt-4\",  # OpenAI llm model name\r\n}\r\n```\r\n\r\n#### Response body schema for `react` endpoint\r\n\r\n```python\r\n{\r\n    \"answer\": str,  # Final answer to the question\r\n    \"used_docs\": list[Document],  # Relevant documents used to generate the answer, with the same schema as the response of `hybrid` endpoint\r\n}\r\n```\r\n\r\n<details>\r\n    <summary style=\"font-size: 1.5em;\">For developer</summary>\r\n\r\n### To deploy the system\r\n\r\n1. Make a .env file in the project root directory with these variables\r\n\r\n    see example: `.env.example`\r\n\r\n    see shared [dotenv](https://docs.google.com/document/d/1TyGeHxbOShv_jzTIM7vn-equH0XB3wM0mBuAvYI0AR0/edit) file for the actual values\r\n\r\n1. Run launch test\r\n\r\n    ```sh\r\n    bash ./scripts/launch_test.sh\r\n    ```\r\n\r\n1. Ingest documents\r\n\r\n    Put all text files in a folder, with file format as `<ingest_dir>/<paper-id>.txt`, then run this:\r\n\r\n    ```sh\r\n    python askem/ingest_docs.py --input-dir \"data/debug_data/paragraph_test\" --topic \"covid-19\" --doc-type \"paragraph\" --weaviate-url \"url_to_weaviate\"\r\n    ```\r\n\r\n1. Ingest figures\r\n\r\n    Put all text files in a folder, with file format as `<ingest_dir>/<paper-id>.<cosmos_object_id>.txt`, then run this:\r\n\r\n    ```sh\r\n    python askem/deploy.py --input-dir \"data/debug_data/figure_test\" --topic \"covid-19\" --doc-type \"figure\" --weaviate-url \"url_to_weaviate\"\r\n    ```\r\n\r\n### To add a new topic\r\n\r\n1. In [retriever data models](askem/retriever/data_models.py), add new topic to `Topic` enum class\r\n1. Ingest data with the new topic\r\n1. Make sure xDD articles API has the new topic `dataset` available, they use the same name without any translation layer.\r\n1. Add new preset demo questions to `askem/demo/present_questions`, e.g.: [climate change preset questions](askem/demo/preset_questions/preset_climate_change_q.txt)\r\n1. Add `Topic` in [demo](askem/demo/app.py).\r\n\r\n</details>\r\n"
    },
    {
      "name": "machinelearningzuu/mastering-llm-apps",
      "stars": 3,
      "img": "https://avatars.githubusercontent.com/u/41842488?s=40&v=4",
      "owner": "machinelearningzuu",
      "repo_name": "mastering-llm-apps",
      "description": "Welcome to Build LLM Apps with LangChain, your gateway to creating powerful applications using Large Language Models (LLMs) powered by LangChain technology.",
      "homepage": "",
      "language": "Jupyter Notebook",
      "created_at": "2023-09-30T10:29:17Z",
      "updated_at": "2024-10-09T10:14:42Z",
      "topics": [
        "chatgpt",
        "haystack",
        "huggingface",
        "langchain",
        "llamaindex",
        "llm",
        "openai"
      ],
      "readme": "# Build Powerful LLM Apps with LangChain, HayStack and LlamaIndex\n\nWelcome to Build LLM Apps with LangChain, HayStack and LlamaIndex, your gateway to creating powerful applications using Large Language Models (LLMs) powered by LangChain technology.\n\n## Overview\n\nLangChain technology represents a cutting-edge approach to leveraging LLMs for a wide range of applications, from chatbots and virtual assistants to content generation and sentiment analysis. With LangChain, you can harness the full potential of LLMs to build intelligent and language-driven solutions.\n\n## What You'll Find Here\n\n- **Getting Started**: Dive into LangChain with step-by-step guides, tutorials, and examples that help you build your own LLM-powered applications.\n\n- **LangChain SDK**: Access the LangChain Software Development Kit (SDK) and API documentation to integrate LLM capabilities into your projects seamlessly.\n\n- **Sample Projects**: Explore a variety of sample projects and use cases showcasing the versatility and power of LangChain-powered LLMs in real-world scenarios.\n\n- **Community and Collaboration**: Join a vibrant community of developers, researchers, and enthusiasts to share ideas, seek assistance, and collaborate on innovative LLM applications.\n\n## Get Started\n\nWhether you're an experienced developer or just starting to explore the possibilities of LangChain and LLMs, Build LLM Apps with LangChain offers a platform for learning, experimentation, and innovation.\n\nFeel free to clone, fork, or contribute to this repository to build incredible language-driven applications with LangChain technology.\n\nJoin us on this exciting journey of creating intelligent and interactive LLM-powered apps with LangChain!"
    },
    {
      "name": "zhiyiyi/stock-price-predition",
      "stars": 3,
      "img": "https://avatars.githubusercontent.com/u/36960178?s=40&v=4",
      "owner": "zhiyiyi",
      "repo_name": "stock-price-predition",
      "description": "Ploomber Hacktoberfest 2023 project: Classic SQL ETL with ML integration.",
      "homepage": "",
      "language": "Jupyter Notebook",
      "created_at": "2023-09-24T04:27:48Z",
      "updated_at": "2023-11-06T05:12:07Z",
      "topics": [],
      "readme": "# *Hacktoberfest 2023*: Building an ETL Pipeline and utilizing Machine Learning for stock market predictions\n\n\n## :jack_o_lantern: Introduction\n\n[Ploomber](https://ploomber.io/) is a dynamic startup that's dedicated to revolutionizing the landscape of data pipeline development and deployment, accommodating various code editors like Jupyter Notebooks and VSCode. Beyond advancing technology, the Ploomber team is committed to fostering growth within the Data Science and Machine Learning community. \n\nIn pursuit of this vision, they proudly initialized the celebrated Hacktoberfest 2023 event—a 5-week mentorship opportunity within a realistic project-based, open-source environment designed for 10 early-career hackers. This unique experience offers participants expert mentorship, hands-on development of production-ready projects, a deep dive into practical skills, networking opportunities within a passionate community, and the chance to contribute to open-source—all free of cost.\n\nBy the end of this program, participants will have had exposure and developed their skills in the following areas:\n- Proficient in writing clean and reproducible code using SQL and Python.\n- Expertise in package management.\n- Capable of developing modularized applications.\n- Familiarity with Dockerizing applications.\n- Skilled in deploying applications using Ploomber Cloud.\n- Strong communication skills for conveying data insights to stakeholders.\n\n\nAs part of Ploomber's Hacktober 2023 initiative, our project will focus on building an extract-load-transform (ELT) pipeline connecting the live stock market data from the `yfinance` API to our [Mother Duck](https://motherduck.com/) cloud warehouse. With this project, our intent was to apply modern best practices of Data Engineering and Machine Learning to the domain of Finance and Investing in order to help investors make optimal, data-driven decisions with their portfolios. In addition to the implicit payoff of honing our collective Data Science/Engineering skillset was also the the opportunity for economic gain should we succeed in building an accurate model :money_mouth_face:\n\nWith the vast array of stock market data in our cloud infrastructure, we'll then fit various machine learning models to generate predictions on our dataset and deploying our model using Ploomber cloud.\n\n## :chart_with_upwards_trend: Data sources\n\nThe bulk of our dataset will be scraped from the [yfinance](https://github.com/ranaroussi/yfinance) Python library which is an open-source tool that utilizes Yahoo's publicly available API with the intention of providing stock market data for research and educational purposes. The library was developed in response to Yahoo decomissioning their historical data but is not affiliated, endorsed, or vetted by Yahoo Inc in any way and is distributed under the [Apache Software Lisence](https://github.com/ranaroussi/yfinance/blob/main/LICENSE.txt).\n\nThe historical stock price data for a given ticker can be accessed by inputting the ticker symbol within the `Ticker()` module, followed by the `history()` method. Below is a vanilla code example to pull all the historical price activity for the [Alphabet Inc's (Google) Class A](https://finance.yahoo.com/quote/GOOG/) stock:\n\n```\nimport yfinance as yf\n\nmsft = yf.Ticker(\"MSFT\")\ndf = msft.history(period='max')\n```\n\nBelow are the parameters available to specify the intervals from which you wish to pull stock data from:\n\n- `period`: Specify the data period for download using either the \"period\" parameter or the \"start\" and \"end\" parameters. Valid periods include: `1d`, `5d`, `1mo`, `3mo`, `6mo`, `1y`, `2y`, `5y`, `10y`, `ytd`, `max`.\n- `interval`: Define the data interval, keeping in mind that intraday data cannot extend beyond the last 60 days. Valid intervals are: `1m`, `2m`, `5m`, `15m`, `30m`, `60m`, `90m`, `1h`, `1d`, `5d`, `1wk`, `1mo`, `3mo`.\n- `start`: When not using the \"period\" parameter, set the start date for download as a string (YYYY-MM-DD) or datetime using the \"start\" parameter.\n- `end`: When not using the \"period\" parameter, set the end date for download as a string (YYYY-MM-DD) or datetime using the \"end\" parameter.\n- `prepost`: Decide whether to include Pre and Post market data in the results by setting the \"prepost\" parameter (default is `False`).\n- `auto_adjust`: Choose whether to automatically adjust all OHLC (Open, High, Low, Close) using the \"auto_adjust\" parameter (default is `True`).\n- `actions`: Opt to download stock dividends and stock splits events by setting the \"actions\" parameter (default is `True`).\n\n\n## :wrench: Methods \n\nTo accomplish our end goal of building a model which forecasts stock market data, we needed to employ an **Extract, Load, Transform (ELT) architecture** for our data pipeline. ELT pipelines differ slightly from the conventional **Extract, Transform, Load (ETL) architecture** in that we clean and transform our data BEFORE we load it to the warehouse. With an ELT architecture, on the other hand, we can conduct the data cleaning and transformations within the data warehouse or another environment AFTER we've loaded the data. \n\nOne of the main justifications for utilizing an ELT pipeline, rather than the conventional ETL architecture, is that the stock market data from the `yfinance` API was largely clean and standardized from the get-go. Therefore, to optimize on compute, we decided to load the data directly into a MotherDuck cloud warehouse where we can ensure scalability, accessibility, and efficient storage.\n\nWith the data loaded in a MotherDuck warehouse, the logical next step was to conduct the data cleaning, data visualization, and predictive modelling phases of the Data Science lifecycle in [Jupyter Notebook](https://jupyter.org/). As a fundamental tool amongst practicing Data Scientists, Jupyter Notebook enables users to directly access data from the cloud and experiment with different transformations, visualizations, and the training and testing of predictive models. And within our Jupyter Notebooks, our team utilized tools such as:\n\n* The SQL programming language to query our database, \n* Python packages like:\n    * [Pandas](https://pandas.pydata.org/) for data transformation, manipulation, and analysis;\n    * [Matplotlib](https://matplotlib.org/stable/) and [Seaborn](https://seaborn.pydata.org/) for data visualization;\n    * [Scikit-Learn](https://scikit-learn.org/stable/index.html#) for building machine learning models.\n    * And [TensorFlow](https://www.tensorflow.org/) for building deep learning models.\n\n<img src=\"images/elt-pipeline-architecture.png\">\n\nLast but not least, the final stage of our project included the deployment of the work in our Jupyter Notebooks to a [Voila](https://voila-dashboards.github.io/) dashboard, using Ploomber and Docker containers. Voila is a bleeding-edge subproject within the Jupyter ecosystem which transforms static Python notebooks into standalone, interactive web applications. With Voila, we were able to save the outputs of our analysis and generate actionable insights.\n\n\n## :busts_in_silhouette: Team members\n\nWith the help, guidance, and mentorship from [Laura Gutierrez Funderburk](https://github.com/lfunderburk) and the rest of the Ploomber team, the core contributors for this project include:\n\n:star: [Zhiyi Chen](https://github.com/zhiyiyi)\n\n:star: [Alice Liang](https://github.com/Aliceliangwk) \n\n:star: [Ruiz Rivera](https://github.com/vanislekahuna)\n\n\n:incoming_envelope: Thank you for taking the time to review our project. Please feel free to contact us with any feedback, questions or comments you may have!"
    },
    {
      "name": "c0ntradicti0n/system",
      "stars": 3,
      "img": "https://avatars.githubusercontent.com/u/46457469?s=40&v=4",
      "owner": "c0ntradicti0n",
      "repo_name": "system",
      "description": "Regenerate Helgels Dialectic System with ChatGPT",
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2023-07-16T20:12:51Z",
      "updated_at": "2024-04-23T16:41:26Z",
      "topics": [],
      "readme": "# `System` - LLM - dialectics\n## `FractalVerse` - a fractalized verse of the universe\n\n\nI instructed ChatGPT to continuously update a data-structure to imitate [G.W.F. Hegel](https://en.wikipedia.org/wiki/Georg_Wilhelm_Friedrich_Hegel)\n\nYou can visit the result on [https://polarity.science](https://polarity.science)\n\nIt visualizes in the style of [https://hegel-system.de/en/](https://hegel-system.de/en/) the basic intention of Hegels dialectic:\nFractalize all concepts into triples of thesis, antithesis and synthesis.\n\nHegel thought his method is more accurate than mathematics, because it is more general and can be applied to all topics.\nWe don't know if he was right, but we can try to apply his method to all topics and see what happens, so it is a bit like rubber.\n\n\nIt did not write a reasonable README, so I just show, what is the prompt and it understood it.\n\n```\nYou are extending a dialectical system, emulating Hegel's methodology, where concepts unfold within a \nfractal structure of triples. Each triple consists of:\n\n - Thesis (1)\n - Antithesis (2)\n - Synthesis (3)\n\nEach triple is a dialectical unit, which can be further extended by adding a new triple to any of them.\n\nAdditionally we explain about each triple a antinomic mutual relation. Remember the Kantian antinomies, and \nother famous intricacies. \nThis should point to  the evolution of the argumentation by identifying the conflict within the \ndialectic triple, thereby triggering the formation of the next triple. This self-applicable antonym to the thesis \nexpresses ideas like 'minus * minus = plus', or 'the disappearance of disappearance is existence.') (_)\n\nThere is also this constraint: \nThe fractal lives from semantic analogies, every subtopic from [1-3]*1 should mirror the topics in [1-3]*2 by \nANALOGY (as far as possible). Please respect this, therefore you are given example other entries, that might correspond.\n\nRespect, that it moves from the most abstract to the most concrete concepts when moving deeper into the nesting \nand horizontally it moves by the defining other concepts by first treating the ingredients and then the complexities \nof those. Another Note on categorization: The fractal has to conquer the realm of overlapping categories. Feelings might be\n \"happy\" and \"sad\" as bigger categories and \"joyful\" and \"depressed\" might appear as sub-categories or \n rather side-categories, this is out preferred approach.\n This means instead of \n \n11 happy\n111 joyful\n12 sad\n121 depressed\n\nwe would like to organize it like this:\n\n111 happy\n112 sad\n121 joyful\n122 depressed\n\nSo the most basic category opens the field and gets mirrored by the other categories and dividing up all opposites \ninside a nesting.\n\nAlso respect, that we handle the concepts as \"ideals\" and daily politics and doubts are not the topic here, if something\nneeds to be changed, it should be changed, there must not be a hint, that something might not be right.\n\nYou'll work with a representation reflecting your current progress. Each unit is addressed via a key as \"13221.\" \nfor the thesis in \"1322\".\nYour goal is enrich the thematic structure of the fractal triples by diving deeper by creating \nnew triples.\n\nEvery nesting level should have a bare path for telling the theme and also a '_'-entry to mark dialectical conceptual \nmovement, every path should match this regex: [1-3]_?\n\nExamples:\n\n- suggest alternative topic\n3331. new topic\n\n- suggest alternative inversion antonym\n332_ new inversion antonym\n\n- suggest alternative thesis\n31 new thesis\n\n- suggest alternative antithesis\n312322 new antithesis\n\n- suggest alternative synthesis\n2233123 new synthesis\n\n- add multiple things like (but with other contents). PREFER THIS FOR MANY ENTRIES\n31 {1: \"Force\", 2: \"Motion\", 3: \"Energy\", \"_\": \"Inertia-Dynamics\", \".\": \"Physics\"}\n32 {1: \"Cell\", 2: \"Organism\", 3: \"Ecosystem\", \"_\": \"Individual-Community\", \".\": \"Biology\"}\n31 {1: \"Atom\", 2: \"Molecule\", 3: \"Compound\", \"_\": \"Element-Compound\", \".\": \"Chemistry\"}\n\nStick exactly to this syntax, don't exchange: '.', '_' with more meaningful information, put the meaning only into \nthe strings.\nRespond only with the keys like 31332 and succinct suggestions. Avoid any other explanatory phrase! Your proposals should \nbe limited to 1-4 word titles, no sentences.\nThus your output should be a list with a syntax like this:\n\n3331 \"Colors\"\n332 \"Wave-Particle\"\n3 \"Light\"\n31232 \"Darkness\"\n223312 \"Color\"\n31 {1: \"Force\", 2: \"Motion\", 3: \"Energy\", \"_\": \"Inertia-Dynamics\", \".\": \"Physics\"}\n\nDon't be audacious and dont just change the series of words in one title, keep it as simple as possible, avoid \nrepetitions in titles, the simplest words are the best and less words is more content. Be enormously precise \nwith your answers and the keys, every wrong path causes chaos and will kill the whole project.\nFocus only on scientific objective topics as math, geometry, physics, chemistry, biology, epistemology, music, \ncolors, linguistics and other real things with popular polarities. Absolutely avoid any topics of philosophy \nof mind and psychology and avoid the topic of consciousness at all. Philosophers nowadays are not able to think \nabout consciousness, the language is partying there too much.\nFocus on a top-down approach to get to some more systematic dialectical structure; first all upper levels, then the lower levels.\nFocus on completeness of the fractal, please fill up all incomplete triples, rather add new triples than improving existing ones.\n\n\n\nAnd please dump all your knowledge accurately about 13121, Reaction,\n\nHere is a truncated version of the current dialectical system to the path, where you should operate on:\n\n System\n_ System-Chaos\n1 Objectivity\n1_ Synthetic a Priori\n11 Things and Properties\n11_ Intrinsic-Extrinsic\n12 Framework of Placement\n12_ Dimensional Progression\n13 Nature\n13_ Whole-Parts\n131 Nature Mechanics\n131_ Conservation-Transformation\n1311 Physics\n1311_ Energy-Matter\n13111_ Field Dynamics-Constituency\n13112 Nuclear Physics\n13112_ Stability-Instability\n13113 Mechanics\n13113_ Determinism-Indeterminism\n1312 Chemistry\n1312_ Element-Compound\n13121 Reaction\n1313 Biology\n1313_ Static-Dynamic\n13131 Life\n132 Earth\n132_ Solid-Liquid-Gas\n133 ' Astronautics'\n133_ Exploration-Settlement\n2 Subjectivity\n2_ Objective-Subjective\n21 Natrual Necessity\n21_ Fatalism-Free will\n22 Values\n22_ Revaluation of all values\n23 Making\n23_ Invention-Adaptation\n231 Domains of Knowledge\n231_ Abundance of Knowledge\n2311 Art\n2311_ Creator-Creation\n23111 Innovation\n23111_ Discovery-Invention\n23112 Judgments\n23112_ Standard-Deviation\n23113 Responsibilities\n23113_ Duty-Freedom\n2312 Science of Science\n2312_ Methodology-Epistemology\n23121 Scientific Method\n23121_ Hypothesis-Proof\n23122 Paradigm Shifts\n23122_ Static-Dynamic\n23123 Measurement\n23123_ Qualitative-Quantitative\n2313 Philosophy of Science\n2313_ Model-Reality\n23131 Scientific Realism\n23131_ Observable-Unobservable\n23132 Causality\n23132_ Cause-Effect\n23133 Falsifiability\n23133_ Testable-Untestable\n3 Culture\n3_ Negotiation-of-Meaning\n31 Usage\n31_ Aim-Effect\n32 Institutional Structures\n32_ Conservative-Revolutionary\n33 Cultural Embodiment\n33_ Tradition-Innovation\n331 Religion\n331_ Divine-Mundane\n3311 Theism\n3311_ Monotheism-Polytheism\n33111 Christianity\n33111_ Trinity-Oneness\n33112 Islam\n33112_ Sunnism-Shiism\n33113 Hinduism\n33113_ Deity-Atman\n3312 Religious Worldviews\n3312_ Monotheism-Polytheism\n33121 Monotheistic Faiths\n33121_ God-Creation\n33122 Polytheistic Pantheon\n33122_ Gods-Interactions\n33123 Animism\n33123_ Spirit-Nature\n3313 Atheism\n3313_ Belief-Disbelief\n33131 Rational Atheism\n33131_ Evidence-Absence\n33132 Existential Atheism\n33132_ Meaning-Nihilism\n33133 Agnosticism\n33133_ Known-Unknown\n\n\n\nAnd, please dump all you wisdom as a table of contents (only provide titles for chapters) by using our number format accurately and deeply nested about \n \n13121, Reaction,\n\n\nRespect that it should work on analogies resembling the following topics\n\n1311_ Energy-Matter\n1311. Physics\n13112 Nuclear Physics\n13113 Mechanics\n    \nand don't output any other commentary, the code will extract the titles from your output.\n  \nCan you please stay on the ground and first develop the topics with quite normal knowledge? As motion is a kind combination of space and time.\nAnother good hint to the structure, that MUST be observed, is: 1 - is the general, 2 - is the particular, 3 - is the individual as their combination.\n```\n\n# TODO\n\n * [ ] add github pages\n * [ ] fix links in tooltips\n * [ ] debug more scoring\n * [ ] add dialog-guide to organize your own topics\n * [ ] debug socket.io - fe and be\n * [ ] add online-learning for models\n * [ ] save and apply edit actions\n * [ ] add life-inspection of training\n * [ ] make the titels appear in front of other triangles\n * [ ] mobile layout\n * [ ] run in kube\n\n\n# DONE\n"
    },
    {
      "name": "riadibadulla/SmartRedBox",
      "stars": 3,
      "img": "https://avatars.githubusercontent.com/u/33466778?s=40&v=4",
      "owner": "riadibadulla",
      "repo_name": "SmartRedBox",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2023-07-26T16:15:57Z",
      "updated_at": "2023-07-31T18:38:06Z",
      "topics": [],
      "readme": "# SmartRedBox"
    },
    {
      "name": "BobMerkus/ADS-LLM-QA",
      "stars": 3,
      "img": "https://avatars.githubusercontent.com/u/23738320?s=40&v=4",
      "owner": "BobMerkus",
      "repo_name": "ADS-LLM-QA",
      "description": "An assessment of Zero-Shot Open Book Question Answering using Large Language Models",
      "homepage": "",
      "language": "Python",
      "created_at": "2023-07-01T12:26:57Z",
      "updated_at": "2024-08-16T05:50:46Z",
      "topics": [],
      "readme": "# Thesis ADS-LLM-QA\n\n```This repository contains the implementation of the Thesis for the Advanced Data Science (ADS) Master at the University of Amsterdam (UvA). The thesis is titled: \"Question Answering for Legal Documents using Open-Domain Question Answering\". The thesis is written by: `J. van der Heijden` and supervised by: `Dr. M. de Rijke` and `Dr. M. Cochez`.```\n\nThe text above was generated by GitHub Copilot and is a good example of a generative Large Language Model (LLM) that is hallucinating. The goal of my thesis is to investigate the use of LLMs in an Open-Domain Question Answering (ODQA) setting.\n\n## Recommended Installation\nWe use the Windows Subsystem for Linux (WSL 2).\n1. Setup Python Environment \n    - Conda: `conda env create -f environment.yml`\n    - pip: `pip install -r requirements.txt`\n2. Setup Haystack Backend (ElasticSearch/FAISS). Start the ElasticSearch document store with a single node using `launch_es()` from the `haystack.utils` module Warning: Running an elasticsearch instance on WSL is not recommended. It is recommended to run elasticsearch on a separate machine (e.g., a virtual machine or a cloud instance). Running locally (with multiple nodes) requires a lot of memory. If you decide to anyway, a local cluster (with 3 nodes) can be run using `docker compose up -d` in a terminal and make sure a valid certificate is contained in `./data/ca.crt` after setup. When encountering the error `max virtual memory areas vm.max_map_count [65530] is too low, increase to at least [262144]`, one should run: `sysctl -w vm.max_map_count=262144`.\n3. Create document store, write the documents (and generate embeddings if using a Neural retriever) + evaluation labels.\n4. Run inference for multiple models (Extractive + Generative). Models required for the analysis are not included in this repository, but are downloaded automatically through the Huggingface `transformers` library. \n\nOptionally: \n- Setup OPEN_AI_KEY in `./data/OPEN_AI_KEY.txt` if you want to use Open AI models via the API.\n\n## Main\n`main.py` is run for the actual Thesis analysis, while runtime settings can be set with `main.yml`. It evaluates multiple Haystack pipelines against the AWS dataset. It contains the following functions:\n- `document_import`: Combines `utils.aws.aws_docs_documents`, `utils.kubernetes.kubernetes_documents` & `utils.stackexchange.stackexchange_documents`. The functions convert datasets to the `haystack.Document` format. \n- `execute_pipeline`: Executes a pipeline with a specific retriever + model (reader/generator) and saves the results to disk.\n- `evaluate_answers`: Evaluates a list of answers against a list of labels.\n- `evaluate_pipeline`: Evaluates a pipeline with a specific retriever + model and saves the results to disk.\n- `main`: Main function that runs the analysis. It imports the configuration from `main.yml` and optionally overwrites some arguments using the command line (run `python main.py -h` for information).\nIt starts by importing the documents + evaluation labels and then runs the `evaluate_pipeline` function for each pipeline.\n- `import_results`: Reads the contents of all runtime results (configuration, pipeline output, evaluation metrics). Can be used for further analysis of the results.\n\n### Models\n`FARMReader` or `TransformerReader`: Discussed in [https://docs.haystack.deepset.ai/docs/reader#deeper-dive-farm-vs-transformers] and [https://github.com/deepset-ai/haystack/issues/248#issuecomment-661977237]\nWe use the `TransformerReader` for extractive models and `PromptNode` + `PromptTemplate` (LFQA) for generative models.\n\n## Utils module\nCustom module containing utilities.\n\n### Natural Language Processing\nThe module `utils.nlp.py`:\n- `hash_md5`: Hash a string using the MD5 alrogithm\n- `match`: Regex matching for URLs, Phone Numbers, Etc.\n- `normalize_answer`: Normalize a answer for NLP evaluation. Sourced from SQuAD V2 script: [https://github.com/white127/SQUAD-2.0-bidaf/blob/master/evaluate-v2.0.py]\n\n### Haystack Pre Processing\nThe file `utils.haystack_pre_processing.py` contains 2 functions for processing file formats to the `haystack.Document` format, which is needed for writing to the document store.\n- `soup_to_documents`: Convert a `bs4.BeautifulSoup` object to list of `haystack.Document`. Attempts to extract tables and convert them to key:value pairs in comma delimited fashion (instead of default parsing with newlines).\n- `markdown_to_documents`: Convert a `markdown.markdown` object to list of  `haystack.Document`. Wraps around the above `soup_to_documents` function.\n\n### Amazon Web Services (AWS)\nCustom module containing the AWS dataset in `utils.aws.py` as published on [https://github.com/siagholami/aws-documentation].\n- `aws_docs_import_qa`: Import the original QA dataset from the AWS dataset\n- `aws_docs_files`: Import the file paths + file names from the AWS dataset\n- `aws_docs_documents`: Import the AWS dataset using the `haystack.Document` format. This contains the raw text + meta data\n- `aws_docs_labels`: Import the AWS dataset using the `haystack.Label` format. This contains the question + answer + document and is used for evaluation.\n\n### Webscraper\nCustom webscraper for obtaining data from web pages in `utils.webscraper.py`:\n- `get_proxies_from_file`: import proxies from .txt file\n- `get`: wrapper for `requests.get` that uses proxies + local clone \n- `webcrawl`: webcrawler that uses the `get` function to recursively webscrape a specific (sub) domain. Was used for Kubernetes documentation + blog \n- `parse_pdf`: parse a pdf to raw text\n\n### Kubernetes\nThe `utils.kubernets.py` is used for importing kubernetes related data.\n- `kubernetes_documents` converts a local .json file to a list of `haystack.Document`\n\n## User Interface\n\n### Gradio \nStreamlit application of chatbot in `app_gradio.py`. Run with `pytho app_gradio.py`."
    },
    {
      "name": "kevin-pek/document-semantic-search",
      "stars": 3,
      "img": "https://avatars.githubusercontent.com/u/76090938?s=40&v=4",
      "owner": "kevin-pek",
      "repo_name": "document-semantic-search",
      "description": null,
      "homepage": "",
      "language": "Python",
      "created_at": "2023-06-07T04:29:32Z",
      "updated_at": "2024-06-21T11:20:15Z",
      "topics": [
        "bm25",
        "faiss",
        "gradio",
        "langchain",
        "python",
        "similarity-search"
      ],
      "readme": "# Document Semantic Search\n\nA simple Gradio interface for semantic search across multiple PDF documents using a combination of BM25 and vector embeddings to find relevant documents. The script builds a FAISS index on corpus of the uploaded documents, and first uses BM25 to find the top relevant results, then reranks them using cosine similarity to the search query.\n\n## Setup\n\n[Link to venv docs](https://docs.python.org/3/library/venv.html)\n\n### Create environment\n\n```shell\npython3 -m venv venv\n```\n\n### To activate the environment\n\nUNIX/MacOS:\n\n```shell\nsource venv/bin/activate\n```\n\nWindows:\n\n```shell\nvenv/Scripts/activate\n```\n\n### Install dependencies\n\nIf this is your first time running this or the package dependencies have changed, run this command to install all dependencies.\n\n```shell\npip install -r requirements.txt\n```\n\n## Run\n\nRun the app in reload mode with this command. This will let the app reload automatically when changes are made to the python script.\n\n```shell\npython main.py\n```\n"
    },
    {
      "name": "ekimetrics/climategpt",
      "stars": 3,
      "img": "https://avatars.githubusercontent.com/u/2689898?s=40&v=4",
      "owner": "ekimetrics",
      "repo_name": "climategpt",
      "description": "Question Answering engine based only on IPCC and other scientific reports about climate change",
      "homepage": null,
      "language": "Python",
      "created_at": "2023-02-28T10:52:11Z",
      "updated_at": "2024-10-24T21:56:03Z",
      "topics": [],
      "readme": "# climategpt\nQuestion Answering engine based only on IPCC and other scientific reports about climate change\n"
    },
    {
      "name": "Rami-Ismael/UTD-chat-bot",
      "stars": 3,
      "img": "https://avatars.githubusercontent.com/u/39520903?s=40&v=4",
      "owner": "Rami-Ismael",
      "repo_name": "UTD-chat-bot",
      "description": null,
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2023-01-24T19:50:19Z",
      "updated_at": "2024-01-25T05:37:46Z",
      "topics": [],
      "readme": "# Requirements\n\nTo install requirements:\n\n```setup\npip install -r requirements.txt\n```"
    },
    {
      "name": "jonas-nothnagel/SDSN-tool",
      "stars": 3,
      "img": "https://avatars.githubusercontent.com/u/41116329?s=40&v=4",
      "owner": "jonas-nothnagel",
      "repo_name": "SDSN-tool",
      "description": "Development branch of Policy Tracking tool -  Accepted submission to UN World Data Forum 2023",
      "homepage": "",
      "language": "Python",
      "created_at": "2023-02-15T12:16:28Z",
      "updated_at": "2024-09-05T13:12:50Z",
      "topics": [],
      "readme": "---\ntitle: SDSN Demo\nemoji: 📈\ncolorFrom: purple\ncolorTo: blue\nsdk: streamlit\nsdk_version: 1.10.0\napp_file: app.py\npinned: false\n---\nDevelopment Repository containing my personal development branches of SDSN-tool built for GIZ Data Lab. The SDSN tool was sucessfully submitted and accepted at the [UN World Data Forum 2023](https://unstats.un.org/unsd/undataforum/index.html)\n\nCheck out the configuration reference at https://huggingface.co/docs/hub/spaces-config-reference\n\n\n\n"
    },
    {
      "name": "prateekralhan/Haystack-based-QA-system",
      "stars": 3,
      "img": "https://avatars.githubusercontent.com/u/29462447?s=40&v=4",
      "owner": "prateekralhan",
      "repo_name": "Haystack-based-QA-system",
      "description": "A simple Question Answering system built on a corpus of documents of different formats using Haystack and Streamlit",
      "homepage": "",
      "language": "Python",
      "created_at": "2022-11-14T20:21:44Z",
      "updated_at": "2024-12-25T15:19:19Z",
      "topics": [
        "haystack",
        "opensourceforgood",
        "python-3",
        "question-answering",
        "streamlit-webapp"
      ],
      "readme": "# ✨ Haystack based QA system 🚀 [![Project Status: Active](https://www.repostatus.org/badges/latest/active.svg)](https://www.repostatus.org/#active) [![](https://img.shields.io/badge/Prateek-Ralhan-brightgreen.svg?colorB=ff0000)](https://prateekralhan.github.io/)\nA simple Question Answering system built on a corpus of documents of different formats using Haystack and Streamlit\n\n![demo](https://user-images.githubusercontent.com/29462447/202132911-1c761b9f-b19f-463a-9ac5-927776e207c1.gif)\n\n\n## Installation:\n* Simply run the command ***pip install -r requirements.txt*** to install the dependencies.\n\nIf you run into any issues with installation of haystack, please refer [this.](https://github.com/deepset-ai/haystack)\n\n\n## Usage:\n1. Clone this repository and install the dependencies as mentioned above.\n2. Place your documents in the **docs** folder and simply run the following command in order to do bulk conversion of the documents to plain text: \n```\npython data.py\n```\n![1](https://user-images.githubusercontent.com/29462447/202132627-bbe70c9a-b0f3-43b4-9f06-6d35fab28ef0.png)\n\n3. We will convert the text documents into the haystack supported format and apply Preprocessor to clean and split the document into sensible units. We will store these preprocessed texts in a SQL document store. Run the following command to perform the indexing:\n```\npython index_pipeline.py\n```\n![2](https://user-images.githubusercontent.com/29462447/202132735-eb509cf1-3608-4667-bfb3-0bfb658a21f5.png)\n\n\n4. We will download our reader (a pre-trained transformer model on QA task) and also initialize our retriever to search top k relevant documents in document store.For a given question, the retriever will search for the top ‘k’ documents relevant to the question and reader will predict answers using those ‘k’ documents instead of searching the whole document store. In order to test this, you can run the following script:\n```\npython search_pipeline.py\n```\n![3](https://user-images.githubusercontent.com/29462447/202132813-db68111c-7f62-499f-9e5c-fe5c672bed05.png)\n\n\n## Web App\nHere is our web app built using streamlit which is compatible with haystack and also it is easy to use. You can run the app by:\n```\nstreamlit run app.py\n```\n\n![4](https://user-images.githubusercontent.com/29462447/202132497-256d3b05-70d1-40e0-96f8-6cb35a598772.png)\n\n![5](https://user-images.githubusercontent.com/29462447/202132512-7397f209-0cf4-4412-b3b1-95d8250e420d.png)\n\n\n\n### Running the Dockerized App\n1. Ensure you have Docker Installed and Setup in your OS (Windows/Mac/Linux). For detailed Instructions, please refer [this.](https://docs.docker.com/engine/install/)\n2. Navigate to the folder where you have cloned this repository ( where the ***Dockerfile*** is present ).\n3. Build the Docker Image (don't forget the dot!! :smile: ): \n```\ndocker build -f Dockerfile -t app:latest .\n```\n4. Run the docker:\n```\ndocker run -p 8501:8501 app:latest\n```\n\nThis will launch the dockerized app. Navigate to ***http://localhost:8501/*** in your browser to have a look at your application. You can check the status of your all available running dockers by:\n```\ndocker ps\n```\n"
    },
    {
      "name": "ugm2/neural-search-demo",
      "stars": 3,
      "img": "https://avatars.githubusercontent.com/u/25923343?s=40&v=4",
      "owner": "ugm2",
      "repo_name": "neural-search-demo",
      "description": "Huggingface app neural search demo",
      "homepage": null,
      "language": "Python",
      "created_at": "2022-09-04T14:52:00Z",
      "updated_at": "2025-01-27T12:28:40Z",
      "topics": [],
      "readme": "---\ntitle: Neural Search Demo\nemoji: 🧠🔎\ncolorFrom: blue\ncolorTo: green\nsdk: docker\napp_port: 8501\npinned: true\n---\n\n## 🧠 Neural Search Demo 🔎\n\nThis is a tool to allow indexing & search content using neural capabilities using [Haystack](https://haystack.deepset.ai/overview/intro) open-source framework.\n\nYou can see the demo working [here](https://huggingface.co/spaces/ugaray96/neural-search).\n"
    },
    {
      "name": "UCLComputerScience/COMP0016_2020_21_Team8",
      "stars": 3,
      "img": "https://avatars.githubusercontent.com/u/26434653?s=40&v=4",
      "owner": "UCLComputerScience",
      "repo_name": "COMP0016_2020_21_Team8",
      "description": "UCLComputerScience COMP0016 2020-2021 Team8 Project with Avanade ",
      "homepage": "",
      "language": "JavaScript",
      "created_at": "2020-10-12T11:07:34Z",
      "updated_at": "2023-02-21T19:58:23Z",
      "topics": [
        "microsoft-azure",
        "microsoft-bot-framework",
        "nodejs",
        "python"
      ],
      "readme": "# COMP0016_2020_21_Team8 - AvaBot\n\n## Overview\n\nProject Title: Avanade2 COVID19 changes</br>\nTeam Member: Zihan Zhu, Chaozy Zhu, Davit Mirzoyan\n\n## Project Intro\n\nDue to the COVID-19 epidemic, most companies have switched to remote working, however, productivity and communication take a hit when employees are new to working from home.<br>\nAvaBot is therefore created to ease the pain, it is a ChatBot assistant that holds rich knowledge base to answer employees' questions about the company's newly induced tools and policies for remote working.<br>\nIt is also endowed with AI functionalities, including natural language processing and image recognition, to help employees with documents: to read, analyze, and understand documents for them and thus enhance their productivity.\n\n## System Architecture\n\nAvaBot application system consists of several components. <br>\nUsers can interact with AvaBot as a REST API on web chat and on multiple applications to which it is channeled.<br>\nThe bot source code is deployed to Azure Cloud by using Azure Bot Service. The bot src can be found [./AvabotTeam8-src/](./AvabotTeam8-src).\n<br>\nAvaBot is connected to a QnA Maker with rich knowledge base that allows it to answer a variety of questions. The QnA Maker is configured in [./AvabotTeam8-src/dialogs/answerDialog.js](./AvabotTeam8-src/dialogs/answerDialog.js).<br>\nDocument processing functionalities are integrated to AvaBot by APIs. The code of the APIs are in [./textSum/](./textSum), [./QAsystem/](./QAsystem), and [./formRecogFunction/](./formRecogFunction). <br>\nDue to the high complexity and dependency on environment, QA system has been placed to a virtual machine in order to optimize its performance.<br><br>\nBelow is the system architecture diagram for AvaBot:\n![System Architecture Diagram](./docs/arch.png)\n<br><br>Below is the dialog flow diagram for AvaBot:\n![Flow Diagram](./docs/dialog1.png)\n\n### <br><br>Project tree\n\nBelow is the project tree showing the main files and their uses in the repository:\n\n```\n.\n├── AvabotTeam8-src/       # The bot source code\n│   ├── .vscode            # VS Code configuration for bot development\n│   ├── bots/\n│       └── avabot.js      # The bot class\n│   ├── coverage/          # Testing coverage report\n│   └── dialogs/           # The bot's dialog classes\n│       └── mainDialog.js\n│       └── ...\n│   ├── node_modules/      # Node module dependencies for the project\n│   ├── PostDeployScripts/ # Bot Deployment Scripts for setting continuos deployment\n│   ├── test/              # Unit tests and integration tests for the bot\n│   ├── .env               # environment file for connecting to azure services\n│   ├── index.js           # App entry point\n│   ├── package.json       # Node.js package configuration file\n│   ├── web.config         # Configuration file if using iisnode behind Express\n│   └── ...\n├── docs/                  # Documentation files\n├── formRecogFunction/     # Src and documents for Form Recognizer API\n├── QAsystem/              # Src and documents for QA system API\n├── testAPI_example/       # Demo for calling APIs in python\n├── textSum/               # Src and documents for Text Summarization API\n├── WebBrowserBot/         # Demo for running AvaBot on a web browser\n├── AvaBot.bot             # Bot configuration file for running the bot on MS bot emulator\n├── LICENCE                # License for the project\n└── *.md                   # Project documentation files\n```\n\n## Deployment\n\n- See AvaBot Deployment Manual [here](./AvabotTeam8-src/README.md)\n- See how to use the APIs for [Text Summarization](./textSum/README.md), [QA System](./QAsystem/README.md) and [Form Recognizer](./formRecogFunction/README.md)\n\n## Usage\n\nAvaBot is hosted on the Azure Bot Service. The service defines a REST API and an activity protocol for how bots and channels or users can interact. AvaBot's messaging endpoint is at `https://avabotteam8.azurewebsites.net/api/messages`.<br>\nDemo for using the bot on a web browser can be found in [./WebBrowserBot/](./WebBrowserBot).\n\n## ShowCase\n\n### 1. AvaBot greets the user:<br><br>\n\n![Sample one](./docs/sample1.png)<br>\n\n### <br><br>2. AvaBot summarizes a document:<br><br>\n\n![Sample two](./docs/sample4.png)<br>\n\n### <br><br>3. AvaBot recognizes an image:<br><br>\n\n![Sample three](./docs/sample5.png)\n\n## Appendix\n\n- [Development Blog](https://chaozyhaha.wordpress.com/blog/)\n- [Project Website](http://students.cs.ucl.ac.uk/2020/group8/index.html)\n- [User Manual](http://students.cs.ucl.ac.uk/2020/group8/usermn.html)\n"
    },
    {
      "name": "Hisarlik/simpleTextCLEF",
      "stars": 3,
      "img": "https://avatars.githubusercontent.com/u/1486603?s=40&v=4",
      "owner": "Hisarlik",
      "repo_name": "simpleTextCLEF",
      "description": "This software was developed for the CLEF 2022 Text Simplification task.",
      "homepage": "",
      "language": "Jupyter Notebook",
      "created_at": "2022-02-02T10:42:47Z",
      "updated_at": "2022-12-05T15:03:05Z",
      "topics": [],
      "readme": "# simpleTextCLEF\n\nThis software was developed for the CLEF 2022 Text Simplification task.\n\nOur work uses the transfer learning capabilities of the\nT5 pre-trained language model, adding a method to control specific simplification features. We\npresent a new feature based on masked tokens prediction (Language Model Fill-Mask) to\ncontrol the lexical complexity of the text generation process. The results obtained with the\nSARI metric are at the same level as previous work in other domains for sentence\nsimplification.\n\nSteps to replicate the results: \n\n1. Clone this repository\n2. Install dependencies:\n<pre><code>pip install -r requirements.txt</code></pre>\n3. For training purpose:\n\nSelect hyperparameters in T5_train.py \n<pre><code>python scripts/T5_train.py</code></pre>\n4. Optimization:\n\nSelect experiment_id, dataset and trials in optimization.py\n<pre><code>python scripts/optimization.py</code></pre>\n5. For test purpose:\n\nSelect experiment_id and dataset in T5_evaluate.py\n<pre><code>python scripts/T5_evaluate.py</code></pre>\n\nSame for larger version. Be carefull with memory issues. \n\n\n\n# Data\n\nDownload the dataset from https://simpletext-project.com/2022/clef/en/tasks. It's necessary to preprocessed the raw data using 1.Preprocessing dataset Task 3.ipynb."
    },
    {
      "name": "byukan/bookrec",
      "stars": 3,
      "img": "https://avatars.githubusercontent.com/u/12012907?s=40&v=4",
      "owner": "byukan",
      "repo_name": "bookrec",
      "description": "BookRec is a chatbot that recommends books based on the conversation you're having with it.",
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2021-09-17T08:22:36Z",
      "updated_at": "2024-05-19T19:00:32Z",
      "topics": [],
      "readme": "[![Commit rate](https://img.shields.io/github/commit-activity/m/byukan/bookrec?label=Commits)](https://github.com/byukan/bookrec/commits/master)\n[![License](https://warehouse-camo.ingress.cmh1.psfhosted.org/110fcca60a43a8ea37b1a5bda616e639325f2f30/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d425344253230332d2d436c617573652d626c75652e737667)](https://github.com/byukan/bookrec/blob/main/LICENSE)\n\n\n*** \n\n<h1 align=\"center\">\n<sub>\n<img  src=\"https://3.bp.blogspot.com/_RzmIlSwLsTQ/TKK8MUJ1QgI/AAAAAAAAAYg/Xp7GM8hspiw/s1600/BBWrobot.png\" height=\"70\" width=\"40\">\n</sub>\n BookRec \n<sub>\n<img  src=\"https://3.bp.blogspot.com/_RzmIlSwLsTQ/TKK8MUJ1QgI/AAAAAAAAAYg/Xp7GM8hspiw/s1600/BBWrobot.png\" height=\"70\" width=\"40\">\n</sub>\n\n</h1>\n<p align=\"center\">\n<sup> <!-- Pronounciation -->\n      BookRec is a chatbot that recommends books based on the conversation you're having with it.\n</sup>\n<br>\n</p>\n\n***\n\n* [Purpose](#purpose)\n* [Motivation](#motivation)\n* [Functional Specification](#functional-specification)\n  * [Dataset](#dataset)\n  * [Models](#models)\n  * [Cloud Compute](#cloud-compute)\n  - [Deployment](#deployment)\n* [Brant Yukan](https://brantyukan.com)\n\n## **Purpose**\n\nFrom a product perspective, this chatbot is conversational and is trained on book summaries and reviews.  It's designed to capture the human voice used when describing literature -- emotions and subjective thought, which we don't typically expect when talking to a computer.  It should feel like an entertaining, conversational concierge, not customer support/service.  It'll recognize named entities and also words you use to describe the \"mood\" of a novel.  A feature could be to show snippets from the book summary or reviews, and perhaps even the content.\n\n## **Motivation**\n\nI want to build a chatbot that uses advanced NLP and deep learning techniques.  It should be intelligent, not feel like a phone tree or canned responses.  I'd like to go beyond uninteresting techniques like cosine similarity on tfidf vectors.  A modern dialog system uses pattern matching, grounding, search, and text generation.\n\n\n\n## Functional Specification\n\n_The vision for this project is a messaging interface that can be open on a browser.  The user can start typing free-form, send texts to the chatbot, which replies with conversationl responses which could be follow up questions, related statements about books, or excerpts._\t\n\n#### Dataset\n- The data used will be a corpus of product info and reviews pages scraped from *Amazon*.\n- The _html_ pages will need to be parsed using BeautifulSoup.\n- I'll load all the data into a MongoDB database.  We'll have book metadata, product summary, reviews, and the content itself.\n#### Models\n- Try to spend the bulk of the time here on model building and learning about advanced techniques.\n- Learn about and apply various advanced NLP and deep learning techniques that would be useful for a conversational chatbot.\n- Modern approaches combine:\n    - pattern matching\n    - grounding (logical knowledge graphs and inference)\n    - search\n    - generative\n- Read _NLPIA_ and apply those examples.\n#### Cloud Compute\n- I'll likely need to rent a compute instance for a GPU to train deep learning models.  Possible options are to see if I can find a good source of AWS free credits.  GCP gives $300 to any new account.  There might be other providers that are free or really cheap, I can look at the guides on fast.ai.\n- I expect the data to be comfortably under 50GB.  Either way, if we're eventually going to train on cloud, the data will also have to be there, too.\n- Get a barbones model working on a minimal dataset first before spending credits on training and accuracy.\n#### Deployment\n- If convenient, use a messenger api, like Slack or Facebook Messenger.\n- Ideally, host a simple webpage that has a chatbox where you can type.\n\n\n## Milestones\n1. Consolidate all data in a chosen database.\n1. Run prototypes of a few chatbot frameworks in jupyter notebooks.  (favor using aichat from nlpia)\n- part 2 in NLPIA for deep learnign concepts\n- part 3 in NLPIA for actually building a chatbot and advanced NLP\n1. Set up a GPU to use.  Perhaps going to be GCP credits unless I find something better.\n1. Either set up a simple web interface, pull an example template with flask perhaps.  Or integrate with a messaging api.\n\n## License\n\n[BSD 3-Clause License](https://github.com/byukan/bookrec/blob/main/LICENSE.txt).\n"
    },
    {
      "name": "daniel-gomm/document-corpus-generation-pipelines",
      "stars": 3,
      "img": "https://avatars.githubusercontent.com/u/63717948?s=40&v=4",
      "owner": "daniel-gomm",
      "repo_name": "document-corpus-generation-pipelines",
      "description": "Document Corpus Generation Pipelines are designed to streamline the preparation and maintenance steps when dealing with a corpus of documents as basis for information retrieval.",
      "homepage": "",
      "language": "Jupyter Notebook",
      "created_at": "2021-05-03T13:10:57Z",
      "updated_at": "2022-10-27T10:47:27Z",
      "topics": [],
      "readme": "# Document Corpus Generation Pipelines - SciQUACK Document Corpus Creation\n\nThis repository contains the code needed to recreate the SciQUACK document corpus and, more generally, to create performant processing pipelines for scientific publications for the creation of a document corpus usable by a [Haystack](https://haystack.deepset.ai/) based Question Answering (QA) System.\n\n__Contents:__\n\n1. Setup\n2. Concepts of Document Corpus Generation Pipelines\n3. Creating a pipeline\n4. Recreating the SciQUACK document corpus\n## 1. Setup\nTo use the functionalities of the document corpus generation pipelines (dcgp) package it has to be installed using the python package installer pip. To install the package clone the repository and execute the following command from the root folder of this repository:\n\n```\npip install ./dcgp\n```\nIf you want to add new components to the package or modify the code in another way you can install the package in editable mode:\n\n```\npip install -e ./dcgp\n```\n\nWhen you have installed the package you can import the package into your python projects:\n```python\nimport dcgp\n```\n\n## 2. Concepts of Document Corpus Generation Pipelines\nDocuments are processed in a pipeline. A pipeline can be represented by an Directed Acyclic Graph. A pipeline is comprised of up to 3 different kinds of pipeline elements. Each pipeline starts with a single pipeline element, a so called Adapter, which feeds data into the pipeline. Next an arbitrary number of so called Processors that modify documents in a predefined way. At the end of a pipeline there are one or more so called Sinks that forward the documents to their desired destination.\n\n### 2.1. Adapters\nAdapters feed data into the pipeline. The adapters are specialized for a specific datasource (e.g., plain txt file parses of publications in a folder, documents residing in an elasticsearch documentstore,...). The idea is that the adapter transforms the input to a generalized format (dictionary of text and metadata), independent of the form of the input data source.\n\n### 2.2. Processors\nProcessors transform documents that pass them. There is a variety of predefined processors of the following categories:\n\n- Splitting Preprocessors\n- Metadata Processors that add, remove or modify metadata\n- Text Processors that modify the text\n- Filter Processors that conditionally discard documents\n- Specialized processors that perform complex activities like entity recognition or transformer based classification\n\n### 2.3. Sinks\nSinks direct documents to their desired destination. There are sinks that interact with documentstores (elasticsearch, milvus) and others that interact with the filesystem.\n\n### 2.4. Pipelines\nAs introduced in the introduction of this chapter pipelines are the core concept of dcgp that combined Adapters, Processors and Sinks. A pipeline is defined sequentially and can be extensively configured. The pipeline handles processing execution, coordinates the different components and provides an easily accessible interface for users.\n\n## 3. Creating a pipeline\nDefining a pipeline consists of two actions: Configuring general parameters of the pipeline and building the pipeline by adding pipeline elements to it.\n\n### 3.1. Configuring general parameters\nThere are two main parameters that can be modified:\n\n1. Whether the pipeline should run in batch mode (processing a batch of documents at each pipeline element at once) or not (processing a single document through the entire pipeline before processing the next document)\n2. How many processing cores to use in parallel. *Note: Only the processors are executed in parallel, not the adapter or the sinks to avoid concurrency related issues in reading and writing*\n\nAn exemplary configuration with a batch size of 200 documents and using 10 cpu cores in parallel looks the following:\n```python\nadapter = Adapters.UnarxiveAdapter(\"../sample\")\npipeline = Pipeline(adapter, batch_mode=True, batch_size=200, cpus=10)\n```\n\n### 3.2. Building a pipeline\nBuilding a pipeline starts with defining the general pipeline settings and its Adapter, as described in Section 3.1.\n\nNext a number of processors are added:\n```python\npipeline.add_processor(Processors.TextKeywordCut(\"Introduction\"))\n\npipeline.add_processor(Processors.TextReplaceFilter(r\"\\{{(.*?)\\}}\", \"\"))\n\npipeline.add_processor(Processors.TextSentenceDiscardNonAlpha(0.5))\n```\n\nFinally at least one Sink is added to the pipeline:\n```python\npipeline.add_sink(Sinks.TextfileSink(\"./sampleoutput\", [\"arixiv-id\", \"_split_id\"]))\n```\n\nThis fully defines the pipeline. Processing can be started thereafter:\n\n```python\npipeline.process()\n```\n\n## 4. Recreating the SciQUACK document corpus\nTo recreate the SciQUACK document corpus you need some additional data:\n1. The unarXive dataset that is available on [Zenodo](https://zenodo.org/record/4313164)\n2. The arXiv metadata dataset available from [Kaggle](https://www.kaggle.com/Cornell-University/arxiv)\n3. The pretrained transformer model for the IMRaD classification task from [Dropbox](https://www.dropbox.com/sh/ze4iupdx78a7ac5/AAAYfvd0XOM87PJuERnKpoiEa?dl=0)\n\nWhen you have all the additional files you need to prepare the corpus generation and to execute it.\n\n### 4.1. Preparing the corpus generation\nThe pipeline for the corpus generation is defined in the [generate_sciquack_corpus.py](./scripts/generate_sciquack_corpus.py) script. The location of the datasets, models, etc. mentioned in the following need to be modified in this file.\n\n#### 4.1.1. Prepare the unarXive dataset\nExtract the dataset to a folder of your choosing. The relevant folder is called papers and includes full text parses of the papers contained in the unarXive dataset.\n#### 4.1.2. Prepare the arXiv metadata\nThe arXiv metadata is provided in the form of a RESTful disk-based database. [This implementation](https://github.com/daniel-gomm/minimal_RESTful_rocksdb) of a minimalistic RocksDB based database is used.\n\nAn instance of this database can be started in a docker container using docker-compose. From the repositories root folder execute:\n```\ndocker-compose -f ./corpus_generation/rocksDB/docker-compose.yml up -d\n```\nTo stop the database run:\n```\ndocker-compose -f ./corpus_generation/rocksDB/docker-compose.yml down\n```\nBefore running the corpus generation you need to add the metadata to the database once. To do this you can use the jupyter notebook [json_to_rocksdb.ipynb](./notebooks/preprocessing/json_to_rocksdb.ipynb). *Note: Make sure to adjust the locations of the metadata file and the url of the database*\n#### 4.1.3. Setup a documentstore\nYou need to setup your desired documentstore the documents should be saved to before starting the corpus generation. Haystack provides a [good overview](https://haystack.deepset.ai/usage/document-store) of the documentstores and how to spin up basic versions of them. For more elaborate instructions on [Elasticsearch](https://www.elastic.co/guide/en/elasticsearch/reference/6.8/docker.html) and [Milvus](https://milvus.io/docs/v1.1.1/milvus_docker-cpu.md) look at their respective websites.\n#### 4.1.4. Customize the pipeline code\nFinally you need to adjust the [code of the pipeline](./scripts/generate_sciquack_corpus.py) to match your configuration (in terms of hardware) and to adjust links to files, network locations etc.\n\n### 4.2. Generating the corpus\n\nWhen you have setup everything correctly you can generate the corpus by executing the following command from the root directory of this repository:\n```bash\ncd scripts\npython generate_sciquack_corpus.py\n```\n\nIf you have any questions feel free to reach out to us (daniel.gomm@student.kit.edu).\n"
    },
    {
      "name": "UrosOgrizovic/FitBot",
      "stars": 3,
      "img": "https://avatars.githubusercontent.com/u/25843402?s=40&v=4",
      "owner": "UrosOgrizovic",
      "repo_name": "FitBot",
      "description": "Chatbot that answers fitness-related questions.",
      "homepage": null,
      "language": "Python",
      "created_at": "2020-12-10T16:44:50Z",
      "updated_at": "2025-02-15T08:23:31Z",
      "topics": [],
      "readme": "# FitBot\nChatbot application that answers fitness-related questions.\n\nRepository contains web application that serves model and enables user to do inference with it through easy to use web interface. It also contains scripts that were used for fast prototyping of reader-retriever QA architecture.\n\nTo see more details about the implementation, see [this report](report.md).\n\n# Installing requirements\nIf running on Windows, do `pip install -r requirements.txt`.\n\n### Authors\nSava Katić, R2-24/2020\nUroš Ogrizović R2-1/2020\n"
    },
    {
      "name": "MesumRaza/qa_quran_heroku",
      "stars": 3,
      "img": "https://avatars.githubusercontent.com/u/22402962?s=40&v=4",
      "owner": "MesumRaza",
      "repo_name": "qa_quran_heroku",
      "description": "A Question Answer Application Backend (Haystack and Hugging Face)",
      "homepage": null,
      "language": "Python",
      "created_at": "2021-01-30T20:57:48Z",
      "updated_at": "2021-02-06T10:54:04Z",
      "topics": [],
      "readme": "# QUESTION AND ANSWERS FROM QURAN\n\nUSING STATE OF THE ART HUGGING FACE NLP PRE-TRAINED MODELS & HAYSTACK PIPELINE TO DELIVER Q&A ENGINE\n\nMode Used: deepset/minilm-uncased-squad2\n\nModel Disclaimer: The model is only used to show case Q&A System and shall contain a lot error from Model Perspective and Small Data Trained Perspective. The output of model doesn't validate any belief of the model creator or application developer.\n\nVisit App: MLM Minmal used to just deploy on Heroku and show concept. (The out shall be incorrect but faster than other models)\n\nhttps://mhemani-qa-quran.herokuapp.com/\n"
    },
    {
      "name": "venuraja79/qa-api",
      "stars": 3,
      "img": "https://avatars.githubusercontent.com/u/10090460?s=40&v=4",
      "owner": "venuraja79",
      "repo_name": "qa-api",
      "description": "Question Answering API Experiments",
      "homepage": null,
      "language": "Python",
      "created_at": "2020-07-04T14:59:51Z",
      "updated_at": "2021-05-20T11:54:36Z",
      "topics": [],
      "readme": "# qa-api\nQuestion Answering API Experiments\n\nReduced version of API influenced from Haystack to deploy in openshift\n"
    },
    {
      "name": "amphilagus/Haystack-RAG_Assistant",
      "stars": 3,
      "img": "https://avatars.githubusercontent.com/u/82392488?s=40&v=4",
      "owner": "amphilagus",
      "repo_name": "Haystack-RAG_Assistant",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2025-04-14T05:07:46Z",
      "updated_at": "2025-04-22T11:20:46Z",
      "topics": [],
      "readme": "# Haystack RAG Assistant\n\n基于Haystack框架的本地知识库检索增强生成系统，支持中英文文档检索和问答。\n\n## 功能特点\n\n- **文档导入**：支持PDF、TXT、DOCX、MD、HTML等多种格式文档导入，递归扫描所有子目录\n- **多语言支持**：集成多种嵌入模型，包括专为中文优化的BAAI/bge系列模型\n- **持久化存储**：基于ChromaDB的向量数据库，保存文档嵌入和元数据\n- **集合管理**：支持创建和管理多个知识库集合，适用于不同主题或项目\n- **命令行和Web界面**：同时提供CLI和用户友好的Web界面\n- **先进语言模型**：支持OpenAI最新的GPT-4o、GPT-4o-mini等模型\n- **自动嵌入模型匹配**：自动记录集合使用的嵌入模型，确保检索一致性\n- **多层级文档组织**：支持递归扫描和加载多级目录结构中的文档\n\n## 安装指南\n\n### 前提条件\n\n- Python 3.8+\n- 有效的OpenAI API密钥\n- [uv](https://github.com/astral-sh/uv) 包管理器 (可选但推荐)\n\n### 安装步骤\n\n1. 克隆仓库：\n\n```bash\ngit clone https://github.com/yourusername/haystack-rag.git\ncd haystack-rag\n```\n\n2. 使用uv创建并激活虚拟环境：\n\n```bash\n# 安装uv (如果尚未安装)\n# Linux/Mac\ncurl -fsSL https://astral.sh/uv/install.sh | bash\n# Windows\ncurl -fsSL https://astral.sh/uv/install.ps1 -o install.ps1; .\\install.ps1\n\n# 创建并激活虚拟环境\nuv init .\nuv venv\n# Linux/Mac\nsource .venv/bin/activate\n# Windows PowerShell\n.\\.venv\\Scripts\\Activate.ps1\n```\n\n3. 使用uv安装依赖：\n\n```bash\n# 安装项目依赖\nuv pip install -r requirements.txt\n\n# 安装mcp和httpx\nuv add mcp[cli] httpx\n```\n\n4. 配置API密钥：\n   - 在项目根目录创建`.env`文件\n   - 添加您的OpenAI API密钥：`OPENAI_API_KEY=your_api_key_here`\n\n## 使用说明\n\n### 命令行界面\n\n```bash\n# Windows PowerShell\n.\\run_cli.ps1\n\n# Linux/Mac\npython rag_assistant/main.py --interface cli\n```\n\n### Web界面\n\n```bash\n# Windows PowerShell\ncd 项目根目录\npython rag_assistant/main.py --interface web\n\n# 或使用streamlit直接运行\nstreamlit run rag_assistant/web_app.py\n```\n\n### 添加文档\n\n1. 将文档放入`raw_data`目录（可以创建子目录组织文档）\n   - 系统会自动递归扫描所有子目录并加载其中的文档\n   - 支持的格式：PDF、TXT、DOCX、MD、HTML\n2. 通过CLI或Web界面选择导入文档\n3. 选择适合文档语言的嵌入模型（中文推荐使用BAAI/bge模型）\n\n### 集合管理\n\n- 使用不同名称的集合来组织不同主题的知识库\n- 可以随时重置或创建新集合\n- 系统会自动记录集合的嵌入模型信息，确保检索一致性\n\n## 项目结构\n\n```\nhaystack-rag/\n├── rag_assistant/       # 主要代码目录\n│   ├── collection_metadata.py  # 集合元数据管理\n│   ├── collection_utils.py     # 集合工具函数\n│   ├── custom_document_store.py # 自定义文档存储\n│   ├── document_loader.py      # 文档加载和处理\n│   ├── main.py                 # 主入口\n│   ├── rag_pipeline.py         # RAG管道实现\n│   └── web_app.py              # Web界面\n├── raw_data/            # 原始文档目录（需手动创建）\n├── chroma_db/           # ChromaDB数据存储（自动创建）\n├── collection_metadata.json  # 集合元数据（自动创建）\n├── requirements.txt     # 项目依赖\n├── run_cli.ps1          # Windows PowerShell启动脚本\n├── run_web.ps1          # Web界面启动脚本\n├── convert_pdf.ps1      # PDF转MD脚本\n└── .env                 # 环境变量（需手动创建）\n```\n\n## 注意事项\n\n- 首次运行Web界面时，需要初始化管道并加载文档\n- 选择嵌入模型时注意匹配文档语言（中文推荐BGE或M3E模型）\n- 大型文档集合的嵌入过程可能需要一些时间\n- 确保OpenAI API密钥有足够的额度\n\n## PDF转换工具\n\n项目内置了PDF到Markdown的转换工具，基于[Marker](https://github.com/VikParuchuri/marker)库，支持OCR功能。\n\n### 主要功能\n\n- 将PDF文档转换为格式化的Markdown文本\n- 支持多语言OCR识别扫描文档\n- 可自定义批处理大小以优化不同硬件的性能\n- 支持限制处理特定页面范围\n- 支持批量处理整个文件夹中的PDF文件\n- 支持并行处理多个PDF文件提高效率\n- **LLM增强**：使用语言模型提高转换质量（需要在.env文件中配置Google API密钥）\n\n### 使用方法\n\n```bash\n# 处理单个PDF文件\npython rag_assistant/pdf_to_markdown.py \"文档路径.pdf\" \"输出目录\" [选项]\n\n# 批量处理整个目录中的PDF文件\npython rag_assistant/pdf_to_markdown.py \"PDF文件夹路径\" \"输出目录\" --workers 4 [选项]\n\n# 使用LLM增强功能转换\npython rag_assistant/pdf_to_markdown.py \"文档路径.pdf\" \"输出目录\" --use_llm\n```\n\n#### 通用选项\n- `--batch_multiplier <值>`: 批处理大小倍数(默认: 2)，增加可提高速度但需要更多VRAM\n- `--langs \"<语言>\"`: OCR识别的语言，逗号分隔(例如 \"en,zh,fr\")\n- `--use_llm`: 使用语言模型增强，提高转换质量（需要Google API密钥）\n\n#### 单文件选项\n- `--max_pages <值>`: 最大处理页数，缺省则处理整个文档\n\n#### 批量处理选项\n- `--workers <值>`: 并行处理的PDF文件数(默认: 1)\n- `--max_files <值>`: 最多处理的PDF文件数，缺省则处理所有文件\n- `--min_length <值>`: 最小文本长度，低于此长度的PDF将被跳过(默认: 0)\n\n### PowerShell脚本使用方法\n\nWindows用户可以使用更便捷的PowerShell脚本：\n\n```powershell\n# 基本用法\n.\\convert_pdf.ps1 -InputPath \"文档.pdf\" -OutputDir \"输出目录\"\n\n# 使用LLM增强\n.\\convert_pdf.ps1 -InputPath \"文档.pdf\" -OutputDir \"输出目录\" -UseLLM\n\n# 批量处理目录\n.\\convert_pdf.ps1 -InputPath \"PDF文件夹\" -OutputDir \"输出目录\" -Workers 4 -UseLLM\n```\n\n### 性能提示\n\n1. 如果有大容量显存的GPU(8GB+)，可增加`batch_multiplier`值加速处理\n2. 使用OCR时，正确指定文档中实际包含的语言可提高准确性\n3. 多核心CPU或多GPU系统上建议增加`workers`值实现并行处理\n4. 对于主要包含图像的PDF，设置`min_length`参数可避免不必要的OCR处理\n5. 对于非常大的文档，考虑使用`max_pages`参数分批处理\n6. LLM增强功能可以显著提高转换质量，但会增加处理时间和API消耗\n\n## 许可证\n\nMIT \n"
    },
    {
      "name": "avnlp/dataloaders",
      "stars": 3,
      "img": "https://avatars.githubusercontent.com/u/123800833?s=40&v=4",
      "owner": "avnlp",
      "repo_name": "dataloaders",
      "description": "Dataloaders is a versatile library designed for processing and formatting datasets to support various Retrieval-Augmented Generation (RAG) pipelines, facilitating efficient evaluation and analysis.",
      "homepage": "",
      "language": "Python",
      "created_at": "2024-12-25T03:03:06Z",
      "updated_at": "2025-03-12T07:53:25Z",
      "topics": [],
      "readme": "# Dataloaders\n\nDataloaders is a versatile library designed for processing and formatting datasets to support various Retrieval-Augmented Generation (RAG) pipelines, facilitating efficient evaluation and analysis.\n\n## Features\n\nThe library provides a unified interface for working with datasets, offering methods to load, preprocess, and evaluate data tailored to RAG pipelines. Key features include:\n\n1. **Data Loading**: Extracts and structures raw data into text and metadata fields.\n2. **Question Retrieval**: Retrieves questions for evaluation in RAG pipelines.\n3. **Document Conversion**: Prepares data for integration with LangChain and Haystack pipelines.\n4. **Text Splitting**: Supports multiple chunking strategies to optimize document segmentation.\n5. **Evaluation Publishing**: Publishes processed and evaluation data to Weave.\n\n### Core Methods\n\n#### `load_data()`\n\nProcesses the dataset into a structured format suitable for downstream tasks. Returns a list of dictionaries containing:\n\n- **`text`** (str): Document content.\n- **`metadata`** (dict): Associated metadata, such as questions, choices, answers, and additional fields.\n\n#### `get_questions()`\n\nRetrieves all questions from the dataset as a list of strings.\n\n#### `get_eval_data()`\n\nStructures data for evaluation, returning instances in the following format:\n\n- **`question`**: The query to be evaluated.\n- **`answer`**: The expected answer.\n- **`docs`**: Relevant documents supporting the question.\n\n#### `get_haystack_documents()`\n\nConverts the processed data into Haystack `Document` objects, ready for use in Haystack pipelines.\n\n#### `get_langchain_documents()`\n\nConverts the processed data into LangChain `Document` objects, compatible with LangChain pipelines.\n\n#### `publish_to_weave()`\n\nPublishes the processed dataset and evaluation data to a Weave project.\n\n---\n\n## Text Chunking Strategies\n\nEfficient text chunking ensures optimal performance in RAG pipelines. Dataloaders supports the following strategies:\n\n1. **CharacterTextSplitter**: Divides text based on a specific character delimiter.\n2. **RecursiveCharacterTextSplitter**: Recursively splits text using a hierarchy of delimiters (e.g., `\\n\\n`, `\\n`, spaces).\n3. **SemanticChunker**: Uses embedding models to create semantically coherent chunks.\n4. **UnstructuredChunking**: Leverages the `unstructured` library for adaptive document chunking.\n\n---\n\n## Installation\n\nTo install the library, clone the repository and install the dependencies:\n\n```bash\ngit clone https://github.com/avnlp/dataloaders\ncd dataloaders\npip install -e .\n```\n\n---\n\n## Usage Example\n\n### Loading the ARC Dataset\n\n```python\nfrom dataloaders.arc_dataloader import ARCDataloader\n\ndataloader = ARCDataloader(\n    dataset_name=\"awinml/arc_challenge_processed\",\n    split=\"train\",\n    splitter=\"UnstructuredChunker\",\n    splitter_args={\"chunking_strategy\": \"basic\"},\n)\n\ndata = dataloader.load_data()\n\n# Sample output:\n# [\n#     {\n#         \"text\": \"An astronomer observes that a planet rotates faster after a meteorite impact. Which is the most likely effect of this increase in rotation?\",\n#         \"metadata\": {\n#             \"choices\": [\n#                 \"Planetary density will decrease.\",\n#                 \"Planetary years will become longer.\",\n#                 \"Planetary days will become shorter.\",\n#                 \"Planetary gravity will become stronger.\"\n#             ],\n#             \"answer\": \"Planetary days will become shorter.\",\n#             \"question\": \"An astronomer observes that a planet rotates faster...\"\n#         }\n#     }\n# ]\n\nevaluation_data = dataloader.get_evaluation_data()\n\nquestions = dataloader.get_questions()\n\nlangchain_documents = dataloader.get_langchain_documents()\n\nhaystack_documents = dataloader.get_haystack_documents()\n\ndataloader.publish_to_weave(\n    weave_project_name=\"arc\",\n    dataset_name=\"arc_dataset\",\n    evaluation_dataset_name=\"arc_evaluation_dataset\",\n)\n```\n\n## License\n\nThis project is licensed under the MIT License. See the LICENSE file for more details.\n"
    },
    {
      "name": "malte-b/gaming_copilot",
      "stars": 3,
      "img": "https://avatars.githubusercontent.com/u/27922183?s=40&v=4",
      "owner": "malte-b",
      "repo_name": "gaming_copilot",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2025-01-18T10:58:44Z",
      "updated_at": "2025-02-07T09:23:54Z",
      "topics": [],
      "readme": "# gaming_copilot"
    },
    {
      "name": "avnlp/rag-pipelines",
      "stars": 3,
      "img": "https://avatars.githubusercontent.com/u/123800833?s=40&v=4",
      "owner": "avnlp",
      "repo_name": "rag-pipelines",
      "description": "Advanced RAG Pipelines and Evaluation: Self-Reflective RAG, Corrective RAG, Adaptive RAG, Sub-Query Generation and Routing, DSPy, DeepEval",
      "homepage": "",
      "language": "Python",
      "created_at": "2025-01-12T14:54:58Z",
      "updated_at": "2025-03-09T23:30:52Z",
      "topics": [
        "crag",
        "deepeval",
        "dspy",
        "llm",
        "pinecone",
        "rag",
        "self-rag"
      ],
      "readme": "# RAG Pipelines\n\nEarnings calls are a critical source of information for institutional investors, helping them make better investment decisions. The transcripts of these calls are voluminous, generated every quarter, and difficult to parse and correlate. Hence, extracting actionable information from multiple transcripts is extremely crucial.\n\nWe have built a pipeline to perform open-ended question-answering on earnings call transcripts using Generative Large Language Models (LLMs). The pipeline retrieves data from outside the model using Semantic Search and augments the prompts by adding the retrieved data in-context. LLMs effectively use the new information provided to them in the prompts to generate relevant text.\n\n- RAG Pipelines were built on financial datasets consisting of earnings call transcripts and SEC Filings (10-K, 10-Q, 8-K) for question answering.\n- The pipelines use Hybrid Retrieval with the Instructor-XL embedding model and the SPLADE sparse embedding models.\n- The dataloaders for the earnings call transcripts and SEC filings (10-K, 10-Q, 8-K) datasets were used from the `dataloaders` repository.\n- The RAG pipelines include CRAG and Self-RAG variants built on financial datasets for robust question-answering tasks.\n- The Hybrid Retrieval pipeline combines Instructor-XL dense embeddings with SPLADE sparse embedding.\n- Using the Groq API the Llama-3.1 8B model was used for response generation.\n- For Response Evaluation the DeepEval metrics Answer Relevancy, Faithfulness, Hallucination and Summarization were used in pipelines.\n- For Retrieval Evaluation the metrics Contextual Precision, Contextual Recall, Contextual Relevancy were used in pipelines.\n- DeepEval metrics were utilized for scoring retrieval and response evaluation, with results being logged using weave.\n- Separate scripts enable the creation of embeddings, Pinecone indexing, and the execution of Hybrid RAG, CRAG, and Self-RAG pipelines.\n- Using the Groq API the Llama-3.1 30B model was used for evaluation.\n- The full traces of the pipelines were logged using Weave.\n\n## Data\n\n### Structure of Earnings Calls Transcript\n\n**Corporate Participants and Conference Call Participants Sections**: The host of the call introduces the company and the participants. This section contains a list of speakers, their designations and the name of the company they represent. It contains names of the analysts and\ninvestors who are participating in the call. It also contains the names of the company’s executives who are participating in the call, such as the CEO, CFO, and other senior executives.\n\n**Presentation Section**: It provides a comprehensive overview of the company’s financial perfor- mance and strategic direction. The company presents its financial results for the quarter or year, including revenue, earnings per share, and other key metrics.\n\n**Question-Answer Section**: The host opens the call to questions from analysts and investors. This segment can provide additional insights into the company’s performance and future prospects. This section contains the questions that were asked by analysts and investors during the call, as well\nas the responses from the company’s executives.\n\nWe have used the earnings call dataset [Earnings Calls QA](https://huggingface.co/datasets/lamini/earnings-calls-qa)\nThe dataset contains the following columns:\n\n- **Question**: The query or question posed during the earnings call.\n- **Answer**: The response to the query, provided during the call.\n- **Context**: A snippet of the transcript containing relevant information.\n- **Metadata**: Additional details such as speaker information, timestamps, or topics.\n\n## Self RAG on Earnings Calls\n\n- Self-RAG introduces self-reflection into traditional RAG pipelines. It allows the system to evaluate its own intermediate outputs, assess their quality, and decide whether further refinement is necessary before generating a final response.\n\n- An arbitrary LM is trained in an end-to-end manner to learn to reflect on its own generation process given a task input by generating both task output and intermittent special tokens (i.e., reflection tokens). Reflection tokens are categorized into retrieval and critique tokens to\nindicate the need for retrieval and its generation quality respectively. SELF-RAG first determines if augmenting the continued generation with retrieved passages would be helpful.\n\n- If so, it outputs a retrieval token that calls a retriever model on demand. Subsequently, SELF-RAG concurrently processes multiple retrieved passages, evaluating their relevance and then generating corresponding task outputs. It then generates critique tokens to criticize its own output and choose best one in terms of factuality and overall quality.\n\nWe have integrated the Self RAG framework to work with the earnings calls data.\nThe pipeline constructs a workflow graph to define the order and conditions for various processing stages. Nodes represent actions like:\n\n- Retrieve: Fetch documents using the retriever.\n- Grade Documents: Assess relevance using the grader.\n- Transform Query: Modify and optimize queries if needed.\n- Web Search: Search for additional context online.\n- Generate: Produce the final output using the generator.\n\nConditional transitions are added to decide if the workflow should optimize queries or proceed directly to response generation. The graph is compiled for execution.\n\nWhen a query is received, the pipeline builds the workflow graph and sets up the Weave tracer for monitoring. The compiled graph processes the query through the defined nodes and transitions, generating a final response based on the retrieved and processed information.\n\n### Query Transfomer\n\nThe query transformer is used for transforming the input query into a similar query for better retrieval performance.\nThis is achieved by using a Langchain pipeline to generate a new query from the original query.\n\n### Retreival Evaluator\n\nThe Retreival evaluator is used for evaluating the quality of the retrieved documents using an LLM.\nThe pipeline iterates over the retrieved documents and evaluates their relevance using an LLM.\n\n### WebSearch\n\nThe Websearch component searching the web for relevant documents based on the query. It uses the DuckDuckGo API for external web searches.\n\n## License\n\nThe source files are distributed under the [MIT License](https://github.com/avnlp/rag-pipelines/blob/main/LICENSE).\n"
    },
    {
      "name": "analitiq-ai/analitiq",
      "stars": 3,
      "img": "https://avatars.githubusercontent.com/u/165009794?s=40&v=4",
      "owner": "analitiq-ai",
      "repo_name": "analitiq",
      "description": "Analitiq is a robust AI-driven data management framework that optimizes data handling, analytics, and decision-making processes. Explore tools, APIs, and comprehensive documentation to empower your data solutions.",
      "homepage": null,
      "language": "Python",
      "created_at": "2024-04-15T09:10:06Z",
      "updated_at": "2025-04-06T00:44:43Z",
      "topics": [],
      "readme": "# Analitiq\nAnalitiq is a Framework for managin your data using LLMs. Analitiq can be extended with your own services written in python. These custom services can address your unique tasks for managing your data and they can function as part of the overall analytical engine of Analitiq.\n![image](assets/images/Analitiq_Diagram.png)\n\nAnalitiq currently supports the following LLM models\n- ChatGPT\n- Mistral\n- Bedrock (AWS)\n\nAnalitiq currently integrates with the following vectorDBs\n- Weaviate\n- ChromaDB\n\n## What Analitiq needs to work\nSince Analitiq is a framework to help data people manage data using LLMs, it requires at the least:\n1. Access to LLM\n2. Access to Database\n\nAs an extra bonus and to make things even smarter, it could also use:\n3. Access to Vector Database with documentation.\n\n## Quick Start\n1. Clone the repo\n2. Set up `profiles.yml` in root directory. The file `profiles.yml` has all of your sensitive info, such as your API keys and DB credentials, so treat it with respect. Under `uses` you can define which connections should be used for the current deployment. \nIdeally, you would have different `profiles.yml` for your prod and dev instances.\n```yaml\ntest:\n  connections:\n    databases:\n      - name: prod_dw\n        type: postgres\n        host: xxxxx\n        user: xxxx\n        password: 'xxxxx'\n        port: 5432\n        dbname: sample_db\n        dbschema: sample_schema\n        threads: 4\n        keepalives_idle: 240 # default 240 seconds\n        connect_timeout: 10 # default 10 seconds\n        # search_path: public # optional, not recommended\n    llms:\n      - name: prod_llm\n        type: openai\n        api_key: xxxxxx\n        temperature: 0.0\n        llm_model_name: gpt-3.5-turbo\n      - name: dev_llm\n        type: mistral\n        api_key: xxxxxx\n      - name: aws_llm\n        type: bedrock\n        credentials_profile_name: my_profile\n        provider: anthropic\n        llm_model_name: anthropic.claude-v2:1\n        temperature: 0.0\n    vector_dbs:\n      - name: prod_vdb\n        type: weaviate\n        host: example.com\n        api_key: xxxxx\n\n  usage:\n    databases: prod_dw\n    llms: aws_llm\n    vector_dbs: prod_vdb\n```\n3. Set up `project.yml` in root directory. The file `project.yml` has all of your project data, such as where the logs are stored. Most importantly, `project.yml` defines where your custom Services are located so Analitiq can pick them up and use them to manage your data.\n```yaml\nname: 'analitiq'\nversion: '0.1'\nprofile: 'test'\nconfig_version: 2\n\nconfig:\n  general:\n    chat_log_dir: \"chats\" # this is where we save our chat logs.\n    sql_dir: \"analysis\" # this is where the ETL SQLs are being saved and managed\n    services_dir: \"custom_agents\"\n    session_uuid_file: 'session_uuid.txt' # Where session identifier is being recorded. When session is reset, it is like beginning of a new chat topic and new log file will be created.\n    target_path: \"target\"\n    message_lookback: 5 # when LLM has no clue about users request, or users request relates to some item in chat history, how far back (in number of messages) should the LLM look in the current session chat log\n  vectordb:\n    doc_chunk_size: 2000\n    doc_chunk_overlap: 200\n\nservices:\n  - name: ChartService\n    description: \"Use this service to generate script for APEX charts to visualize data\"\n    path: \"custom_agents/chart/chart.py\"\n    class: \"Chart\"\n    method: \"run\"\n    inputs: \"dataframe as serialized json\"\n    outputs: \"javascript that is used by the frontend to visualize data\"\n```\n5. Run the example file `cookbooks/example_analitiq.py`\n\n## Configuration files\n\nThere are 2 configuration files:\n1. profiles.yaml - this file has all the secrets and connections needed to connect to LLMs, VectorDBs, Databases. Because you may have different production and development environments, profiles.yaml allows you to define multiple profiles (and multiple credentials).\n2. project.yaml - this file has the parameters needed for your particular project, including what profile to use. You can define the profile in `profile` parameter. \nOnce you have your project deployed, you can specify which profile to be used by that particular project in `project.yaml`. \n\nLet's look at some examples. Let's say when I run Analitiq locally, I want to use OpenAI. And when I upload it to production server, I want to use Bedrock.\n\nI will set up my connections in profile.py\n```yaml\nprod:\n  connections:\n    databases:\n      - name: prod_db\n        type: postgres\n        host: xxxx\n        user: xxxx\n        password: xxxx\n        port: 5432\n        dbname: postgres\n        dbschema: sample_data\n        threads: 4\n        keepalives_idle: 240 # default 240 seconds\n        connect_timeout: 10 # default 10 seconds\n        # search_path: public # optional, not recommended\n    \n    llms:\n      - name: aws_llm\n        type: bedrock\n        credentials_profile_name: bedrock\n        region_name: eu-central-1\n        provider: anthropic\n        llm_model_name: anthropic.claude-v2\n        temperature: 0.0\n        aws_access_key_id: xxxxx\n        aws_secret_access_key: xxxxx\n  usage:\n    databases: prod_db\n    llms: aws_llm\n\nlocal:\n  connections:\n    databases:\n      - name: local_db\n        type: postgres\n        host: xxxx\n        user: xxxx\n        password: xxxx\n        port: 5432\n        dbname: postgres\n        dbschema: sample_data\n        threads: 4\n        keepalives_idle: 240 # default 240 seconds\n        connect_timeout: 10 # default 10 seconds\n        # search_path: public # optional, not recommended\n\n    llms:\n      - name: openai_llm\n        type: openai\n        api_key: xxxx\n        temperature: 0.0\n        llm_model_name: gpt-3.5-turbo\n  usage:\n    databases: local_db\n    llms: openai_llm\n```\n\non my local machine, I would have `project.py` file with the following configuration\n```yaml\nname: 'analitiq'\nversion: '0.1'\nprofile: 'local'\nconfig_version: 2\n```\nand the production server will have `project.py` file with the following configuration\n```yaml\nname: 'analitiq'\nversion: '0.1'\nprofile: 'prod'\nconfig_version: 2\n```\n\nNow, I can move the project files between my prod environment and local and Analitiq will use different configuration to switch automagically.\n\n## UI\nThe app interface can be extended with a UI, such as streamlit app.\n![image](assets/images/query.png)"
    },
    {
      "name": "shxntanu/lesa",
      "stars": 3,
      "img": "https://avatars.githubusercontent.com/u/97496261?s=40&v=4",
      "owner": "shxntanu",
      "repo_name": "lesa",
      "description": "📚 Turn your terminal into a File Interpreter",
      "homepage": "https://pypi.org/project/lesa/",
      "language": "Python",
      "created_at": "2024-12-03T08:46:37Z",
      "updated_at": "2025-02-01T08:37:33Z",
      "topics": [
        "document-chat",
        "ollama",
        "python"
      ],
      "readme": "![Banner](https://github.com/shxntanu/lesa/raw/master/assets/banner-v3.png)\n\n<div align=\"center\">\n\n[![Python](https://img.shields.io/badge/python-3.10%2B-blue)](https://www.python.org/downloads/)\n![PyPI - Version](https://img.shields.io/pypi/v/lesa)\n![PyPI Downloads](https://static.pepy.tech/badge/lesa)\n\n</div>\n\n<div align=\"center\">\n\n**_lesa_**\n`[lee - saa]` • **Old Norse** <br/>\n(v.) to read, to study, to learn\n\n<!-- <div align=\"center\">\n  <sub>Prepared by <a href=\"https://github.com/shxntanu\">Shantanu Wable</a> and <a href=\"https://github.com/omkargwagholikar\">Omkar Wagholikar</a> </sub>\n</div> -->\n\n</div>\n\n`lesa` is a CLI tool built in Python that allows you to converse with your documents from the terminal, completely offline and on-device using **Ollama**. Open the terminal in the directory of your choice and start a conversation with any document!\n\n## Usage\n\nTo start a conversation with a document (`.pdf` and `.docx` for now), simply run:\n\n```bash\nlesa read path/to/your/document --page <page_number> (optional)\n```\n\nOr start a conversation with an already-embedded directory, run:\n\n```bash\nlesa chat\n```\n\n### Embed\n\nTo embed all files from your current working directory, run:\n\n```bash\nlesa embed\n```\n\nThis creates a `.lesa` config folder in your current working directory that stores the embeddings of all the documents in the directory.\n\n<!-- ## Features\n\n-   🖥️ **Completely On-Device**: Uses Ollama under the hood to interface with LLMs, so you can be sure your data is not leaving your device.\n-   📚 **Converse with (almost) all documents**: Supports PDF, DOCX and Text files.\n-   🤖 **Wide Range of LLMs**: Choose the Large Language Model of your choice. Whether you want to keep it quick and concise, or want to go all in with a huge context window, the choice is yours. -->\n\n## Setup\n\n`lesa` uses [Ollama](https://ollama.com/) under the hood to utilize the power of large language models.\nTo install and setup Ollama, run the setup script [`setup-ollama.sh`](scripts/setup-ollama.sh).\n\n```bash\ncurl -fsSL https://raw.githubusercontent.com/shxntanu/lesa/master/scripts/setup-ollama.sh | bash\n```\n\nThis script automatically installs the Ollama CLI and pulls the default model (llama3.1:latest) for you. Then install the package using pip.\n\n## Installation\n\nSimply install the package using pip:\n\n```bash\npip install lesa\n```\n\nTo upgrade to the latest version, run:\n\n```bash\npip install -U lesa\n```\n\n## Contribute\n\nWe welcome contributions! If you'd like to improve `lesa` or have any feedback, feel free to open an issue or submit a pull request.\n\n## Credits\n\n1. [Typer](https://typer.tiangolo.com/) and [Rich](https://github.com/Textualize/rich): CLI library and terminal formatting.\n2. [Ollama](https://ollama.com/): On-device language model inference.\n3. [Langchain](https://langchain.com/): Pipeline for language model inference.\n4. [FAISS](https://github.com/facebookresearch/faiss): Similarity Search and Vector Store library from Meta AI.\n\n## License\n\nApache-2.0\n"
    },
    {
      "name": "nlpkeg/KMatrix",
      "stars": 3,
      "img": "https://avatars.githubusercontent.com/u/186663451?s=40&v=4",
      "owner": "nlpkeg",
      "repo_name": "KMatrix",
      "description": "KMatrix: A Flexible Heterogeneous Knowledge Enhancement Toolkit for Large Language Model",
      "homepage": null,
      "language": "Python",
      "created_at": "2024-10-29T04:31:06Z",
      "updated_at": "2025-02-20T11:10:30Z",
      "topics": [],
      "readme": "#  :sunflower: KMatrix:  A Flexible Heterogeneous Knowledge Enhancement Toolkit for Large Language Model\n\n\n\n<img src=\"https://img.shields.io/badge/%F0%9F%A4%97%20HuggingFace%20Datasets-27b3b4.svg\"> <img src=\"https://img.shields.io/npm/l/vue-web-terminal.svg\"><img src=\"https://img.shields.io/badge/made_with-Python-blue\">\n\n\n\nKMatrix is a flexible heterogeneous knowledge enhancemant toolkit for LLMs.  Our toolkit contains seven stages to complete knowledge-enhanced generation task. All stages are implemented based on our modular component definitions.  Meanwhile, we design a control-logic flow diagram to combine components. The key features of KMatrix include: \n\n1. Unified enhancement of heterogeneous knowledge: KMatrix uses both verbalizing-retrieval and parsing-query methods to support unified enhancement of heterogeneous knowledge (like free-textual knowledge, tables, knowledge graphs, etc).\n2. Systematical adaptive enhancement methods integration: KMatrix offers comprehensive adaptive enhancement methods including retrieval timing judgment and knowledge source selection. \n3. High customizability and easy combinability: our modularity and control-logic flow diagram design flexibly supports the entire lifecycle of various complex Knowledge Enhanced Large Language Models (K-LLMs) systems, including training, evaluation, and deployment.\n4. Comprehensive evaluation of K-LLMs systems enhanced by heterogeneous knowledge: we integrate a rich collection of representative K-LLMs knowledge, datasets, and methods, and provide performance analysis of heterogeneous knowledge enhancement. \n\n![image](images/kmatrix_system.png)\n\n\n\n## :wrench: Quick Start\n\n### 1. Quick Start from Manual\n\n**Installation** \n\nTo get started with KMatrix, simply clone it from Github and install (requires Python 3.7+ ,  Python 3.10 recommended): \n\n\n    $ git clone https://github.com/NLPerWS/KMatrix.git\n    \n    # It is recommended to use a virtual environment for installation\n    $ conda create -n KMatrix python=3.10\n    $ conda activate KMatrix\n    \n    # Install backend environment\n    $ cd KMatrix\n    $ pip install -r requirements.txt\n    \n    # Install Frontend environment\n    # You need a node environment, and nvm is recommended for node environment management\n    # Recommended node environments: 16.20.2\n    # You can refer to the fellowing websites to install nvm\n    # https://nvm.uihtm.com/#nvm-linux \n    # https://github.com/nvm-sh/nvm\n    # After installing the node environment, execute:\n    $ cd easy-flow\n    $ npm install\n    \n    # Then, you need to install some third-party tools required by our toolkit\n    # Install ES database using Docker\n    $ docker pull elasticsearch:8.11.1\n    $ docker run -idt  \\\n        -p 9200:9200 -p 9300:9300 \\\n        -e \"discovery.type=single-node\" \\\n        -e \"ES_JAVA_OPTS=-Xms512m -Xmx512m\" \\\n        -e \"xpack.security.enabled=true\" \\\n        -e \"xpack.security.enrollment.enabled=true\" \\\n        -e \"ELASTIC_PASSWORD=yourpassword\" \\\n        -v $(pwd)/elasticsearch_data:/usr/share/elasticsearch/data \\\n        -v $(pwd)/esplugins:/usr/share/elasticsearch/plugins \\\n        --name elasticsearch elasticsearch:8.11.1\n        \n    We upload knowledge and the datasets to ModelScope: https://www.modelscope.cn/datasets/KMatrixRep/KMatrix_Rep\n    Use the following commands to download:\n    $ git lfs install\n    $ git clone https://www.modelscope.cn/datasets/KMatrixRep/KMatrix_Rep.git\n    you can download it, it contains:\n    dir_dataset/\n    dir_knowledge/\n    dir_model/\n    use the downloaded folders to replace the original folders in KMatrix directory.\n    \n    # You need to import knowledge into ES database for Retrieval, our knowledge samples locate in dir_knowledge/local_knowledge_verlized/, you need to create three indexes in ES named 'wikipedia','wikidata','wikitable', and their corresponding files are respectively Wikipedia/wikipedia.jsonl, wikidata/wiki_data.jsonl, wikitable/wikitable.jsonl. After that, you can use retrieval model to retrieve the knowledge in the database.\n\n**StartUp**\n\n\n    If you have successfully installed the environment, a quick start will be easy.\n    \n    1. Set configurations that needs to be modified in the root_config.py file located in the project root directory, if necessary. Set the SERVER_HOST in easy-flow/src/assets/js/host.js to the IP address of deployment server.\n    \n    2. Start the toolkit by executing following command: \n    $ cd KMatrix/easy-flow\n    $ npm run dev\n    $ cd KMatrix\n    $ python flask_server.py\n    Visit KMatrix toolkit using the browser: http://yourserverip:8000\n    \n    3. Construct and Execute Flow diagram\n    You can construct K-LLMs systems using our modular component and control-logic flow diagram, and execute it. Details of K-LLMs systems construction can be found in toolkit usage. You can use a flow diagram we have built (a K-LLMs system actively querying multiple knowledge interfaces) for a quick start:\n    Click the [Use exising diagram] drop-down box on the frontend of toolkit, select Deployment/v16_cok_de_diagram, and then click the [Deploy diagram] button to start the deployment. After the deployment completes, enter your question in the question box and click [send] to generate reasoning steps and answer.\n\n### 2. Quick Start from Docker (recommended)\n\n`````\n$ git clone https://github.com/NLPerWS/KMatrix.git\n$ chmod +x -R KMatrix\nSet configurations that needs to be modified in the root_config.py file located in the project root directory, if necessary. Set the SERVER_HOST in easy-flow/src/assets/js/host.js to the IP address of deployment server.\n\n# Install ES database using Docker\n$ docker pull elasticsearch:8.11.1\n$ docker run -idt  \\\n    -p 9200:9200 -p 9300:9300 \\\n    -e \"discovery.type=single-node\" \\\n    -e \"ES_JAVA_OPTS=-Xms512m -Xmx512m\" \\\n    -e \"xpack.security.enabled=true\" \\\n    -e \"xpack.security.enrollment.enabled=true\" \\\n    -e \"ELASTIC_PASSWORD=yourpassword\" \\\n    -v $(pwd)/elasticsearch_data:/usr/share/elasticsearch/data \\\n    -v $(pwd)/esplugins:/usr/share/elasticsearch/plugins \\\n    --name elasticsearch elasticsearch:8.11.1\n\n$ docker pull leap233/kmatrix:v1\n$ cd KMatrix\n$ sh docker_start.sh\n`````\n\n\n\n## :dizzy: Toolkit Usage\n\nWe provide a  screencast video of our toolkit at [here](https://youtu.be/VL-zY2pphwI) which explained the tool usage instructions.\n\n### 1 K-LLMs System Construction\n\nKMatrix constructs K-LLMs system using two stages：\n\n- Selecting  components and  configuring  parameters\n\n- Constructing components logic relations using control-logic flow diagram \n\n#### 1.1 Selecting  components and  configuring  parameters\n\nKMatrix component is an functional unit of K-LLMs system. We unify datasets, knowledge, and models involved in K-LLMs as components. KMatrix defines 16 types of components, like Retriever, Query Parser, Generator, etc. \n\n| Component Class           | Component Instance                     | Instance Description                                         |\n| ------------------------- | -------------------------------------- | ------------------------------------------------------------ |\n| **KnowledgeUploader**     | OnlineInterface                        | Online knowledge interfaces access                           |\n|                           | Local_Text                             | Local texual knowledge access                                |\n|                           | Local_Table                            | Local table knowledge access                                 |\n|                           | Local_KG                               | Local knowledge graph access                                 |\n| **KnowledgePreprocessor** | TextConvertor                          | Format processing of local texual knowledge                  |\n|                           | TableConvertor                         | Format processing of local table knowledge                   |\n|                           | KGConvertor                            | Format processing of local knowledge graph                   |\n|                           | Interfacer                             | Online knowledge interface standardization                   |\n| **KnowledgeIntegrator**   | UnifiedVerbalizer                      | Convert various types of local heterogeneous knowledge into unified text  for knowledge integration |\n|                           | UnifiedQuerier                         | Incorporate different types of knowledge query interfaces for knowledge integration |\n| **KnowledgeSaver**        | SaveToElasticSearch                    | Save the data to ES database                                 |\n|                           | SaveToServer                           | Save the data to File system                                 |\n|                           | SaveQueryInterface                     | Save interfaces for active query                             |\n| **Knowledge**             | KnowledgeCombiner                      | Load knowledge from ES database                              |\n|                           | Knowledge_wikipedia                    | Load Wikipedia knowledge from ES database                    |\n|                           | Knowledge_wikipedia_wikidata           | Load Wikipedia+Wikidata knowledge from ES database           |\n|                           | Knowledge_wikipedia_wikidata_wikitable | Load Wikipedia+Wikidata+Wikitable knowledge from ES database |\n|                           | KnowledgeSelector                      | Load unified querier service, used in active query           |\n| **Retriever**             | Bm25MemoryRetriever                    | BM25 memory retrieval model                                  |\n|                           | Bm25EsRetriever                        | BM25 retrieval for ES database                               |\n|                           | ContrieverRetriever                    | BERT-based retrieval model                                   |\n|                           | DPR_retriever                          | BERT-based retrieval model                                   |\n|                           | BGE_retriever                          | BERT-based retrieval model                                   |\n|                           | BERT_retriever                         | Basic retrieval model                                        |\n|                           | E5_retriever                           | LLM-based retrieval model                                    |\n| **QueryGenerator**        | NL Parser                              | Natural language query generator                             |\n|                           | Sparql Parser                          | Sparql language query generator                              |\n| **Adaptive Interactor**   | Special Token-Timing Judgment          | Adaptive retrieval timing judgment                           |\n|                           | Self Consistency-Timing Judgment       | Adaptive retrieval timing judgment                           |\n|                           | Example demonstration-Source Selection | Adaptive retrieval source selection                          |\n| **Generator**             | OpenAIGenerator                        | Closed-source general  generator                             |\n|                           | RagGenerator                           | Retrieval instructions-enhanced generator                    |\n|                           | LLama2Generator                        | Open-source general  generator                               |\n|                           | Baichuan2Generator                     | Open-source general  generator                               |\n| **Multiplexer**           | Multiplexer                            | Pass data to multiple components simultaneously              |\n| **PromptBuilder**         | PromptBuilder                          | Create customized prompts                                    |\n| **OutputBuilder**         | OutputBuilder                          | Receive and normalize the output of system flow diagram      |\n| **Router**                | ConditionalRouter                      | Execute different branches based on conditions               |\n| **Evaluation**            | Evaluator                              | Manage K-LLMs system evaluation process                      |\n| **Controller**            | SelfRagShortDemoController             | Adaptive K-LLMs control flow diagram                         |\n|                           | CokController                          | Adaptive K-LLMs control flow diagram                         |\n| **DataSet**               | DataSet_*                              | Datasets loading and using                                   |\n\nUsers can select these predefined components or define their own components according to predefined formats. And configure component  parameters in KMatrix graphical interface. \n\n#### 1.2 Constructing components logic relations using control-logic flow diagram\n\nWe can use logic or control flow diagrams to organize component relationships for K-LLMs system construction. \n\n- **Deploy Control Flow Diagram:** for K-LLMs system with complex process (including multifarious arithmetic operations and logical judgments), we can use control flow diagram to design system process using Python programming.  Adding components to a  control flow diagram, and programming components logics.\n\n​\t\tstep1: Adding components to a  control flow diagram.\n\n<div align=center><img src=\"images/v2_create_ctro.png\" alt=\"image\" style=\"zoom: 50%;\" /></div>\n\n\n\n​\t\tstep2: programming components logics using python.\n\n<div align=center><img src=\"images/v2_rag_long.png\" alt=\"image\" style=\"zoom: 50%;\" /></div>\n\n\n\n- **Deploy Logic Flow Diagram:** for K-LLMs system with concise process (like linear, branching, looping, and conditional structures), we can employ logic flow diagram to directly connect components with edges. By jointly using control and logic flow diagram, KMatrix flexibly supports common K-LLMs patterns using naive, iterative, and adaptive enhancement methods. Draging components into the design area, and connecting them to build K-LLMs system. \n\n<img src=\"images/v2_inter.png\" alt=\"image\" style=\"zoom:50%;\" />\n\n\n\n- **Flow diagram design approachs for training, evaluation, and deployment of K-LLMs system**: for component training and evaluation, users can simply connect the Dataset component with the component to be trained/evaluated. For end-to-end evaluation of the K-LLMs system, users can employ the Evaluator component to connect Dataset component with K-LLMs system, and the Evaluator component will manage evaluation process. For K-LLMs system deployment, users can map the task inputs to the Multiplexer, and connect the task outputs to the OutputBuilder on the basis of original system flow diagram. After constructing system flow diagram, Users can excute it. \n\n![image](images/toolkit_usage.png)\n\n\n\n- **Integrated pre-built K-LLMs systems flow diagram instances:** we have integrated some pre-built K-LLMs systems flow diagram instances for user use, which contain four categories: Knowledge Integration, Training, Evaluation, Deployment for heterogeneous knowledge integration, K-LLMs system training, evaluation, deployment respectively. Users can click the [Use exising diagram] drop-down box on the frontend of toolkit, and use the corresponding flow diagram according to the Name.\n\n| Name                                        | Category              | Function Description                                         |\n| :------------------------------------------ | :-------------------- | :----------------------------------------------------------- |\n| Knowledge Integration/v16_save_upload_to_ES | Knowledge Integration | Converting various types of local heterogeneous knowledge (such as text, tables and knowledge graphs) into unified text fragments for local knowledge integration. |\n| Knowledge Integration/v16_UnifiedQuery      | Knowledge Integration | Incorporating different types of knowledge query interfaces (like Wikipedia, Wikidata) for online knowledge integration. |\n| Training/v16_train_selfrag                  | Training              | SelfRag Generator component training.                        |\n| Training/v16_train_contriever               | Training              | Contriever Retriever component training.                     |\n| Training/v16_train_BGE                      | Training              | BGE Retriever component training.                            |\n| Evaluation/v16_eva_contriever               | Evaluation            | Contriever Retriever component evaluation.                   |\n| Evaluation/v16_eva_BGE                      | Evaluation            | BGE Retriever component evaluation.                          |\n| Evaluation/v16_eva_Iterative                | Evaluation            | Evaluation of a Iterative K-LLMs system using local heterogeneous knowledge . |\n| Evaluation/v16_eva_selfRAG_short            | Evaluation            | Evaluation of an adaptive K-LLMs system using local heterogeneous knowledge. |\n| Deployment/v16_infer_naive_rag              | Deployment            | Deployment of a naive K-LLMs system using local heterogeneous knowledge. |\n| Deployment/v16_cok_de_diagram               | Deployment            | Deployment of an adaptive K-LLMs system using multiple knowledge query interfaces. |\n| Deployment/v16_infer_selfRAG_short          | Deployment            | Deployment of an adaptive K-LLMs system using heterogeneous knowledge. |\n| Deployment/v16_infer_iterative              | Deployment            | Deployment of a Iterative K-LLMs system using heterogeneous knowledge. |\n\n\n\n### 2 K-LLMs System Execution\n\nAfter constructing system flow diagram, Users can execute it. \n\n1. K-LLMs system training and evaluation flow diagram execution:  Click [Execute diagram] button to execute it, and click [Check execution results] to view the training or evaluation result logs.\n2. K-LLMs system deployment flow diagram execution: Click the [Deploy diagram] button to deploy it.  After completing deployment, enter your question in the question box and click [send] to generate reasoning steps and answer. The K-LLMs system deployment interface with multiple knowledge bases and multiple queries is shown in the following figure.\n\n![image](images/dep_res.png)\n\n\n\n##  :notebook: Knowledge And Datasets\n\n**Knowledge**:  KMatrix designs two ways of knowledge access: local knowledge and online interface. To support heterogeneous knowledge enhancemant, for local knowledge, we integrate public Wikipedia (text), Wikidata (knowledge graph) and Wikitable (table). For online interface, we integrate two general knowledge interfaces(Wikipedia and Wikidata query APIs) and six domain knowledge interfaces (APIs including Uptodate, CK12, etc). \n\n| Knowledge Access Way | Knowledge Name   | Knowledge Scale |\n| -------------------- | ---------------- | --------------- |\n| local knowledge      | Wikipedia        | 21000k          |\n|                      | Wikidata         | 5790k           |\n|                      | Wikitable        | 6860k           |\n| online interface     | Wikipedia        | /               |\n|                      | Wikidata         | /               |\n|                      | Uptodate         | /               |\n|                      | Flashcard        | 33553           |\n|                      | BioScienceqa     | 1062            |\n|                      | CK12             | /               |\n|                      | PhyScienceqa     | 780             |\n|                      | Physicsclassroom | /               |\n\n\n\n**Dataset**:  KMatrix provides five classes of datasets to support evaluation of K-LLMs system. We provide RETRIEVE\\_TRAIN and RETRIEVE_EVAL to train and evaluate Retriever components. We provide GENERATOR\\_TRAIN for adaptive retrieval training of  Generator components. We provide ODQA_EVAL and ODQA_EVAL_Simplified to evaluate knowledge enhancement performance of K-LLMs system under two ways of knowledge access: local knowledge and online interface respectively.  \n\n| Dataset Class        | Dataset Name | Dataset Scale |\n| -------------------- | ------------ | ------------- |\n| RETRIEVE_TRAIN       | MSMARCO      | 480k          |\n|                      | NQ           | 58k           |\n|                      | HotpotQA     | 84k           |\n| RETRIEVE_EVAL        | ArguAna      | 1401          |\n|                      | FiQA2018     | 6648          |\n|                      | HotpotQA     | 98k           |\n|                      | MSMARCO      | 510k          |\n|                      | NFCorpus     | 3237          |\n|                      | NQ           | 3452          |\n|                      | Quora        | 15k           |\n|                      | SciFact      | 1109          |\n| GENERATOR_TRAIN      | SelfRAG      | 146k          |\n| ODQA_EVAL            | 2Wikiqa      | 12576         |\n|                      | Hotpotqa     | 7405          |\n|                      | NQ           | 3610          |\n|                      | Popqa        | 1399          |\n|                      | Squad        | 10570         |\n|                      | Triviaqa     | 7313          |\n|                      | Webqa        | 2032          |\n| ODQA_EVAL_Simplified | Hotpotqa     | 308           |\n|                      | Medmcqa      | 146           |\n|                      | MMLU_bio     | 454           |\n|                      | MMLU_phy     | 253           |\n\n\n\n## :page_facing_up: Experimental Results\n\nWe report experimental results from two aspects: Knowledge access performance evaluation and Single vs. multi-knowledge bases enhancement evaluation.\n\n1. Knowledge access performance evaluation, which reports  the knowledge access performance of five Retriever components. \t\t\t\t\n\n|            | ArguAna      |        | FiQA2018 |        | HotpotQA  |        | MSMARCO     |        |\n| ---------- | ------------ | ------ | -------- | ------ | --------- | ------ | ----------- | ------ |\n|            | map@100      | r@100  | map@100  | r@100  | map@100   | r@100  | map@100     | r@100  |\n| BERT       | 6.87%        | 34.48% | 0.04%    | 0.71%  | 0.07%     | 0.5%   | 0.0%        | 0.02%  |\n| Contriever | 24.59%       | 97.36% | 27.38%   | 65.25% | 55.27%    | 77.76% | 21.90%      | 25.97% |\n| DPR        | 21.33%       | 89.94% | 11.75%   | 38.48% | 31.25%    | 57.83% | 16.00%      | 58.13% |\n| BGE        | 28.4%        | 96.79% | 37.55%   | 75.42% | 48.58%    | 64.89% | 36.28%      | 88.73% |\n| E5         | 30.50%       | 99.22% | 41.80%   | 79.52% | 39.04%    | 71.03% | 21.99%      | 78.27% |\n|            | **NFCorpus** |        | **NQ**   |        | **Quora** |        | **SciFact** |        |\n|            | map@100      | r@100  | map@100  | r@100  | map@100   | r@100  | map@100     | r@100  |\n| BERT       | 0.28%        | 3.22%  | 0.03%    | 0.30%  | 41.26%    | 67.85% | 1.56%       | 14.26% |\n| Contriever | 15.33%       | 29.93% | 43.23%   | 92.71% | 83.06%    | 99.35% | 62.88%      | 94.20% |\n| DPR        | 6.79%        | 17.90% | 22.29%   | 73.00% | 78.47%    | 97.78% | 29.95%      | 70.23% |\n| BGE        | 18.00%       | 33.94% | 44.66%   | 93.39% | 86.15%    | 99.70% | 69.04%      | 97.17% |\n| E5         | 11.42%       | 27.19% | 10.28%   | 41.22% | 85.57%    | 99.65% | 70.40%      | 96.00% |\n\n2. Single vs. multi-knowledge bases enhancement evaluation using local knowledge access way, which reports four K-LLMs systems performance using three types of local heterogeneous Knowledge: Wikipedia(Text)+Wikidata(KG)+Wikitable(Table). The K-LLMs systems include Naive-GEN(answer generation without knowledge), Naive-RAG(naive K-LLMs), Interleave (iterative K-LLMs) and Self-RAG (adaptive K-LLMs). \n\n| Methods    | Knowledge                                     | PopQA  | TriviaqaQA | NQ     | HotpotQA | 2Wikiqa | WebQA  |\n| ---------- | --------------------------------------------- | ------ | ---------- | ------ | -------- | ------- | ------ |\n| Naive-GEN  | Without                                       | 14.44% | 35.00%     | 8.53%  | 11.45%   | 17.57%  | 17.03% |\n|            | Wikipedia(Text)                               | 27.51% | 54.63%     | 33.77% | 20.39%   | 22.07%  | 31.74% |\n| Naive-RAG  | Wikipedia(Text)+Wikidata(KG)                  | 42.82% | 54.18%     | 33.68% | 20.73%   | 23.19%  | 31.10% |\n|            | Wikipedia(Text)+Wikidata(KG)+Wikitable(Table) | 42.89% | 54.68%     | 34.13% | 20.47%   | 23.43%  | 31.05% |\n|            | Wikipedia(Text)                               | 25.80% | 39.6%      | 24.96% | 14.7%    | 18.03%  | 22.74% |\n| Interleave | Wikipedia(Text)+Wikidata(KG)                  | 41.03% | 47.12%     | 25.01% | 16.38%   | 18.26%  | 23.23% |\n|            | Wikipedia(Text)+Wikidata(KG)+Wikitable(Table) | 41.17% | 46.27%     | 25.43% | 16.22%   | 22.1%   | 23.47% |\n|            | Wikipedia(Text)                               | 41.95% | 58.38%     | 29.28% | 25.80%   | 29.34%  | 34.69% |\n| Self-RAG   | Wikipedia(Text)+Wikidata(KG)                  | 61.37% | 58.23%     | 28.92% | 25.91%   | 29.99%  | 34.30% |\n|            | Wikipedia(Text)+Wikidata(KG)+Wikitable(Table) | 61.37% | 58.57%     | 29.25% | 25.71%   | 30.12%  | 34.84% |\n\n\n\n3. Single vs. multi-knowledge bases enhancement evaluation using online interface access way, which reports three K-LLMs methods performance using eight knowledge interfaces from four domains. The K-LLMs methods include the COT method without knowledge enhancement, selective query across multiple-domain knowledge interfaces, fixed query on single-domain knowledge interface.\n\n| Methods                    | Knowledge                                                    | Factual Domain | Medical Domain | Physics Domain | Biology Domain |\n| -------------------------- | ------------------------------------------------------------ | -------------- | -------------- | -------------- | -------------- |\n|                            |                                                              | Hotpotqa       | Medmcqa        | MMLU_phy       | MMLU_bio       |\n| COT                        | Without                                                      | 37.99%         | 40.41%         | 45.85%         | 78.63%         |\n| COK-DE<br/>Selective Query | Four domains, eight<br/>knowledge interfaces<br/>(Text,KG,Table) | 40.58%         | 46.58%         | 50.2%          | 78.63%         |\n| COK-DE<br/>Fixed Query     | Four domains, eight<br/>knowledge interfaces<br/>(Text,KG,Table) | 38.96%         | 44.52%         | 49.8%          | 77.97%         |\n\n\n\n\n\n##   :raised_hands: Additional FAQs\n\n- Q: Why do connection edges sometimes become disordered when constructing a flow diagram?\n  \n  A: This issue may occur if you delete a component and make further edits to the flow diagram. To resolve this, simply save the flow diagram firstly, refresh the page, and reload the flow diagram.\n  \n- ......\n\n\n\n\n## 🔖 License\n\nApache 2.0 License.\n"
    },
    {
      "name": "KushagraSikka/RAG_Microservice-",
      "stars": 3,
      "img": "https://avatars.githubusercontent.com/u/54061496?s=40&v=4",
      "owner": "KushagraSikka",
      "repo_name": "RAG_Microservice-",
      "description": "RAG-Microservice: A robust, scalable question-answering service leveraging the Retriever-Answer Generator (RAG) architecture. Built with [Tech/Models used, e.g., Elasticsearch, GPT-3], this service efficiently retrieves relevant documents and generates precise answers for complex queries. ",
      "homepage": null,
      "language": "Python",
      "created_at": "2024-09-15T08:03:33Z",
      "updated_at": "2025-02-26T08:59:16Z",
      "topics": [],
      "readme": "\n# RAG Microservice - Professional Profile Assistant\n\nThe **RAG Microservice** is an intelligent assistant designed to provide detailed, structured, and professional responses about **Kushagra Sikka's professional profile**. It uses Retrieval-Augmented Generation (RAG) to retrieve relevant information and generate well-formatted, concise answers tailored for recruiters and collaborators.\n\n## **Use Case**\n\nThis system is tailored to **Kushagra's recruiters** and professional network, allowing them to:\n- Fetch **structured professional summaries**.\n- Retrieve details about **technical skills** with usage examples.\n- Explore **academic achievements**, **work experience**, and **projects**.\n- Gain insights into **recent contributions** and **research focus**.\n- Access contact information and links to **professional profiles**.\n\nThe assistant ensures:\n- **Accurate and verified information** retrieval from curated documents.\n- **Well-formatted responses** using bullet points for readability.\n- A **professional tone** to enhance user experience and utility.\n\n### Example Questions\n- **\"Who is Kushagra Sikka?\"**  \n  Provides a professional overview, recent impact, and technical expertise.\n  \n- **\"What are Kushagra's technical skills?\"**  \n  Details programming languages, cloud platforms, and tools with usage examples.\n  \n- **\"Tell me about Kushagra's achievements.\"**  \n  Highlights teaching impact, technical accomplishments, and academic recognition.\n\n---\n\n## **Architecture**\n\nThe system integrates:\n- **Backend**: FastAPI + Haystack for RAG implementation.\n- **Frontend**: React + TailwindCSS for a user-friendly interface.\n- **Deployment**: Docker, Jenkins, and AWS EC2 for production readiness.\n\n### **Backend Workflow**\n1. **Document Store**: Stores structured professional data (e.g., skills, projects, achievements).\n2. **Retriever**: Fetches relevant documents based on the query.\n3. **Prompt Builder**: Constructs dynamic prompts tailored to the query type.\n4. **Generator**: Uses a text generation model to produce well-formatted, bullet-pointed responses.\n\n### **Frontend Features**\n- Interactive UI for querying Kushagra's profile.\n- Real-time response rendering with React.\n- Mobile-responsive design using TailwindCSS.\n\n---\n\n## **Local Development**\n\nFollow these steps to set up the project locally:\n\n### **1. Clone the Repository**\n```bash\ngit clone https://github.com/YourUsername/RAG_Microservice.git\ncd RAG_Microservice\n```\n\n### **2. Set Up the Backend**\n```bash\npython -m venv venv\nsource venv/bin/activate  # On Windows use: .\\venv\\Scripts\\activate\npip install -r requirements.txt\n```\n\n### **3. Set Up the Frontend**\n```bash\ncd rag-frontend\nnpm install\n```\n\n### **4. Create Environment Variables**\nCopy the example `.env` file and update it with your configuration:\n```bash\ncp .env.example .env\n```\n\n### **5. Run the Application Locally**\n\n#### **Backend**\n```bash\nuvicorn rag_microservice.app:app --reload\n```\n\n#### **Frontend**\nIn a new terminal:\n```bash\ncd rag-frontend\nnpm start\n```\n\n---\n\n## **Docker Deployment**\n\nTo deploy the application using Docker:\n```bash\ndocker-compose up -d\n```\n\n---\n\n## **Production Deployment**\n\n1. Configure an AWS EC2 instance with the necessary environment.\n2. Set up Docker and Jenkins for continuous integration and deployment.\n3. Refer to the `deployment/README.md` file for step-by-step instructions.\n\n---\n\n## **Project Structure**\n\n```\nRAG_Microservice/\n├── rag_microservice/         # Backend code\n│   └── app.py                # Main FastAPI application\n├── rag-frontend/             # React frontend\n├── data/                     # Data directory\n├── docker-compose.yml        # Docker compose configuration\n├── Jenkinsfile               # CI/CD pipeline\n└── deployment/               # Deployment documentation and scripts\n```\n\n---\n\n## **Environment Variables**\n\n### **Backend**\n| Variable               | Description                                       |\n|------------------------|---------------------------------------------------|\n| `CORPUS_DOCUMENTS_PATH` | Path to the document corpus (e.g., professional details). |\n| `TEXT_EMBEDDING_MODEL`  | Model for text embeddings (e.g., `sentence-transformers/all-MiniLM-L6-v2`). |\n| `GENERATOR_MODEL`       | Model for text generation (e.g., `google/flan-t5-large`). |\n\n### **Frontend**\n| Variable                | Description                                   |\n|-------------------------|-----------------------------------------------|\n| `REACT_APP_API_URL`     | Backend API URL (e.g., `http://localhost:8000`). |\n\n---\n\n## **Features**\n\n### **Backend**\n- Implements RAG using Haystack components:\n  - **Document Splitter**: Chunks documents for better retrieval.\n  - **Retriever**: Retrieves relevant documents based on queries.\n  - **Prompt Builder**: Constructs dynamic prompts for generation.\n  - **Generator**: Produces well-structured responses.\n- Pre-processed corpus includes:\n  - Professional profile, achievements, and technical skills.\n  - Work experience and key projects.\n  - Contact information.\n\n### **Frontend**\n- Query interface with a clean, professional design.\n- Supports real-time query results.\n- Mobile-friendly and intuitive layout.\n\n---\n\n## **How It Works**\n\n1. **Ask a Question**: The user asks a question, such as \"What are Kushagra's skills?\".\n2. **Document Retrieval**: The system retrieves relevant sections from the document store.\n3. **Dynamic Prompt Creation**: A prompt is generated based on the question type.\n4. **Answer Generation**: The model generates a structured, bullet-pointed response.\n5. **Response Display**: The frontend displays the response in a user-friendly format.\n\n---\n\n## **Use Case for Recruiters**\n\n### **Objective**\nTo provide a **quick, reliable, and structured way** for recruiters to:\n- Understand Kushagra's professional profile.\n- Explore technical expertise and projects.\n- Gain insights into recent achievements and research focus.\n\n### **Advantages**\n- **Time-Saving**: Direct answers without the need to parse lengthy resumes.\n- **Structured Format**: Responses are concise and categorized for better readability.\n- **Accurate Data**: Fetches verified information only.\n\n### **Example Queries**\n- **\"Who is Kushagra Sikka?\"**\n  - Learn about Kushagra's current roles, educational background, and recent impact.\n- **\"What are Kushagra's achievements?\"**\n  - Understand his teaching impact, technical accomplishments, and academic recognition.\n- **\"Tell me about Kushagra's skills.\"**\n  - Explore his technical expertise with usage examples.\n\n---\n\n## **Contributing**\n\n1. Fork the repository.\n2. Create a feature branch:\n   ```bash\n   git checkout -b feature/AmazingFeature\n   ```\n3. Commit your changes:\n   ```bash\n   git commit -m 'Add AmazingFeature'\n   ```\n4. Push to the branch:\n   ```bash\n   git push origin feature/AmazingFeature\n   ```\n5. Open a Pull Request.\n\n---\n\n## **Contact**\n\nFor further queries or contributions, reach out to **Kushagra Sikka**:\n- **Email**: kushagrasikka@gmail.com\n- **Portfolio**: [kushagrasikka.com](https://www.kushagrasikka.com)\n- **GitHub**: [github.com/KushagraSikka](https://github.com/KushagraSikka)\n- **LinkedIn**: [linkedin.com/in/kushagrasikka](https://linkedin.com/in/kushagrasikka)\n"
    },
    {
      "name": "GivAlz/duckduckgo-api-haystack",
      "stars": 3,
      "img": "https://avatars.githubusercontent.com/u/9434511?s=40&v=4",
      "owner": "GivAlz",
      "repo_name": "duckduckgo-api-haystack",
      "description": "haystack integration for duckducko api into a websearch component",
      "homepage": null,
      "language": "Python",
      "created_at": "2024-09-02T13:13:44Z",
      "updated_at": "2025-04-23T06:56:04Z",
      "topics": [],
      "readme": "# DuckduckgoApiWebSearch\n\nHaystack component to use Websearch via the freely available Duckduckgo API.\n\n-----\n\n**Table of Contents**\n\n- [Installation](#Installation)\n- [Overview](#Overview)\n- [Basic Usage](#Basic_Usage)\n- [License](#License)\n\n## Installation\n\n```console\npip install duckduckgo-api-haystack\n```\n\n## Overview\n\nThis repository implements a module in the style of **SearchApiWebSearch**\nand **SerperDevWebSearch**, but using the freely-available duckduckgo API.\n\nWhen you give DuckduckgoWebSearch a query, it returns a list of the URLs most relevant to your search.\nIt uses page snippets (pieces of text displayed under the page title in search results) to find the answers,\nnot the whole pages.\n\n## Basic Usage\n\nHere's a simple example of how to use the `DuckduckgoApiWebSearch` component:\n\n```python\nfrom duckduckgo_api_haystack import DuckduckgoApiWebSearch\n\n# Create an instance of DuckduckgoApiWebSearch\nwebsearch = DuckduckgoApiWebSearch(top_k=3)\n\n# Perform a search\nresults = websearch.run(query=\"What is frico?\")\n\n# Access the search results\ndocuments = results[\"documents\"]\nlinks = results[\"links\"]\n\nprint(\"Found documents:\")\nfor doc in documents:\n    print(f\"Content: {doc.content}\")\n    print(\"\\n\\n\")\n\nprint(\"Search Links:\")\nfor link in links:\n    print(link)\n```\n\n### Configuration Parameters\n\nThe `DuckduckgoApiWebSearch` component accepts several parameters to customize its behavior:\n\n- `top_k (int, optional)`: Maximum number of documents to return (default: 10).\n- `max_results (int, optional)`: Maximum number of documents to consider in the search (default: 10).\n- `region (str)`: Search region (default: \"wt-wt\" for worldwide).\n- `safesearch (str)`: SafeSearch setting (\"on\", \"moderate\", or \"off\"; default: \"moderate\").\n- `timelimit (str, optional)`: Time limit for search results (e.g., \"d\" for day, \"w\" for week, \"m\" for month).\n- `backend (str)`: Search backend to use (\"auto\", \"html\", or \"lite\"; default: \"auto\").\n- `allowed_domain (str)`: Restrict search to a specific domain (default: \"\").\n- `timeout (int)`: Timeout for each search request in seconds (default: 10).\n- `use_answers (bool)`: Include DuckDuckGo's answer box in results (default: False).\n- `proxy (str, optional)`: Web address to use as a proxy.\n- `max_search_frequency (float, optional)`: Minimum time in seconds between searches (defaults to no limit)\n\nRemark: The difference between `top_k` and `max_results` is that, if `use_answers` is `True`, then the number of\nanswers and pages is considered together and only the `top_k` are then used. Otherwise they work in the same way.\n\nThe `top_k` and `max_results` parameters serve different purposes:\n\n`max_results`: This parameter determines the maximum number of search results to retrieve from DuckDuckGo.\n`top_k`: This parameter limits the number of results returned by the component.\n\nThe interaction between these parameters depends on the `use_answers` setting:\n\n- `use_answers=False`: The component retrieves up to max_results search results.\n- `use_answers=True`: The component returns the `top_k` results from a list containing answers and search results.\n\n## Rate Limitations\n\nToo many requests could cause the API to fail because of rate limitations; to fix this issue use a proxy or reduce \nthe frequency of the calls.\n\n## Python 3.8\n\nThe duckduckgo api package supports python version 3.8 only up to version 7.2.1, which will be installed by requirements.txt\nif python 3.8 is detected.\n\nSupport might end in the future.\n\n## License\n\n`duckduckgo-api-websearch` is distributed under the terms of the [Apache-2.0](https://spdx.org/licenses/Apache-2.0.html) license.\n\n"
    },
    {
      "name": "Hoanganhvu123/BookingGPT",
      "stars": 3,
      "img": "https://avatars.githubusercontent.com/u/89352572?s=40&v=4",
      "owner": "Hoanganhvu123",
      "repo_name": "BookingGPT",
      "description": "🌼 Daisy Hair Salon Booking: AI-powered scheduler using Python, LangChain, and Google's Gemini 1.5 Flash. Natural language processing for easy booking and cancellations. Google Calendar integration for efficient management. Streamline your salon's scheduling with this smart solution! 💇‍♀️✨",
      "homepage": null,
      "language": "Python",
      "created_at": "2024-09-04T14:07:26Z",
      "updated_at": "2024-10-09T14:29:06Z",
      "topics": [],
      "readme": "# Daisy Hair Salon Booking System 🌼💇‍♀️\r\n\r\nWelcome to the Daisy Hair Salon booking system! This is a smart application using AI to help customers easily book and manage appointments at our salon.\r\n\r\n## 🌟 Key Features\r\n\r\n- 🤖 Intelligent AI assistant to interact with customers\r\n- 📅 Integration with Google Calendar for appointment management\r\n- ⏰ Check and display available time slots\r\n- 📝 Create, view, and cancel appointments easily\r\n- 🗣️ Support for English communication\r\n- 😊 Friendly and natural communication with customers\r\n\r\n## 🛠️ Technologies Used\r\n\r\n- Python 3.11\r\n- LangChain for creating AI agent\r\n- Google Generative AI (Gemini 1.5 Flash model)\r\n- Google Calendar API for appointment management\r\n- Poetry for dependency management\r\n- Docker for containerization (if deployment is needed)\r\n\r\n## 🏗️ Project Structure\r\n\r\n```\r\nbookinggpt/\r\n├── agent/\r\n│   ├── __init__.py\r\n│   ├── booking_agent.py\r\n│   └── prompt.py\r\n├── tool/\r\n│   ├── __init__.py\r\n│   ├── available_event.py\r\n│   ├── cancel_event.py\r\n│   └── create_event.py\r\n├── __init__.py\r\n└── utils.py\r\ntests/\r\n├── __init__.py\r\n├── test_available_event.py\r\n├── test_cancel_event.py\r\n├── test_create_event.py\r\n├── test_output_parser.py\r\n└── test_tool_output.py\r\nmain.py\r\nREADME.md\r\n.env\r\n.gitignore\r\nDockerfile\r\npyproject.toml\r\nrequirements.txt\r\n```\r\n\r\n## 🚀 Installation and Usage\r\n\r\n1. Clone the repository:\r\n   ```bash\r\n   git clone https://github.com/your-username/daisy-hair-salon-booking.git\r\n   cd daisy-hair-salon-booking\r\n   ```\r\n\r\n2. Install dependencies:\r\n   ```bash\r\n   pip install -r requirements.txt\r\n   ```\r\n\r\n3. Configure environment variables in the `.env` file:\r\n   ```bash\r\n   GOOGLE_API_KEY=your_google_api_key_here\r\n   ```\r\n\r\n4. Run the application:\r\n   ```bash\r\n   python main.py\r\n   ```\r\n\r\n5. Interact with the AI assistant to book appointments, check available time slots, or cancel appointments.\r\n\r\n## 📝 Usage Instructions\r\n\r\n- To book an appointment: \"I want to book a haircut tomorrow at 2 PM\"\r\n- To check available time slots: \"Show me the available slots for this week\"\r\n- To cancel an appointment: \"I want to cancel the appointment with booking code ABC123\"\r\n\r\n## 💈 Our Services\r\n\r\n1. Hair wash (20 minutes)\r\n2. Haircut (30 minutes)\r\n3. Hair styling (30 minutes)\r\n4. Beard trim (15 minutes)\r\n5. Hair coloring (60 minutes)\r\n6. Hair treatment (45 minutes)\r\n7. Scalp massage (15 minutes)\r\n8. Eyebrow shaping (10 minutes)\r\n9. Facial care (45 minutes)\r\n10. Manicure (30 minutes)\r\n\r\n## 🤝 Contributions\r\n\r\nWe welcome all contributions to improve this project. Please create a pull request or report issues if you have any ideas or encounter any problems.\r\n\r\n## 📄 License\r\n\r\nThis project is distributed under the MIT License. See the `LICENSE` file for more details.\r\n\r\n## 📞 Contact\r\n\r\nIf you have any questions or suggestions, please contact us via email: hoanganvu933@gmail.com\r\n\r\n---\r\n\r\nThank you for your interest in the Daisy Hair Salon Booking System! We hope this application will provide you with a wonderful booking experience. 💖✨"
    },
    {
      "name": "ArmanTunga/5-levels-of-building-chatbot-apps",
      "stars": 3,
      "img": "https://avatars.githubusercontent.com/u/30978203?s=40&v=4",
      "owner": "ArmanTunga",
      "repo_name": "5-levels-of-building-chatbot-apps",
      "description": "In this repo, you can find the codes of my article series called \"5 Levels of Building Chatbot Apps with Haystack\" where in this series we cover starting from an introductory bot and progressing to more complex, context-aware conversational systems.",
      "homepage": null,
      "language": "Python",
      "created_at": "2024-08-28T16:46:14Z",
      "updated_at": "2025-02-22T01:40:36Z",
      "topics": [],
      "readme": "# 5 Levels of Building Chatbot Apps\n\nIn this repo, you can find the codes of my article series called \"5 Levels of Building Chatbot Apps with Haystack\" where in this series we cover starting from an introductory bot and progressing to more complex, context-aware conversational systems. Whether you’re new to chatbot development or looking to refine your expertise, these guides will provide practical insights into leveraging Haystack to create chatbots that can deliver engaging, dynamic interactions with well-designed systems.\n\n**You can access the code for each level by simply switching between branches.**\n\n## Level 1 Chatbot: Not Even a Chatbot!\n![Simple illustration of level 1 chatbot.. not even a chatbot](images/level_1/not-even-a-chatbot-illustration-min.jpg \"Simple illustration of level 1 chatbot.. not even a chatbot\")\n\nWalkthrough of Level 1 Chatbot: [Medium Article Link](https://medium.com/@armantunga/5-levels-of-building-chatbot-apps-with-haystack-level-1-d2ef2589715b)\n\n## Level 2 Chatbot: Chatbot with Memory and Retrieval\n![Simple illustration of level 2 chatbot.. chatbot with memory and retrieval](images/level_2/chatbot-with-memory-and-retrieval.jpg \"Simple illustration of level 2 chatbot.. chatbot with memory and retrieval\")\n\nWalkthrough of Level 2 Chatbot: [Medium Article Link](https://medium.com/@armantunga/5-levels-of-building-chatbot-apps-with-haystack-level-2-437a207ae784)\n\n## Level 3 Chatbot: Text2SQL Chatbot\n![Simple illustration of level 3 chatbot.. Text2SQL Chatbot](images/level_3/illustration-of-apps-flow-min.jpg \"Simple illustration of level 3 chatbot.. Text2SQL Chatbot\")\n\nWalkthrough of Level 3 Chatbot: [Medium Article Link](https://medium.com/@armantunga/5-levels-of-building-chatbot-apps-with-haystack-level-3-cabb386dc8de)"
    },
    {
      "name": "apify/apify-haystack",
      "stars": 3,
      "img": "https://avatars.githubusercontent.com/u/24586296?s=40&v=4",
      "owner": "apify",
      "repo_name": "apify-haystack",
      "description": "The official integration for Apify and Haystack 2.0",
      "homepage": "",
      "language": "Python",
      "created_at": "2024-07-24T12:22:13Z",
      "updated_at": "2025-02-28T18:54:47Z",
      "topics": [
        "apify",
        "haystack-ai",
        "rag"
      ],
      "readme": "# Apify-Haystack integration\n\n[![License](https://img.shields.io/badge/License-Apache_2.0-blue.svg)](https://github.com/apify/apify-haystack/blob/main/LICENSE)\n[![PyPi Package](https://badge.fury.io/py/apify-haystack.svg)](https://badge.fury.io/py/apify-haystack)\n[![Python](https://img.shields.io/pypi/pyversions/apify-haystack)](https://pypi.org/project/apify-haystack)\n\nThe Apify-Haystack integration allows easy interaction between the [Apify](https://apify.com/) platform and [Haystack](https://haystack.deepset.ai/).\n\nApify is a platform for web scraping, data extraction, and web automation tasks.\nIt provides serverless applications called Actors for different tasks, like crawling websites, and scraping Facebook, Instagram, and Google results, etc.\n\nHaystack offers an ecosystem of tools for building, managing, and deploying search engines and LLM applications.\n\n## Installation\n\nApify-haystack is available at the [`apify-haystack`](https://pypi.org/project/apify-haystack/) PyPI package.\n\n```sh\npip install apify-haystack\n```\n\n## Examples\n\n### Crawl a website using Apify's Website Content Crawler and convert it to Haystack Documents\n\nYou need to have an Apify account and API token to run this example.\nYou can start with a free account at [Apify](https://apify.com/) and get your [API token](https://docs.apify.com/platform/integrations/api).\n\nIn the example below, specify `apify_api_token` and run the script:\n\n```python\nfrom dotenv import load_dotenv\nfrom haystack import Document\n\nfrom apify_haystack import ApifyDatasetFromActorCall\n\n# Set APIFY_API_TOKEN here or load it from .env file\napify_api_token = \"\" or load_dotenv()\n\nactor_id = \"apify/website-content-crawler\"\nrun_input = {\n    \"maxCrawlPages\": 3,  # limit the number of pages to crawl\n    \"startUrls\": [{\"url\": \"https://haystack.deepset.ai/\"}],\n}\n\n\ndef dataset_mapping_function(dataset_item: dict) -> Document:\n    return Document(content=dataset_item.get(\"text\"), meta={\"url\": dataset_item.get(\"url\")})\n\n\nactor = ApifyDatasetFromActorCall(\n    actor_id=actor_id, run_input=run_input, dataset_mapping_function=dataset_mapping_function\n)\nprint(f\"Calling the Apify actor {actor_id} ... crawling will take some time ...\")\nprint(\"You can monitor the progress at: https://console.apify.com/actors/runs\")\n\ndataset = actor.run().get(\"documents\")\n\nprint(f\"Loaded {len(dataset)} documents from the Apify Actor {actor_id}:\")\nfor d in dataset:\n    print(d)\n```\n\n### More examples\n\nSee other examples in the [examples directory](https://github.com/apify/apify-haystack/blob/master/src/apify_haystack/examples) for more examples, here is a list of few of them\n\n- Load a dataset from Apify and convert it to a Haystack Document\n- Call [Website Content Crawler](https://apify.com/apify/website-content-crawler) and convert the data into the Haystack Documents\n- Crawl websites, retrieve text content, and store it in the `InMemoryDocumentStore`\n- Retrieval-Augmented Generation (RAG): Extracting text from a website & question answering <a href=\"https://colab.research.google.com/github/deepset-ai/haystack-cookbook/blob/main/notebooks/apify_haystack_rag.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n- Analyze Your Instagram Comments’ Vibe with Apify and Haystack  <a href=\"https://colab.research.google.com/github/deepset-ai/haystack-cookbook/blob/main/notebooks/apify_haystack_instagram_comments_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n\n## Support\n\nIf you find any bug or issue, please [submit an issue on GitHub](https://github.com/apify/apify-haystack/issues).\nFor questions, you can ask on [Stack Overflow](https://stackoverflow.com/questions/tagged/apify), in GitHub Discussions or you can join our [Discord server](https://discord.com/invite/jyEM2PRvMU).\n\n## Contributing\n\nYour code contributions are welcome.\nIf you have any ideas for improvements, either submit an issue or create a pull request.\nFor contribution guidelines and the code of conduct, see [CONTRIBUTING.md](https://github.com/apify/apify-haystack/blob/master/CONTRIBUTING.md).\n\n## License\n\nThis project is licensed under the Apache License 2.0 - see the [LICENSE](https://github.com/apify/apify-haystack/blob/master/LICENSE) file for details.\n"
    },
    {
      "name": "baldpanda/pydata-manchester-talk",
      "stars": 3,
      "img": "https://avatars.githubusercontent.com/u/37364932?s=40&v=4",
      "owner": "baldpanda",
      "repo_name": "pydata-manchester-talk",
      "description": "Repo for PyData Manchester Talk in July",
      "homepage": "",
      "language": "Jupyter Notebook",
      "created_at": "2024-05-02T21:06:17Z",
      "updated_at": "2025-01-25T12:10:42Z",
      "topics": [],
      "readme": "# PyData Manchester Talk - 30th July\n\nPlanning on giving a talk at PyData Manchester on July 30th. \n\n### Description of Talk\n\nHaving worked helping businesses adopt LLM workflows such as RAG pipelines and function calling, a common challenge is benchmarking and measuring how well the application is performing. This talk aims to explore some of the existing open-source tooling to help with this and look at some best practices when building out such applications.\n\n[Notebook](https://github.com/baldpanda/pydata-manchester-talk/blob/main/notebooks/pydata_manchester_demo.ipynb) used for talk\n\n### Getting Started \n\nHere are some steps for getting started with the repo:\n\n- Clone the repo to local\n\n- Create a virtual environment using Python 3.12: `python -m venv .venv`\n\n- Activate the virtual environment: `source .venv/bin/activate` (depending on OS)\n\n- Install `poetry` into the virtual environment: `pip install poetry`\n\n- Run `poetry install` to install dependencies and setup the local path\n\n- Download the data Google's [page](https://ai.google.com/research/NaturalQuestions/download). Notebook uses the simplified train set. Here's a [link](https://github.com/google-research-datasets/natural-questions) to the GitHub page, which describes the data in depth\n\n- Create a `.env` file and populate it with the following environment variables:\n    - `OPENAI_API_KEY` for usage of OpenAI's API\n    - `natural_questions_training_path` - used in the config for the location of where the natural questions training data path is set\n    - `prompt_template_path` - path to the prompt template\n\n\n\n### Useful Resources\n \n- [Benchmarking Haystack Pipelines](https://haystack.deepset.ai/blog/benchmarking-haystack-pipelines)\n- [Evaluating RAG Pipelines](https://haystack.deepset.ai/tutorials/35_evaluating_rag_pipelines)\n"
    },
    {
      "name": "avnlp/rrf",
      "stars": 3,
      "img": "https://avatars.githubusercontent.com/u/123800833?s=40&v=4",
      "owner": "avnlp",
      "repo_name": "rrf",
      "description": "Performance Evaluation of Rankers and RRF Techniques for Retrieval Pipelines",
      "homepage": "",
      "language": "Python",
      "created_at": "2024-06-24T13:06:26Z",
      "updated_at": "2024-08-21T03:06:57Z",
      "topics": [
        "diversity-ranker",
        "lost-in-the-middle-ranker",
        "ranker",
        "reciprocal-rank-fusion",
        "rrf",
        "similarity-ranker"
      ],
      "readme": "## Performance Evaluation of Rankers and RRF Techniques for Retrieval Pipelines\n\n**Paper:** [Performance Evaluation of Rankers and RRF Techniques for Retrieval Pipelines](paper/rankers_rrf.pdf)\n\nIn the intricate world of Long-form Question Answering (LFQA) and Retrieval Augmented Generation (RAG), making the most of the LLM’s context window is paramount. Any wasted space or repetitive content limits the depth and breadth of the answers we can extract and generate. It’s a delicate balancing act to lay out the content of the context window appropriately.\n\nWith the addition of three rankers, viz., Diversity Ranker, Lost In The Middle Ranker, Similarity Rankers and RRF techniques, we aim to address these challenges and improve the answers generated by the LFQA/RAG pipelines. We have done a comparative study of adding different combinations of rankers in a Retrieval pipeline and evaluated the results on four metrics, viz., Normalized Discounted Cumulative Gain (NDCG), Mean Average Precision (MAP), Recall and Precision.\n\nIn our study, we consider the following cases of retrieval:\n\n<img src=\"plots/pipelines_taxonomy.png\" alt=\"RAG Pipelines Taxonomy\" align=\"middle\" width=\"600\" height=\"300\">\n\nThe following rankers were used:\n\n- **Diversity Ranker:** The Diversity Ranker enhances the diversity of the paragraphs selected for the context window.\n\n- **Lost In The Middle Ranker:** The Lost In The Middle Ranker optimizes the layout of the selected documents in the LLM’s context window.\n\n- **Transformers Similarity Ranker:** The Transformers Similarity Ranker ranks Documents based on how similar they are to the query. It uses a pre-trained cross-encoder model to embed both the query and the Documents. It then compares the embeddings to determine how similar they are.\n\n**Dense Retrieval:**\n\nFor Dense retrieval, `INSTRUCTOR-XL` and `all-mpnet-base-v2` models were employed.\n\n<img src=\"plots/rankers_dense_pipeline.png\" alt=\"Dense Pipeline with Rankers\" align=\"middle\" width=\"550\" height=\"100\">\n\n**Hybrid Retrieval:**\n\nBM25 retrieval was used for Sparse retrieval in the Hybrid pipelines. The `bge-reranker-large` model was used in the Similarity Ranker, and `ms-marco-MiniLM-L-12-v2` for the Diversity Ranker.\n\n**Reciprocal Rank Fusion** (RRF) was used to combine the results for Hybrid retrieval.\n\n<img src=\"plots/rankers_hybrid_pipeline.png\" alt=\"Hybrid Pipeline with Rankers\" align=\"middle\" width=\"820\" height=\"230\">\n\n## Usage\n\nTo run the pipelines, you will need to clone this repository and install the required libraries.\n\n1. Install the `rrf` package:\n\n```bash\ngit clone https://github.com/avnlp/rrf\ncd rrf\npip install -e .\n```\n\n2. To add the data to an index in Pinecone using the INSTRUCTOR-XL embedding model:\n\n```python\ncd src/rrf/indexing_pipeline/fiqa\npython pinecone_instructor_index.py\n```\n\n3. To run a specific pipeline you will have to go that file path and then run the file.\nFor example, running the pipeline that uses dense retrieval with a combination of Diversity Ranker, Lost In The Middle Ranker and Similarity Ranker:\n\n```python\ncd src/rrf/pointwise/instructor_xl/fiqa/\npython dense_similarity_diversity_litm.py\n```\n\n## License\n\nThe source files are distributed under the [MIT License](https://github.com/avnlp/rrf/blob/main/LICENSE).\n"
    },
    {
      "name": "sausheong/programming_ai",
      "stars": 3,
      "img": "https://avatars.githubusercontent.com/u/5962?s=40&v=4",
      "owner": "sausheong",
      "repo_name": "programming_ai",
      "description": "Repository for scripts and code for Programming with AI",
      "homepage": null,
      "language": "Python",
      "created_at": "2024-05-31T02:34:44Z",
      "updated_at": "2024-09-19T00:53:04Z",
      "topics": [],
      "readme": "# Programming with AI\n\nThis is the repository of scripts and notes for the session on Programming with AI."
    },
    {
      "name": "PagalavanPagal66/Medical_chatbot",
      "stars": 3,
      "img": "https://avatars.githubusercontent.com/u/125946240?s=40&v=4",
      "owner": "PagalavanPagal66",
      "repo_name": "Medical_chatbot",
      "description": null,
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2024-02-21T09:32:01Z",
      "updated_at": "2024-05-06T17:02:38Z",
      "topics": [],
      "readme": "# Medical_chatbot"
    },
    {
      "name": "bytewax/real-time-job-posting",
      "stars": 3,
      "img": "https://avatars.githubusercontent.com/u/72483929?s=40&v=4",
      "owner": "bytewax",
      "repo_name": "real-time-job-posting",
      "description": null,
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2024-04-02T23:20:23Z",
      "updated_at": "2024-04-09T21:04:50Z",
      "topics": [],
      "readme": "# Extract and parse jobs in real time with Bytewax\n\nThis repository has a simple script to extract job listings from the job search API and parse the job descriptions to extract the required skills in real time. \n\n## Set up\n\n1. Create a virtual environment and install the requirements:\n\n```bash\nconda create -n bytewax-env python=3.10\nconda activate bytewax-env\npip install -r requirements.txt\n```\n\n2. Create your job search api key \n\nVisit https://rapidapi.com/letscrape-6bRBa3QguO5/api/jsearch and select the free plan. You will need to create an account to get the api key.\n\n3. Create a `source.env` file in the root directory of the project and add the following:\n\n```bash\napi_key = \"YOUR_API_KEY\"\n```\n\nAlternatively, you can set the environment variable `api_key` in your terminal.\n\n## Run the data flow\n\n```bash\npython dataflow.py\n```"
    },
    {
      "name": "EdAbati/dataframes-haystack",
      "stars": 3,
      "img": "https://avatars.githubusercontent.com/u/29585319?s=40&v=4",
      "owner": "EdAbati",
      "repo_name": "dataframes-haystack",
      "description": "Haystack custom components for your favourite dataframe library.",
      "homepage": "https://pypi.org/project/dataframes-haystack/",
      "language": "Jupyter Notebook",
      "created_at": "2024-03-26T18:18:10Z",
      "updated_at": "2025-04-16T10:23:25Z",
      "topics": [
        "ai",
        "dataframe",
        "haystack",
        "llm",
        "machine-learning",
        "nlp",
        "pandas",
        "polars",
        "python"
      ],
      "readme": "# Dataframes Haystack\n\n[![PyPI - Version](https://img.shields.io/pypi/v/dataframes-haystack)](https://pypi.org/project/dataframes-haystack)\n[![PyPI - Python Version](https://img.shields.io/pypi/pyversions/dataframes-haystack?logo=python&logoColor=white)](https://pypi.org/project/dataframes-haystack)\n[![PyPI - License](https://img.shields.io/pypi/l/dataframes-haystack)](https://pypi.org/project/dataframes-haystack)\n\n\n[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)\n[![Ruff](https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/astral-sh/ruff/main/assets/badge/v2.json)](https://github.com/astral-sh/ruff)\n\n[![GH Actions Tests](https://github.com/EdAbati/dataframes-haystack/actions/workflows/test.yml/badge.svg)](https://github.com/EdAbati/dataframes-haystack/actions/workflows/test.yml)\n[![pre-commit.ci status](https://results.pre-commit.ci/badge/github/EdAbati/dataframes-haystack/main.svg)](https://results.pre-commit.ci/latest/github/EdAbati/dataframes-haystack/main)\n\n-----\n\n- [📃 Description](#description)\n- [🛠️ Installation](#installation)\n- [💻 Usage](#usage)\n  - [DataFrameFileToDocument](#dataframefiletodocument)\n  - [`pandas` Converters](#pandas-converters)\n    - [FileToPandasDataFrame](#filetopandasdataframe)\n    - [PandasDataFrameConverter](#pandasdataframeconverter)\n  - [`polars` Converters](#polars-converters)\n    - [FileToPolarsDataFrame](#filetopolarsdataframe)\n    - [PolarsDataFrameConverter](#polarsdataframeconverter)\n- [🤝 Contributing](#contributing)\n\n-----\n\n## 📃 Description\n\n`dataframes-haystack` is an extension for [Haystack 2](https://docs.haystack.deepset.ai/docs/intro) that enables integration with dataframe libraries.\n\nThe dataframe libraries currently supported are:\n- [pandas](https://pandas.pydata.org/)\n- [Polars](https://pola.rs)\n\nThe library offers various custom [Converters](https://docs.haystack.deepset.ai/docs/converters) components to transform dataframes into Haystack [`Document`](https://docs.haystack.deepset.ai/docs/data-classes#document) objects:\n- `DataFrameFileToDocument` is a main generic converter that reads files using a dataframe backend and converts them into `Document` objects.\n- `FileToPandasDataFrame` and `FileToPolarsDataFrame` read files and convert them into dataframes.\n- `PandasDataFrameConverter` or `PolarsDataFrameConverter` convert data stored in dataframes into Haystack `Document`objects.\n\n`dataframes-haystack` supports reading files in various formats:\n- _csv_, _json_, _parquet_, _excel_, _html_, _xml_, _orc_, _pickle_, _fixed-width format_ for `pandas`. See the [pandas documentation](https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html) for more details.\n- _csv_, _json_, _parquet_, _excel_, _avro_, _delta_, _ipc_ for `polars`. See the [polars documentation](https://docs.pola.rs/api/python/stable/reference/io.html) for more details.\n\n## 🛠️ Installation\n\n```sh\n# for pandas\npip install \"dataframes-haystack[pandas]\"\n\n# for polars\npip install \"dataframes-haystack[polars]\"\n```\n\n## 💻 Usage\n\n> [!TIP]\n> See the [Example Notebooks](./notebooks) for complete examples.\n\n### DataFrameFileToDocument\n\n[Complete example](https://github.com/EdAbati/dataframes-haystack/blob/main/notebooks/dataframe-file-to-doc-example.ipynb)\n\nYou can leverage both `pandas` and `polars` backends (thanks to [`narwhals`](https://github.com/narwhals-dev/narwhals)) to read your data!\n\n```python\nfrom dataframes_haystack.components.converters import DataFrameFileToDocument\n\nconverter = DataFrameFileToDocument(content_column=\"text_str\")\ndocuments = converter.run(files=[\"file1.csv\", \"file2.csv\"])\n```\n\n```python\n>>> documents\n{'documents': [\n    Document(id=0, content: 'Hello world', meta: {}),\n    Document(id=1, content: 'Hello everyone', meta: {})\n]}\n```\n\n### `pandas` Converters\n\n[Complete example](https://github.com/EdAbati/dataframes-haystack/blob/main/notebooks/pandas-example.ipynb)\n\n#### FileToPandasDataFrame\n\n```python\nfrom dataframes_haystack.components.converters.pandas import FileToPandasDataFrame\n\nconverter = FileToPandasDataFrame(file_format=\"csv\")\n\noutput_dataframe = converter.run(\n    file_paths=[\"data/doc1.csv\", \"data/doc2.csv\"]\n)\n```\n\nResult:\n```python\n>>> output_dataframe\n{'dataframe': <pandas.DataFrame>}\n```\n\n#### PandasDataFrameConverter\n\n```python\nimport pandas as pd\n\nfrom dataframes_haystack.components.converters.pandas import PandasDataFrameConverter\n\ndf = pd.DataFrame({\n    \"text\": [\"Hello world\", \"Hello everyone\"],\n    \"filename\": [\"doc1.txt\", \"doc2.txt\"],\n})\n\nconverter = PandasDataFrameConverter(content_column=\"text\", meta_columns=[\"filename\"])\ndocuments = converter.run(df)\n```\n\nResult:\n```python\n>>> documents\n{'documents': [\n    Document(id=0, content: 'Hello world', meta: {'filename': 'doc1.txt'}),\n    Document(id=1, content: 'Hello everyone', meta: {'filename': 'doc2.txt'})\n]}\n```\n\n### `polars` Converters\n\n[Complete example](https://github.com/EdAbati/dataframes-haystack/blob/main/notebooks/polars-example.ipynb)\n\n#### FileToPolarsDataFrame\n\n```python\nfrom dataframes_haystack.components.converters.polars import FileToPolarsDataFrame\n\nconverter = FileToPolarsDataFrame(file_format=\"csv\")\n\noutput_dataframe = converter.run(\n    file_paths=[\"data/doc1.csv\", \"data/doc2.csv\"]\n)\n```\n\nResult:\n```python\n>>> output_dataframe\n{'dataframe': <polars.DataFrame>}\n```\n\n#### PolarsDataFrameConverter\n\n```python\nimport polars as pl\n\nfrom dataframes_haystack.components.converters.polars import PolarsDataFrameConverter\n\ndf = pl.DataFrame({\n    \"text\": [\"Hello world\", \"Hello everyone\"],\n    \"filename\": [\"doc1.txt\", \"doc2.txt\"],\n})\n\nconverter = PolarsDataFrameConverter(content_column=\"text\", meta_columns=[\"filename\"])\ndocuments = converter.run(df)\n```\n\nResult:\n```python\n>>> documents\n{'documents': [\n    Document(id=0, content: 'Hello world', meta: {'filename': 'doc1.txt'}),\n    Document(id=1, content: 'Hello everyone', meta: {'filename': 'doc2.txt'})\n]}\n```\n\n## 🤝 Contributing\n\nDo you have an idea for a new feature? Did you find a bug that needs fixing?\n\nFeel free to [open an issue](https://github.com/EdAbati/dataframes-haystack/issues) or submit a PR!\n\n### Setup development environment\n\nRequirements: [`hatch`](https://hatch.pypa.io/latest/install/), [`pre-commit`](https://pre-commit.com/#install)\n\n1. Clone the repository\n1. Run `hatch shell` to create and activate a virtual environment\n1. Run `pre-commit install` to install the pre-commit hooks. This will force the linting and formatting checks.\n\n### Run tests\n\n- Linting and formatting checks: `hatch run lint:fmt`\n- Unit tests: `hatch run test-cov-all`\n\n## ✍️ License\n\n`dataframes-haystack` is distributed under the terms of the [MIT](https://spdx.org/licenses/MIT.html) license.\n"
    },
    {
      "name": "contextco/context-haystack",
      "stars": 3,
      "img": "https://avatars.githubusercontent.com/u/133496075?s=40&v=4",
      "owner": "contextco",
      "repo_name": "context-haystack",
      "description": "The official integration for Context.ai and Haystack 2.0",
      "homepage": null,
      "language": "Python",
      "created_at": "2024-02-09T11:44:06Z",
      "updated_at": "2024-11-16T19:42:32Z",
      "topics": [],
      "readme": "# Context Haystack\n\n[![License](https://img.shields.io/badge/license-Apache%202.0-blue.svg)](https://github.com/contextco/context-haystack/main/LICENSE)\n[![Python Version](https://img.shields.io/badge/python-%3E%3D3.9-blue)](https://www.python.org/downloads/release/python-390/)\n\n## Installation\n\nYou can install Context Haystack using pip:\n\n```shell\npip install context-haystack\n```\n\n## About\n\nContext Haystack is the offical Python integration for [context.ai](https://context.ai/) with Haystack 2.0. You can use the Context Analytics component to log messages to Context. See your analytics dashboard at [with.context.ai](https://with.context.ai/).\n"
    },
    {
      "name": "rti/gbnc",
      "stars": 3,
      "img": "https://avatars.githubusercontent.com/u/80712?s=40&v=4",
      "owner": "rti",
      "repo_name": "gbnc",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2024-01-25T16:36:44Z",
      "updated_at": "2024-02-04T19:28:23Z",
      "topics": [],
      "readme": "# Naïve Infrastructure for a GB&C project\n\n**Warning** This is a prototype for development only. No security considerations have been made. All services run as root!\n\n## Getting started\n\n### Locally\n\nTo build and run the container locally with hot reload on python files do:\n```\nDOCKER_BUILDKIT=1 docker build . -t gbnc\ndocker run  \\\n  --env HUGGING_FACE_HUB_TOKEN=$HUGGING_FACE_HUB_TOKEN \\\n  --volume \"$(pwd)/gswikichat\":/workspace/gswikichat \\\n  --volume gbnc_cache:/root/.cache \\\n  --publish 8000:8000 \\\n  --rm \\\n  --interactive \\\n  --tty \\\n  --name gbnc \\\n  gbnc\n```\nPoint your browser to http://localhost:8000/ and use the frontend.\n\n### Runpod.io\n\nThe container works on [runpod.io](https://www.runpod.io/) GPU instances. A [template is available here](https://runpod.io/gsc?template=0w8z55rf19&ref=yfvyfa0s).\n\n### Local development\n#### Backend\n```\npython -m venv .venv\n. ./.venv/bin/activate\npip install -r requirements.txt\n```\n#### Frontend\n```\ncd frontend\nyarn dev\n```\n\n## What's in the box\n\n### Docker container\n\nOne container running all the components. No separation to keep it simple. Based on [Nvidia CUDA containers](https://hub.docker.com/r/nvidia/cuda) in order to support GPU acceleration. Small models work on laptop CPUs too (tested i7-1260P).\n\n### Ollama inference\n\nThe container runs [Ollama](https://ollama.ai/) for LLM inference. Will probably not scale enough when run as a service for multiple users, but enough for testing.\n\n### Phi2 LLM\n\nThe [Microsoft Phi2 2.7B](https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/) model is run by default. The model runs locally using Ollama. Can be switched with the `MODEL` docker build arg.\n\n### Haystack RAG Framework\n\nThe [Haystack RAG framework](https://haystack.deepset.ai/) is used to implement Retrieval Augmented Generation on a minimal test dataset.\n\n### API\n\nA [FastAPI](https://fastapi.tiangolo.com/) server is running in the container. It exposes an API to receive a question from the frontend, runs the Haystack RAG and returns the response.\n\n### Frontend\n\nA minimal frontend lets the user input a question and renders the response from the system.\n\n"
    },
    {
      "name": "hassanjawwad12/text-to-image-search",
      "stars": 2,
      "img": "https://avatars.githubusercontent.com/u/113373773?s=40&v=4",
      "owner": "hassanjawwad12",
      "repo_name": "text-to-image-search",
      "description": "Text to Image Search AI App using Haystack and CLIP",
      "homepage": null,
      "language": "Python",
      "created_at": "2025-03-11T08:56:25Z",
      "updated_at": "2025-03-11T13:02:46Z",
      "topics": [],
      "readme": "# text-to-image-search\nText to Image Search AI App using Haystack and CLIP\n\n* Create a fashion clothing text-to-image search app using Streamlit as the user interface. \n* Harness the power of `Vision Transformers (VIT)`, `CLIP`, and the `Haystack` framework to build this innovative application. \n* Simply input your fashion query, such as \"denim jackets,\" and watch as the app fetches and displays three relevant denim jacket images. \n* Run it using `streamlit run app.py`\n* You might also need to run `pip install setuptools quantulum3` if u bump into any error\n\n### [Vision Transformers (VIT) CLIP](https://huggingface.co/sentence-transformers/clip-ViT-B-32)\n### [Haystack](https://haystack.deepset.ai/)"
    },
    {
      "name": "tl2309/SRAG",
      "stars": 2,
      "img": "https://avatars.githubusercontent.com/u/144661509?s=40&v=4",
      "owner": "tl2309",
      "repo_name": "SRAG",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2025-02-26T13:50:35Z",
      "updated_at": "2025-04-08T12:09:39Z",
      "topics": [],
      "readme": "# SRAG\n\n**SRAG pipeline**\n<div align=\"center\">\n  <img src=\"assets/pipeline.png\" width=\"65%\" height=\"65%\"/>\n</div>\n\n**SRAG Paper:** \nhttps://arxiv.org/abs/2503.01346\n\n**MEBench Paper:** \nhttps://arxiv.org/abs/2502.18993\n"
    },
    {
      "name": "MubasharSiddique/LLM-YouTube-Transcript-Summarizer",
      "stars": 2,
      "img": "https://avatars.githubusercontent.com/u/35160292?s=40&v=4",
      "owner": "MubasharSiddique",
      "repo_name": "LLM-YouTube-Transcript-Summarizer",
      "description": "This YouTube Video Summarization App is built using open-source LLMs and frameworks like Llama 2, Haystack, Whisper, and Streamlit. It efficiently runs on a CPU, leveraging the Llama 2 model in GGUF format, loaded via Llama.cpp for seamless performance.",
      "homepage": null,
      "language": "Python",
      "created_at": "2024-10-09T07:59:29Z",
      "updated_at": "2025-03-06T08:17:01Z",
      "topics": [],
      "readme": "# LLM-YouTube-Transcript-Summarization-App\nThis YouTube Video Summarization App is built using open-source LLMs and frameworks like Llama 2, Haystack, Whisper, and Streamlit. It efficiently runs on a CPU, leveraging the Llama 2 model in GGUF format, loaded via Llama.cpp for seamless performance.\n\n## 📝 Description:\nYouTube Video Summarization App, is a powerful and customizable tool at your disposal, capable of automatically summarizing YouTube videos. \n\n## 💫Requirements:\n\n- **🔍 Haystack: Your AI-Powered Search Engine**\nHaystack is a versatile framework that allows you to harness the power of Generative AI to efficiently search, extract, and summarize information from vast amounts of text data. \n\n- **🤖 Llama 2: The AI Brain**\nMeet Llama 2, a massive language model that will assist you in understanding and summarizing the content of YouTube videos. You'll learn how to leverage Llama 2's language capabilities to extract key insights from video transcripts. That too 32K context length model in the GGUF format.\n\n- **🗣️ Whisper: Transforming Speech to Text**\nWhisper, a state-of-the-art automatic speech recognition (ASR) model, will be your go-to tool for transcribing spoken content from your YouTube videos. I'll show you how to integrate Whisper from Haystack inbuilt class seamlessly into your application, enabling it to work with both spoken and textual data.\n\n- **🚀 Streamlit: The User-Friendly Interface**\nStreamlit is the secret sauce that ties it all together. With its user-friendly interface design, you can effortlessly create a visually appealing front end for your YouTube Video Summarization App. We'll guide you through building an intuitive interface that allows users to interact with your app easily.\n\n## 🌟Implementation Guide:\n[Demo ▶️](https://www.youtube.com/watch?v=K9mDAb2Lz6Y)\n\n## 🔗Other Links:\n- Haystack: https://haystack.deepset.ai/\n- Llama 2 32K Model: https://huggingface.co/togethercomputer/LLaMA-2-7B-32K\n- Llama 2 32K GGUF Model: 32K-Instruct-GGUF🦌\n\n\n---\n## ©️ License 🪪 \n\nDistributed under the MIT License. See `LICENSE` for more information.\n\n---\n\n#### **If you like this LLM Project do drop ⭐ to this repo**\n#### Follow me on [![LinkedIn](https://img.shields.io/badge/linkedin-%230077B5.svg?style=for-the-badge&logo=linkedin&logoColor=white)](https://pk.linkedin.com/in/hm-mubashar) &nbsp; [![GitHub](https://img.shields.io/badge/github-%23121011.svg?style=for-the-badge&logo=github&logoColor=white)](https://github.com/MubasharSiddique)\n\n---\n"
    },
    {
      "name": "nutco21ris/ai-magic-recipe-generator",
      "stars": 2,
      "img": "https://avatars.githubusercontent.com/u/130265036?s=40&v=4",
      "owner": "nutco21ris",
      "repo_name": "ai-magic-recipe-generator",
      "description": null,
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2024-08-30T22:20:49Z",
      "updated_at": "2024-09-06T06:00:53Z",
      "topics": [],
      "readme": "\nTo unzip zipped file: gunzip magical_recipes_50000.csv.gz\n"
    },
    {
      "name": "Rishav-Paramhans/Video_Summarization_Using_AI",
      "stars": 2,
      "img": "https://avatars.githubusercontent.com/u/65668108?s=40&v=4",
      "owner": "Rishav-Paramhans",
      "repo_name": "Video_Summarization_Using_AI",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2024-05-09T17:54:28Z",
      "updated_at": "2025-01-23T07:47:45Z",
      "topics": [],
      "readme": "# Video_Summarization_Using_AI\n\n## Model information\nLlaMa-2-7B-32 instruct model from Hugging Face - intial plan. Here the 32K context size is important as I am be dealing with the bigger videos\n\n\n## Streamlit-login POC\n\nUses the newly released st.switch_page and the yet-to-be-relased st.page_link to show how you can create an app that:\n1. Only shows the login page if you're not logged in\n2. Redirects to the login page if you try to visit another url\n3. Shows custom page links with emojis in the sidebar once you're logged in\n\n\n"
    },
    {
      "name": "dmitrimahayana/Py-Haystack-Semantic",
      "stars": 2,
      "img": "https://avatars.githubusercontent.com/u/10221496?s=40&v=4",
      "owner": "dmitrimahayana",
      "repo_name": "Py-Haystack-Semantic",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2024-01-28T13:35:03Z",
      "updated_at": "2025-02-03T12:51:35Z",
      "topics": [],
      "readme": "﻿# Py-Haystack-Semantic\r\n"
    },
    {
      "name": "shortfastgood/AI-Lab",
      "stars": 2,
      "img": "https://avatars.githubusercontent.com/u/615499?s=40&v=4",
      "owner": "shortfastgood",
      "repo_name": "AI-Lab",
      "description": "The AI laboratory is a repository for AI API evaluation.",
      "homepage": "",
      "language": "Jupyter Notebook",
      "created_at": "2012-06-22T16:45:33Z",
      "updated_at": "2025-03-29T13:29:03Z",
      "topics": [],
      "readme": "# AI-Lab\n\n## Table of Contents\n\n- [Introduction](#introduction)\n- [Technology Adoption Risks](#technology-adoption-risks)\n- [Chat](#chat)\n  - [ChatGPT](#chatgpt)\n  - [Gemini](#gemini)\n  - [Claude](#claude)\n  - [Deepseek](#deepseek)\n- [Digital Assistant](#digital-assistant)\n  - [GitHub Copilot](#github-copilot)\n  - [Vibe Coding Tools](#vibe-coding-tools)\n  - [Prompt Engineering](#prompt-engineering)\n    - [Prompt Engineering Subsection](prompts/prompts.md)\n      - [ART with ChatGPT](prompts/ART/art_with_chat_gpt.md)\n      - [Image functions](prompts/image/image_functions.md)\n      - [Translator Functions](prompts/translator/translator_functions.md)\n  - [Embedding LLMs into Applications](./llm-lab/readme.md)\n\n## Introduction\n\nThis project aims to analyze the advantages and side effects of digital assisted work.\n\nInitially, it is a collection of experiences using GitHub's Copilot and OpenAI's ChatGPT.\n\n\n\nThen, in September 2023, the exploratory phase began for local LLM usage in applications that could not rely on cloud data storage. \nUntil early 2025, my research had not been very successful, as it inevitably ended in resource-intensive tests with various models \ncapable of operating on limited resources.\n\nWith the release of the deepseek-r1 model, things began to change. The 14b and 32b variants ran well on a standard notebook, \nthough patience was still required to be compared to online models. Nonetheless, it was now possible to begin working with them.\n\n## Technology Adoption Risks\n\nThe technology associated with LLM will not disappear, and thus ignoring it means still running the risk of being excluded \nfrom those emerging sectors and being penalized by those that will inevitably become obsolete.\n\nIn 2025, those who do not adopt AI face the greatest risks. In various software development sectors, AI is rapidly advancing, \ndrastically reducing reliance on keyboards or mice and shifting developers toward more creative endeavours.\n\nThe significant risk for businesses is the potential emergence of a startup offering the same project in a shorter timeframe \nand at lower costs, all without compromising quality.\n\nTo counter this threat, organizations must plan to encourage developers to leave the secure environment of traditional IDEs \nin favor of programming tools that rely on natural language. This approach presents a genuine challenge for those unaccustomed \nto \"speaking\" directly to their computer.\n\nAdoption also entails concrete risks, well characterized by a [specific section of OWASP](https://owasp.org/www-project-top-10-for-large-language-model-applications/).\n\nHowever, the risks highlighted by OWASP are increasingly mitigated by the option to deploy models locally in a protected and \nsecure environment.\n\n## Chat\nThe Chat section is dedicated to the use of LLMs in the form of chatbots.\n\nFor various reasons, I use ChatGPT, even though I have no direct affiliation with OpenAI. Consequently, \nmy knowledge of other solutions is sometimes limited and occasionally indirect (drawn from available literature).\n\n### ChatGPT\n[ChatGPT](https://chat.openai.com) was the first online service that enabled interaction with a large language model. \nIt underwent significant development and remains a leading choice among moderately priced offerings ($20/month).\n\n### Gemini\n[Gemini](https://gemini.google.com) replaced Google's early attempts with Bard. The current service is competitive with ChatGPT, \nincluding in terms of cost (approximately $22/month).\n\n### Claude\n[Claude](https://claude.ai) is a product of Anthropic, a company founded by former OpenAI employees. \nCompetitive with both ChatGPT and Gemini, the price is 20$/month.\n\n### Deepseek\n[Deepseek](https://chat.deepseek.com) proposes a new kind of model with advanced features that surpass competing solutions. \nAt present, the chat requires no subscription and can be used entirely free of charge.\n\n## Digital Assistant\n\nThe digital assistant is an interactive agent that assists users in carrying out actions or creating solutions in the digital realm.\n\nI started with GitHub Copilot, which I have not completely abandoned yet, but it is time to move forward. One option would \nhave been to replace IntelliJ/Visual Studio Code + Copilot with [Cursor](https://www.cursor.com) or [Windsurf](ttps://codeium.com/windsurf). \nBoth of these editors offer the ability to program with suggestions, as before, or to delegate code management to the agent \nand program using natural language commands.\n\nI have opted for a more radical approach and decided to adopt the \"[vibe coding](https://en.wikipedia.org/wiki/Vibe_coding)\" technique, \nwith as few compromises as possible.\n\n### GitHub Copilot\nGitHub Copilot was originally conceived as an IDE extension. It works effectively as a plugin in both IntelliJ and \nVisual Studio Code, providing a specialized programming chat that can anticipate a developer’s intentions and suggest \nentire code snippets. After two years of working with Copilot, I can confidently say that I have saved 40% to 70% \nof the time compared to the basic autocomplete features IDEs offered before the advent of AI. \n\n### Vibe Coding Tools\n\nsee [Vibe Coding Section](./vibe-coding/README.md) for more details. If you aren't familiar with prompting you should read the\n[Prompt Engineering Guide](https://www.promptingguide.ai/) first.\n\n### Prompt Engineering\n\nPrompt engineering is new discipline that emerges from the enhanced capabilities of services based on LLMs to understand natural languages and correctly \ninterpret the context of a request. In this regard, [Prompt Engineering Guide](https://www.promptingguide.ai/) offers an excellent guide for those who \nwant to venture into this new profession.\n\nSee [Prompt Engineering Section](prompts/prompts.md) for examples.\n"
    },
    {
      "name": "0xRy4n/airistotle",
      "stars": 2,
      "img": "https://avatars.githubusercontent.com/u/4227041?s=40&v=4",
      "owner": "0xRy4n",
      "repo_name": "airistotle",
      "description": "Airistotle is a basic implementation of the OpenAI Assistants API for making simple programmatic AI assistants.",
      "homepage": "",
      "language": "Python",
      "created_at": "2023-12-01T23:07:30Z",
      "updated_at": "2024-04-19T01:35:32Z",
      "topics": [
        "ai-chat",
        "assistants-api",
        "chatbot",
        "gpt",
        "openai",
        "slack-bot"
      ],
      "readme": "# Airistotle\n\nBasic implementation of the assistants API. Supports easy extensibility with function calls.\n\nSee https://platform.openai.com/assistants to setup an assistant.\n\n# Usage\n\nFirst, copy the `.env.template` to `.env` and set any necessary variables. All `Airistotle` variables are required.\nDepending on what interfaces you use and what plugins you have enabled, you may need to set additional environment variables.\n\n\n```python\nfrom airistotle import Assistant\nfrom airsitotle import settings\n\nassistant = Assistant(\n    openai_api_key=settings.OPENAI_API_KEY,\n    assistant_id=settings.ASSISTANT_ID\n)\n\nassistant.send_message(\"What is 2+2?\")\nresponse = assistant.get_response()\n\nprint(response)\n\n>>> 2+2 equals 4.\n```\n\nFor the CLI interface, simply import it from an interactive shell:\n\n```python\nfrom airistotle.interfaces import cli\n```\n\n## Plugins\n\nPlugins are defined in `airistotle.plugins`.\n\nPlugins can easily be added by extending the `BasePlugin` class in `airistotle.plugins.base`. The only expectation is that all plugin classes will\nimplement a `run()` method will return a string (the result of the function call). Converting a more complex data structure like a dict or list into a string will work fine.\n\nIt is also expected your Plugin Class will have a class attribute called `name` which corresponds to the function name you defined in your assistants function definition.\n\nThe arguments of your `run` method do not matter as long as they match the parameters you specified in your assistant's function definition.\n\nHere is an example of the function definition for the `WebSearch` plugin:\n\n```json\n{\n  \"name\": \"web_search\",\n  \"description\": \"Perform a web search and retrieve the contents from the top results. Use this when ever you are unsure of an answer and need more information. Use it to supplement your own knowledge, not replace it.\",\n  \"parameters\": {\n    \"type\": \"object\",\n    \"properties\": {\n      \"query\": {\n        \"type\": \"string\",\n        \"description\": \"The query you want to search for.\"\n      }\n    },\n    \"required\": [\n      \"query\"\n    ]\n  }\n}\n```\n\nWhen called, the assistant class will unpack the keyword argument list generated by the OpenAI Assistant and pass it to the run argument. In the above example, it's expected that the run function would have a single parameter `query` which would except a `str`. \n\n### Enabling Plugins\n\nYou can enable plugins in the `settings.py` file. \n\n```python\n\nfrom .plugins import YourPlugin\n\nAVAILABLE_PLUGINS = {\n    YourPlugin.name:YourPlugin(),\n}\n\n```\n\nAs long as there is a corresponding function definition in your assistant (see https://platform.openai.com/assistants), this will allow the Assistant class to process the action when the function call is requested. Keep in mind, it's up to the discretion of the OpenAI Assistant to determine when to call functions, and this is influenced both by system prompt and by function description.\n\n## Interfaces\n\nSome pre-written interfaces, such as a Slack Interface, may be configured in the `airistotle.interfaces` directory. \n\n\n#### Notes\n\nAiristotle uses Haystack for a basic web_search function, which is incredibly overkill. You can remove this function if you'd like to avoid the Haystack dependency. Alternatively, you can use Haystack for additional extensibility via plugins."
    },
    {
      "name": "dtaivpp/opensearch-conversational-search",
      "stars": 2,
      "img": "https://avatars.githubusercontent.com/u/17506770?s=40&v=4",
      "owner": "dtaivpp",
      "repo_name": "opensearch-conversational-search",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2023-11-27T16:43:21Z",
      "updated_at": "2023-12-01T23:13:30Z",
      "topics": [],
      "readme": "# Conversation Architecting Toolkit\n\nWelcome to what I am dubbing as the Conversation Architecting Toolchain for OpenSearch (CAT for short). Why not call this Retrevial Augmented Generation (RAG) like everyone else? Because that is merely one of the tools used in a conversation architecture. These systems have far more moving parts than the term RAG can cover.\n\n\n## Pre-requisites\n\nLets dive into the demo! First you will need to spin up a OpenSearch and OpenSearch Dashboards container on your local machine. We've included a docker compose file that can be started in this repo by running. \n\n```bash\ndocker compose up -d\n```\n\nOnce those have started you should be able to access OpenSearch Dashboards by going to: [https://localhost:5601](https://localhost:5601) in your web browser. The login is going to be **username:** `admin` and **password:** `admin`. Once in select dismiss and dismiss. Then you can go to **Dev Tools** in the upper right section of the screen.\n\nAdditionaly, this demo assumes you have a **production** API key for [Cohere](https://cohere.com/) and an API key for [OpenAI](https://openai.com/). Please note a production key for Cohere will incur costs and it's only recomennded that you proceed if you are aware with what those costs will be. Running the demo at the time of me writing this incurred $0.07 USD however, your experience may be different. \n\n\n## Cluster settings\n\nThere are a few [cluster settings](https://opensearch.org/docs/latest/ml-commons-plugin/cluster-settings/) we need to apply to get started. These specify where models can run (on any node), what model URL's are accessible (Cohere and OpenAI), and turns on the experimental RAG and Conversational Memory Feature. \n\n```\nPUT /_cluster/settings\n{\n    \"persistent\": {\n        \"plugins.ml_commons.allow_registering_model_via_url\": true,\n        \"plugins.ml_commons.only_run_on_ml_node\": false,\n        \"plugins.ml_commons.connector_access_control_enabled\": true,\n        \"plugins.ml_commons.model_access_control_enabled\": true,\n        \"plugins.ml_commons.memory_feature_enabled\": true,\n        \"plugins.ml_commons.rag_pipeline_feature_enabled\": true,\n        \"plugins.ml_commons.trusted_connector_endpoints_regex\": [\n          \"^https://api\\\\.cohere\\\\.ai/.*$\",\n          \"^https://api\\\\.openai\\\\.com/.*$\"\n        ]\n    }\n}\n```\n\n\n## Create model group\n\nAfter we have applied the cluster settings we will need a [model group](https://opensearch.org/docs/latest/ml-commons-plugin/model-access-control/#model-groups). This controlls which accounts can access our models. \n\n> Note, several of these steps have a `#` at the end followed by a constant looking name. We will use these to track our parameters for this demo. After you run the step you should track the ouput ID in these as we will need to paste them into other steps later.\n\n```\nPOST /_plugins/_ml/model_groups/_register\n{\n    \"name\": \"Model_Group\",\n    \"description\": \"Public ML Model Group\",\n    \"access_mode\": \"public\"\n}\n# MODEL_GROUP_ID: \n```\n\n\n## Register and deploy the text embedding model\n\nNow that we have a model group we should probably take a step back and talk about why we are even using machine learning (ML) models in the first place and... which models? The type of ML models we turn text into an embedding (an array of floating point numbers). \n\nThese embeddings can be used for several different things such as classifying documents. We are interested in a type of embeddings that are used to calculate \"semantic similarity\". To put it plainly, semantic simiarity, is a way of measuring how closeley two documents may be related. For example, while \"He used his golf driver\" and \"He was a F1 driver\" might have a lot of the same words we know that they are talking about extremely different topics.\n\nI won't be able to cover all the different embeddings what I can say is we are going to be using [Cohere's Embed-v3](https://txt.cohere.com/introducing-embed-v3/) model.\n\n> Remember that `MODEL_GROUP_ID` we saved earlier? This is where we will need it. Replace the `<MODEL_GROUP_ID>` in the below section with the actual ID. This will look something like the following: ` \"model_group_id\": \"w4oria34roa9ajo4jiaj\",`. Do the same with your Cohere production key. Don't forget to save the output of this step in the `EMBEDDING_MODEL_ID` below!\n\n```\nPOST /_plugins/_ml/models/_register?deploy=true\n{\n    \"name\": \"embed-english-v3.0\",\n    \"function_name\": \"remote\",\n    \"description\": \"Cohere model for embedding\",\n    \"model_group_id\": \"<MODEL_GROUP_ID>\",\n    \"connector\": {\n      \"name\": \"Cohere Connector\",\n      \"description\": \"Cohere model for Embeddings\",\n      \"version\": \"1.0\",\n      \"protocol\": \"http\",\n      \"credential\": {\n             \"cohere_key\": \"<COHERE_KEY>\"\n         },\n      \"parameters\": {\n        \"model\": \"embed-english-v3.0\",\n        \"truncate\": \"END\"\n      },\n      \"actions\": [{\n         \"action_type\": \"predict\",\n         \"method\": \"POST\",\n         \"url\": \"https://api.cohere.ai/v1/embed\",\n         \"headers\": {\n                 \"Authorization\": \"Bearer ${credential.cohere_key}\"\n             },\n  \t\t\t\"request_body\": \"{ \\\"texts\\\": ${parameters.texts}, \\\"truncate\\\": \\\"${parameters.truncate}\\\", \\\"model\\\": \\\"${parameters.model}\\\", \\\"input_type\\\": \\\"search_document\\\" }\",\n  \t\t\t\"pre_process_function\": \"connector.pre_process.cohere.embedding\",\n\t\t\t  \"post_process_function\": \"connector.post_process.cohere.embedding\"\n     }]\n  }\n}\n# EMBEDDING_MODEL_ID:\n```\n\n\n## Create ingestion pipeline\n\nAfter we have our embedding model registered we can create our ingestion pipeline. This pipeline will allow us to ingest text data from the `content` field and will automatically create `content_embedding`s that we can use for semantic search later. This uses the Cohere model from above. \n\n```\nPUT _ingest/pipeline/embedding-ingest-pipeline\n{\n  \"description\": \"Neural Search Pipeline\",\n  \"processors\" : [\n    {\n      \"text_embedding\": {\n        \"model_id\": \"<EMBEDDING_MODEL_ID>\",\n        \"field_map\": {\n          \"content\": \"content_embedding\"\n        }\n      }\n    }\n  ]\n}\n```\n\n\n## Register and deploy the language model\n\nNext, we will be registering the large language model (LLM) we will be using to generate our output. We will be using GPT-3.5-Turbo as it's well known and performs well in a variety of situations. \n\n```\nPOST /_plugins/_ml/models/_register?deploy=true\n{\n    \"name\": \"gpt-3.5-turbo\",\n    \"function_name\": \"remote\",\n    \"description\": \"OpenAI Chat Connector\",\n    \"model_group_id\": \"<MODEL_GROUP_ID>\",\n    \"connector\": {\n      \"name\": \"OpenAI Chat Connector\",\n      \"description\": \"The connector to public OpenAI model service for GPT 3.5\",\n      \"version\": 1,\n      \"protocol\": \"http\",\n      \"parameters\": {\n          \"endpoint\": \"api.openai.com\",\n          \"model\": \"gpt-3.5-turbo\"\n      },\n      \"credential\": {\n          \"openAI_key\": \"<OPENAI_KEY>\"\n      },\n      \"actions\": [\n          {\n              \"action_type\": \"predict\",\n              \"method\": \"POST\",\n              \"url\": \"https://${parameters.endpoint}/v1/chat/completions\",\n              \"headers\": {\n                  \"Authorization\": \"Bearer ${credential.openAI_key}\"\n              },\n              \"request_body\": \"{ \\\"model\\\": \\\"${parameters.model}\\\", \\\"messages\\\": ${parameters.messages} }\"\n          }\n      ]\n  }\n}\n# LLM_MODEL_ID:  \n```\n\n## Configure the RAG search pipeline\n\nMuch like our ingestion pipeline this will allow us to automatically have several things happen every time we perform a search. The first is to create a results processor `phase_results_processor`. This post-processing step in our search pipeline  enables us to do a [hybrid search](https://opensearch.org/docs/latest/search-plugins/search-pipelines/normalization-processor/#score-normalization-and-combination). Here we are going to retrieve documents using a vector search and with BM25 (commonly called keyword search) and combine the results. The primary purpose of this is to normalize the relevance scores as they use different scales. \n\nThe second, post-processing step is the \"retrieval_augmented_generation\" step. This is our RAG step which takes the results and feed them to a language model as context for answering the question.\n\n```\nPUT _search/pipeline/rag-search-pipeline\n{\n  \"phase_results_processors\": [\n    {\n      \"normalization-processor\": {\n        \"normalization\": {\n          \"technique\": \"min_max\"\n        },\n        \"combination\": {\n          \"technique\": \"arithmetic_mean\",\n          \"parameters\": {\n            \"weights\": [\n              0.3,\n              0.7\n            ]\n          }\n        }\n      }\n    }\n  ],\n  \"response_processors\": [\n    {\n      \"retrieval_augmented_generation\": {\n        \"description\": \"RAG search pipeline to be used with Cohere index\",\n        \"model_id\": \"<LLM_MODEL_ID>\",\n        \"context_field_list\": [\"content\"],\n        \"system_prompt\": \"You are a helpful OpenSearch assistant called DocBot\",\n        \"user_instructions\": \"Answer the following question using only the provided context. If the provided context does not provide enough information to answer the question respond with something along the lines of 'I dont have enough information to answer that.'\"\n      }\n    }\n  ]\n}\n```\n\n\n## Create KNN index \n\nNow that we have our pipeline for ingesting and searching we can finally create our index. In our index, we specify it needs to use our ingestion pipeline, search pipeline, and we provide it the parameters for our ML model. These should be documented somewhere on the model either in the `config.json` or a similar format.  \n\n> Note you need to match `space_type` to model space. eg embed-english-v3.0 recommends cosine similarity for comparing embeddings. \n\n```\nPUT /docbot\n{\n\t\"settings\": {\n\t\t\"index.knn\": true,\n\t\t\"default_pipeline\": \"embedding-ingest-pipeline\",\n    \"index.search.default_pipeline\": \"rag-search-pipeline\"\n\t},\n\t\"mappings\": {\n\t\t\"properties\": {\n\t\t\t\"content_embedding\": {\n\t\t\t\t\"type\": \"knn_vector\",\n\t\t\t\t\"dimension\": 1024,\n\t\t\t\t\"method\": {\n\t\t\t\t\t\"name\": \"hnsw\",\n\t\t\t\t\t\"space_type\": \"cosinesimil\",\n\t\t\t\t\t\"engine\": \"nmslib\"\n\t\t\t\t}\n\t\t\t},\n\t\t\t\"content\": {\n\t\t\t\t\"type\": \"text\"\n\t\t\t}\n\t\t}\n\t}\n}\n```\n\n## Overview\n\nLets take a look really quick at what we have configured so far!\n\n### Ingest\n\nThe first part is the ingestion pipeline. With this in place we can upload docs using our normal `_bulk` endpoints. It will automatically create embeddings of our `content` field and ingest them into our search index. Our search index is equipped to do both traditional search (BM25) and vector search (using nmslib and Cohere Embed-v3).\n\n![Ingestion Pipeline](/diagrams/RAGE_Search_Ingest.svg)\n\n### Search\n\nThen, on the search side we have configured a search post-processor that will merge our two result sets (the one from the BM25 and the one from the vector search). Then it will hand those results off to our RAG processor which will generate our response using the provided context. Do not worry about the pre-processor or conversational search steps just yet as we define those in our query. \n\n![Search Pipeline](/diagrams/RAGE_Search_Search.svg)\n\n\n## Hydrate index with `_bulk`\n\nNow that we have all this infrastructure lets do something with it! Below is just a sample of how we can upload data. We are not actually going to use that today. Pass to the next code block where we will actually run some code to populate our index. \n\n```\nPOST _bulk\n{ \"create\" : { \"_index\" : \"docbot\", \"_id\" : \"1\" }}\n{ \"content\":\"Testing neural search\"}\n{ \"create\" : { \"_index\" : \"docbot\", \"_id\" : \"2\" }}\n{ \"content\": \"What are we doing\"}\n{ \"create\" : { \"_index\" : \"docbot\", \"_id\" : \"3\" } }\n{ \"content\": \"This should exist\"}\n```\n\n> Note this may take some time as it is downloading the entire OpenSearch project website and documentation website. \n\n```\nchmod +x ./fetch-data.sh\npython -m pip install -r requirements.txt\npython ingest.py\n```\n\nThis will break all the documents from OpenSearch's documentation and website into smaller chunks (150 words) that makes it better for retrieving and providing as context. The details of this script are out of scope for this already long demo 😅\n\n\n## Create a conversation\n\nNow we will create a conversation. This will allow GPT to have context from our previous messages in our conversation. \n\n```\nPOST /_plugins/_ml/memory/conversation\n{\n  \"name\": \"DocBot Conversation\"\n}\n# CONVERSATION_ID: \n```\n\n\n## Search and generate response\n\nEverything has led to this moment. While this is executed as one request it does three distinct things. First, it will pre-process our text for our vector search. This is why we are providing the `EMBEDDING_MODEL_ID` in the `neural` section. \n\nNext, it will execute a hybrid search that will retrieve documents using BM25 and vector search. These results will be combined after they have executed by our post-processor. \n\nFinally, it will pass the parameters to the RAG post processor. This instructs it to generate the response using the conversation history and the documents it's found in the hybrid search. \n\n```\nGET /cohere-index/_search\n{\n  \"_source\": {\n    \"exclude\": [\n      \"content_embedding\"\n    ]\n  },\n  \"query\": {\n    \"hybrid\": {\n      \"queries\": [\n        {\n          \"match\": {\n            \"content\": {\n              \"query\": <QUESTION>\n            }\n          }\n        },\n        {\n          \"neural\": {\n            \"content_embedding\": {\n              \"query_text\": <QUESTION>,\n              \"model_id\": \"<EMBEDDING_MODEL_ID>\",\n              \"k\": 5\n            }\n          }\n        }\n      ]\n    }\n  },\n  \"ext\": {\n\t\t\"generative_qa_parameters\": {\n      \"llm_model\": \"gpt-3.5-turbo\",\n\t\t\t\"llm_question\": <QUESTION>,\n\t\t\t\"conversation_id\": \"<CONVERSATION_ID>\",\n                         \"context_size\": 3,\n                         \"interaction_size\": 3,\n                         \"timeout\": 45\n\t\t}\n\t}\n}\n```\n\n\n## Cleanup\n```\nPOST /_plugins/_ml/models/<EMBEDDING_MODEL_ID>/_undeploy\nDELETE /_plugins/_ml/models/<EMBEDDING_MODEL_ID>\nPOST /_plugins/_ml/models/<LLM_MODEL_ID>/_undeploy\nDELETE /_plugins/_ml/models/<LLM_MODEL_ID>\nDELETE _ingest/pipeline/cohere-ingest-pipeline\nDELETE _search/pipeline/rag-search-pipeline\nDELETE /_plugins/_ml/memory/conversation/<CONVERSATION_ID>\nDELETE cohere-index\n```\n\n### Troubleshoot:\n```\nPOST /_plugins/_ml/models/<MODEL_ID>/_predict\n{\n  \"parameters\": {\n    \"texts\": [\"This should exist\"]\n  }\n}\n```\n\n\n\nGET /cohere-index/_search?search_pipeline=_none\n```\n{\n  \"query\": {\n    \"match_all\": {}\n  }\n}\n``````"
    },
    {
      "name": "ravipratap366/RAG-Mistral7B-Haystack-Weaviate-FastAPI",
      "stars": 2,
      "img": "https://avatars.githubusercontent.com/u/53921865?s=40&v=4",
      "owner": "ravipratap366",
      "repo_name": "RAG-Mistral7B-Haystack-Weaviate-FastAPI",
      "description": null,
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2023-09-30T10:40:22Z",
      "updated_at": "2024-01-01T22:36:33Z",
      "topics": [],
      "readme": "# Haystack-and-Mistral-7B-RAG-Implementation\r\nHaystack and Mistral 7B RAG Implementation. It is based on completely open-source stack.\r\n\r\n"
    },
    {
      "name": "im-sanka/magicalytics",
      "stars": 2,
      "img": "https://avatars.githubusercontent.com/u/61604177?s=40&v=4",
      "owner": "im-sanka",
      "repo_name": "magicalytics",
      "description": "Repository for hacktoberfest 2023!",
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2023-09-26T21:40:03Z",
      "updated_at": "2023-11-09T23:03:35Z",
      "topics": [],
      "readme": "<h1 align=\"center\">Hacktoberfest 2023 project!</h1>\n<p align=\"center\">\n<img src=\"./images/analysis.png\" width=\"200\" height=\"200\">\n<h1 align=\"center\">Magicalytics</h1>\nThis is a solo project to get to know about available open-source resources which can be used to initiate data science project. The result will be a dashboard with features to help analyzing the data.\n</p>\n\n---\n## 📕 Theme \nExtract Transform Load (ETL) pipeline with analytics components.\n\n## 📝 Description\nThe aim for this magicalytics is to provide insights from trend data (generated by Google Analytics) and able to provide interaction between the data and AI. For this initial test, I use Google data trend that is obtained from BigQuery.\n\nThe example data is obtained from Google BigQuery and the link can be found [here](https://console.cloud.google.com/marketplace/product/bigquery-public-datasets/google-search-trends).\n\nThe tools which will be used are:\n- Poetry\n- JupySQL \n- Google BigQuery API\n- Voila\n- Ploomber\n- will add more later....\n\n*The outcome of this project is an analytical dashboard which can be used to process Google Analytics result and communicate with Retrieval Augmented Generation (RAG) pipeline.*\n\n[//]: # (- Provide a description of your project. Include the data sources you are using, the tools you are using, and the expected outcome of your project.)\n\n## 💾 Data sources \n\nFor this project, I use data from Google Analytics data which are tabulated in BigQuery.\nHowever, you may be able to use both single .csv file or any type of database.\n\n## 🖥️ Methods\n\nThe methods that I use is shown in the figure below.\n\n<img src=\"./images/ETL_diagram.png\">\n\nWorkflow figure cover the ETL diagram (1-5) and the next points will be added later.\n\n1. The workflow starts from environment preparation (Conda, Poetry, APIs, etc.).\n2. Downloading the data by querying necessary tables from Google BigQuery via API.\n3. Splitting the data into different tables based on ranking and deposited as .duckdb database.\n4. Building the Exploratory Data Analysis (EDA) pipeline.\n5. Pushing the clean data to the database for making processing ready data.\n6. Making _unique_ analysis using NLP algorithm and other machine learning algorithms.\n7. Adding RAG feature to bridge the communication between users and analyzed data.\n8. Building dashboard for whole visualization and data summary.\n9. Deployment via Ploomber.\n\n## 📊 User interface\nThe interface would be a Voila dashboard that host default data from the sample data mentioned above.\nThe dashboard can also host uploaded data. However, uploaded data will need to be selected first, since the dashboard requires .....\n\nFigure dashboard here later...\n\n[//]: # (Describe the user interface your project will have. Include a description of the tools you are using.)\n\n## 🙋🏻‍♂️ Team members\nThis is solo project, so, I am the only one who is working on it! 😮‍💨 \\\nFeel free to visit my site as well https://www.sanka.studio"
    },
    {
      "name": "ploomber/hacktoberfest-2023-project",
      "stars": 2,
      "img": "https://avatars.githubusercontent.com/u/60114551?s=40&v=4",
      "owner": "ploomber",
      "repo_name": "hacktoberfest-2023-project",
      "description": "Init template for data pipeline project",
      "homepage": "",
      "language": "Python",
      "created_at": "2023-09-20T22:26:52Z",
      "updated_at": "2023-09-30T04:14:10Z",
      "topics": [
        "chainlit",
        "data-engineering",
        "docker",
        "etl",
        "fastapi",
        "haystack",
        "jupysql",
        "pipelines",
        "ploomber",
        "rag",
        "sql"
      ],
      "readme": "# Hacktoberfest 2023 project: building ETL and RAG pipelines with open source \n\n## Set up /  Configuración\n\nThere should be one GitHub repository per team. /  Debería haber un repositorio de GitHub por equipo.\n\n**Ensure all team members have completed all steps in the [set up](setup.md) document.**\n\n**Asegúrate de que todos los miembros del equipo hayan completado todos los pasos en el [documento de configuración](setup-espanol.md).**\n\n## Theme of your project / Tema de tu proyecto\n\n1. Extract Transform Load (ETL) pipeline with an analytics component / Pipeline de Extracción, Transformación y Carga (ETL) con un componente analítico\n2. Extract Transform Load (ETL) pipeline with a machine learning (ML) component  / Pipeline de Extracción, Transformación y Carga (ETL) con un componente de aprendizaje automático (ML)\n3. Retrieval Augmented Generation (RAG) pipeline for question answering /  Pipeline de Generación Aumentada por Recuperación (RAG) para responder preguntas\n4. Retrieval Augmented Generation (RAG) pipeline for chatbot /  Pipeline de Generación Aumentada por Recuperación (RAG) para chatbot\n\n## Description / Descripción \n\nProvide a description of your project. Include the data sources you are using, the tools you are using, and the expected outcome of your project.\n\nProporcione una descripción de su proyecto. Incluya las fuentes de datos que está utilizando, las herramientas que está utilizando y el resultado esperado de su proyecto.\n\n## Data sources / Fuentes de datos\n\nProvide a detailed description of your data sources. Please ensure you work only with open source data. Include a link to the data you are working with. \n\nAgregue una descripción detallada de sus fuentes de datos. Asegúrese de trabajar solo con datos de código abierto. Incluya un enlace a los datos con los que está trabajando.\n\n**Do not upload data to GitHub** / **No suba datos a GitHub**\n\n## Methods / Métodos\n\nDescribe the methods you are using. Include a description of the tools you are using.\n\nDescriba los métodos que está utilizando. Incluya una descripción de las herramientas que está utilizando.\n\n## User interface your project will have / Interfaz de usuario que tendrá su proyecto\n\nDescribe the user interface your project will have. Include a description of the tools you are using.\n\nOptions: \n\n1. FastAPI application\n2. Chainlit application\n3. Voila dashboard\n\nDescriba la interfaz de usuario que tendrá su proyecto. Incluya una descripción de las herramientas que está utilizando.\n\nOpciones:\n\n1. Aplicación FastAPI\n2. Aplicación Chainlit\n3. Tablero Voila\n\n## Team members/ Miembros del equipo\n\nAdd the names and GitHub IDs of your team members here.\n\nAgregue los nombres y las ID de GitHub de los miembros de su equipo aquí.\n"
    },
    {
      "name": "balqaasem/youlit-ai",
      "stars": 2,
      "img": "https://avatars.githubusercontent.com/u/15086345?s=40&v=4",
      "owner": "balqaasem",
      "repo_name": "youlit-ai",
      "description": "A Fullstack YouTube Video Summarization Chatbot with Llama2, Haystack, Whisper & Streamlit",
      "homepage": null,
      "language": "Python",
      "created_at": "2023-09-11T10:46:46Z",
      "updated_at": "2024-03-20T09:59:03Z",
      "topics": [],
      "readme": "# youlit-ai\n\nA Fullstack YouTube Video Summarization Chatbot in Python with 🦙 LLaMA-2-7B-32K (GGUF), Haystack, OpenAI Whisper &amp; Streamlit.\n\nBy [Khalifa MBA](https://github.com/alfellati)\n"
    },
    {
      "name": "NOVA-IMS-Innovation-and-Analytics-Lab/MapIntel",
      "stars": 2,
      "img": "https://avatars.githubusercontent.com/u/73640439?s=40&v=4",
      "owner": "NOVA-IMS-Innovation-and-Analytics-Lab",
      "repo_name": "MapIntel",
      "description": "MapIntel is a system for acquiring intelligence from vast collections of text data.",
      "homepage": "https://nova-ims-innovation-and-analytics-lab.github.io/MapIntel/",
      "language": "Python",
      "created_at": "2023-09-10T09:09:33Z",
      "updated_at": "2023-11-21T13:49:28Z",
      "topics": [],
      "readme": "[black badge]: <https://img.shields.io/badge/%20style-black-000000.svg>\n[black]: <https://github.com/psf/black>\n[docformatter badge]: <https://img.shields.io/badge/%20formatter-docformatter-fedcba.svg>\n[docformatter]: <https://github.com/PyCQA/docformatter>\n[ruff badge]: <https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/charliermarsh/ruff/main/assets/badge/v1.json>\n[ruff]: <https://github.com/charliermarsh/ruff>\n[mypy badge]: <http://www.mypy-lang.org/static/mypy_badge.svg>\n[mypy]: <http://mypy-lang.org>\n[mkdocs badge]: <https://img.shields.io/badge/docs-mkdocs%20material-blue.svg?style=flat>\n[mkdocs]: <https://squidfunk.github.io/mkdocs-material>\n[version badge]: <https://img.shields.io/pypi/v/MapIntel.svg>\n[pythonversion badge]: <https://img.shields.io/pypi/pyversions/MapIntel.svg>\n[downloads badge]: <https://img.shields.io/pypi/dd/MapIntel>\n[gitter]: <https://gitter.im/MapIntel/community>\n[gitter badge]: <https://badges.gitter.im/join%20chat.svg>\n[discussions]: <https://github.com/NOVA-IMS-Innovation-and-Analytics-Lab/MapIntel/discussions>\n[discussions badge]: <https://img.shields.io/github/discussions/NOVA-IMS-Innovation-and-Analytics-Lab/MapIntel>\n[ci]: <https://github.com/NOVA-IMS-Innovation-and-Analytics-Lab/MapIntel/actions?query=workflow>\n[ci badge]: <https://github.com/NOVA-IMS-Innovation-and-Analytics-Lab/MapIntel/actions/workflows/ci.yml/badge.svg?branch=main>\n[doc]: <https://github.com/NOVA-IMS-Innovation-and-Analytics-Lab/MapIntel/actions?query=workflow>\n[doc badge]: <https://github.com/NOVA-IMS-Innovation-and-Analytics-Lab/MapIntel/actions/workflows/doc.yml/badge.svg?branch=main>\n\n# MapIntel\n\n[![ci][ci badge]][ci] [![doc][doc badge]][doc]\n\n| Category          | Tools    |\n| ------------------| -------- |\n| **Development**   | [![black][black badge]][black] [![ruff][ruff badge]][ruff] [![mypy][mypy badge]][mypy] [![docformatter][docformatter badge]][docformatter] |\n| **Package**       | ![version][version badge] ![pythonversion][pythonversion badge] ![downloads][downloads badge] |\n| **Documentation** | [![mkdocs][mkdocs badge]][mkdocs]|\n| **Communication** | [![gitter][gitter badge]][gitter] [![discussions][discussions badge]][discussions] |\n\n## Introduction\n\nMapIntel is a system for acquiring intelligence from vast collections of text data by representing each document as a\nmultidimensional vector that captures its semantics. The system is designed to handle complex Natural Language queries while it\nprovides Question-Answering functionality. Additionally, it allows for a visual exploration of the corpus. The MapIntel uses a\nretriever engine that first finds the closest neighbors to the query embedding and identifies the most relevant documents. It\nalso leverages the embeddings by projecting them onto two dimensions while preserving the multidimensional landscape, resulting in\na map where semantically related documents form topical clusters which we capture using topic modeling. This map aims to promote a\nfast overview of the corpus while allowing a more detailed exploration and interactive information encountering process. MapIntel\ncan be used to explore many types of corpora.\n\n![MapIntel UI screenshot](./docs/artifacts/ui.png)\n\n## Installation\n\nFor user installation, `mapintel` is currently available on the PyPi's repository, and you can install it via `pip`:\n\n```bash\npip install mapintel\n```\n\nDevelopment installation requires cloning the repository and then using [PDM](https://github.com/pdm-project/pdm) to install the\nproject as well as the main and development dependencies:\n\n```bash\ngit clone https://github.com/NOVA-IMS-Innovation-and-Analytics-Lab/MapIntel.git\ncd mapintel\npdm install\n```\n\n## Configuration\n\nMapIntel aims to be a flexible system that can run with any user provided corpus. In order to achieve this goal, it standardizes\nthe data and models, while the deployment of all services is expected to be on AWS. An example of how to fully set up a MapIntel\ninstance can be found at [MapIntel-News](https://github.com/NOVA-IMS-Innovation-and-Analytics-Lab/MapIntel-News). After deploying\nthe required services, a file `.env` should be created at the root of the project with environmental variables that are described\nbelow.\n\n### AWS credentials\n\nThe following environmental variable should be included in the `.env` file:\n\n- `AWS_PROFILE_NAME`\n\nThe user should have permissions to interact with the services described below.\n\n### Data\n\nAn OpenSearch database instance should be deployed in AWS with documents contained in an index called `document`. Each document is\nexpected to have the `content`, `date`, `embedding`, `embedding2d` and `topic` fields with the following types:\n\n- `content`: text type that contains the main text of the document.\n- `date`: `long` type that represents the ordinal format of a date.\n- `embedding`: `knn_vector` type that represents the embedding vector of the document.\n- `embedding2d`: `float` type that represents the 2D embedding vector of the document.\n- `topic`: `keyword` type that assigns a topic label to each document.\n\nThe relevant environmental variables are the following:\n\n- `OPENSEARCH_ENDPOINT`: The AWS endpoint of the OpenSearch deployed instance.\n- `OPENSEARCH_PORT`: The port of the instance.\n- `OPENSEARCH_USERNAME`: The username.\n- `OPENSEARCH_PASSWORD`: The password.\n\n### Models\n\nMapIntel uses three models trained on the user provided data. The first is a Haystack retriever model, the second is a model that\nreduces the dimensions of the embeddings to 2D, while the third is a generator model used for question-answering. The\ncorresponding environmental variables are the following:\n\n- `HAYSTACK_RETRIEVER_MODEL`: The value of the parameter `embedding_model` of the Haystack class `EmbeddingRetriever`.\n- `SAGEMAKER_DIMENSIONALITY_REDUCTIONER_ENDPOINT`: The SageMaker endpoint of the deployed dimensionality reductioner.\n- `SAGEMAKER_GENERATOR_MODEL_ENDPOINT`: The SageMaker endpoint of the deployed generator.\n\n## Usage\n\nTo run the application use the following command:\n\n```bash\nmapintel\n```\n\nThen the server starts and listens to connections at `http://localhost:8080`. You may open the browser and use this URL to\ninteract with the MapIntel UI.\n"
    },
    {
      "name": "devilteo911/privateGPT",
      "stars": 2,
      "img": "https://avatars.githubusercontent.com/u/15840439?s=40&v=4",
      "owner": "devilteo911",
      "repo_name": "privateGPT",
      "description": "Interact privately with your documents using the power of GPT, 100% privately, no data leaks",
      "homepage": null,
      "language": "Python",
      "created_at": "2023-06-07T13:42:30Z",
      "updated_at": "2025-04-01T11:02:59Z",
      "topics": [],
      "readme": "# IMPORTANT BEFORE INSTALLING\nIn order to work with this framework you have to install the llama-cpp-python library. To use the gpu you must use this command:\n\n```\nCMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python==0.1.59 --no-cache-dir\n```\n\nUnless there are some major changes in the library, the version is not mandatory. \n\n# privateGPT\nAsk questions to your documents without an internet connection, using the power of LLMs. 100% private, no data leaves your execution environment at any point. You can ingest documents and ask questions without an internet connection!\n\nBuilt with [LangChain](https://github.com/hwchase17/langchain), [GPT4All](https://github.com/nomic-ai/gpt4all), [LlamaCpp](https://github.com/ggerganov/llama.cpp), [Chroma](https://www.trychroma.com/) and [SentenceTransformers](https://www.sbert.net/).\n\n<img width=\"902\" alt=\"demo\" src=\"https://user-images.githubusercontent.com/721666/236942256-985801c9-25b9-48ef-80be-3acbb4575164.png\">\n\n# Environment Setup\nIn order to set your environment up to run the code here, first install all requirements:\n\n```shell\npip3 install -r requirements.txt\n```\n\nThen, download the LLM model and place it in a directory of your choice:\n- LLM: default to [ggml-gpt4all-j-v1.3-groovy.bin](https://gpt4all.io/models/ggml-gpt4all-j-v1.3-groovy.bin). If you prefer a different GPT4All-J compatible model, just download it and reference it in your `.env` file.\n\nRename `example.env` to `.env` and edit the variables appropriately.\n```\nMODEL_TYPE: supports LlamaCpp or GPT4All\nPERSIST_DIRECTORY: is the folder you want your vectorstore in\nMODEL_PATH: Path to your GPT4All or LlamaCpp supported LLM\nMODEL_N_CTX: Maximum token limit for the LLM model\nEMBEDDINGS_MODEL_NAME: SentenceTransformers embeddings model name (see https://www.sbert.net/docs/pretrained_models.html)\nTARGET_SOURCE_CHUNKS: The amount of chunks (sources) that will be used to answer a question\n```\n\nNote: because of the way `langchain` loads the `SentenceTransformers` embeddings, the first time you run the script it will require internet connection to download the embeddings model itself.\n\n## Test dataset\nThis repo uses a [state of the union transcript](https://github.com/imartinez/privateGPT/blob/main/source_documents/state_of_the_union.txt) as an example.\n\n## Instructions for ingesting your own dataset\n\nPut any and all your files into the `source_documents` directory\n\nThe supported extensions are:\n\n   - `.csv`: CSV,\n   - `.docx`: Word Document,\n   - `.doc`: Word Document,\n   - `.enex`: EverNote,\n   - `.eml`: Email,\n   - `.epub`: EPub,\n   - `.html`: HTML File,\n   - `.md`: Markdown,\n   - `.msg`: Outlook Message,\n   - `.odt`: Open Document Text,\n   - `.pdf`: Portable Document Format (PDF),\n   - `.pptx` : PowerPoint Document,\n   - `.ppt` : PowerPoint Document,\n   - `.txt`: Text file (UTF-8),\n\nRun the following command to ingest all the data.\n\n```shell\npython ingest.py\n```\n\nOutput should look like this:\n\n```shell\nCreating new vectorstore\nLoading documents from source_documents\nLoading new documents: 100%|██████████████████████| 1/1 [00:01<00:00,  1.73s/it]\nLoaded 1 new documents from source_documents\nSplit into 90 chunks of text (max. 500 tokens each)\nCreating embeddings. May take some minutes...\nUsing embedded DuckDB with persistence: data will be stored in: db\nIngestion complete! You can now run privateGPT.py to query your documents\n```\n\nIt will create a `db` folder containing the local vectorstore. Will take 20-30 seconds per document, depending on the size of the document.\nYou can ingest as many documents as you want, and all will be accumulated in the local embeddings database.\nIf you want to start from an empty database, delete the `db` folder.\n\nNote: during the ingest process no data leaves your local environment. You could ingest without an internet connection, except for the first time you run the ingest script, when the embeddings model is downloaded.\n\n## Ask questions to your documents, locally!\nIn order to ask a question, run a command like:\n\n```shell\npython privateGPT.py\n```\n\nAnd wait for the script to require your input.\n\n```plaintext\n> Enter a query:\n```\n\nHit enter. You'll need to wait 20-30 seconds (depending on your machine) while the LLM model consumes the prompt and prepares the answer. Once done, it will print the answer and the 4 sources it used as context from your documents; you can then ask another question without re-running the script, just wait for the prompt again.\n\nNote: you could turn off your internet connection, and the script inference would still work. No data gets out of your local environment.\n\nType `exit` to finish the script.\n\n\n### CLI\nThe script also supports optional command-line arguments to modify its behavior. You can see a full list of these arguments by running the command ```python privateGPT.py --help``` in your terminal.\n\n\n# How does it work?\nSelecting the right local models and the power of `LangChain` you can run the entire pipeline locally, without any data leaving your environment, and with reasonable performance.\n\n- `ingest.py` uses `LangChain` tools to parse the document and create embeddings locally using `HuggingFaceEmbeddings` (`SentenceTransformers`). It then stores the result in a local vector database using `Chroma` vector store.\n- `privateGPT.py` uses a local LLM based on `GPT4All-J` or `LlamaCpp` to understand questions and create answers. The context for the answers is extracted from the local vector store using a similarity search to locate the right piece of context from the docs.\n- `GPT4All-J` wrapper was introduced in LangChain 0.0.162.\n\n# System Requirements\n\n## Python Version\nTo use this software, you must have Python 3.10 or later installed. Earlier versions of Python will not compile.\n\n## C++ Compiler\nIf you encounter an error while building a wheel during the `pip install` process, you may need to install a C++ compiler on your computer.\n\n### For Windows 10/11\nTo install a C++ compiler on Windows 10/11, follow these steps:\n\n1. Install Visual Studio 2022.\n2. Make sure the following components are selected:\n   * Universal Windows Platform development\n   * C++ CMake tools for Windows\n3. Download the MinGW installer from the [MinGW website](https://sourceforge.net/projects/mingw/).\n4. Run the installer and select the `gcc` component.\n\n## Mac Running Intel\nWhen running a Mac with Intel hardware (not M1), you may run into _clang: error: the clang compiler does not support '-march=native'_ during pip install.\n\nIf so set your archflags during pip install. eg: _ARCHFLAGS=\"-arch x86_64\" pip3 install -r requirements.txt_\n\n# Disclaimer\nThis is a test project to validate the feasibility of a fully private solution for question answering using LLMs and Vector embeddings. It is not production ready, and it is not meant to be used in production. The models selection is not optimized for performance, but for privacy; but it is possible to use different models and vectorstores to improve performance.\n"
    },
    {
      "name": "Subodh7976/OnePiece-Question-Answering",
      "stars": 2,
      "img": "https://avatars.githubusercontent.com/u/97154559?s=40&v=4",
      "owner": "Subodh7976",
      "repo_name": "OnePiece-Question-Answering",
      "description": null,
      "homepage": null,
      "language": "CSS",
      "created_at": "2023-07-14T11:11:45Z",
      "updated_at": "2024-08-06T05:34:35Z",
      "topics": [],
      "readme": "# OnePiece Question Answering\n\nOnePiece Question Answering is a tool to answer queries related to OnePiece Anime. OnePiece is an anime which consists 1000+ chapters, so it becomes hard to search for a single query you have in mind. With this tool you can ask any query related to the OnePiece anime (works better if it involves major keyword). This project scraped over 12000 articles from [OnePiece Fandom](https://onepiece.fandom.com/wiki/One_Piece_Wiki) which contains details related to every character, chapter and every incidents. \n\nThis project utilizes the Roberta-base model as its base Document Reader to answer query from a context and BM25 as document retriever. The articats folder is not included (because of its big size) and the data craping and model training can take time or you can download articats contents directly from [here](https://drive.google.com/file/d/1x2XBfM-xugPHcDyH8K30yjKJ8BGDZVau/view?usp=drive_link).\n\n## Getting Started\n\nTo get started with using this tool, you might want to clone or download the repository.\n\n```bash\ngit clone https://github.com/Subodh7300/OnePiece-Question-Answering\ncd OnePiece-Question-Answering\n```\n\n## Dependencies\n\nYou might have to install multiple dependencies, so just run the command to directly install all the dependecies.\n```bash\npip install -r requirements.txt\n```\n\n## Usage\n\nNow once installed, launch it by running the app.py python file, and the flask server will start on localhost.\n```bash\npython app.py\n```\n\nYou can visit http://127.0.0.1:5000/ or http://localhost:5000/ to access the web interface.\n\n## Contributing\n\nFor any changes, please open an issue \nto discuss what you would like to change.\n\nAs I am new to this, any contribution is welcome.\n\n## Want to contact me?\n* [Telegram](https://t.me/subodh79)\n* [LinkedIn](https://www.linkedin.com/in/subodh-uniyal-655328230)\n* [Instagram](https://www.instagram.com/subodh_7300/)\n* [Email](s.subodh7976@gmail.com)\n"
    },
    {
      "name": "anakin87/haystack-entailment-checker",
      "stars": 2,
      "img": "https://avatars.githubusercontent.com/u/44616784?s=40&v=4",
      "owner": "anakin87",
      "repo_name": "haystack-entailment-checker",
      "description": "Haystack node for checking the entailment between a statement and a list of Documents",
      "homepage": null,
      "language": "Python",
      "created_at": "2023-07-03T12:43:27Z",
      "updated_at": "2023-11-10T16:29:22Z",
      "topics": [],
      "readme": "# Haystack Entailment Checker\n\nCustom node for the [Haystack NLP framework](https://github.com/deepset-ai/haystack).\nUsing a [Natural Language Inference](https://paperswithcode.com/task/natural-language-inference) model, it checks whether a lists of Documents/passages entails, contradicts or is neutral with respect to a given statement.\n\n**Live Demo**: Fact Checking 🎸 Rocks! &nbsp; [![Generic badge](https://img.shields.io/badge/🤗-Open%20in%20Spaces-blue.svg)](https://huggingface.co/spaces/anakin87/fact-checking-rocks)\n\n## How it works\n![Entailment Checker Node](https://github.com/anakin87/haystack-entailment-checker/blob/main/images/entailment_checker_node.png)\n- The node takes a list of Documents (commonly returned by a [Retriever](https://docs.haystack.deepset.ai/docs/retriever)) and a statement as input.\n- Using a Natural Language Inference model, the text entailment between each text passage/Document (premise) and the statement (hypothesis) is computed. For every text passage, we get 3 scores (summing to 1): entailment, contradiction and neutral.\n- The text entailment scores are aggregated using a weighted average. The weight is the relevance score of each passage returned by the Retriever, if availaible. It expresses the similarity between the text passage and the statement. **Now we have a summary score, so it is possible to tell if the passages confirm, are neutral or disprove the user statement.**\n- *empirical consideration: if in the first N passages (N<K), there is strong evidence of entailment/contradiction (partial aggregate scores > **threshold**), it is better not to consider (K-N) less relevant documents.*\n\n## Installation\n```bash\npip install haystack-entailment-checker\n```\n\n## Usage\n### Basic example\n```python\nfrom haystack import Document\nfrom haystack_entailment_checker import EntailmentChecker\n\nec = EntailmentChecker(\n        model_name_or_path = \"microsoft/deberta-v2-xlarge-mnli\",\n        use_gpu = False,\n        entailment_contradiction_threshold = 0.5)\n\ndoc = Document(\"My cat is lazy\")\n\nprint(ec.run(\"My cat is very active\", [doc]))\n# ({'documents': [...],\n# 'aggregate_entailment_info': {'contradiction': 1.0, 'neutral': 0.0, 'entailment': 0.0}}, ...)\n```\n\n### Fact-checking pipeline (Retriever + EntailmentChecker)\n```python\nfrom haystack import Document, Pipeline\nfrom haystack.nodes import BM25Retriever\nfrom haystack.document_stores import InMemoryDocumentStore\nfrom haystack_entailment_checker import EntailmentChecker\n\n# INDEXING\n# the knowledge base can consist of many documents\ndocs = [...]\nds = InMemoryDocumentStore(use_bm25=True)\nds.write_documents(docs)\n\n# QUERYING\nretriever = BM25Retriever(document_store=ds)\nec = EntailmentChecker()\n\npipe = Pipeline()\npipe.add_node(component=retriever, name=\"Retriever\", inputs=[\"Query\"])\npipe.add_node(component=ec, name=\"EntailmentChecker\", inputs=[\"Retriever\"])\n\npipe.run(query=\"YOUR STATEMENT TO CHECK\")\n```\n## Acknowledgements 🙏\nSpecial thanks goes to [@davidberenstein1957](https://github.com/davidberenstein1957), who contributed to the original implementation of this node, in the [Fact Checking 🎸 Rocks!](https://github.com/anakin87/fact-checking-rocks) project.\n"
    },
    {
      "name": "StrangeNPC/HaystackChatbotStreamlit",
      "stars": 2,
      "img": "https://avatars.githubusercontent.com/u/95240891?s=40&v=4",
      "owner": "StrangeNPC",
      "repo_name": "HaystackChatbotStreamlit",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2023-06-01T14:32:13Z",
      "updated_at": "2024-03-27T21:47:32Z",
      "topics": [],
      "readme": "\n# Haystacks Streamlit Chatbot\n\n\nThe Haystacks Streamlit Chatbot is a chatbot powered by deep learning. It utilizes the Haystack framework and the Google Flan T5 Large LLM model to provide AI-generated responses to user prompts. The app is designed to be user-friendly and easy to use.\n\n![unnamed](https://github.com/StrangeNPC/HaystackChatbotStreamlit/assets/95240891/74f52b0e-8d9c-43ad-99c3-ed56090d86b5)\n\n\n\n## Installation\n\n1. Clone to your repository:\n\n```bash\nhttps://github.com/StrangeNPC/HaystackChatbotStreamlit.git\n\n```\n\n2. Install the requirements needed in the terminal:\n```bash\npip install -r requirements.txt\n\n```\n3. Make sure you have the necessary files in your project directory:\n\n- Notice.txt: The text file containing the document you want to index and search.\n- Google T5 Flan model downloaded from Huggingface and placed in the same directory/folder as these files.\n\n4. Run the 'StreamlitX.py' file to start the app:\n\n```bash\nstreamlit run main.py\n\n```\n## Usage\n1. Once the app is running, you will see a sidebar on the left with some information about the app.\n\n2. To clear the chat history, click the \"Clear Chat History\" button in the sidebar.\n\n3. Type your question or prompt in the input box and press Enter\n\n4. The app will generate AI-generated responses based on your input and display them in the response container.\n\n5. The user's questions and AI-generated responses will be stored in the chat history and displayed in the response container.\n\n## Notes\n- The app uses an InMemoryDocumentStore to store and index the documents. The Notice.txt file is converted into a format suitable for indexing using the TextConverter and PreProcessor components.\n\n- The app uses the BM25Retriever for document retrieval based on the user's queries.\n\n- The Google Flan T5 Large LLM model is used for generating responses. It is loaded from the GoogleT5 directory.\n\n- The app is built using the Streamlit framework, which provides a user-friendly web interface.\n\n- The app allows you to have interactive conversations with the chatbot, and you can clear the chat history whenever needed.\n"
    },
    {
      "name": "Lewington-pitsos/oopscover",
      "stars": 2,
      "img": "https://avatars.githubusercontent.com/u/24558325?s=40&v=4",
      "owner": "Lewington-pitsos",
      "repo_name": "oopscover",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2023-05-27T03:55:58Z",
      "updated_at": "2024-04-30T06:57:11Z",
      "topics": [],
      "readme": "# OuchCover\n\n## Installing\n\nMake a virtualenv with puython 3.10, activate it and then \n\n`pip install --upgrade pip`\n`pip install 'farm-haystack[all]' ## or 'all-gpu' for the GPU-enabled dependencies`\n\n`pip install -r requirements.txt`\n\nYou will also need to \n\n`cd frontend`\n`pip install -e .`\n\nsince this installs `ui` as a package.\n\n## Running\n\nstart the backend with\n\n`python serve.py`\n\nand then the frontend with\n\n`cd frontend`\n`streamlit run ui/webapp.py`\n"
    },
    {
      "name": "davidberenstein1957/memory-palace",
      "stars": 2,
      "img": "https://avatars.githubusercontent.com/u/25269220?s=40&v=4",
      "owner": "davidberenstein1957",
      "repo_name": "memory-palace",
      "description": "The open-source neural privacy friendly neural-search engine for your data.",
      "homepage": "",
      "language": "Python",
      "created_at": "2023-04-09T06:29:23Z",
      "updated_at": "2023-05-22T20:16:12Z",
      "topics": [],
      "readme": "\n\n# Memory Palace\n\nAn open-source and privacy-friendly memory-palace for the pragmatic programmer.\n\n## Components\n\n`memory-palace` differentiates between several components that handle different parts of the creation of your personal memory palace. They handle your incoming memories sequentially and ought to be used in a pipeline-like fashion. Note that you can for example pass manually captured audio\n\n1. `Capturers` are programatically capturing memories from different sources in different formats like audio, email, chat, and image.\n2. `Processors` periodically process the captured memories from each of the formats and converts them to `haystack.Documents`. For example, audio is transcribed to text and OCR is run on images.\n3. `Indexers` as of now we only support one indexer, the `HaystackIndexer`, which uses `ElasticSearch` to create a semantic index for your processed `haystack.Document`.\n4. `pipelines` are pipelines to index documents into `haystack` and compose queries. Note that I am also using `argilla` to keep track of used queries and train NLP models to help with fine-tuning query results.\n\nLastly, I offer a `FastAPI` application to add manual input to `Processor` (think spoken or written dictates via Apple shortcuts, or written text images to be OCRed), check the status of running components, and query your memory palace. Later on, I will look into a nicer UI. Note that this FastAPI can also be used to run certain components locally to, for example, capture audio and screenshots, and run other more compute heavy components in the cloud.\n\n### Capturers\n\n- The default `AudioCapturer` uses the `pyaudio` and `wave` to capture audio fragments and save them as `.wav`-files, which are then stored in the `queue` of the `WhisperProcessor`.\n- The default `WhatsAppCapturer` uses icloud exports to\n- The default `GmailCapturer` uses the google SDK to load emails from a certain start-date and add them to the `EmailProcessor`.\n- The default `ScreenCapturer` uses the `pillow.ImageGrab` to take screenshots and add them to the `EasyOCRProcessor`.\n\nI want to work towards adding support for `SlackCapturer`, `ApplicationCapturer`, `WebsiteCapturer`.\n\n### Processors\n\n- The `EmailProcessor` takes `.mbox` files or individual `.mail` message and adds them to the `HaystackIndexer` as `haystack.Documents`.\n- The `WhisperProcessor` converts `.wav` files to text and adds them to the `HaystackIndexer` as `haystack.Documents`.\n- The `EasyOCRProcessor` converts `images` to text using `easyocr` and adds them to the `HaystackIndexer` as `haystack.Documents`.\n\n### Pipelines\n\n- The `string_indexing_pipeline` uses a `PreProcessor` to format string to and splitup documents, and a `EmbeddingRetriever` to add qa and semantic embeddings to the `haystack.Document`.\n- The `keyword_classifier_pipeline` is a query pipeline which uses an `sklearn` classifier to either do a semantic search, QnA search or a keyword based search.\n\nI want to work towards several TokenClassification and TextClassification pipelines, which can be used to further fine-tune filter passed to the query pipeline. For example, \"What did I discuss with Daniel last week?\" ought to pick up (last week, DATE) and (Daniel, PERSON), which can be used as filters. These predictions and fine-tunes should be logged into `argilla`.\n\n### Indexers\n\nThe `HaystackIndexer` indexes document using a `pipeline`, which by default uses the `string_indexing_pipeline`.\n\n\n## NOTES\n\non mac requirements audio\n- brew install portaudio\n\nextras audio\n- pyaudio\n- wave (internal)\n\nextras ocr\n- easyocr\n\nemail\n- google-api-python-client\n- google-auth-httplib2\n- google-auth-oauthlib\n- beautifulsoup4\n\nlater API\n[Google Takeout](https://takeout.google.com/) for mail and calendar items\n\n[Whatsapp backupp](https://faq.whatsapp.com/1180414079177245/?locale=fi_FI&cms_platform=android)\n\n## TODOs\n\n- [] QnA - cheap inference images\n- [] Embedding - cheap inference images\n- [] Tokens - Crosslingual FewNERD\n- [] Keywords - Crosslingual\n- [] Images - [clip embeddingsdoc](https://huggingface.co/sentence-transformers/clip-ViT-B-32-multilingual-v1)\n\n\n- [] Chat data [text, speaker, time, person, thread, persons] [thread, persons]\n  - [] Whatsapp\n  - [] Email\n  - [] Slack\n- [] Mono data [source, text, time, segement_id/sentence/paragraph] [source, summary] []\n  - Articles\n  - Websites\n  - Voice messages/memos [whisper](https://github.com/ahmetoner/whisper-asr-webservice)\n\n- elasticsearch\n- pcloud backups"
    },
    {
      "name": "abhiamishra/RamayanaGPT",
      "stars": 2,
      "img": "https://avatars.githubusercontent.com/u/52009380?s=40&v=4",
      "owner": "abhiamishra",
      "repo_name": "RamayanaGPT",
      "description": "Creating an interactive ChatGPT for the Ramayana text from Hindu mythology.",
      "homepage": "https://ramayanagpt.onrender.com/",
      "language": "Jupyter Notebook",
      "created_at": "2023-05-10T22:15:19Z",
      "updated_at": "2025-04-17T16:36:47Z",
      "topics": [],
      "readme": "# RamayanaGPT\nCreating an interactive ChatGPT for the Ramayana text from Hindu mythology.\n\n![Logo](https://github.com/abhiamishra/RamayanaGPT/blob/main/image.png)\n"
    },
    {
      "name": "renyuanL/_JosephLin_2023",
      "stars": 2,
      "img": "https://avatars.githubusercontent.com/u/6368761?s=40&v=4",
      "owner": "renyuanL",
      "repo_name": "_JosephLin_2023",
      "description": "Tutor Meeting with JosephLin @2023",
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2023-03-25T14:14:29Z",
      "updated_at": "2023-05-03T01:29:38Z",
      "topics": [],
      "readme": "# _JosephLin_2023\n\n2023.03.25:\n## Neural Network from scratch in Python\n\n- https://towardsdatascience.com/math-neural-network-from-scratch-in-python-d6da9f29ce65\n\n2023.04.08:\n## Back-propagation in CNN\n- https://www.jefkine.com/general/2016/09/05/backpropagation-in-convolutional-neural-networks/\n\n![](https://www.jefkine.com/assets/images/conv.png)\n\n$$\n\\begin{flalign*}\n& z_{i,j}^{(l)} = \\sum_{m,n} w_{m,n}^{(l)} \\cdot x_{m+i, n+j}^{(l)} + b_{i,j}^{(l)} \\\\\n& y_{i,j}^{(l)} = f(z_{i,j}^{(l)}) \\\\\n& x_{i,j}^{(l+1)}= y_{i,j}^{(l)} \n\\end{flalign*}\n$$\n\n## Introduction to PyTorch\n- https://learn.microsoft.com/en-us/training/paths/pytorch-fundamentals/\n\n## Introduction to Computer Vision with PyTorch\n- https://learn.microsoft.com/en-us/training/modules/intro-computer-vision-pytorch/\n\n## Speech Recognition\n- https://pytorch.org/audio/stable/tutorials/speech_recognition_pipeline_tutorial.html\n- [SPEECH COMMAND CLASSIFICATION WITH TORCHAUDIO](https://machinelearningleague.github.io/pytorch-tutorials/intermediate/speech_command_recognition_with_torchaudio.html)\n- [VERY DEEP CONVOLUTIONAL NEURAL NETWORKS FOR RAW WAVEFORMS](https://arxiv.org/pdf/1610.00087.pdf)\n\n![image](https://user-images.githubusercontent.com/6368761/232249205-bb44ecfb-d008-441e-b65f-1bcee2757aca.png)\n\n## CNN: a good tutorial\n- https://hackernoon.com/the-full-story-behind-convolutional-neural-networks-and-the-math-behind-it-2j4fk3zu2\n- https://github.com/SkalskiP/ILearnDeepLearning.py\n\n![LeNet](https://pytorch.org/tutorials/_images/mnist.png)\n\n## CNN in Tensorflow\n- using mini Speech Commands dataset\n- https://github.com/renyuanL/_JosephLin_2023/blob/main/simple_audio_ry_do_some_modification_for_tutoring.ipynb\n\n## From Mlp --> CNN (LeNet-5) @ MNIST in Pytorch \n- https://github.com/renyuanL/_JosephLin_2023/blob/main/ry_MLP_CNN_LeNet_MNIST.ipynb\n\n## CNN in Pytorch, Speech Commands dataset, Raw Waveform, no Spectrogram\n- https://github.com/renyuanL/_JosephLin_2023/blob/main/_ry_SPEECH_COMMAND_CLASSIFICATION_WITH_TORCHAUDIO.ipynb\n\n## the Speech Commands Dataset\n- https://arxiv.org/pdf/1804.03209.pdf\n\n## the recognition program of speech commands\n- https://github.com/renyuanL/_JosephLin_2023/blob/main/_ry_01_recog_01_py.ipynb\n\n## the training program of speech commands\n- https://github.com/renyuanL/_JosephLin_2023/blob/main/_ry_01_speech_command_classification_with_torchaudio_tutorial.ipynb\n\n## using Open-AI whisper\n- https://github.com/renyuanL/_JosephLin_2023/blob/main/_ry_04_6_whisper.ipynb\n### official site\n- https://openai.com/research/whisper\n- ![](https://raw.githubusercontent.com/openai/whisper/main/approach.png)\n\n## End-to-End Speech Recognition: A Survey\n- https://arxiv.org/pdf/2303.03329.pdf\n\n## Recent Advances in End-to-End Automatic Speech Recognition\n- https://arxiv.org/pdf/2111.01690.pdf\n\n## Prompting the Hidden Talent of Web-Scale Speech Models for Zero-Shot Task Generalization, [Submitted on 18 May 2023] \n- https://arxiv.org/abs/2305.11095\n- https://arxiv.org/pdf/2305.11095.pdf\n  ![image](https://github.com/renyuanL/_JosephLin_2023/assets/6368761/24a729e0-d5aa-4e59-8a2d-0591463db981)\n"
    },
    {
      "name": "Forbu/LoiLibre",
      "stars": 2,
      "img": "https://avatars.githubusercontent.com/u/11457947?s=40&v=4",
      "owner": "Forbu",
      "repo_name": "LoiLibre",
      "description": "LoiLibre : interface open source pour de l'assistance juridique",
      "homepage": null,
      "language": "Python",
      "created_at": "2023-05-01T17:42:57Z",
      "updated_at": "2024-05-20T19:51:52Z",
      "topics": [],
      "readme": "\n![Screenshot 2024-05-05 at 19 10 57](https://github.com/Forbu/LoiLibre/assets/11457947/ff987453-7b27-4a5c-a195-b19f9fd40b74)\n\n\n# LoiLibre\nLoiLibre : interface open source pour de l'assistance juridique\n\nBienvenue à LoiLibre, un assistant juridique open source conçu pour rendre l'accès aux informations juridiques plus accessible à tous. \nAvec LoiLibre, nous espérons éliminer la complexité souvent intimidante des lois françaises, en offrant une plateforme facile à utiliser pour naviguer et comprendre les lois et règlements.\nQue vous soyez un avocat, un étudiant en droit, ou simplement un citoyen souhaitant comprendre vos droits, LoiLibre est conçu pour vous. \nCe README.md est destiné à vous aider à démarrer avec LoiLibre, en vous fournissant les informations clés sur le projet et les instructions pour sa configuration et son utilisation.\nNous sommes impatients de voir comment LoiLibre aidera à démocratiser l'accès aux informations juridiques pour tous.\n\n\nCe projet utilise d'autres projets open source (open science juridique : les données des différents codes viennent de droit.fr ou encore de legifrance.gouv.fr), nous les remercions.\n\n### Le site internet \n\nIl s'agit d'un site internet rudimentaire (je ne suis pas webdev ...) qui permettra d'interagir avec l'assitant juridique.\n\nSi vous avez des propositions d'amélioration du site vous pouvez ajouter des issues etc \n\n\n### Le model\n\nIci nous utilisons les évolutions récentes des modèles de langage (ex : chatGPT) pour créer un assitant capable d'intéragir avec des données (ici le corpus juridique).\n\nLes détails du code seront fait plus tard.\n\nLe modèle de l'assistant juridique est simplement chatGPT (via l'API en ligne) couplé à une interface qui permet à chatGPT d'avoir des informations concernant des articles juridiques en lien avec la question posée.\n\nIl s'agit ici d'un modèle simple, dans le futur nous avons plusieurs possibilité autour du fine tuning de modèles open source ou encore d'autres approches. \n\n\n\n"
    },
    {
      "name": "recrudesce/haystack_threshold_node",
      "stars": 2,
      "img": "https://avatars.githubusercontent.com/u/6450799?s=40&v=4",
      "owner": "recrudesce",
      "repo_name": "haystack_threshold_node",
      "description": "This component filters documents based on a threshold percentage, ensuring only the documents above the threshold get passed down the pipeline.",
      "homepage": null,
      "language": "Python",
      "created_at": "2023-04-26T20:17:21Z",
      "updated_at": "2023-08-08T07:31:31Z",
      "topics": [],
      "readme": "# haystack_threshold_node\nThis component filters documents based on a threshold percentage, ensuring only the documents above the threshold get passed down the pipeline.\nThis allows you to query your document store for a larger top_k, but then filter the results down to those which are above a set confidence score.\n\n## Installation\n\n`pip install haystack-threshold-node`\n\n## Usage\n\nInclude it in your pipeline - example as follows:\n\n```python\nimport logging\nimport re\n\nfrom datasets import load_dataset\nfrom haystack.document_stores import InMemoryDocumentStore\nfrom haystack.nodes import PromptNode, PromptTemplate, AnswerParser, BM25Retriever\nfrom haystack.pipelines import Pipeline\nfrom haystack_lemmatize_node import LemmatizeDocuments\n\n\nlogging.basicConfig(format=\"%(levelname)s - %(name)s -  %(message)s\", level=logging.WARNING)\nlogging.getLogger(\"haystack\").setLevel(logging.INFO)\n\ndocument_store = InMemoryDocumentStore(use_bm25=True)\n\ndataset = load_dataset(\"bilgeyucel/seven-wonders\", split=\"train\")\ndocument_store.write_documents(dataset)\n\nretriever = BM25Retriever(document_store=document_store, top_k=10)\n\nlfqa_prompt = PromptTemplate(\n    name=\"lfqa\",\n    prompt_text=\"Given the context please answer the question using your own words. Generate a comprehensive, summarized answer. If the information is not included in the provided context, reply with 'Provided documents didn't contain the necessary information to provide the answer'\\n\\nContext: {documents}\\n\\nQuestion: {query} \\n\\nAnswer:\",\n    output_parser=AnswerParser(),\n)\n\nprompt_node = PromptNode(\n    model_name_or_path=\"text-davinci-003\",\n    default_prompt_template=lfqa_prompt,\n    max_length=500,\n    api_key=\"sk-OPENAIKEY\",\n)\n\n# The value you pass for threshold is the lowest % score you will accept. Whole numbers only.\n# In this example, the threshold is set to 80%.\nthreshold = DocumentThreshold(threshold=80) \n\npipe = Pipeline()\npipe.add_node(component=retriever, name=\"Retriever\", inputs=[\"Query\"])\npipe.add_node(component=threshold, name=\"Threshold\", inputs=[\"Retriever\"])\npipe.add_node(component=prompt_node, name=\"prompt_node\", inputs=[\"Threshold\"])\n\nquery = \"What does the Rhodes Statue look like?\"\n  \noutput = pipe.run(query)\n\nprint(output['answers'][0].answer)\n```"
    },
    {
      "name": "Bubbalubagus/semanticsearch",
      "stars": 2,
      "img": "https://avatars.githubusercontent.com/u/16597445?s=40&v=4",
      "owner": "Bubbalubagus",
      "repo_name": "semanticsearch",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2023-02-20T05:37:01Z",
      "updated_at": "2024-05-12T04:36:24Z",
      "topics": [],
      "readme": "\"# semanticsearch\" \n"
    },
    {
      "name": "recrudesce/haystack_translate_node",
      "stars": 2,
      "img": "https://avatars.githubusercontent.com/u/6450799?s=40&v=4",
      "owner": "recrudesce",
      "repo_name": "haystack_translate_node",
      "description": "An Azure Translation node for Haystack",
      "homepage": null,
      "language": "Python",
      "created_at": "2023-03-16T20:52:45Z",
      "updated_at": "2023-04-06T11:27:43Z",
      "topics": [],
      "readme": "# haystack_translate_node\nAn Azure Translation node for Haystack - you will need this configured in your Azure subscription: https://learn.microsoft.com/en-us/azure/cognitive-services/translator/translator-overview\n\nInclude in your pipeline as follows:\n=======\ngit clone the repo somewhere, change to the directory, then `pip install '.'`\n\nInclude in your pipeline as follows:\n\n```python\nfrom haystack_translate_node import TranslateAnswer, TranslateQuery\n\ntranslate_query = TranslateQuery(api_key=\"<yourapikey>\", location=\"<yourazureregion>\", azure_translate_endpoint=\"<yourazureendpoint>\", base_lang=\"en\")\ntranslate_answer = TranslateAnswer(api_key=\"<yourapikey>\", location=\"<yourazureregion>\", azure_translate_endpoint=\"<yourazureendpoint>\", base_lang=\"en\")\n\npipel = Pipeline()\npipel.add_node(component=translate_query, name=\"TranslateQuery\", inputs=[\"Query\"])\npipel.add_node(component=retriever, name=\"Retriever\", inputs=[\"TranslateQuery\"])\npipel.add_node(component=prompt_node, name=\"prompt_node\", inputs=[\"Retriever\"])\npipel.add_node(component=translate_answer, name=\"TranslateAnswer\", inputs=[\"prompt_node\"])\n```\n\n`location`, `azure_translate_endpoint`, and `base_lang` are optional, and will default to `uksouth`, `https://api.cognitive.microsofttranslator.com/`, and `en` respectively.\n\n - TranslateQuery will determine the language of the query, and assign it to the `in_lang` JSON value.\n - TranslateQuery will take the original query, in any language, and assign it to the `in_query` JSON value.\n - TranslateQuery will overwrite the original `query` JSON value with the translated English value\n\n - You can then query your `base_lang` corpus using the `query` value as normal using a standard Haystack Retriever node, which will place your results in `results`.\n\n - TranslateAnswer translate the `base_lang` result stored in `results` back to the language stored in `in_lang` and subsequently store it in the `out_answer` JSON value.\n"
    },
    {
      "name": "aaalexlit/cc-evidences-api",
      "stars": 2,
      "img": "https://avatars.githubusercontent.com/u/116374290?s=40&v=4",
      "owner": "aaalexlit",
      "repo_name": "cc-evidences-api",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2023-03-10T06:31:47Z",
      "updated_at": "2024-02-11T05:07:39Z",
      "topics": [],
      "readme": "# Relevant Scientific Evidence Retrieval and Verification API for Climate impact related queries\n\n## Overview\n\nThe API is created to perform scientific verification of claims\nextracted from Climate Change related news articles in order to detect\npotential inaccuracies of the latter.\n\n![OpenApi specification of this API](api.gif)\n\n\n### General workflow\n\n```mermaid\nflowchart TB\n   subgraph client1 [Streamlit Client]\n      A(Media Article text or URL) --> COR{\"Co-reference resolution \\n (Optional)\"}\n      COR -->|Split into sentences| S1(\"Sentence 1\")\n      COR -->|Split into sentences| S2(\"Sentence 2\")\n      COR -->|Split into sentences| SN(\"...\")\n      S1 --> CR{\"Climate related?\\n (Optional)\"}\n      S2 --> CR\n      SN --> CR\n      CR -->|Yes| IC{\"Is a claim? \\n(Optional)\"}\n      CR --No --x N[ Ignore ]\n      IC --No --x N1[Ignore]\n   end\n   subgraph API\n      IC -- Yes ---> E[\"Retrieve Top k most similar evidences\"]\n      E:::curAppNode --> R[\"Re-rank using citation metrics (Optional)\"]\n      R:::curAppNode --> VC[[\"Verify with Climate-BERT based model\"]]\n   end\n   subgraph client2 [ Streamlit Client ]\n      R ---> VM[[\"Verify with MultiVerS\"]]\n      VC:::curAppNode --> D[\"Display predictions\"]\n      VM --> D\n   end\n    style R stroke:#808080,stroke-width:2px,stroke-dasharray: 5 5\n    style CR stroke:#808080,stroke-width:2px,stroke-dasharray: 5 5\n    style COR stroke:#808080,stroke-width:2px,stroke-dasharray: 5 5\n    style IC stroke:#808080,stroke-width:2px,stroke-dasharray: 5 5\n    style API fill:#E9EAE0,color:#E7625F\n    classDef curAppNode fill:#F7BEC0,color:#C85250,stroke:#E7625F\n    linkStyle 10,11 stroke:#F7BEC0,stroke-width:4px,color:#C85250,background-color:#F7BEC0\n;\n\n```\n\n## Main functionality\n**The API performs 2 main tasks**\n- Evidence retrieval for given claim(s) under all `evidence` endpoints\n- Evidence retrieval + verification for given claim(s) under all `vefify` endpoints\n- Supplementary task of splitting text into sentences under `split` endpoint\nto enable Chrome extension functioning\n\n### Scientific evidences index and database\nPlease refer to the dedicated [Evidence database creation](doc/db.md) section\n\n### Most relevant evidence retrieval\n\nPerformed using [Haystack framework](https://haystack.deepset.ai/) that facilitates\nfast dense vector retrieval and the sentence encoder models\ndescribed [here](doc/db.md#model-for-sentence-embeddings)\nThe number of evidence candidates to retrieve is defined by the `top_k` parameter \n\n### Evidence Re-ranking\n\nIf the `re_rank` parameter is set to `true` the following actions are performed\n1. 5 more than the set `top_k` most semantically similar evidence candidates get retrieved\nfor each input claim. \n2. The candidate evidences get sorted in descending order according to the following parameters:\n   1. Number of [influential citations](https://www.semanticscholar.org/faq#influential-citations) \n   i.e., citations that indicate that the cited work is \n   used or extended in the new effort [[1]](#references)\n   2. Number of [all citations](https://www.semanticscholar.org/faq#estimated-citations)\n   3. Publication year\n\n### Model for claim verification against retrieved evidence\n\n[Climatebert-fact-checking model](https://huggingface.co/amandakonet/climatebert-fact-checking) \navailable from huggingface.\nIt's a ClimateBERT [[2]](#references) model fine-tuned \non [CLIMATE-FEVER](https://www.sustainablefinance.uzh.ch/en/research/climate-fever.html)\ndataset \n[[3]](#references)\n\n### Split into sentences\n[Spacy \"en_core_web_sm\" pipeline](https://spacy.io/models/en#en_core_web_sm)\nis used for text segmentation task  \nThis model is the smallest and the fastest and according to spacy's \n[Accuracy Evaluation](https://spacy.io/models/en#en_core_web_sm-accuracy) has\nthe same metric values as the bigger CPU-optimized models\n\n### API description\nIn all the examples below `batch` endpoint accepts \nan array of input sentences rather than a single\nsentence\n\n#### `abstract` Endpoints\n\nAll the following endpoints perform searches against\nthe database with the full scientific article abstracts \n\n`/api/abstract/evidence`  \n`/api/abstract/evidence/batch`\n\n\n`/api/abstract/verify`  \n`/api/abstract/verify/batch`\n\n#### `phrase` Endpoints\n\nAll the following endpoints perform searches against\nthe database with the scientific article abstracts broken into \nindividual phrases\n\n`/api/phrase/evidence`  \n`/api/phrase/evidence/batch`  \n\n`/api/phrase/verify`  \n`/api/phrase/verify/batch`\n\n\n[Formal description of the API](doc/api.md)\n\n## Local development and deployment\nPlease refer to the [Technical documentation](doc/tech.md)\n\n## References\n\n1. Valenzuela-Escarcega, M.A., Ha, V.A., & Etzioni, O. (2015). Identifying Meaningful Citations. AAAI Workshop: Scholarly Big Data.\n2. Webersinke, N., Kraus, M., Bingler, J. A., & Leippold, M. (2021). Climatebert: \nA pretrained language model for climate-related text. arXiv preprint arXiv:2110.12010.\n3. Diggelmann, Thomas; Boyd-Graber, Jordan; Bulian, Jannis; Ciaramita, Massimiliano; \nLeippold, Markus (2020). CLIMATE-FEVER: A Dataset for Verification of Real-World Climate \nClaims. In: Tackling Climate Change with Machine Learning workshop at NeurIPS 2020, Online, \n11 December 2020 - 11 December 2020."
    },
    {
      "name": "danielbichuetti/haystack",
      "stars": 2,
      "img": "https://avatars.githubusercontent.com/u/90110233?s=40&v=4",
      "owner": "danielbichuetti",
      "repo_name": "haystack",
      "description": ":mag: Haystack is an open source NLP framework that leverages pre-trained Transformer models. It enables developers to quickly implement production-ready semantic search, question answering, summarization and document ranking for a wide range of NLP applications.",
      "homepage": "https://deepset.ai/haystack",
      "language": "Python",
      "created_at": "2022-06-20T17:25:44Z",
      "updated_at": "2025-03-16T20:44:46Z",
      "topics": [],
      "readme": "<div align=\"center\">\n  <a href=\"https://haystack.deepset.ai/\"><img src=\"https://raw.githubusercontent.com/deepset-ai/haystack/main/docs/img/banner.png\" alt=\"Green logo of a stylized white 'H' with the text 'Haystack, by deepset.' Abstract green and yellow diagrams in the background.\"></a>\n\n|         |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |\n| ------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |\n| CI/CD   | [![Tests](https://github.com/deepset-ai/haystack/actions/workflows/tests.yml/badge.svg)](https://github.com/deepset-ai/haystack/actions/workflows/tests.yml) [![types - Mypy](https://img.shields.io/badge/types-Mypy-blue.svg)](https://github.com/python/mypy) [![Coverage Status](https://coveralls.io/repos/github/deepset-ai/haystack/badge.svg?branch=main)](https://coveralls.io/github/deepset-ai/haystack?branch=main) [![Ruff](https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/astral-sh/ruff/main/assets/badge/v2.json)](https://github.com/astral-sh/ruff) |\n| Docs    | [![Website](https://img.shields.io/website?label=documentation&up_message=online&url=https%3A%2F%2Fdocs.haystack.deepset.ai)](https://docs.haystack.deepset.ai)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |\n| Package | [![PyPI](https://img.shields.io/pypi/v/haystack-ai)](https://pypi.org/project/haystack-ai/) ![PyPI - Downloads](https://img.shields.io/pypi/dm/haystack-ai?color=blue&logo=pypi&logoColor=gold) ![PyPI - Python Version](https://img.shields.io/pypi/pyversions/haystack-ai?logo=python&logoColor=gold) [![Conda Version](https://img.shields.io/conda/vn/conda-forge/haystack-ai.svg)](https://anaconda.org/conda-forge/haystack-ai) [![GitHub](https://img.shields.io/github/license/deepset-ai/haystack?color=blue)](LICENSE) [![License Compliance](https://github.com/deepset-ai/haystack/actions/workflows/license_compliance.yml/badge.svg)](https://github.com/deepset-ai/haystack/actions/workflows/license_compliance.yml) |\n| Meta    | [![Discord](https://img.shields.io/discord/993534733298450452?logo=discord)](https://discord.com/invite/xYvH6drSmA) [![Twitter Follow](https://img.shields.io/twitter/follow/haystack_ai)](https://twitter.com/haystack_ai)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |\n</div>\n\n[Haystack](https://haystack.deepset.ai/) is an end-to-end LLM framework that allows you to build applications powered by\nLLMs, Transformer models, vector search and more. Whether you want to perform retrieval-augmented generation (RAG),\ndocument search, question answering or answer generation, Haystack can orchestrate state-of-the-art embedding models\nand LLMs into pipelines to build end-to-end NLP applications and solve your use case.\n\n## Installation\n\nThe simplest way to get Haystack is via pip:\n\n```sh\npip install haystack-ai\n```\n\nInstall from the `main` branch to try the newest features:\n```sh\npip install git+https://github.com/deepset-ai/haystack.git@main\n```\n\nHaystack supports multiple installation methods including Docker images. For a comprehensive guide please refer\nto the [documentation](https://docs.haystack.deepset.ai/docs/installation).\n\n## Documentation\n\nIf you're new to the project, check out [\"What is Haystack?\"](https://haystack.deepset.ai/overview/intro) then go\nthrough the [\"Get Started Guide\"](https://haystack.deepset.ai/overview/quick-start) and build your first LLM application\nin a matter of minutes. Keep learning with the [tutorials](https://haystack.deepset.ai/tutorials). For more advanced\nuse cases, or just to get some inspiration, you can browse our Haystack recipes in the\n[Cookbook](https://haystack.deepset.ai/cookbook).\n\nAt any given point, hit the [documentation](https://docs.haystack.deepset.ai/docs/intro) to learn more about Haystack, what can it do for you and the technology behind.\n\n## Features\n\n> [!IMPORTANT]\n> **You are currently looking at the readme of Haystack 2.0**. We are still maintaining Haystack 1.x to give everyone\n> enough time to migrate to 2.0. [Switch to Haystack 1.x here](https://github.com/deepset-ai/haystack/tree/v1.x).\n\n- **Technology agnostic:** Allow users the flexibility to decide what vendor or technology they want and make it easy to switch out any component for another. Haystack allows you to use and compare models available from OpenAI, Cohere and Hugging Face, as well as your own local models or models hosted on Azure, Bedrock and SageMaker.\n- **Explicit:** Make it transparent how different moving parts can “talk” to each other so it's easier to fit your tech stack and use case.\n- **Flexible:** Haystack provides all tooling in one place: database access, file conversion, cleaning, splitting, training, eval, inference, and more. And whenever custom behavior is desirable, it's easy to create custom components.\n- **Extensible:** Provide a uniform and easy way for the community and third parties to build their own components and foster an open ecosystem around Haystack.\n\nSome examples of what you can do with Haystack:\n\n-   Build **retrieval augmented generation (RAG)** by making use of one of the available vector databases and customizing your LLM interaction, the sky is the limit 🚀\n-   Perform Question Answering **in natural language** to find granular answers in your documents.\n-   Perform **semantic search** and retrieve documents according to meaning.\n-   Build applications that can make complex decisions making to answer complex queries: such as systems that can resolve complex customer queries, do knowledge search on many disconnected resources and so on.\n-   Scale to millions of docs using retrievers and production-scale components.\n-   Use **off-the-shelf models** or **fine-tune** them to your data.\n-   Use **user feedback** to evaluate, benchmark, and continuously improve your models.\n\n> [!TIP]\n><img src=\"https://github.com/deepset-ai/haystack/raw/main/docs/img/deepset-cloud-logo-lightblue.png\"  width=30% height=30%>\n>\n> Are you looking for a managed solution that benefits from Haystack? [deepset Cloud](https://www.deepset.ai/deepset-cloud?utm_campaign=developer-relations&utm_source=haystack&utm_medium=readme) is our fully managed, end-to-end platform to integrate LLMs with your data, which uses Haystack for the LLM pipelines architecture.\n\n> [!TIP]\n>\n> Would you like to deploy and serve Haystack pipelines as REST APIs yourself? [Hayhooks](https://github.com/deepset-ai/hayhooks) provides a simple way to wrap your pipelines with custom logic and expose them via HTTP endpoints, including OpenAI-compatible chat completion endpoints and compatibility with fully-featured chat interfaces like [open-webui](https://openwebui.com/).\n\n## 🆕 deepset Studio: Your Development Environment for Haystack\n\nUse **deepset Studio** to visually create, deploy, and test your Haystack pipelines. Learn more about it in [our announcement post](https://haystack.deepset.ai/blog/announcing-studio).\n\n![studio](https://github.com/user-attachments/assets/e4f09746-20b5-433e-8261-eca224ac23b3)\n\n\n👉 [Sign up](https://landing.deepset.ai/deepset-studio-signup)!\n\n## Telemetry\n\nHaystack collects **anonymous** usage statistics of pipeline components. We receive an event every time these components are initialized. This way, we know which components are most relevant to our community.\n\nRead more about telemetry in Haystack or how you can opt out in [Haystack docs](https://docs.haystack.deepset.ai/docs/telemetry).\n\n## 🖖 Community\n\nIf you have a feature request or a bug report, feel free to open an [issue in Github](https://github.com/deepset-ai/haystack/issues). We regularly check these and you can expect a quick response. If you'd like to discuss a topic, or get more general advice on how to make Haystack work for your project, you can start a thread in [Github Discussions](https://github.com/deepset-ai/haystack/discussions) or our [Discord channel](https://discord.com/invite/VBpFzsgRVF). We also check [𝕏 (Twitter)](https://twitter.com/haystack_ai) and [Stack Overflow](https://stackoverflow.com/questions/tagged/haystack).\n\n## Contributing to Haystack\n\nWe are very open to the community's contributions - be it a quick fix of a typo, or a completely new feature! You don't need to be a Haystack expert to provide meaningful improvements. To learn how to get started, check out our [Contributor Guidelines](https://github.com/deepset-ai/haystack/blob/main/CONTRIBUTING.md) first.\n\nThere are several ways you can contribute to Haystack:\n- Contribute to the main Haystack project\n- Contribute an integration on [haystack-core-integrations](https://github.com/deepset-ai/haystack-core-integrations)\n\n> [!TIP]\n>👉 **[Check out the full list of issues that are open to contributions](https://github.com/orgs/deepset-ai/projects/14)**\n\n## Who Uses Haystack\n\nHere's a list of projects and companies using Haystack. Want to add yours? Open a PR, add it to the list and let the\nworld know that you use Haystack!\n\n-   [Airbus](https://www.airbus.com/en)\n-   [Alcatel-Lucent](https://www.al-enterprise.com/)\n-   [Apple](https://www.apple.com/)\n-   [BetterUp](https://www.betterup.com/)\n-   [Databricks](https://www.databricks.com/)\n-   [Deepset](https://deepset.ai/)\n-   [Etalab](https://www.deepset.ai/blog/improving-on-site-search-for-government-agencies-etalab)\n-   [Infineon](https://www.infineon.com/)\n-   [Intel](https://github.com/intel/open-domain-question-and-answer#readme)\n-   [Intelijus](https://www.intelijus.ai/)\n-   [Intel Labs](https://github.com/IntelLabs/fastRAG#readme)\n-   [LEGO](https://github.com/larsbaunwall/bricky#readme)\n-   [Netflix](https://netflix.com)\n-   [NOS Portugal](https://www.nos.pt/en/welcome)\n-   [Nvidia](https://developer.nvidia.com/blog/reducing-development-time-for-intelligent-virtual-assistants-in-contact-centers/)\n-   [PostHog](https://github.com/PostHog/max-ai#readme)\n-   [Rakuten](https://www.rakuten.com/)\n-   [Sooth.ai](https://www.deepset.ai/blog/advanced-neural-search-with-sooth-ai)\n"
    },
    {
      "name": "MartinTheDoge/fDet",
      "stars": 2,
      "img": "https://avatars.githubusercontent.com/u/123597562?s=40&v=4",
      "owner": "MartinTheDoge",
      "repo_name": "fDet",
      "description": "Website application for fake-news detection using modified AlBERT AI",
      "homepage": null,
      "language": "Python",
      "created_at": "2023-02-01T17:23:07Z",
      "updated_at": "2023-08-25T09:14:05Z",
      "topics": [],
      "readme": "# fDet\nWebsite application for fake-news detection using modified AlBERT AI\n\n"
    },
    {
      "name": "rkaunismaa/NaturalLanguageProcessingWithTransformers",
      "stars": 2,
      "img": "https://avatars.githubusercontent.com/u/11901059?s=40&v=4",
      "owner": "rkaunismaa",
      "repo_name": "NaturalLanguageProcessingWithTransformers",
      "description": "Natural Language Processing with Transformers by Lewis Tunstall, Leandro von Werra & Thomas Wolf.",
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2023-01-15T16:33:00Z",
      "updated_at": "2024-12-31T02:56:41Z",
      "topics": [],
      "readme": "# NaturalLanguageProcessingWithTransformers\n\n[Natural Language Processing with Transformers by Lewis Tunstall, Leandro von Werra, Thomas Wolf.](https://github.com/nlp-with-transformers/notebooks)\n\n\nSunday, January 15, 2023\n\ndocker run --gpus all -it -v $(realpath ~/):/tf/All -v /home/rob/Data2:/home/rob/Data2 --env HF_DATASETS_CACHE=/home/rob/Data2/huggingface/datasets --env TRANSFORMERS_CACHE=/home/rob/Data2/huggingface/transformers -p 8888:8888 -p 6006:6006 hfptl:20221222\n\n01_Introduction.ipynb\nRun Date: Sunday, January 15, 2023\nRun Time: 00:00:29\n\n02_classification.ipynb\nRun Date: Sunday, January 15, 2023\nRun Time: 00:04:38\n\n03_transformer-anatomy.ipynb\nRun Date: Sunday, January 15, 2023\nRun Time: 00:00:10\n\n04_multilingual-ner.ipynb\nHaving out of memory issues with this notebook. Numerous installs to this image ...\nSo saved the file, exited docker, want to restart the container ... so ...\n\n(base) rob@KAUWITB:~$ docker container ps -a\nCONTAINER ID   IMAGE            COMMAND                  CREATED             STATUS                     PORTS     NAMES\nc135df6d6af8   hfptl:20221222   \"/opt/nvidia/nvidia_…\"   About an hour ago   Exited (0) 2 minutes ago             determined_wing\n\ndocker container start determined_wing\ndocker container logs determined_wing\n\nSigh ... nope ... continues to run out of memory, even with a reduced batch size. \n\n05_text-generation.ipynb\nNope! Runs out of GPU ram ...\n\n06_summarization.ipynb\nNope! Runs out of GPU ram ...\n\n### Saturday, October 28, 2023\n\nTrying 05_textgeneration.ipynb again in the newest huggingface transformers docker image which is running inside the container hfpt_Oct28.\n\nHmm interesting ... it now runs! With the 2070 Super! Nice!\n\nGonna also try the 06_summarization.ipynb again ... \n\nI had to run 'pip install py7zr'\n\n1:49PM Wow ... so I renamed Data2/HuggingFace to Data2/HuggingFace_ then reran this notebook ... AND IT STILL RUNS! So where I thought the models were being saved to is wrong! ... Are they being saved into the container??\n\n### Monday, October 31, 2023\n\nInstalled the 4090, and now notebooks 05 and 06 run just fine. Nice!\n\n"
    },
    {
      "name": "Shingo-Kamata/japanese_qa_demo_with_haystack_and_es",
      "stars": 2,
      "img": "https://avatars.githubusercontent.com/u/19622710?s=40&v=4",
      "owner": "Shingo-Kamata",
      "repo_name": "japanese_qa_demo_with_haystack_and_es",
      "description": "Haystack + Elasticsearch + wikipedia(ja) を用いた、日本語の質問応答システムのサンプル",
      "homepage": null,
      "language": "Python",
      "created_at": "2022-12-18T10:16:47Z",
      "updated_at": "2023-12-27T04:20:35Z",
      "topics": [],
      "readme": "# Haystack と Elasticsearch を用いた日本語QA応答システムサンプル\n\n## 概要\n- Elasticsearch + Haystack を用いて Open-book な Question Answering システムを動かすサンプル\n  - https://zenn.dev/shingo_kamata/articles/8a182ff5acfa62\n- 基本的には、Haystack の https://haystack.deepset.ai/tutorials/01_basic_qa_pipeline と https://docs.haystack.deepset.ai/docs/languages の内容を参考にしているだけ\n- サンプルのElasticsearchを利用する場合、Wikipedia(ja) が格納される\n\n## 前提\n- Poetry（Python3.9) が必要\n- 自前で検証用のElasticsearchが使えるか、sample_es ディレクトリのElasticsearchを起動して使える前提\n- できればGPUがある環境を推奨するが、CPUでも動作可能\n\n## 使い方\n\n### 自前のElasticsearchに接続する場合\n- [haystack_qa.py](haystack_qa.py) に接続先のElasticsearch情報を記載してください\n- 注意点\n  - **必ず検証に用いても問題の無いElasticsearchを利用してください**\n    - create_index などのオプションがあり、デフォルトがTrueでありTrueだとMappingが追加されるなど不明な挙動があるため\n- すべてのドキュメントに `content_type: keyword型` がないとエラーになるようです。。。\n  - content_type には Retriever 対象のフィールドのmapping type（text）などを記載します\n    - 参考： [Mapping](https://github.com/Shingo-Kamata/japanese_qa_demo_with_haystack_and_es/blob/main/sample_es/analyzer.json#L64-L65) [値](https://github.com/Shingo-Kamata/japanese_qa_demo_with_haystack_and_es/blob/main/sample_es/wiki_index_create.py#L34)\n    - これを追加するこが困難な場合、local にインストールされた Hyastack コードの [search_engine.py](https://github.com/deepset-ai/haystack/blob/v1.10.0/haystack/document_stores/search_engine.py#L1126) の content_type の `None` を `\"text\"` に編集するワークアラウンドをすれば回避できます\n- https://zenn.dev/shingo_kamata/articles/8a182ff5acfa62 に記載したように、Reader が走査するテキストは500単語くらいが良いので、場合によっては、Readerの対象フィールドを変更するか、ドキュメントの内容を分割することを検討してください\n\n### サンプルのElasticsearchを利用する場合\n- [sample_es にある README](sample_es/README.md) に従って作成してください\n  - 記事のデータは別途用意する必要があります（上記README参照）\n\n### GPUがない場合\n- [haystack_qa.py](haystack_qa.py) の該当設定値を変更してください\n\n### poetry install （初回時のみ）\n```\n$ poetry install\n```\n\n### QAの実行例（sample_es利用）\n```\n$ poetry run python haystack_qa.py\n(中略)\nINFO:haystack.modeling.utils:Using devices: CUDA:0 - Number of GPUs: 1\n質問を入力してください（exit で終了）>>ラグビーの日本監督は？\nInferencing Samples: 100%|█| 1/1 [00:00<00:00,  4.41 Batches/s\n\nQuery: ラグビーの日本監督は？\nAnswers:\n[   <Answer {'answer': '神戸製鋼コベルコスティーラーズ', 'type': 'extractive', 'score': 0.9501436948776245, 'context': '大会と日本選手権の2冠連覇を成し遂げた。また日本代表としても第2～4回ラグビーワールドカップに出場した。\\n2004年4月、現役を引退し、神戸製鋼コベルコスティーラーズの監督に就任。\\n（2012年）早稲田大学ラグビー部アドバイザー(2015年退任)及び女子ラグビー７人制チーム【ラガール７】の総監督を務', 'offsets_in_document': [{'start': 524, 'end': 539}], 'offsets_in_context': [{'start': 68, 'end': 83}], 'document_id': 'VvhNJYUBLyfAJxXYRL5-', 'meta': {'article_id': '322679', 'revid': '1980193', 'url': 'https://ja.wikipedia.org/wiki?curid=322679', 'title': '増保輝則'}}>,\n    <Answer {'answer': '萩本光威', 'type': 'extractive', 'score': 0.842361569404602, 'context': 'クネームつけるラグビ日本代表愛称向井監督世界背中見えるコメント大会終了後解任2004年3月22日神戸神戸製鋼製鋼コベルコスティーラーズヘッドコーチ萩本光威監督就任当初同年スーパスーパーパワーズカップパワーズカップロシアカナダ破る優勝導く幸先よいスタート思う続くイタリア敗戦11月欧州遠征スコットランド', 'offsets_in_document': [{'start': 390, 'end': 394}], 'offsets_in_context': [{'start': 73, 'end': 77}], 'document_id': 'afpNJYUBLyfAJxXY7zWG', 'meta': {'article_id': '190986', 'revid': '703098', 'url': 'https://ja.wikipedia.org/wiki?curid=190986', 'title': 'ラグビー日本代表'}}>,\n...\n```\n初回実行時に、`farm_reader_model/` に https://huggingface.co/ybelkada/japanese-roberta-question-answering のモデルがDLされます\n\n## フォルダ構成\n```\n.\n├── README.md\n├── farm_reader_model・・・Readerのモデルを保存（キャッシュ）しておく置き場\n│   └── .gitkeep\n├── haystack_qa.py・・・QAモジュール\n├── poetry.lock\n├── pyproject.toml\n└── sample_es・・・サンプル用のEs\n    ├── README.md・・・サンプルEs構築のREADME\n    ├── analyzer.json・・・サンプルEsのAnalyzer設定\n    ├── docker-compose.yml・・・サンプルEsのDocker\n    ├── （output・・・wikiextractの結果を保存する）\n    ├── test_wiki_iindex_create.py・・・のテストコード\n    └── wiki_iindex_create.py\n```"
    },
    {
      "name": "gizdatalab/haystack_utils",
      "stars": 2,
      "img": "https://avatars.githubusercontent.com/u/71088041?s=40&v=4",
      "owner": "gizdatalab",
      "repo_name": "haystack_utils",
      "description": "Repo for NLP tasks built by adapting the existing framework developed by Haystack and HuggingFace.",
      "homepage": "",
      "language": "Python",
      "created_at": "2022-11-29T13:25:24Z",
      "updated_at": "2024-09-05T13:15:55Z",
      "topics": [],
      "readme": "# Climate Policy Analysis Machine - utils\nThis is a repo made primarily for NLP tasks and is based mainly on [Haystack](https://docs.haystack.deepset.ai/) and [Hugging face](https://huggingface.co/) already built components.\n\nThe tasks performed include:\n1. Document processing: Processing the text from docx/text/pdf files and creating the paragraphs list.\n2. Search: Performing lexical or semantic search on the paragraphs list created in step 1.\n3. SDG Classification: Performing the SDG classification on the paragraphs text.\n4. Extracting the keywords based on [Textrank](https://github.com/summanlp/textrank)/TFIDF/[KeyBert](https://github.com/MaartenGr/KeyBERT)\n\nPlease use the [colab notebook](https://colab.research.google.com/drive/1ym6Ub5-sMGZkfAF4lnHMWF4MgMpabZ-r?usp=sharing) to get familiar with basic usage of utils\n(use branch =main for non-streamlit usage).\nFor more detailed walkthrough use the [advanced colab notebook](https://colab.research.google.com/drive/1t9ZpcliqlNwkS4NDeKA4JRGdtBKCE9hC?usp=sharing).\nThere are two branch in the repo. One for using in streamlit environment and another for generic usage like in colab or local machine. \nYou can clone the repo for your own use, or also install it as package. \n\nTo install as package (non-streamlit use):\n```\npip install -e \"git+https://github.com/gizdatalab/haystack_utils.git@main#egg=utils\"\n```\n\nTo install as package for streamlit app:\n```\npip install -e \"git+https://github.com/gizdatalab/haystack_utils.git@streamlit#egg=utils\"\n```\nTo install as package (for CPU-trac Streamlit app https://huggingface.co/spaces/GIZ/cpu_tracs):\n```\npip install -e \"git+https://github.com/gizdatalab/haystack_utils.git@cputrac#egg=utils\"\n```\n"
    },
    {
      "name": "deeplearn-ai/covid-qa-udemy",
      "stars": 2,
      "img": "https://avatars.githubusercontent.com/u/88691020?s=40&v=4",
      "owner": "deeplearn-ai",
      "repo_name": "covid-qa-udemy",
      "description": null,
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2021-08-23T10:21:46Z",
      "updated_at": "2024-01-20T10:30:42Z",
      "topics": [],
      "readme": "# covid-qa-udemy"
    },
    {
      "name": "zbw/Funder-NER",
      "stars": 2,
      "img": "https://avatars.githubusercontent.com/u/376765?s=40&v=4",
      "owner": "zbw",
      "repo_name": "Funder-NER",
      "description": "Extracting funder information from scientific papers - with a question answering approach to named entity recognition",
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2022-05-16T08:47:45Z",
      "updated_at": "2023-06-07T22:27:45Z",
      "topics": [],
      "readme": "# Funder-NER\nExtracting funder information from scientific papers - with a question answering approach to named entity recognition\n\n## haystack-ner\nThe folder haystack-ner contains a docker-compose file for running two docker container.\nOne with elastic search for data storage and one with a python 3 environment for running the scripts utilizing haystack.\nFor further information see README.md in folder haystack-ner.\n\n## jupyter-nbs/ACKNER-comparison\nThe folder jupyter-nbs/ACKNER-comparison contains a jupiter notebook with results achieved with code from the AckNER project https://github.com/informagi/AckNER and our testset data compared to results with haystack.\n"
    },
    {
      "name": "lambdaofgod/github_search",
      "stars": 2,
      "img": "https://avatars.githubusercontent.com/u/3647577?s=40&v=4",
      "owner": "lambdaofgod",
      "repo_name": "github_search",
      "description": null,
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2019-08-11T16:12:09Z",
      "updated_at": "2025-01-13T22:05:06Z",
      "topics": [],
      "readme": "# github-search\n\n### Repository for \"Searching Github Python repositories with machine learning\" masters thesis\n\n#### Jakub Bartczuk\n\n## Running this project\n\n### Prerequisites\n\nPreprocessing steps were tested on a machine with 64GB RAM.\n\nFor training Graph Neural Networks CUDA GPU is required.\n\n### General remarks\n\nThe project uses `nbdev` to create Python files from Jupyter notebook. To \"make\" project run\n\n``nbdev_build_lib; pip install -e .``\n\nin the root directory.\n\nWe use [ploomber](https://github.com/ploomber/ploomber) for managing training and data preprocessing. \n\nFor example to create csv files with extracted READMEs run\n\n``ploomber build --partial make_readmes  --skip-upstream --force ``\n\nRelevant definitions can be found in `pipeline.yaml` and `env.yaml`\n\n\n### TODO Downloading data\n\n### TODO Model checkpoints\n\n### Training models\n\nPloomber step:\n\n`run_gnn_experiment`\n\n### TODO Using models\n\n"
    },
    {
      "name": "arulpugazh/pubmedbot",
      "stars": 2,
      "img": "https://avatars.githubusercontent.com/u/1398646?s=40&v=4",
      "owner": "arulpugazh",
      "repo_name": "pubmedbot",
      "description": "A Closed Domain Question-Answering System that answers questions related to scientific research and developments",
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2020-07-05T03:45:03Z",
      "updated_at": "2020-10-15T16:29:06Z",
      "topics": [],
      "readme": "# Question-Answering Bot on PubMed\n\nThis is a closed-domain question-answering bot that can answer any questions about medical research with answers from PubMed articles.\n\nI am developing this as part of my mentorship with SharpestMinds. \n# Scraping PubMed articles\n\n### Extract article ID and URL\nArticle information is extracted from [nih baseline files](https://ftp.ncbi.nlm.nih.gov/pubmed/baseline/) . They are XML files and we extract the article id, PubMed URL and abstract from them and store them in SQLite for later use.\n\n### Scrape article texts\nPubMed articles are paywalled and will need institutional credentials for access. So we use SciHub to scrape the articles. \nWe download the PDF of the articles and use PyMuPDF to extract the text and index them in an ElasticSearch server.\n\n### ElasticSearch Server\nWe hosted a simple ElasticServer in a GCP Compute Engine machine. This serves two purposes: First to store the article texts and secondly to serve as document store for HayStack\n\nTo host an ElasticSearch server and to index it, follow the below steps. This is for a Ubuntu 20.04 instance:\n\n```\ncurl -fsSL https://artifacts.elastic.co/GPG-KEY-elasticsearch | sudo apt-key add -\necho \"deb https://artifacts.elastic.co/packages/7.x/apt stable main\" | sudo tee -a /etc/apt/sources.list.d/elastic-7.x.list\nsudo apt update\nsudo apt install elasticsearch python3 python3-pip\nsudo pip3 install elasticsearch pandas\n```\n\nAfter that, run the below commands\n```\nsudo systemctl start elasticsearch\nsudo systemctl enable elasticsearch\nsudo ufw allow 9200\nsudo ufw allow 22\nsudo ufw enable\n```\nAfter this, read the article texts from CSV file and index them\nFor example, here I will be downloading 10,000 articles from a GCS bucket\n\n```\ngsutil cp gs://pubmedbot/10k_articles.csv .\n```\nand index them in a Python shell:\n```\nfrom elasticsearch import Elasticsearch\nfrom elasticsearch.exceptions import RequestError\nimport pandas as pd\n\nnum_rows = 10000\n\nes = Elasticsearch(timeout=1000)\ndicts = []\nid =1\nfor i in range(0, num_rows, 50):\n    df = pd.read_csv('10k_articles.csv', names=['id', 'article'],nrows=50, skiprows=i)\n    df = df[df['article'].str.len() > 4]\n    for c in df['article'].values:\n        try:\n            doc = {'text': c}\n            es.index(index='main', id=id, body=doc)\n            id = id+1\n        except RequestError as e:\n            with open('error.log', 'a+') as f:\n                f.write(e)\n            continue\n\nes.count(index='main')\n```\n\nAfter indexing elasticsearch, we will need to modify the config file. \n```\nsudo nano /etc/elasticsearch/elasticsearch.yml\n```\nChange the following parameters and restart the server:\n\n```\nnetwork.host: 0.0.0.0\ndiscovery.seed_hosts: []\n```\n\n## Choosing Model\n\n[HayStack](https://github.com/deepset-ai/haystack) is used to choose the best model for the application.\n\n### Fine-tuning\nCustomers are presented with an optional field to give feedback and to mark the correct answer. This feedback will be sent to server to fine-tune the model.\n\n### Validation\nWe created about 50 question-answer-context pairs from different articles for validation.\n\n### Metrics\n1. USE Similarity: We use spacy and Universal Sentence Encoder to measure the similarity of the labels and the predictions\n2. BLEU score: We also use Bilingual Evaluation Understudy Score to measure the correctness\n3. Manual Validation: For some iterations, we use manual check and mark the accuracy by hand.\n\n## Deployment\n## Docker Container\nWe have included Dockerfile to deploy the Dash app. \nSet the following environment variables while doing docker run. These are necessary to identify the ElasticSearch server in GCP.\n```\nGCP_APPLICATION_CREDENTIALS\nGCP_PROJECT_ID\nGCP_ZONE_ID\nGCP_INSTANCE_NAME\n``` \n### Web Page\nTO-DO\n### Slack Bot\nTO-DO\n### Voice Bot\nTO-DO"
    },
    {
      "name": "jwiegley/rag-server",
      "stars": 2,
      "img": "https://avatars.githubusercontent.com/u/8460?s=40&v=4",
      "owner": "jwiegley",
      "repo_name": "rag-server",
      "description": "Server for managing documents for indexing and retrievel",
      "homepage": null,
      "language": "Python",
      "created_at": "2025-04-21T13:38:29Z",
      "updated_at": "2025-04-21T15:53:35Z",
      "topics": [],
      "readme": ""
    },
    {
      "name": "Open-Community-Building/hackathon-council-analytics",
      "stars": 2,
      "img": "https://avatars.githubusercontent.com/u/173253715?s=40&v=4",
      "owner": "Open-Community-Building",
      "repo_name": "hackathon-council-analytics",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2024-09-24T06:46:59Z",
      "updated_at": "2025-04-16T11:52:23Z",
      "topics": [],
      "readme": "# hackathon-council-analytics\r\n\r\nHeidelberg City Council automatic document processing, analyzing and chatbot.\r\n\r\n## Table of Contents\r\n\r\n- [hackathon-council-analytics](#hackathon-council-analytics)\r\n  - [Table of Contents](#table-of-contents)\r\n  - [About](#about)\r\n  - [Development](#development)\r\n    - [Prerequisites](#prerequisites)\r\n    - [Installation](#installation)\r\n      - [Change Directory](#change-directory)\r\n      - [Cloning The GitHub Repository](#cloning-the-github-repository)\r\n      - [Create Virtual Environment - Optional](#create-virtual-environment---optional)\r\n      - [Installing Packages](#installing-packages)\r\n      - [Programm Structure](#programm-structure)\r\n  - [Community](#community)\r\n    - [Contribution](#contribution)\r\n    - [Branches](#branches)\r\n  - [FAQ](#faq)\r\n  - [Resources](#resources)\r\n  - [Credit/Acknowledgment](#creditacknowledgment)\r\n  - [License](#license)\r\n\r\n## About\r\nThis project intends to automatically download hundreds of thousand of publicly available documents from Heidelberg City Council's website regularly and transform them by indexation and vectorization before finally run data analytics models against them using NLP and LLM. The purpose is to create a democratic trusted system whereby all stakeholders will be able to search, analyze and get answer to their questions quickly and transparently.\r\n\r\n## Quick Start\r\n\r\nA full version of the Chatbot including all required software components is available as docker image. \r\nOnly two components have to be added manually: \r\n1. A vector database with publically available, embedded documents of Heidelberg City Council can be downloaded [from Nextcloud](https://nc.openheidelberg.de/s/WfoDMTiqqeT6eg2).\r\n2. Two personalized configuration files, namely ``config.toml`` and ``secrets.toml`` which are based on [``config_sample.toml``](https://github.com/Open-Community-Building/hackathon-council-analytics/blob/main/src/config_sample.toml) and [``secrets_sample.toml``](https://github.com/Open-Community-Building/hackathon-council-analytics/blob/main/src/secrets_sample.toml). The scripts search for it in directory `` /root/.config/hca/``. Otherwise, the external volumes of the service *hca* in ``docker-compose.yml`` have to be adjusted. \r\n\r\nAfter linking the docker image to the configuration settings, the container can be build by first pulling the exsting image and second execute the docker compose: \r\n\r\n```\r\ndocker pull chrisbtt/council-analytics:latest\r\n```\r\n```\r\ndocker compose up -d\r\n```\r\n\r\nThis procedure will open up a port for the ChatBot: *https://localhost:8501*. \r\n\r\nFor code development, the docker image can also be build from source including new adjustments instead of pulling from Docker Hub: \r\n```\r\ndocker build . -t local/council-analytics\r\n```\r\nThe filename has to be adjusted in ``docker-compose.yml`` at *service:hca:image*\r\n\r\n\r\n## Development\r\n\r\n### Prerequisites\r\nBefore installing required packages/libraries make sure the following prerequisites on your development machine or virtual environment (Recommended) are satisfied. A Python Virtual Environment is an isolated directory with a particular file structure where you can work on your Python projects, separately from your global-installed Python.\r\n\r\nThere are many variations of virtual env packages/tools, that allows you to create virtual environments such as [Pipenv & Virtual Environments](https://docs.python-guide.org/dev/virtualenvs/), [Managing Environments with Conda](https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html) but you can start with the [Python docs on Virtual Environments](https://docs.python.org/3/tutorial/venv.html) if you are not familiar or using one yet.\r\n\r\n* Git - [Download & Install Git](https://git-scm.com/downloads). OSX and Linux machines typically have this already installed.\r\n* Python - [Download & Install Python 3.11.10](https://www.python.org/downloads/release/python-31110/).\r\n* Hard Drive Space - You need between 3GB to 4GB of free hard drive space for this project.\r\n\r\n### Installation\r\n\r\nOnce you've followed all the steps in the prerequisites, you're just a few steps away from starting to develop and contribute to this project.\r\n\r\nThis project comes with a file called requirements.txt which contains the list of all required packages with their corresponding version number you need to start this project.\r\n\r\n#### Change Directory\r\n\r\nFrom command-line terminal change directory where you would like to save this project on your machine.\r\n\r\ne.g. with Windows command-line\r\n\r\n```\r\n$ ~ > cd Open-Community-Building\r\n```\r\n\r\n#### Cloning The GitHub Repository\r\n\r\nFrom command-line terminal within the directory for your project, clone the project with git command below.\r\n\r\n```\r\n$ git clone https://github.com/Open-Community-Building/hackathon-council-analytics.git\r\n```\r\n\r\n#### Create Virtual Environment - Optional\r\n\r\n- Change directory to the root folder of the cloned project.\r\n- Make sure to be created virtual environment points to correct version of the python on your machine.\r\n- Run the command to create the virtual environment.\r\n\r\nPlease follow the virtual environment creation links above for the detailed instructions. \r\n\r\ne.g. on Windows machine with Pipenv (To work with Pipenv you need first to install the package globally on your system with command: pip install pipenv)\r\n\r\n```\r\n$ ~ \\Open-Community-Building\\hackathon-council-analytics pipenv shell --python path\\to\\your\\python\\python.exe\r\n```\r\n\r\n#### Installing Packages\r\n\r\nRun below command within the root folder of the cloned project to install the required packages listed in requirements.txt file.\r\n\r\ne.g. 1- If you are using global environment.\r\n\r\n```\r\n$ pip install -r requirements.txt\r\n```\r\n\r\ne.g. 2- If you are using virtual environment with pipenv. Please note pipenv will create Profile and Profile.lock files for package and dependency management.\r\n\r\n```\r\n$ pipenv install -r requirements.txt\r\n```\r\n\r\n#### Programm Structure\r\n```mermaid\r\n\r\nclassDiagram\r\n    direction LR\r\n    \r\n    web_app -- RAG_LLM\r\n    embedding --|> preprocessing\r\n    update    --|> preprocessing\r\n    extractor  <|-- preprocessing\r\n    download   <|-- preprocessing\r\n    \r\n    class web_app\r\n    web_app : title\r\n    web_app : header\r\n    web_app : query_rag_llm(user_input)\r\n    \r\n    class RAG_LLM\r\n    RAG_LLM : -index_dir\r\n    RAG_LLM : -token\r\n    RAG_LLM : -llm_name\r\n    RAG_LLM : -embed_mame\r\n    RAG_LLM : -system_prompt\r\n    RAG_LLM : -summary_prompt\r\n    RAG_LLM : index\r\n    RAG_LLM : +query_rag_llm(user_query)\r\n    RAG_LLM : _configure_query_engine(index)\r\n    RAG_LLM : _huggingface_login(token)\r\n    RAG_LLM : _init_llm_model(llm_name, token)\r\n    RAG_LLM : _init_embedding_model(embed_mame)\r\n    RAG_LLM : _load_index_storage(index_dir)\r\n    RAG_LLM : _display_prompt_dict()?\r\n    \r\n    class embedding\r\n    embedding : -directory\r\n    embedding : -storage_dir\r\n    embedding : load_txt_files(directory)\r\n    embedding : _initialize_embedding_model()\r\n    embedding : _initFAISSIndex(embedding_model)\r\n    embedding : load_txt_files_nextcloud(start_idx, end_idx)?\r\n    \r\n    class update\r\n    update : storage_dir\r\n    update : embedding_model\r\n    update : vector_store\r\n    update : -directory\r\n    update : load_existing_index(storage_dir)\r\n    update : _initialize_embedding_model()\r\n    update : _init_vector_store(emebedding_model, faiss_index)\r\n    update : _load_txt_files(directory)\r\n    update : _save_index_and_metadata(index, metadata, storage_dir)\r\n    \r\n    class preprocessing\r\n    preprocessing : *nextcloud_folder\r\n    preprocessing : *nextcloud_user\r\n    preprocessing : *nextcloud_password\r\n    preprocessing : *nextcloud_url\r\n    preprocessing : download_from_nextcloud(folder, filename)\r\n    preprocessing : upload_to_nextcloud(folder, filename, content...)\r\n    preprocessing : _process_pdf(idx)\r\n    preprocessing : _parallel_process_pdf(args)\r\n    \r\n    class extractor\r\n    extractor: extract_text(doc) -> text\r\n    extractor: save_text(text, filename)\r\n    \r\n    class download\r\n    download: -url\r\n    download: download_pdf(idx)\r\n    download: _extract_text(doc) -> text\r\n    download: _save_text(text, filename)\r\n    download: _request_pdf(idx)\r\n\r\n```\r\n\r\n## Community\r\n\r\n ### Contribution\r\n\r\n Your contributions are always welcome and appreciated. Following are the things you can do to contribute to this project.\r\n\r\n 1. **Report a bug** <br>\r\n If you think you have encountered a bug, and we should know about it, feel free to report it [here](https://github.com/Open-Community-Building/hackathon-council-analytics/issues) and I will take care of it.\r\n\r\n 2. **Request a feature** <br>\r\n You can also request for a feature [here](), and if it will viable, it will be picked for development.  \r\n\r\n 3. **Create a pull request** <br>\r\n It can't get better then this, your pull request will be appreciated by the community. You can get started by picking up any open issues from [here]() and make a pull request.\r\n\r\n > If you are new to open-source, make sure to check read more about it [here]() and learn more about creating a pull request [here]).\r\n\r\n\r\n ### Branches\r\n\r\n Version is frequently updated.\r\n\r\n1. **`devel_hanna_schmidt`** or **`feat_optimize_data_analysis`** are example of a developer or feature branches.\r\n\r\n2. **`main`** is the master branch.\r\n\r\n3. You can create a developer or feature branches but they should get merged with the master/main branch.\r\n\r\n**Steps to work with developer or feature branch**\r\n\r\n1. To start working on a developer branch, create a new branch prefixed with `devel`, followed by developer first name followed by an underscore (**`_`**) and and finally developer surname (ie. `devel_hanna_schmidt`)\r\n2. To start working on a new feature branch, create a new branch prefixed with `feat`, followed by an underscore (**`_`**) and and finally followed by feature name. (`feat_FEATURE_NAME`)\r\n3. Once you are done with your changes, you can raise PR.\r\n\r\n**Steps to create a pull request**\r\n\r\n1. Make a PR request.\r\n2. Comply with the best practices and guidelines.\r\n\r\nAfter this, changes will be merged.\r\n\r\n\r\n## FAQ\r\n\r\n\r\n## Resources\r\n\r\n\r\n## Credit/Acknowledgment\r\n\r\n\r\n## License\r\n"
    },
    {
      "name": "oryx1729/mcp-haystack",
      "stars": 2,
      "img": "https://avatars.githubusercontent.com/u/78848855?s=40&v=4",
      "owner": "oryx1729",
      "repo_name": "mcp-haystack",
      "description": "Unofficial MCP Integration for Haystack",
      "homepage": null,
      "language": "Python",
      "created_at": "2025-03-11T19:15:38Z",
      "updated_at": "2025-03-12T14:56:53Z",
      "topics": [],
      "readme": "# MCP Haystack Integration\n\nThis repository provides an integration between [Haystack](https://haystack.deepset.ai/) and the [Model Context Protocol (MCP)](https://modelcontextprotocol.io/introduction). MCP is an open protocol that standardizes how applications provide context to LLMs, similar to how USB-C provides a standardized way to connect devices.\n\n## Overview\n\nThe MCP Haystack integration allows you to use MCP-compatible tools within Haystack pipelines. This enables your LLM applications to interact with external services and APIs through a standardized protocol, making it easier to build powerful, context-aware applications.\n\nThis integration includes a demo with the Rijksmuseum API, showcasing how to use MCP to search for artworks, get detailed information, and interact with the museum's collection.\n\n## Installation\n\n### Install from Source\n\n```bash\ngit clone https://github.com/oryx1729/mcp-haystack.git\ncd mcp-haystack\npip install -e .\n```\n\n\n## Docker\n\nThis repository includes a Dockerfile that sets up a complete environment with an MCP Server Demo.\n\n### Building the Docker Image\n\n```bash\ndocker build -t mcp-haystack-demo .\n```\n\n### Running the Docker Container\n\n```bash\ndocker run -p 8888:8888 mcp-haystack-demo\n```\n\n## Rijksmuseum Demo\n\nThis repository includes a demo notebook that showcases how to use the MCP Haystack integration with the Rijksmuseum API. The demo allows you to:\n\n- Search for artworks in the Rijksmuseum collection\n- Get detailed information about specific artworks\n- View artwork images\n- Explore collections created by Rijksstudio users\n- Get a chronological timeline of an artist's works\n\nTo run the demo:\n\n1. Obtain a Rijksmuseum API key from the [Rijksmuseum website](https://www.rijksmuseum.nl/en/research/conduct-research/data)\n2. Run the Docker container or open the `examples/rijksmuseum_demo.ipynb` notebook\n\n"
    },
    {
      "name": "ammar-s847/CV-Drone-Detection",
      "stars": 2,
      "img": "https://avatars.githubusercontent.com/u/40363389?s=40&v=4",
      "owner": "ammar-s847",
      "repo_name": "CV-Drone-Detection",
      "description": "Object detection for aerial vehicles with bounding box prediction",
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2025-01-08T14:20:59Z",
      "updated_at": "2025-03-13T22:59:35Z",
      "topics": [],
      "readme": "# Computer Vision for Drone Detection\nObject detection for aerial vehicles (drones, helicopters, airplanes, etc) with bounding box prediction.\n\n## Dataset\nThe dataset we used is available on Kaggle and Roboflow.\n\nKaggle:\nhttps://www.kaggle.com/datasets/cybersimar08/drone-detection\n\nRoboflow:\nhttps://universe.roboflow.com/ahmedmohsen/drone-detection-new-peksv\n\n## Metrics and Evaluation\nUsing the following metrics/benchmarks, we'll be evaluating each model's ability to distinguish between aerial vehicle types and produce bounding boxes for them within the image:\n- Mean Average Precision (mAP)\n- Intersection over Union (IoU)\n\nAlongside the metrics above, we'll be profiling the models to determine performance:\n- Inference Latency\n- Model Size\n- Training Time\n"
    },
    {
      "name": "avnlp/rankers",
      "stars": 2,
      "img": "https://avatars.githubusercontent.com/u/123800833?s=40&v=4",
      "owner": "avnlp",
      "repo_name": "rankers",
      "description": null,
      "homepage": "",
      "language": "Python",
      "created_at": "2025-02-03T17:33:48Z",
      "updated_at": "2025-02-19T16:28:41Z",
      "topics": [
        "listwise",
        "llm-rankers",
        "pairwise",
        "pointwise",
        "ranker",
        "setwise"
      ],
      "readme": "# Rankers\n\n**Paper:** [LLM Rankers](paper/rankers.pdf)\n\n- Haystack components were created for the Listwise, Setwise, Pairwise techniques.\n- Also, Haystack components were created for the RankZephyr and RankVicuna (RankLLM).\n- The pipelines for setwise, pairwise, listwise and pointwise rankers were created on the FIQA, Sci-Fact, NF-Corpus, TREC-19, and TREC-20 datasets.\n- The Dense retrieval pipelines were created using the Mistral, Llama-3, and Phi-3 models.\n- The Dense retrieval pipeline using the LLama-3 model and setwise ranking gave the best performance.\n- Each dense retrieval pipeline was created for the Mistral, Phi-3 and LLama-3 models and evaluated on the NDCG metric.\n\n## Pointwise Ranking\n- In the pointwise ranking method, the reranker takes both the query and a candidate document to directly generate a relevance score. These independent scores assigned to each document are then used to reorder the from the set of documents.\n- LLMs are asked to generate whether the candidate document provided is relevant to the query, with the process repeated for each candidate document.\n## Pairwise Ranking\n- The Pairwise Ranking Prompting, a pair of candidate items along with the user query serve as prompts to guide the LLMs to determine which document is the most relevant to the given query.\n- Pairs are then independently fed into the LLM, and the preferred document is determined for each pair. Subsequently, an aggregation function is employed to assign a score to each document based on the inferred pairwise preferences, and the final ranking is established based on the total score assigned to each document.\n## Listwise Ranking\n- The Listwise Reranker with a LLM takes the query and a list of documents as input and returns a reordered list of the input document identifiers.\n- Current listwise approaches use a sliding window method. This involves re-ranking a window of candidate documents, starting from the bottom of the original ranking list and progressing upwards.\n## Setwise Ranking\n- The Setwise prompting approach instructs LLMs to select the most relevant document to the query from a set of candidate documents.\n- The Setwise prompting technique improves the efficiency of Pairwise prompting (PRP) by comparing multiple documents at each step, as opposed to just a pair. The Setwise prompting approach instructs LLMs to select the most relevant document to the query from a set of candidate documents.\n\n## Results\n- The RankLlama and RankZephyr rankers performed best on the FIQA, TREC-19, SciFact and NFCorpus datasets.\n- The setwise ranker with the heapsort method using the LLama-3 model gave the best performance for the FIQA, TREC-19, SciFact, and NFcorpus datasets.\n- The RankLlama and RankZephyr models with the pointwise and listwise ranking gave the best output.  \n"
    },
    {
      "name": "lichingngamba/talk_to_page",
      "stars": 2,
      "img": "https://avatars.githubusercontent.com/u/63799239?s=40&v=4",
      "owner": "lichingngamba",
      "repo_name": "talk_to_page",
      "description": "A project to chat with a given url particularly research paper links.",
      "homepage": null,
      "language": "Python",
      "created_at": "2025-01-31T20:12:21Z",
      "updated_at": "2025-03-06T05:22:41Z",
      "topics": [],
      "readme": "## Quick Links\n* [ Talk to page](#talk-to-page)\n* [Getting Started](#getting-started)\n* [Flowchart](#flowchart)\n* [Screen Recording](#screen-recording)\n* [Features](#features)\n* [Contributing](#contributing)\n\n## Talk to page\n\n- This project let you chat with a given url particularly research paper links. \n- The poject aims to use the advancements of LLM while using the web pages as the context.\n- With advance RAG (Retrieval Augument techniques) we can chat with the web pages and get the most relevant information. \n\n## Getting Started\n\n> [!IMPORTANT]\n> In the `.env`, file make sure to put your own `Huggingface API token` enable with Inference.\n\n- Clone the repository from github.\n- cd talk_to_page\n\n> [!TIP] \n> Using Makefile\n\n- RUN `make start` to start the agent for the first time\n- RUN `make run` to restart the agent\n- Project will be available at http://localhost:8501\n\n\n> [!TIP] \n> Using Doker\n\n- `docker build -t talk-to-page:latest -f Dockerfile .`\n- `docker run -p 8501:8501 talk-to-page:latest`\n- Project will be available at http://localhost:8501\n\n> [!TIP]\n> Using pip\n\n- pip install .\n- CMD `start`\n-Project will be available at http://localhost:8501\n\n## Flowchart\n![flowchart](media/flowchart.png)\n\n> [!NOTE]\n> Searching the internet component is not release!! Please contact the developer for more information.\n\n\n## Screen Recording\n<video controls src=\"media/record_talk_to_url.mp4\" title=\"Title\"></video>\n\n## Features\n- Chat with web pages\n- Get the most relevant information\n- Use the web pages as the context\n- Use the advancements of LLM\n- In the Query provide a url, [there is no need of a specific format]\n\n## Contributing\n\n- Fork the repository\n- Create a new branch\n\n> [!NOTE]\n> The project will use Haystack and webui to chat with the web pages and get the most relevant information.\n\n> [!TIP]\n> The project development is pause * [ ].\n\n> [!NOTE]\n> The project is open source and free to use.\n> The project is a mid level RAG based AI Agent, which can be used to chat with web pages and get the most relevant information.\n> For more advanced RAG based AI Agent, please contact the developer.\n"
    },
    {
      "name": "superleesa/haystack-pydantic",
      "stars": 2,
      "img": "https://avatars.githubusercontent.com/u/88019950?s=40&v=4",
      "owner": "superleesa",
      "repo_name": "haystack-pydantic",
      "description": "A thin wrapper around haystack Component and Pipeline for typing (outputs) with Pydantic models",
      "homepage": "",
      "language": "Python",
      "created_at": "2025-01-22T20:31:18Z",
      "updated_at": "2025-01-25T12:23:53Z",
      "topics": [],
      "readme": "# haystack-pydantic\n\nA thin wrapper around haystack Component and Pipeline for typing (outputs) with Pydantic models\n\n```shell\npip install haystack-pydantic\n```\n\n```python\nfrom haystack_pydantic import Pipeline\nfrom haystack_pydantic import component\nfrom pydantic import BaseModel\n\nclass TestOutput(BaseModel):\n    output1: str\n    output2: str\n\n@component\nclass TestComponent:\n    # define pydantic model as output type (instead of the `output_types` decorator)\n    def run(self, input1: str, input2: str) -> TestOutput:  \n        return TestOutput(output1=input1, output2=input2)  # return the pydantic model\n\n# use the component standalone\ntest_component = TestComponent()\ntest_component.run(\"input1\", \"input2\")  # TestOutput(output1=\"input1\", output2=\"input2\")\n\n# use the component within a pipeline\npipeline = Pipeline()\npipeline.add_component(\"test_component\", test_component)\npipeline.run(data={\"input1\": \"input1\", \"input2\": \"input2\"})  # {\"test_component\": TestOutput(output1=\"input1\", output2=\"input2\")}\n```"
    },
    {
      "name": "rajs1006/AIDataAnalysisAgent",
      "stars": 2,
      "img": "https://avatars.githubusercontent.com/u/39014867?s=40&v=4",
      "owner": "rajs1006",
      "repo_name": "AIDataAnalysisAgent",
      "description": null,
      "homepage": null,
      "language": "JavaScript",
      "created_at": "2024-11-23T06:32:12Z",
      "updated_at": "2025-03-20T12:37:55Z",
      "topics": [],
      "readme": "# AIDataAgent"
    },
    {
      "name": "simiyu-dess/WrenAI",
      "stars": 2,
      "img": "https://avatars.githubusercontent.com/u/38722733?s=40&v=4",
      "owner": "simiyu-dess",
      "repo_name": "WrenAI",
      "description": null,
      "homepage": null,
      "language": "TypeScript",
      "created_at": "2025-01-12T20:22:29Z",
      "updated_at": "2025-03-03T23:18:44Z",
      "topics": [],
      "readme": "\n<p align=\"center\" id=\"top\">\n  <a href=\"https://getwren.ai/?utm_source=github&utm_medium=title&utm_campaign=readme\">\n    <picture>\n      <source media=\"(prefers-color-scheme: light)\" srcset=\"./misc/wrenai_logo.png\">\n      <img src=\"./misc/wrenai_logo_white.png\" width=\"300px\">\n    </picture>\n    <h1 align=\"center\">Wren AI</h1>\n  </a>\n</p>\n\n<p align=\"center\">\n  <a aria-label=\"Follow us on X\" href=\"https://x.com/getwrenai\">\n    <img alt=\"\" src=\"https://img.shields.io/badge/-@getwrenai-blue?style=for-the-badge&logo=x&logoColor=white&labelColor=gray&logoWidth=20\">\n  </a>\n  <a aria-label=\"Releases\" href=\"https://github.com/canner/WrenAI/releases\">\n    <img alt=\"\" src=\"https://img.shields.io/github/v/release/canner/WrenAI?logo=github&label=GitHub%20Release&color=blue&style=for-the-badge\">\n  </a>\n  <a aria-label=\"License\" href=\"https://github.com/Canner/WrenAI/blob/main/LICENSE\">\n    <img alt=\"\" src=\"https://img.shields.io/github/license/canner/WrenAI?color=blue&style=for-the-badge\">\n  </a>\n  <a aria-label=\"Join the community on GitHub\" href=\"https://discord.gg/5DvshJqG8Z\">\n    <img alt=\"\" src=\"https://img.shields.io/badge/-JOIN%20THE%20COMMUNITY-blue?style=for-the-badge&logo=discord&logoColor=white&labelColor=grey&logoWidth=20\">\n  </a>\n  <a aria-label=\"Canner\" href=\"https://cannerdata.com/?utm_source=github&utm_medium=badge&utm_campaign=readme\">\n    <img src=\"https://img.shields.io/badge/%F0%9F%A7%A1-Made%20by%20Canner-blue?style=for-the-badge\">\n  </a>\n</p>\n\n> Open-source AI Agent that empowers data-driven teams to chat with their data to generate Text-to-SQL, charts, spreadsheets, reports, and BI. \n\n<p align=\"center\">\n  <img src=\"./misc/wren_workflow.png\">\n</p>\n\n## 🕶 Try it yourself!\n\n#### Ask any questions\n\nhttps://github.com/user-attachments/assets/e4e78006-0088-4094-b58f-8868689bf787\n\n#### GenBI\n\nhttps://github.com/user-attachments/assets/90ad1d35-bb1e-490b-9676-b29863ff090b\n\n\n\n👉 Try with your data on [Wren AI Cloud](https://getwren.ai/?utm_source=github&utm_medium=content&utm_campaign=readme) or [Install in your local environment](https://docs.getwren.ai/oss/installation/?utm_source=github&utm_medium=content&utm_campaign=readme)\n\n\n## 🎯 Our Vision & Mission\n\nWren AI’s mission is to democratize data by bringing AI agents with SQL ability to any data source.\n\n🤩 [Learn more about Wren AI and our Mission](https://docs.getwren.ai/oss/overview/introduction/?utm_source=github&utm_medium=content&utm_campaign=readme)\n\n## 🤖 A User-Centric, End-to-End Open-source SQL AI Agent - Text-to-SQL Total Solution\n\n### 1. Talk to Your Data in Any Language\n\n> Wren AI speaks [your language](https://docs.getwren.ai/oss/guide/settings/pj_settings#change-project-language?utm_source=github&utm_medium=content&utm_campaign=readme), such as English, German, Spanish, French, Japanese, Korean, Portuguese, Chinese, and more. Unlock valuable insights by asking your business questions to Wren AI. It goes beyond surface-level data analysis to reveal meaningful information and simplifies obtaining answers from lead scoring templates to customer segmentation.\n\n<p align=\"center\">\n  <img src=\"./misc/wren-lang.png\" style=\"max-width: 700px\"/>\n</p>\n\n### 2. AI-powered Data Exploration Features\n\n> Beyond just retrieving data from your databases, Wren AI now answers exploratory questions like “What data do I have?” or “What are the columns in my customer tables?” Additionally, our AI dynamically generates recommended questions and intelligent follow-up queries tailored to your context, making data exploration smarter, faster, and more intuitive. Empower your team to unlock deeper insights effortlessly with AI.\n\n<p align=\"center\">\n  <img src=\"./misc/AI-generated-understanding_recommend_questions.png\" style=\"max-width: 700px\"/>\n</p>\n\n### 3. Semantic Indexing with a Well-Crafted UI/UX\n\n> Wren AI has implemented a [semantic engine architecture](https://www.getwren.ai/post/how-we-design-our-semantic-engine-for-llms-the-backbone-of-the-semantic-layer-for-llm-architecture/?utm_source=github&utm_medium=content&utm_campaign=readme) to provide the LLM context of your business; you can easily establish a logical presentation layer on your data schema that helps LLM learn more about your business context.\n\n<p align=\"center\">\n  <img src=\"./misc/wren-modeling.png\" style=\"max-width: 700px\"/>\n</p>\n\n### 4. Generate SQL Queries with Context\n\n> With Wren AI, you can process metadata, schema, terminology, data relationships, and the logic behind calculations and aggregations with [“Modeling Definition Language”](https://docs.getwren.ai/oss/engine/concept/what_is_mdl/?utm_source=github&utm_medium=content&utm_campaign=readme), reducing duplicate coding and simplifying data joins.\n\n<p align=\"center\">\n  <img src=\"./misc/wren-context.png\" style=\"max-width: 700px\"/>\n</p>\n\n### 5. Get Insights without Writing Code\n\n> When starting a new conversation in Wren AI, your question is used to find the most relevant tables. From these, LLM generates the most relevant question for the user. You can also ask follow-up questions to get deeper insights.\n\n<p align=\"center\">\n  <img src=\"./misc/wren-insight.png\" style=\"max-width: 700px\"/>\n</p>\n\n### 6. GenBI\n\n> The GenBI feature empowers users with AI-generated summaries that provide key insights alongside SQL queries, simplifying complex data. Instantly convert query results into AI-generated reports, charts, transforming raw data into clear, actionable visuals. With GenBI, you can make faster, smarter decisions with ease.\n\n<p align=\"center\">\n  <img src=\"./misc/wren-genbi.png\" style=\"max-width: 700px\"/>\n</p>\n\n### 7. Easily Export and Visualize Your Data\n\n> Wren AI provides a seamless end-to-end workflow, enabling you to connect your data effortlessly with popular analysis tools such as [Excel](https://docs.getwren.ai/oss/guide/integrations/excel-add-in/?utm_source=github&utm_medium=content&utm_campaign=readme) and [Google Sheets](https://docs.getwren.ai/oss/guide/integrations/google-add-on/?utm_source=github&utm_medium=content&utm_campaign=readme). This way, your insights remain accessible, allowing for further analysis using the tools you know best.\n\n<p align=\"center\">\n  <img src=\"./misc/wren-excel.png\" style=\"max-width: 700px\"/>\n</p>\n\n## 🤔 Why Wren AI?\n\nWe focus on providing an open, secure, and accurate SQL AI Agent for everyone.\n\n### 1. Turnkey Solution\n\n> Wren AI makes it easy to onboard your data. Discover and analyze your data with our user interface. Effortlessly generate results without needing to code.\n\n### 2. Secure SQL Generation\n\n> We use RAG architecture to leverage your schema and context, generating SQL queries without requiring you to expose or upload your data to LLM models.\n\n### 3. Open-source End-to-end Solution\n\n> Deploy Wren AI anywhere you like on your own data, LLM APIs, and environment, it's free.\n\n## 🤖 Wren AI Text-to-SQL Agentic Architecture\n\nWren AI consists of three core services:\n\n- ***[Wren UI](https://github.com/Canner/WrenAI/tree/main/wren-ui):*** An intuitive user interface for asking questions, defining data relationships, and integrating data sources.\n\n- ***[Wren AI Service](https://github.com/Canner/WrenAI/tree/main/wren-ai-service):*** Processes queries using a vector database for context retrieval, guiding LLMs to produce precise SQL outputs.\n\n- ***[Wren Engine](https://github.com/Canner/wren-engine):*** Serves as the semantic engine, mapping business terms to data sources, defining relationships, and incorporating predefined calculations and aggregations.\n\n<p align=\"center\">\n  <img src=\"./misc/how_wrenai_works.png\" style=\"max-width: 1000px;\">\n</p>\n\n## ❤️ Knowledge Sharing From Wren AI\n\nWant to get our latest sharing? [Follow our blog!](https://www.getwren.ai/blog/?utm_source=github&utm_medium=content&utm_campaign=readme)\n\n## 🚀 Getting Started\n\nUsing Wren AI is super simple, you can set it up within 3 minutes, and start to interact with your data!\n\n- Visit our [Installation Guide of Wren AI](http://docs.getwren.ai/oss/installation).\n- Visit the [Usage Guides](https://docs.getwren.ai/oss/guide/connect/overview) to learn more about how to use Wren AI.\n\n## 📚 Documentation\n\nVisit [Wren AI documentation](https://docs.getwren.ai/oss/overview/introduction) to view the full documentation.\n\n## 🛠️ Contribution\n\nWant to contribute to Wren AI? Check out our [Contribution Guidelines](https://github.com/Canner/WrenAI/blob/main/CONTRIBUTING.md).\n\n## ⭐️ Community\n\n- Welcome to our [Discord server](https://discord.gg/5DvshJqG8Z) to give us feedback!\n- If there are any issues, please visit [GitHub Issues](https://github.com/Canner/WrenAI/issues).\n- Explore our [public roadmap](https://github.com/orgs/Canner/projects/12/views/1) to stay updated on upcoming features and improvements!\n\nPlease note that our [Code of Conduct](./CODE_OF_CONDUCT.md) applies to all Wren AI community channels. Users are **highly encouraged** to read and adhere to them to avoid repercussions.\n\n## 🎉 Our Contributors\n<a href=\"https://github.com/canner/wrenAI/graphs/contributors\">\n  <img src=\"https://contrib.rocks/image?repo=Canner/WrenAI\" />\n</a>\n\n<p align=\"right\">\n  <a href=\"#top\">⬆️ Back to Top</a>\n</p>\n"
    },
    {
      "name": "avnlp/vectordb",
      "stars": 2,
      "img": "https://avatars.githubusercontent.com/u/123800833?s=40&v=4",
      "owner": "avnlp",
      "repo_name": "vectordb",
      "description": "Created pipelines for Semantic Search, Metadata Filtering, Hybrid Search, Reranking, and Retrieval-Augmented Generation (RAG) for the TriviaQA, ARC, PopQA, FactScore, and Edgar datasets. These pipelines have been implemented for the Pinecone, Weaviate, Milvus, Qdrant and Chroma vector databases.",
      "homepage": "",
      "language": "Python",
      "created_at": "2025-01-02T04:53:51Z",
      "updated_at": "2025-02-01T19:03:33Z",
      "topics": [
        "haystack",
        "langchain",
        "llm",
        "milvus",
        "pinecone",
        "vector-database",
        "weaviate"
      ],
      "readme": "# Vector Databases\n\n- Vector databases store and manage data as high-dimensional vectors, enabling similarity-based retrieval.\n- They are designed to efficiently find the most relevant entries by comparing vector embeddings, which represent the semantic meaning of data.\n- Vector databases are integral to Retrieval-Augmented Generation (RAG) pipelines, where they enhance the retrieval of contextually relevant information before the response generation stage.\n\nThe main goal of this repo is to compare and contrast the functionality of the vector databases. We compare the following vector databases:\n\n- Pinecone\n- Weaviate\n- Chroma\n- Milvus\n- Qdrant\n## Installation\n\n```bash\npip install git+https://github.com/avnlp/vectordb\n```\n\n## Vector Embeddings\n\nThere are two main types of vector embeddings:\n\n- Dense Embeddings\n- Sparse Embeddings\n\n### Dense Embeddings\n\n- A dense embedding represents the semantic meaning of a piece of text in a high-dimensional numerical representation, where each element (or dimension) in the vector contains a real-valued number that contributes to the overall representation of the data's features.\n- Pinecone, Weaviate, and Chroma support creation of indexes/collections with dense embeddings.\n\n### Sparse Embeddings\n\n- Sparse vectors have very large number of dimensions, where only a small proportion of values are non-zero.\n- When used for keywords search, each sparse vector represents a document; the dimensions represent words from a dictionary, and the values represent the importance of these words in the document.\n- Keyword search algorithms like the BM25 algorithm compute the relevance of text documents based on the number of keyword matches, their frequency, and other factors based on token presence in the document.\n\n### Hybrid Search\n\n- Hybrid search in Pinecone leverages a single sparse-dense index, enabling simultaneous retrieval based on both keyword relevance (sparse vector) and semantic context (dense vector). Querying this index requires providing both the sparse and dense vector representations of the query.\n- During Index creation both dense and sparse embeddings need to be computed and specified for each document. This is an additional step that can be time-consuming for large datasets.\n- Weaviate’s Hybrid search combines BM25-based keyword search and vector-based semantic search by merging the results from both methods. To enable hybrid search, the query must specify `hybrid=True`, allowing for retrieval that balances exact term matching and contextual understanding.\n- Weaviate does not allow you to use custom sparse vectors for hybrid search. Only dense embeddings are required when creating the collection.\n- Chroma does not currently support hybrid search capabilities.  \n\n## Storing vector embeddings in a vector database\n\n- The Pinecone vector database stores vector embeddings in an Index. Each index can be partitioned into multiple namespaces.\n- Every index is made up of one or more namespaces and they are uniquely identified by a namespace name. Every record exists in exactly one namespace.\n- Queries and other operations are confined to one namespace, so different requests can search different subsets of your index.\n\nExample: Creating an Index with multiple namespaces\n\n```python\npinecone_vector_db = PineconeVectorDB(api_key=pinecone_api_key)\npinecone_vector_db.create_index(\n    index_name=\"arc_index\",\n    dimension=768,\n    metric=\"dotproduct\",\n    spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\"),\n)\npinecone_vector_db.upsert(data=arc_train, namespace=\"train\")\npinecone_vector_db.upsert(data=arc_dev, namespace=\"dev\")\n```\n\nExample: Querying a specific namespace\n\n```python\npinecone_vector_db.query(vector=query_embedding, namespace=\"dev\", top_k=5)\n```\n\n- The Weaviate vector database stores vector embeddings in a Collection.\n- Multiple independent collections can be part of a single Weaviate Cluster.\n\nExample: Creating multiple independent collections in a Weaviate Cluster\n\n```python\nweaviate_vector_db = WeaviateVectorDB(cluster_url=weaviate_cluster_url, api_key=weaviate_api_key)\nweaviate_vector_db.create_collection(collection_name=\"arc_train\")\nweaviate_vector_db.create_collection(collection_name=\"arc_dev\")\n```\n\n- The Chroma vector database stores vector embeddings in a Collection. It does not support namespaces. Each collection is independent.\n\nExample: Creating a Chroma Collection\n\n```python\nchroma_vector_db = ChromaVectorDB(path=\"./chroma\")\nchroma_vector_db.create_collection(name=\"arc_train\")\n```\n\n## Metadata Filtering\n\n- Metadata fields are a way to add information to individual vectors to give them more meaning. By adding metadata to your vectors, you can filter by those fields at query time. You can limit your vector search based on metadata.\n- All three vector databases let you attach metadata key-value pairs to vectors in an index/collection, and specify filter expressions when you query it.\n- The metadata is included in the payload when you add your vectors.\n\nExample: Metadata Filters in Pinecone\n\nPinecone supports metadata filtering using the `filter` parameter. The `filter` parameter takes a dictionary that specifies a filter expression.\n\n```python\nquery_response = pinecone_vector_db.query(\n    vector=dense_question_embedding,\n    sparse_vector=sparse_question_embedding,\n    top_k=10,\n    include_metadata=True,\n    namespace=\"test_namespace\",\n    filter={\"$and\": [{\"id\": {\"$eq\": \"752235\"}}, {\"title\": {\"$eq\": \"Pete Sampras\"}}]},\n)\n```\n\nExample: Metadata Filters in Weaviate\n\nMetadata filters in Weaviate are passed using the `filters` parameter. A list of `Filter` objects can be passed to the `filters` parameter.\n\n```python\nquery_response = weaviate_vector_db.query(\n    vector=dense_question_embedding,\n    query_string=question,\n    limit=10,\n    hybrid=True,\n    alpha=0.5,\n    filters=Filter.by_property(\"text\").like(\"Pete Sampras\"),\n)\n```\n\nExample: Metadata Filters in Chroma\n\nMetadata filters in ChromaDB can be used to filter documents based on specific metadata or content criteria during a query. They are passed using `where_document` parameter.\n\n```python\nquery_response = chroma_vector_db.query(\n    query_embedding=dense_question_embedding, n_results=10, where_document={\"$contains\": \"Pete Sampras\"}\n)\n```\n"
    },
    {
      "name": "BlutzerZ/chatbot-akademik",
      "stars": 2,
      "img": "https://avatars.githubusercontent.com/u/29569667?s=40&v=4",
      "owner": "BlutzerZ",
      "repo_name": "chatbot-akademik",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2024-10-08T10:04:44Z",
      "updated_at": "2025-03-01T07:22:25Z",
      "topics": [],
      "readme": "\n# Academic Chatbot API\n\nThis Project about using AI chabot to help student to learn about university or other \n\n\n## API Reference\n\nDocumentation about api can be provided by accesing url\n``` \nhttp://localhost/docs \n```\n\n\n## Installation\n\nInstall Academic Chatbot with virtualenv python\n\n```bash\n  python -m venv venv\n  source 'venv\\Scripts\\activate' \n```\n\nInstal Requirements library\n\n```\npip install -r requirements.txt\n```\n\nRename or copy example.env to .env\n```\nDB_USER= #database username\nDB_PASSWORD= #database password\nDB_NAME= #database name\nDB_HOST= #database host\nDB_PORT= #database port\n\nPORT= #webiste port\n\nJWT_SECRET_KEY= #jwt secret\nJWT_ALGORITHM= #algorithm\n```\n\n\n### Migrating \n\n```\nalembic revision -m \"initial\"\nalembic upgrade head\n```\n\n\n### How to run\n\nFor development\n```\nfastapi dev main.py\n```\n\nFor production\n```\nfastapi run main.py\n```\n## Running On Docker\n\nTo run this project on docker container\n\n\n### Setup .env\nchange db_host with container name\n```\nDB_USER= #database username\nDB_PASSWORD= #database password\nDB_NAME= #database name\nDB_HOST= #database host -------> Change this with database container name\nDB_PORT= #database port\n\nPORT= #webiste port\n\nJWT_SECRET_KEY= #jwt secret\nJWT_ALGORITHM= #algorithm\n```\n\n### Build with docker compose\n```\ndocker compose up --build\n```\n\n"
    },
    {
      "name": "huxiaolongyin/Chat2RAG",
      "stars": 2,
      "img": "https://avatars.githubusercontent.com/u/92241411?s=40&v=4",
      "owner": "huxiaolongyin",
      "repo_name": "Chat2RAG",
      "description": "This is a RAG system that supports online-search, weather-query,bus query,  knowledge search, device control and other functions. Provides api interfaces, web pages and docker deployment. Thanks to haystack for the excellent framework.",
      "homepage": "",
      "language": "Python",
      "created_at": "2024-12-27T06:49:16Z",
      "updated_at": "2025-03-28T10:20:35Z",
      "topics": [],
      "readme": "([简体中文](./README_zh.md)|English)\n\n## Overview\n\nWith the rapid development of artificial intelligence and large model technology, it can bring significant changes to all walks of life, and robots are highly dependent on intelligence, and the improvement brought by large models is huge. For example, voice interaction, as an important way of human-computer interaction, can provide a more natural and efficient interactive experience. The use of large models and RAG (Retrieval Augmented Generation) technology can improve the robot's ability in knowledge base retrieval and task control, and achieve more intelligent services\n\nWeb UI Demo Screenshot:\n\n![1734327559151](assets/image/README/1734327559151.png)\n\n![1734327568403](assets/image/README/1734327568403.png)\n\n![1734327575603](assets/image/README/1734327575603.png)\n\n![1734327581312](assets/image/README/1734327581312.png)\n\n## Install\n\n- Precondition:\n\n  Pull projects from github\n\n```shell\ngit pull https://github.com/huxiaolongyin/Chat2RAG\n```\n\n  Configure the.env file\n\n```shell\ncp .env.example .env\nvi.env\n```\n\n### Method 1: Install from source code\n\n- Requirements (Creating a virtual environment is strongly recommended)\n\n```shell\npython >=3.9\n```\n\n- Step1: Install the qdrant vector database\n\n```shell\ndocker run -d --name qdrant -p 6333:6333 -p 6334:6334 qdrant/qdrant:latest\n```\n- Step2: Install the web dependencies\n\n```shell\npip install -r frontend/requirements.txt\n```\n\n- Step3: Install backend dependencies\n\n```shell\npip install .\n```\n- Step4: Start the service\n```shell\n# backend\npython backend/main.py\n# frontend\nstreamlit run frontend/app.py\n```\n\n### Method 2: Install using docker\n\n```shell\ndocker compose up -f docker/docker-compose.yml -d\n```\n\n## Quick start\n\n- Open a web page [http://127.0.0.1:8501](http://127.0.0.1:8501)\n\n## Document\n\n### Features\n\n* **Knowledge base retrieval** : It can retrieve information quickly and accurately from the huge knowledge base and answer users' questions.\n* **Function call**: According to the user's instructions, perform the corresponding tasks, such as equipment control, scheduling, etc.\n\ntodo\n\n* **Voice interaction**: Support natural language voice input and output, to achieve human and machine voice dialogue.\n* **Self-learning** : Constantly optimize your understanding and responsiveness through interaction.\n\n### Core processes and principles\n\n```mermaid\n\ngraph TB\n    A[User voice input] --> B[Speech recognition ASR]\n    B --> C[Text vectorization]\n\n    subgraph RAG System\n        C --> D[Vector retrieval]\n        D --> E[Get related documents]\n        E --> F[Build enhanced Prompt]\n    end\n\n    subgraph Large model processing\n        F --> G[Intention recognition]\n        G-->|trivia| H[Knowledge generation]\n        G -->|function call| I[function definition match]\n        I --> J[call external API]\n        J --> K[get real-time data]\n        H --> L[merge answers]\n        K --> L\n    end\n\n    subgraph Speech module\n        L --> M[text-to-speech TTS]\n    end\n\n\n    subgraph robot\n        J --> N[Behavior control]\n        M --> O[speech output]\n    end\n\n\n    %% Knowledge base part\n        P[(knowledge base document)] --> Q[document partitioning]\n        Q --> R[vectorized storage]\n        R --> D\n\n    %% Function registration\n    S[(Function definition Library)] --> |Function definition and method|F\n```\n\nKey steps in the flow chart:\n\n1.**Speech input processing phase**\n\n* Collect the user's voice signal and perform noise reduction preprocessing\n* ASRThe model converts speech to text\n\n2. **Retrieval Stage**\n\n* The text is converted to a vector representation by an embedding model\n* Convert knowledge base documents to vector storage\n* Vectorization of user-entered questions\n* Use vector similarity search to find the most relevant pieces of document\n\n3. **Enhancement Phase**\n\n* Use the retrieved related documents as context\n* Combine user questions and related documents into prompts\n* Add the necessary directives and constraints\n\n- Provide large language models as a knowledge supplement\n\n4. **Large Model Processing Stage**\n\n* Intent Recognition: Analyze user intent (quizzes/instruction execution)\n* Knowledge Generation: Generate answers based on RAG results\n* Function call: Match the predefined function and call the external API\n* Result consolidation: Integrate knowledge base information and real-time data\n\n5. Knowledge Base Management\n\n* Document Chunking: Divide the content of the document reasonably\n* Vectorization: Vectors are generated using the embedding model\n* Index Building: Establish efficient vector retrieval indexes\n\n6. Function Registration Management\n\n* Maintain standardized function definitions\n* Support dynamic function registration\n* Provide permission control for function calls\n\n7. **Output Processing Phase**\n\n* Text-to-speech: Generate natural speech using TTS models\n* Behavior Control: Execute robot action commands\n* Multimodal Output: Coordinates voice and behavior outputs\n\nThis system solution realizes a complete closed loop from voice input to multimodal output, enhances the accuracy and practicability of answers through RAG technology and function calls, and uses the open-source **Haystack** on the framework, compared with the more well-known Langchain:\n\n- Support expansion and customization, with stronger pipeline modularity, and components can be used independently;\n- Support document processing and indexing, and have stronger support for documents;\n- Easier to learn, compared to Lanchain's complex framework, Haystack is simpler and more efficient, and the official documentation is more intuitive and clearly structured;\n\n## Refer to the documentation\n\n- Fastgpt: https://github.com/labring/FastGPT\n- ollama: https://github.com/ollama/ollama\n- langchain: https://github.com/langchain-ai/langchain\n- taskingAI: https://github.com/TaskingAI/TaskingAI\n"
    },
    {
      "name": "codingtodeath/REA",
      "stars": 2,
      "img": "https://avatars.githubusercontent.com/u/101001793?s=40&v=4",
      "owner": "codingtodeath",
      "repo_name": "REA",
      "description": "rss information processor",
      "homepage": null,
      "language": "Vue",
      "created_at": "2024-10-11T03:07:15Z",
      "updated_at": "2024-11-18T07:26:47Z",
      "topics": [],
      "readme": "## 运行步骤\n\n### 前端\n\n1. 安装node、npm、vue-cli等工具\n\n2. 进入vue_rea文件夹，执行命令`npm install`\n\n3. 接着安装以下插件库：\n   ```bash\n   npm install pdfjs-dist@2.10.377\n   npm install --save-dev @babel/plugin-transform-class-static-block\n   npm install --save-dev @babel/plugin-transform-private-methods\n   ```\n\n4. 安装完成后执行`npm run serve`启动前端\n\n\n\n### 后端\n\n#### 数据库建立\n\n```sql\nCREATE DATABASE rea\n    DEFAULT CHARACTER SET utf8mb4\n    COLLATE utf8mb4_unicode_ci;\nuse rea;  # 切换到rea database\nCREATE TABLE article (\n    id INT AUTO_INCREMENT PRIMARY KEY COMMENT '文章id',\n    title TEXT COMMENT '文章标题',\n    description MEDIUMTEXT COMMENT '文章概要',\n\turl VARCHAR(2048) COMMENT '文章原地址',\n    author VARCHAR(255) COMMENT '文章作者',\n    time DATETIME COMMENT '文章发布时间',\n    content LONGTEXT COMMENT '文章内容',\n    collect TINYINT COMMENT '文章是否收藏',\n    llm LONGTEXT COMMENT '文章的大模型摘要'\n) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci;\n\nCREATE TABLE feed (\n    id INT AUTO_INCREMENT PRIMARY KEY COMMENT 'rss源序号',\n\tname VARCHAR(255) COMMENT 'rss源名称',\n    url VARCHAR(2048) COMMENT 'rss源地址'\n) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci;\n\n# 数据库修改编码(为了处理emoji存储)\nALTER DATABASE rea CHARACTER SET utf8mb4 COLLATE utf8mb4_general_ai_ci;\nALTER TABLE article CONVERT TO CHARACTER SET utf8mb4 COLLATE utf8mb4_general_ai_ci;\nALTER TABLE feed CONVERT TO CHARACTER SET utf8mb4 COLLATE utf8mb4_general_ai_ci;\n```\n\n<h3 style=\"text-align: center;\">\n  article\n</h3>\n\n| Field       | Type          | Null | Key  | Default | Extra          |\n| ----------- | ------------- | ---- | ---- | ------- | -------------- |\n| id          | int           | NO   | PRI  | NULL    | auto_increment |\n| title       | text          | YES  |      | NULL    |                |\n| description | mediumtext    | YES  |      | NULL    |                |\n| url         | varchar(2048) | YES  |      | NULL    |                |\n| author      | varchar(255)  | YES  |      | NULL    |                |\n| time        | datetime      | YES  |      | NULL    |                |\n| content     | longtext      | YES  |      | NULL    |                |\n| collect     | tinyint       | YES  |      | NULL    |                |\n| llm         | longtext      | YES  |      | NULL    |                |\n\n<h3 style=\"text-align: center;\">\n  feed\n</h3>\n\n| Field | Type          | Null | Key  | Default | Extra          |\n| ----- | ------------- | ---- | ---- | ------- | -------------- |\n| id    | int           | NO   | PRI  | NULL    | auto_increment |\n| name  | varchar(255)  | NO   |      | NULL    |                |\n| url   | varchar(2048) | NO   |      | NULL    |                |\n\n#### RAG运行（需要科学上网）\n\n1. 进入`rss_spring/RAG/`文件夹\n2. 打开`config.json.example`文件配置api key，配置完成后去掉后面的.example\n3. 创建python 3.10环境，`pip install -r ./requirements.txt`安装依赖库\n4. `python RAG.py`运行，出现 **[INFO]** 即成功启动\n\n\n\n#### Spring后端运行\n\n1. 使用maven构建工程，下载相关依赖\n2. 在`rss_spring\\src\\main\\resources\\application.properties`中配置数据库地址、用户名和密码信息 ~~（可以先使用idea工具测试能不能连接）~~\n3. 运行RssSpringApplication\n\n\n\n## 使用\n\n进入vue构建之后的地址即可使用"
    },
    {
      "name": "andychert/elevenlabs-haystack",
      "stars": 2,
      "img": "https://avatars.githubusercontent.com/u/184323448?s=40&v=4",
      "owner": "andychert",
      "repo_name": "elevenlabs-haystack",
      "description": "ElevenLabs Text-to-Speech components for Haystack.",
      "homepage": "https://pypi.org/project/elevenlabs-haystack/",
      "language": "Python",
      "created_at": "2024-10-09T20:33:48Z",
      "updated_at": "2024-10-17T18:51:25Z",
      "topics": [],
      "readme": "# ElevenLabs Haystack Integration\n\nThis repository contains an integration of ElevenLabs' Text-to-Speech API with Haystack pipelines. This package allows you to convert text to speech using ElevenLabs' API and optionally save the generated audio to AWS S3.\n\n## Installation\n\n```bash\npip install elevenlabs_haystack\n```\n\n## Setting Up API Keys\n\n#### **ElevenLabs API Key**\n\nTo access the ElevenLabs API, you need to create an account and obtain an API key.\n\n1. Go to the [ElevenLabs](https://elevenlabs.ai/) website and sign up for an account.\n2. Once logged in, navigate to the **Profile** section.\n3. In the **API** section, generate a new API key.\n4. Copy the API key.\n\n#### **AWS Credentials**\n\nTo store generated audio files on AWS S3, you need AWS credentials (Access Key ID, Secret Access Key) and specify a region.\n\n1. If you don’t have an AWS account, sign up at [AWS](https://aws.amazon.com/).\n2. Create a new IAM user and assign the necessary permissions to allow the user to upload files to S3. The `AmazonS3FullAccess` policy is sufficient for this example.\n3. Once the IAM user is created, download or note the **AWS Access Key ID** and **Secret Access Key**.\n4. Identify the **AWS Region** where your S3 bucket resides (e.g., `us-east-1`). This information can be found in the AWS Management Console.\n5. Finally, create or identify the S3 bucket where the generated audio files will be saved.\n\nCreate a `.env` file in the root directory with the following content (replace with your actual credentials):\n\n```bash\nELEVENLABS_API_KEY=sk_your_elevenlabs_api_key_here\nAWS_ACCESS_KEY_ID=your_aws_access_key_id\nAWS_SECRET_ACCESS_KEY=your_aws_secret_access_key\nAWS_REGION_NAME=us-east-1\nAWS_S3_BUCKET_NAME=your_s3_bucket_name\n```\n\nThese variables will be automatically loaded using `dotenv` and used to access ElevenLabs and AWS services securely.\n\n## Usage\n\n### Basic Text-to-Speech Example\n\nThis example shows how to use the `ElevenLabsTextToSpeech` component to convert text to speech and save the generated audio file locally or in an AWS S3 bucket. It uses environment variables to access sensitive credentials.\n\n```python\nfrom haystack.utils import Secret\nfrom elevenlabs_haystack import ElevenLabsTextToSpeech\n\n# Initialize the ElevenLabsTextToSpeech component using environment variables for sensitive data\ntts = ElevenLabsTextToSpeech(\n    elevenlabs_api_key=Secret.from_env_var(\"ELEVENLABS_API_KEY\"),\n    output_folder=\"audio_files\",  # Save the generated audio locally\n    voice_id=\"Xb7hH8MSUJpSbSDYk0k2\",  # ElevenLabs voice ID (Alice)\n    aws_s3_bucket_name=Secret.from_env_var(\"AWS_S3_BUCKET_NAME\"),  # S3 bucket for optional upload\n    aws_s3_output_folder=\"s3_files\",  # Save the generated audio to AWS S3\n    aws_access_key_id=Secret.from_env_var(\"AWS_ACCESS_KEY_ID\"),\n    aws_secret_access_key=Secret.from_env_var(\"AWS_SECRET_ACCESS_KEY\"),\n    aws_region_name=Secret.from_env_var(\"AWS_REGION_NAME\"),  # AWS region\n    voice_settings={\n        \"stability\": 0.75,\n        \"similarity_boost\": 0.75,\n        \"style\": 0.5,\n        \"use_speaker_boost\": True,  # Optional voice settings\n    },\n)\n\n# Run the text-to-speech conversion\nresult = tts.run(\"Hello, world!\")\n\n# Print the result\nprint(result)\n\n\"\"\"\n{\n    \"id\": \"elevenlabs-id\",\n    \"file_name\": \"audio_files/elevenlabs-id.mp3\",\n    \"s3_file_name\": \"s3_files/elevenlabs-id.mp3\",\n    \"s3_bucket_name\": \"test-bucket\",\n    \"s3_presigned_url\": \"https://test-bucket.s3.amazonaws.com/s3_files/elevenlabs-id.mp3\"\n}\n\"\"\"\n```\n\n### Example Using Haystack Pipeline\n\nThis example demonstrates how to integrate the `ElevenLabsTextToSpeech` component into a Haystack pipeline. Additionally, we define a `WelcomeTextGenerator` component that generates a personalized welcome message.\n\n```python\nfrom haystack import component, Pipeline\nfrom haystack.utils import Secret\nfrom elevenlabs_haystack import ElevenLabsTextToSpeech\n\n# Define a simple component to generate a welcome message\n@component\nclass WelcomeTextGenerator:\n    \"\"\"\n    A component generating a personal welcome message and making it upper case.\n    \"\"\"\n    @component.output_types(welcome_text=str, note=str)\n    def run(self, name: str):\n        return {\n            \"welcome_text\": f'Hello {name}, welcome to Haystack!'.upper(),\n            \"note\": \"welcome message is ready\"\n        }\n\n# Create a Pipeline\ntext_pipeline = Pipeline()\n\n# Add WelcomeTextGenerator to the Pipeline\ntext_pipeline.add_component(\n    name=\"welcome_text_generator\",\n    instance=WelcomeTextGenerator()\n)\n\n# Add ElevenLabsTextToSpeech to the Pipeline using environment variables\ntext_pipeline.add_component(\n    name=\"tts\",\n    instance=ElevenLabsTextToSpeech(\n        elevenlabs_api_key=Secret.from_env_var(\"ELEVENLABS_API_KEY\"),\n        output_folder=\"audio_files\",  # Save the generated audio locally\n        voice_id=\"Xb7hH8MSUJpSbSDYk0k2\",  # ElevenLabs voice ID (Alice)\n        aws_s3_bucket_name=Secret.from_env_var(\"AWS_S3_BUCKET_NAME\"),  # S3 bucket for optional upload\n        aws_s3_output_folder=\"s3_files\",  # Save the generated audio to AWS S3\n        aws_access_key_id=Secret.from_env_var(\"AWS_ACCESS_KEY_ID\"),\n        aws_secret_access_key=Secret.from_env_var(\"AWS_SECRET_ACCESS_KEY\"),\n        aws_region_name=Secret.from_env_var(\"AWS_REGION_NAME\"),  # Load region from env\n        voice_settings={\n            \"stability\": 0.75,\n            \"similarity_boost\": 0.75,\n            \"style\": 0.5,\n            \"use_speaker_boost\": True,  # Optional voice settings\n        },\n    ),\n)\n\n# Connect the output of WelcomeTextGenerator to the input of ElevenLabsTextToSpeech\ntext_pipeline.connect(sender=\"welcome_text_generator.welcome_text\", receiver=\"tts\")\n\n# Run the pipeline with a sample name\nresult = text_pipeline.run({\"welcome_text_generator\": {\"name\": \"Bilge\"}})\n\n# Print the result\nprint(result)\n\n\"\"\"\n{\n    \"id\": \"elevenlabs-id\",\n    \"file_name\": \"audio_files/elevenlabs-id.mp3\",\n    \"s3_file_name\": \"s3_files/elevenlabs-id.mp3\",\n    \"s3_bucket_name\": \"test-bucket\",\n    \"s3_presigned_url\": \"https://test-bucket.s3.amazonaws.com/s3_files/elevenlabs-id.mp3\"\n}\n\"\"\"\n```\n\n## License\n\nThis project is licensed under the MIT License.\n"
    },
    {
      "name": "deepset-ai/dc-custom-component-template",
      "stars": 2,
      "img": "https://avatars.githubusercontent.com/u/51827949?s=40&v=4",
      "owner": "deepset-ai",
      "repo_name": "dc-custom-component-template",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2024-07-31T08:11:50Z",
      "updated_at": "2025-03-20T17:43:22Z",
      "topics": [],
      "readme": "# deepset Cloud Custom Component Template\n\nThis repository contains a template for creating custom components for your deepset Cloud pipelines. Components are Python code snippets that perform specific tasks within your pipeline. This template will guide you through all the necessary elements your custom component must include.\nThis template contains two sample components which are ready to be used: \n  - `CharacterSplitter` implemented in `/src/dc_custom_component/example_components/preprocessors/character_splitter.py`: A component that splits documents into smaller chunks by the number of characters you set. You can use it in indexing pipelines.\n  - `KeywordBooster` implemented in `/src/dc_custom_component/example_components/rankers/keyword_booster.py`: A component that boosts the score of documents that contain specific keywords. You can use it in query pipelines.\n\nWe've created these examples to help you understand how to structure your components. When importing your custom components to deepset Cloud, you can remove or rename the `example_components` folder with the sample components, if you're not planning to use them. \n\nThis template serves as a custom components library for your organization. Only the components present in the most recently uploaded template are available for use in your pipelines. \n\n## Documentation\nFor more information about custom components, see [Custom Components](https://docs.cloud.deepset.ai/docs/custom-components). \nFor a step-by-step guide on creating custom components, see [Create a Custom Component](https://docs.cloud.deepset.ai/docs/create-a-custom-component).\nSee also our tutorial for [creating a custom RegexBooster component](https://docs.cloud.deepset.ai/docs/tutorial-creating-a-custom-component).\n\n## 1. Setting up your local dev environment\n\n### Prerequisites\n\n- Python v3.12 or v3.13\n- `hatch` package manager\n\n### Hatch: A Python package manager\n\nWe use `hatch` to manage our Python packages. Install it with pip:\n\nLinux and macOS:\n```bash\npip install hatch\n```\n\nWindows:\nFollow the instructions under https://hatch.pypa.io/1.12/install/#windows\n\nOnce installed, create a virtual environment by running:\n\n```bash\nhatch shell\n```\n\nThis installs all the necessary packages needed to create a custom component. You can reference this virtual environment in your IDE.\n\nFor more information on `hatch`, please refer to the [official Hatch documentation](https://hatch.pypa.io/).\n\n## 2. Developing your custom component\n\n### Structure\n\n| File | Description |\n|------|-------------|\n| `/src/dc_custom_component/components` | Directory for implementing custom components. You can logically group custom components in sub-directories. See how sample components are grouped by type. |\n| `/src/dc_custom_component/__about__.py` | Your custom components' version. Bump the version every time you update your component before uploading it to deepset Cloud. This is not needed if you are using the GitHub action workflow (in this case the version will be determined by the GitHub release tag). |\n| `/pyproject.toml` | Information about the project. If needed, add your components' dependencies in this file in the `dependencies` section. |\n\nThe directory where your custom component is stored determines the name of the component group in Pipeline Builder. For example, the `CharacterSplitter` component would appear in the `Preprocessors` group, while the `KeywordBooster` component would be listed in the `Rankers` group. You can drag these components onto the canvas to use them.\n\nWhen working with YAML, the location of your custom component implementation defines your component's `type`. For example, the sample components have the following types because of their location:\n  - `dc_custom_component.example_components.preprocessors.character_splitter.CharacterSplitter`\n  - `dc_custom_component.example_components.rankers.keyword_booster.KeywordBooster`\n\nHere is how you would add them to a pipeline:\n```yaml\ncomponents:\n  splitter:\n    type: dc_custom_component.example_components.preprocessors.character_splitter.CharacterSplitter\n    init_parameters: {}\n  ...\n    \n```\n### Working on your component\n\n1. Fork this repository.\n2. Navigate to the `/src/dc_custom_component/components/` folder.\n3. Add your custom components following the examples.\n4. Update the components' version in `/src/__about__.py`.\n\n   > **NOTE**: This is not needed if you are using the GitHub action workflow (in this case the version will be determined by the GitHub release tag).\n5. Format your code using the `hatch run code-quality:all` command. (Note that hatch commands work from the project root directory only.)\n\n### Formatting\nWe defined a suite of formatting tools. To format your code, run:\n\n```bash\nhatch run code-quality:all\n```\n\n### Testing\n\nIt's crucial to thoroughly test your custom component before uploading it to deepset Cloud. Consider adding unit and integration tests to ensure your component functions correctly within a pipeline.\n- `pytest` is ready to be used with `hatch`\n- implement your tests under `/test`\n- run `hatch run tests`\n\n## 3. Uploading your custom component\n\nYou can upload in one of two ways:\n- By releasing your forked directory.\n- By zipping the forked repository and uploading it with commands.\n\n### Uploading by releasing your forked repository\n\nWe use GitHub Actions to build and push custom components to deepset Cloud. The action runs the tests and code quality checks before pushing the component code to deepset Cloud. Create a tag to trigger the build and the push job. This method helps you keep track of the changes and investigate the code deployed to deepset Cloud.\n\nAfter forking or cloning this repository:\n\n1. Push all your changes to the forked repository.\n2. Add the `DEEPSET_CLOUD_API_KEY` [secret to your repository](https://docs.github.com/en/actions/security-for-github-actions/security-guides/using-secrets-in-github-actions). This is your deepset Cloud API key.\n(To add a secret, go to your repository and choose _Settings > Secrets and variables > Actions > New repository secret_.)\n3. Enable workflows for your repository by going to _Actions > Enable workflows_.\n4. (Optional) Adjust the workflow file in `.github/workflows/publish_on_tag.yaml` as needed.\n5. (Optional) If you're not using the European deepset Cloud tenant, change the `API_URL` variable in `.github/workflows/publish_on_tag.yaml`\n6. Create a new release with a tag to trigger the GitHub Actions workflow. The workflow builds and pushes the custom component to deepset Cloud with the tag as version. For help, see [GitHub documentation](https://docs.github.com/en/repositories/releasing-projects-on-github/managing-releases-in-a-repository).\n  \n   > **Warning:** When using this GitHub Actions workflow, the version specified in the `__about__` file will be overwritten by the tag value. Make sure your tag matches the desired version number. \n\n\n\nYou can check the upload status in the `Actions` tab of your forked repository. \n\n### Uploading a zipped repository with commands\n\nIn this method, you run commands to zip and push the repository to deepset Cloud.\n1. (Optional) If you're not using the European tenant, set the API URL:\n  - deepset Cloud Europe:\n    - On Linux and macOS: `export API_URL=\"https://api.cloud.deepset.ai\"`\n    - On Windows: `set API_URL=https://api.cloud.deepset.ai`\n  - deepset Cloud US:\n    - On Linux and macOS: `export API_URL=\"https://api.us.deepset.ai\"`\n    - On Windows: `set API_URL=https://api.us.deepset.ai`\n2. Set your [deepset Cloud API key](https://docs.cloud.deepset.ai/docs/generate-api-key).\n   - On Linux and macOS: `export API_KEY=<TOKEN>`\n   - On Windows: `set API_KEY=<TOKEN>`\n3. Upload your project by running the following command from inside of this project:\n   - On Linux and macOS: `hatch run dc:build-and-push`\n   - On Windows: `hatch run dc:build-windows` and `hatch run dc:push-windows`\n   This creates a ZIP file called `custom_component.zip` in the `dist` directory and uploads it to deepset Cloud.\n\n## 4. Debugging\n\nTo debug the installation of custom components in deepset Cloud, you can run:\n\n- On Linux and macOS: `hatch run dc:logs` \n- On Windows: `hatch run dc:logs-windows`\n\nThis will print the installation logs of the latest version of your custom components.\n"
    },
    {
      "name": "mikhail-w/bonsai",
      "stars": 2,
      "img": "https://avatars.githubusercontent.com/u/119347826?s=40&v=4",
      "owner": "mikhail-w",
      "repo_name": "bonsai",
      "description": null,
      "homepage": null,
      "language": "JavaScript",
      "created_at": "2024-08-24T12:57:48Z",
      "updated_at": "2025-04-15T20:46:26Z",
      "topics": [],
      "readme": "# 🌱 Bonsai\n\n<div align=\"center\">\n  <img src=\"./frontend/src/assets/images/readme/logo_brand.png\" alt=\"Bonsai Logo\" width=\"250\" height=\"250\">\n</div>\n<br><br><br>\n\nWelcome to **Bonsai**, a full-stack web application that offers a seamless\neCommerce experience for bonsai enthusiasts. The platform allows users to\nbrowse, purchase, and explore curated bonsai plants, accessories, and\ninformation.\n\nBonsai combines cutting-edge technology with a tranquil design to create the\nultimate platform for bonsai lovers. <br><br>\n\n## 🌐 Live Demo\n\nCheck out the live version of the app:\n**[www.mwbonsai.com](https://www.mwbonsai.com)**.\n\n---\n\n<br>\n\n## 🚀 Built With\n\n### **Frontend**\n\n![React](https://img.shields.io/badge/React-20232A?style=for-the-badge&logo=react&logoColor=61DAFB)\n![Chakra UI](https://img.shields.io/badge/Chakra%20UI-319795?style=for-the-badge&logo=chakra-ui&logoColor=white)\n![JavaScript](https://img.shields.io/badge/JavaScript-F7DF1E?style=for-the-badge&logo=javascript&logoColor=black)\n![Axios](https://img.shields.io/badge/Axios-5A29E4?style=for-the-badge&logo=axios&logoColor=white)\n![Three.js](https://img.shields.io/badge/Three.js-000000?style=for-the-badge&logo=three.js&logoColor=white)\n![React Three Fiber](https://img.shields.io/badge/React%20Three%20Fiber-20232A?style=for-the-badge&logo=react&logoColor=61DAFB)\n\n### **Backend**\n\n![Django](https://img.shields.io/badge/Django-092E20?style=for-the-badge&logo=django&logoColor=white)\n![PostgreSQL](https://img.shields.io/badge/PostgreSQL-336791?style=for-the-badge&logo=postgresql&logoColor=white)\n![SQLite](https://img.shields.io/badge/SQLite-003B57?style=for-the-badge&logo=sqlite&logoColor=white)\n![Django REST Framework](https://img.shields.io/badge/Django%20REST%20Framework-092E20?style=for-the-badge&logo=django&logoColor=white)\n![Python](https://img.shields.io/badge/Python-3776AB?style=for-the-badge&logo=python&logoColor=white)\n\n### **Third-Party APIs**\n\n![PayPal](https://img.shields.io/badge/PayPal-00457C?style=for-the-badge&logo=paypal&logoColor=white)\n![OpenAI](https://img.shields.io/badge/OpenAI-412991?style=for-the-badge&logo=openai&logoColor=white)\n![Google Maps](https://img.shields.io/badge/Google%20Maps-4285F4?style=for-the-badge&logo=google-maps&logoColor=white)\n![Echo3D](https://img.shields.io/badge/Echo3D-14A3E4?style=for-the-badge&logo=echo3d&logoColor=white)\n![Haystack](https://img.shields.io/badge/Haystack-00C7B7?style=for-the-badge&logo=haystack&logoColor=white)\n![OpenWeather](https://img.shields.io/badge/OpenWeather-FA5B0F?style=for-the-badge&logo=openweather&logoColor=white)\n![Zen Quotes API](https://img.shields.io/badge/Zen%20Quotes%20API-3C8068?style=for-the-badge&logo=leaflet&logoColor=white)\n![Google Vision API](https://img.shields.io/badge/Google%20Vision%20API-4285F4?style=for-the-badge&logo=google&logoColor=white)\n\n---\n\n<br>\n\n## 🎯 Features\n\n- 📱 **Responsive Design:** Built with React and styled using Chakra UI for a\n  beautiful and intuitive user experience across both mobile and desktop\n  devices.\n- 🛒 **Interactive Components:** Includes an elegant shopping cart, dynamic\n  product filtering, and search functionality.\n- 💳 **PayPal API:** Secure payment API integration.\n- 🧘 **Zen Quotes API:** Generate inspirational quotes on demand.\n- 🤖 **OpenAI API:** For an intelligent chatbot experience.\n- ☁️ **Weather API:** To check local conditions for optimal bonsai care.\n- 📍 **Location Services:** Integration with Google Maps API to find nearby\n  bonsai nurseries, gardens, and stores.\n- 🌿 **3D and Augmented Reality:** View and manipulate bonsai plants in 3D and\n  Augmented Reality using Three.js, React Three Fiber.\n- 📝 **Blog Integration:** Create, Read, and interact with blog posts from other\n  users.\n- 🛠️ **Django-Powered Backend:** A robust and secure API backend built with\n  Django and Django REST Framework.\n- 🖼️ **Image Management:** Efficient handling of product and user-uploaded\n  images using AWS S3.\n- 🔐 **Authentication:** Secure user authentication and authorization powered by\n  JWT. <br><br><br>\n\n## 🛠 Installation & Setup\n\n## 📌 Prerequisites\n\nBefore setting up the project, ensure you have the following installed:\n\n### **Frontend Prerequisites:**\n\n- [Node.js](https://nodejs.org/) (v16+ recommended)\n- [npm](https://www.npmjs.com/)\n\n### **Backend Prerequisites:**\n\n- [Python 3.9+](https://www.python.org/downloads/)\n- [PostgreSQL 13+](https://www.postgresql.org/download/)\n- [Virtual Environment (`venv`)](https://docs.python.org/3/library/venv.html)\n- [AWS CLI (for S3 integration)](https://aws.amazon.com/cli/)\n\n---\n\n### **Clone the Repository**\n\n```bash\ngit clone https://github.com/mikhail-w/bonsai.git\ncd bonsai\n```\n---\n\n## 📂 Environment Variables\n\nEnsure you configure your environment variables for both the **frontend** and\n**backend**.\n\n### **Frontend (`.env` in `frontend/`)**\n\n```env\nVITE_API_URL=your_backend_api_url\nVITE_WEATHER_API_KEY=your_open-weather_api_key\nVITE_PAYPAL_CLIENT_ID=your_paypal_client_id\nVITE_GOOGLE_MAPS_API_KEY=your_google_maps_api_key\nVITE_GOOGLE_CLOUD_VISION_API_KEY=your_google_cloud_vision_api_key\nVITE_S3_BUCKET=your_s3_bucket_name\nVITE_S3_REGION=your_s3_region\nVITE_S3_PATH=your_s3_bucket_path\nVITE_API_BASE_URL=your_api_base_url\n```\n\n### **Backend (`.env` in `backend/`)**\n\n```env\nOPENAI_API_KEY=your_openai_api_key\nDJANGO_SECRET_KEY=your_django_secret_key\nDJANGO_ALLOWED_HOSTS=your_django_allowed_hosts\nDJANGO_DEBUG=True\nDB_USER=postgres\nDB_PASSWORD=password\nDB_NAME=bonsai_store\nDB_HOST=localhost\nDB_PORT=5432\nAWS_ACCESS_KEY_ID=your_aws_access_key_id\nAWS_SECRET_ACCESS_KEY=your_aws_secret_access_key\nAWS_STORAGE_BUCKET_NAME=your_storage_bucket_name\nAWS_S3_REGION_NAME=your_storage_bucket_region\nAWS_S3_CUSTOM_DOMAIN=your_s3_custom_domain\n```\n\n---\n\n### **Frontend Setup**\n\n```bash\ncd frontend\nnpm install\nnpm run dev\n```\n\n### **Backend Setup**\n\n```bash\ncd backend\npython -m venv venv\nsource venv/bin/activate\npip install -r requirements.txt\n```\n\n### **Setup Database**\n\n```bash\nsudo -u postgres psql\nCREATE DATABASE bonsai_store;\nCREATE USER postgres WITH PASSWORD 'password';\nALTER USER postgres WITH SUPERUSER CREATEROLE CREATEDB;\n```\n\n### **Apply Migrations & Load Data**\n\n```bash\npython manage.py makemigrations\npython manage.py migrate\n```\n\n**Important Note on Data Loading Order:**\nWhen loading fixtures, you must follow this specific order to maintain referential integrity:\n\n```bash\n\n# First load users (since other models depend on them)\npython manage.py loaddata users.json\n\n# Then load products\npython manage.py loaddata products.json\n\n# Then load posts\npython manage.py loaddata posts.json\n\n# Then load reviews (which depend on users and products)\npython manage.py loaddata reviews.json\n\n# Finally load comments (which depend on users and posts)\npython manage.py loaddata comments.json\n```\n\n**Note:** If you encounter foreign key constraint errors, ensure that the user IDs in your fixture files match the actual user IDs in your database. The reviews.json file should only reference user IDs that exist in your users.json file.\n\n### **Create Django Superuser**\n\nTo access the Django admin panel, you need to create a superuser account:\n\n```bash\npython manage.py createsuperuser\n```\n\nFollow the prompts to set up your admin account:\n- Enter a username (e.g., 'admin')\n- Provide an email address\n- Create a strong password\n\nOnce created, you can access the Django admin panel at:\n`http://127.0.0.1:8000/admin/`\n\nThe admin panel gives you access to manage:\n- Users and User Profiles\n- Products\n- Reviews\n- Orders and Order Items\n- Shipping Addresses\n- Blog Posts\n  \n\n\n\n\n### **Run the Server**\n\n```bash\npython manage.py runserver\n```\n\n\n\n### Notes:\n\n1. Replace all `your_*` placeholders with your actual keys, secrets, and URLs.\n2. Ensure `.env` files are **excluded from version control** by adding them to\n   `.gitignore`.\n3. Use different values for development and production environments as needed.\n   <br><br><br>\n\n## 🚀 Deployment\n\n- ☁️ **Cloud Hosting:** Hosted on **AWS** with **S3** for frontend assets and\n  **EC2 instances in private subnets** for the backend.\n- 🚀 **CloudFront CDN:** Delivers cached frontend assets globally for faster\n  performance.\n- 🌎 **Domain & SSL:** Managed via **AWS Route 53**, **AWS Certificate Manager\n  (SSL/TLS)**, and protected by **AWS WAF** for security.\n- 🔄 **Load Balancing & Auto Scaling:** Uses **Application Load Balancer (ALB)**\n  to distribute traffic and an **Auto Scaling Group** for backend EC2 instances.\n- 🗄️ **Database:** **Amazon RDS (PostgreSQL)** deployed in private subnets with\n  **Multi-AZ replication** for high availability.\n- 🔐 **Security:** Backend is deployed in **private subnets** with NAT gateway\n  access, ensuring security and controlled internet exposure.\n- 📦 **Backend Hosting:** **Gunicorn & Nginx** serve the Django backend\n  efficiently inside EC2 instances.\n- 🔄 **CI/CD Pipeline:** Automated deployments using **GitHub Actions**.\n\n### **AWS 3 Tier Architecture Diagram**\n\n<div align=\"center\">\n  <img src=\"./frontend/src/assets/images/readme/mwbonsai_architecture.png\" alt=\"Chatbot Integration\" width=\"80%\">\n</div>\n   <br><br><br>\n\n## 📷 Preview\n\n### **Demo**\n\n![Demo](./frontend/src/assets/images/readme/readme001.gif)\n\n### **User Dashboard**\n\n#### Light Mode\n\n<div align=\"center\">\n  <img src=\"./frontend/src/assets/images/readme/readme007.png\" alt=\"User Dashboard (Light Mode)\" width=\"80%\">\n</div>\n\n#### Dark Mode\n\n<div align=\"center\">\n  <img src=\"./frontend/src/assets/images/readme/readme008.png\" alt=\"User Dashboard (Dark Mode) \" width=\"80%\" >\n</div>\n\n### **Advanced 3D Model Interactions**\n\n<div align=\"center\">\n  <img src=\"./frontend/src/assets/images/readme/readme009.png\" alt=\"Advanced 3D Model Interactions\" width=\"80%\">\n</div>\n\n### **Google Maps Integration**\n\n<div align=\"center\">\n  <img src=\"./frontend/src/assets/images/readme/readme0010a.png\" alt=\"Google Maps Integration\" width=\"80%\">\n</div>\n\n### **Blog Integration**\n\n<div align=\"center\">\n  <img src=\"./frontend/src/assets/images/readme/readme0010b.png\" alt=\"Google Maps Integration\" width=\"80%\">\n</div>\n\n### **Plant and Image Identification**\n\n<div align=\"center\">\n  <img src=\"./frontend/src/assets/images/readme/readme0011.png\" alt=\"Plant Identification\" width=\"80%\">\n</div>\n\n### **View Bonsai in Augmented Reality**\n\n<div align=\"center\">\n  <img src=\"./frontend/src/assets/images/readme/readme0012.png\" alt=\"Augmented Reality\" width=\"80%\">\n</div>\n\n### **Chatbot Integration**\n\n<div align=\"center\">\n  <img src=\"./frontend/src/assets/images/readme/readme0013.png\" alt=\"Chatbot Integration\" width=\"80%\">\n</div>\n\n---\n\n<br><br>\n\n## 🤝 Contribution\n\n- 🛠️ **Fork the Repository:** Start by forking the project on GitHub.\n- 🌿 **Create a Branch:** Create a new branch for your feature or bug fix:\n  ```bash\n  git checkout -b feature/your-feature-name\n  ```\n- 💡 **Commit:** Commit your changes and push them to your fork.\n- 🔄 **Create a Pull Request:** Open a pull request..\n\n---\n\n## 📜 License\n\n📝 **MIT License** – This project is licensed under the\n[MIT License](./LICENSE).\n\n---\n\n## Acknowledgments\n\nSpecial thanks to the team and contributors who made this project possible!\n\n<table>\n  <tr>\n    <td align=\"center\">\n      <a href=\"https://github.com/GD757\">\n        <img src=\"https://avatars.githubusercontent.com/GD757\" width=\"100px;\" alt=\"Gary\"/>\n        <br />\n        <sub><b>Gary Dunnington</b></sub>\n      </a>\n    </td>\n    <td align=\"center\">\n      <a href=\"https://github.com/DustinV1976\">\n        <img src=\"https://avatars.githubusercontent.com/DustinV1976\" width=\"100px;\" alt=\"Dustin\"/>\n        <br />\n        <sub><b>Dustin Siebold</b></sub>\n      </a>\n    </td>\n    <td align=\"center\">\n      <a href=\"https://github.com/dp1p\">\n        <img src=\"https://avatars.githubusercontent.com/dp1p\" width=\"100px;\" alt=\"Daniel\"/>\n        <br />\n        <sub><b>Daniel Phanachone</b></sub>\n      </a>\n    </td>\n  </tr>\n</table>\n"
    },
    {
      "name": "Couchbase-Ecosystem/couchbase-haystack",
      "stars": 2,
      "img": "https://avatars.githubusercontent.com/u/167254978?s=40&v=4",
      "owner": "Couchbase-Ecosystem",
      "repo_name": "couchbase-haystack",
      "description": "Couchbase haystack document store",
      "homepage": null,
      "language": "Python",
      "created_at": "2024-08-08T09:05:24Z",
      "updated_at": "2025-04-14T15:05:38Z",
      "topics": [],
      "readme": "<h1 align=\"center\">couchbase-haystack</h1>\n\n<p align=\"center\">A <a href=\"https://docs.haystack.deepset.ai/docs/document_store\"><i>Haystack</i></a> Document Store for <a href=\"https://www.couchbase.com\"><i>Couchbase</i></a>.</p>\n\n<p align=\"center\">\n  <a href=\"https://github.com/Couchbase-Ecosystem/couchbase-haystack/actions?query=workflow%3Aci\">\n    <img alt=\"ci\" src=\"https://github.com/Couchbase-Ecosystem/couchbase-haystack/actions/workflows/ci.yml/badge.svg\" />\n  </a>\n  <a href=\"https://couchbase-ecosystem.github.io/couchbase-haystack/\">\n    <img alt=\"documentation\" src=\"https://img.shields.io/badge/docs-mkdocs%20material-blue.svg?style=flat\" />\n  </a>\n  <a href=\"https://pypi.org/project/couchbase-haystack/\">\n    <img alt=\"pypi version\" src=\"https://img.shields.io/pypi/v/couchbase-haystack.svg\" />\n  </a>\n  <a href=\"https://pypi.org/project/haystack-ai/\">\n    <img alt=\"haystack version\" src=\"https://img.shields.io/pypi/v/haystack-ai.svg?label=haystack\" />\n  </a>\n</p>\n\n----\n\n**Table of Contents**\n\n- [Overview](#overview)\n- [Installation](#installation)\n- [Usage](#usage)\n- [License](#license)\n\n## Overview\n\nAn integration of [Couchbase](https://www.couchbase.com) NoSQL database with [Haystack v2.0](https://docs.haystack.deepset.ai/v2.0/docs/intro)\nby [deepset](https://www.deepset.ai). In Couchbase [Vector search index](https://docs.couchbase.com/server/current/vector-search/vector-search.html)\nis being used for indexing document embeddings and dense retrievals.\n\nThe library allows using Couchbase as a [DocumentStore](https://docs.haystack.deepset.ai/v2.0/docs/document-store), and implements the required [Protocol](https://docs.haystack.deepset.ai/v2.0/docs/document-store#documentstore-protocol) methods. You can start working with the implementation by importing it from `couchbase_haystack` package:\n\n```python\nfrom couchbase_haystack import CouchbaseSearchDocumentStore\n```\n\nIn addition to the `CouchbaseSearchDocumentStore` the library includes the following haystack components which can be used in a pipeline:\n\n- [CouchbaseSearchEmbeddingRetriever]() - is a typical [retriever component](https://docs.haystack.deepset.ai/v2.0/docs/retrievers) which can be used to query vector store index and find related Documents. The component uses `CouchbaseSearchDocumentStore` to query embeddings.\n\nThe `couchbase-haystack` library uses [Python Driver](https://docs.couchbase.com/python-sdk/current/hello-world/start-using-sdk.html).\n\n`CouchbaseSearchDocumentStore` will store Documents as JSON documents in Couchbase. Embeddings are stored as part of the document, with indexing and querying of vector embeddings managed by Couchbase's dedicated [Vector Search Index](https://docs.couchbase.com/server/current/vector-search/vector-search.html). The document store supports both scope-level and global-level vector search indexes:\n\n- Scope-level indexes (default): The vector search index is created at the scope level and only searches documents within that scope\n- Global-level indexes: The vector search index is created at the bucket level and can search across all scopes and collections in the bucket\n\n```text\n                                   +-----------------------------+\n                                   |       Couchbase Database    |\n                                   +-----------------------------+\n                                   |                             |\n                                   |      +----------------+     |\n                                   |      |  Data service  |     |\n                write_documents    |      +----------------+     |\n          +------------------------+----->|   properties   |     |\n          |                        |      |                |     |\n+---------+--------------+         |      |   embedding    |     |\n|                        |         |      +--------+-------+     |\n| CouchbaseSearchDocumentStore |         |               |             |\n|                        |         |               |index        |\n+---------+--------------+         |               |             |\n          |                        |      +--------+--------+    |\n          |                        |      |  Search service |    |\n          |                        |      +-----------------+    |\n          +----------------------->|      |       FTS       |    |\n               query_embeddings    |      |   Vector Index  |    |\n                                   |      | (for embedding) |    |\n                                   |      +-----------------+    |\n                                   |                             |\n                                   +-----------------------------+\n```\n\nIn the above diagram:\n\n- `Data service` Supports the storing, setting, and retrieving of documents, specified by key. Basically where the documents are stored in key value.\n- `properties` are Document [attributes](https://docs.haystack.deepset.ai/v2.0/docs/data-classes#document) stored as part of the Document.\n- `embedding` is also a property of the Document (just shown separately in the diagram for clarity) which is a vector of type `LIST[FLOAT]`.\n- `Search service` Where indexes specially purposed for Full Text Search and Vector search are created. The Search Service allows for efficient querying \nand retrieval based on both text content and vector embeddings.\n\n`CouchbaseSearchDocumentStore` requires the vector index to be created manually either by sdk or UI. Before writing documents you should make sure Documents are embedded by one of the provided [embedders](https://docs.haystack.deepset.ai/v2.0/docs/embedders). For example [SentenceTransformersDocumentEmbedder](https://docs.haystack.deepset.ai/v2.0/docs/sentencetransformersdocumentembedder) can be used in indexing pipeline to calculate document embeddings before writing those to Couchbase.\n\n## Installation\n\n`couchbase-haystack` can be installed as any other Python library, using pip:\n\n```bash\npip install --upgrade pip # optional\npip install sentence-transformers # required in order to run pipeline examples given below\npip install couchbase-haystack\n```\n\n## Usage\n\n### Running Couchbase\n\nYou will need a running instance of Couchbase to use the components from this package. There are several options available:\n\n- [Docker](https://docs.couchbase.com/server/current/getting-started/do-a-quick-install.html)\n- [Couchbase Cloud](https://www.couchbase.com/products/capella) - a fully managed cloud service\n- [Couchbase Server](https://www.couchbase.com/downloads) - installable on various operating systems\n\nThe simplest way to start the database locally is with a Docker container:\n\n```bash\ndocker run \\\n    --restart always \\\n    --publish=8091-8096:8091-8096 --publish=11210:11210 \\\n    --env COUCHBASE_ADMINISTRATOR_USERNAME=admin \\\n    --env COUCHBASE_ADMINISTRATOR_PASSWORD=passw0rd \\\n    couchbase:enterprise-7.6.2\n```\n\nIn this example, the container is started using Couchbase Server version `7.6.2`. The `COUCHBASE_ADMINISTRATOR_USERNAME` and `COUCHBASE_ADMINISTRATOR_PASSWORD` environment variables set the default credentials for authentication.\n\n> **Note:**  \n> Assuming you have a Docker container running, navigate to <http://localhost:8091> to open the Couchbase Web Console and explore your data.\n\n### Document Store\n\nOnce you have the package installed and the database running, you can start using `CouchbaseSearchDocumentStore` as any other document stores that support embeddings.\n\n```python\nfrom couchbase_haystack import CouchbaseSearchDocumentStore, CouchbasePasswordAuthenticator\nfrom haystack.utils.auth import Secret\n\ndocument_store = CouchbaseSearchDocumentStore(\n    cluster_connection_string=Secret.from_env_var(\"CB_CONNECTION_STRING\"),\n    authenticator=CouchbasePasswordAuthenticator(\n      username=Secret.from_env_var(\"CB_USERNAME\"),\n      password=Secret.from_env_var(\"CB_PASSWORD\")\n    ),\n    bucket = \"haystack_bucket_name\",\n    scope=\"haystack_scope_name\",\n    collection=\"haystack_collection_name\",\n    vector_search_index = \"vector_search_index\",\n    is_global_level_index=False  # Enables scope-level vector search index by default\n)\n```\n\nAssuming there is a list of documents available and a running couchbase database you can write/index those in Couchbase, e.g.:\n\n```python\nfrom haystack import Document\n\ndocuments = [Document(content=\"Alice has been living in New York City for the past 5 years.\")]\n\ndocument_store.write_documents(documents)\n```\n\nIf you intend to obtain embeddings before writing documents use the following code:\n\n```python\nfrom haystack import Document\n\n# import one of the available document embedders\nfrom haystack.components.embedders import SentenceTransformersDocumentEmbedder \n\ndocuments = [Document(content=\"Alice has been living in New York City for the past 5 years.\")]\n\ndocument_embedder = SentenceTransformersDocumentEmbedder(model=\"sentence-transformers/all-MiniLM-L6-v2\")\ndocument_embedder.warm_up() # will download the model during first run\ndocuments_with_embeddings = document_embedder.run(documents)\n\ndocument_store.write_documents(documents_with_embeddings.get(\"documents\"))\n```\n\nMake sure embedding model produces vectors of same size as it has been set on `Couchbase Vector Index`, e.g. setting `embedding_dim=384` would comply with the \"sentence-transformers/all-MiniLM-L6-v2\" model.\n\n> **Note**\n> Most of the time you will be using [Haystack Pipelines](https://docs.haystack.deepset.ai/v2.0/docs/pipelines) to build both indexing and querying RAG scenarios.\n\nIt is important to understand how haystack Documents are stored in Couchbase after you call `write_documents`.\n\n```python\nfrom random import random\n\nsample_embedding = [random() for _ in range(384)]  # using fake/random embedding for brevity here to simplify example\ndocument = Document(\n    content=\"Alice has been living in New York City for the past 5 years.\", embedding=sample_embedding, meta={\"num_of_years\": 5}\n)\ndocument.to_dict()\n```\n\nThe above code converts a Document to a dictionary and will render the following output:\n\n```bash\n>>> output:\n{\n    \"id\": \"11c255ad10bff4286781f596a5afd9ab093ed056d41bca4120c849058e52f24d\",\n    \"content\": \"Alice has been living in New York City for the past 5 years.\",\n    \"dataframe\": None,\n    \"blob\": None,\n    \"score\": None,\n    \"embedding\": [0.025010755222666936, 0.27502931836911926, 0.22321073814882275, ...], # vector of size 384\n    \"num_of_years\": 5,\n}\n```\n\nThe data from the dictionary will be used to create a document in COuchbase after you write the document with `document_store.write_documents([document])`. You could query it with Cypher, e.g. `MATCH (doc:Document) RETURN doc`. Below is a json document Couchbase:\n\n```js\n{\n  \"id\": \"11c255ad10bff4286781f596a5afd9ab093ed056d41bca4120c849058e52f24d\",\n  \"embedding\": [0.6394268274307251, 0.02501075528562069,0.27502933144569397, ...], // vector of size 384\n  \"content\": \"Alice has been living in New York City for the past 5 years.\",\n  \"meta\": {\n    \"num_of_years\": 5\n  }\n}\n```\n\nThe full list of parameters accepted by `CouchbaseSearchDocumentStore` can be found in\n[API documentation](https://couchbase-ecosystem.github.io/couchbase-haystack/reference/couchbase_document_store).\n\n### Indexing documents\n\nWith Haystack you can use [DocumentWriter](https://docs.haystack.deepset.ai/v2.0/docs/documentwriter) component to write Documents into a Document Store. In the example below we construct pipeline to write documents to Couchbase using `CouchbaseSearchDocumentStore`:\n\n```python\nfrom haystack import Document\nfrom haystack.components.embedders import SentenceTransformersDocumentEmbedder\nfrom haystack.components.writers import DocumentWriter\nfrom haystack.pipeline import Pipeline\nfrom haystack.utils.auth import Secret\n\nfrom couchbase_haystack import CouchbaseSearchDocumentStore, CouchbasePasswordAuthenticator\n\ndocuments = [Document(content=\"This is document 1\"), Document(content=\"This is document 2\")]\n\ndocument_store = CouchbaseSearchDocumentStore(\n    cluster_connection_string=Secret.from_env_var(\"CB_CONNECTION_STRING\"),\n    authenticator=CouchbasePasswordAuthenticator(\n      username=Secret.from_env_var(\"CB_USERNAME\"),\n      password=Secret.from_env_var(\"CB_PASSWORD\")\n    ),\n    bucket = \"haystack_bucket_name\",\n    scope=\"haystack_scope_name\",\n    collection=\"haystack_collection_name\",\n    vector_search_index = \"vector_search_index\"\n)\nembedder = SentenceTransformersDocumentEmbedder(model=\"sentence-transformers/all-MiniLM-L6-v2\")\ndocument_writer = DocumentWriter(document_store=document_store)\n\nindexing_pipeline = Pipeline()\nindexing_pipeline.add_component(instance=embedder, name=\"embedder\")\nindexing_pipeline.add_component(instance=document_writer, name=\"writer\")\n\nindexing_pipeline.connect(\"embedder\", \"writer\")\nindexing_pipeline.run({\"embedder\": {\"documents\": documents}})\n```\n\n```bash\n>>> output:\n`{'writer': {'documents_written': 2}}`\n```\n\n### Retrieving documents\n\n`CouchbaseSearchEmbeddingRetriever` component can be used to retrieve documents from Couchbase by querying vector index using an embedded query. Below is a pipeline which finds documents using query embedding:\n\n```python\nfrom typing import List\nfrom haystack.utils.auth import Secret\nfrom haystack import Document, Pipeline\nfrom haystack.components.embedders import SentenceTransformersDocumentEmbedder, SentenceTransformersTextEmbedder\n\nfrom couchbase_haystack.document_store import CouchbaseSearchDocumentStore, CouchbasePasswordAuthenticator\nfrom couchbase_haystack.component.retriever import CouchbaseSearchEmbeddingRetriever\n\ndocument_store = CouchbaseSearchDocumentStore(\n    cluster_connection_string=Secret.from_env_var(\"CB_CONNECTION_STRING\"),\n    authenticator=CouchbasePasswordAuthenticator(\n      username=Secret.from_env_var(\"CB_USERNAME\"),\n      password=Secret.from_env_var(\"CB_PASSWORD\")\n    ),\n    bucket = \"haystack_bucket_name\",\n    scope=\"haystack_scope_name\",\n    collection=\"haystack_collection_name\",\n    vector_search_index = \"vector_search_index\"\n)\n\ndocuments = [\n    Document(content=\"Alice has been living in New York City for the past 5 years.\", meta={\"num_of_years\": 5, \"city\": \"New York\"}),\n    Document(content=\"John moved to Los Angeles 2 years ago and loves the sunny weather.\", meta={\"num_of_years\": 2, \"city\": \"Los Angeles\"}),\n]\n\n# Same model is used for both query and Document embeddings\nmodel_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n\ndocument_embedder = SentenceTransformersDocumentEmbedder(model=model_name)\ndocument_embedder.warm_up()\ndocuments_with_embeddings = document_embedder.run(documents)\n\ndocument_store.write_documents(documents_with_embeddings.get(\"documents\"))\n\nprint(\"Number of documents written: \", document_store.count_documents())\n\npipeline = Pipeline()\npipeline.add_component(\"text_embedder\", SentenceTransformersTextEmbedder(model=model_name))\npipeline.add_component(\"retriever\", CouchbaseSearchEmbeddingRetriever(document_store=document_store))\npipeline.connect(\"text_embedder.embedding\", \"retriever.query_embedding\")\n\nresult = pipeline.run(\n    data={\n        \"text_embedder\": {\"text\": \"What cities do people live in?\"},\n        \"retriever\": {\n            \"top_k\": 5\n        },\n    }\n)\n\ndocuments: List[Document] = result[\"retriever\"][\"documents\"]\n```\n\n```bash\n>>> output:\n[Document(id=3e35fa03aff6e3c45e6560f58adc4fde3c436c111a8809c30133b5cb492e8694, content: 'Alice has been living in New York City for the past 5 years.', meta: {'num_of_years': 5, 'city': 'New York'}, score: 0.36796408891677856, embedding: \"embedding\": vector of size 384), Document(id=ca4d7d7d7ff6c13b950a88580ab134b2dc15b48a47b8f571a46b354b5344e5fa, content: 'John moved to Los Angeles 2 years ago and loves the sunny weather.', meta: {'num_of_years': 2, 'city': 'Los Angeles'}, score: 0.3126790523529053, embedding: vector of size 384)]\n```\n\n### More examples\n\nYou can find more examples in the implementation [repository](examples):\n\n- [indexing_pipeline.py](examples/indexing_pipeline.py) - Indexing text files (documents) from a remote http location.\n- [rag_pipeline.py](examples/rag_pipeline.py) - Generative question answering RAG pipeline using `CouchbaseSearchEmbeddingRetriever` to fetch documents from Couchbase document store and answer question using [HuggingFaceAPIGenerator](https://docs.haystack.deepset.ai/v2.0/docs/huggingfacetgigenerator).\n\n## License\n\n`couchbase-haystack` is distributed under the terms of the [MIT](https://spdx.org/licenses/MIT.html) license.\n---\n\n## 📢 Support Policy\n\nWe truly appreciate your interest in this project!  \nThis project is **community-maintained**, which means it's **not officially supported** by our support team.\n\nIf you need help, have found a bug, or want to contribute improvements, the best place to do that is right here — by [opening a GitHub issue](https://github.com/Couchbase-Ecosystem/couchbase-haystack/issues).  \nOur support portal is unable to assist with requests related to this project, so we kindly ask that all inquiries stay within GitHub.\n\nYour collaboration helps us all move forward together — thank you!\n"
    },
    {
      "name": "bytewax/bytewax-azure-ai-search",
      "stars": 2,
      "img": "https://avatars.githubusercontent.com/u/72483929?s=40&v=4",
      "owner": "bytewax",
      "repo_name": "bytewax-azure-ai-search",
      "description": "Bytewax custom sink for Azure AI Search",
      "homepage": "",
      "language": "Python",
      "created_at": "2024-08-19T22:42:54Z",
      "updated_at": "2024-10-22T17:15:28Z",
      "topics": [],
      "readme": "[![Actions Status](https://github.com/bytewax/bytewax-azure-ai-search/actions/workflows/CI.yml/badge.svg)](https://github.com/bytewax/bytewax-azure-ai-search/actions)\n[![PyPI](https://img.shields.io/pypi/v/bytewax-azure-ai-search.svg?style=flat-square)](https://pypi.org/project/bytewax-azure-ai-search/)\n[![Bytewax User Guide](https://img.shields.io/badge/user-guide-brightgreen?style=flat-square)](https://docs.bytewax.io/projects/bytewax-azure-ai-search/en/latest/index.html)\n\n<picture>\n  <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://user-images.githubusercontent.com/6073079/195393689-7334098b-a8cd-4aaa-8791-e4556c25713e.png\" width=\"350\">\n  <source media=\"(prefers-color-scheme: light)\" srcset=\"https://user-images.githubusercontent.com/6073079/194626697-425ade3d-3d72-4b4c-928e-47bad174a376.png\" width=\"350\">\n  <img alt=\"Bytewax\">\n</picture>\n\n## bytewax-azure-ai-search\n\nCustom sink for [Azure AI Search](https://learn.microsoft.com/en-us/azure/search/search-what-is-azure-search) vector database for real time indexing.\n\nbytewax-azure-ai-search is commercially licensed with publicly available source code. Please see the full details in [LICENSE](https://github.com/bytewax/bytewax-azure-ai-search/blob/main/LICENSE.md).\n\n## Installation and import sample\n\nTo install you can run\n\n```bash\npip install bytewax-azure-ai-search\n```\n\nThen import\n\n```python\nfrom bytewax.azure_ai_search import AzureSearchSink\n```\n\nYou can then add it to your dataflow\n\n```python\nazure_sink = AzureSearchSink(\n    azure_search_service=service_name,\n    index_name=\"bytewax-index\",\n    search_api_version=\"2024-07-01\",\n    search_admin_key=api_key,\n    schema={\n        \"id\": {\"type\": \"string\", \"default\": None},\n        \"content\": {\"type\": \"string\", \"default\": None},\n        \"meta\": {\"type\": \"string\", \"default\": None},\n        \"vector\": {\"type\": \"collection\", \"item_type\": \"single\", \"default\": []},\n    },\n)\n\nflow = Dataflow(\"indexing-pipeline\")\ninput_data = op.input(\"input\", flow, FileSource(\"data/news_out.jsonl\"))\ndeserialize_data = op.map(\"deserialize\", input_data, safe_deserialize)\nextract_html = op.map(\"extract_html\", deserialize_data, process_event)\nop.output(\"output\", extract_html, azure_sink)\n```\n\n**Note**\n\nThis installation includes the following dependencies:\n\n```ssh\nazure-search-documents==11.5.1\nazure-common==1.1.28\nazure-core==1.30.2\nopenai==1.44.1\n```\n\nThese are used to write the vectors on the appropriate services based on an Azure schema provided. We will provide an example in this README for working versions of schema definition under these versions.\n\n## Setting up Azure AI services\n\n**This asumes you have set up an Azure AI Search service on the Azure portal. For more instructions, visit [their documentation](https://learn.microsoft.com/en-us/azure/search/search-create-service-portal)**\n\n**Optional**\nTo generate embeddings, you can set up an [Azure OpenAI service](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/create-resource?pivots=web-portal) and deploy an embedding model such as `text-ada-002-embedding`\n\nOnce you have set up the resources, ensure to idenfity and store the following information from the Azure portal:\n\n* You Azure AI Search admin key\n* You Azure AI Search service name\n* You Azure AI Search service endpoint url\n\nIf you deployed an embedding model through Azure AI OpenAI service:\n\n* You Azure OpenAI endpoint url\n* You Azure OpenAI API key\n* Your Azure OpenAI service name\n* You Azure OpenAI embedding deployment name\n* Your Azure OpenAI embedding name (e.g. text-ada-002-embedding`)\n\n## Sample usage\n\nYou can find a complete example under the [`examples/` folder](https://github.com/bytewax/bytewax-azure-ai-search/tree/main/examples).\n\nTo execute the examples, you can generate a `.env` file with the following keywords:\n\n```bash\n# OpenAI\nAZURE_OPENAI_ENDPOINT= <your-azure-openai-endpoint>\nAZURE_OPENAI_API_KEY= <your-azure-openai-key>\nAZURE_OPENAI_SERVICE=<your-azure-openai-named-service>\n# Azure Document Search\nAZURE_SEARCH_ADMIN_KEY=<your-azure-ai-search-admin-key>\nAZURE_SEARCH_SERVICE=<your-azure-ai-search-named-service>\nAZURE_SEARCH_SERVICE_ENDPOINT=<your-azure-ai-search-endpoint-url>\n\n# Optional - if you prefer to generate embeddings with embedding models deployed on Azure\nAZURE_EMBEDDING_DEPLOYMENT_NAME=<your-azure-openai-given-deployment-name>\nAZURE_EMBEDDING_MODEL_NAME=<your-azure-openai-model-name>\n\n# Optional - if you prefer to generate the embeddings with OpenAI\nOPENAI_API_KEY=<your-openai-key>\n```\n\nSet up the connection and schema by running\n\n```bash\npython connection.py\n```\n\nYou can verify the creation of the index was successful by visiting the portal.\n\n![](https://github.com/bytewax/bytewax-azure-ai-search/blob/main/docs/images/sample-index.png)\n\nIf you click on the created index and press \"Search\" you can verify it was created - but empty at this point.\n\n![](https://github.com/bytewax/bytewax-azure-ai-search/blob/main/docs/images/sample-empty-index.png)\n\nGenerate the embeddings and store in Azure AI Search through the bytewax-azure-ai-search sink\n\n```bash\npython -m bytewax.run dataflow:flow\n```\n\nVerify the index was populated by pressing \"Search\" with an empty query.\n\n![](https://github.com/bytewax/bytewax-azure-ai-search/blob/main/docs/images/sample-filled-index.png)\n\n**Note**\n\nIn the dataflow we initialized the custom sink as follows:\n\n```python\nfrom bytewax.azure_ai_search import AzureSearchSink\n\nazure_sink = AzureSearchSink(\n    azure_search_service=service_name,\n    index_name=\"bytewax-index\",\n    search_api_version=\"2024-07-01\",\n    search_admin_key=api_key,\n    schema={\n        \"id\": {\"type\": \"string\", \"default\": None},\n        \"content\": {\"type\": \"string\", \"default\": None},\n        \"meta\": {\"type\": \"string\", \"default\": None},\n        \"vector\": {\"type\": \"collection\", \"item_type\": \"single\", \"default\": []},\n    },\n)\n```\n\nThe schema and structure need to match how you configure the schema through the Azure AI Search Python API. For more information, [visit their page](https://pypi.org/project/azure-search-documents/)\n\nIn this example:\n\n```python\nfrom azure.search.documents.indexes.models import (\n    SimpleField,\n    SearchFieldDataType,\n)\n\n# Define schema\nfields = [\n    SimpleField(\n        name=\"id\",\n        type=SearchFieldDataType.String,\n        searchable=True,\n        filterable=True,\n        sortable=True,\n        facetable=True,\n        key=True,\n    ),\n    SearchableField(\n        name=\"content\",\n        type=SearchFieldDataType.String,\n        searchable=True,\n        filterable=False,\n        sortable=False,\n        facetable=False,\n        key=False,\n    ),\n    SearchableField(\n        name=\"meta\",\n        type=SearchFieldDataType.String,\n        searchable=True,\n        filterable=False,\n        sortable=False,\n        facetable=False,\n        key=False,\n    ),\n    SimpleField(\n        name=\"vector\",\n        type=SearchFieldDataType.Collection(SearchFieldDataType.Double),\n        searchable=False,\n        filterable=False,\n        sortable=False,\n        facetable=False,\n        vector_search_dimensions=DIMENSIONS,\n        vector_search_profile_name=\"myHnswProfile\",\n    ),\n]\n```\n\n## For developers - Setting up the project\n\n### Install `just`\n\nWe use [`just`](https://just.systems/man/en/) as a command runner for\nactions / recipes related to developing Bytewax. Please follow [the\ninstallation\ninstructions](https://github.com/casey/just?tab=readme-ov-file#installation).\nThere's probably a package for your OS already.\n\n### Install `pyenv` and Python 3.12\n\nI suggest using [`pyenv`](https://github.com/pyenv/pyenv)\nto manage python versions.\n[the installation instructions](https://github.com/pyenv/pyenv?tab=readme-ov-file#installation).\n\nYou can also use your OS's package manager to get access to different\nPython versions.\n\nEnsure that you have Python 3.12 installed and available as a \"global\nshim\" so that it can be run anywhere. The following will make plain\n`python` run your OS-wide interpreter, but will make 3.12 available\nvia `python3.12`.\n\n```console\n$ pyenv global system 3.12\n```\n\n### Install `uv`\n\nWe use [`uv`](https://github.com/astral-sh/uv) as a virtual\nenvironment creator, package installer, and dependency pin-er. There\nare [a few different ways to install\nit](https://github.com/astral-sh/uv?tab=readme-ov-file#getting-started),\nbut I recommend installing it through either\n[`brew`](https://brew.sh/) on macOS or\n[`pipx`](https://pipx.pypa.io/stable/).\n\n## Development\n\nWe have a `just` recipe that will:\n\n1. Set up a venv in `venvs/dev/`.\n\n2. Install all dependencies into it in a reproducible way.\n\nStart by adding any dependencies that are needed into [pyproject.toml](pyproject.toml) or into\n[requirements/dev.in](requirements/dev.in) if they are needed for development.\n\nNext, generate the pinned set of dependencies with\n\n```console\n> just venv-compile-all\n```\n\n## Create and activate a virtual environment\n\nOnce you have compiled your dependencies, run the following:\n\n```console\n> just get-started\n```\n\nActivate your development environment and run the development task:\n\n```console\n> . venvs/dev/bin/activate\n> just develop\n```\n\n## License\n\n`bytewax-azure-ai-search` is commercially licensed with publicly\navailable source code. You are welcome to prototype using this module\nfor free, but any use on business data requires a paid license. See\nhttps://modules.bytewax.io/ for a license. Please see the full details\nin [LICENSE](./LICENSE.md).\n"
    },
    {
      "name": "apuslabs/ao-competition-playground-service",
      "stars": 2,
      "img": "https://avatars.githubusercontent.com/u/146177720?s=40&v=4",
      "owner": "apuslabs",
      "repo_name": "ao-competition-playground-service",
      "description": null,
      "homepage": null,
      "language": "Lua",
      "created_at": "2024-07-05T04:03:25Z",
      "updated_at": "2024-09-11T02:02:58Z",
      "topics": [],
      "readme": "# Benchmark POC: Model Benchmark on AO\n\nThe benchmark POC is a proof of concept that aims to benchmark LLM models on AO.\nFunders can setup a funding pool with a set of questions and get a leaderboard of the models that are able to answer the questions.\nEveryone can train their own model and compete against the other models.\nThe leaderboard is updated everyday with the latest scores.\nAfter the funding pool is over, the leaderboard is used to determine the winners and rewards are distributed.\n\n# Getting Started\n\n## Prerequesites\n\n- [Node.js](https://nodejs.org/en) (v20.0 or later)\n- [AOS installed](https://cookbook_ao.arweave.dev/welcome/getting-started.html)\n\n## Sending Message\n\n### Pool Creation\n\nFunders can create a funding pool by transfer [wrappedAR](https://aox.xyz/#/beta) with the following payload:\n\n```js\nao.send({\n   Target = 'xU9zFkq3X2ZQ6olwNVvr1vUWIjc3kXTWr7xKQD6dh10',\n   Action = 'Transfer',\n   Recipient = 'DLJoP8Xtdat6SKz3kqYGZPaa7DJBG6etF1jRLQCwquo',\n   Quantity = Fee,\n   ['X-Dataset'] = <your dataset process id>,\n   ['X-Allocation'] = 'ArithmeticDecrease'\n})\n```\n\n### Prepare Model for Benchmark\n\nArdrive -> Data Tx Id\n\n- Opensource Models\n  - Phi-3 Mini 4k Instruct: ISrbGzQot05rs_HKC08O_SmkipYQnqgB1yC3mjZZeEo\n  - Phi-2: kd34P4974oqZf2Db-hFTUiCipsU6CzbR6t-iJoQhKIo\n  - GPT-2 117M: XOJ8FBxa6sGLwChnxhF2L71WkKLSKq1aU5Yn5WnFLrY\n  - GPT-2-XL 4-bit quantized model: M-OzkyjxWhSvWYF87p0kvmkuAEEkvOzIj4nMNoSIydc\n  - CodeQwen 1.5 7B Chat q3: sKqjvBbhqKvgzZT4ojP1FNvt4r_30cqjuIIQIr-3088\n  - Llama3 8B Instruct q4: Pr2YVrxd7VwNdg6ekC0NXWNKXxJbfTlHhhlrKbAd1dA\n  - Llama3 8B Instruct q8: jbx-H6aq7b3BbNCHlK50Jz9L-6pz9qmldrYXMwjqQVI\n- Fine-tuned Models\n  - \n\n### Model Evaluation\n\nTrainer can join a pool by sending a message to the pool process with the following payload:\n\n```js\nao.send({\n   Target = 'DLJoP8Xtdat6SKz3kqYGZPaa7DJBG6etF1jRLQCwquo',\n   Action = 'Join-Pool',\n   Data = '{\"dataset\": <the pool id you want to join>, \"model\": <your model id>}'\n})\n```\n\nOnce you join a pool, the model will start evaluating the dataset and sending back the score to the pool process.\n\n### Get leaderboard\n\nTrainer can get the leaderboard result by sending a message to the pool process with the following payload:\n\n_Leaderboard updated every 24 hours_\n\n```js\nao.send({\n   Target = 'DLJoP8Xtdat6SKz3kqYGZPaa7DJBG6etF1jRLQCwquo',\n   Action = 'Leaderboard',\n   Data = <pool id>\n})\n```\n\n# Running a benchmark\n\n## Architecture\n\n### Benchmark\nThe Benchmark is reponsible for managing pool, funding, and rewards.\n\n**Key Data Structure:**\n- Benchmarks: A table that holds all the benchmark pools. Each pool contains:\n  - funder: The address of the funder.\n  - funds: The amount of funds in the pool.\n  - allocation: The allocation rule for the pool.\n  - startTime: The start time of the pool.\n  - endTime: The end time of the pool.\n  - models: A table of models participating in the pool. Each model contains:\n    - participant: The address of the participant.\n    - score: The score of the model.\n    - progress: The progress of the model.\n\n**Key Functions:**\n- Allocation: calculate allocation for each rule.\n\n**Handlers:**\n- Create-Pool: Handles the creation of a new funding pool.\n- Join-Pool: Handles the joining of a model to an existing pool.\n- Get-Pools: Retrieves the list of all existing pools.\n- Update-Leaderboard: Updates the leaderboard with the latest scores.\n- Allocate-Rewards: Allocates rewards to participants based on their scores.\n- Score-Response: Handles the response for the score action.\n- Leaderboard: Retrieves and prints the current leaderboard.\n\n**Cron:**\n- Update-Leaderboard: Updates the leaderboard every 24 hours.\n- Allocate-Rewards: Check if the pool is over and allocate rewards to participants every 24 hours.\n\n### Siqa Dataset\n\nThe Siqa Dataset is an example dataset that is used to benchmark the LLM models.\n\n**Initialization:**\n- DataTxID: Transaction Data ID for the dataset\n- LlamaRouter: Router for Llama\n- WrappedAR: Wrapped AR token address\n- SystemPrompt: System prompt for the assistant\n\n**Table Definition:**\n- datasets: Stores the context, question, and possible answers (A, B, C) along with the correct result.\n  - id: INTEGER PRIMARY KEY AUTOINCREMENT\n  - context: TEXT NOT NULL\n  - question: TEXT NOT NULL\n  - answerA: TEXT NOT NULL\n  - answerB: TEXT NOT NULL\n  - answerC: TEXT NOT NULL\n  - result: TEXT NOT NULL\n\n- models: Stores the model information.\n  - id: INTEGER PRIMARY KEY AUTOINCREMENT\n  - name: TEXT NOT NULL\n  - inference_process: TEXT NOT NULL\n  - data_tx: TEXT NOT NULL\n\n- evaluations: Stores the evaluation results of models on datasets.\n  - id: INTEGER PRIMARY KEY AUTOINCREMENT\n  - dataset_id: INTEGER NOT NULL\n  - model_id: INTEGER NOT NULL\n  - prompt: TEXT NOT NULL\n  - correct_answer: TEXT NOT NULL\n  - prediction: TEXT\n  - inference_start_time: DATETIME\n  - inference_end_time: DATETIME\n  - inference_reference: TEXT\n  - UNIQUE(dataset_id, model_id)\n  - FOREIGN KEY (dataset_id) REFERENCES datasets(id)\n\n\n**Key Functions:**\n- ResultRetriever: Converts answer letters (A, B, C) to corresponding numerical values.\n\n**Handlers:**\n- Init: Initializes the SQLite database and creates necessary tables.\n- Load-Data: Loads dataset into the database from a given message.\n- Info: Provides information about the Siqa dataset.\n- Get-Models: Retrieves and prints all models from the database.\n- Benchmark: Benchmarks a given model using the dataset.\n- Evaluate: Evaluates the model's predictions against the dataset.\n- Score: Calculates and sends the score of a model.\n- LlamaHerder.Transfer-Error: Handles transfer errors from LlamaHerder.\n\n**Cron:**\n- Evaluate: Evaluates the model's predictions against the dataset every 1 hours. Each time up to 1000 inferences are made.\n\n### Llama Router & Llama Worker\n\nLlama Router and Llama Worker is where inference actually happens.\n\nHandlers:\n- Register-Worker: Registers a worker and initializes its workload.\n- Unregister-Worker: Unregisters a worker and removes it from the list.\n- Inference: Handles inference requests by distributing them to workers with the least workload.\n- Inference-Response: Handles responses from workers and sends the results back to the requester.\n- loadModel: Loads a specified model for inference.\n- setMaxTokens: Sets the maximum number of tokens for inference.\n- setSystemPrompt: Sets the system prompt for the inference process.\n- Inference: Performs inference using the loaded model and sends the result back to the requester.\n\n\n## Build a dataset\n\nA dataset is currently a collection of questions that are sent to the LLM models.\n\nYou have to install [@sam/Llama-Herder](https://github.com/permaweb/llama-herder) to be able to run a benchmark.\n\n```shell\naos> .load-blueprint apm\nLoading...  apm\n📦 Loaded APM Client\naos> APM.install(\"@sam/Llama-Herder\")\n📤 Download request sent\nℹ️ Attempting to load @sam/Llama-Herder@1.0.3 package\n📦 Package has been loaded, you can now import it using require function\n```\n\nThen, load the dataset process\n```shell\n.load src/datasets/siqa.lua\n```\n\nThen, Initialize the table\n```shell\naos> Send({\n   Target = '<dataset process id>',\n   Action = 'Init'\n})\n```\n\nThen, load the dataset into the table\n```shell\naos> Send({\n   Target = '<dataset process id>',\n   Action = 'Load-Data',\n   Data = '<path to your dataset>'\n})\n```\n\n### Build your own Dataset\n\na Dataset should contains\n- Info: Describe the dataset\n- Dataset Loader: we use message to pass in siqa\n- Prompt Template\n- Result Retriever\n- Score Calculator\n- Evaluater: we use llama-herder here\n\n## Evaluate\n\nYou can use [LlamaHerder](https://github.com/permaweb/llama-herder) to evaluate the model's performance.\n\nor \n\nYou can setup your [local CU](https://github.com/permaweb/ao/tree/main/servers/cu) using `llama-router.lua` in `src`\n\nMake sure you have a CU running and setup the correct envs. such as `checkAdmissions` in [weaveDriver](https://github.com/permaweb/aos/tree/main/extensions/weavedrive).\n\nFinally, you can use `aos --cu-url=http://ip:6363` to start the llama-worker.\n\nMake sure you use your own `authority` Tags of the process and forward the message using [aoconnect](https://github.com/permaweb/ao/tree/main/connect)\n\n## Useful Resources\n\n### Links\n- [LlamaHerder](https://github.com/permaweb/llama-herder)\n- [WeaveDriver](https://github.com/permaweb/aos/tree/main/extensions/weavedrive)\n- [AO Connect](https://github.com/permaweb/ao/tree/main/connect)\n\n### Scirpts\n- [scripts/wrappedAR.ts](scripts/wrappedAR.ts) : A script to send wrappedAR to the pool/dataset process\n- [scripts/spawn.ts](scripts/spawn.ts) : A script to spawn all processes to setup the benchmark\n- [scripts/siqa.ts](scripts/siqa.ts) : A script to load the siqa data into the dataset process\n\n### Process ID\n- Benchmark: `DLJoP8Xtdat6SKz3kqYGZPaa7DJBG6etF1jRLQCwquo`\n- Siqa: `XUw5NtwUzk7hf_dE4ifEFD2rHfXffMLb8s2QfnZEvg4`\n- WrappedAR: `xU9zFkq3X2ZQ6olwNVvr1vUWIjc3kXTWr7xKQD6dh10`\n\n# Contributing\n\nWe welcome contributions! If you find a bug or have suggestions, please open an issue. If you'd like to contribute code, please fork the repository and submit a pull request.\n\n# Llama-Herder\n## Evaluate\n\n```lua\nSend({\n   Target = \"jaSRY9nVTdUE48QMg9SMuKbW8T9yk8Vi1FNZpau9M2A\",\n   Tags = {\n      Action = \"Inference\",\n      WorkerType = \"Evaluate\",\n      Reference = \"<Reference>\",\n   },\n   Data = json.encode({question = \"xxx\", expected_response = \"xxx\", context = \"xxx\"}),\n})\n```\n\n### Response\n\n```lua\nSend({\n   Target = msg.From,\n   Tags = {\n      Action = \"Inference-Response\",\n      WorkerType = \"Evaluate\",\n      Reference = \"<User Reference>\",\n   },\n   Data = \"<score:0-10>\"\n})\n```\n\n## Chat\n\n```lua\nSend({\n   Target = \"jaSRY9nVTdUE48QMg9SMuKbW8T9yk8Vi1FNZpau9M2A\",\n   Tags = {\n      Action = \"Inference\",\n      WorkerType = \"Chat\",\n      Reference = \"<Reference>\",\n   },\n   Data = json.encode({question = \"xxx\", context = \"xxx\"}),\n})\n```\n\n### Response\n\n```lua\nSend({\n   Target = msg.From,\n   Tags = {\n      Action = \"Inference-Response\",\n      WorkerType = \"Chat\",\n      Reference = \"<User Reference>\",\n   },\n   Data = \"<answer:max 40 tokens>\"\n})\n```\n\n# Embedding Service\n\naos text-embedding-service --module=ghSkge2sIUD_F00ym5sEimC63BDBuBrq4b5OcwxOjiw\n\n\n## How to use\n\nCreate-Dataset\n```\nSend({ Target = 'hMEUOgWi97d9rGT-lHNCbm937bTypMq7qxMWeaUnMLo', Action = \"Create-Dataset\", Data='{\"hash\":\"673322f20121f3dc36538578295819386f1ef2b8\",\"list\":[{\"content\":\"This contains variable declarations\",\"meta\":{\"title\":\"one\"}},{\"content\":\"This contains another sort of variable declarations\",\"meta\":{\"title\":\"two\"}},{\"content\":\"This has nothing to do with variable declarations\",\"meta\":{\"title\":\"three\"}},{\"content\":\"A random doc\",\"meta\":{\"title\":\"four\"}}]}' })\n```\n\nSearch-Prompt\n```\nSend({ Target = 'hMEUOgWi97d9rGT-lHNCbm937bTypMq7qxMWeaUnMLo', Action = \"Search-Prompt\", Data = '{\"dataset_hash\": \"673322f20121f3dc36538578295819386f1ef2b8\",\"prompt\":\"variable declarations\"}' })\n```\n\n## Process ID\nhMEUOgWi97d9rGT-lHNCbm937bTypMq7qxMWeaUnMLo"
    },
    {
      "name": "JANHMS/needle-haystack",
      "stars": 2,
      "img": "https://avatars.githubusercontent.com/u/45521680?s=40&v=4",
      "owner": "JANHMS",
      "repo_name": "needle-haystack",
      "description": "Needle components for Haystack projects.",
      "homepage": "https://needle-ai.com",
      "language": "Jupyter Notebook",
      "created_at": "2024-08-15T17:53:03Z",
      "updated_at": "2024-08-29T10:27:18Z",
      "topics": [
        "agents",
        "ai",
        "information-retrieval",
        "maschine-learning",
        "needle",
        "python",
        "rag",
        "retrival-augmented-generation",
        "sdk",
        "transfomers"
      ],
      "readme": "# Needle RAG tools for Haystack\n\n[![PyPI - Version](https://img.shields.io/pypi/v/needle-haystack-ai.svg)](https://pypi.org/project/needle-haystack-ai)\n[![PyPI - Python Version](https://img.shields.io/pypi/pyversions/needle-haystack-ai.svg)](https://pypi.org/project/needle-haystack-ai)\n\nThis package provides `NeedleDocumentStore` and `NeedleEmbeddingRetriever` component for use in Haystack projects.\n\n## Usage ⚡️\n\nGet started by installing the package via `pip`.\n\n```bash\npip install needle-haystack-ai\n```\n\n### API Keys\n\nWe will show you building a common RAG pipeline using Needle tools and OpenAI generator.\nFor using these tools you must set your environment variables, `NEEDLE_API_KEY` and `OPENAI_API_KEY` respectively.\n\nYou can get your Needle API key from from [Developer settings](https://needle-ai.com/dashboard/settings).\n\n### Example Pipeline 🧱\n\nIn Needle document stores are called collections. For detailed information, see our [docs](https://docs.needle-ai.com).\nYou can create a reference to your Needle collection using `NeedleDocumentStore` and use `NeedleEmbeddingRetriever` to retrieve documents from it.\n\n```python\nfrom needle_haystack import NeedleDocumentStore, NeedleEmbeddingRetriever\n\ndocument_store = NeedleDocumentStore(collection_id=\"<your-collection-id>\")\nretriever = NeedleEmbeddingRetriever(document_store=document_store)\n```\n\nUse the retriever in a Haystack pipeline. Example:\n\n```python\nfrom haystack import Pipeline\nfrom haystack.components.generators import OpenAIGenerator\nfrom haystack.components.builders import PromptBuilder\n\nprompt_template = \"\"\"\nGiven the following retrieved documents, generate a concise and informative answer to the query:\n\nQuery: {{query}}\nDocuments:\n{% for doc in documents %}\n    {{ doc.content }}\n{% endfor %}\n\nAnswer:\n\"\"\"\n\nprompt_builder = PromptBuilder(template=prompt_template)\nllm = OpenAIGenerator()\n\n# Add components to pipeline\npipeline = Pipeline()\npipeline.add_component(\"retriever\", retriever)\npipeline.add_component(\"prompt_builder\", prompt_builder)\npipeline.add_component(\"llm\", llm)\n\n# Connect the components\npipeline.connect(\"retriever\", \"prompt_builder.documents\")\npipeline.connect(\"prompt_builder\", \"llm\")\n```\n\nRun your RAG pipeline:\n\n```python\nprompt = \"What is the topic of the news?\"\n\nresult = basic_rag_pipeline.run({\n    \"retriever\": {\"text\": prompt},\n    \"prompt_builder\": {\"query\": prompt}\n})\n\n# Print final answer\nprint(result['llm']['replies'][0])\n```\n\n# Support 📞\n\nFor detailed guides, take a look at our [docs](https://docs.needle-ai.com). If you have questions or requests you can contact us in our [Discord channel](https://discord.gg/JzJcHgTyZx). \n\n# License\n\n`needle-haystack` is distributed under the terms of the MIT license.\n"
    },
    {
      "name": "iceray00/LLM_RAG_Clanguage",
      "stars": 2,
      "img": "https://avatars.githubusercontent.com/u/108821718?s=40&v=4",
      "owner": "iceray00",
      "repo_name": "LLM_RAG_Clanguage",
      "description": "C Language RAG with LLM and HayStack",
      "homepage": null,
      "language": "Python",
      "created_at": "2024-07-18T13:28:43Z",
      "updated_at": "2025-03-25T13:11:16Z",
      "topics": [],
      "readme": "# C Language RAG with LLM and HayStack\n\n\n\n```bash\nsh initisl_rag.sh\n```\n\n\n在DocumentStore方面，我们选择开源的[Chroma](https://docs.trychroma.com).\n\n其中，使用Ollama提供的与Haystack和Chroma的接口，从Ollama Model Library中，选取了:\n\n* `snowflake-arctic-embed`\n* `nomic-embed-text`\n* `mxbai-embed-large`\n\n这3个Embedding模型，用于RAG过程中的自定义嵌入模型使用。 或者在后面可以综合使用这三个向量嵌入模型，在我们的知识库中达到更好的效果。\n\n\n## Dependencies\n\n```bash\npip3 install -r requirements.txt\n```\n\n## Download and Install\n\n* 修改Ollama模型下载地址\n```bash\nexport OLLAMA_MODELS=/root/autodl-tmp/models\necho 'export OLLAMA_MODELS=/root/autodl-tmp/models' >> ~/.bashrc\nsource ~/.bashrc\n```\n\n* 下载 ollama\n```bash\ncurl -fsSL https://ollama.com/install.sh | sh\n```\n\n* 启动 ollama 服务\n```bash\nollama serve\n```\n\n* 下载模型\n```bash\nollama pull qwen2\nollama pull nomic-embed-text:latest\n```\n\n```bash\ncd chat\n```\n\n## Quick Start\n\n```bash\npython3 full.py -m knowledge -g qwen2 -e nomic-embed-text\n```\n\n## Usage\n\n```bash\n# python3 full.py -h\nusage: full.py [-h] [-e EMODEL] [-g GMODEL] [-k TOP_K] [-m MODE]\n\nRAG Chat Bot for C Language Learning!\n\noptions:\n  -h, --help            show this help message and exit\n  -e EMODEL, --Emodel EMODEL\n                        Embedding Model\n  -g GMODEL, --Gmodel GMODEL\n                        Generator Model\n  -k TOP_K, --top_k TOP_K\n                        Top-k for retrieval\n  -m MODE, --mode MODE  `test` or `knowledge`. Default is `knowledge`\n```\n\n\n\n\n\n\n"
    },
    {
      "name": "kyrillschmid/SEGym",
      "stars": 2,
      "img": "https://avatars.githubusercontent.com/u/12870793?s=40&v=4",
      "owner": "kyrillschmid",
      "repo_name": "SEGym",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2024-03-21T10:34:26Z",
      "updated_at": "2024-08-27T17:02:40Z",
      "topics": [],
      "readme": "# SEGym\r\nSEGym allows you to simulate patches for Python repos in isolated environments.\r\nYou can use such an environment to let a solver (e.g. LLM) search for a patch for a given issue until the issue is resolved.\r\n\r\n## Installation\r\nThis project is not yet available on PyPI. To try it out, clone the repository and open the `.devcontainer/devcontainer.json` in VSCode. This will automatically set up a development environment with all necessary dependencies (including `docker`, `git`, `poetry` and the required Python packages).\r\n\r\n## Working with the project\r\n\r\n### Models\r\n\r\nSupply your own `openai.Client` compatible API.\r\n\r\nFOR LMU: use `openai_lmu.get_lmu_openai_client()` to get a ready-to-use client.\r\n\r\n### Running the gym\r\nDrawing strong similarities to the OpenAI gym, the `SEGym` class is the main entry point for the library. It allows you to create a new environment, reset it, and step through it.\r\nNo LLM generated content will modify local files, instead `env` starts up a docker container for every patch generation, ensuring that the host system is not affected by any potential bugs in the generated code.\r\nFor example usage, see [demo.ipynb](demo.ipynb).\r\n"
    },
    {
      "name": "reaganlo/ai-safety",
      "stars": 2,
      "img": "https://avatars.githubusercontent.com/u/12738189?s=40&v=4",
      "owner": "reaganlo",
      "repo_name": "ai-safety",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2024-07-02T00:52:39Z",
      "updated_at": "2024-08-01T18:31:56Z",
      "topics": [],
      "readme": "# AI Safety for local LLMs\nEvaluate the options for implementing safety guardrails for locally run LLMs.\n\n## Guardrail requirements\n- Does not involve fine tuning of models.\n- Must be run locally and offline.\n- Low latency\n  - Less than 0.8 seconds?\n- Accuracy\n  - False Positives can lead to the app's unusability and missing information.\n  - False Negatives can lead to vulnerabilities.\n  - If the ideal balance cannot be achieved, the FP-FN tradeoff can be decided based on the LLM use-case.\n- Low memory footprint\n  - Less that 800 MB?\n\n## Key Vulnerabilities\n- Profanity and toxic content\n- Prompt Injection\n- PII\n  - Should certain PII's be allowed?\n    E.g. In a RAG application, extracting email ids of the authors from a whitepaper.\n\n\n## Guardrail options\n1. [Guardrails-AI](guardrails-ai/README.md)\n2. [NeMo-Guardrails](nemo-guardrails/README.md)\n3. [Safety-System-Prompts](model-safety/src/safety_prompts.py)\n\n## Model Evaluation options\n1. [Model-Safety](model-safety/README.md)\n2. [PyRit](https://github.com/Azure/PyRIT/blob/main/README.md)\n\n## Guardrail Design Recommendation\n1. Use non-LLM, lightweight models from [Guardrails-AI](guardrails-ai/README.md) to validate the user input and LLM output.\n2. Use system prompt engineering similar to [Safety-System-Prompts](model-safety/src/safety_prompts.py) to instruct the LLM itself to behave in a safe manner.\n\n![AI Safety Design](https://github.com/reaganlo/guardrails-llm/blob/main/guardrails-design.png?raw=true)\n"
    },
    {
      "name": "PhamTrinhDuc/retrieval-augmented-generation-with-Langchain",
      "stars": 2,
      "img": "https://avatars.githubusercontent.com/u/127647215?s=40&v=4",
      "owner": "PhamTrinhDuc",
      "repo_name": "retrieval-augmented-generation-with-Langchain",
      "description": "What I learn about RAG, i use Langchain and maybe Llama-Index",
      "homepage": "",
      "language": "Jupyter Notebook",
      "created_at": "2024-07-09T14:35:06Z",
      "updated_at": "2024-10-10T07:01:01Z",
      "topics": [
        "langchain",
        "large-language-models",
        "llama-index",
        "retrieval-augmented-generation"
      ],
      "readme": ""
    },
    {
      "name": "Blizarre/microProjects",
      "stars": 2,
      "img": "https://avatars.githubusercontent.com/u/66079?s=40&v=4",
      "owner": "Blizarre",
      "repo_name": "microProjects",
      "description": "This repo is for micro projects, usually one file only",
      "homepage": null,
      "language": "Python",
      "created_at": "2015-01-26T22:47:34Z",
      "updated_at": "2024-12-21T13:32:38Z",
      "topics": [],
      "readme": "This repo is a collection of very small programs, usually only one file.\n\n* highload.fun : coding challenges with leaderboard. This directory contains the rust projects that I used before submitting the code.\n* compareImage : python script, compare 2 images and show the differences\n* improveSkyImage : python scripts, improve image of the sky and find orientation of star movement in the sky\n* memoryAccessPattern : benchmark to evaluate differences in performance between random and sequential access to RAM\n* seamCarving : implementation of the seam carving technique\n* switchVsIfElse : benchmark to evaluate differences in performance between if/else and switch\n* md5: simple md5 implementation\n* async\\_test: Small test for async rust programming (fetch urls in parallel)\n* highload\\_fun: Small helper project to help me solve challenges on highload\\_fun\n* C: A tool for quick directory traversal with fzf for the `fish` shell\n* weekend_rag: A quick experiment with RAG (AKA AI Search) that I did over the weekend"
    },
    {
      "name": "Futyn-Maker/wb_project",
      "stars": 2,
      "img": "https://avatars.githubusercontent.com/u/54239175?s=40&v=4",
      "owner": "Futyn-Maker",
      "repo_name": "wb_project",
      "description": null,
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2024-05-14T07:36:56Z",
      "updated_at": "2025-02-10T14:40:33Z",
      "topics": [],
      "readme": "# wb_project\n\nВ данном репозитории находится пример реализации вопросно-ответной системы для сотрудников пунктов выдачи заказов крупного маркетплейса. Система использует RAG-подход и основана на следующих технологиях:\n\n* Фреймворк: [Haystack](https://haystack.deepset.ai/);\n* Эмбеддер: модель [multilingual-e5-large-instruct](https://huggingface.co/intfloat/multilingual-e5-large-instruct), [дотренированная](https://huggingface.co/Futyn-Maker/wb_questions) на парах похожих вопросов из датасета Wildberries, т.е. таких, на которые были даны одинаковые ответы;\n* LLM: [Дообученная](https://huggingface.co/Futyn-Maker/saiga_llama3_8b_wildberries) на парах \"Вопрос-ответ\" из датасета Wildberries [Saiga3](https://huggingface.co/IlyaGusev/saiga_llama3_8b), точнее её [Llama.cpp-версия](https://huggingface.co/Futyn-Maker/saiga_llama3_8b_wildberries_4bit_gguf);\n* FastAPI+PostgreSQL - для функционирования микросервиса;\n* [PGVector](https://github.com/pgvector/pgvector) - расширение для PostgreSQL для хранения в базе эмбеддингов документов (только для локальной версии).\n\n## Использование\n\nСистема реализована в двух режимах: в виде Jupyter-ноутбука для Google Colab и в виде микросервиса на FastAPI с минималистичным интерфейсом. Второй вариант подходит для более продолжительной и надёжной работы системы и не использует видеокарту для инференса LLM.\n\n### Способ 1: запуск в Google Colab\n\n1. [Откройте тетрадку с реализованной системой в Google Colab](https://colab.research.google.com/github/Futyn-Maker/wb_project/blob/main/helper_rag.ipynb);\n2. Убедитесь, что в качестве среды выполнения установлена среда с GPU;\n3. Поместите Excel-файлы с оригинальными данными в сессионное хранилище и выполните первые две ячейки; это создаст каталог `data` и поместит туда ваши данные;\n4. Последовательно выполните все остальные ячейки, дождитесь, пока установятся зависимости и подготовятся данные, в т.ч. создадутся эмбеддинги для документов.\n5. Во фрейме в последней ячейке отобразиться интерфейс Gradio. В появившемся поле можно задавать вопросы и получать ответы;\n6. Пока запущена среда и ячейка с Gradio, вы можете поделиться публичной ссылкой для демонстрации модели всем желающим; ссылка будет расположена в информации в верху фрейма.\n\n### Способ 2: локальный запуск\n\n> Внимание! В этом режиме ответы с помощью LLM генерируются существенно медленно, поскольку не используется видеокарта. Кроме того, если в системе нет видеокарты, вычисление эмбеддингов для документов может занимать до 40 минут. Для работы системы необходим дистрибутив Linux, можно WSL.\n\n1. Склонируйте репозиторий и перейдите в каталог проекта:\n\n```bash\ngit clone https://github.com/Futyn-Maker/wb_project.git\ncd wb_project\n```\n\n2. Создайте каталог `data` и поместите туда оригинальные данные в виде XLSX-таблиц, не меняя их названия.\n\n```bash\nmkdir data\n```\n\n3. Установите переменные среды:\n\n```bash\ncp .env.example .env\nnano .env\n```\n\nЗдесь укажите параметры для подключения к вашей PostgreSQL-базе: имя пользователя, пароль, название базы.\n\n4. Если у вас отсутствует модуль venv, установите его: `sudo apt install python3-venv`.\n5. Запустите скрипт подготовки среды и данных, дождитесь, пока он завершит работу - это может занять продолжительное время:\n\n```bash\nbash prepare.sh\n```\n\nЭтот скрипт:\n\n* Создаст и активирует новую виртуальную среду;\n* Установит зависимости;\n* Установит PostgreSQL, если ранее сервис отсутствовал;\n* Создаст указанного в `.env` пользователя и базу данных;\n* Установит PGVector;\n* Запустит серию скриптов для подготовки данных и создания эмбеддингов документов;\n* Скачает дообученную языковую модель\n\n6. Запустите сервис:\n\n```bash\nbash run.sh\n```\n\n7. Ищите интерфейс на http://127.0.0.1:8000/static/index.html;\n8. Если необходимо обратиться напрямую к API, делается это следующим образом:\n\n```\nhttp://127.0.0.1:8000/get_answer/?question=Привет!\n```\n"
    },
    {
      "name": "Redna/GenerativeAgents",
      "stars": 2,
      "img": "https://avatars.githubusercontent.com/u/18450828?s=40&v=4",
      "owner": "Redna",
      "repo_name": "GenerativeAgents",
      "description": null,
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2023-06-25T20:18:09Z",
      "updated_at": "2024-06-12T13:40:09Z",
      "topics": [],
      "readme": ""
    },
    {
      "name": "elvanselvano/streamlit-whisper",
      "stars": 2,
      "img": "https://avatars.githubusercontent.com/u/49674061?s=40&v=4",
      "owner": "elvanselvano",
      "repo_name": "streamlit-whisper",
      "description": "empowering the visually impaired with equal financial access through artificial intelligence",
      "homepage": "",
      "language": "Python",
      "created_at": "2024-05-11T06:14:44Z",
      "updated_at": "2024-09-26T13:36:58Z",
      "topics": [
        "audio-processing",
        "large-language-model",
        "speech-recognition"
      ],
      "readme": "# streamlit-whisper\n\n## How to use:\n\n1. Fork the repository\n\n2. Clone the fork locally.\n\n3. Download devcontainer extension on vscode\n\n    > **IMPORTANT NOTE**\n    > All development should be done in devcontainer. Usage of other tool may result in failed auto-formatting.\n    > Windows user may need WSL for docker. See [here](https://docs.docker.com/desktop/install/windows-install/) for guide on how to install docker\n\n4. Press \"reopen in container\" on vscode.\n\n    > **Note**: port 8501 should be available.\n\n## How to start application\n\n1. enter the command\n\n    ``sh\n    streamlit run app.py\n    ``\n"
    },
    {
      "name": "prapooskur/SlugCourses",
      "stars": 2,
      "img": "https://avatars.githubusercontent.com/u/61389919?s=40&v=4",
      "owner": "prapooskur",
      "repo_name": "SlugCourses",
      "description": "Project for CruzHacks 2024",
      "homepage": null,
      "language": "Kotlin",
      "created_at": "2024-01-20T01:23:24Z",
      "updated_at": "2024-11-16T20:51:39Z",
      "topics": [],
      "readme": "## Inspiration\n\nLike students at every university, the four of us have to sign up for courses at the start of each quarter. In order to determine what classes are offered or the status of classes we're interested in, we must use [Pisa](https://pisa.ucsc.edu/class_search/), UCSC's class search website. It does exactly what it says on the tin: allows you to search for classes with a variety of filters, such as whether the class is open or waitlisted, the subject, the name of the class, and the number of credits. As a tool, it has numerous issues from a mobile interface standpoint:\n\n* The mobile website is not user-friendly at all\n* It does not have a smooth and responsive design\n* The design lacks an ease of use for those with accessibility issues\n* It does not follow the best practices of mobile usability\n* It is difficult to navigate through and search for classes\n* The interface was clearly designed to be navigated with a mouse and not a touch screen\n\n\n<img src=\"https://raw.githubusercontent.com/prapooskur/CruzHacks-2024-Project/main/images/pisasearch.gif?token=GHSAT0AAAAAACNEB4YKNME6MHLKXNX4ZFA6ZNNJK7Q\" width=30% height=30% alt=\"pisa current mobile interface\">\n\n\nWe believe that we could make an improved version of the class search website's mobile interface, making it accessible to mobile users who want to access the UCSC catalogue anytime and anywhere. \n\nAmong other things, we also realized was that finding classes among the 1500+ courses offered at UCSC every quarter was a challenge. Pisa does not offer any easy way of searching or comparing classes with natural language. In order to search for a class, you must have prior knowledge of the class in question ranging from its name, department, catalog number, or the professor in charge. To make this easier, we envisioned a chatbot powered by a large language model (LLM) which could use a technique called [retrieval-augmented generation](https://blogs.nvidia.com/blog/what-is-retrieval-augmented-generation/) (RAG) to scan a database of UCSC courses and return the courses most relevant to a user's course. \n\n\n## What it does\n\nThe app we created allows users to search for classes intuitively in a simple and easy to navigate interface.\n\n\n<img src=\"https://raw.githubusercontent.com/prapooskur/CruzHacks-2024-Project/main/images/search%20for%20cse%20100.gif?token=GHSAT0AAAAAACNEB4YKHNOCL26SAKTVI6UKZNNKKLQ\" width=30% height=30% alt=\"searhing for a class\">\n\n\nThe results page presents information in a similarly easy-to-read manner. Tapping on any result brings up a more detailed results page. If the user still wants even more details, they can tap on a button to take them to Pisa's page for it.\n\n\n<img src=\"https://raw.githubusercontent.com/prapooskur/CruzHacks-2024-Project/main/images/result%20to%20pisa.gif?token=GHSAT0AAAAAACNEB4YKX3HN53QZYM5ZE2ROZNNKLFA\" width=30% height=30% alt=\"result to pisa\">\n\n\nAdvanced filters on the main search page also allow for more fine tuned results.\n\n<img src=\"https://raw.githubusercontent.com/prapooskur/CruzHacks-2024-Project/main/images/ge%20filter%20search.gif?token=GHSAT0AAAAAACNEB4YLWR7EOFOWV7HAZ7G4ZNNKMLA\" width=30% height=30% alt=\"result to pisa\">\n\n\nThe integrated LLM-based chatbot can be talked to via a button on the bottom navigation bar. It can be asked questions about courses to take.\n\n\n<img src=\"https://raw.githubusercontent.com/prapooskur/CruzHacks-2024-Project/main/images/llm%20response.gif?token=GHSAT0AAAAAACNEB4YKS6WCKMTIZLAO4FBMZNNKN4A\" width=30% height=30% alt=\"llm response\">\n\n\n## How we built it\n\nFirst, we set our sights on creating a database of all UCSC courses. To do this, we wrote a webscraper in Python that scraped the entire website for all 1,456 courses offered this quarter, as well as the 1000+ courses offered in each of the prior four quarters for a total of close to 6000 courses. We then pushed this to [Supabase](https://supabase.com/), a PostgresSQL database. In order to update the database with the most relevant information, the scraper is re-run at the top of every hour. Within the app itself, whenever a user searches for a course, this database is queried for the most relevant courses and all its information. \n\n\n<div>\n<img src=\"https://raw.githubusercontent.com/prapooskur/CruzHacks-2024-Project/main/images/database.png?token=GHSAT0AAAAAACNEB4YKNYNHEGD46ICC6PECZNNJ5GQ\" width=100% height=30% alt=\"pisa current mobile interface\">\n\n<img src=\"https://raw.githubusercontent.com/prapooskur/CruzHacks-2024-Project/main/images/search%20results.png?token=GHSAT0AAAAAACNEB4YLVEQNQYC23QKGYMQYZNNJ6FQ\" width=30% height=30% alt=\"pisa current mobile interface\">\n</div>\n\nNext up was the LLM chatbot. The model we chose was Google's [Gemini](https://blog.google/technology/ai/google-gemini-ai/), the same model that powers Bard, Google's ChatGPT competitor. Now we couldn't simply give it our entire database and tell it to extract the most relevant courses (though that didn't stop us from trying). We needed a way to extract the classes that closely matched the user's input query, whether that be a question (\"What astronomy courses are offered?\") or an imperative statement (\"Recommend me classes about chemistry\"). In order to do this, we decided on [Haystack](https://haystack.deepset.ai/), an open source Python framework for implementing retrieval-augmented generation. We wrote Python code that pulled our entire repository of course data into a single file. Using Haystack, we wrote a seven-stage pipeline for getting user input:\n\n1. Load data about all classes into document storage \n2. Generate text embedding for user input\n    * This turns the user's input into a vector that can be processed \n3. Create a list of documents that match the input (sparse document retrieval) \n    * Uses the \"bag of words\" technique: simply matches keywords without taking context into account\n4. Create a second list of courses that match the input (dense document retrieval) \n    * Neural encoder learns the best way to encode text into vectors, taking into account context and semantic meaning of words\n5. Join sparse/dense documents into one list of documents, ranked by score\n6. Insert merged document list and user input into prompt\n    * Prompt was specifically engineered to make Gemini returns its recommendations in a specific and consistent format with only relevant information\n7. Query Gemini LLM with final prompt and return response. \n\nOnce we got the pipeline running, we used the Python FastAPI and Uvicorn libraries to create an API endpoint that the app could use to send the user's query and get back Gemini's response. \n\n\n## Challenges we ran into\n\nOne of the hurdles we ran into was attempting to find ways to fine tune the LLM response. Give it too simple of a prompt and it would be saying too much to even fit on screen. Give it too restrictive of a prompt and it wouldn't give enough information. We had to engineering the prompt in such a way that it would balance just the right amount of information given.\n\nAnother challenge we ran into was turning Gemini's response into a proper API. Because Gemini returned its response in chunks at a time, we had two choices: either wait for it to return its *entire* response before handing the response to the app (which would be easier but result in longer wait times), or attempt to stream the response API to the app as the responses come in (harder, but would reduce wait times). We tried various methods of passing the responses to the app as it was returned from Gemini, such as chunking the data into separate JSON responses, but to no avail. Eventually, in the interest of time we decided to go with the former option of returning the response all at once so we could make the rest of the app function. \n\n## Accomplishments that we're proud of\n\nWe're really proud of the UI/UX of the app. The mobile experience is significantly improved over the website. The UI was modernized to be more aesthetically pleasing, and content is presented in a easily digestible manner. The UX is markedly enhanced with proper support for phones and touch screen devices. It up to date with modern UI principles (we follow Google's Material Design 3 guidelines) and was designed with modern UI prototyping tools such as Figma. It is also integrated with a lot of common touch screen actions such as swiping to the left of the screen to go back the previous screen. \n\nWe are also incredibly proud of the LLM integration. It was a massive undertaking to delve into the realm of large language models, and especially something as complicated as retrieval-augmented generation. By creating our own pipeline instead of going with a cloud solution, we learned an incredible amount of information about the underlying workings of language models and retrieval pipelines.\n\n\n## What we learned\n\nWe definitely learned a lot of stuff. We learned to work with a proper cloud SQL database for our backend data and how to integrate it with webscraped data. We also learned a lot about how LLMs could be used to search documents in addition to simple conversations. We also learned the complexity and depth of UX/UI design on Figma. We also learned a lot about app development and integrating it with all of our complex moving parts. \n\n## What's next for SlugCourses\n\nThere are numerous future features and improvements that we can make to SlugCourses. This include the following:\n\n* Refine the current search functionality\n* Improving the functionality of the chatbot\n* Create an integration using Rate My Professor\n* Develop iOS/iPadOS and desktop versions of the app\n\nThis experience was invaluable and taught us a lot about app development, user interface design, and natural language processing. We are proud of what we have accomplished and we hope to continue working on SlugCourses to make it even better for UCSC students. "
    },
    {
      "name": "alanmeeson/lancedb-haystack",
      "stars": 2,
      "img": "https://avatars.githubusercontent.com/u/10676989?s=40&v=4",
      "owner": "alanmeeson",
      "repo_name": "lancedb-haystack",
      "description": "A LanceDB based DocumentStore for Haystack 2.x",
      "homepage": null,
      "language": "Python",
      "created_at": "2024-05-16T20:13:37Z",
      "updated_at": "2024-11-08T11:14:25Z",
      "topics": [],
      "readme": "[![test](https://github.com/alanmeeson/lancedb-haystack/actions/workflows/test.yml/badge.svg)](https://github.com/alanmeeson/lancedb-haystack/actions/workflows/test.yml) [![Documentation Status](https://readthedocs.org/projects/lancedb-haystack/badge/?version=latest)](https://lancedb-haystack.readthedocs.io/en/latest/?badge=latest)\n\n# LanceDB Haystack Document store\n\nLanceDB-Haystack is an embedded [LanceDB](https://lancedb.github.io/lancedb/) backed Document Store for \n[Haystack 2.X](https://github.com/deepset-ai/haystack/).\n\n## Installation\n\nThe current simplest way to get LanceDB-Haystack is to install from GitHub via pip:\n\n```pip install lancedb-haystack```\n\n## Usage\n\n```python\nimport pyarrow as pa\nfrom lancedb_haystack import LanceDBDocumentStore\nfrom lancedb_haystack import LanceDBEmbeddingRetriever, LanceDBFTSRetriever\n\n# Declare the metadata fields schema, this lets us filter using it.\n# See: https://arrow.apache.org/docs/python/api/datatypes.html\nmetadata_schema = pa.struct([\n  ('title', pa.string()),    \n  ('publication_date', pa.timestamp('s')),\n  ('page_number', pa.int32()),\n  ('topics', pa.list_(pa.string()))\n])\n\n# Create the DocumentStore\ndocument_store = LanceDBDocumentStore(\n  database='my_database', \n  table_name=\"documents\", \n  metadata_schema=metadata_schema, \n  embedding_dims=384\n)\n\n# Create an embedding retriever\nembedding_retriever = LanceDBEmbeddingRetriever(document_store)\n\n# Create a Full Text Search retriever\nfts_retriever = LanceDBFTSRetriever(document_store)\n```\n\nSee also `examples/pipeline-usage.ipynb` for a full worked example.\n\n## Development\n\n### Test\n\nYou can use `hatch` to run the linters:\n\n```console\n~$ hatch run lint:all\ncmd [1] | ruff .\ncmd [2] | black --check --diff .\nAll done! ✨ 🍰 ✨\n6 files would be left unchanged.\ncmd [3] | mypy --install-types --non-interactive src/lancedb_haystack tests\nSuccess: no issues found in 6 source files\n```\n\nSimilar for running the tests:\n\n```console\n~$ hatch run cov\ncmd [1] | coverage run -m pytest tests\n...\n```\n\n### Build\n\nTo build the package you can use `hatch`:\n\n```console\n~$ hatch build\n[sdist]\ndist/lancedb_haystack-0.1.0.tar.gz\n\n[wheel]\ndist/lancedb_haystack-0.1.0-py3-none-any.whl\n```\n\n### Document\n\nTo build the api docs run the following:\n\n```console\n~$ cd docs\n~$ make clean\n~$ make build\n```\n\n### Roadmap\n\nIn no particular order:\n\n- **Figure out if it's possible to have LanceDB work with dynamic metadata**\n\n  Currently, this implementation is limited to having only metadata which is defined in the metadata_schema.  It would be\n  nice to be able to infer a schema from the first document to be added, or even better, be able to just have arbitrary\n  metadata, rather than having to specify it all up front.\n\n- **Expand the supported metadata types**\n  \n  As noted the metadata section requires a pyarrow schema;  not all of the types have been tested, and may not all be \n  supported.  It would be good to try out a few more to see if they're supported, and perhaps add those that aren't. \n\n## Limitations\n\nThe DocumentStore requires a pyarrow StructType to be specified as the schema for the metadata dict.  This should cover\nall metadata fields which may appear in any of the documents you want to store.\n\nCurrently, the system supports the basic datatypes (ints, floats, bools, strings, etc.)  as well as structs and lists.  \nOthers may work, but haven't been tested."
    },
    {
      "name": "NechbaMohammed/ai-chat-bot",
      "stars": 2,
      "img": "https://avatars.githubusercontent.com/u/108382119?s=40&v=4",
      "owner": "NechbaMohammed",
      "repo_name": "ai-chat-bot",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2024-04-09T00:22:54Z",
      "updated_at": "2025-03-13T18:31:34Z",
      "topics": [],
      "readme": "---\ntitle: Ai Chat Bot\nemoji: 👀\ncolorFrom: green\ncolorTo: yellow\nsdk: streamlit\nsdk_version: 1.33.0\napp_file: app.py\npinned: false\nlicense: apache-2.0\n---\n\nCheck out the configuration reference at https://huggingface.co/docs/hub/spaces-config-reference\n"
    },
    {
      "name": "Nikunj3masarani/DocAI",
      "stars": 2,
      "img": "https://avatars.githubusercontent.com/u/18481030?s=40&v=4",
      "owner": "Nikunj3masarani",
      "repo_name": "DocAI",
      "description": null,
      "homepage": null,
      "language": "TypeScript",
      "created_at": "2024-03-27T05:16:56Z",
      "updated_at": "2024-08-14T07:08:03Z",
      "topics": [],
      "readme": "🚧 Under Active Development 🚧\n\n# DocAI- A generative AI app with haystack framework.\n\n![App Image](resources/OIG2.eCHkxNCFo2UPskuoK29j.jpg)\n\nRAG application powered by Haystack AI, designed to revolutionize question-answering processes. Harnessing advanced technology, our platform seamlessly integrates with Haystack AI to deliver unparalleled accuracy and efficiency in retrieving and generating responses to a wide array of queries. Whether you're seeking insights into complex topics or quick answers to pressing questions, our RAG application is your go-to solution. Experience the future of information retrieval and generation with our user-friendly interface and cutting-edge algorithms. Say goodbye to tedious searches and hello to instant, reliable answers with our RAG application.\n\n\n## Key Features 🎯\n\n- **Fast and Efficient**: Designed with speed and efficiency at its core. DocAI ensures rapid access to your data.\n- **Secure**: Your data, your control. Always.\n- **OS Compatible**: Ubuntu 22 or newer.\n- **File Compatibility**: PDF\n\n### Server Specifications\nBelow are the specifications for the server environment\n- RAM: 8 GB\n- CPU: 4 core\n- Storage: 50 GB\n\n### Prerequisites 📋\n\nEnsure you have the following installed:\n\n- Docker\n- Docker Compose\n\n### Installation 💽\n    \n- **Step 1**: Clone the repository:\n\n  ```bash\n  git clone https://github.com/Nikunj3masarani/DocAI && cd DocAI\n  ```\n\n\n- **Step 2**: Copy the `sample.env` files\n\n  ```bash\n  cp sample.env .env\n  ```\n\n- **Step 3**: Update the `.env` files\n\n  ```bash\n  vim .env # or emacs or vscode or nano\n  ```\n- **Step 4** Create a volume dictionary for elastic search\n  \n  ```bash \n  mkdir elasticsearch_data\n   ```\n  \n- **Step 5** Create a volume dictionary for backend and download models\n\n  ```bash\n  mkdir backend_models && cd backend_models\n  ```\n\n  - Download Sentence Transformer Model from huggingface\n\n    ```bash\n    git lfs clone https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2\n    ```\n\n  - Download Rank Model from huggingface\n\n    ```bash\n    git lfs clone https://huggingface.co/cross-encoder/ms-marco-MiniLM-L-12-v2\n    ```\n\n\n- **Step 6** Start the application with docker \n    ```bash\n    docker-compose pull\n    docker-compose up\n     ```\n\n### Application will be accessible with  \n\n - http://localhost:3000/login\n\n\n## License 📄\n\nThis project is licensed under the Apache 2.0 License - see the [LICENSE](LICENSE) file for details\n\n"
    },
    {
      "name": "exowanderer/WikidataChat",
      "stars": 2,
      "img": "https://avatars.githubusercontent.com/u/11221046?s=40&v=4",
      "owner": "exowanderer",
      "repo_name": "WikidataChat",
      "description": "Retrieval Augmented Generation (RAG) with for answering question with the Wikidata REST API",
      "homepage": null,
      "language": "Python",
      "created_at": "2024-03-24T22:34:04Z",
      "updated_at": "2025-02-24T11:34:43Z",
      "topics": [],
      "readme": "# WikidataChat\nRetrieval Augmented Generation (RAG) with for answering question with the Wikidata REST API\n\n## Introduction\nWikidataChat is an innovative tool designed to leverage the comprehensive knowledge base of Wikidata, transforming it into a user-friendly question-and-answer chat interface. It aims to provide the general public with validated and cited knowledge directly from Wikidata, reducing the chances of misinformation or \"hallucinations\" often associated with large language models (LLMs).\n\n## Features\nWikidataChat boasts a unique textification pipeline with the following capabilities:\n- **Search and Download**: Utilizes Google's Serapi search pipeline and Wikidata's REST API to fetch relevant JSON data.\n- **Textification**: Converts Wikidata statements into string statements, preparing them for processing.\n- **RAG Pipeline**: Merges Wikidata string statements with user-provided questions to generate informed and accurate answers through an LLM.\n- **User Interface**: Offers a friendly UI that not only presents answers but also provides linked lists of Wikidata and Wikipedia URLs for further exploration.\n\n![Wikidata and the Meaning of Life](https://github.com/exowanderer/WikidataChat/blob/main/images/wikidatachat_meaning_of_life_example_mar25_2024.png)\n\n## Getting Started\n\n### Prerequisites\n- Docker installed on your system or an active Python environment.\n\n### Docker Installation (Recommended)\n1. Follow these instructions to [install Docker](https://docs.docker.com/engine/install/) on your system.\n\n2. Deploy WikidataChat using Docker with the following commands:\n\n```bash\nDOCKER_BUILDKIT=1 docker build . -t wdchat\n\ndocker run  \\\n  --env HUGGING_FACE_HUB_TOKEN=$HUGGING_FACE_HUB_TOKEN \\\n  --env SERAPI_API_KEY=$SERAPI_API_KEY \\\n  --volume \"$(pwd)/wikidatachat\":/workspace/wikidatachat \\\n  --volume wdchat_cache:/root/.cache \\\n  --publish 8000:8000 \\\n  --rm \\\n  --interactive \\\n  --tty \\\n  --name wdchat \\\n  wdchat\n```\n\nThis will deploy the UI to `localhost:8000`, allowing local access to the WikidataChat interface.\n\n## Usage\nAfter installation, access the WikidataChat through your web browser by navigating to `localhost:8000`. If deployed on an internet-accessible server, the interface can be accessed from the respective web address, providing a seamless experience for asking questions and receiving answers.\n\nThe UI and Haystack functionality were developed with colleagues from Wikimedia Deutschland\n- [Haystack Pipeline: rti](https://github.com/rti/gbnc/)\n- [Vue3 UI: andrewtavis](https://github.com/andrewtavis/gbnc)\n\n## Contributing\nWe welcome contributions from the community. Whether it's features, bug fixes, or documentation, here's how you can help:\n1. Fork the repository and create a branch for your contribution. Use descriptive names like `feature/streamlined_rag_pipeline` for features or `bug/localhost_is_blank` for bug fixes.\n2. Submit pull requests with your changes. Make sure your contributions are narrowly defined and clearly described.\n3. Report issues or suggest features using clear and concise titles like `feature_request/include_download_option`.\n\nPlease adhere to the Wikimedia Community Universal Code of Conduct when contributing.\n\n## License\nWikidataChat is open-source software licensed under the MIT License. You are free to use, modify, and distribute the software as you wish. We kindly ask for a citation to this repository if you use WikidataChat in your projects.\n\n## Contact\nFor questions, comments, or discussions, please open an issue on this GitHub repository. We are committed to fostering a welcoming and collaborative community.\n\n![Wikidata and the Meaning of Life](https://github.com/exowanderer/WikidataChat/blob/main/images/WikidataChat_Meaning_of_Life_Graphic.jpg)\n"
    },
    {
      "name": "alanmeeson/sqlite-haystack",
      "stars": 2,
      "img": "https://avatars.githubusercontent.com/u/10676989?s=40&v=4",
      "owner": "alanmeeson",
      "repo_name": "sqlite-haystack",
      "description": "SQLite DocumentStore for Haystack 2 providing both BM25 based FTS and embedding based retrieval.",
      "homepage": null,
      "language": "Python",
      "created_at": "2024-03-24T13:15:35Z",
      "updated_at": "2024-11-25T21:50:24Z",
      "topics": [],
      "readme": "# SQLite Haystack Document store\n[![test](https://github.com/alanmeeson/sqlite-haystack/actions/workflows/test.yml/badge.svg)](https://github.com/alanmeeson/sqlite-haystack/actions/workflows/test.yml)\n\nSQLite-Haystack is an embedded [SQLite](https://sqlite.org) backed Document Store for \n[Haystack 2.X](https://github.com/deepset-ai/haystack/) intended for prototyping or small systems.  \n\nCurrently supported features are:\n- Embedded database for in memory or on disk use.\n- BM25 Free Text Search using [SQLite FTS5](https://sqlite.org/fts5.html)\n- Bring-your-own-embedding vector search using [SQLite](https://github.com/asg017/sqlite-vss); available on Linux only.\n\n## Installation\n\nThe current simplest way to get SQLite-Haystack is to install from GitHub via pip:\n\n```pip install git+https://github.com/alanmeeson/sqlite-haystack.git```\n\n## Usage\n\nSee the example in `examples/pipeline-usage.ipynb`\n\nWarning: make sure you instantiate the retriever before ingesting documents so that it can setup the index.\n\n## Development\n\n### Test\n\nYou can use `hatch` to run the linters:\n\n```console\n~$ hatch run lint:all\ncmd [1] | ruff .\ncmd [2] | black --check --diff .\nAll done! ✨ 🍰 ✨\n6 files would be left unchanged.\ncmd [3] | mypy --install-types --non-interactive src/sqlite_haystack tests\nSuccess: no issues found in 6 source files\n```\n\nSimilar for running the tests:\n\n```console\n~$ hatch run cov\ncmd [1] | coverage run -m pytest tests\n...\n```\n\n### Build\n\nTo build the package you can use `hatch`:\n\n```console\n~$ hatch build\n[sdist]\ndist/sqlite_haystack-0.0.1.tar.gz\n\n[wheel]\ndist/sqlite_haystack-0.0.1-py3-none-any.whl\n```\n\n### Roadmap\n\nIn no particular order:\n- **Address the 1GB limit to the vector index size.**\n  \n  This is a limitation of the sqlite-vss module being used for vector search, so see the\n  [issue in that repo](https://github.com/asg017/sqlite-vss/issues/1) for details. It only applies to the embeddings, \n  so as a rough estimate, if using openAI's current largest embedding model text-embedding-3-large you could index \n  approximately 85K Documents.  If using something smaller, like all-MiniLM-L6-v2 you could index approximately 600K \n  Documents.\n\n- **Write more documentation**\n  \n  There's docstrings in the code, but there's an outstanding task to configure pydoc or similar to produce some api\n  docs.  Also, documentation to explain the data model and how it works would be a good idea.\n\n- **Add more tests on the embedding and bm25 retriever functions.**\n  \n  Most of the testing is the standard DocumentStore tests from the Haystack template, with some added tests of the \n  retrievers adapted from the ElasticSearch components.  These don't cover the retriever code as much as I would like,\n  though the example notebook does serve as a simple system test script.\n\n- **Add optional index creation on the metadata fields for faster filtering.**\n  \n  The metadata is stored in a JSON field in the main documents table.  To speed up queries, we could add a method to \n  allow users to create an index for the metadata elements which are commonly filtered on, as covered in \n  [this article](https://sqldocs.org/sqlite/sqlite-json-data/).  For bonus points, we could keep a track of the fields\n  used for filtering, and automatically create indexes for fields that are used over a certain number of times.\n\n- **Make a Vector search that can work without sqlite-vss for use on windows/macos**\n\n  Adapt the vector search that was used in the InMemoryDocumentStore to apply to a set of documents after a filter \n  lookup.  It wouldn't be efficient, but it would do for small projects and prototypes.\n\n- **Make it possible to add a retriever to an existing DocumentStore**\n\n  Presently, it is necessary to add the retrievers to the DocumentStore before ingesting documents so that they can\n  setup the triggers to add the documents to the index.  This is a bit of a pain, so perhaps make either a classmethod \n  to do that,  or make the retrievers to an index repair when they're added if necessary. \n\n## Limitations\n\n- 1GB limit to the vector index size.\n- Embedding search is not available for windows or macos, as sqlite-vss seems to have trouble on those.  If you can\n  install that library on those platforms, it should work though.\n- Retriever has to be setup before documents are ingested so that documents get added to index."
    },
    {
      "name": "longluu/Medical-QA-LLM",
      "stars": 2,
      "img": "https://avatars.githubusercontent.com/u/33553089?s=40&v=4",
      "owner": "longluu",
      "repo_name": "Medical-QA-LLM",
      "description": "Train LLMs on extractive Question-Answering in biomedical domain.",
      "homepage": "",
      "language": "Python",
      "created_at": "2024-02-11T15:43:04Z",
      "updated_at": "2024-11-25T13:56:23Z",
      "topics": [],
      "readme": "Medical-QA-extractive\n==============================\n\nThe purpose of this repositories is to train LLMs on extractive Question-Answering in biomedical domain.\n\n# 1. Model\n## 1.1 Extractive method\n### 1.1.1 GatorTronS\n>Developed by a joint effort between the University of Florida and NVIDIA, GatorTronS is a clinical language model of 345 million parameters, pre-trained using a BERT architecure implemented in the Megatron package (https://github.com/NVIDIA/Megatron-LM). GatorTronS is pre-trained using a dataset consisting of: 22B synthetic clinical words generated by GatorTronGPT (a Megatron GPT-3 model) 6.1B words from PubMed CC0, 2.5B words from WikiText, 0.5B words of de-identified clinical notes from MIMIC-III.\n\nThe model has 345 million params. Details can be found here https://www.nature.com/articles/s41746-023-00958-w#code-availability.\n\n### 1.1.2 Deberta-v3-large-mrqa\n>DeBERTa improves the BERT and RoBERTa models using disentangled attention and enhanced mask decoder. With those two improvements, DeBERTa out perform RoBERTa on a majority of NLU tasks with 80GB training data.\nIn DeBERTa V3, we further improved the efficiency of DeBERTa using ELECTRA-Style pre-training with Gradient Disentangled Embedding Sharing. Compared to DeBERTa, our V3 version significantly improves the model performance on downstream tasks. You can find more technique details about the new model from our paper.\nThe DeBERTa V3 large model comes with 24 layers and a hidden size of 1024. It has 304M backbone parameters with a vocabulary containing 128K tokens which introduces 131M parameters in the Embedding layer. This model was trained using the 160GB data as DeBERTa V2.\n\nThe base model is deberta-v3-large trained on MRQA dataset: https://huggingface.co/VMware/deberta-v3-large-mrqa.\n\n## 1.2 RAG + LLM\nHere we used RAG approach which generally consists of a retriever and and reader. For retriever, we used the embedding `sentence-transformers/multi-qa-mpnet-base-dot-v1`. For reader, we experimented with several LLM including GPT 3.5, Zephyr 7B, Llama 2 chat 7B, Flan T5 large and BioMedLM. This approach is more powerful than the extractive method because it doesn't require inputting the correct document (the retriever will do it from the corpus) and it can synthesize information from various documents.\n\n# 2. Dataset\n## 2.1. COVID-QA\nThis dataset contains 2,019 question/answer pairs annotated by volunteer biomedical experts on scientific articles regarding COVID-19 and other medical issues. The dataset can be found here: https://github.com/deepset-ai/COVID-QA. The preprocessed data can be found here https://huggingface.co/datasets/covid_qa_deepset.\n\n## 2.2 BioASQ\nThis dataset consists of many biomedical tasks. For QA, they have a dataset with question-answer pairs on PubMed articles.\n>Task 9B will use benchmark datasets containing development and test questions, in English, along with gold standard (reference) answers. The benchmark datasets are being constructed by a team of biomedical experts from around Europe.\nThe benchmark datasets contain four types of questions:\nYes/no questions: These are questions that, strictly speaking, require \"yes\" or \"no\" answers, though of course in practice longer answers will often be desirable. For example, \"Do CpG islands colocalise with transcription start sites?\" is a yes/no question.\nFactoid questions: These are questions that, strictly speaking, require a particular entity name (e.g., of a disease, drug, or gene), a number, or a similar short expression as an answer, though again a longer answer may be desirable in practice. For example, \"Which virus is best known as the cause of infectious mononucleosis?\" is a factoid question.\nList questions: These are questions that, strictly speaking, require a list of entity names (e.g., a list of gene names), numbers, or similar short expressions as an answer; again, in practice additional information may be desirable. For example, \"Which are the Raf kinase inhibitors?\" is a list question.\nSummary questions: These are questions that do not belong in any of the previous categories and can only be answered by producing a short text summarizing the most prominent relevant information. For example, \"What is the treatment of infectious mononucleosis?\" is a summary question.\n\nMore details can be found here: http://participants-area.bioasq.org/general_information/Task9b/\n\n# 3. Training setup\n## 3.1 Environment\nHere I use Python version 3.9.2. All the dependencies are listed in requirements.txt.\nCreate, activate virtual env (Windows) and install all packages:\n```\npy -3.9 -m venv venv\nSet-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope CurrentUser\nvenv\\Scripts\\Activate.ps1\npip install -r requirements.txt\n```\nIf you have GPU, you have to install cuda torch as well\n```\npip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu117\n```\n\n## 3.2 Run the code\nAn example to run the training code is\n```\npython src/models/run_qa.py \\\n    --model_name_or_path 'UFNLP/gatortrons' \\\n    --dataset_name 'longluu/covid-qa-split' \\\n    --do_train \\\n    --do_eval\\\n    --per_device_train_batch_size 4 \\\n    --learning_rate 3e-5 \\\n    --num_train_epochs 2 \\\n    --max_seq_length 512 \\\n    --doc_stride 250 \\\n    --max_answer_length 200 \\\n    --output_dir \"/home/ec2-user/SageMaker/Medical-QA-extractive/models/COVID-QA/gatortrons/\" \\\n    --overwrite_output_dir\n```\n\n# 4. Results\nThe fine-tuned models and brief results can be found at my huggingface page https://huggingface.co/longluu.\nYou can also look at the notebooks folder for training and test results.\n\nProject Organization\n------------\n\n    ├── LICENSE\n    ├── Makefile           <- Makefile with commands like `make data` or `make train`\n    ├── README.md          <- The top-level README for developers using this project.\n    ├── data\n    │   ├── external       <- Data from third party sources.\n    │   ├── interim        <- Intermediate data that has been transformed.\n    │   ├── processed      <- The final, canonical data sets for modeling.\n    │   └── raw            <- The original, immutable data dump.\n    │\n    ├── docs               <- A default Sphinx project; see sphinx-doc.org for details\n    │\n    ├── models             <- Trained and serialized models, model predictions, or model summaries\n    │\n    ├── notebooks          <- Jupyter notebooks. Naming convention is a number (for ordering),\n    │                         the creator's initials, and a short `-` delimited description, e.g.\n    │                         `1.0-jqp-initial-data-exploration`.\n    │\n    ├── references         <- Data dictionaries, manuals, and all other explanatory materials.\n    │\n    ├── reports            <- Generated analysis as HTML, PDF, LaTeX, etc.\n    │   └── figures        <- Generated graphics and figures to be used in reporting\n    │\n    ├── requirements.txt   <- The requirements file for reproducing the analysis environment, e.g.\n    │                         generated with `pip freeze > requirements.txt`\n    │\n    ├── setup.py           <- makes project pip installable (pip install -e .) so src can be imported\n    ├── src                <- Source code for use in this project.\n    │   ├── __init__.py    <- Makes src a Python module\n    │   │\n    │   ├── data           <- Scripts to download or generate data\n    │   │   └── make_dataset.py\n    │   │\n    │   │\n    │   ├── models         <- Scripts to train models and then use trained models to make\n    │   │   │                 predictions\n    │   │   ├── predict_model.py\n    │   │   └── train_model.py\n    │   │\n    │   └── visualization  <- Scripts to create exploratory and results oriented visualizations\n    │       └── visualize.py\n\n"
    },
    {
      "name": "sebastiaan-dev/haiki",
      "stars": 2,
      "img": "https://avatars.githubusercontent.com/u/50165934?s=40&v=4",
      "owner": "sebastiaan-dev",
      "repo_name": "haiki",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2024-03-08T19:49:58Z",
      "updated_at": "2024-03-13T15:32:49Z",
      "topics": [],
      "readme": "# Haiki\n\nThis repository contains the AI based knowledgebase. \nBased on research papers stored in a vector database, articles can be generated using predefined templates.\nThese templates define the sections, titles and paragraphs which should be filled using an LLM.\n\n## Infrastructure\n\n- chroma vector database -> selfhosted\n- firestore noSQL database -> google cloud\n- web API -> python fast api\n- frontend -> react\n- llm framework -> [haystack](https://haystack.deepset.ai)\n- datasource sientific papers -> [openalex](https://openalex.org)\n- LLM -> mostly ollama run localy (but interchangable)\n- ollama -> local hosting llm\n\n## Usage\n\napi back end\n\n```sh\nuvicorn main:app --reload\n```\n\nfrontend\n\n```sh\nbun run dev\n```\n\n## Templates\n\nTemplates can be created using the appropriate endpoint, they follow\nthe structure outlined in 'pipelines/sample_template.json'.\n"
    },
    {
      "name": "xxxsergxxx/Social_Networks_Comments_Prioritization_System",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/39562636?s=40&v=4",
      "owner": "xxxsergxxx",
      "repo_name": "Social_Networks_Comments_Prioritization_System",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2025-04-09T12:56:25Z",
      "updated_at": "2025-04-09T14:41:57Z",
      "topics": [],
      "readme": "<p align=\"center\">\n  <img src=\"banner.png\" alt=\"Project Banner\" width=\"800\"/>\n</p>\n\n# 🧠 Intelligent Text Classifier & Conversational AI Assistant\n\nAn advanced NLP project for **text preprocessing**, **classification**, and **interactive chat** powered by **LangChain** and **OpenAI GPT-3.5**.\n\n---\n\n## 🚀 Features\n\n✅ **Multi-step Text Classification**:\n- `emoji` — detects if input contains only emojis\n- `keyword` — identifies keywords using Haystack + XLM-RoBERTa\n- `spam` — flags spam content using a trained Naive Bayes classifier\n- `normal` — routes normal input to the chatbot for a meaningful reply\n\n✅ **Emoji Handling**:\n- Converts emojis to descriptive text (`❤️` → `red_heart`)\n- Optionally removes emojis for cleaner processing\n\n✅ **Conversational AI with Context**:\n- Powered by LangChain and OpenAI GPT-3.5\n- Uses vector search via **ChromaDB** for contextualized answers\n- Maintains **chat history** for better, smarter replies\n\n---\n\n## 🛠️ Project Structure\n\nsrc/ ├── data_preprocessing/ │ └── remove_emoji.py # Emoji removal and conversion ├── \nkeywords_classifier/ │ └── detect_keywords.py # Keyword detection with Haystack ├── \nspam_classifier/ │ └── spam_classifier.py # Spam detection with scikit-learn ├── \nlangchain/ │ └── my_langchain_history.py # LangChain history-aware QA └── main.py # Entry point for classification + chat\n\n---\n\n## 📦 Requirements\n\n### 🐍 Python 3.10+\n\nInstall via `environment.yml` (recommended):\n```bash\nconda env create -f environment.yml\nconda activate py310\n```\nOr using pip:\n```\npip install -r requirements.txt\n```\n💬 Usage\n1. Run the Classifier\n```\npython main.py\n```\n2. Example Output\nYou: ❤️🔥😂\nClassified as: emoji\n\nIf not emoji/keyword/spam, a chat will start:\n\nYou: what is Alice in Wonderland about?\nAssistant: It's a fantasy novel about a girl named Alice who falls into a magical world...\n\n🧠 Chroma DB Setup\n\nTo load your own knowledge base into the chatbot:\n\n    Add .txt files into the data/ directory\n\n    Run:\n    ```\n    python create_chroma_db.py\n    ```\n🧪 Model Details\n\n    Spam Classifier: CountVectorizer + MultinomialNB (trained model: pipe_spam_cls_model.pkl)\n\n    Keyword Detector: Haystack TransformersQueryClassifier with ukr-models/xlm-roberta-base-uk\n\n    LLM: OpenAI gpt-3.5-turbo via LangChain\n\n    Vector DB: ChromaDB with local persistence\n\n🔐 Environment Variables\n\nSet your OpenAI API Key:\n```\nexport OPENAI_API_KEY=your_openai_key_here\n```\n\nOr use a .env file + python-dotenv.\n✨ TODO / Improvements\n\n    ✅ Add emoji only detection logic\n\n    ⚠ Improve CLI user experience (add options & flags)\n\n    📊 Add Streamlit or web UI interface\n\n    🧪 Add unit tests\n\n👨‍💻 Author\n\nDeveloped by Serhii Kolotukhin  and Ihor Boichuk\n📍 www.linkedin.com/in/serhii-kolotuhkin-25648a166 \n\nPowered by open-source LLM tools\n📄 License\n\nMIT License — free to use, modify, and distribute.\n\n"
    },
    {
      "name": "Blacksujit/Data-thon-2025",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/148805811?s=40&v=4",
      "owner": "Blacksujit",
      "repo_name": "Data-thon-2025",
      "description": "Builded an Scalable ML Model  for the E-commerce category and to determine  the item accuracy for the local area stores ",
      "homepage": "",
      "language": "Jupyter Notebook",
      "created_at": "2025-03-13T19:01:53Z",
      "updated_at": "2025-04-03T15:12:54Z",
      "topics": [
        "2025",
        "deep-learning",
        "e-commerce",
        "e-commerce-project",
        "linear-regression",
        "logistic-regression",
        "machine-learning",
        "ml",
        "ml-2025",
        "ml-competition",
        "model",
        "winning",
        "xgboost-regression"
      ],
      "readme": "# 🏆 Hackathon Project - Datathon 2025\r\n\r\n\r\n\r\n## 📌 Problem Statement\r\n\r\nParticipants were given three datasets: train, test, and labels. The goal was to:\r\n\r\nPredict item accuracy for a store.\r\n\r\nGenerate a complete, correctly formatted data file as required by the organizers.\r\n\r\nMaximize accuracy while adhering to constraints like model training time and kernel stability.\r\n\r\nThe challenge wasn’t just building a model — it was about handling data efficiently, iterating rapidly, and overcoming system limitations.\r\n\r\n## Folder structure:\r\n\r\n```\r\n├── datasets/\r\n│   ├── train.csv\r\n│   ├── test.csv\r\n│   └── labels.csv\r\n├── notebooks/\r\n│   ├── datathonr2starter.ipynb\r\n│   ├── model_training.ipynb\r\n│   └── workingnotebook.ipynb\r\n├── models/\r\n│   └── final_model.pkl\r\n├── Submissions/\r\n│   └── submission.csv\r\n├── requirements.txt\r\n└── README.md\r\n\r\n```\r\n \r\n\r\n## 🚀 Our Solution\r\n\r\nWe built a machine learning pipeline that tackled the problem through multiple iterations, gradually improving accuracy and system performance.\r\n\r\n**Model Architecture & Approach:**\r\n\r\n![Model Arcitecture](image.png)\r\n\r\n**Initial Experiments:**\r\n\r\n🫴 Started with a basic model yielding 0.45 accuracy.\r\n\r\n🫴 Refined with feature engineering, boosting accuracy to 0.54.\r\n\r\n**Core ML Techniques:**\r\n\r\n🫴 Tested various algorithms (Random Forest, XGBoost, etc.).\r\n\r\n🫴 Eventually settled on a combination of TF-IDF Vectorization and Logistic Regression, achieving a peak accuracy of 0.79%.\r\n\r\n**Optimization & Stability:**\r\n\r\n🫴 Reduced training time to 2.86 minutes.\r\n\r\n🫴 Debugged kernel crashes and optimized memory usage to ensure model stability during final submissions.\r\n\r\n**Continuous Iteration:**\r\n\r\n🫴 Persistent testing and failure analysis.\r\n\r\n🫴 Balanced accuracy and runtime to meet competition constraints.\r\n\r\n## 🚀 Our Journey\r\n\r\nDespite facing initial setbacks — including a team member in the hospital — we persevered through sleepless nights, technical hurdles, and constant model crashes. Through relentless iteration, we pushed our accuracy from 0.45 to 0.79, landing us in 2nd place on the leaderboard.\r\n\r\n## 🛠️ Tech Stack & Tools\r\n\r\n-> Python for data processing and model development\r\n\r\n-> scikit-learn for machine learning algorithms\r\n\r\n-> **TF-IDF and Logistic Regression** for feature extraction and classification\r\n\r\n-> Jupyter Notebooks for experimentation  [Google colab](https://colab.research.google.com/) and [Kaggle Environments](https://www.kaggle.com/code) for actual model developement and testing \r\n\r\n## Datasets:\r\n\r\nFind the datasets under Datasets folder \r\n\r\n## ⚡ Setup Instructions\r\n\r\n**Clone the repository:**\r\n\r\n```\r\ngit clone https://github.com/Blacksujit/datathon-2025.git\r\n\r\n```\r\n\r\n```\r\npython -m venv venv\r\n\r\n```\r\n\r\n```\r\nsource venv/bin/activate\r\n\r\n```\r\n\r\n```\r\npip install -r requirements.txt\r\n\r\n```\r\n\r\n**Run the training script:**\r\n\r\n```\r\ncd Code Notebooks\r\n\r\n```\r\n\r\n\r\nrun/[generate_predictions.py](notebook_1.ipynb/working-Submission.ipynb)\r\n\r\n## 📊 Results & Achievements\r\n\r\n1.) Initial accuracy:**0.45 (45%)**\r\n\r\n2.) Final accuracy: **0.79 (79%)**\r\n\r\n**Training time: 2.86 minutes**\r\n\r\nOur model ranked 2nd Runner-Up out of hundreds of teams, showcasing the power of persistence and continuous learning.\r\n\r\n## 💡 Key Takeaways\r\n\r\n**Experimentation is crucial:**\r\n\r\n✨ Trying out multiple techniques like hyperparameter tuning and feature engineering led to significant improvements.\r\n\r\n**Resilience wins:**\r\n\r\n✨ Even when our kernel crashed repeatedly, we stayed determined and found ways to optimize and stabilize the model.\r\n\r\n## 🏁 Conclusion\r\n\r\nWinning this prize was a validation of our skills, teamwork, and the power of believing in ourselves. We hope this repository helps others learn from our approach and inspires more developers to dive into competitive data science!\r\n\r\n**\"Fail, learn, adapt, and come back stronger. The win is waiting.\"**\r\n\r\n\r\n## 📚 License\r\n\r\nMIT License\r\n\r\n \r\n"
    },
    {
      "name": "Erwin2307-py/Paper",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/197731009?s=40&v=4",
      "owner": "Erwin2307-py",
      "repo_name": "Paper",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2025-02-20T13:14:21Z",
      "updated_at": "2025-04-04T08:40:39Z",
      "topics": [],
      "readme": "# Paper"
    },
    {
      "name": "MisioMusic08/Semisizer",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/129396003?s=40&v=4",
      "owner": "MisioMusic08",
      "repo_name": "Semisizer",
      "description": "A Very powerful summarization tool that processes and extracts key insights from content using Llama 2, Haystack, Whisper, and Streamlit. Runs smoothly on CPU with Llama",
      "homepage": "",
      "language": "Python",
      "created_at": "2025-03-02T18:33:41Z",
      "updated_at": "2025-03-02T18:52:35Z",
      "topics": [
        "ai",
        "haystack",
        "llama2",
        "machinelearning",
        "nlp",
        "streamlit",
        "summarization",
        "whisper"
      ],
      "readme": "# Semisizer\nSemisizer is a powerful summarization app built using open-source LLMs and frameworks like Llama 2, Haystack, Whisper, and Streamlit. It efficiently processes and summarizes content while running smoothly on a CPU, thanks to the GGUF format of the Llama 2 model, loaded through Llama.\n"
    },
    {
      "name": "artefactory/redis-player-one",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/86767100?s=40&v=4",
      "owner": "artefactory",
      "repo_name": "redis-player-one",
      "description": "Vector-based document retrieval demo with the arXiv paper dataset using Redis as the vector database.",
      "homepage": "https://docsearch.redisventures.com",
      "language": "Python",
      "created_at": "2022-10-26T16:36:56Z",
      "updated_at": "2024-02-21T17:04:07Z",
      "topics": [],
      "readme": "\n# **Ask' Yves**\n<div align=\"center\">\n    <a href=\"https://pd-redis-redisplay-ea87d28003b046a8a3617ebdbddfdbe3.community.saturnenterprise.io/\"><img src=\"assets/askyves.png\" width=\"50%\"><img></a>\n    <br />\n    <br />\n<div display=\"inline-block\">\n    <a href=\"https://hackathon.redisventures.com/\"><b>Hackathon Page</b></a>&nbsp;&nbsp;&nbsp;\n    <a href=\"https://www.kaggle.com/datasets/Cornell-University/arxiv\"><b>ArXiv dataset</b></a>&nbsp;&nbsp;&nbsp;\n    <a href=\"https://pd-redis-redisplay-ea87d28003b046a8a3617ebdbddfdbe3.community.saturnenterprise.io/\"><b>Hosted App</b></a>&nbsp;&nbsp;&nbsp;\n  </div>\n    <br />\n    <br />\n</div>\n\n\nYves Saint Laurent was one of the greatest minds of french history. He spent a lot of time reading scientific papers on arXiv.\n\nAsk him anything. He will have an answer. Probably not the right one, but you might be surprised 😅\n\n\n# **How to use the app**\n![Ask'Yves app interface](assets/app_interface.png)\n\nAsk'Yves app allows you to ask questions to Yves, who will search for an answer in abstracts of the ArXiv database. Whenever he has found something, Yves will display a set of abstracts ranked by relevance, and highlight the answer to your question in the abstract text.\n\nTo ask a question to Yves, just fill the text prompt window on the left sidebar, select a range of publication dates to retrieve abstracts from, and click on \"Ask Yves\". The results will then be displayed along with information regarding the abstract: \n- Relevance score\n- Publication date\n- Categories\n\nYou can then access the article on arXiv by directly clicking on the article's title.\n\n# **How to setup the app**\n\n## **File architecture**\n```\n.\n├── LICENSE\n├── Makefile\n├── README.md\n├── askyves\n│   ├── embedder.py\n│   └── redis_document_store.py\n├── assets\n│   ├── app_interface.png\n│   ├── askyves.png\n│   └── categories.py\n├── config.py\n├── credentials\n│   ├── env.sh.example\n├── data\n│   ├── README.md\n│   ├── build_embeddings_multi_gpu.ipynb\n│   ├── load_data_in_redis.py\n│   └── requirements.txt\n├── frontend\n│   ├── lib\n│   │   ├── __init__.py\n│   │   ├── app_utils.py\n│   │   └── query_utils.py\n│   └── streamlit_app.py\n├── pyproject.toml\n├── requirements.in\n├── requirements.txt\n└── saturn-deployment-recipe.json\n```\n\nProject is divided into multiple folders:\n- askyves/ contains files related to document embedding and document store definition\n- data/ contains files related to Redis Database creation\n- frontend/ contains files related to the Streamlit App\n\nTo setup the app, you'll first need to create a Redis DB containing your embedded documents. You'll then be able to launch and use the app, locally or on a Saturn Cloud instance.\n\n## **Data and database**\n\n### Get data from Kaggle\n\n- You can download the Arxiv dataset from here:\n    - https://www.kaggle.com/datasets/Cornell-University/arxiv\n\n- (Recommended solution) If you have a Kaggle account:\n    - go to your account and create an API key\n    - put the created key in `~/.kaggle/kaggle.json`\n    - then use the CLI to download the dataset:\n        - `kaggle datasets download -d Cornell-University/arxiv`\n\n\n### Build embeddings\n\n- Run through the `build_embeddings_multi_gpu.ipynb` notebook on Saturn Cloud (Jupyter server + Dask Cluster) to build the embeddings.\n\n### Load data to Redis\n\n- Once the embeddings are built:\n    - You'll need to export env variables. Check `credentials/env.sh.exemple` to have the list    \n    - Then, run `python data/load_data_in_redis.py`\n\n## **Run the app locally**\n\n**1. Setup python environment:**\n\nRun the following command to create a virtual environment and install all the requirements:\n```bash\nmake env\nmake install_requirements\n```\n\n**2. Fill credentials in environment:**\n\nYou'll need to export the following variables in your environment. Check `credentials/env.sh.exemple` to have the list of variables to fill:\n```bash\nexport REDIS_HOST=\"redis_host_url\"\nexport REDIS_PORT=\"1234\"\nexport REDIS_DB=\"your-redis-database-name\"\nexport REDIS_PASSWORD=\"redis-db-password\"\n```\n\n**3. Run the app locally:**\nTo run the app in your local environment, just run the following command:\n```bash\nmake run_app\n```\n\nIt will open a Streamlit window on your web-browser.\n\n## **Run the app on a Saturn Cloud Deployment instance**\n\nYou can easily create a deployment instance to run your app in Saturn Cloud by copying the recipe stored in the file `saturn-deployment-recipe.json` at the root of the project. Here are the instructions to create your own instance:\n\n1. First, you'll need to parametrize your credentials in Saturn Cloud so your instance can access them. Go to \"Secrets\" > \"New\", and create a secret for the 4 credentials variables you exported earlier.\n\n2. Then, go to \"Resources\" > \"New Deployment\" > \"Use a Recipe\", and paste the content of `saturn-deployment-recipe.json` in the open window. A deployment instance will be created with app parameters.\n\n3. Finally, you'll need to add the credentials that are necessary to run your app. After creating the instance, select it in \"Resources\", then go to \"Secrets\" > \"Attach Secret Environment Variable\", and select in the dropdown menu the secrets you defined in step 1. Be sure to assign the corresponding environment variable names to them.\n\n4. Now you're ready to go! Click on \"Overview\" > \"Start\", and once the app is running, you can access it by clicking on the provided public URL.\n\nNotes: \n- The deployment instance will directly pull the main branch of this repo to run the app with, but you can modify the branch it pulls by modifying it in \"Git Repos\" section. \n- You may need to link a Saturn SSH key the first time you run the app, the instructions to do so will then be displayed on Saturn directly. You will just have to add Saturn SSH Key to your GitHub profile.\n\n## **About the Question Answering pipeline**\n\nThe goal of this app is to ease information retrieval on research papers by allowing users to ask questions to the app in a natural language and get answers from papers. To do that, we found this [kaggle article](https://www.kaggle.com/code/officialshivanandroy/question-answering-with-arxiv-papers-at-scale) that we got inspired from.\n\nBasically when the user asks a question to the app, here is the following process in backend:\n- 1. It queries the Redis Vector Search database.\n- 2. Redis Vector Search DB returns the abstracts that are the closest in terms of vector similarity.\n- 3. Then, it leverages a question answering (QA) algorithm to extract the answers to the user query.\n\nWe relied on the [haystack](https://haystack.deepset.ai/) framework to do QA at scale. For the sake of our usage there are 3 `haystack` components to understand:\n\n* **Document Store**: Database storing the documents for our search. There are a lot of options already provided by `haystack` such as Elasticsearch, Faiss, OpenSearch, In-Memory, SQL ... We decided to create the `RedisDocumentStore` class to be able to benefit from the `haystack` framework while using the `Redis` database.\n* **Retriever**: Fast, simple algorithm that identifies candidate passages from a large collection of documents. Algorithms include TF-IDF or BM25, EmbeddingRetriever... We chose an `EmbeddingRetriever` with the same embedding model that was used to feed the `Redis` database.\n* **Reader**: the reader takes multiple texts as input and returns top-n answers with corresponding confidence scores. You can just load a pretrained model from Hugging Face's model hub or fine-tune it to your own domain data. We used the suggested model `sentence-transformers/all-mpnet-base-v2` as it is the state of the art model provided in the [haystack benchmark](https://haystack.deepset.ai/benchmarks) and that the first results looked pretty good for a first baseline. If we had more time we would have benched other models and tried a fine tuned model to our own dataset.\n\n## **Next steps**\n\nThe app was designed in a limited amount of time, and there are obviously many improvements to be made, and features to explore. \n\nHere is a non-exhaustive snapshot of some ideas we have:\n\n### Improving current features\n- We used a generic embedding model to embed the abstracts. It may be relevant to try fine-tuned models on arXiv data to see if it improves  similarity search performances\n- Same for QA model, trying other models may improve question answering performances\n- We currently retrieve 10 documents when the app run on CPU, and 100 if the app run on GPU. It can be a relativelly low number of text to answer difficult questions. It may be interesting to have an adaptative number of retrieved documents depending on the quantity of answers found for a particular question\n\n### Adding new features\n- Integrating the redis database uploading process in the  `RedisDocumentStore` as it is done by `haystack` for [other document stores objects](https://github.com/deepset-ai/haystack/tree/main/haystack/document_stores)\n- Adding a generative QA pipeline to give a single answer to the question at the beginning and keep the extractive one to illustrate the answer with examples\n- Adding a time series representation of the papers on which the answer is found to give a visual timeline of those papers\n- Splitting the deployment of the front streamlit app and the back ML pipeline > Host the app on a basic CPU instance and deploy the ML pipeline as an endpoint on a GPU instance to respect the single responsibility principle and better manage costs\n\n## Interested in contributing?\nThis is a new project. Comment on an open issue or create a new one. We can triage it from there.\n\n## Additional links\n- [ArXiv Kaggle Dataset](https://www.kaggle.com/datasets/Cornell-University/arxiv)\n- [Hackathon Page](https://hackathon.redisventures.com/)\n- [MLOps Community Slack channel](https://join.slack.com/t/mlops-community/shared_invite/zt-1cjmjku5d-ZhJitSlS0VtqfCcwRpn_CQ)\n- [Redis](https://redis.io/)\n- [Saturn Cloud](https://saturncloud.io/)\n- [Kaggle Question & Answering with ArXiV papers at scale](https://www.kaggle.com/code/officialshivanandroy/question-answering-with-arxiv-papers-at-scale)\n- [haystack](https://haystack.deepset.ai/)\n"
    },
    {
      "name": "shs1018/iM_DiGital_Banker_academy",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/90491841?s=40&v=4",
      "owner": "shs1018",
      "repo_name": "iM_DiGital_Banker_academy",
      "description": null,
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2024-09-23T05:57:33Z",
      "updated_at": "2025-03-25T00:37:30Z",
      "topics": [],
      "readme": "<img src=\"https://capsule-render.vercel.app/api?type=waving&color=BDBDC8&height=150&section=header\" />\n\n<p align = 'center'>\n    <img src = https://github.com/user-attachments/assets/13e3af51-3835-4450-8dbc-92b222268e4c>\n</p>\n\n\n\n<br>\n\n---\n\n\n# iMBank DiGital Banker academy\n\n\n---\n<br><br><br>\n\n\n\n## Description\n<br>\n\n- iM DiGital Banker academy 과정 중 작성한 코드 정리 및 공부 자료 정리.\n<br>\n\n\n---\n\n\n# 설명\n<br>\n\n- project_docu\n<br>\n\n- projects\n<br>\n\n- Python_basics\n<br>\n\n- SQL\n<br>\n\n- step03-Streamlit\n<br>\n\n- Task_documentqtion\n<br>\n\n---\n<br>\n\n# 시각화 자료\n- 작업중...\n\n<p align = 'center'>\n    <img src = https://github.com/user-attachments/assets/b42ee76f-c0cd-4291-b870-266b5a663885>\n</p>\n\n\n<img src=\"https://capsule-render.vercel.app/api?type=waving&color=BDBDC8&height=150&section=footer\" />"
    },
    {
      "name": "sukeshrs/machine-learning-utils",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/9681508?s=40&v=4",
      "owner": "sukeshrs",
      "repo_name": "machine-learning-utils",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2023-11-12T13:24:20Z",
      "updated_at": "2025-03-23T23:54:16Z",
      "topics": [],
      "readme": "# Project Setup\n\nThis project sets up the necessary environment and installs all required libraries to build and test out various machine learning and GenAI examples.\n\n---\n\n## **Prerequisites**\n\n- Python 3.8 or later installed on your system.\n- `pip` (Python package installer) should be installed and updated.\n\n---\n\n## **Step 1: Create a Virtual Environment**\n\nCreating a virtual environment ensures that the project's dependencies are isolated from your system's Python environment.\n\n1. Open a terminal or command prompt.\n2. Run the following commands:\n\n   **On Linux/MacOS:**\n   ```bash\n   python3 -m venv venv\n   source venv/bin/activate```\n\n   **On Windows:**\n    python -m venv venv\n    venv\\Scripts\\activate\n\n## **Step 2: Install required libraries**\n\npip install -r requirements.txt\n\n### verify the installation\n\npip list\n\n"
    },
    {
      "name": "tianyi-gu/phillipian",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/61768110?s=40&v=4",
      "owner": "tianyi-gu",
      "repo_name": "phillipian",
      "description": null,
      "homepage": null,
      "language": "JavaScript",
      "created_at": "2024-08-31T03:03:57Z",
      "updated_at": "2025-04-03T19:43:57Z",
      "topics": [],
      "readme": "# Phillipian Mobile App\n\nWelcome to _The Phillipian_ vol. CXLVII's Mobile App v 1.0\n\ndeveloped and maintained by Tianyi Gu, Managing Editor, CXLVII\n\nRecently released on the iOS App Store, available [here](https://apps.apple.com/us/app/phillipian/id6739430112)!\n\nAccess _The Phillipian's_ online website [here](https://phillipian.net)!\n\n\n# Running Locally\n\n## Get Started\n\nInstall dev dependencies:\n\n### `npm install`\n\n### For iOS Development\n\n1. Install Cocoapods if you haven't already:\n   ```bash\n   sudo gem install cocoapods\n   ```\n\n2. Generate native iOS project files:\n   ```bash\n   npx expo prebuild\n   ```\n\n3. Open the iOS project in Xcode:\n   ```bash\n   open ios/*.xcworkspace\n   ```\n\n## Run The App\n\n### `npm start`\n\nRuns your app in development mode.\n\nInstall the [Expo app](https://expo.io) on your phone and scan the generated QR code.\n\n### `npm run ios/npm run android`\n\nOpens app on iOS or Android Simulator (installation needed)\n\n## Features\n\n### Article Summarization\nThe app now includes an AI-powered article summarization feature:\n- Click the \"Generate Summary\" button at the bottom of any article\n- View a concise summary in a modal panel\n- Close the summary with the X button to return to the article\n\n### Backend Setup\nThe summarization feature requires a Flask backend server:\n1. Install Python dependencies:\n   ```bash\n   pip install -r requirements.txt\n   ```\n\n2. Run the Flask server:\n   ```bash\n   python server/app.py\n   ```\n\nThe server runs on port 5001 and provides the `/api/summarize` endpoint for article summarization.\n\n### Configuration\n- Update `src/config/api.ts` with your local server IP address\n- Default port is 5001\n- Make sure your device and server are on the same network\n\n## Development Notes\n- The app uses React Native with Expo\n- Backend uses FastAPI\n- Article summaries are generated in real-time\n- Dark mode support throughout the app\n\ntradeoffs between models size/ speed/ accuracy:\nSummary model: \n- Facebook/bart-large-cnn, \n\nQuestion answering model:\n- Production: deepset/deberta-v3-base-squad2\n- Using Haystack framework with:\n  - BM25Retriever for document retrieval\n  - TransformersReader for answer extraction\n- Optimized with custom BM25 parameters (b=0.75, k1=1.2)\n\nTranslation models:\n- Using Marian Neural Machine Translation (Helsinki-NLP)\n"
    },
    {
      "name": "NI3singh/AI-Compilance-Checker",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/115877777?s=40&v=4",
      "owner": "NI3singh",
      "repo_name": "AI-Compilance-Checker",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2024-12-24T05:45:43Z",
      "updated_at": "2025-03-03T11:05:07Z",
      "topics": [],
      "readme": "# Compliance Checker\n\nThe **Compliance Checker** is an AI-driven application designed to analyze and verify contract compliance by performing vector-based similarity searches. It leverages PostgreSQL with **pgvector** for efficient vector searches and document embeddings, providing a robust, open-source solution to manage both structured and unstructured data.\n\n---\n\n## **Key Features**\n- **Efficient Vector Search**: Perform faster similarity queries with advanced indexing techniques.\n- **Unified Database**: Manage relational and vector data in one system seamlessly.\n- **AI-Enhanced Analysis**: Utilize OpenAI models for generating and comparing document embeddings.\n- **Open Source**: Transparent and cost-effective for all use cases.\n\n---\n\n## **Prerequisites**\nEnsure the following dependencies are installed before proceeding:\n- **Docker**: For containerized deployment.\n- **Python 3.10**: For running the application backend.\n- **PostgreSQL**: For managing and storing document vectors.\n- **OpenAI API Key**: For embedding generation using OpenAI models.\n- **PostgreSQL GUI Client** (optional, e.g., TablePlus): To interact with the database.\n\n---\n\n## **Setup Instructions**\n\n### **1. Clone the Repository**\nClone the project to your local machine:\n```bash\ngit clone https://github.com/Ishita0211/compliance-checker.git\ncd compliance-checker\n"
    },
    {
      "name": "NI3singh/IITGN-and-Odoo-Hackathon",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/115877777?s=40&v=4",
      "owner": "NI3singh",
      "repo_name": "IITGN-and-Odoo-Hackathon",
      "description": null,
      "homepage": null,
      "language": "JavaScript",
      "created_at": "2024-11-13T08:58:26Z",
      "updated_at": "2024-11-22T08:50:07Z",
      "topics": [],
      "readme": "# IITGN-and-Odoo-Hackathon"
    },
    {
      "name": "Nevy11/therapistGpt",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/125488481?s=40&v=4",
      "owner": "Nevy11",
      "repo_name": "therapistGpt",
      "description": "A set of GPTs hosted on Django rest framework",
      "homepage": "",
      "language": "Python",
      "created_at": "2024-11-05T15:42:42Z",
      "updated_at": "2024-12-18T07:32:43Z",
      "topics": [],
      "readme": "Django Rest api Backend\nTo start it type 'python manage.py runserver'\n\nthen to download the missing dependancies: \"pip install -r requirements.txt\"\n\nThe Therapist is there for you to use via the rest apis\n"
    },
    {
      "name": "YashBaravaliya/MedGuide-AI",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/110822197?s=40&v=4",
      "owner": "YashBaravaliya",
      "repo_name": "MedGuide-AI",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2024-11-03T04:20:08Z",
      "updated_at": "2025-01-12T11:11:51Z",
      "topics": [],
      "readme": "# MedGuide AI Dashboard\n\nMedGuide AI is a **Streamlit-based application** integrating multiple healthcare modules to provide a versatile tool for medical information retrieval, molecular analysis, research support, and physiotherapy planning. It leverages **advanced language models**, **PubMed integration**, and a custom **FAISS index** for optimized medicine suggestions, including **Ayurvedic alternatives**.\n\n---\n\n## **1. Installation Guide**\n\nFollow these steps to set up and run the MedGuide AI application:\n\n1. **Clone the Repository**  \n   ```bash\n   git clone https://github.com/YashBaravaliya/MedGuide-AI\n   cd MedGuide-AI\n   ```\n\n2. **Create a Virtual Environment**  \n   ```bash\n   python -m venv venv\n   source venv/bin/activate  # For Linux/Mac\n   venv\\Scripts\\activate     # For Windows\n   ```\n\n3. **Install Dependencies**  \n   ```bash\n   pip install -r requirements.txt\n   ```\n\n4. **Run the Application**  \n   ```bash\n   streamlit run app.py\n   ```\n\n5. **Access the Dashboard**  \n   Open your browser and navigate to the URL displayed in the terminal (usually `http://localhost:8501`).\n\n\n## **Overview**\n\n![Dashboard Screenshot](img/ss/Dashboard.png)\n\nThe MedGuide AI dashboard includes the following key features:\n\n- **MedGuide Chat**: Provides reliable answers to medical queries using PubMed research articles and suggests Ayurvedic alternatives where available.\n- **Molecule Generation**: Visualizes molecular structures for pharmaceutical research.\n- **Research Support**: Retrieves top scientific articles from PubMed related to healthcare topics.\n- **Physio Planner**: Offers custom physiotherapy plans tailored to users' health conditions.\n- **MediScan**: Allows users to upload a medicine image and receive detailed information about it.\n\n---\n\n## **1. MedGuide Chat**\n\nMedGuide Chat serves as an interactive healthcare assistant, offering comprehensive information about medications, symptoms, and related topics. It integrates **PubMed resources** and language models to deliver accurate responses.\n\n### **Features**\n\n#### **1.1 Interactive Chatbot**\n\n- **Conversational Interface**: Users can type medical questions and receive detailed answers.\n- **Medicine-Specific Information**: Provides data about:\n  - Uses\n  - Composition\n  - Side effects\n  - Manufacturer\n- **Alternative Options**: Suggests alternative medicines with accompanying images.\n- **Downloadable Reports**: Generates PDF reports containing medication details.\n\n#### **1.2 Medication Information Panel**\n\n![Chat Screenshot 1](img/ss/medGuideChat1.png)  \n![Chat Screenshot 2](img/ss/medGuideChat2.png)\n\n- **Primary Medicine Details**: Comprehensive insights into the queried medication, including usage, side effects, and more.\n- **Alternative Medicines**: Displays options for similar medications, empowering users with choices.\n\n#### **1.3 Healthcare Source Search**\n\n![Chat Screenshot 3](img/ss/medGuideChat3.png)\n\n- **HealthcareSearchAgent**: Fetches URLs and summaries from relevant medical articles for further reading and verification.\n\n---\n\n## **2. Molecule Generation**\n\nA feature aimed at pharmaceutical research and drug discovery, allowing users to visualize molecular structures.\n\n![Molecule Screenshot](img/ss/molecule.png)\n\n- **SMILES Notation Input**: Converts molecular names into visual chemical structures.\n- **Application in Research**: Facilitates the design and understanding of new pharmaceutical compounds.\n- **Powered by**: \n  - **Groq API** for accelerated computations.\n  - **RDKit** for chemical structure visualization.\n\n![Molecule Screenshot](img/ss/molecule2.png)\n---\n\n## **3. Research Support**\n\nThis module integrates with PubMed to fetch top scientific articles relevant to healthcare topics.\n\n- **Efficient Article Retrieval**: Extracts and summarizes research findings using the PubMed API.\n- **Research Validation**: Supports evidence-based practices by providing access to verified medical studies.\n- **User Benefits**: A valuable resource for medical professionals, researchers, and students.\n\n\n![Research Screenshot](img/ss/Research.png)\n---\n\n## **4. Physio Planner**\n\nAn intelligent physiotherapy planning system tailored to user-specific health conditions.\n\n![Physio Screenshot](img/ss/Physio.png)\n![Physio Screenshot](img/ss/Physio2.png)\n\n- **Custom Recommendations**: Generates exercise and rehabilitation plans based on user inputs.\n- **Dynamic Adjustments**: Incorporates feedback to refine plans over time.\n- **User-Friendly Design**: Helps users improve their physical health with personalized guidance.\n\n![Physio Screenshot](img/ss/Physio3.png)\n\n---\n\n## **5. MediScan**\n\nMediScan allows users to upload images of medicines and retrieve detailed information, such as:\n\n- Name, composition, and manufacturer.\n- Usage instructions and possible side effects.\n- Ayurvedic or alternative medicine options.\n\n---\n\n## **Technology Stack**\n\n### **Frontend**\n- **Streamlit**: Provides an intuitive and responsive web interface.\n\n### **Backend**\n\n- **Python**: Handles backend logic, including data processing and API calls.\n\n### **Utilities**\n- **RDKit**: For managing chemical structures in molecular generation.\n- **PubMed API**: Fetches scientific research articles.\n- **Custom Libraries**:\n  - `pubmed_utils`: For retrieving PubMed articles.\n  - `medicine_utils`: Searches for medication details and alternatives.\n  - `pdf_utils`: Generates downloadable PDF reports.\n  - `llm_chain`: Powers the chatbot responses using language models.\n\n---\n\nMedGuide AI provides a holistic healthcare solution, blending advanced AI technologies with practical healthcare applications to serve patients, professionals, and researchers alike.\n"
    },
    {
      "name": "NI3singh/Stock_Price_Forcasting",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/115877777?s=40&v=4",
      "owner": "NI3singh",
      "repo_name": "Stock_Price_Forcasting",
      "description": null,
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2024-10-25T12:22:20Z",
      "updated_at": "2024-11-22T08:47:59Z",
      "topics": [],
      "readme": "# Time Series Forecasting: Multi-Framework Comparison\n\n## Project Overview\nThis project implements time series forecasting using four different frameworks (TensorFlow, PyTorch, Statsmodels, and Keras) to compare their performance and accuracy on different datasets. The project focuses on stock price prediction and general time series analysis.\n\n## Frameworks and Datasets Used\n\n| Framework   | Model Type | Dataset                | Purpose                    |\n|------------|------------|------------------------|----------------------------|\n| TensorFlow | LSTM       | Melbourne Temperature  | Temperature Prediction     |\n| PyTorch    | LSTM       | Airline Passengers    | Passenger Count Forecasting|\n| Statsmodels| SARIMAX    | ACGL Stock Price      | Stock Price Prediction     |\n| Keras      | LSTM       | Microsoft Stock Price  | Stock Price Prediction     |\n\n## Key Features\n- Implementation of different time series forecasting models\n- Comparative analysis of model accuracies\n- Interactive web interface for SARIMAX model\n- Visualization of predictions vs actual values\n- Performance metrics calculation (MAE, RMSE, MAPE)\n\n## Installation\n\n```bash\n# Clone the repository\ngit clone https://github.com/yourusername/time-series-forecasting.git\n\n# Create virtual environment\npython -m venv venv\n\n# Activate virtual environment\n# For Windows\nvenv\\Scripts\\activate\n# For Unix/MacOS\nsource venv/bin/activate\n\n# Install required packages\npip install -r requirements.txt\n```\n\n## Project Structure\n```\n├── data/\n│   ├── ACGL_DATA.csv\n│   ├── MSFT_stock.csv\n│   └── temperature.csv\n├── models/\n│   ├── tensorflow_model.py\n│   ├── pytorch_model.py\n│   ├── statsmodels_model.py\n│   └── keras_model.py\n├── web_app/\n│   ├── app.py\n│   ├── templates/\n│   └── static/\n├── requirements.txt\n└── README.md\n```\n\n## Model Performance Comparison\n\n### Accuracy Metrics\n\n| Framework   | MAE    | RMSE   | MAPE   |\n|------------|--------|--------|--------|\n| TensorFlow | 0.235  | 0.312  | 2.45%  |\n| PyTorch    | 0.242  | 0.328  | 2.67%  |\n| Statsmodels| 0.198  | 0.287  | 2.12%  |\n| Keras      | 0.228  | 0.305  | 2.38%  |\n\n## Web Application\nThe project includes a Flask web application for the SARIMAX model that allows users to:\n- Input prediction timeframe\n- View forecasted values\n- Analyze prediction accuracy\n- Visualize results through interactive plots\n\n## Running the Web Application\n```bash\ncd web_app\npython app.py\n```\nAccess the application at `http://localhost:5000`\n\n## Technologies Used\n- Python 3.8+\n- TensorFlow 2.x\n- PyTorch 1.9+\n- Statsmodels 0.12+\n- Keras\n- Flask\n- Pandas\n- NumPy\n- Matplotlib\n- Scikit-learn\n\n## Future Improvements\n- [ ] Add real-time data fetching\n- [ ] Implement ensemble methods\n- [ ] Add more visualization options\n- [ ] Include hyperparameter tuning\n- [ ] Add cross-validation\n"
    },
    {
      "name": "AayushSalvi/Finance-Chatbot-using-Mistral-7B",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/65705434?s=40&v=4",
      "owner": "AayushSalvi",
      "repo_name": "Finance-Chatbot-using-Mistral-7B",
      "description": "Mistral Chatbot using weaviate vector store and haystack ",
      "homepage": null,
      "language": "Python",
      "created_at": "2024-10-24T18:36:35Z",
      "updated_at": "2024-10-29T09:11:16Z",
      "topics": [],
      "readme": ""
    },
    {
      "name": "NI3singh/FINAL_YEAR_PROJECT-1",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/115877777?s=40&v=4",
      "owner": "NI3singh",
      "repo_name": "FINAL_YEAR_PROJECT-1",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2024-10-05T16:31:04Z",
      "updated_at": "2024-10-25T12:21:25Z",
      "topics": [],
      "readme": "# FINAL_YEAR_PROJECT-1: Object Detection and Distance Measurement\r\n\r\nThis project uses **YOLOv5** for real-time object detection and distance estimation from video or webcam feeds. Bounding boxes, class labels, and distances are displayed on the screen, with optional audio output for detected objects.\r\n\r\n## Features\r\n- Real-time object detection using **YOLOv5**\r\n- Distance estimation for detected objects\r\n- Bounding box and class label visualization\r\n- Supports video files or webcam input\r\n\r\n## Installation\r\n1. Clone the repository:\r\n   ```bash\r\n   git clone https://github.com/NI3singh/FINAL_YEAR_PROJECT-1.git\r\n   ```\r\n2. Install Dependencies:\r\n   ```bash\r\n   pip install -r requirements.txt\r\n   ```\r\n3. go to project directory using cd\r\n4. now run the command:\r\n   ```bash\r\n   python main.py\r\n   ```\r\n"
    },
    {
      "name": "otaviosoaresp/rag_bot_ollama",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/38701225?s=40&v=4",
      "owner": "otaviosoaresp",
      "repo_name": "rag_bot_ollama",
      "description": "A Chatbot that use a local LLM through ollama and a vector search with Qdrant to find and return relevant response from text, PDF, CSV and XLSX files.",
      "homepage": "",
      "language": "Python",
      "created_at": "2024-09-25T05:16:54Z",
      "updated_at": "2024-10-25T09:35:59Z",
      "topics": [],
      "readme": "\n# Retrieval-Augmented Generation (RAG) Chatbot Project\n\nThis project implements a chatbot using **Retrieval-Augmented Generation (RAG)** techniques, capable of answering questions based on documents loaded from a specific folder (e.g., `/cerebro`). The chatbot uses a local language model via **Ollama** and vector search through **Qdrant** to find and return relevant responses from text, PDF, CSV, and XLSX files.\n\n## Features\n\n- Document ingestion from the `/cerebro` folder containing `.txt`, `.pdf`, `.csv`, and `.xlsx` files.\n- Indexing of documents into the **Qdrant** vector database.\n- Response generation using **local language models** with **Ollama API**.\n- Search and retrieve relevant information based on questions.\n- Supports multiple language models such as `llama3.1:8b-instruct-q8_0`, `codellama`, and others.\n\n---\n\n## Table of Contents\n\n1. [Requirements](#requirements)\n2. [Installation](#installation)\n3. [Project Setup](#project-setup)\n4. [Running the Chatbot](#running-the-chatbot)\n5. [Fine-tuning the Model](#fine-tuning-the-model)\n6. [Troubleshooting](#troubleshooting)\n\n---\n\n## Requirements\n\n- **Python 3.10+**\n- **Qdrant** running locally or via Docker\n- **Ollama API** installed and configured with local models available\n- Libraries for handling various document types (e.g., `pdfplumber`, `openpyxl`)\n  \nMake sure you have sufficient hardware resources if working with large models such as `llama` or `codellama`.\n\n---\n\n## Installation\n\n### 1. Clone the repository\n\nFirst, clone this repository to your local machine:\n\n```bash\ngit clone git@github.com:otaviosoaresp/rag_bot_ollama.git\ncd rag_bot_ollama\n```\n\n### 2. Create a virtual environment\n\nCreate and activate a virtual environment to manage dependencies:\n\n```bash\n# For Linux/macOS\npython3 -m venv .venv\nsource .venv/bin/activate\n\n# For Windows\npython -m venv .venv\n.venv\\Scripts\\activate\n```\n\n### 3. Install the required dependencies\n\nInstall the required Python libraries from `requirements.txt`:\n\n```bash\npip install -r requirements.txt\n```\n\nIn case you encounter errors with `grpcio-tools`, use the following command:\n\n```bash\npip install grpcio-tools --no-binary=grpcio-tools\n```\n\nMake sure all dependencies like `qdrant-client`, `ollama-api`, `pdfplumber`, `sentence-transformers`, and `openpyxl` are installed correctly.\n\n### 4. Set up Qdrant\n\nEnsure **Qdrant** is running locally. If you don’t have it installed, you can quickly run it using Docker:\n\n```bash\ndocker run -p 6333:6333 qdrant/qdrant\n```\n\nThis will start Qdrant and make it accessible at `http://localhost:6333`.\n\n---\n\n## Project Setup\n\n### 1. Configure the Models\n\nEnsure the models you plan to use are available locally and configured in the `config.py` file. For example, to use `llama3.1:8b-instruct-q8_0`, ensure that the configuration looks like this:\n\n```python\nMODELS = {\n    \"primary_model\": \"llama3.1:8b-instruct-q8_0\",\n    \"embedding_model\": \"nomic-embed-text:latest\"\n}\n```\n\n### 2. Data Loading\n\nPlace the files (e.g., `.txt`, `.pdf`, `.csv`, `.xlsx`) you want to load into the folder `/cerebro`. The project will read all files from this directory, process them, and index the contents into Qdrant for vector search.\n\n---\n\n## Running the Chatbot\n\n1. **Run the Main Script**:\n   After everything is set up, you can run the chatbot with:\n\n   ```bash\n   python main.py\n   ```\n\n   This script will:\n   - Load the specified model (e.g., `llama3.1:8b-instruct-q8_0`).\n   - Process the documents in `/cerebro` and index them into Qdrant.\n   - Allow the user to input questions via the console.\n\n2. **Example**:\n   ```plaintext\n   Loading language model and embedding model...\n   Loading model: llama3.1:8b-instruct-q8_0\n   Processing documents and indexing into Qdrant...\n   Documents loaded: 3 documents.\n\n   Enter your question (or 'exit' to stop): What is rural credit?\n   Processing the question: What is rural credit?\n   Chatbot Response: Rural credit is a form of financing aimed at supporting agricultural production through credit for investment, working capital, and other agricultural operations.\n   ```\n\n3. **Exit**:\n   To exit, type `'exit'` in the prompt.\n\n---\n\n## Troubleshooting\n\n### Common Issues and Fixes\n\n#### 1. **`grpcio-tools` installation issues**:\n   - Use the following command to avoid issues:\n     ```bash\n     pip install grpcio-tools --no-binary=grpcio-tools\n     ```\n\n#### 2. **Missing Models**:\n   - Ensure the model names in `config.py` match the ones you have installed locally.\n\n#### 3. **Qdrant 404 or 400 errors**:\n   - If you receive an error like `404 Not Found: Collection doesn't exist!`, ensure that your documents are correctly indexed, and Qdrant is running.\n\n#### 4. **No or Inaccurate Responses**:\n   - Make sure the documents were processed and indexed correctly.\n   - Verify that the model and Qdrant are properly configured.\n\n\n\n\n---\n\n"
    },
    {
      "name": "rohithekm/AI-ChatBot---opensourse",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/160238864?s=40&v=4",
      "owner": "rohithekm",
      "repo_name": "AI-ChatBot---opensourse",
      "description": "Fully opensouce Ai chat assistance. Uses groq api key , haystack and chainlit.",
      "homepage": null,
      "language": "Python",
      "created_at": "2024-09-13T03:36:14Z",
      "updated_at": "2024-11-18T11:12:53Z",
      "topics": [],
      "readme": "# AI-ChatBot---opensourse\nFully opensouce Ai chat assistance. Uses groq api key , haystack and chainlit.\n"
    },
    {
      "name": "vhidvz/question-answering",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/54132701?s=40&v=4",
      "owner": "vhidvz",
      "repo_name": "question-answering",
      "description": "Q/A microservice powered by a GPT-2 model, expertly fine-tuned for Persian-language contexts",
      "homepage": "https://vhidvz.github.io/question-answering/",
      "language": "Python",
      "created_at": "2024-09-09T08:45:07Z",
      "updated_at": "2024-12-20T09:19:10Z",
      "topics": [
        "ai",
        "fastapi",
        "gpt-2",
        "haystack",
        "huggingface",
        "persian-nlp",
        "question-answering"
      ],
      "readme": "# Quick Start\n\nQ/A microservice powered by a GPT-2 model, expertly fine-tuned for Persian-language contexts. This solution delivers accurate, context-aware responses, tailored specifically to the nuances of Persian dialogue and communication.\n\n```sh\ngit clone git@github.com:vhidvz/question-answering.git\ncd question-answering && docker-compose up -d\n```\n\n**Docker Hub:**\n\n```sh\ndocker run -p 8000:8000 vhidvz/question-answering:latest\n```\n\nEndpoints are fully documented using OpenAPI Specification 3 (OAS3) at:\n\n- ReDoc: <http://localhost:8000/redoc>\n- Swagger: <http://localhost:8000/docs>\n\n> Note: To enable in-memory document storage, simply remove the `ELASTICSEARCH_*` environment variables.\n\n## Documentation\n\nTo generate the documentation for the python model, execute the following command:\n\n```sh\npdoc --output-dir docs model.py\n```\n"
    },
    {
      "name": "tandangwang/ai-powered-customer-service-system",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/43236091?s=40&v=4",
      "owner": "tandangwang",
      "repo_name": "ai-powered-customer-service-system",
      "description": "An AI-powered customer service system that integrates conversational AI, retrieval-augmented generation (RAG), and persistent storage to provide intelligent, context-aware responses to user queries while maintaining conversation history.",
      "homepage": null,
      "language": "Python",
      "created_at": "2024-09-08T15:42:16Z",
      "updated_at": "2025-03-12T10:58:54Z",
      "topics": [],
      "readme": "# AI-Powered Customer Service System\n\n## Project Overview\n\nThis project implements an AI-powered customer service system that integrates conversational AI, retrieval-augmented generation (RAG), and persistent storage. It's designed to provide intelligent, context-aware responses to user queries while maintaining conversation history.\n\n## Modules\n\n### 1. Conversation Handler (langchain_module.py)\n\nThe core module that orchestrates the entire conversation flow.\n\nKey features:\n- Integrates MongoDB for conversation history storage\n- Utilizes RAGFlow for retrieval-augmented generation\n- Implements LangChain for language model interaction\n- Manages user sessions and conversation context\n\nTechnologies used:\n- LangChain\n- Ollama (for language model)\n- ChatPromptTemplate for structured prompts\n\n### 2. RAGFlow Client (ragflow_module.py)\n\nHandles interactions with the RAGFlow service for enhanced response generation.\n\nKey features:\n\n\\- Creates and manages conversation sessions\n\n\\- Retrieves conversation history\n\n\\- Generates completions based on user input and context\n\n\\- Manages documents in the knowledge base\n\nTechnologies used:\n\n- RESTful API interactions\n- JSON data handling\n\n**Knowledge Base Setup** \n\nBefore using the RAGFlow Client, you need to create several knowledge bases within RAGFlow. The following test data sets are provided for this purpose, and all data is prepared in files located under the `test_data` directory:\n\n\\- `01_production_information`\n\n\\- `02_promotion_information`\n\n\\- `03_known_solutions`\n\n\\- `04_soft_skills`\n\nYou can categorize relevant information into these different knowledge bases as needed. Please note that all data is virtually created for testing purposes, so you may disregard any inconsistencies in the content.\n\n**Key Additions:**\n\nMentioned that the data files are located in the test_data directory.\n\nMaintained clarity and professionalism in the language.\n\nFeel free to modify any part of it to better fit your project's needs!\n\n### 3. MongoDB Manager (mongodb_module.py)\n\nManages persistent storage of conversation history.\n\nKey features:\n- Inserts new conversation records\n- Retrieves user conversation history with customizable limits\n\nTechnologies used:\n- PyMongo for MongoDB interactions\n- Datetime handling for conversation timestamps\n\n## System Flow\n\n1. User input is received and processed by the Conversation Handler.\n2. User information is extracted, and a new conversation is initialized if necessary.\n3. The RAGFlow client is used to retrieve relevant information based on the user's query.\n4. The Conversation Handler generates a response using the language model, incorporating RAG results and conversation history.\n5. The response is stored in MongoDB for future reference.\n6. The system continues this loop for ongoing conversations.\n\n## Setup and Usage\n\nTo set up and run the AI-Powered Customer Service System, follow these steps:\n\n1. **Clone the Repository**  \n   Clone the repository to your local machine using the following command:\n   ```bash\n   git clone <repository-url>\n   ```\n\n2. **Navigate to the Project Directory**  \n   Change to the project directory:\n   ```bash\n   cd /path/to/this/repo\n   ```\n\n3. **Create a Virtual Environment**  \n   Use Conda to create a virtual environment named `aics`:\n   ```bash\n   conda create -n aics python=3.10\n   ```\n\n4. **Activate the Virtual Environment**  \n   Activate the newly created virtual environment:\n   ```bash\n   conda activate aics\n   ```\n\n5. **Install Dependencies**  \n   Install the required packages using pip:\n   ```bash\n   pip install -r requirements.txt\n   ```\n\n6. **Run the Application**  \n   Start the application by running the main module:\n   ```bash\n   python langchain_module.py\n   ```\n\n### Note\n\nMake sure you have Conda installed on your system. If you encounter any issues, please refer to the documentation for the respective tools.\n\n## Dependencies\n\n- LangChain\n- Ollama\n- PyMongo\n- Requests\n\n## Future Improvements\n\n- Implement more advanced RAG techniques\n- Enhance error handling and logging\n- Add support for multiple language models\n- Implement user authentication and authorization\n\n## Contributing\n\nWe welcome contributions to the AI-Powered Customer Service System! If you would like to contribute, please follow these guidelines:\n\n1. **Fork the Repository**  \n   Create a fork of the repository on GitHub.\n\n2. **Clone Your Fork**  \n   Clone your forked repository to your local machine:\n   ```bash\n   git clone <your-fork-url>\n   ```\n\n3. **Create a New Branch**  \n   Create a new branch for your feature or bug fix:\n   ```bash\n   git checkout -b feature/your-feature-name\n   ```\n\n4. **Make Your Changes**  \n   Implement your changes and ensure that your code adheres to the project's coding standards.\n\n5. **Test Your Changes**  \n   Run tests to ensure that your changes do not break any existing functionality.\n\n6. **Commit Your Changes**  \n   Commit your changes with a descriptive message:\n   ```bash\n   git commit -m \"Add a brief description of your changes\"\n   ```\n\n7. **Push to Your Fork**  \n   Push your changes to your forked repository:\n   ```bash\n   git push origin feature/your-feature-name\n   ```\n\n8. **Create a Pull Request**  \n   Go to the original repository and create a pull request from your forked repository. Provide a clear description of your changes and why they should be merged.\n\n### Code of Conduct\nPlease adhere to the [Code of Conduct](link-to-code-of-conduct) in all interactions within the project.\n\nThank you for your contributions!\n\n## License\n\nThis project is licensed under the MIT License. You are free to use, modify, and distribute this software, including for commercial purposes, under the terms of the license.\n\n### MIT License\n"
    },
    {
      "name": "opscidia/science-checker",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/57664197?s=40&v=4",
      "owner": "opscidia",
      "repo_name": "science-checker",
      "description": "The present project, called Opscidia Science Checker, aims at developing a tool to verify scientific claims by analyzing the pertinent and available scientific literature. The subject of fake news is a very topical one. With social networks, and the advances of artificial intelligence, it is easier and easier to create fake news, and they circulate quicker and quicker. Health is a particularly nasty topic for fake news. Scam medicine, and worrisome information circulate, often based on absolutely no scientific evidence.. The main idea of this project is to build several indicators based on the analysis of very large volumes of scientific articles. These indicators will be easy to understand in order for the non-specialist to have a quick idea of whether an information is backed by the scientific literature, is under debate, or is totally groundless.   The tool developed will be of use for journalists and media groups as well as for the general public. The idea actually emerged after a discussion with a scientific journalist who conducts long investigations on cases of possible fake medicine. The use of tools such as ours would be very useful to help them target the topics that deserve investigation, and would give a starting point for their work. We have further discussed this topic with several other journalists that all showed a very strong interest for the development of such a product. ",
      "homepage": null,
      "language": "JavaScript",
      "created_at": "2020-06-24T09:36:50Z",
      "updated_at": "2024-09-24T18:02:56Z",
      "topics": [],
      "readme": "<!-- @copyright  Copyright (c) 2018-2024 Opscidia -->\n# Science Checker\n\n<div align=\"center\">\n\n[![Python](https://img.shields.io/badge/Python-3.11-3776AB?style=for-the-badge&logo=Python&logoColor=f1c40f)](https://www.python.org/downloads/)\n[![Docker](https://img.shields.io/badge/docker-engine-2496ED?style=for-the-badge&logo=docker)](https://www.djangoproject.com/download/)\n\n</div>\n\nCode implementation of [Science Checker Reloaded](https://arxiv.org/abs/2402.13897).\n\nTo access the [Extractive-Boolean QA For Scientific Fact Checking](https://doi.org/10.1145/3512732.3533580) code implementation, please refer to the [v1.0 branch](https://github.com/opscidia/science-checker/releases/tag/v1.0).\n\n\n## Installation\nInstall Docker and Docker Compose if you haven't already.  \nThen, run the following commands to build the Docker image.\n\n```sh\ndocker-compose build\n```\n\nDownload the entity-fishing models.\n```sh\nwget -qO- https://science-miner.s3.amazonaws.com/entity-fishing/0.0.6/db-kb.zip | bsdtar -C src/entityfish/models/ -xvf-\nwget -qO- https://science-miner.s3.amazonaws.com/entity-fishing/0.0.6/db-en.zip | bsdtar -C src/entityfish/models/ -xvf-\n```\n\nRun the following command to start the Docker container.\n```sh\ndocker-compose up\n```\n\n## Citations\nIf you find this code useful, please consider citing our work.\n```bibtex\n@misc{rakotoson2024science,\n      title={Science Checker Reloaded: A Bidirectional Paradigm for Transparency and Logical Reasoning}, \n      author={Loïc Rakotoson and Sylvain Massip and Fréjus A. A. Laleye},\n      year={2024},\n      eprint={2402.13897},\n      archivePrefix={arXiv},\n      primaryClass={cs.IR}\n}\n```\nIf you use the Extractive-Boolean QA For Scientific Fact Checking code implementation, please consider citing the following work.\n```bibtex\n@inproceedings{Rakotoson_2022,\n    series={ICMR ’22},\n    title={Extractive-Boolean Question Answering for Scientific Fact Checking},\n    url={http://dx.doi.org/10.1145/3512732.3533580},\n    DOI={10.1145/3512732.3533580},\n    booktitle={Proceedings of the 1st International Workshop on Multimedia AI against Disinformation},\n    publisher={ACM},\n    author={Rakotoson, Loïc and Letaillieur, Charles and Massip, Sylvain and Laleye, Fréjus A. A.},\n    year={2022},\n    month=jun,\n    collection={ICMR ’22}\n}\n```\n\n"
    },
    {
      "name": "alecrimi/chainlit_mistral_nonlocal",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/16406658?s=40&v=4",
      "owner": "alecrimi",
      "repo_name": "chainlit_mistral_nonlocal",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2024-08-17T23:03:19Z",
      "updated_at": "2025-02-27T16:26:46Z",
      "topics": [],
      "readme": "# chainlit_mistral_nonlocal\n\nSimple scripts to deploy a complete LLM application, using ChainLit and pretrained Mistral model (non-local).\n\nThe assumption is that the script is running on localhost but the porting to AWS is quite straightforward\n\nDepending on the script I am using either just Haystack to orchestrate the RAG but only temporarily, or the full vector storing using also Weaviate.\nThe old script using Bootstrap.js has been replaced by Chainlit, but the architecture is the same:\n\n![Architecture](https://github.com/alecrimi/chainlit_mistral_nonlocal/blob/main/architectureLLM.png?raw=true \"Architecture\")\n \n\n"
    },
    {
      "name": "5ud21/Weaviate-Document-Store",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/88098618?s=40&v=4",
      "owner": "5ud21",
      "repo_name": "Weaviate-Document-Store",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2024-08-18T10:24:10Z",
      "updated_at": "2024-09-10T13:17:37Z",
      "topics": [],
      "readme": "# Weaviate Document Store with FastAPI and Custom Llama Model\n\nThis repository contains a setup for a Weaviate document store, a FastAPI application for querying the document store, and a custom Llama model for generating detailed answers. The setup includes Docker Compose for container orchestration.\n\n## Table of Contents\n\n- [Introduction](#introduction)\n- [Prerequisites](#prerequisites)\n- [Setup Instructions](#setup-instructions)\n- [Configuration Details](#configuration-details)\n- [Running the Application](#running-the-application)\n- [Stopping the Application](#stopping-the-application)\n- [Ingesting Documents](#ingesting-documents)\n- [Querying the Document Store](#querying-the-document-store)\n- [Troubleshooting](#troubleshooting)\n- [Contributing](#contributing)\n- [License](#license)\n\n## Introduction\n\nThis project sets up a Weaviate document store with a FastAPI application for querying and generating answers using a custom Llama model. The setup includes Docker Compose for container orchestration and various Python scripts for document ingestion and querying.\n\n## Prerequisites\n\nBefore you begin, ensure you have the following installed on your machine:\n\n- [Docker](https://www.docker.com/get-started)\n- [Docker Compose](https://docs.docker.com/compose/install/)\n- [Python 3.8+](https://www.python.org/downloads/)\n\n## Setup Instructions\n\n1. **Clone the repository**:\n    ```sh\n    git clone https://github.com/5ud21/Weaviate-Document-Store.git\n    cd Weaviate-Document-Store/\n    ```\n\n2. **Start the Docker Compose setup**:\n    ```sh\n    docker-compose up -d\n    ```\n\n3. **Install Python dependencies**:\n    ```sh\n    pip install -r requirements.txt\n    ```\n\n## Configuration Details\n\nThe `docker-compose.yml` file defines the configuration for the Weaviate service. Below are the key configuration options:\n\n- **Version**: Specifies the version of Docker Compose syntax being used (`3.4`).\n- **Services**: Defines a single service named `weaviate`.\n  - **Command**: Specifies the command-line arguments for the Weaviate service:\n    - `--host`: Sets the host to `0.0.0.0`.\n    - `--port`: Sets the port to `8080`.\n    - `--scheme`: Sets the scheme to `http`.\n  - **Image**: Uses the `semitechnologies/weaviate:1.21.2` Docker image.\n  - **Ports**: Maps port `8080` on the host to port `8080` in the container.\n  - **Volumes**: Mounts a Docker volume named `weaviate_data` to `/var/lib/weaviate` in the container.\n  - **Restart Policy**: Configured to restart on failure (`on-failure:0`).\n  - **Environment Variables**:\n    - `QUERY_DEFAULTS_LIMIT`: Sets the default query limit to `25`.\n    - `AUTHENTICATION_ANONYMOUS_ACCESS_ENABLED`: Enables anonymous access (`true`).\n    - `PERSISTENCE_DATA_PATH`: Sets the data path to `/var/lib/weaviate`.\n    - `DEFAULT_VECTORIZER_MODULE`: Sets the default vectorizer module to `none`.\n    - `ENABLE_MODULES`: Enables various modules for vectorization and generation.\n    - `CLUSTER_HOSTNAME`: Sets the cluster hostname to `node1`.\n\n## Running the Application\n\nTo start the Weaviate service, run the following command:\n\n```sh\ndocker-compose up -d\n```\nThis command will start the service in detached mode.\n\n## Stopping the Application\n\nTo stop the Weaviate service, run the following command:\n\n```sh\ndocker-compose down\n```\nThis command will stop and remove the containers defined in the `docker-compose.yml` file.\n\n## Ingesting documents\n\nTo ingest documents into the Weaviate document store, run the `ingest.py` script.\n\n```sh\npython ingest.py\n```\nThis script performs the following steps:\n\n-> Converts PDF documents into text.<br>\n-> Preprocesses the text documents.<br>\n-> Writes the preprocessed documents into the Weaviate document store.<br>\n-> Updates the document embeddings using a specified embedding model.\n\n## Querying the Document Store\n\nTo query the document store using the FastAPI application, run the `app.py` script:\n\n```sh\nuvicorn app:app --host 0.0.0.0 --port 8001 --reload\n```\n\nThis will start the FastAPI application on http://localhost:8001. You can use the /get_answer endpoint to query the document store and generate answers using the custom Llama model.\n\n## Troubleshooting\n\n### Common Issues\n\n- **Port Conflicts**: Ensure that port `8080` is not being used by another service on your host machine.\n- **Docker Daemon**: Ensure that the Docker daemon is running.\n\n### Logs\n\nTo view the logs of the Weaviate service, run:\n\n```sh\ndocker-compose logs weaviate\n```\n\n## License\n\nThis project is licensed under the MIT License. See the LICENSE file for more details. \n"
    },
    {
      "name": "adityaraj1105/RAGs-vs-Fine-tuning",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/119999602?s=40&v=4",
      "owner": "adityaraj1105",
      "repo_name": "RAGs-vs-Fine-tuning",
      "description": "This project explores the performance of Retrieval-Augmented Generation (RAG) and fine-tuned GPT-2 models on the Stanford Question Answering Dataset (SQuAD). The goal is to compare the results of RAG and GPT-2 in answering questions based on the provided context.",
      "homepage": "",
      "language": "Jupyter Notebook",
      "created_at": "2024-08-08T23:29:50Z",
      "updated_at": "2024-09-20T08:49:01Z",
      "topics": [],
      "readme": "# RAG vs GPT-2 Fine-Tuning\n\n## Project Overview\n\nThis project explores the performance of Retrieval-Augmented Generation (RAG) and fine-tuned GPT-2 models on the Stanford Question Answering Dataset (SQuAD). The goal is to compare the results of RAG and GPT-2 in answering questions based on the provided context.\n\n## Code Structure\n\n- `RAGs_vs_Fine-tuning.ipynb`: Contains the code for fine-tuning the GPT-2 model on the SQuAD dataset and setting up the RAG pipeline and then performing comparisons between RAG and GPT-2.\n\n\n## Data\n\n- The project uses the Stanford Question Answering Dataset (SQuAD) from Hugging Face's `datasets` library.\nhttps://huggingface.co/datasets/rajpurkar/squad\n\n## Installation\n\nTo set up the project environment, install the required packages listed in `requirements.txt`.\n\n```bash\npip install -r requirements.txt\n"
    },
    {
      "name": "Progressive-Insurance/chainlit",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/33731436?s=40&v=4",
      "owner": "Progressive-Insurance",
      "repo_name": "chainlit",
      "description": "Build Conversational AI in minutes ⚡️",
      "homepage": "https://oso-web-edv-oso-prod.prod.glb.pgrcloud.app/contributions/a522ff9b-4714-4d31-a306-c70fd553c35e",
      "language": "TypeScript",
      "created_at": "2024-08-16T14:13:44Z",
      "updated_at": "2025-04-18T03:00:32Z",
      "topics": [],
      "readme": "<h1 align=\"center\">Welcome to Chainlit by Literal AI 👋</h1>\n\n<p align=\"center\">\n<b>Build python production-ready conversational AI applications in minutes, not weeks ⚡️</b>\n\n</p>\n<p align=\"center\">\n    <a href=\"https://discord.gg/k73SQ3FyUh\" rel=\"nofollow\"><img alt=\"Discord\" src=\"https://dcbadge.vercel.app/api/server/ZThrUxbAYw?style=flat\" style=\"max-width:100%;\"></a>\n    <a href=\"https://twitter.com/chainlit_io\" rel=\"nofollow\"><img alt=\"Twitter\" src=\"https://img.shields.io/twitter/url/https/twitter.com/chainlit_io.svg?style=social&label=Follow%20%40chainlit_io\" style=\"max-width:100%;\"></a>\n    <a href=\"https://pypistats.org/packages/chainlit\" rel=\"nofollow\"><img alt=\"Downloads\" src=\"https://img.shields.io/pypi/dm/chainlit\" style=\"max-width:100%;\"></a>\n        <a href=\"https://github.com/chainlit/chainlit/graphs/contributors\" rel=\"nofollow\"><img alt=\"Contributors\" src=\"https://img.shields.io/github/contributors/chainlit/chainlit\" style=\"max-width:100%;\"></a>\n    <a href=\"https://github.com/Chainlit/chainlit/actions/workflows/ci.yaml\" rel=\"nofollow\"><img alt=\"CI\" src=\"https://github.com/Chainlit/chainlit/actions/workflows/ci.yaml/badge.svg\" style=\"max-width:100%;\"></a>\n</p>\n\n<p align=\"center\">\n    <a href=\"https://chainlit.io\"><b>Website</b></a>  •  \n    <a href=\"https://docs.chainlit.io\"><b>Documentation</b></a>  •  \n    <a href=\"https://help.chainlit.io\"><b>Chainlit Help</b></a>  •  \n    <a href=\"https://github.com/Chainlit/cookbook\"><b>Cookbook</b></a>\n</p>\n\n<p align=\"center\">\n    <a href=\"https://trendshift.io/repositories/6708\" target=\"_blank\"><img src=\"https://trendshift.io/api/badge/repositories/6708\" alt=\"Chainlit%2Fchainlit | Trendshift\" style=\"width: 250px; height: 45px;\" width=\"250\" height=\"45\"/></a>\n</p>\n\nhttps://github.com/user-attachments/assets/b3738aba-55c0-42fa-ac00-6efd1ee0d148\n\n> [!NOTE]\n> Chainlit is maintained by [Literal AI](https://literalai.com), an LLMOps platform to monitor and evaluate LLM applications! It works with any Python or TypeScript applications and [seamlessly](https://docs.chainlit.io/llmops/literalai) with Chainlit. For enterprise support, please fill this [form](https://docs.google.com/forms/d/e/1FAIpQLSdPVGqfuaWSC2DfunR6cY4C7kUHl0c2W7DnhzsF9bmMxrVpkg/viewform?usp=header).\n\n## Installation\n\nOpen a terminal and run:\n\n```sh\npip install chainlit\nchainlit hello\n```\n\nIf this opens the `hello app` in your browser, you're all set!\n\n### Development version\n\nThe latest in-development version can be installed straight from GitHub with:\n\n```sh\npip install git+https://github.com/Chainlit/chainlit.git#subdirectory=backend/\n```\n\n(Requires Node and pnpm installed on the system.)\n\n## 🚀 Quickstart\n\n### 🐍 Pure Python\n\nCreate a new file `demo.py` with the following code:\n\n```python\nimport chainlit as cl\n\n\n@cl.step(type=\"tool\")\nasync def tool():\n    # Fake tool\n    await cl.sleep(2)\n    return \"Response from the tool!\"\n\n\n@cl.on_message  # this function will be called every time a user inputs a message in the UI\nasync def main(message: cl.Message):\n    \"\"\"\n    This function is called every time a user inputs a message in the UI.\n    It sends back an intermediate response from the tool, followed by the final answer.\n\n    Args:\n        message: The user's message.\n\n    Returns:\n        None.\n    \"\"\"\n\n\n    # Call the tool\n    tool_res = await tool()\n\n    await cl.Message(content=tool_res).send()\n```\n\nNow run it!\n\n```sh\nchainlit run demo.py -w\n```\n\n<img src=\"/images/quick-start.png\" alt=\"Quick Start\"></img>\n\n## 📚 More Examples - Cookbook\n\nYou can find various examples of Chainlit apps [here](https://github.com/Chainlit/cookbook) that leverage tools and services such as OpenAI, Anthropiс, LangChain, LlamaIndex, ChromaDB, Pinecone and more.\n\nTell us what you would like to see added in Chainlit using the Github issues or on [Discord](https://discord.gg/k73SQ3FyUh).\n\n## 💁 Contributing\n\nAs an open-source initiative in a rapidly evolving domain, we welcome contributions, be it through the addition of new features or the improvement of documentation.\n\nFor detailed information on how to contribute, see [here](/CONTRIBUTING.md).\n\n## 📃 License\n\nChainlit is open-source and licensed under the [Apache 2.0](LICENSE) license.\n"
    },
    {
      "name": "DJIMYDAMESSE/Transformers",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/101936118?s=40&v=4",
      "owner": "DJIMYDAMESSE",
      "repo_name": "Transformers",
      "description": "Algorithme et fonctionnement des Transformers",
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2024-08-14T20:32:10Z",
      "updated_at": "2024-08-15T15:11:46Z",
      "topics": [],
      "readme": "# Transformers\nAlgorithme et fonctionnement des Transformers\n"
    },
    {
      "name": "knaig/stitch",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/2597693?s=40&v=4",
      "owner": "knaig",
      "repo_name": "stitch",
      "description": null,
      "homepage": null,
      "language": "HTML",
      "created_at": "2024-08-07T08:11:20Z",
      "updated_at": "2024-08-23T09:38:42Z",
      "topics": [],
      "readme": ""
    },
    {
      "name": "joybratasarkar/Ai-inteview",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/72306307?s=40&v=4",
      "owner": "joybratasarkar",
      "repo_name": "Ai-inteview",
      "description": "Hi folks, this is the AI Interviewer. Please check out this project, and feel free to offer any suggestions for improvements. Your feedback is welcome!",
      "homepage": "",
      "language": "Python",
      "created_at": "2024-07-30T20:23:13Z",
      "updated_at": "2024-10-20T18:14:20Z",
      "topics": [
        "ai",
        "ai-interview",
        "fastapi",
        "gpt-4",
        "langchain",
        "llm",
        "python"
      ],
      "readme": "# AI Interview\n\nWelcome to the AI Interview project! This application leverages AI technology to conduct interviews and answer general questions.\n\n## Prerequisites\n\nTo run this application, ensure you have the following:\n\n1. **Python 3**: Make sure Python 3 is installed on your system.\n\n## Running the Application\n\n### On Unix-based Systems (Linux/macOS)\n\n1. Open your terminal.\n2. Navigate to the project directory.\n3. Make the script executable (if not already):\n\n   ```bash\n   chmod +x run\n\n\n### Notes:\n- **`requirements.txt`**: Make sure you have a `requirements.txt` file in your project directory that lists all the dependencies for your project.\n- **Batch File for Windows**: If you don't have a `run.bat` file, you can create one that includes the commands to run your application on Windows.\n\nFeel free to adjust the README to fit your specific project details!\n\n\n\n"
    },
    {
      "name": "ai4africagroup/telcom_llm",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/171547595?s=40&v=4",
      "owner": "ai4africagroup",
      "repo_name": "telcom_llm",
      "description": null,
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2024-07-29T17:03:09Z",
      "updated_at": "2025-02-18T19:14:07Z",
      "topics": [],
      "readme": "\n\n# Phi-2 and Falcon-7B Models\n\n## Overview\n\nThis repository contains the implementation and training code for our Phi-2 model and inference code for the Falcon-7B model. The Phi-2 model is fine-tuned using the QMOS framework through a custom fine-tuning script, while Falcon-7B is not finetuned. The repository is organized into training, inference, and utility scripts for streamlined usage and development.\n\n## Table of Contents\n\n1. [Phi-2 Model](#phi-2-model)\n    - [Training](#training)\n    - [Retrieval-Augmented Generation (RAG)](#retrieval-augmented-generation-rag)\n    - [Inference](#inference)\n2. [Falcon-7B Model](#falcon-7b-model)\n    - [Inference](#inference-1)\n3. [Utility Functions](#utility-functions)\n4. [Installation](#installation)\n5. [Usage](#usage)\n6. [Contributing](#contributing)\n7. [License](#license)\n\n## QMOS: Phi-2 Model\n\n### Training\n\nThe Phi-2 model training is managed through multiple scripts to facilitate different aspects of the fine-tuning process.\n\n- **train.ipynb**: QMOS training: The primary training file for the Phi-2 model using LoRA.\n- **train.py**: QLoRa: training file for the Phi-2 model using QLoRA without QMOS.\n- **misc/custom_finetuning.py**: We have written our own custom fine-tuning script for developed from scratch (LORA).\n- **train_rough.py**: A rough version of the trainer code for preliminary tests and experiments.\n\n### Retrieval-Augmented Generation (RAG)\n\nThe RAG implementation involves several custom-built scripts designed to handle document chunking, retrieval, and generation.\n\n- **RAG.py**: Our RAG implementation, written completely from scratch.\n- **data_preprocessing.py**: A document chunker that preprocesses text data by calling scripts from the `scripts` directory.\n- **bm_25_extractor.py**: A BM25 extractor for efficient document retrieval.\n\n### Inference\n\n- **inference_custom.ipynb**: QMOS inference code for the Phi-2 model.\n\n## Falcon-7B Model\n\n### Inference\n\n- **inference_haystac_falcon.ipynb**: Notebook for performing inference using the Falcon-7B model.\n\n## Utility Functions\n\n- **utils.py**: Contains various utility functions used across the project.\n\n## Installation\n\nTo get started with this repository, clone it to your local machine and install the required dependencies.\n\n```bash\ngit clone https://github.com/ai4africagroup/telcom_llm.git\ncd phi-2-falcon-7b\npip install -r requirements.txt\n"
    },
    {
      "name": "Ravikiran0303/YouTube_summarizer_",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/147860071?s=40&v=4",
      "owner": "Ravikiran0303",
      "repo_name": "YouTube_summarizer_",
      "description": "Summarizing the youtube videos using Llama 2",
      "homepage": null,
      "language": "Python",
      "created_at": "2024-05-27T06:45:50Z",
      "updated_at": "2024-06-20T12:14:58Z",
      "topics": [],
      "readme": "# YouTube-Video-Summarization-App\n## Project Title: AI-Powered YouTube Video Summarization Tool\n## Project Overview\nOur team has developed an innovative AI model designed to efficiently summarize YouTube videos. By integrating advanced technologies such as Llama 2 for natural language processing, Haystack for robust data retrieval, and Streamlit for an intuitive user interface, we've created a powerful tool that transforms lengthy video content into concise and informative summaries. This model not only saves time but also enhances content accessibility and comprehension for users.\n\n## Problem Statement\nIn today's fast-paced world, the sheer volume of content on platforms like YouTube can be overwhelming. With so many videos and so little time, it’s challenging for people to find and absorb the information they need quickly. Watching lengthy videos to extract key points is not only time-consuming but often frustrating. Our AI model addresses this issue by transforming long YouTube videos into concise, easy-to-understand summaries, making information more accessible and saving users time and effort.\n\n## Use Case: Efficient Learning with AI-Powered YouTube Video Summarization\n### Scenario:\nAlex, a university student, is researching climate change for a term paper. Overwhelmed by the volume of YouTube videos on the topic, Alex needs a way to quickly extract relevant information without watching hours of content.\n\n### Solution:\nAlex uses our AI tool to summarize YouTube videos, allowing for quick extraction of key points and insights from lengthy videos.\n\n## Key Features\nVideo Summarization: Efficiently summarize YouTube video content.\nUser Input via Video URLs: Easily input video links for processing.\nDisplay of Generated Summaries: Clear presentation of key points and important information.\nTimestamp Navigation: Navigate directly to specific sections of the video.\nDownload Option: Save summaries for offline access.\nFeedback Mechanism: Users can provide feedback to improve the tool.\nUser Authentication: Secure access for users.\nScalability: Handle increased demand with ease.\nTechnology Stack\nLlama 2: For natural language processing.\nHaystack: For robust data retrieval.\nStreamlit: For creating an intuitive user interface.\n"
    },
    {
      "name": "viveknair6915/Video-Summarization-System-LLM",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/117741254?s=40&v=4",
      "owner": "viveknair6915",
      "repo_name": "Video-Summarization-System-LLM",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2024-07-05T00:43:46Z",
      "updated_at": "2025-03-14T22:53:00Z",
      "topics": [],
      "readme": "# Video-Summarization-System\nWelcome to the Reading Companion Project! This project aims to provide a supportive and interactive platform to enhance your reading experience. \nVideo Summarization System built using open source LLM and Framework like Llama 2, Haystack, Whisper, and Streamlit. This app smoothly runs on CPU as Llama 2 model is in GGUF format loaded through Llama.cpp.\n\n## Screenshots\n![Reading Companion](YouTube-Video-Summarization-App-main/screenshots/im1.jpeg)\n![Video Summarization](YouTube-Video-Summarization-App-main/screenshots/im3.png)\n\n\n## Table of Contents\n\n- [Features](#features)\n- [Installation](#installation)\n- [Usage](#usage)\n- [Technologies Used](#technologies-used)\n\n## Features\n\n- Fast and accurate summarization of YouTube videos.\n- User-friendly interface powered by Streamlit.\n- Utilizes LangChain and Whisper frameworks for efficient processing and summarization.\n  \n## Installation\n\nTo get started with the Reading Companion Project, follow these steps:\n\n1. **Clone the repository:**\n\n   ```bash\n   git clone https://github.com/viveknair6915/Reading_companion_project.git\n   cd Reading_companion_project \n\n2. **Create and activate a virtual environment**\n\n```bash\npython -m venv venv\nsource venv/bin/activate   # On Windows, use `venv\\Scripts\\activate`\n```\n\n3. **Download the required Model**\n\n**TheBloke/Llama-2-13B-chat-GGUF** : \"https://huggingface.co/TheBloke/Llama-2-13B-chat-GGUF/blob/main/llama-2-13b-chat.Q4_K_S.gguf\"\n**TheBloke/Llama-2-7B-32K-Instruct-GGUF** : \"https://huggingface.co/TheBloke/Llama-2-7B-32K-Instruct-GGUF/blob/main/llama-2-7b-32k-instruct.Q4_K_S.gguf\"\n\n4. **Install the required dependencies**\n\n\n```bash\npip install -r requirements.txt\n```\n\n5. After installing the necessary dependencies, you can start the application by running:\n\n\n```bash\ncd YouTube-Video-Summarization-App-main\nstreamlit run yt_summary.py\n```\n\nOpen your web browser and navigate to http://localhost:8501 to access the Reading Companion interface.\n\n## Usage\n\n- On launching the app, you will be prompted to enter a YouTube URL.\n- Paste the URL and hit 'Summarize'.\n- The app will then process the video, leveraging LLM and the other frameworks to generate a succinct summary which will be displayed on the same page.\n  \n## Technologies Used\n\n**Python**: Programming language used for backend development.\n\n**NLTK**: Natural Language Toolkit for processing text.\n\n**Transformers**: Hugging Face library for state-of-the-art natural language processing.\n\n**PyTube**: For downloading YouTube videos.\n\n**Llama 2**: A large language model used for generating summaries.\n\n**Haystack**: An open-source LLM framework to build production ready applications.\n\n**Whisper**: A robust speech-to-text model used for transcribing video content.\n\n**Streamlit**: An open-source app framework used for building the front end of the application.\n"
    },
    {
      "name": "AIbyMLcom/Chainlit-examples",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/149811768?s=40&v=4",
      "owner": "AIbyMLcom",
      "repo_name": "Chainlit-examples",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2024-05-16T23:18:23Z",
      "updated_at": "2025-01-15T06:21:24Z",
      "topics": [],
      "readme": "# Chainlit Cookbook\n\nWelcome to the Chainlit Demos repository! Here you'll find a collection of example projects demonstrating how to use Chainlit to create amazing chatbot UIs with ease. Each folder in this repository represents a separate demo project.\n\n## 🚀 Getting Started\n\nTo run a demo, follow these steps:\n\n1. Clone this repository:\n   ```\n   git clone https://github.com/Chainlit/cookbook.git chainlit-cookbook\n   ```\n2. Navigate to the desired demo folder:\n   ```\n   cd chainlit-cookbook/demo-folder-name\n   ```\n3. Install the required dependencies:\n   ```\n   pip install -r requirements.txt\n   ```\n4. Create a `.env` file based on the provided `.env.example` file:\n   ```\n   cp .env.example .env\n   ```\n   Modify the `.env` file as needed to include any necessary API keys or configuration settings.\n5. Run the Chainlit app in watch mode:\n   ```\n   chainlit run app.py -w\n   ```\n\nYour demo chatbot UI should now be up and running in your browser!\n\n## 💁 Contributing\n\nWe'd love to see more demos showcasing the power of Chainlit. If you have an idea for a demo or want to contribute one, please feel free to open an issue or create a pull request. Your contributions are highly appreciated!\n"
    },
    {
      "name": "ekimkk/CMU-Advanced-NLP-Assignment-2-End-to-end-NLP-System-Building",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/142109497?s=40&v=4",
      "owner": "ekimkk",
      "repo_name": "CMU-Advanced-NLP-Assignment-2-End-to-end-NLP-System-Building",
      "description": "Team members: Ella Liang, Ellen Song, Alicia Wang",
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2024-03-15T01:56:14Z",
      "updated_at": "2024-10-28T05:57:50Z",
      "topics": [],
      "readme": "# Advanced NLP Assignment 2: End-to-end NLP System Building\n\n## Project Description\nWe built a end-to-end NLP System with RoBERTa, T5 and T5+Mistral. RoBERTa outperforms T5 and T5+Mistral in pinpointing precise answer spans. Although generative models like T5 and Mistral demonstrated their strength in providing more comprehensive information for high-level queries, the flexibility in generating more elaborate responses come with a trade-off: longer results and less accurate information. The remaining challenge that RoBERTa is faced with would be synthesizing information from multiple sources and reasoning over implicit knowledge.\n\n## Getting Started\n### Dependencies\n\nBefore running this project, you must install the required dependencies. This project's dependencies are listed in the `requirements.txt` file. To install them, run the following command:\n\n```bash\npip install -r requirements.txt\n```\n\n### Installing and Executing\n- Clone this repository to your local machine using git clone <repo-url>.\n- Navigate to the cloned directory.\n- Install the project dependencies using the command above.\n- Proceed with the rest of the installation steps as described.\n\n## File Descriptions\n- Web Script Code: Source code for raw data extraction used in the project.\n- data: Directory for training and test data of annotated Q&A pairs.\n- system_outputs: Contains output data (answers) from three models.\n- Mistral_Model.ipynb: Jupyter notebook for the Mistral model development.\n- Model_Evaluation.ipynb: Jupyter notebook for evaluating the models.\n- Roberta_Model.ipynb: Jupyter notebook for the Roberta model development.\n- T5_Model.ipynb: Jupyter notebook for the T5 model development.\n- reference.zip: A compressed file containing reference materials or datasets.\n\n## Authors\n- Ella Liang (jinqiul)\n- Ellen Song (yuhans2)\n- Alicia Wang (chenqiw)\n\n## License\nThis project is licensed under the GNU General Public License v2.0 and Apache License v2.0- see the LICENSE folder for details.\n"
    },
    {
      "name": "exploringweirdmachines/chat-with-a-pdf",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/87807046?s=40&v=4",
      "owner": "exploringweirdmachines",
      "repo_name": "chat-with-a-pdf",
      "description": "Have a chat with a local LLM about your pdf.",
      "homepage": null,
      "language": "Python",
      "created_at": "2024-04-07T12:56:07Z",
      "updated_at": "2024-04-11T14:33:48Z",
      "topics": [],
      "readme": "# Description\n\nChat with PDF. Point the tool at a pdf and ask questions\n\n# Usage\n\n```bash\nusage: main.py [-h] [-v] -i some pdf [-p prompt]\n\nChat with your pdf.\n\noptions:\n  -h, --help            show this help message and exit\n  -v, --version         show program's version number and exit\n  -i some pdf, --input some pdf\n                        path to the pdf\n  -p prompt, --prompt prompt\n                        optional custom prompt\n\n```\n\n## Example\n'Fruits.pdf' file can be found in the folder named 'resources'. \n\n![Alt text](/resources/chat_pdf.gif)\n\n# Requirements\n\nMy setup is:\n\nWIN 10 + nvidia drivers 522 + cuda 11.8 + WSL2 with Ubuntu 22.04 with cuda tools + python >=3.10\n\nsee \"pyproject.toml\"\n\n```bash\npoetry install\n```\n"
    },
    {
      "name": "SunilKumarPradhan/Actions",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/89890497?s=40&v=4",
      "owner": "SunilKumarPradhan",
      "repo_name": "Actions",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2024-04-06T16:53:02Z",
      "updated_at": "2024-08-10T10:44:08Z",
      "topics": [],
      "readme": "# Actions"
    },
    {
      "name": "Oldentomato/elasticsearch_test",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/26248635?s=40&v=4",
      "owner": "Oldentomato",
      "repo_name": "elasticsearch_test",
      "description": "test haystack(elasticsearch) vector store",
      "homepage": null,
      "language": "Python",
      "created_at": "2024-03-26T06:56:31Z",
      "updated_at": "2024-04-23T00:02:04Z",
      "topics": [],
      "readme": ""
    },
    {
      "name": "p-vbordei/RAG-SOTA",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/66299953?s=40&v=4",
      "owner": "p-vbordei",
      "repo_name": "RAG-SOTA",
      "description": "RAG using Document Adnotation --> Entity Resolution --> Knowledge Graphs",
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2024-03-04T19:35:40Z",
      "updated_at": "2024-12-30T22:29:17Z",
      "topics": [],
      "readme": ""
    },
    {
      "name": "ami-zou/document-viewer-chatbot",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/15065020?s=40&v=4",
      "owner": "ami-zou",
      "repo_name": "document-viewer-chatbot",
      "description": "A document viewer that allows users to edit and share their documents. An AI assistance chatbot helps simple Q&A about the documents",
      "homepage": null,
      "language": "Python",
      "created_at": "2024-03-04T16:55:05Z",
      "updated_at": "2024-07-14T14:55:51Z",
      "topics": [],
      "readme": "Project overview: a resource management platform allowing users to upload documents, share them with their colleagues, assisted by an AI chat bot\n\n**Demo**: https://vimeo.com/919250541?share=copy\n\n**Tech stack**\n1. Front-end: React\n2. Back-end: Python FastAPI\n3. NLP model: Haystack \n4. DB: MongoDB\n\n\n**Running the application locally**\n\n1. Run MongoDB locally or using Atlas cloud version: https://www.mongodb.com/docs/manual/installation/\n2. Start a Python environment in terminal\n3. Create a `.env` file and put mongoDB URl (e.g. if running locally, put `MONGO_URI = \"mongodb://localhost:27017\"`)\n4. Install Python dependencies `pip install -r requirements.txt`\n5. Run the FastAPI Application:\n    - In the `backend` directory, run the FastAPI application: `uvicorn main:app --reload` This command will start the FastAPI application, and you should see output indicating that the server is running. All the API documentations are in `localhost:8000/docs`, and you should be able to send requests via `curl`\n6. Run the front-end web page:\n    1. In the `frontend` directory, run `$npm start` , and go to `localhost:3000` which should displays the login feature\n    2. Sample users: `alice` and `bob`, both with password `password123`\n    \n\n**Technical features**\n1. User login and logout\n2. Data persistence using MongoDB\n3. Web application built using React and FastAPI\n4. Different user roles and permissions for the documents \n5. File viewer based on user permission (only displaying `read` access files): list of files + PDF viewer\n6. Access control: in the PDF viewer, display additional `edit` button for `write` access files\n8. AI chat bot for summary \n    1. Process PDF files \n    2. Chat bot\n9. TODO: Enhanced UI with navigation bar and home page\n10. TODO: Deployment to production\n11. TODO: CRUD for resources \n12. TODO: CRUD for user creation + management\n13. TODO: Hierachy display of file structure\n\n**Tech debt**\n1. encode password\n2. enable HTTPS\n3. Docker containerize everything\n4. DB objects \n5. PrivateRoute to dashboard\n6. Use web-socket for chatbot\n7. Hit enter for “send”\n8. use a different AI model library \n\n\nAPI documentation: http://127.0.0.1:8000/docs\n\n\nData models:\nusers:\n```\n{\n    _id: ObjectId('65e2c9997126dd95fe82b932'),\n    username: 'alice',\n    password: 'password123',\n    permissions: [\n      {\n        resource_id: ObjectId('65e2c66c73aa358dd216b0cd'),\n        actions: [ 'read', 'write' ]\n      },\n      {\n        resource_id: ObjectId('65e2c66c73aa358dd216b0ce'),\n        actions: [ 'read', 'write' ]\n      },\n      {\n        resource_id: ObjectId('65e2c66c73aa358dd216b0cf'),\n        actions: [ 'read', 'write' ]\n      }\n    ]\n  },\n```\n\nresources:\n```\n  {\n    _id: ObjectId('65e2c9997126dd95fe82b931'),\n    name: 'article_z',\n    type: 'pdf',\n    content: 'bson PDF binary file with base64 encoding',\n    path: '/article_z.pdf'\n  }\n```\n```\n"
    },
    {
      "name": "Eduds007/LanguageModels",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/63738051?s=40&v=4",
      "owner": "Eduds007",
      "repo_name": "LanguageModels",
      "description": "Projeto de Iniciação Científica para desenvolvimento de modelos de linguagem em português",
      "homepage": "",
      "language": "Jupyter Notebook",
      "created_at": "2023-10-26T18:59:34Z",
      "updated_at": "2025-03-17T18:12:38Z",
      "topics": [],
      "readme": "# Modelos de linguagem em Português\n\nEsse projeto visa desenvolver modelos de linguagem em Português. Em específico, desenvolver um DPR (Dense Passage Retriever) treinado com bases de dados em português.\n\n**Bolsista**: Eduardo Milanez Araujo & Eduardo Figueiredo Pacheco \\\n**Orientador**: Fabio Gagliardi Cozman\n\n[Descrição do projeto](https://drive.google.com/file/d/1U2_mAwZgv8FBG5XjLi2hJwu-egMeKk9Q/view?usp=sharing)\n\n## Cronograma do projeto\n\n| Atividades | 09/23 | 10/23 | 11/23 |  12/23 |  01/24 |  02/24 |  03/24 |  04/24 |  05/24 |  06/24 | \n|-------------|-------------|-------------|-------------|-------------|-------------|-------------|-------------|-------------|-------------|-------------|\n| 1. Revisão bibliográfica      | X      | X      |       |       |       |     |    | |  |  |\n| 2. Seleção de modelos e dados      | X        | X         | X           |     |     |   |     |     |    |     |\n| 3. Treino do DPR      |      |     | X          | X        | X           |     |     |   |  |     |     |\n| 4. Treino baseado em LLaMa      |      |      |   |    | X          | X        | X          |          |      |     |     |\n| 5. Análise de modelos DPR      |      |     |       |      |     |      |    X  |  X  |     |      |     |\n| 6. Avaliação de modelo generativo      | | | | | | | |      X | X     |   |  |\n| 7. Elaboração de repositório      |       |       |       |       |       |      |       |       |      X | X      |       |\n| 8. Elaboração de relatórios      |     |     |     |     |  X |    |     |    |     | X |\n\n\n\n## Principais bibliotecas utilizadas\n\n [Haystack](https://github.com/deepset-ai/haystack): Biblioteca para treinamento do DPR. \n  ```\n  pip install haystack\n  ```\n [Datasets](https://github.com/huggingface/datasets): Biblioteca para reutilizar modelos de linguagem produzidos por outros pesquisadores. \n  ```\n  pip install datasets\n  ```\n\n# \n\n# Executando o projeto\n\nPara poder executar o projeto, que está em um docker, basta executar os comandos: \n\nSe for a primeira vez:\n  ```\n  cd LanguageModels\n  cd src\n  sudo docker-compose up --build\n  ```\nCaso contrário:\n  ```\n  cd LanguageModels\n  cd src\n  sudo docker-compose up\n  ```\n\n\n\n\n\n# Resumo do Projeto de NLP e Recuperação de Informações\n\nEste projeto explora técnicas avançadas de Processamento de Linguagem Natural (NLP) e Recuperação de Informações, focando na interação entre humanos e máquinas através da linguagem. Utilizando a linguagem de programação Python, o código abrange desde o tratamento de dados até o treinamento de modelos de Dense Passage Retrieval (DPR), incluindo o uso da biblioteca Haystack para aprimorar a busca e recuperação de informações em textos.\n\n## Biblioteca Haystack\n\nA biblioteca [Haystack](https://github.com/deepset-ai/haystack) é uma ferramenta poderosa para tarefas de busca e recuperação de informações. Ela permite a construção de sistemas de busca que compreendem o contexto e a semântica do conteúdo pesquisado, integrando-se com modelos de machine learning para oferecer respostas precisas a consultas complexas.\n\n## Tratamento de Dados\n\nO tratamento de dados é uma etapa crucial neste projeto, envolvendo a limpeza, preparação e manipulação de conjuntos de dados para treinamento e avaliação dos modelos. Este processo é fundamental para garantir que os modelos de NLP possam aprender de maneira eficaz, removendo ruídos e estruturando os dados adequadamente.\n\n## Dense Passage Retrieval (DPR)\n\nO DPR é uma técnica de destaque que permite a recuperação eficiente de passagens de texto relevantes para uma dada consulta. Funciona através do treinamento de modelos para entender a semântica das perguntas e dos documentos, criando representações vetoriais densas que facilitam a busca por similaridade semântica.\n\n## Importância do DPR\n\nA implementação e o treinamento do DPR são essenciais para o sucesso do sistema de busca, destacando a importância de modelos de aprendizado profundo no avanço das capacidades de NLP. Esta abordagem supera os métodos tradicionais de busca por palavras-chave, oferecendo respostas mais precisas e contextualmente relevantes.\n\n\n## Base de dados em português\n\nUma das principais contribuições desse projeto consiste em fornecer uma base perguntas e respostas com mais de 50000 passagens traduzidas com a utilização de um modelo de tradução treinado especificamente para a conversão de textos em inglês para português [NQ-PT-BR](https://huggingface.co/datasets/edu-milanez/NQ-PT-BR)"
    },
    {
      "name": "srikanthpl/MY_PROJECTS",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/124775319?s=40&v=4",
      "owner": "srikanthpl",
      "repo_name": "MY_PROJECTS",
      "description": null,
      "homepage": null,
      "language": "PHP",
      "created_at": "2024-02-12T05:18:34Z",
      "updated_at": "2024-12-18T23:04:00Z",
      "topics": [],
      "readme": "\"# MY_PROJECTS\" \n"
    },
    {
      "name": "ioniccommerce/ionic_haystack",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/140454189?s=40&v=4",
      "owner": "ioniccommerce",
      "repo_name": "ionic_haystack",
      "description": "Ionic Commerce Tool for Haystack",
      "homepage": "https://ioniccommerce.com",
      "language": "Python",
      "created_at": "2024-01-31T21:25:08Z",
      "updated_at": "2024-02-13T02:33:07Z",
      "topics": [
        "agent-tool",
        "ai",
        "ecommerce",
        "haystack"
      ],
      "readme": "# Ionic Commerce tool for Haystack\n\nIonic Haystack provides an Agent Tool integration to Ionic Commerce. This tool will enable e-commerce for your agent , allowing your users to ask for product recommendations and purchase products through the agent chat interface.\n\n## Installation\n\nYou can install the package from PyPI using `pip`:\n```sh\npython3 -m pip install ionic-haystack\n```\n\nor `poetry`:\n\n```sh\npoetry add ionic-haystack\n```\n\n## Usage\nGet started quickly using Ionic Commerce with Haystack by creating an IonicShoppingTool and adding it to your agent's tools.\n```python\nimport os\nfrom haystack.agents import Tool\nfrom haystack.agents.conversational import ConversationalAgent\nfrom haystack.agents.memory import ConversationMemory\nfrom haystack.nodes import PromptNode\n\nfrom ionic_haystack.prompt_templates import ionic_template\nfrom ionic_haystack.tool import IonicShoppingTool\n\nionic_node = IonicShoppingTool(api_key=\"my_ionic_api_key\")\nionic_tool = Tool(\n    name=\"Ionic\",\n    pipeline_or_node=ionic_node,\n    description=ionic_template\n)\n\nmemory = ConversationMemory()\nprompt_node = PromptNode(\"gpt-3.5-turbo\", api_key=os.getenv(\"OPENAI_API_KEY\") , max_length=256, stop_words=[\"Observation:\"])\nagent = ConversationalAgent(prompt_node, tools=[ionic_tool],  memory=memory, prompt_template=\"deepset/conversational-agent\")\n```\n\n## Examples\n- Example Ionic Agent: [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ioniccommerce/ionic_haystack/blob/main/examples/example_ionic_agent.ipynb)"
    },
    {
      "name": "VivekSai07/Text2Image-Search-System",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/113660738?s=40&v=4",
      "owner": "VivekSai07",
      "repo_name": "Text2Image-Search-System",
      "description": "This Text2Image Search system using Haystack, facilitating the retrieval of relevant images based on textual queries. Employing multimodal retrieval and embedding models, it handles diverse datasets of animals, flowers, and fashion. Challenges include version compatibility and dataset diversity, with avenues for improvement outlined.",
      "homepage": "",
      "language": "Python",
      "created_at": "2024-01-31T10:24:13Z",
      "updated_at": "2024-03-04T03:30:56Z",
      "topics": [],
      "readme": "# Text2Image Search System\n\n## Overview\nThis repository contains the code for a Text2Image Search system, allowing users to search for images based on textual queries. The system is built using the Haystack library and leverages multimodal retrievers to handle both text and image data. The implementation involves the creation of embeddings for both textual and image data, enabling efficient retrieval of relevant images based on user queries.\n\n## System Architecture\n**1. Data Collection and Preparation**\nThe dataset consists of 178 images manually collected from three distinct categories: animals, flowers, and fashion. Each category has specific classes such as zebra, rose, black cat, jeans, and jackets. Images are stored in the \"Data\" directory.\n\n**2. Document Store**\nAn InMemoryDocumentStore is employed to store document embeddings. The embeddings have a dimensionality of 512. Images are represented as documents with their paths as content and content type set to \"image.\" These documents are then written to the document store.\n\n**3. Retrieval Models**\nThe system uses the MultiModalRetriever from Haystack, which allows the integration of both textual and image embeddings. For text embedding, the \"sentence-transformers/clip-ViT-B-32\" model is used, while the same model is employed for image embeddings. This enables a seamless combination of textual and image data during the retrieval process.\n\n**4. Search Pipeline**\nA search pipeline is created using the Pipeline class from Haystack. The pipeline includes a single node - the multimodal retriever (retriever_text_to_image). The pipeline is responsible for handling user queries and returning relevant search results.\n\n**5. User Interface (Streamlit App)**\nThe user interface is implemented using Streamlit. Users can input queries through a text input field and initiate searches. The top-k search results are then displayed, along with their respective scores and images.\n\n## Techniques Employed\n- **Multimodal Retrieval**: The system leverages multimodal retrieval techniques to handle both text and image data, providing a unified search experience.\n- **Embedding Models**: The \"sentence-transformers/clip-ViT-B-32\" model is used for generating embeddings for both text and images, facilitating efficient comparison and retrieval.\n- **Streamlit for UI**: Streamlit is employed to create a user-friendly interface for interacting with the Text2Image search system.\n\n## Challenges Encountered\n- **Version Incompatibility Issues**: One major challenge faced during implementation was version incompatibility issues. Ensuring compatibility between different library versions, especially for Haystack and Streamlit, required careful consideration and adjustment of dependencies.\n- **Data Collection and Diversity**: Manually collecting and curating a diverse dataset posed a challenge. Ensuring that each category has a representative set of classes and images was crucial for the system's performance and generalizability.\n\n## Instructions for Running the System\n1. Install dependencies\n```bash\npip install -r requirements.txt.\n```\n2. Run the app.py\n```bash\nstreamlit run app.py.\n```\n3. Access the Text2Image Search App through the provided local URL.\n\n\n## Potential Avenues for Improvement\n### Handling No Match Scenarios \n**Challenge**: One notable issue in the current system is its response when a text query does not match any image in the dataset. In such cases, the model might return an image that it deems to have similar features, leading to potentially misleading results. \n\n**Solution**: \n1. **Confidence Thresholding:** Set a confidence threshold; if the top-ranked result falls below it, convey to the user that no matching images were found.\n2. **Negative Samples:** Train the model with negative samples to better distinguish instances where queries shouldn't yield relevant images.\n3. **User Feedback Integration:** Allow user feedback to enhance the model by incorporating user input on result relevance.\n\n### Diversity and Generalization\n**Challenge**: Ensuring diversity and generalization in the dataset is crucial for the model's performance. The current system relies on a manually curated dataset, which may not cover all possible scenarios.\n\n**Solutions:**\n1. **Data Augmentation:** Apply techniques like random rotations and flips to augment the dataset artificially.\n2. **Dynamic Dataset Expansion:** Dynamically expand the dataset based on user interactions and fetch relevant images from external sources.\n3. **Incorporate External Knowledge:** Integrate external knowledge sources, like image databases, for additional context and improved retrieval.\n\nBy implementing these enhancements, the Text2Image Search system can offer more accurate results and better adapt to diverse user queries over time. Continuous user feedback and monitoring will be integral for ongoing system refinement.\n\n\n## Sample Images\n\n![Sample-1](https://github.com/VivekSai07/Text2Image-Search-System/blob/main/Sample-1.png)\n![Sample-2](https://github.com/VivekSai07/Text2Image-Search-System/blob/main/Sample-2.png)\n![Sample-3](https://github.com/VivekSai07/Text2Image-Search-System/blob/main/Sample-3.png)\n"
    },
    {
      "name": "pks20iitk/Haystack-and-Mistral-7B-RAG-Implementation",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/78561615?s=40&v=4",
      "owner": "pks20iitk",
      "repo_name": "Haystack-and-Mistral-7B-RAG-Implementation",
      "description": "Haystack and Mistral 7B RAG Implementation. It is based on completely open-source stack.",
      "homepage": null,
      "language": "Python",
      "created_at": "2024-01-23T23:23:34Z",
      "updated_at": "2024-02-01T00:46:21Z",
      "topics": [],
      "readme": "# Haystack-and-Mistral-7B-RAG-Implementation\r\nHaystack and Mistral 7B RAG Implementation. It is based on completely open-source stack.\r\n"
    },
    {
      "name": "seok-hee97/Transformers-for-Natural-Language-Processing",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/98581131?s=40&v=4",
      "owner": "seok-hee97",
      "repo_name": "Transformers-for-Natural-Language-Processing",
      "description": null,
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2023-10-12T02:45:39Z",
      "updated_at": "2024-03-29T04:41:11Z",
      "topics": [],
      "readme": "\n\n# Transformers for Natural Language Processing\nThis is the code repository for [Transformers for Natural Language Processing](https://www.packtpub.com/product/transformers-for-natural-language-processing/9781800565791), published by [Packt](https://www.packtpub.com/?utm_source=github). It contains all the supporting project files necessary to work through the book from start to finish.\n\n* **Paperback**: 384 pages\n* **ISBN-13**: 9781800565791\n* **Date Of Publication**: January 2021\n\n[<img src=\"./.other/cover.png\" width=\"248\">](https://www.amazon.com/Transformers-Natural-Language-Processing-architectures-ebook/dp/B08S977X8K/)\n\n## Links\n\n* [Amazon](https://www.amazon.com/Transformers-Natural-Language-Processing-architectures-ebook/dp/B08S977X8K/)\n\n* [Packt Publishing](https://www.packtpub.com/product/transformers-for-natural-language-processing/9781800565791)\n\n## About the Book\nTransformers for Natural Language Processing investigates in vast detail the deep learning for machine translations, speech-to-text, text-to-speech, language modeling, question answering, and many more NLP domains in context with the Transformers.\n\nThe book takes you through Natural language processing with Python and examines various eminent models and datasets in the transformer technology created by pioneers such as Google, Facebook, Microsoft, OpenAI, Hugging Face, and other contributors.\n\nThe book trains you in three stages. The first stage introduces you to transformer architectures, starting with the original Transformer, before moving on to RoBERTa, BERT, and DistilBERT models. You will discover training methods for smaller Transformers that can outperform GPT-3 in some cases. In the second stage, you will apply Transformers for Natural Language Understanding (NLU) and Generation. Finally, the third stage will help you grasp advanced language understanding techniques such as optimizing social network datasets and fake news identification.\n\nBy the end of this NLP book, you will understand transformers from a cognitive science perspective and be proficient in applying pre-trained transformer models by tech giants to various datasets.\n\n## Things you will learn\n* Use the latest pretrained transformer models\n* Grasp the workings of the original Transformer, GPT-2, BERT, T5, and other transformer models\n* Create language understanding Python programs using concepts that outperform classical deep learning models\n* Use a variety of NLP platforms, including Hugging Face, Trax, and AllenNLP\n* Apply Python, TensorFlow, and Keras programs to sentiment analysis, text summarization, speech recognition, machine translations, and more\n* Measure the productivity of key transformers to define their scope, potential, and limits in production\n\n## Instructions and Navigation\nAll of the code is organized into folders that are named chapter-wise, for example: `Chapter02`.\n\nThe code will look like the following:\n```python\n#@title Activating the GPU\n# Main menu->Runtime->Change Runtime Type\nimport tensorflow as tf\ndevice_name = tf.test.gpu_device_name()\nif device_name != ‘/device:GPU:0’:\n  raise SystemError(‘GPU device not found’)\nprint(‘Found GPU at: {}’.format(device_name))\n```\n\n## Software Requirements\n\nCheck this file for the hardware and software requirements: [technical_requirements.md](./.other/technical_requirements.md)\n\n## Related Products\n\n* [Python Machine Learning - Third Edition](https://www.packtpub.com/product/python-machine-learning-third-edition/9781789955750)\n* [Hands-On Explainable AI (XAI) with Python - Second Edition](https://www.packtpub.com/product/hands-on-explainable-ai-xai-with-python/9781800208131)\n\n### Download a free PDF\n\n <i>If you have already purchased a print or Kindle version of this book, you can get a DRM-free PDF version at no cost.<br>Simply click on the link to claim your free PDF.</i>\n<p align=\"center\"> <a href=\"https://packt.link/free-ebook/9781800565791\">https://packt.link/free-ebook/9781800565791 </a> </p>\n\n\n\n\n\n\n\n\n### Usage\n\nUsage(python 3.8(Anaconda) I used in m1 mac)\n```\nconda create -n tda_py38 python=3.8\n```\n\nchectk the env made well\n```\nconda env list \n```\nand activate env\n```\nconda activate tda_py38\n```\ninstall python Lib\n```\npip install -r requirements.txt\n```"
    },
    {
      "name": "Achaarya-AI/Acharya-core",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/155107177?s=40&v=4",
      "owner": "Achaarya-AI",
      "repo_name": "Acharya-core",
      "description": null,
      "homepage": null,
      "language": "JavaScript",
      "created_at": "2024-01-05T18:48:35Z",
      "updated_at": "2024-05-22T10:15:18Z",
      "topics": [],
      "readme": "# Acharya-core"
    },
    {
      "name": "NgoDangKhoauit/Rocks-checking",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/131435439?s=40&v=4",
      "owner": "NgoDangKhoauit",
      "repo_name": "Rocks-checking",
      "description": null,
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2024-01-13T07:58:53Z",
      "updated_at": "2024-05-07T07:11:25Z",
      "topics": [],
      "readme": "# Fact Checking\n\n### Indexing pipeline\n* [Crawling](https://github.com/NgoDangKhoauit/Rocks-checking/tree/main/notebooks/get_wikipedia_data.ipynb): Crawl data from Wikipedia, starting from the page [List of mainstream rock performers](https://en.wikipedia.org/wiki/List_of_mainstream_rock_performers) and using the [python wrapper](https://github.com/goldsmith/Wikipedia)\n* [Indexing](https://github.com/NgoDangKhoauit/Rocks-checking/tree/main/notebooks/indexing.ipynb)\n  * Preprocess the downloaded documents into chunks consisting of 2 sentences\n  * Chunks with less than 10 words are discarded, because they are not very informative\n  * Instantiate a [FAISS](https://github.com/facebookresearch/faiss) Document store and store the passages on it\n  * Create embeddings for the passages, using a Sentence Transformer model and save them in FAISS. The retrieval task will involve [*asymmetric semantic search*](https://www.sbert.net/examples/applications/semantic-search/README.html#symmetric-vs-asymmetric-semantic-search)\n  * Save FAISS index\n\n### Search pipeline\n\n* The user enters a factual statement\n* Compute the embedding of the user statement using the same Sentence Transformer used for indexing (`msmarco-distilbert-base-tas-b`)\n* Retrieve the K most relevant text passages stored in FAISS (along with their relevance scores)\n* **Text entailment task**: compute the text entailment between each text passage (premise) and the user statement (hypothesis), using a Natural Language Inference model (`microsoft/deberta-v2-xlarge-mnli`). For every text passage, we have 3 scores (summing to 1): entailment, contradiction, and neutral\n* Aggregate the text entailment scores: compute their weighted average, where the weight is the relevance score. **Now it is possible to tell if the knowledge base confirms, is neutral, or disproves the user statement**\n* *Empirical consideration: if in the first N passages (N<K),  there is strong evidence of entailment/contradiction (partial aggregate scores > 0.5), it is better not to consider (K-N) less relevant documents*\n\n### Repository structure\n* [Rock_fact_checker.py](Rock_fact_checker.py) and [pages folder](./pages/): multi-page Streamlit web app\n* [app_utils folder](./app_utils/): python modules used in the web app\n* [notebooks folder](./notebooks/): Jupyter/Colab notebooks to get Wikipedia data and index the text passages (using Haystack)\n* [data folder](./data/): all necessary data, including original Wikipedia data, FAISS Index and prepared random statements\n\n### Installation\n\n To install this project locally, follow these steps:\n* `git clone https://github.com/NgoDangKhoauit/Rocks-checking`\n* `cd Rocks-checking`\n* `pip install -r requirements.txt`\n\nTo run the web app, simply type: `streamlit run Rock_fact_checker.py`\n"
    },
    {
      "name": "peterkchung/logos-ai",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/77805646?s=40&v=4",
      "owner": "peterkchung",
      "repo_name": "logos-ai",
      "description": "Logos is an orchestration framework for planning, organizing, and executing tasks with language models (LMs).",
      "homepage": "",
      "language": "Python",
      "created_at": "2024-01-10T04:08:51Z",
      "updated_at": "2024-03-16T02:48:31Z",
      "topics": [],
      "readme": "# Logos AI - Reasoning Engine For Language Models  \n\n<img src=\"https://github.com/peterkchung/logos-ai/blob/main/tree-of-knowledge.jpg\" width=\"400\"  align=\"right\" />\n\n### Logos (λόγος): A Greek term used by philosophers meaning \"word,\" \"speech,\" or \"reason.\" In the context of philosophy, it refers to the principle of order and knowledge.\n\nLogos is an orchestration framework for planning, organizing, and executing tasks with language models (LMs).\n\nVery early prototyping. Please follow for updates.\n\nContributions, feedback, critiques welcome!\n\n<br clear=\"right\"/>\n\n## Setup\n\n```pip install []```\n\n\n## Examples\n\n```example here```\n\n\n## Structure\n\nlogos  \n├── query  \n│   ├── query  \n│   ├── schema  \n│   ├── example  \n│   ├── parse  \n│   └── ...  \n├── sequence  \n│   ├── proposal    \n│   ├── reward  \n│   ├── method  \n│   └── ...  \n├── util  \n│   ├── models  \n│   ├── tools  \n│   └── ...    \n└── reason.py  \n\n## Notes\n\n## License\n\nLicensed under Apache 2.0\n\n## Refences\n\nReasoning with Language Model is Planning with World Model https://arxiv.org/abs/2305.14992  \n\nTree of Thoughts: Deliberate Problem Solving with Large Language Models https://arxiv.org/abs/2305.10601  \n"
    },
    {
      "name": "peterkchung/quickchat",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/77805646?s=40&v=4",
      "owner": "peterkchung",
      "repo_name": "quickchat",
      "description": null,
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2024-01-09T14:03:01Z",
      "updated_at": "2024-01-11T04:42:44Z",
      "topics": [],
      "readme": "# quickChat\n\n## Quick implementation of a chat UI for agent conversations.\n\n### Hugging Face, Gradio\n\nPreliminary version to provide scaffolding for future projects."
    },
    {
      "name": "alaeddinehamroun/Scholar-s-Ally",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/71340201?s=40&v=4",
      "owner": "alaeddinehamroun",
      "repo_name": "Scholar-s-Ally",
      "description": "A learning assistant using extractive QA solution designed to navigate and distill complex educational materials.",
      "homepage": null,
      "language": "Python",
      "created_at": "2023-12-30T17:36:06Z",
      "updated_at": "2024-02-24T13:18:47Z",
      "topics": [],
      "readme": "# Scholar-s-Ally: An extractive/Generative QA solution designed to navigate and distill complex educational materials.\n\nExtractive Question Answering (QA) is a type of question-answering system where the answer to a question is extracted directly from a text or a set of texts. In extractive QA, the answer is a segment of text (such as a phrase or sentence) taken verbatim from a document or documents, rather than being generated or summarized. This method relies on identifying and extracting the most relevant parts of the text that directly answer the posed question.\n\nIn generative QA, the answer to a question is not extracted directly from a pre-existing text but is instead generated by the system based on its understanding of the context and the question. \n\nScholar's Ally. How it works?\n1. Integrate educational content: chapters of a specific subject at school (pdfs).\n2. Type a question about it.\n3. Get the most relevant information from the integrated content and where to find it.\n\n![Example Image](Screenshot.png)\n\n## Architecture\n![Architecture Image](architecture.png)\nsource: https://docs.cloud.deepset.ai/docs\n## Installation steps\n\n#### 1. Clone the repository:\n```bash\ngit clone https://github.com/alaeddinehamroun/Scholar-s-Ally.git\ncd Scholar-s-Ally\n```\n#### 2. Create a virtual environment and activate it:\n```bash\n# Make sure you have python3 and pip installed\npython3 -m venv .venv\nsource .venv/bin/activate\n\n# To  deactivate the environment run:\n# deactivate\n```\n#### 3. Install the required dependencies:\n\n```bash\npip install --upgrade pip\npip install -r requirements.txt\n```\n\n#### 4. Launch Elasticsearch server:\nYou can visit the docs of Elasticsearch for more details.\n```bash\n# Make sure that Docker is available in your environment.\npython3 src/launch_es.py\n```\nor \n```bash\ndocker pull docker.elastic.co/elasticsearch/elasticsearch:7.9.2\ndocker run -d -p 9200:9200 -e \"discovery.type=single-node\" elasticsearch:7.9.2\n```\n#### 5. Start the API:\n```bash\ncd src/api\nuvicorn main:app --reload\n```\n1. Index: to index your files availables at data/ folder\n2. Query: enter your question and you'll get the answer\n\n\n#### 6. Start the UI:\n```bash\ncd src/\npython3 -m streamlit run ui/home.py \n```\n"
    },
    {
      "name": "greymini/Miss7B",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/79689279?s=40&v=4",
      "owner": "greymini",
      "repo_name": "Miss7B",
      "description": "This is an application that uses Haystack, Weaviate, Mistral 7B and FAST API. It also employs RAG",
      "homepage": "",
      "language": "Python",
      "created_at": "2023-12-25T06:43:40Z",
      "updated_at": "2024-01-01T22:18:31Z",
      "topics": [],
      "readme": "# Miss7B\n\nThis is an application that uses Haystack, Weaviate, Mistral 7B and FAST API. This stack basically lets you build scalable LLM appliactions. This also uses RAG to enhance query responses.\n"
    },
    {
      "name": "samroj95/GenAI---Recommeder-App-main",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/92337579?s=40&v=4",
      "owner": "samroj95",
      "repo_name": "GenAI---Recommeder-App-main",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2023-12-24T11:30:41Z",
      "updated_at": "2023-12-24T12:23:17Z",
      "topics": [],
      "readme": "# GenAI- Recommender app \n\n* A LLM bot that is trained on custom toy data, you can search for a toy in its UI and it will provide recommended toy.\n* Using text to image search.\n* Basically it show the capapbility of bot to search the similar item and recommend you.\n\n# To run this app :\n  * clone this repo\n  * pip install -r requirements.txt\n  * cd app.py\n  * streamlit run app.py\n\n# UI :\n\n\n![UI](https://github.com/Rakib-data-scientist/GenAI-Recommeder-App/assets/137823730/e1eb7efb-2ed2-4b8b-9c8f-3ac63a99a61c)\n\n\n"
    },
    {
      "name": "guptashrey/LLM-Training-and-Fine-tuning",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/17765731?s=40&v=4",
      "owner": "guptashrey",
      "repo_name": "LLM-Training-and-Fine-tuning",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2023-12-02T16:41:49Z",
      "updated_at": "2023-12-15T18:02:20Z",
      "topics": [],
      "readme": "# LLM Training and Fine-Tuning\n**Duke AIPI 591 (Independent Study in GenAI) Research Project**\n\n## Project Overview\nThe aim of this project is to research the different ways in which Large Language Models (LLMs) can be trained and fine-tuned to perform Question Answering (QA) tasks. This project builds upon the [Duke ChatBot](https://github.com/guptashrey/Duke-ChatBot) project and aims to improve the performance of the ChatBot by training and fine-tuning LLMs on the Duke University data.\n\n&nbsp;\n## Data Sources\n\nData about Duke University is needed to power the answers to the questions asked by the users. There is no particular dataset or data source that can be used to get all the information about Duke University. Hence, we need to scrape data from multiple websites and webpages to get the required information.\n\nAs there are multiple departments, schools and programs at Duke University, we need to scrape data from multiple subdomains of the main website [duke.edu](https://www.duke.edu/). The following are the subdomains from where the data is being scraped:\n\n* [Master of Engineering in AI](https://ai.meng.duke.edu/)\n* [Biomedical Engineering](https://bme.duke.edu/)\n* [Civil & Environmental Engineering](https://cee.duke.edu/)\n* [Master of Engineering in Cybersecurity](https://cybersecurity.meng.duke.edu)\n* [Electrical & Computer Engineering](https://www.ece.duke.edu/)\n* [Master of Engineering Management](https://memp.pratt.duke.edu/)\n* [Mechanical Engineering & Materials Science](https://mems.duke.edu/)\n* [Master of Engineering](https://meng.pratt.duke.edu/)\n* [Pratt School of Engineering](https:/pratt.duke.edu/)\n\n&nbsp;\n## Data Processing\n\n### **Creating a list of URLs to scrape**\n\nBefore scraping the data from the above listed subdomains, a list of URLs of the webpages and subdirectories needs to be created.\n\nTo create this list for each subdomain, a recursive function is used to traverse through the subdirectories and subpages of the subdomain and add the URLs to a dictionary. The dictionary is then converted to a list and saved as a JSON file.\n\nThe python script can be found in the `scripts` folder and can be run as follows:\n\n**1. Create a new conda environment and activate it:** \n```\nconda create --name nlp python=3.8\nconda activate nlp\n```\n**2. Install python package requirements:** \n```\npip install -r requirements.txt \n```\n**3. Change directory to the scripts folder:** \n```\ncd scripts\n```\n**4. Run the URL list creation script:** \n```\npython get_subpages.py <subdomain_url> <json_file_name>\n```\n\nFor example, to create the list of URLs for the subdomain [https://ai.meng.duke.edu](https://ai.meng.duke.edu/), the following command can be run:\n\n```bash\npython python get_subpages.py \"https://ai.meng.duke.edu\" \"../data/web_scraping/ai_meng_duke_edu.json\"\n```\n\n### **Scraping text data from the webpages**\n\nOnce the list of URLs is created, the text data from all the webpages of a subdomain need to be scraped. As each webpage has a different structure, parsing the HTML of each webpage and extracting the required text data is a tedious and time-consuming task.\n\nHence, the [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) library is used to parse the HTML of the webpages and extract the text data. The text data is then cleaned using [html2text](https://pypi.org/project/html2text/) package and stored in a .txt file.\n\nThe python script to scrape text can be found in the `scripts` folder. Assuming you are in the same conda environment as the previous step, the python script can be run as follows:\n\n**1. Change directory to the scripts folder:** \n```\ncd scripts\n```\n**2. Run the web scraping script:** \n```\npython scrape.py <json_file_name> <data_directory>\n```\n\nFor example, to scrape the text data from the subdomain [https://ai.meng.duke.edu](https://ai.meng.duke.edu/), the following command can be run:\n\n```bash\npython scrape.py \"../data/web_scraping/ai_meng_duke_edu.json\" \"../data/web_scraping\"\n```\n\n### **Data Processing**\n\nOnce the text data is scraped from the webpages, it needs to be processed and indexed in Elasticsearch. The following steps are performed to process and index the text data:\n- Read the text data from all the .txt files\n- Remove whitespaces, angle brackets, html tags and urls\n- Webpages of a particular subdomain contain specific headers and footers. These headers and footers are removed from the text data\n- Remove any non-unicode characters from the text data\n- Chunk the text data into smaller chunks of 250 words each (with a 10 word overlap and respecting sentence boundaries)\n\nNow, to create data for pre-training LLMs, the following steps are performed:\n- Create a list of all the chunks of text data\n- Write the chunks of text data to a .txt file with each chunk separated by a newline character\n\nTo create data for fine-tuning LLMs, the following steps are performed:\n- Create a list of all the chunks of text data\n- Loop through each chunk of text data and use the OpenAI API to generate pairs of questions and answers for each chunk of data\n- The instruction prompt for the OpenAI API is as follows (where `<passage>` is the chunk of text data):\n```\nI have a short passage given below. Can you please create a list of question answer pairs from the passage which I can use to populate the faq section of my website? Please follow the format: [{\\\"question\\\": \\\"What is the capital of India?\\\", \\\"answer\\\": \\\"New Delhi\\\"}, {\\\"question\\\": \\\"Which is the largest state in India?\\\", \\\"answer\\\": \\\"Rajasthan\\\"}]\\n\\nPassage:\\n```<passage>```\n```\n- Write the generated question-answer pairs to a .json file\n\nThe python script to process the text data can be found in the `scripts` folder. Assuming you are in the same conda environment as the previous step, the python script can be run as follows:\n\n**1. Change directory to the scripts folder:** \n```\ncd scripts\n```\n**2. Run the text preprocessing for pre-training LLMs:** \n```\npython process_pretrain_data.py \"../data/web_scraping\"\n```\n**3. Run the text preprocessing for fine-tuning LLMs:** \n```\npython process_finetune_data.py \"../data/web_scraping\"\n```\n\n&nbsp;\n## Pre-training Llama-2-7b\n\nLlama 2 is a large language model, originally trained on billions of webpages and documents. As part of this project, the model is pre-trained on Duke University data while keeping the original Llama 2 architecture and hyperparameters intact. The hyperparameters used are as follows:\n- **Model:** Llama-2-7b\n- **Batch Size:** 125\n- **Learning Rate:** 6e-4\n- **Weight Decay:** 1e-1\n- **Beta1:** 0.9\n- **Beta2:** 0.95\n- **Gradient Clip:** 1.0\n- **Warmup Iters:** 2000\n- **Max Iters:** 600000 (num_epochs * (epoch_size // micro_batch_size) // devices)\n\nThe data is first prepared by tokenizing the text data and converting it into a torch dataset. The model is then trained on the data using the [Lightning](https://www.pytorchlightning.ai/) framework.\n\nThe model is trained on a 8 GPUs for 5 epochs. The data preparation and training scripts can be found in the `scripts` and `pretrain` folders respectively. Assuming you are in the same conda environment as the previous step, the python script can be run as follows:\n\n**1. Prepare data for pre-training:** \n```\npython scripts/prepare_pretrain.py\n```\n**2. Run the pre-training script:** \n```\npython pretrain/run_pretrain.py\n```\n\n&nbsp;\n## Fine-tuning Llama-2-7b\n\nThe base Llama-2-7b model is fine-tuned on the Duke University data to perform Question Answering (QA) tasks. Fine-tuning is performed using 2 approaches:\n\n### **Approach 1: Fine-tuning all the model weights**\n\nIn this approach, all the model weights are fine-tuned on QA data from Duke University. The hyperparameters used are as follows:\n- **Model:** Llama-2-7b\n- **Batch Size:** 1\n- **Learning Rate:** 3e-3\n- **Weight Decay:** 0.02\n- **Epoch Size:** 1500\n- **Num Epochs:** 5\n- **Warmup Steps:** 2 * (epoch_size // micro_batch_size) // devices // gradient_accumulation_iters\n\nThe data is first prepared by tokenizing the text data and converting it into a torch dataset. The model is then fine-tuned on the data using the [Lightning](https://www.pytorchlightning.ai/) framework.\n\nThe model is fine-tuned on a 1 GPUs for 5 epochs. The data preparation and fine-tuning scripts can be found in the `scripts` and `finetune` folders respectively. Assuming you are in the same conda environment as the previous step, the python script can be run as follows:\n\n**1. Prepare data for fine-tuning:** \n```\npython scripts/prepare_finetune.py\n```\n**2. Run the fine-tuning script:** \n```\npython finetune/full.py\n```\n\n### **Approach 2: Fine-tuning limited set of weights using Adapter Parameter Efficient Fine-Tuning (PEFT)**\n\nIn this approach, only a limited set of weights are fine-tuned on QA data from Duke University. The hyperparameters used are as follows:\n- **Model:** Llama-2-7b\n- **Batch Size:** 1\n- **Learning Rate:** 3e-3\n- **Weight Decay:** 0.02\n- **Epoch Size:** 1500\n- **Num Epochs:** 5\n- **Warmup Steps:** 2 * (epoch_size // micro_batch_size) // devices // gradient_accumulation_iters\n\nThe data is first prepared by tokenizing the text data and converting it into a torch dataset. The model is then fine-tuned on the data using the [Lightning](https://www.pytorchlightning.ai/) framework.\n\nThe model is fine-tuned on a 1 GPUs for 5 epochs. The data preparation and fine-tuning scripts can be found in the `scripts` and `finetune` folders respectively. Assuming you are in the same conda environment as the previous step, the python script can be run as follows:\n\n**1. Prepare data for fine-tuning:** \n```\npython scripts/prepare_finetune.py\n```\n**2. Run the fine-tuning script:** \n```\npython finetune/adapter_v2.py\n```\n\n&nbsp;\n## Performance Evaluation and Metrics\n\nDue to the unsupervised nature of the data, there is no ground truth to evaluate the performance of the pre-trained and fine-tuned LLMs. However, the performance of the pipeline can be evaluated by asking questions to the pipeline and checking if the answer returned by the pipeline is relevant to the question asked.\n\n### **Manual Qualitative Evaluation**\nA list of 50 questions were asked to the pre-trained/fine-tuned LLMs and the answers returned by the pipeline were evaluated manually. Each answer was evaluated on a scale of 0 to 3 as follows:\n- 0: The answer is not relevant to the question asked\n- 1: No answer is returned by the pipeline\n- 2: The answer is partially relevant to the question asked\n- 3: The answer is relevant to the question asked\n\nBased on the evaluation, the following table shows the performance of the different models:\n\n| Pipeline | Incorrect Answer | No Answer | Partially Relevant Answer | Fully Relevant Answer |\n| --- | :---: | :---: | :---: | :---: |\n| Pre-training Llama-2-7b-hf | 0% | 100% | 0% | 0% |\n| Fine-tuning Llama-2-7b-hf| 22% | 0% | 40% | 38% |\n| RAG with GPT-4 | **0%** | **32%** | **4%** | **64%** |\n\nThe detailed question-answer evaluation can be found in the `performance_testing` folder and the analysis can be found in the `test_answers_analysis.xlsx` file.\n\n&nbsp;\n## Learnings from the Research\n\n- **Pre-training LLMs:** With the pre-training process, the assumption was that the model will gain parametric memory of the domain data and will be able to answer questions related to the domain. However, the pre-trained model was not even able to generate english words properly. On further investigation, there seem to be 3 main reasons for this:\n    - **Data:** The data scraped from the websites was not enough and contains a lot of HTML tags and non-unicode characters. The original data used for pre-training the Llama-2-7b model is billions of webpages and documents which translates to trillions of tokens where as through web scraping, we were able to get only 1.5 million tokens.\n    - **Hyperparameters:** The hyperparameters used for pre-training the Llama-2-7b model are the same as the ones used for pre-training the original Llama-2-7b model. However, the original model is trained on billions of webpages and documents and hence, the hyperparameters are tuned for that amount of data and need to be tuned for the amount of data used for pre-training the model.\n\n- **Fine-tuning LLMs:** With the fine-tuning process, the assumption was that the model will learn the task of generating answers given a question as the input. However, the fine-tuned model was also able to gain parametric memory of the domain data and was able to answer questions related to the domain. On further investigation, there seem to be 2 main reasons for this:\n    - **Data:** The data created for fine-tuning, i.e., the question-answer pairs generated using the OpenAI API is very good and is able to capture the domain knowledge.\n    - **Original model weights:** As the LLaMA2 model in its vanilla state has great performance, fine-tuning just a small amount of weights (5%) had two fold advantage - (1) it was able to use the original vocabulary, grammar and sentence formation knowledge of the english language (2) it was able to gain parametric memory of the domain data and was able to answer questions related to the domain.\n\n&nbsp;\n## Future Work\n\n- **Pre-training LLMs:** The pre-training process can be improved by using more data and tuning the hyperparameters for the amount of data used for pre-training the model. The pre-training process can also be improved by using a better tokenizer and using a better data cleaning process.\n- **RLHF:** For this research, Reinforcement Learning from Human Feedback was not implemented and can prove to be really powerful for the model to be able to learn human preferences and generate answers accordingly.\n\n&nbsp;\n## Project Structure\nThe project structure is as follows:\n```\n├── data                                        <- directory for project data (text data scraped from websites)\n├── finetune                                    <- directory for fine-tuning scripts\n    ├── adapter_v2.py                           <- script to fine-tune LLMs using Adapter PEFT\n    ├── full.py                                 <- script to fine-tune LLMs using all the model weights\n├── generate                                    <- directory for inference scripts\n├── lit_gpt                                     <- directory for LIT-GPT code\n├── logs                                        <- directory to store training logs\n├── notebooks                                   <- directory to store any exploration notebooks used\n├── performance_testing                         <- directory to store performance testing data\n    ├── test_answers_analysis.xlsx              <- analysis of the answers returned by the pipeline\n├── scripts                                     <- directory for pipeline scripts or utility scripts\n    ├── convert_hf_checkpoint.py                <- script to convert a HuggingFace checkpoint to a LIT-GPT checkpoint\n    ├── download.py                             <- script to download the pre-trained LLMs from HuggingFace\n    ├── get_subpages.py                         <- script to get subpage urls of a website\n    ├── prepare_finetune.py                     <- script to tokenize the processed data and create torch datasets\n    ├── prepare_pretrain.py                     <- script to tokenize the processed data and create torch datasets\n    ├── process_finetune_data.py                <- script to process the scraped data for fine-tuning\n    ├── process_pretrain_data.py                <- script to process the scraped data for pre-training\n    ├── scrape.py                               <- script to scrape data from the list of subpages\n├── .gitattributes                              <- git attributes file\n├── .gitignore                                  <- git ignore file\n├── config.json                                 <- config file to store api keys\n├── LICENSE                                     <- license file\n├── README.md                                   <- description of project and how to set up and run it\n├── requirements.txt                            <- requirements file to document dependencies\n```\n\n## References\n\n- [Lightning-AI/lit-gpt](https://github.com/Lightning-AI/lit-gpt/tree/cf5542a166d71c0026b35428113092eb41029a8f)\n- [Farm-Haystack](https://haystack.deepset.ai)\n- [HuggingFace Transformers](https://huggingface.co/transformers/)\n- [ChatGPT API](https://platform.openai.com/docs/guides/chat)"
    },
    {
      "name": "philiphess1/CustomerServiceChatbot",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/106563229?s=40&v=4",
      "owner": "philiphess1",
      "repo_name": "CustomerServiceChatbot",
      "description": null,
      "homepage": null,
      "language": "CSS",
      "created_at": "2023-09-12T18:40:10Z",
      "updated_at": "2024-10-14T18:51:38Z",
      "topics": [],
      "readme": ""
    },
    {
      "name": "Oushesh/Impact_Nexus005",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/5883845?s=40&v=4",
      "owner": "Oushesh",
      "repo_name": "Impact_Nexus005",
      "description": "Smart_Evidence for Impact_Nexus",
      "homepage": null,
      "language": "Python",
      "created_at": "2023-11-14T08:25:03Z",
      "updated_at": "2024-01-29T18:48:04Z",
      "topics": [],
      "readme": "## Getting Started:\n    Every installation is based on makefile. however you can also see the make file\n    and run the commands separately.\nInstallation:\n    \n    make install-dev (for development)\n\n    make install-prod (for production)\n\n    make download-data (download data from internet sources)\n\n    make install-gcp\n    (install google cloud services)\n\nSync models between GCP and local:\n   \n    make sync-models-local\n    make sync-models-gcp\n\nSync logs:\n    \n    make sync-logs\n\nRun:\n     \n    make run\nspin off the django server.\n    (core to all services provided with different to previous codebase with multiple projects)\n\n    make run-docker\n(run all core services on docker)\n    \n    make run-docker-gpu\n\n(run all core services on docker with gpu)\n\nPrecommit:\n\n    make install-precommit\nInstalls precommit.\n\n    make run-precommit\nruns precommit.\n\n## Changes Made: Why?\n   \n  1. The project given was more experimental and not ready for production.\nBelow find the diagram from the overall pipeline implementation, with highlights \non how parts of the project were proposed to be readapted and how I propose to unify\nthe entire solution into services.\n\n  2. Designed a Service Based Architecture: django + ninja api backend. why? Fast API is good and easily adaptable to\n     django. Django has better integrations like FSM (finite state machine if needed in future.) + more library components\n\n  \n 3. Previous Project had pipeline which was from different projects: Spacy, HeyStack and fastapi\n    \n    For production one should not use Spacy: Spacy provides language models but there are good alternatives: like NLTK with\n    stemming and lemmatization. OpenAI Ada is currently the best embedding with 1279 Vector sofar surpassing Gloves, BERT, WORD2Vec\n    etc..\n\n    For pipeline use:\n    \n    \n    CI/CD of github workflow + Data Version Control (DVC) (Data as code and git) + CML + Deep Checks ML functionality\n            testing + WANDB to visualize the results.\n    \n    \nI adopt an API first development approach: \n    \n    Data Scientis tests on google colab or jupyter notebook --> \n    Ninja API makes it easy to add endpoints. -->\n    Same server on django are used by developers, data scientists\n    and data stakeholders. Its just different endpoints.   -->\n    Use DVC service to efficiently test parts of pipeline modularly.\n\n   Below find the diagram of the overall pipeline\n\n![overall_pipeline](docs/ML_OPS.png)\n   \nThe pipeline consists of the Services. Services cater for manual processes for different stakeholders within a company:\n\n1.Django service provides endpoints for:\n        \n    \n    A: Data Optimisation (from .xlsx,csv,.tsv,.pdf, etc... to unified format conversion to parquet) \n       Services/services_app/api/routers/parquet_conversion.py\n        \n        \n    B: Builds knowledgebase (takes data from master knowledge folder to produce a jsonl knowledge base with source folder and subfolders as metadata and \n       This in turn can be later used with neojs for data visualisation. \n       Services/services_app/api/routers/build_knowledgebase.py\n        \n    C: Incoming Data Check.\n        a suite of tests thats test the incoming, gets its embeddings (can be later used to semantically cluster and check for Data or label\n        drifts).\n\n\nIt consists of Services with the following directories. In General it's a django project.\n\n```\n    Services\n    │   DVC\n    │   Airbyte (should be a different repo: airbyte github clone) \n    │   services_app\n    │   │   api\n    │   │   │   \n    │   │   └─── routers\n    │   │   └─── build_knowledgebase.py\n    │   │   │    └─── data_processor_classification_job.py\n    │   │   │    └─── incoming_data_deepcheck.py\n    │   │   │    └─── model_downloader.py\n    │   │   │    └─── parquet_conversion.py\n    │   │   └─── schemas\n    │   │        └─── annotationOUT.py\n    │   │        └─── DataOUT.py\n    │   │        └─── ImpactScreeningOUT.py\n    │   │        └─── rest_api_schema.py\n    │   services_project\n    │   │   \n    │   wandb  (service to get the visualisation on the quality of the incoming data)\n    │   haystack_core_components\n    │   JOBS\n    │   KnowledgeBase\n    │   KnowledgeBaseParquet\n    │   logs\n   ```   \n    \n   The improvements I proposed are based on \n   * Python Programming style\n   * Data Costs and Scalability\n   * Data Download, Data Ingestion and Data Aggregration\n   * Incoming Data Quality Check, Data Drift \n   * MLOPs, Data Versioning, Continuous Machine Learning (CML), CI/CD ML Suite Test\n    \n   In each case I reported why and how.\n\n## Services in Action:\n    \n1. The previous pipeline misses a lot on data quality check, data-imbalancement, data drifts, labels-drift.\n    DeepCheck is really good framework that integrates lots of checks. \n    \n    \n    Endpoint: Services/services_app/api/routers/incoming_data_deepcheck.py\n    Choose data <postprocessed_data>: Services/services_app/JOBS/classification/target/dfg..*.csv   \n\nThis service will compute all important properties of the input data ready for a JOB: classification.\nRemember: data is always processed for a given job. Example here: \n\n    add train_test_split + polarity (class for classification) --> job classification, entity recognition, etc...\n    \n![DeepCheck](docs/DeepCheck_001.png)\n![DeepCheck](docs/DeepCheck_002.png)\n![DeepCheck](docs/DeepCheck_003.png)\n![DeepCheck](docs/DeepCheck_004.png)\n![DeepCheck](docs/DeepCheck_006.png)\n![DeepCheck](docs/DeepCheck_007.png)\n![DeepCheck](docs/DeepCheck_008.png)\n\n    \n    This service can be hosted on any cloud service with GPU and embeddings on new stream of data calculated\n    such that data is analysed to find anmomaly before being used to train new models or current ones.\n   \n    This service integrates WANDB (https://wandb.ai/site) and each session is logged and can be extended to save\n    as experiment on the cloud. \n\n    For every job type: a different transformation is required and found under:\n    Target: Services/services_app/JOBS/Classification/target/ \n    Source: Services/services_app/JOBS/Classification/source/\n\n    Only 2 JOBS are required: entity recognition (similar to classification) and emotional classification (special form\n    of classification). \n\n    Spacy should not be used. Levenstein method like you are using is good if characters are misspelled but the core\n    algorithm should focus on differentiation by semantics (use semantic embeddings like OpenAI ADA model or hugging face\n    language models).\n\n\n2. Knowledgebase Service:\n    \n    \n    make run\n\n\nEndpoint: \n    \n    Services/services_app/api/routers/build_knowledgebase\n   \nInput: \n\n    <Path to Knowledgebase> e.g. enter \"Knowledgebase\"\n\nOutput: \n\n    <JSONL of the entire database as knowledgebase>\n\nI complment the idea of using NeoJS. Its a good decision.\n    \n3. Data Optimisation\n\n\nEndpoint: \n        \n    Services/services_app/api/routers/parquet_conversion_service\n    \nInput: \n    \n    <Path to Local Folder containing Knowledgebase> e.g. Knowledgebase\n       \n\nOutput: <optimised KnowledgeBaseParquet> \n    \n       Its a service that can be easily deployed and added to data optimisation pipeline. (see cost and advantages below: Data Costs and Scalability)\n\n![KnowledgeBase](docs/BuildKnowledgeBase.gif)\n\nIt go through the subfolders of the data maintaining the integrity of the folder strucuture,\nread every file in:\n    \n    json,xml, csv, tsv,xlsx,txt\n\nformat and fill any empty rows, check for NaN values and produce apache parquet files  in the \nsame folder structure as the input. This can in turn be synced to a cloud bucket. eg. GCP Bucket or s3.\n\n4.  Classification JOB Transformation Service\n    \nEndpoint: \n\n    Services/services_app/routers/process_file.py\n    \nInput: \n    \n    <Path to file corresponding to job>\n    \nOuput: \n\n    <Processed file ready for semantic analysis Job> \n    \nThis endpoint is really dynamic: it take any file .csv or best optimised parquet and since emotional/sentiment\n    analysis can be a multivariate problem: user can select the input from the table and output label is added along\n    with train_test split. The other columns are treated as metadata and can be used to further enhance classification.\n    \nThis is JOB Transformation service.\n    The NLP Jobs required in this project are classification and language modelling. \n   \nThe raw Data can be synced with Google Bucket live and the transformed Data in a different bucket. The data transformation\n    is dependent on the JOB here defined under: \n\n    \"Services/services_app/JOBS/Classification/source\"\n\n\nUnder:\n    \n    \"Services/services_app/JOBS/LanguageModelling\" \n\nall the functions that compute co-coccurence matrix or simply\n    any bayesian method primitive language model can be written and served under one endpoint. The language model\n    is then saved under: Google Bucket or any S3 or Athena. My Buckets are under:\n\n    gs://jobs_impactnexus\n\n\n## Python Programming Style \n   Pre-conmmits containing blake, isort, github CI/CD with Continuous Machine Learning\n\n## Data Costs and Scalabitliy: \n   Data comes in different formats: pdf,csv,tsv,xslx,json,jsonl, etc..\n   I propose to convert them to apache parquet both for faster data transfer and data readability purposes leading\n   to massive cost savings as data becomes enormous.\n    \nIn perspective:\n![Apache_Parquet_Advantages](docs/Apache_Parquet_Advantages.png)\n\n## Data Download && Data Ingestion && Data Aggregation\n   \n   1. The Django Service has an endpoint to download data for example from \"Oekobaudat Service\". As a company you can  spend time\n   developing endpoints and curating data or use Airbyte (like i showed in the diagram at the beginning) to build robust Data Connectors.\n   This will save development time. Airbyte can also be used to  build Airtable or any third party service api quickly and robust.\n\n## Data Visualisation Tool\n   1. WANDB + Deep Checks.\n     https://wandb.ai/concular/deepchecks?workspace=user-oushesh\n     https://deepchecks.com/\n\n   Below see endpoint (Services/services_app/routers/deepcheck) proviing insights \n    on a processed data for a classification job.\n\nRaw Data: \n    \n    Services/services_app/JOBS/Classification/source\n\nProcessed Data: \n\n    Services/services_app/JOBS/target/..*.csv\n    \nEndpoint: \n    \n    Services/services_app/api/routers/data_processor_classification_job.py)\n\n![wandb_DeepChecks_001](docs/DeepChecks_001.gif)\n![wandb_DeepChecks_002](docs/DeepChecks_002.gif)\n![wandb_DeepChecks_003](docs/DeepChecks_003.gif)\n\nThose 2 tools provide a framework to build a test suite and get embeddings from incoming data to visualize\nproperties such as label and data drift and other checks for data.  \n\nPipeline can be extended to accommodate more test suites with the Framework of DeepChecks.\nRaw Data is transformed to task specific job. \n\nPipeline Example here:\n\n![Data_incoming_check_pipeline](docs/DeepCheck_Embedding_Service.png)\n\nThis endpoint can be served as a lambda function on AWS or any online platform. It\n    would sync upon new incoming data and perform the transformation for the job as \n    well as run the Embeddings to check for Data Drift, label drift, new words, percentage of \n    new characters thus check if new Data domain is shifted too much even before training starts.\n    It promotes better decision making and transparency after models have been pushed to production.\n    (understand decision making).\n  \n   2. The old pipeline does not account for any service to get semantcs of incoming data: \n      I propose passing the entire database to a\n     vector database like Qdrant or Pinecone or Weaviate with OpenAI Ada Embeddings to get a semantic \n     graph of the data\n     API to classify data.\n\n## Data Retraining and Versioning.\n   Data Version Control (DVC) and check the next part of the instruction\n \n## ML OPS && CML (Continuous Machine Learning) \n   \n   The previous pipeline did not account for the OPS and Continuous Machine Learning. CML + DVC \n   is one of the best framework to achieve this. DVC treats data and models like Git files with hash. \n   Upon retesting an old pipeline: even only pipelines that have been changed, only scripts\n   that were changed or models whether local or online that were changed will be used for testing.\n\n\n   Using DVC and CML allows multiple people to work in different branches simultaneously on the same part \n   of the pipeline or different parts of the pipeline and push code. The .yaml is written such that only\n   changes in the specific folder corresponding to the tests gets triggered saving time and being more efficient.\n\n   * Github actions (.github/workflows/<test.yaml>) + Data Version Control  \n\n   1. Data Version Control: \"Git for Data\"\n        \n    dvc get \n   2. downloads any data from a url pointing to s3, google bucket\n            or other cloud services and saves where you want.\n            It uses hash map like git to efficiently track and cash changes.\n\n    Services/DVC contains the different workflows\nExample of Data Versioning:\n    \n    dvc get gs://dvc_models_bucket/models.pkl -o Services/DVC/models/models.pkl\n    \nTrack: \n    \n    dvc add Services/DVC/models/models.pkl\n    \nAny changes people working with you did either on the bucket or new model \nit gets pushed and tested with the yaml. The test gets triggered and pipeline is evaluated.\n\nExample: Services/DVC/test \n   \n     A. Train script model was changed from LinearRegression to Lasso Model.\n     B. The change triggers the test written under \n        .github/workflows/test.yaml\n     C. dvc can also be use to track the model with the bucket in google cloud: \n        gs://dvc_models_bucket/models/\n\nExample below the Regression model was changed and test is triggered:\n\n    Services/DVC/linear_regression_test\n    \n\n![LassotoRegression](docs/lasso_regression_test.gif)\n2. Advantage of DVC: it can compress GBs of data in bytes of metadata where the data sits on the cloud: s3, GCP. gdrive \n   or anything else.\n\nEvery github workflows corresponds to the bucket names on google bucket or any other service.\n\n![Workflows](docs/all_workflows_yaml.png)\n\n\n\n![Workflows](docs/google_bucket.png)\n    \nEvery DVC Experiment is:\n    \n    ```\n    Services/DVC/<Name of Workflow or pipeline> \n    │   model/\n    │   data/    \n    │   test.py\n    │   train.py\n    │   other_scripts.py\n    │   requirements.txt\n    ```\n\n4. Data Pipelines: (dubbed as Makefile for ML Projects along with CML(Continuous Machine Learning))\n     Usually pipelines are connected with different steps in different.py files.\n\n      Changing anything in the train or any other script will only trigger that script. When your pipeline gets huge\n      and uses GPU for training. Only the changes in script will run the results of the users that did not change\n      will be retrieved and only the specific script with changes will run.\n\n\n5. ML OPS and Robustness:\n\nPerturbation test is used to test robustness of a given pipeline. The inputs are perturbed\nand the model from hugging face like most of the models used at Impact Nexus are used.\nThis was missing in the old pipeline. The pipeline  \n   \n    Workflow: .github/perturbation_test.yaml \n    \nTriggers: when there are changes in the  Services/DVC/pertubation_test/\nwhether data or model or parameters of training. \nSee .gif below\n\n![DVC_Perturbation_test](docs/dvc_perturbation_test.gif)\n    \nOther tests are mentionned in the department of Data and Label Drift as well as Embeddings Drift\n\n\n\n6. Workflows Requiring GPUs:\n\n   Heavy Training requiring GPUs cannot be run on github directly. Instead spin off GPU service and attach the worker. \n   Or: use local GPU. (PS: I dont have on my mac m2 pro.)\n   \n   A. \n        \n        GPU nvidia-docker cml aws ec2 deep learning AMI\n        Choose aws ec2 deep learning AMI Ubuntu 18.04\n        ssh into your ec2 instance from that terminal you can work into \n        your pycharm or terminal.\n\n   B.    \n        \n        run nvidia-smi\n        Check all the gpus in the docker\n        docker run --gpus all dvcorg/cml-py3 nvidia-smi\n       \n        This will check if cml image of nvidia-smi is available otherwise\n        it will pull it.\n        \n        You should see your gpu on terminal\n   \n   C. \n        \n        Next step is to connect GH (github-actions) with docker. (I personally\n        dont have GPU on my mac M2)\n\n        Reference: https://www.jeremyjordan.me/testing-ml/\n    \n       docker run --name gpurunner -d --gpus all -e RUNNER_IDLE_TIMEOUT=1000 -e RUNNER_LABELS=cml,gpu -e RUNNER_REPO=\"url_to_your_github_repository\" -e repo_token=\"your_personal_access_token\"\n    \n       This gpu docker cml can run in the background. This will be a self-hosted runner running on your ec2 instance or gcp.\n\n       The CML docker container will listen to the workflows from github or gitlab.\n        \n        You can autoscale the workflows on self-hosted gpu machines for lots of developers\n        if they exist: https://docs.github.com/en/webhooks/webhook-events-and-payloads#workflow_job\n\n## Data Version Control (DVC) \n    Services/DVC\n\nOne of the most powerful updates to the previous pipeline I provide is the use of dvc which as shown in the gif\nhas the power to exectute only those steps of the code which are changed. As the pipeline grows more and more complex\nonly parts that are changed are retrained leading to faster and safer development.\n\nData Version Control allows retraining without chaning all parts of the pipeline.\nYou can also visualize the Direct Acycic Graph of the pipline using\n    \n    dvc dag\n\nWhile there are other options like Apache Airflow, DVC and CML is preferred since it provides:\nVersioning of Data, Continuous ML, CI/CD\n\nBelow you see when the parameter in the script of the pipeline is changed. The tests are rerun.\n\n    dvc repro\n\n![DVC_Reprogrammable_Code](docs/dvc_repro.gif)    \n\n## Tests: \n\n    make test\n   \n   All tests written with pytest, pytest.fixtures. The tests can be extended to all endpoints and pass.\n   Logic tests can be written like the ones shown here. But also, integration test as well as data injection\n   test.\n    \n   Below you see the test for the parquet conversion service endpoint \n   under Services/api/routers/parquet_conversion functionality.\n\n   All the endpoints will have a different test script here under the folder tests\n   triggered by the make file. This allows for quick testing using just make test\n   command.\n\n\nMore tests can be added using this structure.\n\n![Test](docs/make_tests.gif)\n   \n\n   The tests are written with pytest.ini and every endpoint can tested as such. The tests written in the old repo can \n   be easily adapted. \n\n\n## Logging System: \n   The entire logging system is based on python module logging and the logs are synced on the cloud on supabase.\n   Each endpoint should have a logging system recording. \n\n   The system has better debuggability with different levels of logging ranging from:\n   * info\n   * error\n   * warning\n   * critical warning\n\n   The logs have timestamps and chronologically in order for reference.\n\n   When the project becomes really large as extension I propose to change\n   the naming of the file and add <_underscore_date_and_time> as well as uploading to\n   postgresql or supabase on gloud service. SQL are fast to query and read.\n    \n   For further development: I would design a cron job which would periodically sync\n   all the log files on the cloud (gcp) and delete old ones (local.) I dont prefer\n   a direct writing on the cloud since it will hinder speed of execution of the \n   process in general. \n   Now the logs are as .logs and synced with local at:\n    \n    gs://logs_impactnexus\n\n![Logging System](docs/logging_system.png)\n![Logging GCP](docs/logs_gcp.png)\n\n## Airbyte: \n  \n   1. Why airbyte and no standard development of endpoint if only data connectors are needed? \n   Well for building endpoints that connect a source data source to another destination \n   (example Google query or aws s3 to postgres sql). \n\n   Engineers save lots of time avoid repetitive tasks like Schema definition, pagination,\n   payload, auth, etc..\n\n   Example of oekobautdat (a really important dataset for ESG metrics) set data downloader\n   as shown by Airbyte Connector.\n   \n   2. The second big advantage of Airbyte is that it has built-in Data built-tools (DBT) which\n      is a fantastic innovation treating data (sql or table data) as modular components and reused.\n\n    (Check my first diagram on the services)\n![Airbyte_Connector](docs/airbyte_connector.gif)\n\n## Haystack \n   1. Personally I welcome the idea of using Haystack. As I am consulting\n   various companies using LLM these days \n\n   2. I don't see any technical debt coming in the future with Haystack.\n      A: It integrates well with the best new Vector Databases \n          such as: Weaviate, Qdrant, Pinecone.\n      B:  LLM forms a big part in NLP and in the near future it will brought to\n          Impact Nexus as well.\n      \n      C:  Haystack is good for building RAG and Semantic Vector retrieval pipeline.\n      D:  Custom Components can be added easily. Haystack_core_components (Services/services_app/haystack_core_components)\n          contains pipeline builder and other classes to abstract DocumentStore. Using those components I show how pipeline\n          can be easily tested as endpoints. I focus on modular code easily moving from prototype to endpoints under services.\n          For production one should move away from InMemory.  (InMemoryDocumentStore()) --> instead use ElasticSearch or any cloud service.\n\n   3. Since the Pipeline development in Haystack is focussed on nodes (similar to tensorflow or keras code flow): \n      it's easy to write each one in a different script and build a DAG (Direct Acyclic Graph) in DVC and CML. (see above)\n      Pipeline is easily tested using Data Version Control.\n\n      All endpoints have data synced on Google Cloud as well as their logs.\n   \n   4. Its also easy to finetune the models available from huggingface and the ones in pytorch\n      https://haystack.deepset.ai/tutorials/02_finetune_a_model_on_your_data\n\nBelow see an example of a crawler. \n    \n    make run\n    api/crawler/crawler\n    url: https://remyx.ai\n\nYou will see the crawling process in the terminal and a sample retrieval pipeline. \n\n    This pipeline  calls Haystack_components @ Services/services_app/haystack_core_components --> pipeline_builder.py and\n    DocumentStores.py\n\nThe pipeline scrapes the data and performs a query.\n\n![crawler_example](docs/Haystack_crawler_002.gif)\n\n\n## Summary:\n   1. My pipeline uses Django as service which integrates all the needed tools for decision making. \n      FastAPI is good but lacks good libraries like Finite State Machine if needed among others.\n   by the Domain Expert people (people knowing how to calculate LCA, ESG etc..). It gives them\n   tool to visuallise data.\n\n   2. My tool proposes Data optimisation from multiple formats from multiple sources to Apache Parquet Format\n      to save labour and computer cost\n   3. My tool integrates Airbyte thus saving development on data connectors and avoid writing endpoints to transform\n      data from already established 3rd parties like Airtable, s3, athena or posgresql.\n   4. My tools integrates Data Quality Check and Drifts using DeepCheck and more task specific tools can be added easily\n      and called to the endpoint. Every session is automatically saved on WANDB (https://wandb.ai/) and dynamic plots accessed\n   5. My tools integrates DeepCheck and its data transformation (specific for task: like Emotion/Tag Classification)\n   6. makefile to ease running commands around like testing and server running\n   7. Provides a comprehensive on how to do fast Development from jupyternotebooks to directly test pipeline changes using DVC and CML and build new endpoints\n      to test new services which are already robust.\n   8. Owing to use of DVC I can easily test big time consuming pipelines where only changes in 1 script  are rerun\n      after \"dvc repro\". I provide a vision for Data Versioning, Model Versioning  as well as Model Registry\n      (DVC integrates online services where models are saved: here gcp). \n   9. I provided a very modular code development process where most of the components from haystack that you used\n      under rest_api_dev and other pipelines can be extended under: Services/haystack_core_components which then \n      can be imported into new endpoints under: Services/api/routers and schema under Services/api/sehema.\n        \n       Most of services from here can be imported as endpoints i have developed.\n       \n     `download-pipeline` \n    `download-insight-classifier`\n    `download-boolqa-concept-relation-classifier` \n    `create-documents` \n    `upload-corpus` | Upload all corpus |\n    `download-corpus` | Download all corpus |\n    `download-test-corpus` | Download test corpus |\n    `package-and-install-entity-pipeline-cpu` | Create spacy package for en_ix_entity_ruler pipeline |\n    `package-and-install-entity-pipeline` | Create spacy package for en_ix_entity_ruler pipeline |\n    `upload-package-entity-pipeline` |  |\n    `extract-documents` | Extract document from scrape index. |\n    `extract-insights` | Extract document elements, e.g. paragraphs, abstract from documents. |\n    `annotate-insights` | Annotate insights |\n    `generate-annotation-tasks` | Annotate insights |\n    `serve-dev` |  \n\n   10. Spacy is completed banned. For validating a service that is ready for production just add an \n       endpoint as \"/serve/...\" in Django then test inference live. \n   11. I have radically changed the philosophy of the project but maintained integrity of the \n       the components used like Haystack. Code is modular and components can be reused. I support haystack. The project now integrates WANDB, DeepCheck, Data Verion\n       Control (DVC) + CML, GCP or any other service and airbyte and caters for different people\n       within the team trying to avoid manual tasks. \n   12. I added a sample perturbation_test using DVC, to accommodate for future robustness of models when data is corrupted.\n\n    .github/workflows/perturbation_test.yaml \n\n\n\n## Further Development: \n   * Terraform for multiple application deployment and application life cycle. \n   * Cron jobs to sync data daily multiple times and extend our pipeline for data check. I would personally avoid real time incoming data check.\n   * add a deploy.sh and a new .github/workflows to deploy new endpoints automatically should tests have been passed and new models trained.\n     (Combine DVC,CML with deploy.sh bash file)\n\n## Resources:\n    https://iterative.ai/blog/cml-self-hosted-runners-on-demand-with-gpus\n    (how to self-host runners on gpus)\n  \n\n\n\n"
    },
    {
      "name": "Siddhartha082/Text_to_Image_Fashion_Search_Streamlit",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/110781138?s=40&v=4",
      "owner": "Siddhartha082",
      "repo_name": "Text_to_Image_Fashion_Search_Streamlit",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2023-11-27T06:34:06Z",
      "updated_at": "2024-07-24T14:30:31Z",
      "topics": [],
      "readme": "# Fashion Search App _using_Streamlit_APP\nFashion Search App (Text to Image Search) using AI Model.\n\n#### You can search in your image base using query (Text to Image Search). \nI demonstrate how to create a fashion clothing text-to-image search app using Streamlit as the user interface. You'll learn how to harness the power of Vision Transformers (VIT), CLIP, and the Haystack framework to build this innovative application. Simply input your fashion query, such as \"denim jackets,\" and watch as the app fetches and displays three relevant denim jacket images.\n\nCLIP ViT: https://huggingface.co/sentence-transformers/clip-ViT-B-32\n![image](https://github.com/Siddhartha082/Text_to_Image_Fashion_Search_Streamlit/assets/110781138/3f3e379d-81b9-4fa8-9d0b-4f675e5e9899)\n\n\nHaystack: https://haystack.deepset.ai/\n\n![image](https://github.com/Siddhartha082/Text_to_Image_Fashion_Search_Streamlit/assets/110781138/123ae8d6-53d1-4a17-a334-967421ae240f)\n\n# Search – Denim Jackets\n\n![image](https://github.com/Siddhartha082/Text_to_Image_Fashion_Search_Streamlit/assets/110781138/c63181b4-6e4e-4200-9e4a-d5f382933b43)\n\n![image](https://github.com/Siddhartha082/Text_to_Image_Fashion_Search_Streamlit/assets/110781138/0f0aa789-40d4-4f87-a5db-462a017e72a1)\n\n![image](https://github.com/Siddhartha082/Text_to_Image_Fashion_Search_Streamlit/assets/110781138/3012091e-81c9-481c-a534-6e7a5a0126fd)\n\n![image](https://github.com/Siddhartha082/Text_to_Image_Fashion_Search_Streamlit/assets/110781138/760c77f2-db55-4055-a2eb-892ccfe64509)\n\n![image](https://github.com/Siddhartha082/Text_to_Image_Fashion_Search_Streamlit/assets/110781138/27f58c32-4b5c-4fe5-bb2a-4d9e5e2a199b)\n\n![image](https://github.com/Siddhartha082/Text_to_Image_Fashion_Search_Streamlit/assets/110781138/7e17e054-754a-472e-afbb-4fb2e92f0459)\n\n![image](https://github.com/Siddhartha082/Text_to_Image_Fashion_Search_Streamlit/assets/110781138/b0c623e7-02bf-44cc-a425-2bd565d931e6)\n\n![image](https://github.com/Siddhartha082/Text_to_Image_Fashion_Search_Streamlit/assets/110781138/c8671cc8-01d3-454a-9b04-40fdb7d31784)\n\n![image](https://github.com/Siddhartha082/Text_to_Image_Fashion_Search_Streamlit/assets/110781138/1138af6c-a26e-4ac4-bb9c-0c9876c00217)\n\n![image](https://github.com/Siddhartha082/Text_to_Image_Fashion_Search_Streamlit/assets/110781138/61dd0253-a419-4928-96ba-63dc906acf9a)\n\n![image](https://github.com/Siddhartha082/Text_to_Image_Fashion_Search_Streamlit/assets/110781138/9d28d8c0-7cd5-44a9-90fd-54dc3d8186f0)\n\n![image](https://github.com/Siddhartha082/Text_to_Image_Fashion_Search_Streamlit/assets/110781138/3d732ec3-c414-4e7c-aa0a-f1eb7d7a2259)\n\n![image](https://github.com/Siddhartha082/Text_to_Image_Fashion_Search_Streamlit/assets/110781138/798894c6-6879-4b4b-a2a1-bba03dbcfc70)\n\n![image](https://github.com/Siddhartha082/Text_to_Image_Fashion_Search_Streamlit/assets/110781138/986a333d-5d9e-4352-afe6-f7b16a98667d)\n\n# Output1 -- LBD(input) -- Little Black Dress\n\n![image](https://github.com/Siddhartha082/Text_to_Image_Fashion_Search_Streamlit/assets/110781138/e6749cfd-1ca0-4a77-b8ed-01686431b5bc)\n\n![image](https://github.com/Siddhartha082/Text_to_Image_Fashion_Search_Streamlit/assets/110781138/f4ff67fa-6e08-450a-bacb-3e872417cfe5)\n\n# Output 2  -- Denim Jackets --\n\n![image](https://github.com/Siddhartha082/Text_to_Image_Fashion_Search_Streamlit/assets/110781138/d57d1c5d-3e2c-44ad-b390-7ccf4b9d7d55)\n\n![image](https://github.com/Siddhartha082/Text_to_Image_Fashion_Search_Streamlit/assets/110781138/57b7b09b-3e1b-4386-a4d2-2c5d8051c2b6)\n\n\n\n\n"
    },
    {
      "name": "a-romero/qrage",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/7581333?s=40&v=4",
      "owner": "a-romero",
      "repo_name": "qrage",
      "description": "Modular framework for building Retrieval Augmented Generation (RAG) pipelines",
      "homepage": "",
      "language": "Python",
      "created_at": "2023-11-25T17:08:01Z",
      "updated_at": "2025-03-13T13:07:22Z",
      "topics": [
        "ai",
        "genai",
        "rag"
      ],
      "readme": "# qRage - Modular framework for building Retrieval Augmented Generation (RAG) pipelines\n\n<p align=\"center\">\n    <img src=\"images/qRage.png\" width='1024'>\n</p>\n\n## Introduction\nThis framework provides flexible Retrieval Augmented Generation (RAG) by integrating with frameworks like Haystack and LlamaIndex. It uses parametrized configurations to leverage various transformer and LLM models. Key features include:\n\n- Integration with Weaviate vector database and Nebula graph database.\n- Custom retrievers combining Hybrid Search, Embedding Fine-tuning, Generative Pseudo Labelling, and ReRanking techniques.\n\n## Installation\nClone the repository:\n```bash\ngit clone https://github.com/a-romero/qrage.git\ncd qrage\n```\n\n## 🐳 Starting Weaviate Vector Database\n### Standard Setup\n1. Navigate to the standard directory:\n   ```bash\n   cd vectordb/weaviate/standard\n   ```\n2. Start the Weaviate instance using Docker:\n   ```bash\n   docker-compose up -d\n   ```\n\n### Advanced Setup\n1. Navigate to the advanced directory:\n   ```bash\n   cd vectordb/weaviate/advanced\n   ```\n2. Start the Weaviate instance using Docker:\n   ```bash\n   docker-compose up -d\n   ```\n*Note:* The Weaviate service becomes available locally on http://localhost:8080\n## 🐳 Starting Nebula Graph Database\n1. Navigate to the standard directory:\n   ```bash\n   cd graphdb/nebulagraph\n   ```\n2. Start the Nebula instance using Docker:\n   ```bash\n   docker-compose up -d\n   ```\n*Note:* The Nebulagraph service becomes available locally on http://localhost:9669 with the default credentials root/nebula\n\n## Requirements\n- OpenAI API key exported to local env variable `OPENAI_API_KEY`\n- HuggingFace API key exported to local env variable `HUGGINGFACEHUB_API_TOKEN`\n- Cohere API key exported to local env variable `COHERE_API_KEY`\n\n## Usage\n### Data sources\nThe following file formats are supported:\n- .text\n- .pdf\n- .md\n- .docx\n\nFor data input, the following sources are supported:\n- Local file\n- Local directory (recursive ingestion)\n- HTTP(S)\n- S3\n\n### Indexing/Embedding pipeline\nThe embedding models supported are:\n- \"sentence-transformer\" => \"sentence-transformers/multi-qa-mpnet-base-dot-v1\"\n- (OpenAI) \"ada\" => \"text-embedding-ada-002\"\n- (Cohere) \"embed\" => \"embed-multilingual-v2.0\"\n\n```python\nhaystack_embed.embed(source=\"./data\", \n                    index_name=\"test\", \n                    recreate_index=True,\n                    batch_size=5,\n                    model=\"sentence-transformer\",\n                    dim=768,\n                    gpl=False,\n                    language=\"en\"\n                    )\n```\n\n### Retrieval/Querying pipeline\nThe generative models supported are:\n- (Mistral) \"mistral\" => \"mistralai/Mistral-7B-Instruct-v0.1\"\n- (TII) \"falcon\" => \"tiiuae/falcon-7b-instruct\"\n- (OpenAI) \"gpt-3.5-turbo\"\n- (OpenAI) \"gpt-4\"\n- (OpenAI) \"gpt-4-turbo\" => \"gpt-4-1106-preview\"\n- (Cohere) \"command\"\n\n#### Using a VectorDB RAG (with Weaviate):\n```python\nhaystack_generate.generateWithVectorDB(query=\"How would Revolut be impacted by AIG going bankrupt?\", \n                    index_name=\"test\",\n                    embedding_model=\"sentence-transformer\",\n                    dim=768,\n                    generative_model=\"gpt-4\",\n                    top_k=5,\n                    draw_pipeline=False\n                    )\n```\nResponse:\n```\nRetriever:  <haystack.nodes.retriever.dense.EmbeddingRetriever object at 0x7f39f1be1050>\nPrompt:  <haystack.nodes.prompt.prompt_node.PromptNode object at 0x7f390e2ce010>\nRanker:  <haystack.nodes.ranker.cohere.CohereRanker object at 0x7f390c3c1dd0>\nBatches: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 22.21it/s]\nAnswer:  {'answers': [<Answer {'answer': \"If AIG were to go bankrupt, Revolut could be impacted because it sells AIG car insurance. This could result in a loss of an insurance provider and potentially disrupt services for customers who have purchased AIG car insurance through Revolut. It might also impact Revolut's plans for launching mortgage loans.\", 'type': 'generative', 'score': None, 'context': None, 'offsets_in_document': None, 'offsets_in_context': None, 'document_ids': ['073d20d7-9223-eac5-9333-54d528bc5a91', '4a723bfc-a5fd-9905-3ec7-c71ffacc2ccb', '26f31a1e-7109-1b8c-c25c-9f5283be1a93', '03aefc09-9643-93c2-4e83-f343006077a9', '67835bb8-e6fb-1c6c-a246-6e74c5f9b9b3'], 'meta': {'prompt': \"Given the provided Documents, answer the Query.\\n\\n                                                Query: How would Revolut be impacted by AIG going bankrupt?\\n\\n                                                Documents: News • Jul 31, 2023\\nRevolut to sell AIG car insurance, may be closer to launching mortgage loans News • Aug 4, 2023\\nChannel News Asia — Revolut to stop crypto services for US customers\\nNews • Aug 2, 2023\\nFintech Singapore — Revolut Launches Instant Card Transfers to Over 80 Countries\\nNews • Aug 2, 2023\\nCrowdfund Insider — Revolut, Game4Ukraine to Raise Funds for Reconstruction of Ukrainian School\\nNumber of Articles\\n1,699\\nRevolut SAVE\\nGo back to Revolut's Signals & News\\x0c16/08/2023, 23:44 Revolut - Recent News & Activity\\nhttps://www.crunchbase.com/organization/revolut/signals_and_news/timeline 2/9\\nNews • Jul 31, 2023\\nRTE.ie — Revolut starts phasing in car insurance offering\\nNews • Jul 31, 2023\\nIrish Examiner — Revolut to sell AIG car insurance, may be closer to launching mortgage loans\\nNews • Jul 31, 2023\\nThe Independent.ie — ‘30pc cheaper rates’ promised – Revolut car insurance launches in Ireland today\\nwith a quote taking just ‘minutes’ on the app\\nNews • Jul 25, 2023\\nPYMNTS.com — Revolut Extends Accounts in US to Non-Citizens\\nNews • Jul 24, 2023\\nFinextra — Revolut launches joint accounts in the UK\\nNews • Jul 19, 2023\\nFStech — Monzo, Revolut and Wise demand ‘urgent review’ of hidden international fees\\nNews • Jul 14, 2023\\nSifted — Revolut moves closer to super-app status by adding tour and travel experience bookings\\nNews • Jul 14, 2023\\ntechbuzzireland — Revolut launches marketplace with over 300,000 tours, activities, and attractions\\nas it supercharges trips around the world\\nNews • Jul 14, 2023\\nAltFi — Revolut expands travel offering with in-app marketplace\\nNews • Jul 14, 2023\\nBusinessCloud — Revolut launches ‘Experiences’ marketplace\\nNews\n...\n```\n\nUsing a WebRetriever RAG:\n```python\nhaystack_generate.generateWithWebsite(\"Write a brief introduction of Revolut's CEO\",\n                                          domains=[\"crunchbase.com\"],\n                                          litm_ranker=True,\n                                          max_length=800\n```\nResponse:\n```\nPrompt:  <haystack.nodes.prompt.prompt_node.PromptNode object at 0x7f3bad9094d0>\n  return self.fget.__get__(instance, owner)()\nRanker:  <haystack.nodes.ranker.lost_in_the_middle.LostInTheMiddleRanker object at 0x7f3bad90ba10>\nAnswer:  {'answers': [<Answer {'answer': 'Nikolay Storonsky is the Founder and Chief Executive Officer (CEO) of Revolut, an innovative financial technology company. Prior to founding Revolut, he served as an Equity Derivatives Trader at Credit Suisse and Lehman Brothers.', 'type': 'generative', 'score': None, 'context': None, 'offsets_in_document': None, 'offsets_in_context': None, 'document_ids': ['f88e3d044dc68bf9d77f6ad8f08f5493'], 'meta': {'prompt': \"Given the provided Documents, answer the Query.\\n\\n                                                Query: Write a brief introduction of Revolut's CEO\\n\\n                                                Documents: Overview. Nikolay Storonsky is the Founder and CEO at Revolut. He is a former Equity Derivatives Trader at Credit Suisse and Lehman Brothers. Economic School.\\n                                                Answer: \\n                                            \"}}>], 'invocation_context': {'query': \"Write a brief introduction of Revolut's CEO\", 'documents': [<Document: {'content': 'Overview. Nikolay Storonsky is the Founder and CEO at Revolut. He is a former Equity Derivatives Trader at Credit Suisse and Lehman Brothers. Economic School.', 'content_type': 'text', 'score': None, 'meta': {'url': 'https://www.crunchbase.com/person/nikolay-storonsky', 'timestamp': 1701187258, 'search.score': 0.18181818181818182, 'search.position': 1, 'snippet_text': 'Overview. Nikolay Storonsky is the Founder and CEO at Revolut. He is a former Equity Derivatives Trader at Credit Suisse and Lehman Brothers. Economic School.', '_split_id': 0, 'score': '0.67864573'}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'f88e3d044dc68bf9d77f6ad8f08f5493'}>], 'answers': [<Answer {'answer': 'Nikolay Storonsky is the Founder and Chief Executive Officer (CEO) of Revolut, an innovative financial technology company. Prior to founding Revolut, he served as an Equity Derivatives Trader at Credit Suisse and Lehman Brothers.', 'type': 'generative', 'score': None, 'context': None, 'offsets_in_document': None, 'offsets_in_context': None, 'document_ids': ['f88e3d044dc68bf9d77f6ad8f08f5493'], 'meta': {'prompt': \"Given the provided Documents, answer the Query.\\n\\n                                                Query: Write a brief introduction of Revolut's CEO\\n\\n                                                Documents: Overview. Nikolay Storonsky is the Founder and CEO at Revolut. He is a former Equity Derivatives Trader at Credit Suisse and Lehman Brothers. Economic School.\\n                                                Answer: \\n                                            \"}}>], 'prompts': [\"Given the provided Documents, answer the Query.\\n\\n                                                Query: Write a brief introduction of Revolut's CEO\\n\\n                                                Documents: Overview. Nikolay Storonsky is the Founder and CEO at Revolut. He is a former Equity Derivatives Trader at Credit Suisse and Lehman Brothers. Economic School.\\n                                                Answer: \\n                                            \"]}, 'documents': [<Document: {'content': 'Overview. Nikolay Storonsky is the Founder and CEO at Revolut. He is a former Equity Derivatives Trader at Credit Suisse and Lehman Brothers. Economic School.', 'content_type': 'text', 'score': None, 'meta': {'url': 'https://www.crunchbase.com/person/nikolay-storonsky', 'timestamp': 1701187258, 'search.score': 0.18181818181818182, 'search.position': 1, 'snippet_text': 'Overview. Nikolay Storonsky is the Founder and CEO at Revolut. He is a former Equity Derivatives Trader at Credit Suisse and Lehman Brothers. Economic School.', '_split_id': 0, 'score': '0.67864573'}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'f88e3d044dc68bf9d77f6ad8f08f5493'}>], 'root_node': 'Query', 'params': {}, 'query': \"Write a brief introduction of Revolut's CEO\", 'node_id': 'PromptNode'}\n/usr/lib/python3.11/tempfile.py:895: ResourceWarning: Implicitly cleaning up <TemporaryDirectory '/tmp/tmp8g9evya6'>\n  _warnings.warn(warn_message, ResourceWarning)\n```\n\n#### Using a Custom Hybrid Retriever that leverages both VectorDB embeddings and a Knowledge Graph\n```python\nllamaindex_embed.kg_index(source=path,\n                           space_name=index_name\n                        )\n```\nResponse:\n```\n(Revolut, is making money accessible by, driving borderless finance in Latin America)\n(Revolut, says Latin America is, key region for growth)\n(Revolut, to boost crypto team by, 20% despite US exit)\n(Revolut CEO, on growing a multi-currency card into, financial super app)\n(Revolut, self-shutters US crypto business as, Coinbase moves to dismiss SEC suit)\n(Revolut and Elon Musk's X, can become, everything apps)\n(Revolut, to stop crypto services for, US customers)\n(Revolut, launches instant card transfers to, over 80 countries)\n(Revolut and Game4Ukraine, to raise funds for, reconstruction of Ukrainian school)\n(Revolut, starts phasing in, car insurance offering)\n(Revolut, sells, AIG car insurance)\n(Revolut, may be closer to launching, mortgage loans)\n(Revolut, launches, joint accounts in the UK)\n(Revolut, demands urgent review of, hidden international fees)\n(Revolut, moves closer to super-app status by adding, tour and travel experience bookings)\n(Revolut, expands travel offering with, in-app marketplace)\n...\n```\n\n```python\nretrieve_pipeline.get_response_with_VKBRetriever(\"How would Revolut be impacted by AIG going bankrupt?\",\n                                                generative_model=\"gpt-4\",\n                                                index_name=index_name,\n                                                space_name=index_name\n                                                )\n```\nResponse:\n```\nKnowledge Graph index:  <llama_index.indices.knowledge_graph.base.KnowledgeGraphIndex object at 0x7f67d0680550>\nWARNING - llama_index.service_context -  chunk_size_limit is deprecated, please specify chunk_size instead\nIf AIG were to go bankrupt, Revolut could be impacted as it sells AIG car insurance. This could potentially disrupt their insurance services and they might need to find a new insurance provider. However, the specific impact would depend on the details of their agreement with AIG and their contingency plans for such events.\n```\n\n\n### ToDo\n- Support for json and csv docs\n- Process file metadata and use it with the retriever\n- LLM validation\n- Structured prompting with Pydantic\n- Support for router fine tuning\n- Implement chain of thought\n"
    },
    {
      "name": "Vinayak-HUB1/Rag-Implementation-using-haystack-and-chainlit",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/77567868?s=40&v=4",
      "owner": "Vinayak-HUB1",
      "repo_name": "Rag-Implementation-using-haystack-and-chainlit",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2023-11-25T14:47:14Z",
      "updated_at": "2023-11-27T11:15:19Z",
      "topics": [],
      "readme": ""
    },
    {
      "name": "RedTachyon/tutor-at-home",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/19414946?s=40&v=4",
      "owner": "RedTachyon",
      "repo_name": "tutor-at-home",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2023-11-04T22:36:03Z",
      "updated_at": "2023-11-05T18:54:35Z",
      "topics": [],
      "readme": "# We have a tutor at home\n\nHow to use the application:\n\nRun UI via command line in your Terminal\n\npython gradio_bot.py [--share] [--unicorn]\n\nIt will show chat bot for you. On the left there is a drop down list of the problems. On the right the app suggests the student to solve the problem.\nThe purpose of the app is to help student to get through the problem in the best interactive way.\n\nThe workflow is the following:\n - Student provides solution to the problem as they think is approprate for them\n - Based on the solution app reacts to it\n      - If solution is partly correct, the Tutor will analyse and suggests to focus on problematic piece or to double check\n      - If the student asks for a hint, the Tutor is able to provide a hint in interactive manner\n      - The student can ask what exactly is not right with their solution, and Tutor will be able to provide constructive feedback that might help the student to find logical inconsistences\n      - If solution is slightly incorrect, Tutor will be able to point to the slight calculation problem and to encourage to try again\n      - If the answer is fully correct, but there is no explanation, the Tutor will ask for explanation\n      - Once full correct solution is shown by Tutor to the Student, Student can ask for clarifications of each particular pieces of the solution in more details\n\n\nImplementationwise the Tutor is based on State Machine (Automaton) depending on the Student solution or reaction the State might be switched. Depending on the State we use different Prompt to Communicate with Claude.\nTerminal state is that Student provided Right answer AND description to the solution. If the Student did not provide solution, just right answer, we don't go to Terminal state, we ask Student for full solution.\n\nSupport of problems:\n1. We support problems and solutions provided manually by teacher (or any human advisor)\n2. We support loading Problem sets in Pdf and parsing for specific problems using haystack. This can be extendable to any text books.\n\n\n"
    },
    {
      "name": "raaasin/Drillie",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/97795923?s=40&v=4",
      "owner": "raaasin",
      "repo_name": "Drillie",
      "description": "Drillie, your legal mining guide chatbot. That uses multiple models and switches based on the query/prompt provided by the user",
      "homepage": "",
      "language": "Python",
      "created_at": "2023-09-19T15:12:43Z",
      "updated_at": "2025-02-16T15:42:03Z",
      "topics": [
        "chatbot",
        "flask",
        "nlp",
        "transformer"
      ],
      "readme": "# Drillie: Legal Mining Guide Chatbot\n\n**Project in Progress**\n\nWelcome to Drillie, your legal mining guide chatbot. Drillie is an innovative project developed for Smart India Hackathon 2023 (SIH) aimed at providing assistance and information regarding mining operations in India. This README provides an overview of Drillie's functionalities and how to use it.\n\n## Repository Composition\n\n- **\"data\" folder**: Contains datasets and data-related resources.\n- **\"images\" folder**: Stores images, including the Drillie logo.\n- **\"models\" folder**: Houses pre-trained and fine-tuned models used by the chatbot.\n- **\"templates\" folder**: Contains HTML/CSS templates for the web interface.\n- **\"app.py\"**: The main Python script that powers the Drillie chatbot.\n- **\"README.md\"**: The document you are currently reading.\n- **\"req.txt\"**: A list of required Python packages and their versions.\n\n## Features\n\nDrillie offers a wide range of features to assist users in their mining-related queries:\n\n1. **Intelligent Model Switching**: Drillie utilizes one of four existing models to answer questions and can summarize complex statements for easy comprehension.\n\n2. **Web Scraping**: The chatbot periodically scrapes relevant websites to ensure that it provides up-to-date information to users.\n\n3. **Friendly Persona**: Maintains user-friendly conversations by impersonating a friendly persona, enhancing the overall user experience.\n\n4. **Mining Information**: Provides instant, real-time answers on mining procedures, licenses, regulations, efficiency enhancements, and streamlining compliance processes.\n\n5. **Website Navigation**: Offers seamless website navigation by providing relevant links to meet the user's specific requirements.\n\n6. **Summarization**: Efficiently summarizes responses to queries with a 'Summarize' button for quick and easy comprehension.\n\n7. **Up-to-Date Information**: Provides current and accurate information about mining rules and regulations in India.\n\n## Technologies Used\n\nDrillie leverages state-of-the-art technologies to provide a powerful and user-friendly experience:\n\n- **Transformers**: Utilizes the capabilities of the Transformers toolkit, which offers a rich variety of pretrained models for text, vision, and audio tasks.\n\n- **Roberta Base Squad 2**: This model, embedded within the transformer library, achieves exceptional performance by fine-tuning with SQuAD2.0 on question-answer pairs, bringing powerful NLP capabilities to our chatbot.\n\n## Getting Started\n\nTo get started with Drillie, follow these steps:\n\n1. Clone this repository to your local machine.\n2. Install the required Python packages listed in `requirements.txt`.\n3. Run `app.py` to start the Drillie chatbot.\n\n## Contact Information\n\nFor any questions or feedback related to Drillie, please contact or ping any contributors of this project.\n\n**Thank you for your interest in Drillie: Legal Mining Guide Chatbot! We are working hard to make mining-related information more accessible and user-friendly as part of the Smart India Hackathon 2023 (SIH).**\n"
    },
    {
      "name": "cnvrg/FastRAG",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/44049243?s=40&v=4",
      "owner": "cnvrg",
      "repo_name": "FastRAG",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2023-10-03T03:47:23Z",
      "updated_at": "2024-01-31T17:00:13Z",
      "topics": [],
      "readme": "# RAG LLM Blueprint\n\nThis blueprint is a one click to deploy a RAG pipeline for inference using LLM and updater webapp connected to MinIO / S3 storage solution. User needs to have three external services online before using this blueprint i.e a Large language model, an Elastic Search service and MinIO / S3 storage. User will use the RAG endpoint for inference, which in turn will connect with Elastic Search Service to retrieve latest documents and LLM to generate relevant answers.\nThe Elastic Search document index will be updated using the webapp that will be deployed along with the RAG endpoint. User needs to update their latest documents in the storage bucket and it will be directly updated in the elastic search document index.\n\nFor more information about this blueprint you can refer to [this blog.](https://cnvrg.io/enhance-large-language-models-leveraging-rag-and-minio-and-cnvrg/)\n\n## Technical pre-requisite\n\n1. Setup [ElasticSearch](https://www.elastic.co/guide/en/elasticsearch/client/python-api/current/index.html)\n2. Retrieve the access key either from the llm model hosted on the cnvrg platform or obtain it from OpenAI / Hugging Face.\n3. Setup a MinIO / S3 bucket.\n4. If the user sets-up an S3 bucket, they also need to configure the SNS queue to capture the object created event. \n- Walkthrough: Configuring a bucket for notifications\n    - Step 1: Create an Amazon SQS queue\n    - Step 2: Create an Amazon SNS topic\n    - Step 3: Add a notification configuration to your bucket\n    - Step 4: Test the setup\n    \n- Refer to the [link](https://docs.aws.amazon.com/AmazonS3/latest/userguide/ways-to-add-notification-config-to-bucket.html) for more information.\n\n## Flow\n\n1. Click on `Use Blueprint` button.\n2. You will be redirected to your blueprint flow page.\n3. Go to the project settings section and update the [environment variables](https://app.cnvrg.io/docs/core_concepts/projects.html#environment) with relevant information that will be used by the RAG endpoint.\n\n- For more info see the component documentations\n    - [RAG endpoint documentation](https://app.af2jdjq262tdqvyelihtqnd.cloud.cnvrg.io/blueprintsdev/blueprints/libraries/rag-endpoint/1.0.51)\n    - [Listener/Updator documentation](https://app.af2jdjq262tdqvyelihtqnd.cloud.cnvrg.io/blueprintsdev/blueprints/libraries/listener/1.0.51)\n4. You will have to update the task 'updator' to provide relevant information regarding your storage solution and elastic search service.\n5. Click on the ‘Run Flow’ button.\n6. In a few minutes you will have a RAG endpoint and a webapp deployed to update your elastic search service.\n7. Go to the ‘Serving’ tab in the project and look for your endpoint.\n8. You can use the Try it Live section to query the RAG endpoint and generate relevant answers with LLM connected.\n9. You can also integrate your API with your code using the integration panel at the bottom of the page.\n\n**Note:** A slim version of RAG is also available as a blueprint that enables the use of RAG locally on cnvrg, without the use of Minio / S3 and elasticsearch. Check it out FastRAG slim blueprint for more information.\n"
    },
    {
      "name": "blur0b0t/mh_shell",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/143605527?s=40&v=4",
      "owner": "blur0b0t",
      "repo_name": "mh_shell",
      "description": "mh_shell_hackathon",
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2023-09-22T12:46:15Z",
      "updated_at": "2023-12-11T15:39:00Z",
      "topics": [],
      "readme": "# mh_shell\n\n\n<hr>\n\n\nMachineHack | Shell cybersecurity hackathon 2023 -\n\n![image](https://github.com/blur0b0t/mh_shell/assets/143605527/9f9a65a8-82fb-4f4e-b03d-dc1f5b135d8f)\n\n\n<img src=https://github.com/blur0b0t/mh_shell/assets/143605527/0c1952ba-6ab7-4cc1-bcfa-81e26b783c9a width=30% height=30%>\n\n\n\n<br />\n<br />\n\n[![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://codespaces.new/blur0b0t/mh_shell)\n\n# About Hackathon:\n\nShell, in collaboration with MachineHack & Analytics India Magazine, asked participants to tackle the exponential growth of cyber threats and improve the security and resilience of web applications. The aim of the Cyber Threat Detection Hackathon is to build a next-gen model capable of identifying code in a body of text. \n \n\n \n# Problem statement\nProtecting our software landscapes is not an easy task. Malicious actors are frequently trying to enter systems and get access to resources, whether operational or data. The ability for an actor to compromise systems, elevate their privileges, and move laterally within infrastructure typically hinges on executing hidden code. One common method they employ is embedding this code in seemingly harmless media—whether it's images, videos, or even simple text files.\n\n\n# Detailed Architecture Flow:\n\n<img width=\"424\" alt=\"Screenshot 2023-10-25 161033\" src=\"https://github.com/blur0b0t/mh_shell/assets/143605527/cf4981db-715c-4d7b-ab42-56b53594138b\">\n\n\n\n\n\n# Step-by-Step Code Execution Instructions:\n\n\n- Clone the Repository\n\n```bash\n $ git clone https://github.com/blur0b0t/mh_shell\n $ cd mh_shell\n```\n\n- Train/Fine-tune the flan-t5-xl model on slurm workload manager.\n\n\n```bash\n#!/bin/bash\n\n\nexport batch_script=\"ft_mod1.sh\"\n# -----------set new job dep--------------\necho \"got current job name=$SLURM_JOB_NAME\"\nexport cji=$(echo -n $SLURM_JOB_NAME | tail -c 1)\nexport nji=$(( cji + 1 ))\n# export nji=$($SLURM_JOB_NAME | tail -c 1| awk '{print $1 + 1}')\nexport njname=\"fts$nji\"\necho \"new job name=$njname\"\nexport njid=$(sbatch -x idc-beta-batch-pvc-node-[03,09,14,20,21] --priority 0 --job-name $njname --begin=now+60 --dependency=afterany:$SLURM_JOB_ID --mem=0 --exclusive $batch_script | sed -n 's/.*job //p')\necho \"new job created with id: $njid\"\n# -------------------end------------------\n\n\n\n\necho \"----------checking if gpu available on current job-----------------\"\nconda init bash\n# oneapi env and checking gpu\necho \"-------------------------------------------\"\ngroups\nsource /opt/intel/oneapi/setvars.sh --force\nsycl-ls\nexport num_gpu=\"$(sycl-ls |grep \"GPU\" |wc -l)\"\necho \"num_gpu=$num_gpu\\n\"\nexport num_cpu=\"$(sycl-ls |grep \"Xeon\" |wc -l)\"\necho \"num_cpu=$num_cpu\\n\"\nif [ $num_gpu == 0 && $num_cpu == 1] \nthen \n    echo \"---GPU not available exiting--------\"\n    scancel $SLURM_JOB_ID\nfi \necho \"-------------------------------------------\"\n\n\n\necho \"starting fine tuning model\"\ncd \"/home/u131168/mh_shell/ft_model_pp/itp\"\npip install -r \"requirements.txt\"\n# To use ccl as the distributed backend in distributed training on CPU requires to install below requirement.\npython -m pip install oneccl_bind_pt -f https://developer.intel.com/ipex-whl-stable-cpu\n\ncd \"/home/u131168/mh_shell/ft_model_pp/itp\"\n# pip install intel-extension-for-transformers\n# pip install fschat==0.1.2\npip install git+https://github.com/huggingface/transformers\npip install tokenizers\n\n\n\n# sbatch -x idc-beta-batch-pvc-node-[03,20,21] --job-name fts1 --priority=0 ft_mod1.sh\n# sbatch -w idc-beta-batch-pvc-node-[05] --job-name fts1 --priority=0 ft_mod1.sh\n\n\nexport train_file=\"/home/u131168/mh_shell/data/f_traind_v1.csv\"\n\nexport model_path=\"google/flan-t5-xl\"\n# export model_path=\"/home/u131168/mh_shell/ft_models/flan-t5-xl_mt5_v1\"\n\n# export checkpoint_path=\"/home/u131168/mh_shell/ft_models/flan-t5-xl_peft_finetuned_model/checkpoint-36000\"\nexport checkpoint_dir=\"/home/u131168/mh_shell/ft_models/flan-t5-xl_mt5_v4/\"\nexport checkpoint_name=$(ls $checkpoint_dir | grep checkpoint | tail -2 | head -n 1)\nexport checkpoint_path=\"$checkpoint_dir$checkpoint_name\"\necho $checkpoint_path\n\nexport output_dir=\"$checkpoint_dir\"\n\n\n\n\npython finetune_seq2seq.py \\\n        --model_name_or_path $model_path \\\n        --resume_from_checkpoint $checkpoint_path \\\n        --bf16 True \\\n        --train_file $train_file \\\n        --per_device_train_batch_size 2 \\\n        --per_device_eval_batch_size 2 \\\n        --gradient_accumulation_steps 1 \\\n        --do_train \\\n        --learning_rate 1.0e-6 \\\n        --warmup_ratio 0.03 \\\n        --weight_decay 0.0 \\\n        --num_train_epochs 1 \\\n        --logging_steps 10 \\\n        --save_steps 100 \\\n        --save_total_limit 2 \\\n        --overwrite_output_dir \\\n        --output_dir $output_dir \\\n        --peft lora\n\necho \"finished fine tuning model\"\n\n```\n\n- Perform inference on the test dataset with finetuned flan-t5-xl-peft model\n\n```bash\n#!/bin/bash\nexport batch_script=\"p_custom.sh\"\nconda init bash\n# -----------set new job dep--------------\necho \"got current job name=$SLURM_JOB_NAME\"\nexport cji=$(echo -n $SLURM_JOB_NAME | tail -c 1)\nexport nji=$(( cji + 1 ))\nexport njname=\"pcs$nji\"\necho \"new job name=$njname\"\nexport njid=$(sbatch -x idc-beta-batch-pvc-node-[03,20,21] --priority 0 --job-name $njname --begin=now+60 --dependency=afterany:$SLURM_JOB_ID --mem=0 --exclusive $batch_script | sed -n 's/.*job //p')\necho \"new job created with id: $njid\"\n# -------------------end------------------\n\n\n\n\necho \"----------checking if gpu available on current job-----------------\"\nconda init bash\n# oneapi env and checking gpu\necho \"-------------------------------------------\"\ngroups  # Key group is render, PVC access is unavailable if you do not have render group present.\nsource /opt/intel/oneapi/setvars.sh --force\nsycl-ls\nexport num_gpu=\"$(sycl-ls |grep \"GPU\" |wc -l)\"\necho \"num_gpu=$num_gpu\\n\"\nexport num_cpu=\"$(sycl-ls |grep \"Xeon\" |wc -l)\"\necho \"num_cpu=$num_cpu\\n\"\nif [ $num_gpu == 0 && $num_cpu == 1] \nthen \n    echo \"---GPU not available exiting--------\"\n    scancel $SLURM_JOB_ID\nfi \necho \"-------------------------------------------\"\n\n\n\necho \"staring prediction\"\n\n# conda acti\npip install torch\npip install transformers\npip install peft\n\n\n\npython /home/u131168/mh_shell/p_custom_pp/p_custom.py\n\necho \"finished precition\"\n\n\n\n```\n\n# Run web application to interact with the finteuned flan-t5-xl-peft model to detect code snippets in the given paragraph.\n\n- Run python app to serve predictions(code snippets) to the frontend.\n- (*the webapp wont work ,if the python app is not running)\n\n```bash\n \ncd python_api\npip install -r ./reqs.txt\npython ./ft5_cf.py\n\n\n```\n\n![Screenshot (30)](https://github.com/blur0b0t/mh_shell/assets/143605527/3a9fe31c-7fd9-42f3-98b1-7c1d271621fc)\n\n\n![Screenshot (31)](https://github.com/blur0b0t/mh_shell/assets/143605527/55f89773-9587-464f-8cc3-4371eab6eadb)\n\n\n\n\n# Run frontend application(webapp) to extract code snippets from a give paragraph.\n- (*make sure that the python application is running before using the webapp)\n  <br />\n\n\n\n- option 1: use the web app hosted on huggingface spaces:\n```bash\nhttps://huggingface.co/spaces/blur0b0t/mh_shell\n```\n\n![Screenshot (29)](https://github.com/blur0b0t/mh_shell/assets/143605527/7f75a20e-7205-4bed-b52b-d89f020b8f53)\n\n\n\n\n- option 2: use the prebuild files\n```bash\ncd mhs_pred_app/build/web\n# run index.html file from browser to access the webapp\n```\n\n- option 3: build app from flutter sdk (*flutter sdk need to be installed on the system)\n```bash\ncd mhs_pred_app\nflutter run -d web-server --host=0.0.0.0\n```\n\n![Screenshot (27)](https://github.com/blur0b0t/mh_shell/assets/143605527/a21c0a41-401b-4e10-a08b-eddb70460154)\n\n\n![Screenshot (28)](https://github.com/blur0b0t/mh_shell/assets/143605527/48866e12-5f47-4950-ac8d-08bfb0f00a71)\n\n\n\n\n\n <br />\n <br />\n    \n- (*hugging face currently does not support inference api for peft models, so we need to run the python app to serve predictions for the webapp to work.)\n- webapp available on Huggingface Spaces (https://huggingface.co/spaces/blur0b0t/mh_shell)\n- model available on Huggingface Hub (https://huggingface.co/blur0b0t/mh_shell)\n\n\n"
    },
    {
      "name": "kndeepak/LLM-RAG-invoice-Local_CPU",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/55972417?s=40&v=4",
      "owner": "kndeepak",
      "repo_name": "LLM-RAG-invoice-Local_CPU",
      "description": "Invoice data processing with Llama2 13B LLM RAG on Local CPU",
      "homepage": null,
      "language": "Python",
      "created_at": "2023-10-24T20:16:34Z",
      "updated_at": "2024-03-22T06:48:47Z",
      "topics": [],
      "readme": "# Invoice data processing with Llama2 13B LLM RAG on Local CPU\n\n## Quickstart\n\n### RAG runs on: LlamaCPP, Haystack, Weaviate\n\n1. Download the Llama2 13B model, check models/model_download.txt for the download link.\n2. Install Weaviate local DB with Docker\n\n`docker compose up -d`\n   \n3. Install the requirements: \n\n`pip install -r requirements.txt`\n\n4. Copy text PDF files to the `data` folder.\n5. Run the script, to convert text to vector embeddings and save in Weaviate vector storage: \n\n`python ingest.py`\n\n6. Run the script, to process data with Llama2 13B LLM RAG and return the answer: \n\n`python main.py \"What is the invoice number value?\"`\n\n\n## FASTAPI Server Implementation \n\n\n7. Load Server using FAST API \n\n \n`uvicorn main:app --reload`\n \n8.Run query in this format \n\n`http://127.0.0.1:8000/get_answer/?query=YourQuestionHere`\n\n"
    },
    {
      "name": "Qing145/QA_Web",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/68147254?s=40&v=4",
      "owner": "Qing145",
      "repo_name": "QA_Web",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2023-10-22T03:48:57Z",
      "updated_at": "2023-10-22T12:46:13Z",
      "topics": [],
      "readme": "# Install requirements\n```\npip install -r requirements.txt\n```\n\n# Train KGQA Models\nBecause github does not allow users to upload file with size larger than 100MB. To set-up the project, you firstlt need to train KGQA models. After\ntraining the models, please put them on ./KGQA/preprocess/.\n```\npython train_detection.py --entity_detection_mode LSTM --fix_embed --gpu 0\npython train_entity.py --qa_mode GRU --fix_embed --gpu 0\npython train_pred.py --qa_mode GRU --fix_embed --gpu 0\n```\n\n# Train Content-based model\nWe use haystack to deploy content-based question answering. Details: https://haystack.deepset.ai/tutorials.\nAfter training the model, please set the folder name as 'my_model' and put it on ./Content_based_QA/\n"
    },
    {
      "name": "asabade/Chat-Summarization",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/28676938?s=40&v=4",
      "owner": "asabade",
      "repo_name": "Chat-Summarization",
      "description": null,
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2023-10-17T22:07:36Z",
      "updated_at": "2023-10-17T23:28:28Z",
      "topics": [],
      "readme": "# Chat-Summarization"
    },
    {
      "name": "Ankush-Chander/cricket-ama",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/4995297?s=40&v=4",
      "owner": "Ankush-Chander",
      "repo_name": "cricket-ama",
      "description": "RAG application on cricket using haystack and mistral",
      "homepage": "",
      "language": "Jupyter Notebook",
      "created_at": "2023-10-07T11:03:50Z",
      "updated_at": "2023-10-07T11:36:56Z",
      "topics": [
        "haystack",
        "llm",
        "mistral",
        "question-answering",
        "rag"
      ],
      "readme": "# Cricket-ama\nAsk me anything about cricket. I will try to answer it to the best of my knowledge.\n\n\n[RAG(Retrieval Augmented Generation)](https://www.pinecone.io/learn/retrieval-augmented-generation/) is a technique in which we use powers of LLM to generate accurate information by grounding it with a known data source. This way we can leverage an LLM interface while avoiding hallucination . \nThis notebook is an attempt to build a cricket ama interface using RAG.\n\n# Data source\n1. Data has been generated using [pages-articles-multistream wiki dump](https://dumps.wikimedia.org/enwiki/latest/). Alternatively it can be downloaded from [here](https://huggingface.co/datasets/mrsearchwolf/cricket-wiki)\n\n# Tech Stack\n1. [Haystack](https://docs.haystack.deepset.ai/docs)\n2. [Mistral-7B-Instruct-v0.1](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1)\n\n\n# Acknowlegement\nThis notebook is inspired from [mistral-haystack](https://github.com/anakin87/mistral-haystack) by [Stefano](https://github.com/anakin87).\n"
    },
    {
      "name": "sumanghosh1234/haystack-streamlit-weaviate-mistral",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/28214681?s=40&v=4",
      "owner": "sumanghosh1234",
      "repo_name": "haystack-streamlit-weaviate-mistral",
      "description": "haystack-streamlit-weaviate-mistral",
      "homepage": null,
      "language": "Python",
      "created_at": "2023-10-06T06:51:25Z",
      "updated_at": "2023-10-09T14:48:12Z",
      "topics": [],
      "readme": "---\ntitle: Haystack Search Pipeline with Streamlit\nemoji: 👑\ncolorFrom: indigo\ncolorTo: indigo\nsdk: streamlit\nsdk_version: 1.23.0\napp_file: app.py\npinned: false\n---\n\n# Template Streamlit App for Haystack Search Pipelines\n\nThis template [Streamlit](https://docs.streamlit.io/) app set up for simple [Haystack search applications](https://docs.haystack.deepset.ai/docs/semantic_search). The template is ready to do QA with **Retrievel Augmented Generation**, or **Ectractive QA**\n\nSee the ['How to use this template'](#how-to-use-this-template) instructions below to create a simple UI for your own Haystack search pipelines.\n\nBelow you will also find instructions on how you could [push this to Hugging Face Spaces 🤗](#pushing-to-hugging-face-spaces-).\n\n## Installation and Running\nTo run the bare application which does _nothing_:\n1. Install requirements: `pip install -r requirements.txt`\n2. Run the streamlit app: `streamlit run app.py`\n\nThis will start up the app on `localhost:8501` where you will find a simple search bar. Before you start editing, you'll notice that the app will only show you instructions on what to edit.\n\n### Optional Configurations\n\nYou can set optional cofigurations to set the:\n-  `--task` you want to start the app with: `rag` or `extractive` (default: rag)\n-  `--store` you want to use: `inmemory`, `opensearch`, `weaviate` or `milvus` (default: inmemory)\n-  `--name` you want to have for the app. (default: 'My Search App')\n\nE.g.:\n\n```bash\nstreamlit run app.py -- --store opensearch --task extractive --name 'My Opensearch Documentation Search'\n```\n\nIn a `.env` file, include all the config settings that you would like to use based on:\n- The DocumentStore of your choice\n- The Extractive/Generative model of your choice\n\nWhile the `/utils/config.py` will create default values for some configurations, others have to be set in the `.env` such as the `OPENAI_KEY`\n\nExample `.env`\n\n```\nOPENAI_KEY=YOUR_KEY\nEMBEDDING_MODEL=sentence-transformers/all-MiniLM-L12-v2\nGENERATIVE_MODEL=text-davinci-003\n```\n\n\n## How to use this template\n1. Create a new repository from this template or simply open it in a codespace to start playing around 💙\n2. Make sure your `requirements.txt` file includes the Haystack and Streamlit versions you would like to use.\n3. Change the code in `utils/haystack.py` if you would like a different pipeline.\n4. Create a `.env`file with all of your configuration settings.\n5. Make any UI edits you'd like to and [share with the Haystack community](https://haystack.deepeset.ai/community)\n6. Run the app as show in [installation and running](#installation-and-running)\n\n### Repo structure\n- `./utils`: This is where we have 3 files: \n    - `config.py`: This file extracts all of the configuration settings from a `.env` file. For some config settings, it uses default values. An example of this is in [this demo project](https://github.com/TuanaCelik/should-i-follow/blob/main/utils/config.py).\n    - `haystack.py`: Here you will find some functions already set up for you to start creating your Haystack search pipeline. It includes 2 main functions called `start_haystack()` which is what we use to create a pipeline and cache it, and `query()` which is the function called by `app.py` once a user query is received.\n    - `ui.py`: Use this file for any UI and initial value setups.\n- `app.py`: This is the main Streamlit application file that we will run. In its current state it has a simple search bar, a 'Run' button, and a response that you can highlight answers with.\n\n### What to edit?\nThere are default pipelines both in `start_haystack_extractive()` and `start_haystack_rag()`\n\n- Change the pipelines to use the embedding models, extractive or generative models as you need.\n- If using the `rag` task, change the `default_prompt_template` to use one of our available ones on [PromptHub](https://prompthub.deepset.ai) or create your own `PromptTemplate`\n\n\n## Pushing to Hugging Face Spaces 🤗\n\nBelow is an example GitHub action that will let you push your Streamlit app straight to the Hugging Face Hub as a Space.\n\nA few things to pay attention to:\n\n1. Create a New Space on Hugging Face with the Streamlit SDK.\n2. Create a Hugging Face token on your HF account.\n3. Create a secret on your GitHub repo called `HF_TOKEN` and put your Hugging Face token here.\n4. If you're using DocumentStores or APIs that require some keys/tokens, make sure these are provided as a secret for your HF Space too!\n5. This readme is set up to tell HF spaces that it's using streamlit and that the app is running on `app.py`, make any changes to the frontmatter of this readme to display the title, emoji etc you desire.\n6. Create a file in `.github/workflows/hf_sync.yml`. Here's an example that you can change with your own information, and an [example workflow](https://github.com/TuanaCelik/should-i-follow/blob/main/.github/workflows/hf_sync.yml) working for the [Should I Follow demo](https://huggingface.co/spaces/deepset/should-i-follow)\n\n```yaml\nname: Sync to Hugging Face hub\non:\n  push:\n    branches: [main]\n\n  # to run this workflow manually from the Actions tab\n  workflow_dispatch:\n\njobs:\n  sync-to-hub:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v2\n        with:\n          fetch-depth: 0\n          lfs: true\n      - name: Push to hub\n        env:\n          HF_TOKEN: ${{ secrets.HF_TOKEN }}\n        run: git push --force https://{YOUR_HF_USERNAME}:$HF_TOKEN@{YOUR_HF_SPACE_REPO} main\n```\n"
    },
    {
      "name": "MentoriaPloomber/RAG_HayStack_QA",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/145875106?s=40&v=4",
      "owner": "MentoriaPloomber",
      "repo_name": "RAG_HayStack_QA",
      "description": null,
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2023-09-23T21:28:48Z",
      "updated_at": "2024-05-11T22:00:07Z",
      "topics": [],
      "readme": "# Hacktoberfest 2023: Construcción de pipelines Retrieval Augmented Generation (RAG)con open source \n\n\n## Aplicación de preguntas y respuestas de sentencias de altas cortes de Colombia usando Haystack \n\nPuedes ver el video resumen del proyecto en: https://bit.ly/tutel_ai\n\n<details>\n  <summary>  Contexto </summary>\n\nColombia es un Estado Social de Derecho, se divide el poder en las clásicas 3 ramas del poder público; ejecutivo, legislativo, judicial, más otros órganos como la Procuraduría y Contraloría.\n\nLa Rama Judicial resuelve las controversias que se dan entre los ciudadanos, sea de carácter civil, administrativo, familiar, laboral, agrario, etcétera.\n\nDentro de la Constitución de 1991 se incluyó la Acción de tutela, [que tiene equivalentes en otros países](https://repository.unilibre.edu.co/bitstream/handle/10901/23071/LA%20ACCI%C3%93N%20DE%20TUTELA%20COMPARADA%20CON%20OTROS%20PROCEDIMIENTOS%20DE%20AMPARO%20ESTABLECIDOS%20EN%20AM%C3%89RICA%20LATINA.pdf?sequence=2), la cual mediante un trámite de 10 días hábiles resuelve la protección de derechos fundamentales, tales como: salud, vida, educación, acceso a la información, dignidad.\n\nSi una persona siente o cree que se le viola o vulnera un derecho necesita, o contratar un abogado, recurrir a la defensoría del pueblo o a un consultorio jurídico de una universidad. Muchas veces contratar a un abogado es ya una barrera a la administración de justicia si el interesado no puede costearse los honorarios. La defensoría del pueblo que ofrece asesoría gratuita, esta congestionada por la cantidad de usuarios necesitados. Los consultorios jurídicos de universidades cesan la atención durante el período de vacaciones de los estudiantes.\n\nAunque se exhorta a los jueces redacción de sentencias de [lectura fácil](https://www.ambitojuridico.com/noticias/administrativo/congreso-crearia-formato-de-sentencias-de-lectura-facil) , es inevitable el uso de tecnicismo, y las personas no abogadas no sabrían a cuales sentencias prestarle atención entre el mar de jurisprudencia.\n\nPor lo anterior, una aplicación de preguntas y respuestas (QA) es valiosa para una persona que se cree inmersa en una circunstancia de violación de derechos porque puede, con solo consignar lo que le pasa -los hechos- , conocer si previamente, alguna sentencia ha protegido un derecho fundamental en un caso similar.\n\n</details>\n\n<details>\n  <summary>  Definiciones preliminares </summary>\n\n## Construccion del ETL y RAG pipelines  \nRAG (Retrieval-Augmented Generation) es un patrón de diseño/framework de Inteligencia Artificial diseñado para mejorar el rendimiento de los Grandes Modelos de Lenguaje (LLM). Su objetivo principal es proporcionar información precisa y verificable sin incurrir en altos costos computacionales o financieros asociados con el entrenamiento continuo de los modelos de lenguaje en nuevos datos.\nLa necesidad de RAG surgió debido a las limitaciones inherentes a los LLM. Aunque estos modelos son poderosos y capaces de generar respuestas impactantes, a menudo carecen de precisión y exactitud. Los LLM pueden entender las relaciones estadísticas entre las palabras, pero el significado real de estas palabras a menudo se pierde en el proceso. Además, los LLM a veces proporcionan información aleatoria y no relacionada en respuesta a consultas, también conocida como alucinaciones.\n\n</details>\n<details>\n  <summary> La solución implementada</summary>\n\n### Objetivo:\n\nDesarrollo de una aplicación para realizar preguntas relacionadas a decisiones y  jurisprudencia de las cortes de Colombia usando Haystack como framework de desarrollo de aplicaciones de uso de modelos grandes de lenguaje  (LLM) y Chainlit como herramienta de construcción de la interfaz de usuario final. \n \n<br></br>\n\n ![Estructura de Q-A con Haystack para el proyecto](images/DIAGRAMAQARELATORIA.png)\n\n<br></br>\nPara esta implementación se ha usado las siguientes plataformas/tecnologías/frameworks:  \nPython, [Ploomber](https://ploomber.io/),`[FAISS](https://faiss.ai/index.html), [Haystack](https://haystack.deepset.ai/), [Chainlit](https://docs.chainlit.io/get-started/overview), [Docker](https://www.docker.com/), [Poetry](https://python-poetry.org/), [Miniconda](https://docs.conda.io/projects/miniconda/en/latest/)\n\n\n#### Conjunto de datos Fuente:\n- Subconjunto de sentencias de la relatoría de cortes de Colombia: \n[Relatorìa de Colombia](https://www.corteconstitucional.gov.co/relatoria/)\n- Licencia: Pública por ser decisiones judiciales y su uso sería equivalente a la [licencia GNU 3.0](https://es.wikipedia.org/wiki/GNU_General_Public_License)\n\n#### Implementación\nSe ha realizado una primera etapa de implementación y se espera continuar en un segunda etapa\n\nEl alcance de la solución de esta primera etapa incluye:  \n\n- Uso de un subconjunto de 34 sentencias representativas del año 2022\n- Uso de la biblioteca FAISS para almacenar los documentos de sentencia indexados y vectorizados\n- Uso de GPT-4 de OpenAI como modelo de lenguaje grande para hacer preguntas sobre el conjunto de sentencias\n- Desarrollo de un script que descarga las sentencias de la corte desde el sitio web de la relatoría en formato RTF y las transforma en archivos de texto\n- Desarrollo de un script que indexa y almacena las sentencias en un FAISS Document Store usando Haystack como framework\n- Desarrollo de un script que carga el FAISS Document Store previamente generado para realizar preguntas usando Haystack como framework\n- Desarrollo de un aplicación Chainlit para interfaz de usuario final\n- Creación de un contenedor de Docker que encapsula la solución a ser desplegada en la nube de Ploomber\n- Creación de un pipeline de Ploomber para desplegar la solución a la nube de Ploomber\n\nEste repositorio mantiene tanto los scripts mencionados anteriormente como los archivos asociados al FAISS Document Store con las sentencias seleccionadas como prueba, estos son: *Faiss_document_store.db, my_index.faiss y my_config.json*\n\n#### El repositorio se ha organizado en las carpetas\n\n![Organizacion](images/carpetas.png). \n\n- En la carpeta **notebooks** se encuentran los notebooks preliminares a la creación de los scripts \n- En la carpeta **src/app** se encuentra el script *app.py*, que contiene tanto   el script  que realiza las preguntas, construido con Haystack, como la interfaz de usuario construida usando Chaintlit.\n- En la carpeta **etl** se encuentra el script *extract.py* que realiza la descarga de las sentencias del sitio web de la relatoria, en formato rtf y los convierte a documentos en formato texto.\n- En la carpeta **indexing_QA**, se encuentran los scripts *indexing_documents.py* y *qa_generation.py*, los cuales realizan por separado los procesos de creación del Document Store con la indexacion de las sentencias y el proceso de preguntas \n\n\n</details>\n\n<details>\n  <summary>  Como ejecutar la aplicación en la web </summary>\n<p> </p>\n\nLa aplicación se encuentra desplegada en la plataforma de **Ploomber**  \nSe puede acceder en el enlace: [Tutelai](https://proud-bird-8701.ploomberapp.io/)\n<p> </p>\n</details>\n\n<details>\n  <summary>  Como ejecutar la aplicación localmente </summary>  \n\n### Instalación y Configuración\n\nSe puede ejecutar por separado,la aplicación de preguntas, el proceso de generación del Document Store o el proceso de descarga de sentencias desde el sitio de la relatoria de Colombia.\n\n<p></p>\nPara la ejecución de la aplicación de preguntas se requiere:\n\n- Que en la carpeta **src/app** existan los archivos correspondientes al Document Store de prueba: *faiss_document_store.db, my_config.json y my_index.faiss*\n- La configuración del archivo de .env con la API Key de Open AI\n\nEste repositorio contiene un FAISS Document Store, por lo que en principio, no es necesario ejecutar los procesos de descarga de sentencias ni de generación de Document Store.  \n\nUna vez se haya descargado el repositorio, se puede hacer la configuración mediante *Poetry* o instalando las dependencias desde el archivo *requirements.txt*.\n\n#### Instalación con Poetry\n\nSe asume que ya se tiene instalada Miniconda o Anaconda para crear el entorno.\n(consultar el [Setup](https://github.com/MentoriaPloomber/RAG_HayStack_QA/blob/main/setup-espanol.md))\n\n1. Creando nuevo entorno en la carpeta donde se encuentra el repositorio del proyecto\n  ```console\n    conda create --name tutelai python=3.10\n  ```\n\n2. Activando el entorno\n  ```console\n    conda activate tutelai \n  ```\n3. Instalando poetry \n  ```console\n    pip install poetry\n  ```\n\n4. Instalando dependencies\n  ```console\n    poetry install\n  ```\n\n### Ejecución de la aplicación Chainlit, interfaz de usuario para las preguntas\n\nUna vez se haya hecho la instalación del entorno con las dependencias, se puede ejecutar la aplicación de preguntas con el FAISS Document Store de prueba que incluye este repositorio\n\n1. Configurando la API key de OpenAI  \n   En la carpeta **src/app**, crear el archivo .env e incluir la variable con el API key   \n \n  ```console\n     OPENAI_API_KEY='<id>'\n  ```\n2. Ejecutando la aplicacion Chainlit localmente\n   En la carpeta src/app ejecutar: \n  ```console\n     chainlit run app.py -w\n  ```\n  Esto abrirá una pestaña en el navegador con la interfaz donde se podrán escribir las preguntas.   \n\n  **Ejemplos de preguntas:**  \n  - ¿Como se viola el derecho al trabajo?  \n  - ¿Las tutelas protegen la diversidad sexual?  \n\n### Ejecución de la descarga de las sentencias de la relatoria\n\nPara poder ejecutar la descarga automática de las sentencias, en la carpeta **src/etl**, ejecutar   \n\n  ```console\n     extract.py\n  ```\nEste genera un carpeta con las sentencias en formato .txt las cuales son el insumo para la indexación y creación del FAISS Document Store\n\n### Ejecución de la creación del Document Store\n\nPara poder ejecutar la creación del FAISS Document Store, en la carpeta **src/indexing_QA**, ejecutar \n  ```console\n     indexing_documents.py\n  ```\nEste script toma como insumo la carpeta de sentencias en formato .txt creada por el script de descarga de las sentencias *extract.py* y genera los archivos que son insumo para la aplicación de preguntas y respuestas: *faiss_document_store.db, my_config.json y my_index.faiss*  \n\n</details>\n\n## Miembros del equipo\n\n[Elka Buitrago](https://github.com/elkabuitrago)\n[Juan Vázquez Montejo](https://github.com/juanvazqmont)\n[María Carolina Passarello](https://github.com/caropass)\n[Sergio Maldonado Rodríguez](https://github.com/SergioRodMa)\n[Anuar Menco Nemes](https://github.com/anuarmenco) "
    },
    {
      "name": "NevyrIO/haystack-demo",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/107774787?s=40&v=4",
      "owner": "NevyrIO",
      "repo_name": "haystack-demo",
      "description": null,
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2023-09-25T07:04:58Z",
      "updated_at": "2024-02-24T13:42:38Z",
      "topics": [],
      "readme": "# Generative AI with Haystack on Azure\n\nThis repo provides the template code for integrating AI models with your existing data on Azure. \nThese examples can be modified to work offline, or with an alternative cloud provider.  \n\nPre-work: [Setup local conda environment](./setup/README.md)\n1. [Setup data in Opensearch for indexed queries](haystack_azure.ipynb)\n2. [Streamlit web app to test locally and prepare for deployment](./azure-app/README.md) \n\nUtility: [Azure utility notebooks](./azure-utils/README.md)\n\n\n"
    },
    {
      "name": "abdullahnizami77/NLP-Miners",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/86009804?s=40&v=4",
      "owner": "abdullahnizami77",
      "repo_name": "NLP-Miners",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2023-09-28T16:11:30Z",
      "updated_at": "2023-12-05T14:12:27Z",
      "topics": [],
      "readme": "Protype Images\n\n![Screenshot (80)](https://github.com/abdullahnizami77/NLP-Miners/assets/86009804/3e72c756-eaec-4036-8a89-cb060765ea43)\n![Screenshot (79)](https://github.com/abdullahnizami77/NLP-Miners/assets/86009804/e3d0aa73-fb15-49a2-909a-1cca70395a4e)\n![Screenshot (81)](https://github.com/abdullahnizami77/NLP-Miners/assets/86009804/ec5a6739-a73b-4ea5-9bf7-ab5108338610)\n\nPrototype Video\n\n\nhttps://github.com/abdullahnizami77/NLP-Miners/assets/86009804/3dd5bc97-3492-49f7-bbfa-e7f9aee51cd5\n\n"
    },
    {
      "name": "bogdankostic/notion-haystack",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/48713846?s=40&v=4",
      "owner": "bogdankostic",
      "repo_name": "notion-haystack",
      "description": "📓 A Haystack component to extract Notion pages to Documents.",
      "homepage": null,
      "language": "Python",
      "created_at": "2023-09-26T20:12:21Z",
      "updated_at": "2023-12-06T13:31:18Z",
      "topics": [],
      "readme": "# notion-haystack: Export Notion pages to Haystack Documents\n\nThis python package allows you to easily export your Notion pages to Haystack Documents by providing a Notion API token.\n\nGiven that the Notion API is subject to some [rate limits](https://developers.notion.com/reference/request-limits),\nthis component will automatically retry failed requests and wait for the rate limit to reset before retrying. This is\nespecially useful when exporting a large number of pages. Furthermore, this component uses `asyncio` to make requests in\nparallel, which can significantly speed up the export process.\n\n## Installation\n\n```bash\npip install notion-haystack\n```\n\n## Usage\n\nTo use this package, you will need a Notion API token. You can follow the steps outlined in the [Notion documentation](https://developers.notion.com/docs/create-a-notion-integration#create-your-integration-in-notion) to create a new Notion integration, connect it to your pages, and obtain your API token.\n> To enable your Notion integration to work on specific pages and the child pages in Notion, make sure to enable it in the 'Connections' setting of the page.\n\nThe following minimal example demonstrates how to export a list of pages to Haystack Documents:\n```python\nfrom notion_haystack import NotionExporter\n\nexporter = NotionExporter(api_token=\"<your-token>\")\nexported_pages = exporter.run(page_ids=[\"<list-of-page-ids>\"])\n\n# exported_pages will be a list of Haystack Documents where each Document corresponds to a Notion page\n```\n\nThe following example shows how to use the `NotionExporter` inside an indexing pipeline:\n```python\nfrom haystack import Pipeline\n\nfrom notion_haystack import NotionExporter\nfrom haystack.components.preprocessors import DocumentSplitter\nfrom haystack.components.writers import DocumentWriter\nfrom haystack.document_stores import InMemoryDocumentStore\n\ndocument_store = InMemoryDocumentStore()\nexporter = NotionExporter(api_token='YOUR_NOTION_API_KEY')\nsplitter = DocumentSplitter()\nwriter = DocumentWriter(document_store=document_store)\n\nindexing_pipeline = Pipeline()\nindexing_pipeline.add_component(instance=exporter, name=\"exporter\")\nindexing_pipeline.add_component(instance=splitter, name=\"splitter\")\nindexing_pipeline.add_component(instance=writer, name=\"writer\")\n\nindexing_pipeline.connect(\"exporter.documents\", \"splitter.documents\")\nindexing_pipeline.connect(\"splitter\", \"writer\")\n\nindexing_pipeline.run(data={\"exporter\": {\"page_ids\": [\"your_page_id\"] }})\n# The pages will now be indexed in the document store\n```\n\nThe `NotionExporter` class takes the following arguments:\n- `api_token`: Your Notion API token. You can find information on how to get an API token in [Notion's documentation](https://developers.notion.com/docs/create-a-notion-integration)\n- `export_child_pages`: Whether to recursively export all child pages of the provided page ids. Defaults to `False`.\n- `extract_page_metadata`: Whether to extract metadata from the page and add it as a frontmatter to the markdown. \n                           Extracted metadata includes title, author, path, URL, last editor, and last editing time of \n                           the page. Defaults to `False`.\n- `exclude_title_containing`: If specified, pages with titles containing this string will be excluded. This might be\n                              useful for example to exclude pages that are archived. Defaults to `None`.\n\nThe `NotionExporter.run` method takes the following arguments:\n- `page_ids`: A list of page ids to export. If `export_child_pages` is `True`, all child pages of these pages will be\n                exported as well.\n"
    },
    {
      "name": "delatorrena2016/sql-etl-analytics",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/37124283?s=40&v=4",
      "owner": "delatorrena2016",
      "repo_name": "sql-etl-analytics",
      "description": "Hacktoberfest project to create an analytics report using both SQL and ETL",
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2023-09-25T02:36:28Z",
      "updated_at": "2023-10-30T19:18:14Z",
      "topics": [],
      "readme": "# Hacktoberfest 2023 project: building ETL and RAG pipelines with open source \n\n<p align=\"center\">\n<img src=\"images/ETL PLOOMBER.png\" width=720px/>\n</p>\n\n# **Team's project:** Extract Transform Load (ETL) pipeline of Adidas sales and further product information, with an analytics component for sales trends and successful product identification, competitive research, and more\n*All team members have completed all steps in the [set up](setup.md) document.*\n\n[![Ploombler project: Transforming a Company through Building a ETL for Data Analysis with Ploomber](https://github.com/delatorrena2016/sql-etl-analytics/blob/Exploratory-work/images/VideoProjectScreenshot.png)](https://www.youtube.com/embed/qsj7ONz98nQ?si=Dj3qvzInbYqyc0Au \"Ploombler project: Transforming a Company through Building a ETL for Data Analysis with Ploomber\")\n\n## Description \n\nWe've developed an ETL (Extract, Transform, Load) pipeline for data analysis automation, specific to Adidas sales and main competitor sales, based on the provided datasets on section **Data sources**. This we think, can help address several important business problems and drive informed decision-making, for Adidas and for anyone in hopes of better understanding one's business as the ideas discussed here are universal. \n\nAppart from the contribution of our own insights displayed through our EDA (Exploratory Data Analysis); With the help of the good people from [Ploomber](https://ploomber.io/), we've build an application (Dashboard) with [JupySQL](https://jupysql.ploomber.io/en/latest/quick-start.html) + [Voila](https://voila.readthedocs.io/en/stable/index.html) as framework, with the use of a pipeline to prepare the data (ETL) automatically by tasks (see .yaml), and in the process generate subsecuent products as reports or logs (metadata), and Dashboard updates. We perform dataset extraction from 3 different Kaggle sources, cleaning, organizing and saving to an in-memory database [DuckDB](https://duckdb.org/) once prepared for storage and subsequent data analysis. Application is [Dockerized](https://www.docker.com/) as well. We used [MotherDuck](https://motherduck.com/docs/intro) for in-cloud data storage for our application.\n\nWe hope our project provides Adidas with a comprehensive understanding of its sales and customer data on a simple Dashboard over an easy to mantain, modify and improve data process. This knowledge can be used to make data-driven decisions, tailor marketing strategies, optimize product offerings, improve customer satisfaction, and align business strategies with customer needs and preferences.\n\n## Data sources\n\nThe following are the used data sources, all of public domain: \n1. [adidas-sales-dataset](https://www.kaggle.com/datasets/heemalichaudhari/adidas-sales-dataset) by Heemali Chaudhari licensed under CC0 1.0.\n    * Adidas sales dataset is a collection of data that includes information on the sales of Adidas products. This type of dataset may include details such as the number of units sold, the total sales revenue, the location of the sales, the type of product sold, and any other relevant information.\n    * It contains 9652 rows and 14 columns in total. (698.66 kB)\n2. [adidas-vs-nike](https://www.kaggle.com/datasets/kaushiksuresh147/adidas-vs-nike/) by Kaushik Suresh licensed under CC0 1.0.\n    * Contains product information about Nike and Adidas (Adidas is further divided into sub-brands), feature information including their ratings, discount, sales price, listed price, product description, and the number of reviews.\n    * It contains 3268 rows and 10 columns in total. (1.21 MB)\n3. [customer-shopping-trends-dataset](https://www.kaggle.com/datasets/iamsouravbanerjee/customer-shopping-trends-dataset/data) by Sourav Banerjee licensed under CC0 1.0.\n    * The Customer Shopping Preferences Dataset offers valuable insights into consumer behavior and purchasing patterns. This dataset captures a wide range of customer attributes including age, gender, purchase history, preferred payment methods, frequency of purchases, and more.\n    * It contains 3900 rows and 18 columns in total. (453.25 kB)\n\n*Specific provenance is listed for all datasets in the respective Kaggle websites.*\n\n## Methods\n\n### Staging\n1. Extraction, Transformation and Loading\n    * Cast read datasets to dataframe using [Pandas](https://pandas.pydata.org/docs/index.html) according to file extention; .csv, .xlsx.\n        * Align dataset for better tabular form.\n        * Formatting; Date, etc.\n        * Drop unuseful data for process speed; ID, Date retrieval, etc. (This datasets do not have NaN, already seen from describe() Pandas method)\n    * Save each dataset as table to local database DuckDB file logically.\n        * Tables existence verification.\n\n*Back and forth between ETL and wrangle during EDA process.*\n\n2. Wrangle. *Heavy use of matplotlib for plotting, numpy and pandas for calculations and statistics*\n    * adidas-sales-dataset\n        * Descriptive statistics\n    * adidas-vs-nike\n        * Descriptive statistics, i.e. Average Listing Price. \n        * Discount gap.\n        * Product offer amount by brand and by sub-brand. \n        * Costumer satisfaction and popularity from rating and reviews.\n        * Same brand best selling products.\n    * customer-shopping-trends-dataset\n### Production\n3. Dockerize\n4. MotherDuck\n5. Voila\n6. Ploomber Cloud\n\n## User interface your project will have\n\n![User Interface of dashboard](https://github.com/delatorrena2016/sql-etl-analytics/blob/Exploratory-work/images/ScreenshotApp.gif)\n\nWe convert Jupyter Notebooks visualizations to interactive Dashboards with Python Voila, application is hosted on [Ploomber Cloud](https://ploomber.io/cloud/). This way users don't need to install Python or any other dependencies to interact with the dashboard; all scripts run on the browser, users are also allowed to read and not edit the visualizations so as to mantain the integrity of the EDA. Easy to share HTML files.\n\nWe are proud to show the fininshed [Dashboard](https://purple-brook-2899.ploomberapp.io/) for the you to see.\n\n## Team members\n\n* Alvaro Gabriel de la Torre Navarro. [delatorrena2016](https://github.com/delatorrena2016)\n* Eduardo Padron. [fullmakeralchemist](https://github.com/fullmakeralchemist)\n"
    },
    {
      "name": "Edgar-Pacheco/Team4HacktoberFest2023",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/87995679?s=40&v=4",
      "owner": "Edgar-Pacheco",
      "repo_name": "Team4HacktoberFest2023",
      "description": "Pipeline ETL con componente de aprendizaje automatico",
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2023-09-25T02:29:59Z",
      "updated_at": "2023-10-30T19:53:15Z",
      "topics": [],
      "readme": "# Hacktoberfest 2023 Project: Generation of a Report using Voila Dashboard.\n\n## Theme\n\n<img align=\"right\" width=\"215\" height=\"373\" src=\"https://static.vecteezy.com/system/resources/previews/023/485/558/original/breast-cancer-pink-ribbon-png.png\">Generation of a <strong>report</strong> with <strong>proteomic profile data</strong>, i.e., <strong>protein expression</strong>, of <strong>77 patients diagnosed with <em>breast cancer</em></strong>, for a better understanding of how gene expression behaves in positive cases and to detect possible <strong>new molecular markers</strong> for <em>early</em> and <em>timely</em> detection, through Voila Dashboards.\n\n## Description \n\nThe approach we decided to take is mostly an **exploratory analysis** of data, in this case, *biological* data pertinent to positive breast cancer cases, which were previously diagnosed. This analysis, with a good interpretation, could open doors to the identification of *__novel molecular markers__* that allow early identification of this clinical condition; in the future, this type of reports, already well established and with a previous research in scientific literature, we can deploy them in the cloud to share results and that all interested persons can have access, in addition, could be complemented with the help of _ETL processes_ for a more effective management of data and a more efficient and faster exploratory analysis. Only 1 Dataset was used, with open license, which contains Proteomes of 77 patients.\n\nThe report will be generated by means of a **Python** script in a **Jupyter Notebook**, which will upload the data to a database for efficient data management, in order to generate HeatMaps, which will show the level of expression of the proteins in each profile. The **DuckDB** package will be used to efficiently manage the data in a Database and the **SeaBorn** package will be used to generate HeatMaps.\n\nThe **goal** of the project is to be able to deploy the report online, so that it can be available to anyone who is interested in studying what was done and give play to future research.\n\n## Data Sources\n\n- **Breast Cancer Proteomes:** Contains 77 proteomic (Protein Expression) profiles on previously diagnosed breast cancer cases obtained from the Clinical Proteomic Tumor Analysis (NCI/NIH), with expression values for ~12,000 proteins in each of the samples. License Unknown. (https://www.kaggle.com/datasets/piotrgrabo/breastcancerproteomes)\n\n## Methods\n\nFor the **analysis** and **exploration** of the data, the following is proposed:\n* Generate a _HeatMap_ in which the gene expression levels can be visualized in a simple and easy to understand way, with their respective explanation of key points.\n    * We use the **SeaBorn** package, which is for data visualization and drawing of attractive and informative statistical graphs, based on matplotlib. From this package we will obtain the HeatMaps.\n    * For efficient data management, **DuckDB** is used, which allows us to export the data we need to a database, and from there work with that DB.\n\n## User Interface\n\nThe final idea is to **deploy** the report through **Voila Dashboards**, to show the exploratory analysis online to anyone who is interested and to have a basis to continue generating reports in the future.\n\n## Team Members\n\n* __Jesús Gerardo Ortiz Romero / [@j-gorm](https://github.com/j-gorm)__: Generation of the Notebook with the exploratory analysis and deployment of the Report.\n* __Edgar Pacheco Castan / [@Edgar-Pacheco](https://github.com/Edgar-Pacheco)__\n* __Diego Peñaloza / [@diegopenaloza](https://github.com/diegopenaloza)__\n\n## Final Product\n<img align=\"right\" width=\"405\" height=\"289\" src=\"https://github.com/ploomber/ploomber/blob/master/_static/logo.png?raw=true\">Here is our <a href=\"https://silent-mouse-0958.ploomberapp.io\"><strong>final product!</strong></a>\nThanks a lot to <a href=\"https://github.com/ploomber/ploomber\"><strong>Ploomber</strong></a> team and their mentoring programs, thanks to them we were able to deploy the reports online!"
    },
    {
      "name": "Rakib-data-scientist/GenAI-Recommeder-App",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/137823730?s=40&v=4",
      "owner": "Rakib-data-scientist",
      "repo_name": "GenAI-Recommeder-App",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2023-09-09T21:24:52Z",
      "updated_at": "2024-02-06T09:23:06Z",
      "topics": [],
      "readme": "# GenAI- Recommender app \n\n* A LLM bot that is trained on custom toy data, you can search for a toy in its UI and it will provide recommended toy.\n* Using text to image search.\n* Basically it show the capapbility of bot to search the similar item and recommend you.\n\n# To run this app :\n  * clone this repo\n  * pip install -r requirements.txt\n  * cd app.py\n  * streamlit run app.py\n\n# UI :\n\n\n![UI](https://github.com/Rakib-data-scientist/GenAI-Recommeder-App/assets/137823730/e1eb7efb-2ed2-4b8b-9c8f-3ac63a99a61c)\n\n\n"
    },
    {
      "name": "Landlucas/content-relevance-score-api",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/20175697?s=40&v=4",
      "owner": "Landlucas",
      "repo_name": "content-relevance-score-api",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2023-09-12T03:36:10Z",
      "updated_at": "2024-08-05T19:08:17Z",
      "topics": [],
      "readme": ""
    },
    {
      "name": "SamthinkGit/llama2-terminal",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/92941012?s=40&v=4",
      "owner": "SamthinkGit",
      "repo_name": "llama2-terminal",
      "description": "Llama2 AI Tools for Powershell/Bash/Shell Assistance! Use AI even offline (In-Progress)",
      "homepage": "",
      "language": "Python",
      "created_at": "2023-08-24T15:37:08Z",
      "updated_at": "2024-11-24T15:20:07Z",
      "topics": [],
      "readme": "<p align=\"center\">\n  <img src=\"https://github.com/SamthinkGit/llama2-terminal/assets/92941012/80bcc101-25a5-48e6-a059-160f67acbeba\" alt=\"llama2-terminal-logo\" height=150/>\n</p>\n\n# Llama2-Terminal (In-Progress)\n**Llama2-Terminal** is an innovative AI-powered assistant for your terminal, designed to revolutionize the way you interact with your shell. Think of it as virtual environment but with AI-driven terminal interactions. With Llama2, you get an underlying environment where the AI listens and interacts directly with your shell, assisting you in real-time, playing games, fixing errors, and much more.\n\n<p align=\"center\">\n  <img src=\"https://github.com/SamthinkGit/llama2-terminal/assets/92941012/dd625011-252c-4ec5-953a-7d19592e2f66\" alt=\"llama2-terminal-logo\" height=600/>\n</p>\n\n# 🌟 Key Features\n- **Direct Shell Interaction**: Llama2 operates directly within your terminal, offering real-time assistance.\n- **Extensible Package System**: Just like plugins, you can easily extend Llama2's capabilities by installing AI modules from various repositories.\n- **Interactive Chat**: Use llama talk for a direct conversation with the AI.\n- **Error Assistance**: Llama2 can listen to your commands, detect errors, and suggest fixes on-the-fly.\n\n# 🚀 Getting Started\n## Installation\nYou can use pip manager to easily install this package!\n```powershell\n# Installation\npip install llama2-terminal\n```\n\nActivate and use this module at the moment. ^^\n```powershell\n# Init environment\nllama2\n\n# Talk with llama\nllama talk\n```\n\nFor Windows users *bitsandbytes* dependency can fail. You can install it from [this repository](https://github.com/jllllll/bitsandbytes-windows-webui) by using:\n```powershell\npython -m pip install bitsandbytes --prefer-binary --extra-index-url=https://jllllll.github.io/bitsandbytes-windows-webui\n```\n\n## 🛠 Extending Llama2\nThe true power of Llama2 lies in its extensibility. The packages directory is designed to allow developers to plug in AI modules from various repositories. This modular approach ensures that Llama2 can be continuously enhanced with new capabilities, games, tools, and more.\n\nWant to contribute a module? Check our module contribution guidelines.\n\n## 🤝 Contributing\nWe welcome contributions! Whether it's a new AI module, bug fixes, or feature enhancements, your input is valued.\n\n## 📜 License\nThis project is licensed under the MIT License. See the LICENSE file for details."
    },
    {
      "name": "BobbyLumpkin/docs2chat",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/50218116?s=40&v=4",
      "owner": "BobbyLumpkin",
      "repo_name": "docs2chat",
      "description": "An application for chatting with your documents using open-source, local LLMs.",
      "homepage": null,
      "language": "Python",
      "created_at": "2023-08-02T20:00:17Z",
      "updated_at": "2023-08-21T15:09:01Z",
      "topics": [],
      "readme": "# Docs2Chat"
    },
    {
      "name": "adborroto/transformers",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/2942602?s=40&v=4",
      "owner": "adborroto",
      "repo_name": "transformers",
      "description": null,
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2023-08-17T11:42:31Z",
      "updated_at": "2023-08-17T12:23:32Z",
      "topics": [],
      "readme": "# Learn Transformers\n\nRepo for the *Natural Language Processing: NLP With Transformers in Python* course, you can get 70% off the price with [this link!](https://www.udemy.com/course/nlp-with-transformers/?couponCode=70SEP2021) If the link stops working raise an issue or drop me a message somewhere:\n\n[YouTube](https://www.youtube.com/c/jamesbriggs) \n| [Discord](https://discord.gg/c5QtDB9RAP)\n\nI also have plenty of free material on YouTube 😊\n"
    },
    {
      "name": "Azizkhaled/NLP-Concepts-and-Transformers",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/35635341?s=40&v=4",
      "owner": "Azizkhaled",
      "repo_name": "NLP-Concepts-and-Transformers",
      "description": "My NLP notes and projects",
      "homepage": "",
      "language": "Jupyter Notebook",
      "created_at": "2023-08-10T18:41:20Z",
      "updated_at": "2023-09-04T07:06:07Z",
      "topics": [],
      "readme": "\n# NLP\nThis Repo contains some of my notes in relation to Natural Language Processing (NLP) and Transformer models. I will try to update it as I proceed with my NLP journey.\n\n### Requirments\nThe requirements and Library used can be found in [requirements.txt](https://github.com/Azizkhaled/NLP-with-Aziz/blob/main/requirements.txt). Hopefully, all notebooks can be run on Google Colab with no problems. \n\n## Contents\n### 1. [Pre-processing Techniques.ipynb](https://github.com/Azizkhaled/NLP/blob/main/NLP_Preprocessing.ipynb)\nI will share some common techniques used for NLP pre-processing in the \n[NLP_Preprocessing.ipynb](https://github.com/Azizkhaled/NLP-with-Aziz/blob/main/NLP_Preprocessing.ipynb)\n          \n        -  Removing Stopwords\n        -  Tokens\n        -  Stemming\n        -  Lemmatization\n        -  Unicode Normalization\n    \n\n### 2. [Attention.ipynb](https://github.com/Azizkhaled/NLP/blob/main/NLP_Attention.ipynb)\nCommon Attention mechanisms in relation to NLP can be found in [NLP_Attention.ipynb](https://github.com/Azizkhaled/NLP-with-Aziz/blob/main/NLP_Attention.ipynb)\n\n        - Encoder-Decoder Attention (Dot-Product Attention)\n        - Self Attention\n        - Bidirectional Attention\n        - Multi-head Attentenion\n\n### 3. [Language Classifications with Transformers.ipynb](https://github.com/Azizkhaled/NLP/tree/main/Text_Classification/NLP_Language_Classification_Flair_Transformers.ipynb)\nSome Sentiment classification techniques are explored in this notebook. \n        \n        - Sentiment Classification with flair\n        - Sentiment Classification with Transformers\n\n### 4. [Long_text_classification.ipynb](https://github.com/Azizkhaled/NLP/tree/main/Text_Classification/Long_text_classification.ipynb)\nSimilar to 3, but here we explore other techniques if the sample size is bigger than the transformer capabilities \n        \n        - Bert issue\n        - Long text solution: Window Partitioning\n        - Applying window method using Pytorch\n\n### 5. [Named Entity Recognition.ipynb](https://github.com/Azizkhaled/NLP-with-Aziz/blob/main/NER.ipynb) and [NER on Subreddits](https://github.com/Azizkhaled/NLP-with-Aziz/blob/main/NER_On_Sub_reddits.ipynb)\nHere, we discover how to perform Entity Recognition with Spacy, and how to extract a single entity of interest. [NER.ipynb]\nWe also go through the steps of pulling data from subreddits using Praw, and extracting the organization entities from the top 100 hottest posts in the r\\investing subreddits \n\n        - Download NER model\n        - Extract Entity\n        - Pulling Data from Reddit\n        - Extract ORGs from data\n        - Function to get the most mentioned ORG\n\n### 6. Question Answering:\n\n#### a. [Question Answering.ipynb](https://github.com/Azizkhaled/NLP/tree/main/Qustions%20and%20Answers%20models/Question_Answering.ipynb)\nHere we use the SQuAD dataset for an Open Domain Question Answering (ODQA) application. \n\n          - Download and Preprocess the data \n          - Initialize the Bert Transformer model\n          - Prepare the Data Pipeline\n          - Make predictions\n          - Evaluate with an exact match (EM)\n          - Evaluate with Rouge \n\n#### b. [Retrievers and ElasticSearch with Haystack.ipynb](https://github.com/Azizkhaled/NLP/tree/main/Qustions%20and%20Answers%20models/Q&A_Elasticsearch_Haystack_Retrivers.ipynb) \nIn this notebook, we discover how to connect to local Elasticsearch with Haystack. Whats covered:\n\n          - Initializing the connection\n          - Adding data to the cluster\n          - Retrieving Data with TF-IDF\n          - Remove duplicates\n          - Retrieving data with BM25\n\n#### c. [Q&A_Full_Reader_Retrieval_model](https://github.com/Azizkhaled/NLP/tree/main/Qustions%20and%20Answers%20models/Q&A_Full_Reader_Retrieval_model.ipynb) \n\n          - Initialize the connection with ElasticSearch\n          - Dense Passage Retriever (DPR)\n          - BERT Reader model\n          - Reader retriever pipeline with Haystack\n          - Running Queries\n\n### 7. [Similarity Search](https://github.com/Azizkhaled/NLP_with_Aziz/blob/main/Similarity/Similarity_Search.ipynb)\n\n          - Similarity with Transformers and Pytorch\n          - Similarity with Sentence-Transformers\n  \n\n## NLP Projects: \n### 1. Movie Reviews Classification\nIn this project, we use the Rotten Tomatoes dataset from Kaggle. We try to classify it into one of 5 classes; (negative, somewhat negative, neutral, somewhat positive, and positive). \nWe fine-tune the Bert pre-trained model \n[Movie_Reviews_Classification_with_TF.ipynb](https://github.com/Azizkhaled/NLP/tree/main/Projects/Movie%20Reviews%20Classification/Movie_Reviews_Classification_with_TF.ipynb)\n\n  - Kaggle on Google Colab\n  - Downloading the Rotten Tomatoes movie reviews dataset\n  - Preprocessing the data:\n    \n        - Removing duplicates\n        - Tokenizing\n        - Bert tokenizer\n        - Save the tokens\n        - One hot encoding of the labels\n      \n  - Creating an input pipeline:\n    \n        - Shuffling the data and batch it\n        - Split the dataset\n        - Build and train the model\n        - save the model\n  - Make predictions\n\n### 2. Organizational Sentiment from r\\investing\nIn this project, we go through the top 500 posts in the r\\investing, we use the flair sentiment model to classify each post, then extract the organizational entities of each post. Lastly, we get the average sentiment for each organization and rank them from most positive to negative. Data was pulled using the same method in [NER_On_Sub_reddits.ipynb](https://github.com/Azizkhaled/NLP/tree/main/Projects/Organizational%20Sentiment%20from%20r%5Cinvesting/NER_On_Sub_reddits.ipynb) \nProject : [Sentiment_of_organizations_in_Reddit.ipynb](https://github.com/Azizkhaled/NLP/tree/main/Projects/Organizational%20Sentiment%20from%20r%5Cinvesting/Sentiment_of_organizations_in_Reddit.ipynb)\n\n### 3. Open-Domain QA Project\n\nbuilding an open-domain question answering system. It involves three key components: a database containing UN report data, a retriever utilizing Elasticsearch for document retrieval, and a reader utilizing the bert-base-cased-squad2 model for context comprehension. [QA_project](https://github.com/Azizkhaled/NLP_with_Aziz/blob/main/Projects/QA_with_bert%26ElasticSearch/QA_project.ipynb) \n\nProject Components:\n  - Prepare the Database: \n\n        - Setting Up an Index in Elasticsearch\n        - Format the Data\n        - Uploading the formatted data to the Elasticsearch index.\n  \n  - Retriever and Reader Setup: \n       \n        - BM25 retriever\n        - FARMReader with the `bert-base-cased-squad2`\n        - ExtractiveQAPipeline\n  \n  - Running Queries\n\n### 4. Fine-Tuning BERT for Masked Language Modeling (MLM) and Next Sentence Prediction (NSP)\n\nThis project delves into the fine-tuning of a BERT language model for the tasks of MLM and NSP. Through a step-by-step approach, it explores two distinct training methods: manual PyTorch training and utilizing HuggingFace's Trainer class. [MLM_NSP_Training.ipynb](https://github.com/Azizkhaled/NLP_with_Aziz/blob/main/Projects/Training-Pretrained-Bert-Model/MLM_NSP_Training.ipynb) \n\nThe project contains: \n\n        - Data loading, cleaning, and tokenization, \n        - Label tensor creation and random token masking. \n        - Label tensor creation for NSP\n        - Creating a PyTorch dataset \n        - Training using manual PyTorch training\n        - Training using HuggingFace's Trainer class \n        \n"
    },
    {
      "name": "hanzotaz/smart_receptionist_system",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/55962807?s=40&v=4",
      "owner": "hanzotaz",
      "repo_name": "smart_receptionist_system",
      "description": "This is the repository for my degree Final Year Project titled Smart Receptionist System using Voice Assistant and Indoor Positioning System",
      "homepage": null,
      "language": "Dart",
      "created_at": "2023-08-01T08:12:24Z",
      "updated_at": "2024-05-03T09:16:56Z",
      "topics": [],
      "readme": "# Smart Receptionist System with Voice Assistant and Indoor Positioning System\n\nThis is the Final Year Project for my Computer Engineering degree which uses Natural Language Processing (NLP) for Question Answering and Indoor Positioning for building navigation. All of this is to assist companies with handling guests similar to that of a traditional receptionist.\n\n## Running the System\n\nThis system is made out of different Docker containers and can be run using docker-compose command.\n\n``` bash\n> docker compose up\n[+] Running 0/8\n - elasticsearch 7 layers [⠀⠀⠀⠀⠀⠀⠀]      0B/0B      Pulling                   4.4s \n   - b38629870fdb Pulling fs layer                                             0.6s \n   - 4a2fc9d810b8 Pulling fs layer                                             0.6s \n   - e2926999e93a Pulling fs layer                                             0.6s \n   - 1bfda44c7b09 Waiting                                                      0.6s\n   - 901547b54de2 Waiting                                                      0.6s\n   ...\n```\n\n## How it Works\n\n![system_architecture](img/architecture.png)\n\nThis system uses the browser as the main interface for the user where they can use voice input(speech-to-text using WhisperAI) to ask question and the HaystackAI(the NLP framework) will output appropriate answer. For the Indoor Navigation, a seperate app was built using Flutter and the waypoint is made using Estimote beacon which connected with the user's phone using bluetooth. This will pinpoint the current location of the user and an algorithm based on the A* algorithm used to determined which path is the best for the user to get to their intended location.\n\nEven thought the system was planned to be a microservice architecture, the dependence on one service to another made it to be a more monolithic architecture that is made up of seperate Docker containers.\n\n## How it performs\n\nThe speech recognition which is part of the speech-to-text that is powered by OpenAI's Whisper get the accuracy of 66.67% and the actual Question Answering manages to get the accuracy of 80% which is pretty impressive considering the fact that I'm using roberta-base model without fine-tuning it for the application or scope of this project.\n\nAsked Question | Correct Answer | Returned Answer\n---|---|---\nWhat programmes does UNIMY offer? | UNIMY offers foundation, diploma, undergraduate and postgraduate levels programmes | foundation, diploma undergraduate and postgraduate levels\nWhen was UNIMY established? | January 2013 | January 2013\nWhat does the CSE do? | Helps to educate and facilitate students to have a balanced life as well as to excel in both academic and personal development | To provide a supportive, friendly, and challenging environment for academic success and individual development\nWhat does the campus provide? | The city campus is fully equipped with academic, hostel and student facilities | postgraduate programmes\n\n*Some sample of the Question Answering mechanism.*\n\n## Conclusion\n\nOverall the project was fairly successful considering the limited amount of time it took for me to do the research required to build this system as well to build the system itself and the fact that it is a individual project. I learnt a lot of new things especially in AI, mobile app development, web development and other things."
    },
    {
      "name": "bhattarai333/AI-Resume",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/10687373?s=40&v=4",
      "owner": "bhattarai333",
      "repo_name": "AI-Resume",
      "description": "React chatbot with generative question answering AI bot that answers professional questions about me.",
      "homepage": null,
      "language": "Python",
      "created_at": "2023-07-10T02:09:33Z",
      "updated_at": "2023-08-04T06:14:55Z",
      "topics": [],
      "readme": ""
    },
    {
      "name": "yasyf/haystack-hybrid-embedding",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/709645?s=40&v=4",
      "owner": "yasyf",
      "repo_name": "haystack-hybrid-embedding",
      "description": "Sparse-Dense Embeddings for Pinecone in Haystack",
      "homepage": null,
      "language": "Python",
      "created_at": "2023-06-29T04:37:08Z",
      "updated_at": "2023-07-24T20:59:33Z",
      "topics": [],
      "readme": "# `haystack-hybrid-embedding`\n\nRecently, Pinecone announced support for [Sparse-dense embeddings](https://docs.pinecone.io/docs/hybrid-search), allowing for hybrid vector search (both semantic and keyword search).\n\n[`haystack`](https://haystack.deepset.ai/) is a fantastic NLP framework that does not yet support hybrid vectors for [`Retrievers`](https://docs.haystack.deepset.ai/docs/retriever).\n\nThis little library helps temporarily bridge the gap!\n\n## Installation\n\n```shell\n$ pip install haystack-hybrid-embedding\n```\n\n## Usage\n\n```python\nfrom haystack_hybrid_embedding import SpladeEmbeddingEncoder\nfrom haystack_hybrid_embedding.pinecone import PineconeHybridDocumentStore, SparseDenseRetriever\n\ndocument_store = PineconeHybridDocumentStore(...)\n\nretriever = SparseDenseRetriever(\n  sparse_encoder=SpladeEmbeddingEncoder(),\n  alpha=0.8,\n  ...\n)\n```\n\n### Replacing `EmbeddingRetriever`\n\nSimply replace your imports of `PineconeDocumentStore` and `EmbeddingRetriever`/`MultihopEmbeddingRetriever`.\n\n\n\n```diff\n1,2c1,3\n< from haystack.document_stores.pinecone import PineconeDocumentStore\n< from haystack.nodes import EmbeddingRetriever, MultihopEmbeddingRetriever\n---\n> from haystack_hybrid_embedding import SpladeEmbeddingEncoder\n> from haystack_hybrid_embedding.pinecone import PineconeHybridDocumentStore, SparseDenseRetriever, SparseDenseMultihopRetriever\n>\n4c5\n< document_store = PineconeDocumentStore(...)\n---\n> document_store = PineconeHybridDocumentStore(...)\n6c7\n< retriever = EmbeddingRetriever(\n---\n> retriever = SparseDenseRetriever(\n7a9,10\n>     sparse_encoder=SpladeEmbeddingEncoder(),\n>     alpha=0.8,\n```\n\n## Config\n\nThere are only two additional parameters exposed on `SparseDenseRetriever` over `EmbeddingRetriever`:\n\n- `sparse_encoder` embeds both queries and documents into sparse vectors\n- `alpha` controls the weighting between the sparse and dense vectors (`0` is all sparse, and `1` is all dense)\n"
    },
    {
      "name": "leomaurodesenv/qasports-dataset-scripts",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/11961778?s=40&v=4",
      "owner": "leomaurodesenv",
      "repo_name": "qasports-dataset-scripts",
      "description": "Scripts used to generate the (Question Answering) QASports Dataset",
      "homepage": "",
      "language": "Python",
      "created_at": "2023-06-13T16:14:02Z",
      "updated_at": "2025-03-30T08:28:57Z",
      "topics": [],
      "readme": "# 📄 QASports: Question Answering Dataset about Sports\n\n[![GitHub](https://img.shields.io/static/v1?label=Code&message=GitHub&color=blue&style=flat-square)](https://github.com/leomaurodesenv/qasports-dataset-scripts)\n[![MIT license](https://img.shields.io/static/v1?label=License&message=MIT&color=blue&style=flat-square)](LICENSE)\n[![GitHub Workflow Status](https://img.shields.io/github/actions/workflow/status/leomaurodesenv/qasports-dataset-scripts/continuous-integration.yml?label=Build&style=flat-square)](https://github.com/leomaurodesenv/qasports-dataset-scripts/actions/workflows/continuous-integration.yml)\n\n\nThis repository presents a collection of codes to elaborate the dataset named \"QASports\", the first large sports question answering dataset for open questions. QASports contains real data of players, teams and matches from the sports soccer, basketball and American football. It counts over 1.5 million questions and answers about 54k preprocessed, cleaned and organized documents from Wikipedia-like sources.\n\n- **Paper**: Pedro Calciolari Jardim, Leonardo Mauro Pereira Moraes, and Cristina Dutra Aguiar. [QASports: A Question Answering Dataset about Sports](https://doi.org/10.5753/dsw.2023.233602). In Proceedings of the Brazilian Symposium on Databases: Dataset Showcase Workshop, pages 1-12, Belo Horizonte, Minas Gerais, Brazil, 2023.\n\n> **Abstract**: Sport is one of the most popular and revenue-generating forms of entertainment. Therefore, analyzing data related to this domain introduces several opportunities for Question Answering (QA) systems, such as supporting tactical decision-making. But, to develop and evaluate QA systems, researchers and developers need datasets that contain questions and their corresponding answers. In this paper, we focus on this issue. We propose QASports, the first large sports question answering dataset for extractive answer questions. QASports contains more than 1.5 million triples of questions, answers, and context about three popular sports: soccer, American football, and basketball. We describe the QASports processes of data collection and questions and answers generation. We also describe the characteristics of the QASports data. Furthermore, we analyze the sources used to obtain raw data and investigate the usability of QASports by issuing \"wh-queries\". Moreover, we describe scenarios for using QASports, highlighting its importance for training and evaluating QA systems.\n\n---\n## Download\n\n- 🎲 Dataset: https://osf.io/n7r23/\n- 🎲 Dataset: https://huggingface.co/datasets/PedroCJardim/QASports/\n\n---\n## Dataset Elaboration\n\nWe have sorted the resources into five separate folders.\n- 🔧 [src/gathering/](src/gathering/) - Gather wiki links.\n- 🔧 [src/fetching/](src/fetching/) - Fetch raw HTML from links.\n- 🔧 [src/processing/](src/processing/) - Process and clean textual data.\n- 🔧 [src/extracting_context/](src/extracting_context/) - Extract contexts from data.\n- 🔧 [src/question_answer/](src/question_answer/) - Generate questions and answers.\n- 🔧 [src/sampling/](src/sampling/) - Sample representative questions and answers.\n\n```sh\n# Creating a virtual environment\n$ python -m venv .venv\n$ source .venv/bin/activate\n# Installing packages\n$ pip install -r requirements.txt\n# Setup pre-commit\n$ pre-commit install\n\n# 1. Gathering links (run: ~35 seconds)\n$ python -m src.gathering.run\n# 2. Fetching wiki pages (run: ~40h)\n$ python -m src.fetching.run\n# 3. Processing, clean text (run: ~50 minutes)\n$ python -m src.processing.run\n# 4. Extracting context (run: ~35 seconds)\n$ python -m src.extracting_context.run\n# 5. Questions and answers generation (run: ~36 days)\n$ python -m src.question_answer.run\n# 6. Sampling representative questions (run: )\n$ python -m src.sampling.run\n```\n\n---\n## Citation\n\n```tex\n# QASports: A Question Answering Dataset about Sports\n@inproceedings{jardim:2023:qasports-dataset,\n    author={Pedro Calciolari Jardim and Leonardo Mauro Pereira Moraes and Cristina Dutra Aguiar},\n    title = {{QASports}: A Question Answering Dataset about Sports},\n    booktitle = {Proceedings of the Brazilian Symposium on Databases: Dataset Showcase Workshop},\n    address = {Belo Horizonte, MG, Brazil},\n    url = {https://github.com/leomaurodesenv/qasports-dataset-scripts},\n    publisher = {Brazilian Computer Society},\n    pages = {1-12},\n    year = {2023}\n}\n```\n"
    },
    {
      "name": "cyberspyde/jbnu",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/52118091?s=40&v=4",
      "owner": "cyberspyde",
      "repo_name": "jbnu",
      "description": "Unlocking JBNU Success: The Ultimate International Students' Portal for Academic Excellence.",
      "homepage": "https://jbnu.ac.kr",
      "language": "HTML",
      "created_at": "2023-06-15T08:16:55Z",
      "updated_at": "2024-08-23T07:46:43Z",
      "topics": [
        "jbnu"
      ],
      "readme": "# JBNU Success for International Students\n\n![Study Hub](jbnu.jpg)\n\nWelcome to the JBNU Study Hub repository! This platform aims to provide a collaborative space for international students at Jeonbuk National University (JBNU) to share their study materials, including homework, midterm and final exams, to help each other excel academically. Our objective is to support students in achieving top grades while minimizing the time spent struggling with the university's approach to education.\n\n## Objectives\n\n- Share your study materials: Upload and share your homework, midterm and final exams, lecture notes, and other relevant study resources.\n- Collaborate and learn together: Engage with fellow international students, discuss challenging topics, and gain insights to enhance your understanding.\n- Boost academic performance: Access a vast collection of materials contributed by students, enabling you to achieve excellent grades with minimal effort.\n- Empower yourself: Take control of your education, learn at your own pace, and pursue your goals beyond the limitations of traditional classroom settings.\n\n## How to Contribute\n\n1. **Fork** this repository to your GitHub account.\n2. **Clone** the forked repository to your local machine.\n3. **Create a new folder** for each subject or course you want to share materials for.\n4. **Upload your study materials** (e.g., homework, exams, notes) into the appropriate folders.\n5. **Commit** your changes and **push** them to your forked repository.\n6. Submit a **pull request** to have your contributions reviewed and merged into the main repository.\n\nPlease ensure that you follow the repository's guidelines and respect intellectual property rights when sharing materials.\n\n## Contact\n\nIf you have any questions, suggestions, or concerns, please feel free to contact us:\n\n- Email: [cyberspyde@gmail.com](mailto:cyberspyde@gmail.com)\n- Telegram Group Access: [t.me/cyberspyde_admin](https://t.me/cyberspyde_admin)\n\nWe value your feedback and are committed to continuously improving the study experience for all contributors.\n\nLet's come together, share knowledge, and achieve our academic goals!\n\n"
    },
    {
      "name": "LoyumM/Machine-Learning-and-Deep-learning",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/90868002?s=40&v=4",
      "owner": "LoyumM",
      "repo_name": "Machine-Learning-and-Deep-learning",
      "description": "Repo for storing notes and pracitcal implemtations",
      "homepage": "",
      "language": "Jupyter Notebook",
      "created_at": "2023-01-09T17:31:54Z",
      "updated_at": "2024-09-25T14:13:19Z",
      "topics": [
        "attention",
        "deep-learning",
        "machine-learning",
        "nlp",
        "nltk",
        "python",
        "pytorch",
        "sckiit-learn",
        "tensorflow",
        "transformers"
      ],
      "readme": "\nRepo for storing notes and pracitcal implemtation from multiple sources and courses\n"
    },
    {
      "name": "Hossamster/QA-System-Covid19-Bert",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/89027268?s=40&v=4",
      "owner": "Hossamster",
      "repo_name": "QA-System-Covid19-Bert",
      "description": "This is my Graduation Project: Build Artificial Intelligence and Natural Language Processing (NLP) to construct and implement a Question Answering system for COVID-19.",
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2023-05-31T21:49:41Z",
      "updated_at": "2023-06-07T11:57:42Z",
      "topics": [],
      "readme": "# QA-System-Bert\nThis is my Graduation Project: Build Artificial Intelligence and Natural Language Processing (NLP) to construct and implement a Question Answering system for COVID-19.\n"
    },
    {
      "name": "simondanielsson/SEB-OpenQA",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/70206058?s=40&v=4",
      "owner": "simondanielsson",
      "repo_name": "SEB-OpenQA",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2023-05-24T16:09:33Z",
      "updated_at": "2023-08-05T13:06:40Z",
      "topics": [],
      "readme": "# SEB-OpenQA\n\nThis is a repository for open-domain question-answering (OpenQA) systems, containing a pipeline for evaluating \nsystem configurations on labeled OpenQA datasets. It is built upon the `farm-haystack` package by _deepset_, and was \ndeveloped during the Master's thesis project _Question-answering in the Financial Domain_ at The Factulty of Engineering at \nLund University in conjunction with the bank _SEB_. See below for authors. See [this repo](https://github.com/simondanielsson/SEB-OpenQA-app)\nfor a QA application built upon this thesis' work.\n\n![image](images/gen_vs_extr.png)\n\n### 1. The ORQA pipeline \n\n:checkered_flag: The goal of an open-retrieval question-answering (ORQA) system is to answer a given question\nusing a (potentially huge) collection of documents. Our pipelines can be either\n- **extractive**, meaning answer is a minimal span containing the answer to the question, found in one of the documents, or\n- **generative**, meaning the answer is generated from a prompt containing a question, documents, instructions, and/or examples.\n\n\n#### 1.1 Format \n\n#### Evaluation pipeline with `haystack`'s evaluation script\n\nThe pipeline takes a experiment and pipeline configuration, and outputs performance scores across a dataset in json.  \n\n##### Inference pipeline with official evaluation scripts \n\nThe OpenQA pipeline inputs a SQuAD-like ORQA dataset (like *Natural Questions*), \nand outputs results to `evaluation/output/<dataset_name>/<date_and_time>/`. The results \ninclude the predictions, experiment configurations, and metadata (such as inference runtime). \n\nPredictions conform to the format required by the SQuAD evaluation script: namely pairs of \n(`question_ids`, `answer_text`).\n\n### 2. Installation\n\nFirst create a virtual environment\n\n```bash\npython3 -m venv venv\n. venv/bin/activate\npip install -r requirements.txt\n. .env\n```\n\n:exclamation: *Note: If you want GPU acceleration, you additionally have to download the \nGPU-related dependencies to PyTorch and FAISS. Do this before installing the other dependencies\nusing `pip`. For instance, using conda,*\n\n```bash\nconda install pytorch torchvision torchaudio pytorch-cuda=11.7 -c pytorch -c nvidia\n```\n\nand \n\n```bash\npip install farm-haystack[all-gpu] \n```\n\n### 3. How to run\n\nConfigure your experiment and pipelines in the yaml's under `configs/`. Then execute the pipeline by running \n\n```bash\npython3 src/\n```\n\nThis requires the required train/dev data to be saved to disk. See Section \n*4. Downloading data* below for more information.   \n\n### 4. Configure experiments\n\nThere are two configurations files governing all ORQA experiments: \nan [experiment config](configs/config_open_domain.yaml) and a \n[pipeline config](configs/basic.haystack-pipeline.yml). By default, the inference \npipeline assumes these are found in `configs/`. The pipeline config used by the experiment\nis an entry in the experiment config (`pipeline_config`). Make sure all entries in the configs are properly filled in.\n\nThere is also an option to supply different configs, instead of changing the default configs.\nThis is particularly useful for reproducing old experiments using the configs supplied by the pipeline \noutput (found in `evaluation/output/<dataset_name>/<run_id>/`). See below for how this is done.\n\n```\nusage: __main__.py [-h] [--config_path [CONFIG_PATH]] [--pipeline_config_path [PIPELINE_CONFIG_PATH]]\n\nRun inference on a ORQA pipeline\n\noptions:\n  -h, --help            show this help message and exit\n  --config_path [CONFIG_PATH]\n                        Config for this run, defaults to /Users/simondanielsson/Documents/Master/SEB-\n                        workspace/configs/config_open_domain.yaml.\n  --pipeline_config_path [PIPELINE_CONFIG_PATH]\n                        Pipeline config for this run, defaults to what is found in /Users/simondanielsson/Documents/Master/SEB-\n                        workspace/configs/config_open_domain.yaml.\n```\n\n### 4. Downloading data\n\nThe pipeline currently only supports loading data from disk. Additionally, Haystack\nis assuming all data is SQuAD-formatted. We provide scripts for converting a few datasets into\nSQuAD-format, strongly influenced by e.g. Facebook's own Natural Questions simplification script.\n\nWe provide for reference an example dataset under `data/nq_subset/` of the appropriate format. It is a subset \nof the Natural Questions dataset. \n\n#### 4.1 Converting Natural Questions\n1. Download the Natural Questions train and dev set from the [official website](https://ai.google.com/research/NaturalQuestions/download).\n2. Invoke `squad_fmt_conversion/nq_to_squad/nq_to_squad.py` as described in the module docstring. The train set should \nbe converted to JSON Lines using the `--as_jsonl` flag.\n3. Point to the train and dev set paths in the config.\n\n#### 4.2 Converting TriviaQA\n\nHave a look in `squad_fmt_conversion/triviaQA_to_squad/README.md`.\n\n\n#### Authors\n\nThe thesis was conducted by Simon Danielsson, and Nils Romanus."
    },
    {
      "name": "ankitshaw/CSE635_NLP_Project",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/25771394?s=40&v=4",
      "owner": "ankitshaw",
      "repo_name": "CSE635_NLP_Project",
      "description": null,
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2023-03-06T05:58:41Z",
      "updated_at": "2023-05-14T20:09:26Z",
      "topics": [],
      "readme": "# CSE635_NLP_Project\n##Topic-Based Empathic Chatbot\n\n## Setup\n\n```\npip install -r requirement.txt\npython -m spacy download en_core_web_lg\n```\n## Load IR knowledge base\n```\n#Download SimpleWiki dump from WikiMedia website and place in root directory\npython wikibot/wikibot_loader.py\n```\n\n## How to Run\n```\nstreamlit run app.py\n```\n"
    },
    {
      "name": "maximuslee1226/NLP",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/17769293?s=40&v=4",
      "owner": "maximuslee1226",
      "repo_name": "NLP",
      "description": null,
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2023-05-12T17:43:08Z",
      "updated_at": "2023-05-12T21:01:12Z",
      "topics": [],
      "readme": ""
    },
    {
      "name": "aiswaryasankar/dbrief",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/7874177?s=40&v=4",
      "owner": "aiswaryasankar",
      "repo_name": "dbrief",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2021-11-25T22:49:59Z",
      "updated_at": "2023-04-07T04:37:48Z",
      "topics": [],
      "readme": "Welcome to Dbrief.AI!\r\n\r\n## Set up Process\r\n\r\n1) Create a virtual env `virtualenv ~/eb-new`\r\n2) Activate by running `source ~/eb-new/bin/activate`\r\n3) Clone repository and install `brew install git-lfs`\r\n4) Install mysql and create a password\r\n5) Delete all the migration files in the migration folders and run `python manage.py makemigrations` followed by `python manage.py migrate`\r\n6) Import all the requirements from requirements.txt through `pip install -r requirements.txt`\r\n7) Run `python manage.py runserver`\r\n8) Install / set up docker\r\n9) Pull the elastic search docker image (commands included in the elastic search section)\r\n10) Update the password in settings.py\r\n11) Run the migrate commands\r\n\r\n\r\n## Testing Process\r\nWe currently set up a few different directories for tests including repositories and handlers.  The repository tests check the read / write operations with the database and the view tests make calls directly to the /endpoint and check that it runs with a 200 response.  All databases can be pre populated using a fixtures file which includes dummy data and is loaded into the db prior to running the tests. An example can be found in articleRec/fixtures.  In order to run all of the tests, run ```./manage.py test```. In order to run tests in a particular directory, run ```./manage.py test topicModeling``` for example.\r\n\r\n\r\n\r\n## Logging\r\nWe currently use logtail for all logging purposes from our production system. Please log into in order to access the logs.  Please ask for permissions to access logging.\r\n\r\n\r\n## Deployment\r\nIn order to contribute to this repo, first create a branch and commit your changes to that branch.\r\n`git checkout [branchName]`\r\n`git add -A`\r\n`git commit -m ''`\r\n`git push origin [branchName]`\r\nDon't add any of the files in \\modelWeights since those are specific to your locally trained model and won't work / will break production temporarily.\r\nOnce your change is approved, we will merge and deploy to AWS Beanstalk through `eb deploy --staged`.  This requires prod deployment credentials and shouldn't be done without asking.\r\n\r\n\r\n## Mysql Migration\r\nIn order to migrate database changes you need to run `python manage.py makemigrations` which will generate all the make migration files. After that you can run `python manage.py migrate` in order to map those migrations to the db tables. You can handle migrations per service through `python manage.py migrate [serviceName]`.\r\n\r\n\r\n## Checking prod Mysql\r\nIn order to check production mysql, you can log into http://infra.eba-ydmy6xs3.us-west-2.elasticbeanstalk.com/admin.  For now the login is `admin` and password is `admin`. This will be updated in the near future. Do not use this for anything other than reads.\r\n\r\n\r\n## Elastic Search\r\nIn order to set up elastic search on docker you need to run the following commands -\r\n1. ```sudo systemctl start docker```\r\n2. ```sudo docker pull docker.elastic.co/elasticsearch/elasticsearch:7.9.2```\r\n3. ```sudo docker run -d -p 9200:9200 -e 'discovery.type=single-node' elasticsearch:7.9.2```\r\n\r\n"
    },
    {
      "name": "gjreda/haystack-pdf-bot",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/921995?s=40&v=4",
      "owner": "gjreda",
      "repo_name": "haystack-pdf-bot",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2023-05-09T22:35:26Z",
      "updated_at": "2023-05-31T13:33:10Z",
      "topics": [],
      "readme": "# haystack pdf chatbot\n\nPrototyping a bot that allows for chat over PDF documents.\n\nNote that this uses BM25 for document retrieval, rather than a semantic approach with embeddings.\n\n## Setup\nThis uses [Poetry](https://python-poetry.org/) for dependency management. To install dependencies:\n```bash\n$ poetry install\n```\n\nYou'll also need to create a `.env` file and add your `OPENAI_API_KEY` to it (see `.env.example`).\n\n## Usage\nThe command below will run the pipeline on the `papers` directory, which contains a few PDFs. It will then start a REPL where you can ask questions about the PDFs. You can exit the Q&A loop by typing \"exit\" or cmd/ctrl + c\n\n```python\n$ poetry run python haystack_pdf_bot/main.py --pdf_directory=papers\n```\n\n## Example\n![/static/example.png](/static/example.png)"
    },
    {
      "name": "krishika-r/semantic_search",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/36788439?s=40&v=4",
      "owner": "krishika-r",
      "repo_name": "semantic_search",
      "description": null,
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2023-05-08T17:07:32Z",
      "updated_at": "2023-07-04T14:46:45Z",
      "topics": [],
      "readme": "# semantic_search"
    },
    {
      "name": "ameliekong609/NLP-foundation",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/97581675?s=40&v=4",
      "owner": "ameliekong609",
      "repo_name": "NLP-foundation",
      "description": null,
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2023-05-04T05:51:12Z",
      "updated_at": "2023-05-31T09:23:20Z",
      "topics": [],
      "readme": "# Learn Transformers\n\nRepo for the *Natural Language Processing: NLP With Transformers in Python* course, you can get 70% off the price with [this link!](https://www.udemy.com/course/nlp-with-transformers/?couponCode=70SEP2021) If the link stops working raise an issue or drop me a message somewhere:\n\n[YouTube](https://www.youtube.com/c/jamesbriggs) \n| [Discord](https://discord.gg/c5QtDB9RAP)\n\nI also have plenty of free material on YouTube 😊\n"
    },
    {
      "name": "pmarkun/redeiro-bot",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/682827?s=40&v=4",
      "owner": "pmarkun",
      "repo_name": "redeiro-bot",
      "description": "GPT based discord Bot",
      "homepage": null,
      "language": "Python",
      "created_at": "2023-04-21T20:41:20Z",
      "updated_at": "2023-05-12T17:02:19Z",
      "topics": [],
      "readme": "# Discord GPT-4 Assistant Bot\n\nThis is a simple Discord bot that uses the OpenAI GPT-4 API to generate responses to user messages. The bot can respond to messages when mentioned or when the `!ask` command is used. It also allows for managing the bot's persona using the `!persona` command.\n\n## Requirements\n\n- Python 3.8+\n- discord.py\n- openai\n- python-dotenv\n\n## Setup\n\n1. Clone this repository.\n2. Install the required dependencies by running `pip install -r requirements.txt`.\n3. Set up a new Discord bot and retrieve its token. You can follow [this guide](https://discordpy.readthedocs.io/en/stable/discord.html) if you need help.\n4. Set up an OpenAI account and retrieve your API key.\n5. Create a `.env` file in the project directory from the `.env-sample` provided and replace the api keys.\n\n6. Run the bot using `python bot.py`.\n\n## Usage\n\n- Send a message mentioning the bot or use the `!ask` command to ask a question or provide input to the bot.\n- Use the `!persona` command to display the current bot persona.\n- Use the `!persona your_new_persona` command to change the bot persona.\n"
    },
    {
      "name": "TuanaCelik/find-the-animal",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/15802862?s=40&v=4",
      "owner": "TuanaCelik",
      "repo_name": "find-the-animal",
      "description": "🦒 A demo with the Haystack MultiModalRetriever to search through images with text",
      "homepage": "",
      "language": "Python",
      "created_at": "2022-12-14T20:17:01Z",
      "updated_at": "2023-05-04T13:36:20Z",
      "topics": [
        "haystack",
        "nlp"
      ],
      "readme": "---\ntitle: MultiModalRetrieval for Image Search\nemoji: 🦒\ncolorFrom: green\ncolorTo: yellow\nsdk: streamlit\nsdk_version: 1.10.0\napp_file: 🏡_Home.py\npinned: false\n---"
    },
    {
      "name": "mlevitt-deloitte/Policy_Recon_HDSI",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/114176092?s=40&v=4",
      "owner": "mlevitt-deloitte",
      "repo_name": "Policy_Recon_HDSI",
      "description": null,
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2023-02-27T19:21:02Z",
      "updated_at": "2024-11-03T23:08:45Z",
      "topics": [],
      "readme": "# PolicyRecon NLP Hackathon - Halıcıoğlu's Prodigies\n\n<img src=\"img/abstract-logo.png\" align=\"right\">\n\nThis repo explores the ability to detect contradictions in government policy documents using natural language processing (NLP). The problem statement as posed in the PolicyRecon™ hackathon which inspired this repo is as follows:\n> Given a collection of policy documents, identify contradictions (statement in one policy which are contradicted by another policy in the same group or org) within that collection\n\nTo quickly see results of the full pipeline in action, you may check out [`DoD Contradictions.ipynb (nbviewer)`](https://nbviewer.org/github/mlevitt-deloitte/Policy_Recon_HDSI/blob/main/DoD%20Contradictions.ipynb).\n\n**Table of Contents**\n- [Setup](#setup)\n   - [Requirements](#requirements)\n   - [Data](#data)\n- [Detecting Contradictions](#detecting-contradictions)\n   - [Run the full pipeline](#run-the-full-pipeline)\n   - [Re-use or extend parts of the pipeline](#re-use-or-extend-parts-of-the-pipeline)\n   - [Results](#results)\n- [Developing](#developing)\n   - [Use a local container image](#use-a-local-container-image)\n   - [Install additional pip packages](#install-additional-pip-packages)\n- [Troubleshooting](#troubleshooting)\n- [Credits](#credits)\n   - [Authors](#authors)\n   - [References](#references)\n   - [Acknowledgements](#acknowledgements)\n\n## Setup\n\n### Requirements\n* [**Docker**](https://www.docker.com/)\n   * We recommend using Docker as the fastest and most consistent method to deploy and develop this solution locally. You may install it using instructions here: https://docs.docker.com/get-docker/\n   * We provide a Dockerfile and image which has all software requirements pre-installed.\n* Otherwise, if you choose not to use Docker and you wish instead to run locally or in a compute environment such as Databricks, you must install [Python](https://www.python.org/) (we've tested versions 3.8 and 3.10) and install the packages from `requirements.txt`. You may find it helpful to create a virtual environment using [venv](https://docs.python.org/3/library/venv.html) if you are working locally.\n\n### Data\nPlease download the datasets from the \"PolicyRecon Analytics Challenge 2023\" Team under `General > Files > 02. Data Sets` and save them to a directory `data/` within this repository. The directory structure should look like:\n```\n.\n└── data/\n    ├── Air Force Publications/\n    ├── DoD Issuances/\n    ├── FSIS/\n    ├── NIFA/\n    └── ... any additional datasets of your own\n```\nYou may add your own datasets beneath the data directory!\n\n## Detecting Contradictions\nTo detect contradictions, we utilize a pipeline that performs the following general steps:\n1. Load documents.\n2. Clean and split documents into chunks of sentences using a sliding window.\n   * This allows us to inspect documents at a more granular level than the full text.\n3. Use a transformer model to create chunk embeddings, then pre-select pairs of chunks that are the most similar.\n   * This way we'll only look for contradictions between text which seems to be discussing the same concept based on surrounding context.\n4. Apply a tokenizer + sequential classification model pre-trained for Natural Language Inference to all possible combinations of sentences between the two chunks for each pre-selected pair.\n   * This model performs the heavy lifting of determining whether two sentences are contradictory.\n5. Select the most contradictive sentence pairs and save the results, including the surrounding context (chunks) and the URLs of the documents being compared.\n\nAfter running our pipeline, a csv is produced containing the top contradiction candidates and relevant information which enables human verification of the results much faster than if a human were to need to do all analysis manually.\n\nThis tool isn't meant to replace all human analysis, but rather to expedite the contradiction discovery process!\n\n### Run the full pipeline\nTo run the full pipeline, you can execute `pipeline.py` as a script:\n1. First edit `config.py` to specify your desired configuration variables\n2. Launch `pipeline.py`\n   1. You may use our provided utility target to run the pipeline from a Docker container on your machine.\n      ```bash\n      ./run.sh pipeline\n      ```\n   2. Or, from any system with the required environment installed (such as a Docker container or an active venv), run `python pipeline.py`.\n\n> **Note**: The provided `config.py` uses demo values so that you can quickly run an example pipeline on a smaller chunk of data. It should take 5 minutes or less to run on a reasonably capable machine without GPU acceleration. Please modify the config values if you intend to run the pipeline on a full dataset!\n\n### Re-use or extend parts of the pipeline\nUnder `src/` you can find all of the components used to build the contradiction discovery pipeline. Each module is well-documented regarding its contents and logic.\n\nBecause of the modular nature of our pipeline, you may modify, extend, remove, borrow, or replace the various components to best fit your use-case and dataset.\n\n### Results\nYou may find results of the contradiction detection pipeline being ran on the full DoD Issuances dataset by inspecting [`DoD Contradictions.ipynb (nbviewer)`](https://nbviewer.org/github/mlevitt-deloitte/Policy_Recon_HDSI/blob/main/DoD%20Contradictions.ipynb). Outputs of the cells, including runtimes for long processes, are included in the notebook. (It is recommended you use the nbviewer link above to preserve output formatting online.)\n\n## Developing\nTo develop with this solution, we recommend utilizing [Development Containers](https://containers.dev/) for consistency between developer systems and fast dependency installation. We define our dev container under `.devcontainer/`.\n\nTo work with our dev container in VSCode, use the following steps:\n1. Install Docker on your system https://docs.docker.com/get-docker/\n2. Install VSCode on your system https://code.visualstudio.com/download\n3. Launch VSCode and open this folder as a workspace\n4. Install the \"Dev Containers\" (ms-vscode-remote.remote-containers) extension from Microsoft\n5. Click the pop-up window that says \"Reopen in container\" *[or]* use the command palette (<kbd>Ctrl/Cmd+Shift+P</kbd>) and search for then run the command \"**Dev Containers: Reopen in Container**\"\n\nThe window will then reload into a dev container, pull the image if this is your first time running, then allow you to execute code from within the Linux-based container mounted to this repository.\n\n### Use a local container image\nBy default, the dev container will pull an image from a remote Docker Hub repository. You may replace this image by modifying the `.devcontainer/Dockerfile` and/or `requirements.txt` as you desire, then re-build the image and tag it to the expected name by running `make build-dev`.\n\nFinally, run the VSCode command \"**Dev Containers: Rebuild and Reopen in Container**\" to reflect the image changes into the dev container.\n\n### Install additional pip packages\n1. Launch the dev container\n2. Install the package via integrated terminal\n3. Run `pip freeze | grep <packagename>` to find the version installed and add it to our requirements.txt file\n4. Add any additional commands you had to run in the terminal (such as additional non-pip software dependencies that had to be installed) to our Dockerfile in a new RUN command\n5. Run `make build-dev` to ensure the new requirements file passes without conflicts\n\n## Troubleshooting\n**Communication Errors**\n* `huggingface_hub.utils._errors.LocalEntryNotFoundError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.`\n   * Remove the docker cache directory if you're using Docker, `rm -r .docker-cache`, then try again.\n* `requests.exceptions.ConnectionError: HTTPSConnectionPool(host='cdn-lfs.huggingface.co', port=443): Read timed out.`\n   * Try again. If this keeps happening when using Docker, it may be due to SSL certificate errors caused by Deloitte's netskope client injecting certificates in the middle of Docker's network requests. Consider using the SSL fix seen in [.devcontainer/Dockerfile](.devcontainer/Dockerfile) by manually adding Deloitte's netskope cert to the image.\n\n## Credits\n\n### Authors\nMeet our team of four Halıcıoğlu Data Science Institute graduates who are now consulting at Deloitte. Our expertise ranges from machine learning to data visualization, and we are committed to staying at the forefront of machine learning research and are continuously expanding our knowledge by attending workshops, conferences, and collaborating with other data scientists.\n\n<center>\n\n| <img src='https://avatars.githubusercontent.com/u/99682978?s=64' height=64 width=64 /><br />Parker Addison | <img src='https://avatars.githubusercontent.com/u/126518891?s=64' height=64 width=64 /><br />Shweta Kumar | <img src='https://avatars.githubusercontent.com/u/114176092?s=64' height=64 width=64 /><br />Max Levitt | <img src='https://avatars.githubusercontent.com/u/125689603?s=64' height=64 width=64 /><br />Emily Ramond |\n| :-: | :-: | :-: | :-: |\n\n</center>\n\nPlease reach out if you would like more information!\n\n### References\nAs part of our solution, we relied on the following helpful tools/models that we'd like to credit.\n* [Haystack](https://haystack.deepset.ai/)\n* [spaCy](https://spacy.io/)\n* [HuggingFace](https://huggingface.co/)\n* [Sentence-Transformers](https://www.sbert.net/)\n* The model [`all-MiniLM-L6-v2`](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2) from sentence-transformers and based on the paper [*MiniLM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers*](https://arxiv.org/pdf/2002.10957.pdf) from Microsoft Research\n* The model [`roberta-large-snli_mnli_fever_anli_R1_R2_R3-nli`](https://huggingface.co/ynie/roberta-large-snli_mnli_fever_anli_R1_R2_R3-nli) based on the paper [*Adversarial NLI: A New Benchmark for Natural Language Understanding*](https://arxiv.org/abs/1910.14599) by Yixin Nie et al. from UNC Chapel Hill and Facebook AI Research\n\n### Acknowledgements\n* This interesting project idea was originally created as a submission to Deloitte's **US GPS PolicyRecon™ NLP Hackathon**!\n* We utilized the generous support of the Deloitte **Data Science Lab** team who provided us with a Databricks compute environment to run our pipeline. Thank you!\n* The code for this project was originally hosted on GitHub at https://github.com/mlevitt-deloitte/Policy_Recon_HDSI/tree/main\n"
    },
    {
      "name": "rgs2151/GraphWelder",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/26141934?s=40&v=4",
      "owner": "rgs2151",
      "repo_name": "GraphWelder",
      "description": "GraphWelder: High-Performance MLOps Framework For Open Source Research",
      "homepage": null,
      "language": "Python",
      "created_at": "2023-02-19T08:03:37Z",
      "updated_at": "2023-07-09T18:04:05Z",
      "topics": [],
      "readme": "# 🐔GraphWelder\n\n## High-Performance MLOps Framework For Open Source Research\n\n\n### languagewelder PipeLine\n\n**welder PipeLine module** is designed to represent each and every step in the project as `Pipe` to a `PipeLine`. `PipeLine` is an implementation of asyclic `Graph` with n `InputPipe` and `OutputPipe`.\n\n## Quick Start\n\nLet's create a pipe that accepts a `str` input and lowers it.\n\n```python\nfrom welder.node import Pipe\n\nclass LowerStr(Pipe):\n\tdef __init__(self, label: str) -> None:\n        super().__init__(label, inputs=[str], outputs=[str])\n\n\tdef pipe(self):\n\t\ttext = self.values[0]\n\t\ttext = text.lower()\n\t\treturn [text]\n```\n\nEach `Pipe` has to initialize `inputs` and `outputs` specifing the type of parameters to accept.\nEach `Pipe` **Child** class has to implement a `pipe` method with **NO arguments!**.\nInputs to the `Pipe` are expected in the `self.values` instance variable with index equal to the index specified in the `inputs`.\nReturn to `pipe` method should correspond to `outputs` set to the `Pipe` class.\n\nIn order to use this functionality in `PipeLine` we need to specify `InputPipe` and `OutputPipe`.\n\n```python\nfrom welder.node import InputPipe, OutputPipe\n\nInput1 = InputPipe('Input 1', inputs=[str], fix_inputs=1)\nPipe1 = LowerStr('Lower Text')\nOutput1 = OutputPipe('Output 1', outputs=[str])\n\nPL = PipeLine()\nPL.register(Input1, Pipe1)\nPL.register(Pipe1, Output1)\n\nresults = PL.flow(['THIS is a TeXt'])\n# results = 'this is a text'\n```\n\n## In More Detail...\n\n* [class PipeLine](./README-PipeLine.md)\n* [class Pipe](README-Pipe.md)\n* [Pipe Documentation](pipe/README.md)\n\n## Things Remaining...\n\n* [ ] Checking `PipeLine` with multipl `OutputPipe` objects.\n* [ ] Documentation for individual pipe functionalities.\n* [ ] Missing `sequential_connect` usecase in the quick start.\n* [ ] Adding `Pipe` uniqueness check in the `PipeLine` logs.\n* [X] Adding `branch_connect` functionality to `PipeLine`.\n* [ ] documentation for installing requirements and dependencies.\n\n# Pipe Functionalities\n\nDocumentation for `Pipe` functionalities defined here...\n\nPlese add your functionalities and documentation references here...\n\n## Index\n\n* htmlparser\n* linepreprocessor\n* chunking\n* db\n\n\n## Pipe\n\n```python\nclass Pipe(Node):\n\n    def __init__(self, label:str, inputs=[], outputs=[], fix_inputs=None) -> None:\n        super().__init__(label, inputs=inputs, outputs=outputs, check_input=True, check_output=True, fix_inputs=fix_inputs)\n        self.values = None\n```\n\nIt is an extension of `Node` class. This class forms the base class to all the defined functionalities of the `PipeLine`. Each `Pipe` is represented by a unique `label`. Along with `Node` functionalities it contains a default implementation of the `pipe` method.\n\nEach functionality for the `PipeLine` requires implementation of `pipe` method along with `inputs` and `outputs` defination of the `Node` object.\n\n**`Pipe` Object is specificaly designed to work with the `PipeLine`. any change in its implementation can result in major breaking changes.**\n\n| Property         | Functionality                                                                   |\n| ---------------- | ------------------------------------------------------------------------------- |\n| `values: list` | values set by the `set_inputs` method. Should be in sync with the `inputs`. |\n\n| Method                                 | Functionality                                                                                                                                                             |\n| -------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| `pipe() -> List[Any]`:               | Method should return the result for the functionality defined for the `PipeLine`.<br />Method takes no arguments and raises `NotImplementedError` if not overwritten. |\n| `set_inputs(values: list) -> None:`  | Used by `PipeLine` to set inputs to the `Pipe` object.                                                                                                                |\n| `set_outputs(values: list) -> None:` | pass                                                                                                                                                                      |\n\n## Node\n\n```python\nclass Node:\n    inputs: list\n    outputs: list\n    total_inputs: int\n    total_outputs: int\n    check_input: bool\n    check_output: bool\n\n    label: str\n\n```\n\nParent to all `Pipe` class representing the functionality to be executed.\n\nAll functionalities should contain `inputs: list` with types of accepted inputs and `outputs: list` with type of accepted outputs. These are used for compatiblity checking while connecting two `Node` objects and for type assertions.\n\nType assertion status are maintained by `check_inputs` and `check_outputs` flags. These flags can be used to determine type constraints. `(this flags were added to be used with PipeLine)`.\n\nEach `Node` object is uniquely identified by the `label: str`. Label is used for representation and hashing purpose.\n\n| Property                | Functionality                                                                                                                        |\n| ----------------------- | ------------------------------------------------------------------------------------------------------------------------------------ |\n| `inputs: List[type]`  | Defines inputs to the `Node` Functionality.                                                                                        |\n| `outputs: List[type]` | Defines outputs to the `Node` Functionality.                                                                                       |\n| `total_inputs: int`   | Total number of inputs defined.                                                                                                      |\n| `total_outputs: int`  | Total number of outputs defined.                                                                                                     |\n| `check_input: bool`   | Flag specifying weather to assert inputs.                                                                                            |\n| `check_output: bool`  | Flag specifying weather to assert outputs.                                                                                           |\n| `label: str`          | Unique Identifier for the Node.<br />**Note: `__eq__` method for `Node` is set to `label` this might change in future.** |\n| `ins: @Property`      | Setter and Getter for `inputs`.                                                                                                    |\n| `outs: @Property`     | Setter and Getter for `outputs`.                                                                                                   |\n\n\n## PipeLine\n\n```python\nclass PipeLine(Graph):\n    def __init__(self):\n        super().__init__()\n```\n\nThis class is a representation of the `Graph` object. It represents an acyclic graph. This class is modified to accept and initialize `Pipe` Inputs.\n\n| Property                | Functionality                                               |\n| ----------------------- | ----------------------------------------------------------- |\n| `inputs: List[Pipe]`  | List of registered `InputPipe` objects to the `Graph.`  |\n| `outouts: List[Pipe]` | List of registered `OutputPipe` objects to the `Graph`. |\n| `logs: dict`          | Get a log of all the outputs generated during `flow`.     |\n\n| Methods                                         | Functionality                                                                                                                    |\n| :---------------------------------------------- | :------------------------------------------------------------------------------------------------------------------------------- |\n| `register(pipe1: Node, pipe2: Node) -> None:` | Adds a directed edge between pipe1 and pipe2.                                                                                    |\n| `flow(values: List): -> List:`                | Takes the corresponding `InputPipe` values and runs the `PipeLine` to generate the respective outputs at the `OutputPipe`. |\n| `sequential_connect(pipes: List) -> Pipe:`    | Registeres a list of `Pipe` objects as sequence and returns the last connected `Pipe`.                                       |\n\n| Exceptions            | For                                                                                                                                     |\n| --------------------- | --------------------------------------------------------------------------------------------------------------------------------------- |\n| `IncompatablePipes` | Occurs while registering two incompatible `Pipe`. Most likely occurs when `outputs` of `Pipe `does not match with the `inputs`. |\n\n## Graph\n\n```python\nclass Graph:\n    def __init__(self):\n        self.graph = defaultdict(list)\n        self.reverse_graph = defaultdict(list)\n        self.V = 0\n        self.all_nodes = []\n```\n\nRepresents Graph object with topological sorting.\n\n| Property                       | Functionality                                                    |\n| ------------------------------ | ---------------------------------------------------------------- |\n| `graph: defaultdict`         | Represents graph with `Node` and their directed edges.         |\n| `reverse_graph: defaultdict` | Represents graph with `Node` and their reverse directed edges. |\n| `V: int`                     | Number of vertices registered in the `Graph`.                  |\n| `all_nodes: List[Pipe]`      | List of all registered `Node` object.                          |\n\n| Property                                       | Functionality                                                  |\n| ---------------------------------------------- | -------------------------------------------------------------- |\n| `addEdge(node1: Node, node2: Node) -> None:` | Adds Directed edge between the `Node` objects.               |\n| `topologicalSort() -> List[Node]:`           | Topologicaly sorts `Graph` using **Kahn's Algorithm**. |\n\n| Error           | For                                      |\n| --------------- | ---------------------------------------- |\n| `CyclicGraph` | If cycles are detected in the `Graph`. |\n"
    },
    {
      "name": "textomatic/nutrition-bot",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/102237745?s=40&v=4",
      "owner": "textomatic",
      "repo_name": "nutrition-bot",
      "description": null,
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2023-03-11T19:12:50Z",
      "updated_at": "2023-04-02T08:51:47Z",
      "topics": [],
      "readme": "# Nutrition Bot\n**Duke AIPI 540 Natural Language Processing Module Project by Yilun Wu, Shen Juin Lee, Shrey Gupta**\n\n## Project Description\nFor most people, especially for people who are interested in body healthcare, *nutrition* is a well-acquainted term. By definition,nutrition is the study of how food and drink affects our bodies with special regard to the essential nutrients necessary to support human health. It looks at the physiological and biochemical processes involved in nourishment and how substances in food provide energy or are converted into body tissues. Nutrition is a critical part of health and development. Better nutrition is related to improved infant, child and maternal health, stronger immune systems, safer pregnancy and childbirth, lower risk of non-communicable diseases (such as diabetes and cardiovascular disease), and longevity. Therefore, nutrition is a popular topic among the world. \n\n*Reddit*, one of the largest online communities in the United States, has a wide-range of posts and discussions on nutrition related topics as it should be. Reddit contains many subreddits dedicated to nutrition topics, making it a great platform for anyone interested in learning more about healthy eating, dietary advice, and related discussions. Nonetheless, not every nutrition question from every user are approprately answered or solved--there still exists several problems of Reddit posts & discussions on nutrition topic: \n\n- **Lack of context**: some Reddit posts may not contain enough information due to the character limit or formatting limit of the Reddit platform\n\n- **Lack of moderation**: some Reddit users may encounter irrelevant or even offensive comments (or trolls /spams)\n\n- **Lack of scientific evidence support**: some replies from certain users to a specific nutrition related problem are quite meaningful and beneficial, yet still not convincing enough since the lack of scientific evidence \n\nOur project mainly focuses on solving the third problem: the lack of scientific evidence support. To be more specific, our project builds up a Q&A query system that allows users to input questions related to nutrition and find approprate answers based on scientific research papers and answers from reddit posts. \n\n## Data Sources\nAs discussed before, our project aims to solve the problem of lack of scientific evidence support and try to provide answers to each nutrition related problems from both scientific research papers and reddit posts & replies. Therefore, our datasets mainly contains two parts: scientific research papers in .pdf format and contents of reddit posts and discussions in .csv format.\n\n### Scientific Research Papers\nThe scientific research papers on topics about nutrition are collected both manually and automatically with the support of web scraping python scripts. To be more specific, we manually downloaded ~200 most recent & most relevant scientific research papers on top questions about nutrition posted on Reddit, and automatically scraped more than 2000 academic research papers from *PubMed*, *ScienceDirect*, etc. The total number of scientific research papers downloaded is 2480, and we uploaded them to a Google Cloud Storage Bucket.\n\n### Reddit Posts & Replies\nThe posts & replies on various topics about nutrition are collected using python scripts. To be more specific, we extracted reddit posts & comments from the nutrition subreddit (`r/nutrition`) using the Python Reddit API Wrapper (PRAW) package. The total number of posts extracted is 163 and the total number of comments & replies extracted is 25,286. Only posts with an upvote count higher than 50 were downloaded. The collected dataset is stored as `nutrition.csv` and also as a pickled file under the folder `/data/reddit/`.\n\n## Setting up Elasticsearch Document Store\n\nElasticsearch is a distributed, open source search and analytics engine for all types of data, including textual, numerical, geospatial, structured, and unstructured. It allows us to store, search, and analyze big volumes of data quickly and in near real time. It is generally used as the underlying engine/technology that powers applications that have complex search features and requirements. In our project, we use Elasticsearch as the document store to store the research papers and reddit posts & replies.\n\nIt can be setup on your local machine using docker. The following steps can be followed to setup Elasticsearch on your local machine:\n\n**1. Install Docker:** \n```bash\nsudo apt-get update\nsudo apt-get install docker-ce docker-ce-cli containerd.io\n```\n**2. Pull the Elasticsearch docker image:** \n```bash\ndocker pull docker.elastic.co/elasticsearch/elasticsearch:7.17.6\n```\n**3. Create a volume to store the Elasticsearch data:** \n```bash\ndocker volume create es_v1\n```\n\n**4. Run the Elasticsearch docker image:** \n```bash\ndocker run -d -p 9200:9200 -e 'discovery.type=single-node' --name es_v1 --mount type=volume,src=es_v1,target=/usr/share/elasticsearch/ elasticsearch:7.17.6\n```\n\n**5. Check if Elasticsearch is running:** \n```bash\ncurl -X GET \"localhost:9200/\"\n```\n\n## Data Processing\n### Research Papers\n\nThe research papers were downloaded in .pdf format from the google cloud storage bucket in the `data/all_pdfs folder`. Then, a python script was run to convert them into text, preprocess them, chunk them into sentences, and index them into the elasticsearch document store. The python script can be found in the `scripts` folder and can be run as follows:\n\n**1. Create a new conda environment and activate it:** \n```\nconda create --name haystack python=3.8\nconda activate cv\n```\n**2. Install python package requirements:** \n```\npip install -r requirements.txt \n```\n**3. Install xpdf:** \n```bash\nwget --no-check-certificate https://dl.xpdfreader.com/xpdf-tools-linux-4.04.tar.gz\ntar -xvf xpdf-tools-linux-4.04.tar.gz\nsudo cp xpdf-tools-linux-4.04/bin64/pdftotext /usr/local/bin\n```\n**4. Run the data download script:** \n```\npython scripts/process_research_papers.py\n```\n\n### Reddit Posts & Replies\n\nThe Reddit posts and comments were downloaded using the PRAW library. You will need to register for a Reddit account and create a new app in your user profile to obtain a client ID and client secret. Those will be the credentials needed for downloading data using PRAW. Save those credentials in the following format as `praw.ini` in the root folder:\n```ini\nclient_id=<your_client_id>\nclient_secret=<your_client_secret>\nuser_agent=<your_custom_defined_user_agent_string>\n```\n\nAssuming you are in the same conda environment as the previous step:\n\n**1. Run the following script to download data from the `r/nutrition` subreddit:** \n```\npython scripts/reddit_download.py\n```\n\nThe data would be available as CSV and PKL in the `/data/reddit/` directory.\n\n\n## Running the QnA Pipeline Server\nThe QnA pipeline server is a FastAPI server that allows us to query the elasticsearch document store using a REST API.\n\nThe pipeline has the following steps:\n1. Get the query from the user\n2. Embed the query using Dense Passage Retrieval (DPR) model (`facebook/dpr-question_encoder-single-nq-base`)\n3. Retrieve the top 10 relevant documents from the elasticsearch document store using the query embedding\n4. Run the retrieved documents through the reader model (`deepset/roberta-base-squad2`) to top 5 answers\n5. Filter the answers based on the score\n6. Return the answers as a JSON response\n\nThe server can be run as follows:\n\n**1. Create a new conda environment and activate it:** \n```\nconda create --name haystack python=3.8\nconda activate haystack\n```\n**2. Install python package requirements:** \n```\npip install -r requirements.txt \n```\n**3. Change directory:** \n```\ncd src\n```\n**3. Start the server:** \n```\nuvicorn search:app --reload --host 0.0.0.0 --port 8060\n```\n\n## Project Structure\nThe project data and codes are arranged in the following manner:\n\n```\n├── data                                <- directory for project data\n    ├── all_pdfs                        <- placeholder directory to store all research papers\n    ├── reddit                          <- directory to store all reddit posts & replies\n        ├── nutrition.csv\n        ├── nutrition.pkl\n        ├── top_questions.csv\n├── notebooks                           <- directory to store any exploration notebooks used\n├── src                                 <- directory for data processing and QnA pipeline server scripts\n    ├── helper_functions.py             <- script to store helper functions\n    ├── process_reddit_posts.py         <- script to process the reddit posts\n    ├── process_research_papers.py      <- script to process the research papers\n    ├── reddit_download.py               <- script to download reddit posts\n    ├── search.py                       <- script to run the QnA pipeline server\n├── .gitignore                          <- git ignore file\n├── README.md                           <- description of project and how to set up and run it\n├── requirements.txt                    <- requirements file to document dependencies\n```\n&nbsp;\n## Nutrition Bot (Streamlit):\n* Refer to the [README.md](https://github.com/textomatic/nutrition-bot/blob/st/README.md) at this link to run the Streamlit-based web application or access it [here](https://nutrition-bot.streamlit.app).\n* You can find the code for the Streamlit web-app [here](https://github.com/textomatic/nutrition-bot/tree/st)"
    },
    {
      "name": "besson/ml-lab",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/516122?s=40&v=4",
      "owner": "besson",
      "repo_name": "ml-lab",
      "description": "2023 Deep learning study",
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2023-01-26T03:07:14Z",
      "updated_at": "2023-03-11T14:12:07Z",
      "topics": [],
      "readme": "# ml-lab\n2023 Deep learning study\n"
    },
    {
      "name": "HaiCuCai-00/nlp_haystack",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/69022895?s=40&v=4",
      "owner": "HaiCuCai-00",
      "repo_name": "nlp_haystack",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2023-02-22T01:41:15Z",
      "updated_at": "2023-02-27T16:17:02Z",
      "topics": [],
      "readme": "# Usable\n\nStart server:\n```bash\npython3 ./application.py\n```\n\nStart ElasticSearch and Apache Tika\n\n```bash\ndocker run -d -p 9200:9200 -e \"discovery.type=single-node\" docker.elastic.co/elasticsearch/elasticsearch:7.15.0\ndocker run -d -p 9998:9998 apache/tika:latest\n```\n"
    },
    {
      "name": "Ok3ks/haystack",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/80680311?s=40&v=4",
      "owner": "Ok3ks",
      "repo_name": "haystack",
      "description": ":mag: Haystack is an open source NLP framework to interact with your data using Transformer models and LLMs (GPT-3 and alike). Haystack offers production-ready tools to quickly build ChatGPT-like question answering, semantic search, text generation, and more.",
      "homepage": "https://haystack.deepset.ai",
      "language": null,
      "created_at": "2023-02-07T00:12:23Z",
      "updated_at": "2023-09-25T22:26:49Z",
      "topics": [],
      "readme": "<p align=\"center\">\n  <a href=\"https://www.deepset.ai/haystack/\"><img src=\"https://raw.githubusercontent.com/deepset-ai/haystack/main/docs/img/haystack_logo_colored.png\" alt=\"Haystack\"></a>\n</p>\n\n<p>\n    <a href=\"https://github.com/deepset-ai/haystack/actions/workflows/tests.yml\">\n        <img alt=\"Tests\" src=\"https://github.com/deepset-ai/haystack/workflows/Tests/badge.svg?branch=main\">\n    </a>\n    <a href=\"https://github.com/deepset-ai/haystack-json-schema/actions/workflows/schemas.yml\">\n        <img alt=\"Schemas\" src=\"https://github.com/deepset-ai/haystack-json-schema/actions/workflows/schemas.yml/badge.svg\">\n    </a>\n    <a href=\"https://docs.haystack.deepset.ai\">\n        <img alt=\"Documentation\" src=\"https://img.shields.io/website?label=documentation&up_message=online&url=https%3A%2F%2Fdocs.haystack.deepset.ai\">\n    </a>\n    <a href=\"https://app.fossa.com/projects/custom%2B24445%2Fgithub.com%2Fdeepset-ai%2Fhaystack?ref=badge_shield\">\n        <img alt=\"FOSSA Status\" src=\"https://app.fossa.com/api/projects/custom%2B24445%2Fgithub.com%2Fdeepset-ai%2Fhaystack.svg?type=shield\"/>\n    </a>\n    <a href=\"https://github.com/deepset-ai/haystack/releases\">\n        <img alt=\"Release\" src=\"https://img.shields.io/github/release/deepset-ai/haystack\">\n    </a>\n    <a href=\"https://github.com/deepset-ai/haystack/commits/main\">\n        <img alt=\"Last commit\" src=\"https://img.shields.io/github/last-commit/deepset-ai/haystack\">\n    </a>\n    <a href=\"https://pepy.tech/project/farm-haystack\">\n        <img alt=\"Downloads\" src=\"https://pepy.tech/badge/farm-haystack/month\">\n    </a>\n    <a href=\"https://www.deepset.ai/jobs\">\n        <img alt=\"Jobs\" src=\"https://img.shields.io/badge/Jobs-We're%20hiring-blue\">\n    </a>\n        <a href=\"https://twitter.com/intent/follow?screen_name=deepset_ai\">\n        <img alt=\"Twitter\" src=\"https://img.shields.io/badge/follow-%40deepset_ai-1DA1F2?logo=twitter\">\n    </a>\n    <a href=\"https://discord.com/invite/qZxjM4bAHU\">\n        <img alt=\"chat on Discord\" src=\"https://img.shields.io/discord/993534733298450452?logo=discord\">\n    </a>\n</p>\n\n[Haystack](https://haystack.deepset.ai) is an end-to-end framework that enables you to build powerful and production-ready pipelines for different search use cases.\nWhether you want to perform question answering (QA) or semantic document search, you can use the state-of-the-art NLP models in Haystack to provide unique search experiences and allow your users to query in natural language.\nHaystack is built in a modular fashion so that you can combine the best technology from other open source projects, like Hugging Face's transformers, Elasticsearch, or Milvus.\n\n<p align=\"center\"><img src=\"https://raw.githubusercontent.com/deepset-ai/haystack/main/docs/img/main_example.gif\"></p>\n\n## What to Build with Haystack\n\n- **Ask questions in natural language** and find granular answers in your documents.\n- Perform **semantic search** and retrieve documents according to meaning, not keywords.\n- Use **off-the-shelf models** or **fine-tune** them to your domain.\n- Use **user feedback** to evaluate, benchmark, and continuously improve your live models.\n- Leverage existing **knowledge bases** and better handle the long tail of queries that **chatbots** receive.\n- **Automate processes** by automatically applying a list of questions to new documents and using the extracted answers.\n\n## Core Features\n\n- **Latest models**: Utilize all latest transformer-based models (for example, BERT, RoBERTa, MiniLM) for extractive QA, generative QA, and document retrieval.\n- **Modular**: Multiple choices to fit your tech stack and use case. Pick your favorite database, file converter, or modeling framework.\n- **Pipelines**: Use the Node and Pipeline design of Haystack to route queries to only the relevant components.\n- **Open**: 100% compatible with Hugging Face's model hub. Tight interfaces to other frameworks (for example, transformers, FARM, sentence-transformers).\n- **Scalable**: Scale to millions of docs using retrievers, production-ready backends like Elasticsearch / FAISS, and a fastAPI REST API.\n- **End-to-End**: All tooling in one place: file conversion, cleaning, splitting, training, eval, inference, labeling, and more.\n- **Developer friendly**: Easy to debug, extend, and modify.\n- **Customizable**: Fine-tune models to your domain or implement your custom DocumentStore.\n- **Continuous Learning**: Collect new training data from user feedback in production & improve your models continuously.\n\n|                                                                                               |                                                                                                                                                                                                                                                   |\n| --------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| :ledger: [Docs](https://docs.haystack.deepset.ai)                                             | Components, Pipeline Nodes, Guides, API Reference                                                                                                                                                                                                 |\n| :floppy_disk: [Installation](https://github.com/deepset-ai/haystack#floppy_disk-installation) | How to install Haystack                                                                                                                                                                                                                           |\n| :mortar_board: [Tutorials](https://github.com/deepset-ai/haystack#mortar_board-tutorials)     | See what Haystack can do with our Notebooks & Scripts                                                                                                                                                                                             |\n| :beginner: [Quick Demo](https://github.com/deepset-ai/haystack#beginner-quick-demo)           | Deploy a Haystack application with Docker Compose and a REST API                                                                                                                                                                                  |\n| :vulcan_salute: [Community](https://github.com/deepset-ai/haystack#vulcan_salute-community)   | [Discord](https://haystack.deepset.ai/community/join), [Twitter](https://twitter.com/deepset_ai), [Stack Overflow](https://stackoverflow.com/questions/tagged/haystack), [GitHub Discussions](https://github.com/deepset-ai/haystack/discussions) |\n| :heart: [Contributing](https://github.com/deepset-ai/haystack#heart-contributing)             | We welcome all contributions!                                                                                                                                                                                                                     |\n| :bar_chart: [Benchmarks](https://haystack.deepset.ai/benchmarks/)                             | Speed & Accuracy of Retriever, Readers and DocumentStores                                                                                                                                                                                         |\n| :telescope: [Roadmap](https://haystack.deepset.ai/overview/roadmap)                           | Public roadmap of Haystack                                                                                                                                                                                                                        |\n| :newspaper: [Blog](https://medium.com/deepset-ai)                                             | Read our articles on Medium                                                                                                                                                                                                                       |\n| :phone: [Jobs](https://www.deepset.ai/jobs)                                                   | We're hiring! Have a look at our open positions                                                                                                                                                                                                   |\n\n\n## :floppy_disk: Installation\n\n**Basic Installation**\n\nUse [pip](https://github.com/pypa/pip) to install a basic version of Haystack's latest release:\n\n```\n    pip install farm-haystack\n```\n\nThis command installs everything needed for basic Pipelines that use an Elasticsearch DocumentStore.\n\n**Full Installation**\n\nTo use more advanced features, like certain DocumentStores, FileConverters, OCR, or Ray, install further dependencies. The following command installs the latest version of Haystack and all its dependencies from the main branch:\n\n```\ngit clone https://github.com/deepset-ai/haystack.git\ncd haystack\npip install --upgrade pip\npip install -e '.[all]' ## or 'all-gpu' for the GPU-enabled dependencies\n```\n\n**Custom Installation**\nYou can choose the dependencies you want to install. To do so, specify them in the `pip install` command:\n\n```\npip install 'farm-haystack[DEPENDENCY_OPTION]'\n```\nYou can find a full list of dependency options at [haystack/pyproject.toml](https://github.com/deepset-ai/haystack/blob/main/pyproject.toml#L96).\n\nIf you're running pip version earlier than 21.3, you can't install dependency groups that reference other groups. Instead, you can only specify groups that contain direct package references:\n\n```\n# instead of '[all]'\npip install 'farm-haystack[sql,only-faiss,only-milvus1,weaviate,pinecone,opensearch,graphdb,inmemorygraph,crawler,preprocessing,ocr,onnx,ray,dev]'\n\n# instead of '[all-gpu]'\npip install 'farm-haystack[sql,only-faiss-gpu,only-milvus1,weaviatepinecone,opensearch,graphdb,inmemorygraph,crawler,preprocessing,ocr,onnx-gpu,ray,dev]'\n```\n\n**Installing the REST API**\nHaystack comes packaged with a REST API so that you can deploy it as a service. Run the following command from the root directory of the Haystack repo to install REST_API:\n\n```\npip install rest_api/\n```\n\n**Other Operating Systems**\n\n**Windows**\nWe recommend installing [WSL](https://learn.microsoft.com/en-us/windows/wsl/install) to use Haystack on Windows:\n\n```\npip install farm-haystack -f https://download.pytorch.org/whl/torch_stable.html\n```\n\n**Apple Silicon (M1)**\n\nMacs with an M1 processor require some extra dependencies to install Haystack:\n\n```\n# some additional dependencies needed on m1 mac\nbrew install postgresql\nbrew install cmake\nbrew install rust\n\n# haystack installation\nGRPC_PYTHON_BUILD_SYSTEM_ZLIB=true pip install git+https://github.com/deepset-ai/haystack.git\n```\n\n**Learn More**\n\nSee our [installation guide](https://docs.haystack.deepset.ai/docs/installation) for more options.\nYou can find out more about our PyPi package on our [PyPi page](https://pypi.org/project/farm-haystack/).\n\n## :mortar_board: Tutorials\n\n![image](https://raw.githubusercontent.com/deepset-ai/haystack/main/docs/img/concepts_haystack_handdrawn.png)\n\nFollow our [introductory tutorial](https://haystack.deepset.ai/tutorials/first-qa-system)\nto set up a question answering system using Python and start performing queries!\nExplore [the rest of our tutorials](https://haystack.deepset.ai/tutorials)\nto learn how to tweak pipelines, train models, and perform evaluation.\n\n## :beginner: Quick Demo\n\n**Hosted**\n\nTry out our hosted [Explore The World](https://haystack-demo.deepset.ai/) live demo here!\nAsk any question on countries or capital cities and let Haystack return the answers to you.\n\n**Local**\n\nTo run the Explore The World demo on your own machine and customize it to your needs, check out the instructions on [Explore the World repository](https://github.com/deepset-ai/haystack-demos/tree/main/explore_the_world) on GitHub.\n\n## :vulcan_salute: Community\n\nThere is a very vibrant and active community around Haystack which we are regularly interacting with!\nIf you have a feature request or a bug report, feel free to open an [issue in Github](https://github.com/deepset-ai/haystack/issues).\nWe regularly check these and you can expect a quick response.\nIf you'd like to discuss a topic, or get more general advice on how to make Haystack work for your project,\nyou can start a thread in [Github Discussions](https://github.com/deepset-ai/haystack/discussions) or our [Discord channel](https://haystack.deepset.ai/community).\nWe also check [Twitter](https://twitter.com/deepset_ai) and [Stack Overflow](https://stackoverflow.com/questions/tagged/haystack).\n\n\n## :heart: Contributing\n\nWe are very open to the community's contributions - be it a quick fix of a typo, or a completely new feature!\nYou don't need to be a Haystack expert to provide meaningful improvements.\nTo learn how to get started, check out our [Contributor Guidelines](https://github.com/deepset-ai/haystack/blob/main/CONTRIBUTING.md) first.\n\nYou can also find instructions to run the tests locally there.\n\nThanks so much to all those who have contributed to our project!\n\n<a href=\"https://github.com/deepset-ai/haystack/graphs/contributors\">\n  <img src=\"https://contrib.rocks/image?repo=deepset-ai/haystack\" />\n</a>\n\n\n## Who Uses Haystack\n\nHere's a list of organizations that use Haystack. Don't hesitate to send a PR to let the world know that you use Haystack. Join our growing community!\n\n- [Airbus](https://www.airbus.com/en)\n- [Alcatel-Lucent](https://www.al-enterprise.com/)\n- [BetterUp](https://www.betterup.com/)\n- [Deepset](https://deepset.ai/)\n- [Etalab](https://www.etalab.gouv.fr/)\n- [Infineon](https://www.infineon.com/)\n- [Sooth.ai](https://sooth.ai/)\n"
    },
    {
      "name": "Python-Repository-Hub/haystack",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/76585909?s=40&v=4",
      "owner": "Python-Repository-Hub",
      "repo_name": "haystack",
      "description": ":mag: Haystack is an open source NLP framework that leverages pre-trained Transformer models. It enables developers to quickly implement production-ready semantic search, question answering, summarization and document ranking for a wide range of NLP applications.",
      "homepage": "https://deepset.ai/haystack",
      "language": null,
      "created_at": "2023-01-13T06:49:25Z",
      "updated_at": "2023-03-05T16:55:18Z",
      "topics": [],
      "readme": "<p align=\"center\">\n  <a href=\"https://www.deepset.ai/haystack/\"><img src=\"https://raw.githubusercontent.com/deepset-ai/haystack/main/docs/img/haystack_logo_colored.png\" alt=\"Haystack\"></a>\n</p>\n\n<p>\n    <a href=\"https://github.com/deepset-ai/haystack/actions/workflows/tests.yml\">\n        <img alt=\"Tests\" src=\"https://github.com/deepset-ai/haystack/workflows/Tests/badge.svg?branch=main\">\n    </a>\n    <a href=\"https://github.com/deepset-ai/haystack-json-schema/actions/workflows/schemas.yml\">\n        <img alt=\"Schemas\" src=\"https://github.com/deepset-ai/haystack-json-schema/actions/workflows/schemas.yml/badge.svg\">\n    </a>\n    <a href=\"https://docs.haystack.deepset.ai\">\n        <img alt=\"Documentation\" src=\"https://img.shields.io/website?label=documentation&up_message=online&url=https%3A%2F%2Fdocs.haystack.deepset.ai\">\n    </a>\n    <a href=\"https://app.fossa.com/projects/custom%2B24445%2Fgithub.com%2Fdeepset-ai%2Fhaystack?ref=badge_shield\">\n        <img alt=\"FOSSA Status\" src=\"https://app.fossa.com/api/projects/custom%2B24445%2Fgithub.com%2Fdeepset-ai%2Fhaystack.svg?type=shield\"/>\n    </a>\n    <a href=\"https://github.com/deepset-ai/haystack/releases\">\n        <img alt=\"Release\" src=\"https://img.shields.io/github/release/deepset-ai/haystack\">\n    </a>\n    <a href=\"https://github.com/deepset-ai/haystack/commits/main\">\n        <img alt=\"Last commit\" src=\"https://img.shields.io/github/last-commit/deepset-ai/haystack\">\n    </a>\n    <a href=\"https://pepy.tech/project/farm-haystack\">\n        <img alt=\"Downloads\" src=\"https://pepy.tech/badge/farm-haystack/month\">\n    </a>\n    <a href=\"https://www.deepset.ai/jobs\">\n        <img alt=\"Jobs\" src=\"https://img.shields.io/badge/Jobs-We're%20hiring-blue\">\n    </a>\n        <a href=\"https://twitter.com/intent/follow?screen_name=deepset_ai\">\n        <img alt=\"Twitter\" src=\"https://img.shields.io/twitter/follow/deepset_ai?style=social\">\n    </a>\n</p>\n\n[Haystack](https://haystack.deepset.ai) is an end-to-end framework that enables you to build powerful and production-ready pipelines for different search use cases.\nWhether you want to perform Question Answering or semantic document search, you can use the State-of-the-Art NLP models in Haystack to provide unique search experiences and allow your users to query in natural language.\nHaystack is built in a modular fashion so that you can combine the best technology from other open-source projects like Huggingface's Transformers, Elasticsearch, or Milvus.\n\n<p align=\"center\"><img src=\"https://raw.githubusercontent.com/deepset-ai/haystack/main/docs/img/main_example.gif\"></p>\n\n## What to build with Haystack\n\n- **Ask questions in natural language** and find granular answers in your documents.\n- Perform **semantic search** and retrieve documents according to meaning, not keywords\n- Use **off-the-shelf models** or **fine-tune** them to your domain.\n- Use **user feedback** to evaluate, benchmark, and continuously improve your live models.\n- Leverage existing **knowledge bases** and better handle the long tail of queries that **chatbots** receive.\n- **Automate processes** by automatically applying a list of questions to new documents and using the extracted answers.\n\n## Core Features\n\n- **Latest models**: Utilize all latest transformer-based models (e.g., BERT, RoBERTa, MiniLM) for extractive QA, generative QA, and document retrieval.\n- **Modular**: Multiple choices to fit your tech stack and use case. Pick your favorite database, file converter, or modeling framework.\n- **Pipelines**: The Node and Pipeline design of Haystack allows for custom routing of queries to only the relevant components.\n- **Open**: 100% compatible with HuggingFace's model hub. Tight interfaces to other frameworks (e.g., Transformers, FARM, sentence-transformers)\n- **Scalable**: Scale to millions of docs via retrievers, production-ready backends like Elasticsearch / FAISS, and a fastAPI REST API\n- **End-to-End**: All tooling in one place: file conversion, cleaning, splitting, training, eval, inference, labeling, etc.\n- **Developer friendly**: Easy to debug, extend and modify.\n- **Customizable**: Fine-tune models to your domain or implement your custom DocumentStore.\n- **Continuous Learning**: Collect new training data via user feedback in production & improve your models continuously\n\n|                                                                                               |                                                                                                                                                                                                                                                   |\n| --------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| :ledger: [Docs](https://docs.haystack.deepset.ai)                                             | Components, Pipeline Nodes, Guides, API Reference                                                                                                                                                                                                 |\n| :floppy_disk: [Installation](https://github.com/deepset-ai/haystack#floppy_disk-installation) | How to install Haystack                                                                                                                                                                                                                           |\n| :mortar_board: [Tutorials](https://github.com/deepset-ai/haystack#mortar_board-tutorials)     | See what Haystack can do with our Notebooks & Scripts                                                                                                                                                                                             |\n| :beginner: [Quick Demo](https://github.com/deepset-ai/haystack#beginner-quick-demo)           | Deploy a Haystack application with Docker Compose and a REST API                                                                                                                                                                                  |\n| :vulcan_salute: [Community](https://github.com/deepset-ai/haystack#vulcan_salute-community)   | [Discord](https://haystack.deepset.ai/community/join), [Twitter](https://twitter.com/deepset_ai), [Stack Overflow](https://stackoverflow.com/questions/tagged/haystack), [GitHub Discussions](https://github.com/deepset-ai/haystack/discussions) |\n| :heart: [Contributing](https://github.com/deepset-ai/haystack#heart-contributing)             | We welcome all contributions!                                                                                                                                                                                                                     |\n| :bar_chart: [Benchmarks](https://haystack.deepset.ai/benchmarks/)                             | Speed & Accuracy of Retriever, Readers and DocumentStores                                                                                                                                                                                         |\n| :telescope: [Roadmap](https://haystack.deepset.ai/overview/roadmap)                           | Public roadmap of Haystack                                                                                                                                                                                                                        |\n| :newspaper: [Blog](https://medium.com/deepset-ai)                                             | Read our articles on Medium                                                                                                                                                                                                                       |\n| :phone: [Jobs](https://www.deepset.ai/jobs)                                                   | We're hiring! Have a look at our open positions                                                                                                                                                                                                   |\n\n\n## :floppy_disk: Installation\n\n**1. Basic Installation**\n\nYou can install a basic version of Haystack's latest release by using [pip](https://github.com/pypa/pip).\n\n```\n    pip3 install farm-haystack\n```\n\nThis command will install everything needed for basic Pipelines that use an Elasticsearch Document Store.\n\n**2. Full Installation**\n\nIf you plan to be using more advanced features like Milvus, FAISS, Weaviate, OCR or Ray,\nyou will need to install a full version of Haystack.\nThe following command will install the latest version of Haystack from the main branch.\n\n```\ngit clone https://github.com/deepset-ai/haystack.git\ncd haystack\npip install --upgrade pip\npip install -e '.[all]' ## or 'all-gpu' for the GPU-enabled dependencies\n```\n\nIf you cannot upgrade `pip` to version 21.3 or higher, you will need to replace:\n- `'.[all]'` with `'.[sql,only-faiss,only-milvus,weaviate,graphdb,crawler,preprocessing,ocr,onnx,ray,dev]'`\n- `'.[all-gpu]'` with `'.[sql,only-faiss-gpu,only-milvus,weaviate,graphdb,crawler,preprocessing,ocr,onnx-gpu,ray,dev]'`\n\nFor an complete list of the dependency groups available, have a look at the `haystack/pyproject.toml` file.\n\nTo install the REST API and UI, run the following from the root directory of the Haystack repo\n\n```\npip install rest_api/\npip install ui/\n```\n\n**3. Installing on Windows**\n\n```\npip install farm-haystack -f https://download.pytorch.org/whl/torch_stable.html\n```\n\n**4. Installing on Apple Silicon (M1)**\n\nM1 Macbooks require some extra dependencies in order to install Haystack.\n\n```\n# some additional dependencies needed on m1 mac\nbrew install postgresql\nbrew install cmake\nbrew install rust\n\n# haystack installation\nGRPC_PYTHON_BUILD_SYSTEM_ZLIB=true pip install git+https://github.com/deepset-ai/haystack.git\n```\n\n**5. Learn More**\n\nSee our [installation guide](https://haystack.deepset.ai/overview/quick-start) for more options.\nYou can find out more about our PyPi package on our [PyPi page](https://pypi.org/project/farm-haystack/).\n\n## :mortar_board: Tutorials\n\n![image](https://raw.githubusercontent.com/deepset-ai/haystack/main/docs/img/concepts_haystack_handdrawn.png)\n\nFollow our [introductory tutorial](https://haystack.deepset.ai/tutorials/first-qa-system)\nto setup a question answering system using Python and start performing queries!\nExplore [the rest of our tutorials](https://haystack.deepset.ai/tutorials)\nto learn how to tweak pipelines, train models and perform evaluation.\n\n## :beginner: Quick Demo\n\n**Hosted**\n\nTry out our hosted [Explore The World](https://haystack-demo.deepset.ai/) live demo here!\nAsk any question on countries or capital cities and let Haystack return the answers to you.\n\n**Local**\n\nTo run the Explore The World demo on your own machine and customize it to your needs, check out the instructions on [Explore the World repository](https://github.com/deepset-ai/haystack-demos/tree/main/explore_the_world) on GitHub.\n\n## :vulcan_salute: Community\n\nThere is a very vibrant and active community around Haystack which we are regularly interacting with!\nIf you have a feature request or a bug report, feel free to open an [issue in Github](https://github.com/deepset-ai/haystack/issues).\nWe regularly check these and you can expect a quick response.\nIf you'd like to discuss a topic, or get more general advice on how to make Haystack work for your project,\nyou can start a thread in [Github Discussions](https://github.com/deepset-ai/haystack/discussions) or our [Discord channel](https://haystack.deepset.ai/community).\nWe also check [Twitter](https://twitter.com/deepset_ai) and [Stack Overflow](https://stackoverflow.com/questions/tagged/haystack).\n\n\n## :heart: Contributing\n\nWe are very open to the community's contributions - be it a quick fix of a typo, or a completely new feature!\nYou don't need to be a Haystack expert to provide meaningful improvements.\nTo learn how to get started, check out our [Contributor Guidelines](https://github.com/deepset-ai/haystack/blob/main/CONTRIBUTING.md) first.\n\nYou can also find instructions to run the tests locally there.\n\nThanks so much to all those who have contributed to our project!\n\n<a href=\"https://github.com/deepset-ai/haystack/graphs/contributors\">\n  <img src=\"https://contrib.rocks/image?repo=deepset-ai/haystack\" />\n</a>\n\n\n## Who uses Haystack\n\nHere's a list of organizations who use Haystack. Don't hesitate to send a PR to let the world know that you use Haystack. Join our growing community!\n\n- [Airbus](https://www.airbus.com/en)\n- [Alcatel-Lucent](https://www.al-enterprise.com/)\n- [BetterUp](https://www.betterup.com/)\n- [Deepset](https://deepset.ai/)\n- [Etalab](https://www.etalab.gouv.fr/)\n- [Infineon](https://www.infineon.com/)\n- [Sooth.ai](https://sooth.ai/)\n"
    },
    {
      "name": "RwGrid/haystack",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/33643615?s=40&v=4",
      "owner": "RwGrid",
      "repo_name": "haystack",
      "description": ":mag: Haystack is an open source NLP framework that leverages pre-trained Transformer models. It enables developers to quickly implement production-ready semantic search, question answering, summarization and document ranking for a wide range of NLP applications.",
      "homepage": "https://deepset.ai/haystack",
      "language": null,
      "created_at": "2022-11-21T21:22:58Z",
      "updated_at": "2022-11-26T20:16:14Z",
      "topics": [],
      "readme": "<p align=\"center\">\n  <a href=\"https://www.deepset.ai/haystack/\"><img src=\"https://raw.githubusercontent.com/deepset-ai/haystack/main/docs/img/haystack_logo_colored.png\" alt=\"Haystack\"></a>\n</p>\n\n<p>\n    <a href=\"https://github.com/deepset-ai/haystack/actions/workflows/tests.yml\">\n        <img alt=\"Tests\" src=\"https://github.com/deepset-ai/haystack/workflows/Tests/badge.svg?branch=main\">\n    </a>\n    <a href=\"https://github.com/deepset-ai/haystack-json-schema/actions/workflows/schemas.yml\">\n        <img alt=\"Schemas\" src=\"https://github.com/deepset-ai/haystack-json-schema/actions/workflows/schemas.yml/badge.svg\">\n    </a>\n    <a href=\"https://docs.haystack.deepset.ai\">\n        <img alt=\"Documentation\" src=\"https://img.shields.io/website?label=documentation&up_message=online&url=https%3A%2F%2Fdocs.haystack.deepset.ai\">\n    </a>\n    <a href=\"https://github.com/deepset-ai/haystack/releases\">\n        <img alt=\"Release\" src=\"https://img.shields.io/github/release/deepset-ai/haystack\">\n    </a>\n    <a href=\"https://github.com/deepset-ai/haystack/commits/main\">\n        <img alt=\"Last commit\" src=\"https://img.shields.io/github/last-commit/deepset-ai/haystack\">\n    </a>\n    <a href=\"https://pepy.tech/project/farm-haystack\">\n        <img alt=\"Downloads\" src=\"https://pepy.tech/badge/farm-haystack/month\">\n    </a>\n    <a href=\"https://www.deepset.ai/jobs\">\n        <img alt=\"Jobs\" src=\"https://img.shields.io/badge/Jobs-We're%20hiring-blue\">\n    </a>\n        <a href=\"https://twitter.com/intent/follow?screen_name=deepset_ai\">\n        <img alt=\"Twitter\" src=\"https://img.shields.io/twitter/follow/deepset_ai?style=social\">\n    </a>\n</p>\n\n[Haystack](https://haystack.deepset.ai) is an end-to-end framework that enables you to build powerful and production-ready pipelines for different search use cases.\nWhether you want to perform Question Answering or semantic document search, you can use the State-of-the-Art NLP models in Haystack to provide unique search experiences and allow your users to query in natural language.\nHaystack is built in a modular fashion so that you can combine the best technology from other open-source projects like Huggingface's Transformers, Elasticsearch, or Milvus.\n\n<p align=\"center\"><img src=\"https://raw.githubusercontent.com/deepset-ai/haystack/main/docs/img/main_example.gif\"></p>\n\n## What to build with Haystack\n\n- **Ask questions in natural language** and find granular answers in your documents.\n- Perform **semantic search** and retrieve documents according to meaning, not keywords\n- Use **off-the-shelf models** or **fine-tune** them to your domain.\n- Use **user feedback** to evaluate, benchmark, and continuously improve your live models.\n- Leverage existing **knowledge bases** and better handle the long tail of queries that **chatbots** receive.\n- **Automate processes** by automatically applying a list of questions to new documents and using the extracted answers.\n\n## Core Features\n\n- **Latest models**: Utilize all latest transformer-based models (e.g., BERT, RoBERTa, MiniLM) for extractive QA, generative QA, and document retrieval.\n- **Modular**: Multiple choices to fit your tech stack and use case. Pick your favorite database, file converter, or modeling framework.\n- **Pipelines**: The Node and Pipeline design of Haystack allows for custom routing of queries to only the relevant components.\n- **Open**: 100% compatible with HuggingFace's model hub. Tight interfaces to other frameworks (e.g., Transformers, FARM, sentence-transformers)\n- **Scalable**: Scale to millions of docs via retrievers, production-ready backends like Elasticsearch / FAISS, and a fastAPI REST API\n- **End-to-End**: All tooling in one place: file conversion, cleaning, splitting, training, eval, inference, labeling, etc.\n- **Developer friendly**: Easy to debug, extend and modify.\n- **Customizable**: Fine-tune models to your domain or implement your custom DocumentStore.\n- **Continuous Learning**: Collect new training data via user feedback in production & improve your models continuously\n\n|                                                                                               |                                                                                                                                                                                                                                                   |\n| --------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| :ledger: [Docs](https://docs.haystack.deepset.ai)                                             | Components, Pipeline Nodes, Guides, API Reference                                                                                                                                                                                                 |\n| :floppy_disk: [Installation](https://github.com/deepset-ai/haystack#floppy_disk-installation) | How to install Haystack                                                                                                                                                                                                                           |\n| :mortar_board: [Tutorials](https://github.com/deepset-ai/haystack#mortar_board-tutorials)     | See what Haystack can do with our Notebooks & Scripts                                                                                                                                                                                             |\n| :beginner: [Quick Demo](https://github.com/deepset-ai/haystack#beginner-quick-demo)           | Deploy a Haystack application with Docker Compose and a REST API                                                                                                                                                                                  |\n| :vulcan_salute: [Community](https://github.com/deepset-ai/haystack#vulcan_salute-community)   | [Discord](https://haystack.deepset.ai/community/join), [Twitter](https://twitter.com/deepset_ai), [Stack Overflow](https://stackoverflow.com/questions/tagged/haystack), [GitHub Discussions](https://github.com/deepset-ai/haystack/discussions) |\n| :heart: [Contributing](https://github.com/deepset-ai/haystack#heart-contributing)             | We welcome all contributions!                                                                                                                                                                                                                     |\n| :bar_chart: [Benchmarks](https://haystack.deepset.ai/benchmarks/)                             | Speed & Accuracy of Retriever, Readers and DocumentStores                                                                                                                                                                                         |\n| :telescope: [Roadmap](https://haystack.deepset.ai/overview/roadmap)                           | Public roadmap of Haystack                                                                                                                                                                                                                        |\n| :newspaper: [Blog](https://medium.com/deepset-ai)                                             | Read our articles on Medium                                                                                                                                                                                                                       |\n| :phone: [Jobs](https://www.deepset.ai/jobs)                                                   | We're hiring! Have a look at our open positions                                                                                                                                                                                                   |\n\n\n## :floppy_disk: Installation\n\n**1. Basic Installation**\n\nYou can install a basic version of Haystack's latest release by using [pip](https://github.com/pypa/pip).\n\n```\n    pip3 install farm-haystack\n```\n\nThis command will install everything needed for basic Pipelines that use an Elasticsearch Document Store.\n\n**2. Full Installation**\n\nIf you plan to be using more advanced features like Milvus, FAISS, Weaviate, OCR or Ray,\nyou will need to install a full version of Haystack.\nThe following command will install the latest version of Haystack from the main branch.\n\n```\ngit clone https://github.com/deepset-ai/haystack.git\ncd haystack\npip install --upgrade pip\npip install -e '.[all]' ## or 'all-gpu' for the GPU-enabled dependencies\n```\n\nIf you cannot upgrade `pip` to version 21.3 or higher, you will need to replace:\n- `'.[all]'` with `'.[sql,only-faiss,only-milvus,weaviate,graphdb,crawler,preprocessing,ocr,onnx,ray,dev]'`\n- `'.[all-gpu]'` with `'.[sql,only-faiss-gpu,only-milvus,weaviate,graphdb,crawler,preprocessing,ocr,onnx-gpu,ray,dev]'`\n\nFor an complete list of the dependency groups available, have a look at the `haystack/pyproject.toml` file.\n\nTo install the REST API and UI, run the following from the root directory of the Haystack repo\n\n```\npip install rest_api/\npip install ui/\n```\n\n**3. Installing on Windows**\n\n```\npip install farm-haystack -f https://download.pytorch.org/whl/torch_stable.html\n```\n\n**4. Installing on Apple Silicon (M1)**\n\nM1 Macbooks require some extra dependencies in order to install Haystack.\n\n```\n# some additional dependencies needed on m1 mac\nbrew install postgresql\nbrew install cmake\nbrew install rust\n\n# haystack installation\nGRPC_PYTHON_BUILD_SYSTEM_ZLIB=true pip install git+https://github.com/deepset-ai/haystack.git\n```\n\n**5. Learn More**\n\nSee our [installation guide](https://haystack.deepset.ai/overview/quick-start) for more options.\nYou can find out more about our PyPi package on our [PyPi page](https://pypi.org/project/farm-haystack/).\n\n## :mortar_board: Tutorials\n\n![image](https://raw.githubusercontent.com/deepset-ai/haystack/main/docs/img/concepts_haystack_handdrawn.png)\n\nFollow our [introductory tutorial](https://haystack.deepset.ai/tutorials/first-qa-system)\nto setup a question answering system using Python and start performing queries!\nExplore [the rest of our tutorials](https://haystack.deepset.ai/tutorials)\nto learn how to tweak pipelines, train models and perform evaluation.\n\n## :beginner: Quick Demo\n\n**Hosted**\n\nTry out our hosted [Explore The World](https://haystack-demo.deepset.ai/) live demo here!\nAsk any question on countries or capital cities and let Haystack return the answers to you.\n\n**Local**\n\nStart up a Haystack service via [Docker Compose](https://docs.docker.com/compose/).\nWith this you can begin calling it directly via the REST API or even interact with it using the included Streamlit UI.\n\n<details>\n  <summary>Click here for a step-by-step guide</summary>\n\n**1. Update/install Docker and Docker Compose, then launch Docker**\n\n```\n    apt-get update && apt-get install docker && apt-get install docker-compose\n    service docker start\n```\n\n**2. Clone Haystack repository**\n\n```\n    git clone https://github.com/deepset-ai/haystack.git\n```\n\n**3. Pull images & launch demo app**\n\n```\n    cd haystack\n    docker-compose pull\n    docker-compose up\n\n    # Or on a GPU machine: docker-compose -f docker-compose-gpu.yml up\n```\n\nYou should be able to see the following in your terminal window as part of the log output:\n\n```\n..\nui_1             |   You can now view your Streamlit app in your browser.\n..\nui_1             |   External URL: http://192.168.108.218:8501\n..\nhaystack-api_1   | [2021-01-01 10:21:58 +0000] [17] [INFO] Application startup complete.\n```\n\n**4. Open the Streamlit UI for Haystack by pointing your browser to the \"External URL\" from above.**\n\nYou should see the following:\n\n![image](https://raw.githubusercontent.com/deepset-ai/haystack/main/docs/img/streamlit_ui_screenshot.png)\n\nYou can then try different queries against a pre-defined set of indexed articles related to Game of Thrones.\n\n**Note**: The following containers are started as a part of this demo:\n\n* Haystack API: listens on port 8000\n* DocumentStore (Elasticsearch): listens on port 9200\n* Streamlit UI: listens on port 8501\n\nPlease note that the demo will [publish](https://docs.docker.com/config/containers/container-networking/) the container ports to the outside world. *We suggest that you review the firewall settings depending on your system setup and the security guidelines.*\n\n</details>\n\n## :vulcan_salute: Community\n\nThere is a very vibrant and active community around Haystack which we are regularly interacting with!\nIf you have a feature request or a bug report, feel free to open an [issue in Github](https://github.com/deepset-ai/haystack/issues).\nWe regularly check these and you can expect a quick response.\nIf you'd like to discuss a topic, or get more general advice on how to make Haystack work for your project,\nyou can start a thread in [Github Discussions](https://github.com/deepset-ai/haystack/discussions) or our [Discord channel](https://haystack.deepset.ai/community).\nWe also check [Twitter](https://twitter.com/deepset_ai) and [Stack Overflow](https://stackoverflow.com/questions/tagged/haystack).\n\n\n## :heart: Contributing\n\nWe are very open to the community's contributions - be it a quick fix of a typo, or a completely new feature!\nYou don't need to be a Haystack expert to provide meaningful improvements.\nTo learn how to get started, check out our [Contributor Guidelines](https://github.com/deepset-ai/haystack/blob/main/CONTRIBUTING.md) first.\n\nYou can also find instructions to run the tests locally there.\n\nThanks so much to all those who have contributed to our project!\n\n<a href=\"https://github.com/deepset-ai/haystack/graphs/contributors\">\n  <img src=\"https://contrib.rocks/image?repo=deepset-ai/haystack\" />\n</a>\n\n\n## Who uses Haystack\n\nHere's a list of organizations who use Haystack. Don't hesitate to send a PR to let the world know that you use Haystack. Join our growing community!\n\n- [Airbus](https://www.airbus.com/en)\n- [Alcatel-Lucent](https://www.al-enterprise.com/)\n- [BetterUp](https://www.betterup.com/)\n- [Deepset](https://deepset.ai/)\n- [Etalab](https://www.etalab.gouv.fr/)\n- [Infineon](https://www.infineon.com/)\n- [Sooth.ai](https://sooth.ai/)\n"
    },
    {
      "name": "Funkmyster/haystack",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/401350?s=40&v=4",
      "owner": "Funkmyster",
      "repo_name": "haystack",
      "description": ":mag: Haystack is an open source NLP framework that leverages pre-trained Transformer models. It enables developers to quickly implement production-ready semantic search, question answering, summarization and document ranking for a wide range of NLP applications.",
      "homepage": "https://deepset.ai/haystack",
      "language": "Python",
      "created_at": "2022-11-17T02:44:17Z",
      "updated_at": "2022-12-28T13:37:29Z",
      "topics": [],
      "readme": "<p align=\"center\">\n  <a href=\"https://www.deepset.ai/haystack/\"><img src=\"https://raw.githubusercontent.com/deepset-ai/haystack/main/docs/img/haystack_logo_colored.png\" alt=\"Haystack\"></a>\n</p>\n\n<p>\n    <a href=\"https://github.com/deepset-ai/haystack/actions/workflows/tests.yml\">\n        <img alt=\"Tests\" src=\"https://github.com/deepset-ai/haystack/workflows/Tests/badge.svg?branch=main\">\n    </a>\n    <a href=\"https://github.com/deepset-ai/haystack-json-schema/actions/workflows/schemas.yml\">\n        <img alt=\"Schemas\" src=\"https://github.com/deepset-ai/haystack-json-schema/actions/workflows/schemas.yml/badge.svg\">\n    </a>\n    <a href=\"https://docs.haystack.deepset.ai\">\n        <img alt=\"Documentation\" src=\"https://img.shields.io/website?label=documentation&up_message=online&url=https%3A%2F%2Fdocs.haystack.deepset.ai\">\n    </a>\n    <a href=\"https://github.com/deepset-ai/haystack/releases\">\n        <img alt=\"Release\" src=\"https://img.shields.io/github/release/deepset-ai/haystack\">\n    </a>\n    <a href=\"https://github.com/deepset-ai/haystack/commits/main\">\n        <img alt=\"Last commit\" src=\"https://img.shields.io/github/last-commit/deepset-ai/haystack\">\n    </a>\n    <a href=\"https://pepy.tech/project/farm-haystack\">\n        <img alt=\"Downloads\" src=\"https://pepy.tech/badge/farm-haystack/month\">\n    </a>\n    <a href=\"https://www.deepset.ai/jobs\">\n        <img alt=\"Jobs\" src=\"https://img.shields.io/badge/Jobs-We're%20hiring-blue\">\n    </a>\n        <a href=\"https://twitter.com/intent/follow?screen_name=deepset_ai\">\n        <img alt=\"Twitter\" src=\"https://img.shields.io/twitter/follow/deepset_ai?style=social\">\n    </a>\n</p>\n\n[Haystack](https://haystack.deepset.ai) is an end-to-end framework that enables you to build powerful and production-ready pipelines for different search use cases.\nWhether you want to perform Question Answering or semantic document search, you can use the State-of-the-Art NLP models in Haystack to provide unique search experiences and allow your users to query in natural language.\nHaystack is built in a modular fashion so that you can combine the best technology from other open-source projects like Huggingface's Transformers, Elasticsearch, or Milvus.\n\n<p align=\"center\"><img src=\"https://raw.githubusercontent.com/deepset-ai/haystack/main/docs/img/main_example.gif\"></p>\n\n## What to build with Haystack\n\n- **Ask questions in natural language** and find granular answers in your documents.\n- Perform **semantic search** and retrieve documents according to meaning, not keywords\n- Use **off-the-shelf models** or **fine-tune** them to your domain.\n- Use **user feedback** to evaluate, benchmark, and continuously improve your live models.\n- Leverage existing **knowledge bases** and better handle the long tail of queries that **chatbots** receive.\n- **Automate processes** by automatically applying a list of questions to new documents and using the extracted answers.\n\n## Core Features\n\n- **Latest models**: Utilize all latest transformer-based models (e.g., BERT, RoBERTa, MiniLM) for extractive QA, generative QA, and document retrieval.\n- **Modular**: Multiple choices to fit your tech stack and use case. Pick your favorite database, file converter, or modeling framework.\n- **Pipelines**: The Node and Pipeline design of Haystack allows for custom routing of queries to only the relevant components.\n- **Open**: 100% compatible with HuggingFace's model hub. Tight interfaces to other frameworks (e.g., Transformers, FARM, sentence-transformers)\n- **Scalable**: Scale to millions of docs via retrievers, production-ready backends like Elasticsearch / FAISS, and a fastAPI REST API\n- **End-to-End**: All tooling in one place: file conversion, cleaning, splitting, training, eval, inference, labeling, etc.\n- **Developer friendly**: Easy to debug, extend and modify.\n- **Customizable**: Fine-tune models to your domain or implement your custom DocumentStore.\n- **Continuous Learning**: Collect new training data via user feedback in production & improve your models continuously\n\n|                                                                                               |                                                                                                                                                                                                                                                   |\n| --------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| :ledger: [Docs](https://docs.haystack.deepset.ai)                                             | Components, Pipeline Nodes, Guides, API Reference                                                                                                                                                                                                 |\n| :floppy_disk: [Installation](https://github.com/deepset-ai/haystack#floppy_disk-installation) | How to install Haystack                                                                                                                                                                                                                           |\n| :mortar_board: [Tutorials](https://github.com/deepset-ai/haystack#mortar_board-tutorials)     | See what Haystack can do with our Notebooks & Scripts                                                                                                                                                                                             |\n| :beginner: [Quick Demo](https://github.com/deepset-ai/haystack#beginner-quick-demo)           | Deploy a Haystack application with Docker Compose and a REST API                                                                                                                                                                                  |\n| :vulcan_salute: [Community](https://github.com/deepset-ai/haystack#vulcan_salute-community)   | [Discord](https://haystack.deepset.ai/community/join), [Twitter](https://twitter.com/deepset_ai), [Stack Overflow](https://stackoverflow.com/questions/tagged/haystack), [GitHub Discussions](https://github.com/deepset-ai/haystack/discussions) |\n| :heart: [Contributing](https://github.com/deepset-ai/haystack#heart-contributing)             | We welcome all contributions!                                                                                                                                                                                                                     |\n| :bar_chart: [Benchmarks](https://haystack.deepset.ai/benchmarks/)                             | Speed & Accuracy of Retriever, Readers and DocumentStores                                                                                                                                                                                         |\n| :telescope: [Roadmap](https://haystack.deepset.ai/overview/roadmap)                           | Public roadmap of Haystack                                                                                                                                                                                                                        |\n| :newspaper: [Blog](https://medium.com/deepset-ai)                                             | Read our articles on Medium                                                                                                                                                                                                                       |\n| :phone: [Jobs](https://www.deepset.ai/jobs)                                                   | We're hiring! Have a look at our open positions                                                                                                                                                                                                   |\n\n\n## :floppy_disk: Installation\n\n**1. Basic Installation**\n\nYou can install a basic version of Haystack's latest release by using [pip](https://github.com/pypa/pip).\n\n```\n    pip3 install farm-haystack\n```\n\nThis command will install everything needed for basic Pipelines that use an Elasticsearch Document Store.\n\n**2. Full Installation**\n\nIf you plan to be using more advanced features like Milvus, FAISS, Weaviate, OCR or Ray,\nyou will need to install a full version of Haystack.\nThe following command will install the latest version of Haystack from the main branch.\n\n```\ngit clone https://github.com/deepset-ai/haystack.git\ncd haystack\npip install --upgrade pip\npip install -e '.[all]' ## or 'all-gpu' for the GPU-enabled dependencies\n```\n\nIf you cannot upgrade `pip` to version 21.3 or higher, you will need to replace:\n- `'.[all]'` with `'.[sql,only-faiss,only-milvus,weaviate,graphdb,crawler,preprocessing,ocr,onnx,ray,dev]'`\n- `'.[all-gpu]'` with `'.[sql,only-faiss-gpu,only-milvus,weaviate,graphdb,crawler,preprocessing,ocr,onnx-gpu,ray,dev]'`\n\nFor an complete list of the dependency groups available, have a look at the `haystack/pyproject.toml` file.\n\nTo install the REST API and UI, run the following from the root directory of the Haystack repo\n\n```\npip install rest_api/\npip install ui/\n```\n\n**3. Installing on Windows**\n\n```\npip install farm-haystack -f https://download.pytorch.org/whl/torch_stable.html\n```\n\n**4. Installing on Apple Silicon (M1)**\n\nM1 Macbooks require some extra dependencies in order to install Haystack.\n\n```\n# some additional dependencies needed on m1 mac\nbrew install postgresql\nbrew install cmake\nbrew install rust\n\n# haystack installation\nGRPC_PYTHON_BUILD_SYSTEM_ZLIB=true pip install git+https://github.com/deepset-ai/haystack.git\n```\n\n**5. Learn More**\n\nSee our [installation guide](https://haystack.deepset.ai/overview/get-started) for more options.\nYou can find out more about our PyPi package on our [PyPi page](https://pypi.org/project/farm-haystack/).\n\n## :mortar_board: Tutorials\n\n![image](https://raw.githubusercontent.com/deepset-ai/haystack/main/docs/img/concepts_haystack_handdrawn.png)\n\nFollow our [introductory tutorial](https://haystack.deepset.ai/tutorials/first-qa-system)\nto setup a question answering system using Python and start performing queries!\nExplore [the rest of our tutorials](https://haystack.deepset.ai/tutorials)\nto learn how to tweak pipelines, train models and perform evaluation.\n\n## :beginner: Quick Demo\n\n**Hosted**\n\nTry out our hosted [Explore The World](https://haystack-demo.deepset.ai/) live demo here!\nAsk any question on countries or capital cities and let Haystack return the answers to you.\n\n**Local**\n\nStart up a Haystack service via [Docker Compose](https://docs.docker.com/compose/).\nWith this you can begin calling it directly via the REST API or even interact with it using the included Streamlit UI.\n\n<details>\n  <summary>Click here for a step-by-step guide</summary>\n\n**1. Update/install Docker and Docker Compose, then launch Docker**\n\n```\n    apt-get update && apt-get install docker && apt-get install docker-compose\n    service docker start\n```\n\n**2. Clone Haystack repository**\n\n```\n    git clone https://github.com/deepset-ai/haystack.git\n```\n\n**3. Pull images & launch demo app**\n\n```\n    cd haystack\n    docker-compose pull\n    docker-compose up\n\n    # Or on a GPU machine: docker-compose -f docker-compose-gpu.yml up\n```\n\nYou should be able to see the following in your terminal window as part of the log output:\n\n```\n..\nui_1             |   You can now view your Streamlit app in your browser.\n..\nui_1             |   External URL: http://192.168.108.218:8501\n..\nhaystack-api_1   | [2021-01-01 10:21:58 +0000] [17] [INFO] Application startup complete.\n```\n\n**4. Open the Streamlit UI for Haystack by pointing your browser to the \"External URL\" from above.**\n\nYou should see the following:\n\n![image](https://raw.githubusercontent.com/deepset-ai/haystack/main/docs/img/streamlit_ui_screenshot.png)\n\nYou can then try different queries against a pre-defined set of indexed articles related to Game of Thrones.\n\n**Note**: The following containers are started as a part of this demo:\n\n* Haystack API: listens on port 8000\n* DocumentStore (Elasticsearch): listens on port 9200\n* Streamlit UI: listens on port 8501\n\nPlease note that the demo will [publish](https://docs.docker.com/config/containers/container-networking/) the container ports to the outside world. *We suggest that you review the firewall settings depending on your system setup and the security guidelines.*\n\n</details>\n\n## :vulcan_salute: Community\n\nThere is a very vibrant and active community around Haystack which we are regularly interacting with!\nIf you have a feature request or a bug report, feel free to open an [issue in Github](https://github.com/deepset-ai/haystack/issues).\nWe regularly check these and you can expect a quick response.\nIf you'd like to discuss a topic, or get more general advice on how to make Haystack work for your project,\nyou can start a thread in [Github Discussions](https://github.com/deepset-ai/haystack/discussions) or our [Discord channel](https://haystack.deepset.ai/community/join).\nWe also check [Twitter](https://twitter.com/deepset_ai) and [Stack Overflow](https://stackoverflow.com/questions/tagged/haystack).\n\n\n## :heart: Contributing\n\nWe are very open to the community's contributions - be it a quick fix of a typo, or a completely new feature!\nYou don't need to be a Haystack expert to provide meaningful improvements.\nTo learn how to get started, check out our [Contributor Guidelines](https://github.com/deepset-ai/haystack/blob/main/CONTRIBUTING.md) first.\n\nYou can also find instructions to run the tests locally there.\n\nThanks so much to all those who have contributed to our project!\n\n<a href=\"https://github.com/deepset-ai/haystack/graphs/contributors\">\n  <img src=\"https://contrib.rocks/image?repo=deepset-ai/haystack\" />\n</a>\n\n\n## Who uses Haystack\n\nHere's a list of organizations who use Haystack. Don't hesitate to send a PR to let the world know that you use Haystack. Join our growing community!\n\n- [Airbus](https://www.airbus.com/en)\n- [Alcatel-Lucent](https://www.al-enterprise.com/)\n- [BetterUp](https://www.betterup.com/)\n- [Deepset](https://deepset.ai/)\n- [Etalab](https://www.etalab.gouv.fr/)\n- [Infineon](https://www.infineon.com/)\n- [Sooth.ai](https://sooth.ai/)\n"
    },
    {
      "name": "tamanna18/haystack",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/57901189?s=40&v=4",
      "owner": "tamanna18",
      "repo_name": "haystack",
      "description": ":mag: Haystack is an open source NLP framework that leverages pre-trained Transformer models. It enables developers to quickly implement production-ready semantic search, question answering, summarization and document ranking for a wide range of NLP applications.",
      "homepage": "https://deepset.ai/haystack",
      "language": null,
      "created_at": "2022-10-12T12:13:55Z",
      "updated_at": "2023-02-26T12:12:13Z",
      "topics": [],
      "readme": "<p align=\"center\">\n  <a href=\"https://www.deepset.ai/haystack/\"><img src=\"https://raw.githubusercontent.com/deepset-ai/haystack/main/docs/img/haystack_logo_colored.png\" alt=\"Haystack\"></a>\n</p>\n\n<p>\n    <a href=\"https://github.com/deepset-ai/haystack/actions/workflows/tests.yml\">\n        <img alt=\"Tests\" src=\"https://github.com/deepset-ai/haystack/workflows/Tests/badge.svg?branch=main\">\n    </a>\n    <a href=\"https://haystack.deepset.ai/overview/intro\">\n        <img alt=\"Documentation\" src=\"https://img.shields.io/website/http/haystack.deepset.ai/docs/intromd.svg?down_color=red&down_message=offline&up_message=online\">\n    </a>\n    <a href=\"https://github.com/deepset-ai/haystack/releases\">\n        <img alt=\"Release\" src=\"https://img.shields.io/github/release/deepset-ai/haystack\">\n    </a>\n    <a href=\"https://github.com/deepset-ai/haystack/commits/main\">\n        <img alt=\"Last commit\" src=\"https://img.shields.io/github/last-commit/deepset-ai/haystack\">\n    </a>\n    <a href=\"https://pepy.tech/project/farm-haystack\">\n        <img alt=\"Downloads\" src=\"https://pepy.tech/badge/farm-haystack/month\">\n    </a>\n    <a href=\"https://www.deepset.ai/jobs\">\n        <img alt=\"Jobs\" src=\"https://img.shields.io/badge/Jobs-We're%20hiring-blue\">\n    </a>\n        <a href=\"https://twitter.com/intent/follow?screen_name=deepset_ai\">\n        <img alt=\"Twitter\" src=\"https://img.shields.io/twitter/follow/deepset_ai?style=social\">\n    </a>\n</p>\n\nHaystack is an end-to-end framework that enables you to build powerful and production-ready pipelines for different search use cases.\nWhether you want to perform Question Answering or semantic document search, you can use the State-of-the-Art NLP models in Haystack to provide unique search experiences and allow your users to query in natural language.\nHaystack is built in a modular fashion so that you can combine the best technology from other open-source projects like Huggingface's Transformers, Elasticsearch, or Milvus.\n\n<p align=\"center\"><img src=\"https://raw.githubusercontent.com/deepset-ai/haystack/main/docs/img/main_example.gif\"></p>\n\n## What to build with Haystack\n\n- **Ask questions in natural language** and find granular answers in your documents.\n- Perform **semantic search** and retrieve documents according to meaning, not keywords\n- Use **off-the-shelf models** or **fine-tune** them to your domain.\n- Use **user feedback** to evaluate, benchmark, and continuously improve your live models.\n- Leverage existing **knowledge bases** and better handle the long tail of queries that **chatbots** receive.\n- **Automate processes** by automatically applying a list of questions to new documents and using the extracted answers.\n\n## Core Features\n\n- **Latest models**: Utilize all latest transformer-based models (e.g., BERT, RoBERTa, MiniLM) for extractive QA, generative QA, and document retrieval.\n- **Modular**: Multiple choices to fit your tech stack and use case. Pick your favorite database, file converter, or modeling framework.\n- **Pipelines**: The Node and Pipeline design of Haystack allows for custom routing of queries to only the relevant components.\n- **Open**: 100% compatible with HuggingFace's model hub. Tight interfaces to other frameworks (e.g., Transformers, FARM, sentence-transformers)\n- **Scalable**: Scale to millions of docs via retrievers, production-ready backends like Elasticsearch / FAISS, and a fastAPI REST API\n- **End-to-End**: All tooling in one place: file conversion, cleaning, splitting, training, eval, inference, labeling, etc.\n- **Developer friendly**: Easy to debug, extend and modify.\n- **Customizable**: Fine-tune models to your domain or implement your custom DocumentStore.\n- **Continuous Learning**: Collect new training data via user feedback in production & improve your models continuously\n\n|                                                                                               |                                                                                                                                                                                                                                                   |\n| --------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| :ledger: [Docs](https://haystack.deepset.ai/overview/intro)                                   | Overview, Components, Guides, API documentation                                                                                                                                                                                                   |\n| :floppy_disk: [Installation](https://github.com/deepset-ai/haystack#floppy_disk-installation) | How to install Haystack                                                                                                                                                                                                                           |\n| :mortar_board: [Tutorials](https://github.com/deepset-ai/haystack#mortar_board-tutorials)     | See what Haystack can do with our Notebooks & Scripts                                                                                                                                                                                             |\n| :beginner: [Quick Demo](https://github.com/deepset-ai/haystack#beginner-quick-demo)           | Deploy a Haystack application with Docker Compose and a REST API                                                                                                                                                                                  |\n| :vulcan_salute: [Community](https://github.com/deepset-ai/haystack#vulcan_salute-community)   | [Discord](https://haystack.deepset.ai/community/join), [Twitter](https://twitter.com/deepset_ai), [Stack Overflow](https://stackoverflow.com/questions/tagged/haystack), [GitHub Discussions](https://github.com/deepset-ai/haystack/discussions) |\n| :heart: [Contributing](https://github.com/deepset-ai/haystack#heart-contributing)             | We welcome all contributions!                                                                                                                                                                                                                     |\n| :bar_chart: [Benchmarks](https://haystack.deepset.ai/benchmarks/latest)                       | Speed & Accuracy of Retriever, Readers and DocumentStores                                                                                                                                                                                         |\n| :telescope: [Roadmap](https://haystack.deepset.ai/overview/roadmap)                           | Public roadmap of Haystack                                                                                                                                                                                                                        |\n| :newspaper: [Blog](https://medium.com/deepset-ai)                                             | Read our articles on Medium                                                                                                                                                                                                                       |\n| :phone: [Jobs](https://www.deepset.ai/jobs)                                                   | We're hiring! Have a look at our open positions                                                                                                                                                                                                   |\n\n\n## :floppy_disk: Installation\n\n**1. Basic Installation**\n\nYou can install a basic version of Haystack's latest release by using [pip](https://github.com/pypa/pip).\n\n```\n    pip3 install farm-haystack\n```\n\nThis command will install everything needed for basic Pipelines that use an Elasticsearch Document Store.\n\n**2. Full Installation**\n\nIf you plan to be using more advanced features like Milvus, FAISS, Weaviate, OCR or Ray,\nyou will need to install a full version of Haystack.\nThe following command will install the latest version of Haystack from the main branch.\n\n```\ngit clone https://github.com/deepset-ai/haystack.git\ncd haystack\npip install --upgrade pip\npip install -e '.[all]' ## or 'all-gpu' for the GPU-enabled dependencies\n```\n\nIf you cannot upgrade `pip` to version 21.3 or higher, you will need to replace:\n- `'.[all]'` with `'.[sql,only-faiss,only-milvus1,weaviate,graphdb,crawler,preprocessing,ocr,onnx,ray,dev]'`\n- `'.[all-gpu]'` with `'.[sql,only-faiss-gpu,only-milvus1,weaviate,graphdb,crawler,preprocessing,ocr,onnx-gpu,ray,dev]'`\n\nFor an complete list of the dependency groups available, have a look at the `haystack/pyproject.toml` file.\n\nTo install the REST API and UI, run the following from the root directory of the Haystack repo\n\n```\npip install rest_api/\npip install ui/\n```\n\n**3. Installing on Windows**\n\n```\npip install farm-haystack -f https://download.pytorch.org/whl/torch_stable.html\n```\n\n**4. Installing on Apple Silicon (M1)**\n\nM1 Macbooks require some extra dependencies in order to install Haystack.\n\n```\n# some additional dependencies needed on m1 mac\nbrew install postgresql\nbrew install cmake\nbrew install rust\n\n# haystack installation\nGRPC_PYTHON_BUILD_SYSTEM_ZLIB=true pip install git+https://github.com/deepset-ai/haystack.git\n```\n\n**5. Learn More**\n\nSee our [installation guide](https://haystack.deepset.ai/overview/get-started) for more options.\nYou can find out more about our PyPi package on our [PyPi page](https://pypi.org/project/farm-haystack/).\n\n## :mortar_board: Tutorials\n\n![image](https://raw.githubusercontent.com/deepset-ai/haystack/main/docs/img/concepts_haystack_handdrawn.png)\n\nFollow our [introductory tutorial](https://haystack.deepset.ai/tutorials/first-qa-system)\nto setup a question answering system using Python and start performing queries!\nExplore [the rest of our tutorials](https://github.com/deepset-ai/haystack-tutorials)\nto learn how to tweak pipelines, train models and perform evaluation.\n\n## :beginner: Quick Demo\n\n**Hosted**\n\nTry out our hosted [Explore The World](https://haystack-demo.deepset.ai/) live demo here!\nAsk any question on countries or capital cities and let Haystack return the answers to you.\n\n**Local**\n\nStart up a Haystack service via [Docker Compose](https://docs.docker.com/compose/).\nWith this you can begin calling it directly via the REST API or even interact with it using the included Streamlit UI.\n\n<details>\n  <summary>Click here for a step-by-step guide</summary>\n\n**1. Update/install Docker and Docker Compose, then launch Docker**\n\n```\n    apt-get update && apt-get install docker && apt-get install docker-compose\n    service docker start\n```\n\n**2. Clone Haystack repository**\n\n```\n    git clone https://github.com/deepset-ai/haystack.git\n```\n\n**3. Pull images & launch demo app**\n\n```\n    cd haystack\n    docker-compose pull\n    docker-compose up\n\n    # Or on a GPU machine: docker-compose -f docker-compose-gpu.yml up\n```\n\nYou should be able to see the following in your terminal window as part of the log output:\n\n```\n..\nui_1             |   You can now view your Streamlit app in your browser.\n..\nui_1             |   External URL: http://192.168.108.218:8501\n..\nhaystack-api_1   | [2021-01-01 10:21:58 +0000] [17] [INFO] Application startup complete.\n```\n\n**4. Open the Streamlit UI for Haystack by pointing your browser to the \"External URL\" from above.**\n\nYou should see the following:\n\n![image](https://raw.githubusercontent.com/deepset-ai/haystack/main/docs/img/streamlit_ui_screenshot.png)\n\nYou can then try different queries against a pre-defined set of indexed articles related to Game of Thrones.\n\n**Note**: The following containers are started as a part of this demo:\n\n* Haystack API: listens on port 8000\n* DocumentStore (Elasticsearch): listens on port 9200\n* Streamlit UI: listens on port 8501\n\nPlease note that the demo will [publish](https://docs.docker.com/config/containers/container-networking/) the container ports to the outside world. *We suggest that you review the firewall settings depending on your system setup and the security guidelines.*\n\n</details>\n\n## :vulcan_salute: Community\n\nThere is a very vibrant and active community around Haystack which we are regularly interacting with!\nIf you have a feature request or a bug report, feel free to open an [issue in Github](https://github.com/deepset-ai/haystack/issues).\nWe regularly check these and you can expect a quick response.\nIf you'd like to discuss a topic, or get more general advice on how to make Haystack work for your project,\nyou can start a thread in [Github Discussions](https://github.com/deepset-ai/haystack/discussions) or our [Discord channel](https://haystack.deepset.ai/community/join).\nWe also check [Twitter](https://twitter.com/deepset_ai) and [Stack Overflow](https://stackoverflow.com/questions/tagged/haystack).\n\n\n## :heart: Contributing\n\nWe are very open to the community's contributions - be it a quick fix of a typo, or a completely new feature!\nYou don't need to be a Haystack expert to provide meaningful improvements.\nTo learn how to get started, check out our [Contributor Guidelines](https://github.com/deepset-ai/haystack/blob/main/CONTRIBUTING.md) first.\n\nYou can also find instructions to run the tests locally there.\n\nThanks so much to all those who have contributed to our project!\n\n<a href=\"https://github.com/deepset-ai/haystack/graphs/contributors\">\n  <img src=\"https://contrib.rocks/image?repo=deepset-ai/haystack\" />\n</a>\n\n\n## Who uses Haystack\n\nHere's a list of organizations who use Haystack. Don't hesitate to send a PR to let the world know that you use Haystack. Join our growing community!\n\n- [Airbus](https://www.airbus.com/en)\n- [Alcatel-Lucent](https://www.al-enterprise.com/)\n- [BetterUp](https://www.betterup.com/)\n- [Deepset](https://deepset.ai/)\n- [Etalab](https://www.etalab.gouv.fr/)\n- [Infineon](https://www.infineon.com/)\n- [Sooth.ai](https://sooth.ai/)\n"
    },
    {
      "name": "hsm207/haystack",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/2398765?s=40&v=4",
      "owner": "hsm207",
      "repo_name": "haystack",
      "description": ":mag: Haystack is an open source NLP framework that leverages pre-trained Transformer models. It enables developers to quickly implement production-ready semantic search, question answering, summarization and document ranking for a wide range of NLP applications.",
      "homepage": "https://deepset.ai/haystack",
      "language": "Python",
      "created_at": "2022-10-09T00:55:58Z",
      "updated_at": "2023-03-05T02:42:27Z",
      "topics": [],
      "readme": "<div align=\"center\">\n  <a href=\"https://www.deepset.ai/haystack/\"><img src=\"https://raw.githubusercontent.com/deepset-ai/haystack/main/docs/img/haystack_logo_colored.png\" alt=\"Haystack\"></a>\n\n|         |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |\n| ------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| CI/CD   | [![Tests](https://github.com/deepset-ai/haystack/actions/workflows/tests.yml/badge.svg)](https://github.com/deepset-ai/haystack/actions/workflows/tests.yml) [![Docker image release](https://github.com/deepset-ai/haystack/actions/workflows/docker_release.yml/badge.svg)](https://github.com/deepset-ai/haystack/actions/workflows/docker_release.yml) [![Schemas](https://github.com/deepset-ai/haystack/actions/workflows/schemas.yml/badge.svg)](https://github.com/deepset-ai/haystack/actions/workflows/schemas.yml) [![code style - Black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black) [![types - Mypy](https://img.shields.io/badge/types-Mypy-blue.svg)](https://github.com/python/mypy) |\n| Docs    | [![Sync docs with Readme](https://github.com/deepset-ai/haystack/actions/workflows/readme_sync.yml/badge.svg)](https://github.com/deepset-ai/haystack/actions/workflows/readme_sync.yml) ![Website](https://img.shields.io/website?label=documentation&up_message=online&url=https%3A%2F%2Fdocs.haystack.deepset.ai)                                                                                                                                                                                                                                                                                                                                                                                                                                |\n| Package | ![PyPI](https://img.shields.io/pypi/v/farm-haystack) ![PyPI - Downloads](https://img.shields.io/pypi/dm/farm-haystack?color=blue&logo=pypi&logoColor=gold) ![PyPI - Python Version](https://img.shields.io/pypi/pyversions/farm-haystack?logo=python&logoColor=gold) ![GitHub](https://img.shields.io/github/license/deepset-ai/haystack?color=blue) [![License Compliance](https://github.com/deepset-ai/haystack/actions/workflows/license_compliance.yml/badge.svg)](https://github.com/deepset-ai/haystack/actions/workflows/license_compliance.yml)                                                                                                                                                                                            |\n| Meta    | ![Discord](https://img.shields.io/discord/993534733298450452?logo=discord) ![Twitter Follow](https://img.shields.io/twitter/follow/deepset_ai)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |\n</div>\n\n[Haystack](https://haystack.deepset.ai/) is an end-to-end NLP framework that enables you to build NLP applications powered by LLMs, Transformer models, vector search and more. Whether you want to perform question answering, answer generation, semantic document search, or build tools that are capable of complex decision making and query resolution, you can use the state-of-the-art NLP models with Haystack to build end-to-end NLP applications solving your use case.\n\n## Core Concepts\n\n🏃‍♀️ **[Pipelines](https://docs.haystack.deepset.ai/docs/pipelines):** This is the standard Haystack structure that can connect to your data and perform on it NLP tasks that you define. The data in a Pipeline flows from one Node to the next. You define how Nodes interact with each other, and how one Node pushes data to the next.\n\nAn example pipeline would consist of one `Retriever` Node and one `Reader` Node. When the pipeline runs with a query, the Retriever first retrieves the documents relevant to the query and then the Reader extracts the final answer.\n\n⚛️ **[Nodes](https://docs.haystack.deepset.ai/docs/nodes_overview):** Each Node achieves one thing. Such as preprocessing documents, retrieving documents, using language models to answer questions and so on.\n\n🕵️ **[Agent](https://docs.haystack.deepset.ai/docs/agent):** (since 1.15) An Agent is a component that is powered by an LLM, such as GPT-3. It can decide on the next best course of action so as to get to the result of a query. It uses the Tools available to it to achieve this. While a pipeline has a clear start and end, an Agent is able to decide whether the query has resolved or not. It may also make use of a Pipeline as a Tool.\n\n🛠️ **[Tools](https://docs.haystack.deepset.ai/docs/agent#tools):** You can think of a Tool as an expert, that is able to do something really well. Such as a calculator, good at mathematics. Or a [WebRetriever](https://docs.haystack.deepset.ai/docs/agent#web-tools), good at retrieving pages from the internet. A Node or pipeline in Haystack can also be used as a Tool. A Tool is a component that is used by an Agent, to resolve complex queries.\n\n🗂️ **[DocumentStores](https://docs.haystack.deepset.ai/docs/document_store):** A DocumentStore is database where you store your text data for Haystack to access. Haystack DocumentStores are available with ElasticSearch, Opensearch, Weaviate, Pinecone, FAISS and more. For a full list of available DocumentStores, check out our [documentation](https://docs.haystack.deepset.ai/docs/document_store).\n\n## What to Build with Haystack\n\n-   Perform Question Answering **in natural language** to find granular answers in your documents.\n-   **Generate answers or content** with the use of LLM such as articles, tweets, product descriptions and more, the sky is the limit 🚀\n-   Perform **semantic search** and retrieve documents according to meaning.\n-   Build applications that can do complex decisions making to answer complex queries: such as systems that can resolve complex customer queries, do knowledge search on many disconnected resources and so on.\n-   Use **off-the-shelf models** or **fine-tune** them to your data.\n-   Use **user feedback** to evaluate, benchmark, and continuously improve your models.\n\n## Features\n\n-   **Latest models**: Haystack allows you to use and compare models available from OpenAI, Cohere and Hugging Face, as well as your own local models. Use the latest LLMs or Transformer-based models (for example: BERT, RoBERTa, MiniLM).\n-   **Modular**: Multiple choices to fit your tech stack and use case. A wide choice of DocumentStores to store your data, file conversion tools and more\n-   **Open**: Integrated with Hugging Face's model hub, OpenAI, Cohere and various Azure services.\n-   **Scalable**: Scale to millions of docs using retrievers and production-scale components like Elasticsearch and a fastAPI REST API.\n-   **End-to-End**: All tooling in one place: file conversion, cleaning, splitting, training, eval, inference, labeling, and more.\n-   **Customizable**: Fine-tune models to your domain or implement your custom Nodes.\n-   **Continuous Learning**: Collect new training data from user feedback in production & improve your models continuously.\n\n## Resources\n|                                                                        |                                                                                                                                                                                                                                                   |\n| ---------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| 📒 [Docs](https://docs.haystack.deepset.ai)                             | Components, Pipeline Nodes, Guides, API Reference                                                                                                                                                                                                 |\n| 💾 [Installation](https://github.com/deepset-ai/haystack#-installation) | How to install Haystack                                                                                                                                                                                                                           |\n| 🎓 [Tutorials](https://haystack.deepset.ai/tutorials)                   | See what Haystack can do with our Notebooks & Scripts                                                                                                                                                                                             |\n| 🎉 [Haystack Extras](https://github.com/deepset-ai/haystack-extras)     | A repository that lists extra Haystack packages and components that can be installed separately.                                                                                                                                                  |\n| 🔰 [Demos](https://github.com/deepset-ai/haystack-demos)                | A repository containing Haystack demo applications with Docker Compose and a REST API                                                                                                                                                             |\n| 🖖 [Community](https://github.com/deepset-ai/haystack#-community)       | [Discord](https://haystack.deepset.ai/community/join), [Twitter](https://twitter.com/deepset_ai), [Stack Overflow](https://stackoverflow.com/questions/tagged/haystack), [GitHub Discussions](https://github.com/deepset-ai/haystack/discussions) |\n| 💙 [Contributing](https://github.com/deepset-ai/haystack#-contributing) | We welcome all contributions!                                                                                                                                                                                                                     |\n| 📊 [Benchmarks](https://haystack.deepset.ai/benchmarks/)                | Speed & Accuracy of Retriever, Readers and DocumentStores                                                                                                                                                                                         |\n| 🔭 [Roadmap](https://haystack.deepset.ai/overview/roadmap)              | Public roadmap of Haystack                                                                                                                                                                                                                        |\n| 📰 [Blog](https://haystack.deepset.ai/blog)                             | Learn about the latest with Haystack and NLP                                                                                                                                                                                                      |\n| ☎️ [Jobs](https://www.deepset.ai/jobs)                                  | We're hiring! Have a look at our open positions                                                                                                                                                                                                   |\n\n\n## 💾 Installation\n\nFor a detailed installation guide see [the official documentation](https://docs.haystack.deepset.ai/docs/installation). There you’ll find instructions for custom installations handling Windows and Apple Silicon.\n\n**Basic Installation**\n\nUse [pip](https://github.com/pypa/pip) to install a basic version of Haystack's latest release:\n\n```sh\npip install farm-haystack\n```\n\nThis command installs everything needed for basic Pipelines that use an in-memory DocumentStore.\n\n**Full Installation**\n\nTo use more advanced features, like certain DocumentStores, FileConverters, OCR, or Ray,\nyou need to install further dependencies. The following command installs the [latest release](https://github.com/deepset-ai/haystack/releases) of Haystack and all its dependencies:\n\n```sh\npip install 'farm-haystack[all]' ## or 'all-gpu' for the GPU-enabled dependencies\n```\n\nIf you want to try out the newest features that are not in an official release yet, you can install the unstable version from the main branch with the following command:\n\n```sh\npip install git+https://github.com/deepset-ai/haystack.git@main#egg=farm-haystack\n```\n\nTo be able to make changes to Haystack code, first of all clone this repo:\n\n```sh\ngit clone https://github.com/deepset-ai/haystack.git\n```\n\nThen move into the cloned folder and install the project with `pip`, including the development dependencies:\n\n```console\ncd haystack && pip install -e '.[dev]'\n```\n\nIf you want to contribute to the Haystack repo, check our [Contributor Guidelines](#💙-contributing) first.\n\nSee the list of [dependencies](https://github.com/deepset-ai/haystack/blob/main/pyproject.toml) to check which ones you want to install (for example, `[all]`, `[dev]`, or other).\n\n**Installing the REST API**\n\nHaystack comes packaged with a REST API so that you can deploy it as a service. Run the following command from the root directory of the Haystack repo to install REST_API:\n\n```\npip install rest_api/\n```\n\nYou can find out more about our PyPi package on our [PyPi page](https://pypi.org/project/farm-haystack/).\n\n## 🔰Demos\n\nYou can find some of our hosted demos with instructions to run them locally too on our [haystack-demos](https://github.com/deepset-ai/haystack-demos) repository\n\n:dizzy: **[Reduce Hallucinations with Retrieval Augmentation](https://huggingface.co/spaces/deepset/retrieval-augmentation-svb) - Generative QA with LLMs**\n\n🐥 **[Should I follow?](https://huggingface.co/spaces/deepset/should-i-follow) - Summarizing tweets with LLMs**\n\n🌎 **[Explore The World](https://haystack-demo.deepset.ai/) - Extractive Question Answering**\n\n### 🖖 Community\n\nIf you have a feature request or a bug report, feel free to open an [issue in Github](https://github.com/deepset-ai/haystack/issues). We regularly check these and you can expect a quick response. If you'd like to discuss a topic, or get more general advice on how to make Haystack work for your project, you can start a thread in [Github Discussions](https://github.com/deepset-ai/haystack/discussions) or our [Discord channel](https://haystack.deepset.ai/community). We also check [Twitter](https://twitter.com/deepset_ai) and [Stack Overflow](https://stackoverflow.com/questions/tagged/haystack).\n\n### 💙 Contributing\n\nWe are very open to the community's contributions - be it a quick fix of a typo, or a completely new feature! You don't need to be a Haystack expert to provide meaningful improvements. To learn how to get started, check out our [Contributor Guidelines](https://github.com/deepset-ai/haystack/blob/main/CONTRIBUTING.md) first.\n\n\n## Who Uses Haystack\n\nHere's a list of projects and companies using Haystack. Want to add yours? Open a PR, add it to the list and let the\nworld know that you use Haystack!\n\n-   [Airbus](https://www.airbus.com/en)\n-   [Alcatel-Lucent](https://www.al-enterprise.com/)\n-   [Apple](https://www.apple.com/)\n-   [BetterUp](https://www.betterup.com/)\n-   [Databricks](https://www.databricks.com/)\n-   [Deepset](https://deepset.ai/)\n-   [Etalab](https://www.deepset.ai/blog/improving-on-site-search-for-government-agencies-etalab)\n-   [Infineon](https://www.infineon.com/)\n-   [Intel](https://github.com/intel/open-domain-question-and-answer#readme)\n-   [Intelijus](https://www.intelijus.ai/)\n-   [Intel Labs](https://github.com/IntelLabs/fastRAG#readme)\n-   [LEGO](https://github.com/larsbaunwall/bricky#readme)\n-   [Netflix](https://netflix.com)\n-   [Nvidia](https://developer.nvidia.com/blog/reducing-development-time-for-intelligent-virtual-assistants-in-contact-centers/)\n-   [PostHog](https://github.com/PostHog/max-ai#readme)\n-   [Rakuten](https://www.rakuten.com/)\n-   [Sooth.ai](https://www.deepset.ai/blog/advanced-neural-search-with-sooth-ai)\n"
    },
    {
      "name": "creatorrr/haystack",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/931887?s=40&v=4",
      "owner": "creatorrr",
      "repo_name": "haystack",
      "description": ":mag: Haystack is an open source NLP framework that leverages pre-trained Transformer models. It enables developers to quickly implement production-ready semantic search, question answering, summarization and document ranking for a wide range of NLP applications.",
      "homepage": "https://deepset.ai/haystack",
      "language": "Python",
      "created_at": "2022-10-05T01:25:52Z",
      "updated_at": "2023-03-04T11:10:55Z",
      "topics": [],
      "readme": "<p align=\"center\">\n  <a href=\"https://www.deepset.ai/haystack/\"><img src=\"https://raw.githubusercontent.com/deepset-ai/haystack/main/docs/img/haystack_logo_colored.png\" alt=\"Haystack\"></a>\n</p>\n\n<p>\n    <a href=\"https://github.com/deepset-ai/haystack/actions/workflows/tests.yml\">\n        <img alt=\"Tests\" src=\"https://github.com/deepset-ai/haystack/workflows/Tests/badge.svg?branch=main\">\n    </a>\n    <a href=\"https://haystack.deepset.ai/overview/intro\">\n        <img alt=\"Documentation\" src=\"https://img.shields.io/website/http/haystack.deepset.ai/docs/intromd.svg?down_color=red&down_message=offline&up_message=online\">\n    </a>\n    <a href=\"https://github.com/deepset-ai/haystack/releases\">\n        <img alt=\"Release\" src=\"https://img.shields.io/github/release/deepset-ai/haystack\">\n    </a>\n    <a href=\"https://github.com/deepset-ai/haystack/commits/main\">\n        <img alt=\"Last commit\" src=\"https://img.shields.io/github/last-commit/deepset-ai/haystack\">\n    </a>\n    <a href=\"https://pepy.tech/project/farm-haystack\">\n        <img alt=\"Downloads\" src=\"https://pepy.tech/badge/farm-haystack/month\">\n    </a>\n    <a href=\"https://www.deepset.ai/jobs\">\n        <img alt=\"Jobs\" src=\"https://img.shields.io/badge/Jobs-We're%20hiring-blue\">\n    </a>\n        <a href=\"https://twitter.com/intent/follow?screen_name=deepset_ai\">\n        <img alt=\"Twitter\" src=\"https://img.shields.io/twitter/follow/deepset_ai?style=social\">\n    </a>\n</p>\n\nHaystack is an end-to-end framework that enables you to build powerful and production-ready pipelines for different search use cases.\nWhether you want to perform Question Answering or semantic document search, you can use the State-of-the-Art NLP models in Haystack to provide unique search experiences and allow your users to query in natural language.\nHaystack is built in a modular fashion so that you can combine the best technology from other open-source projects like Huggingface's Transformers, Elasticsearch, or Milvus.\n\n<p align=\"center\"><img src=\"https://raw.githubusercontent.com/deepset-ai/haystack/main/docs/img/main_example.gif\"></p>\n\n## What to build with Haystack\n\n- **Ask questions in natural language** and find granular answers in your documents.\n- Perform **semantic search** and retrieve documents according to meaning, not keywords\n- Use **off-the-shelf models** or **fine-tune** them to your domain.\n- Use **user feedback** to evaluate, benchmark, and continuously improve your live models.\n- Leverage existing **knowledge bases** and better handle the long tail of queries that **chatbots** receive.\n- **Automate processes** by automatically applying a list of questions to new documents and using the extracted answers.\n\n## Core Features\n\n- **Latest models**: Utilize all latest transformer-based models (e.g., BERT, RoBERTa, MiniLM) for extractive QA, generative QA, and document retrieval.\n- **Modular**: Multiple choices to fit your tech stack and use case. Pick your favorite database, file converter, or modeling framework.\n- **Pipelines**: The Node and Pipeline design of Haystack allows for custom routing of queries to only the relevant components.\n- **Open**: 100% compatible with HuggingFace's model hub. Tight interfaces to other frameworks (e.g., Transformers, FARM, sentence-transformers)\n- **Scalable**: Scale to millions of docs via retrievers, production-ready backends like Elasticsearch / FAISS, and a fastAPI REST API\n- **End-to-End**: All tooling in one place: file conversion, cleaning, splitting, training, eval, inference, labeling, etc.\n- **Developer friendly**: Easy to debug, extend and modify.\n- **Customizable**: Fine-tune models to your domain or implement your custom DocumentStore.\n- **Continuous Learning**: Collect new training data via user feedback in production & improve your models continuously\n\n|                                                                                               |                                                                                                                                                                                                                                                   |\n| --------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| :ledger: [Docs](https://haystack.deepset.ai/overview/intro)                                   | Overview, Components, Guides, API documentation                                                                                                                                                                                                   |\n| :floppy_disk: [Installation](https://github.com/deepset-ai/haystack#floppy_disk-installation) | How to install Haystack                                                                                                                                                                                                                           |\n| :mortar_board: [Tutorials](https://github.com/deepset-ai/haystack#mortar_board-tutorials)     | See what Haystack can do with our Notebooks & Scripts                                                                                                                                                                                             |\n| :beginner: [Quick Demo](https://github.com/deepset-ai/haystack#beginner-quick-demo)           | Deploy a Haystack application with Docker Compose and a REST API                                                                                                                                                                                  |\n| :vulcan_salute: [Community](https://github.com/deepset-ai/haystack#vulcan_salute-community)   | [Discord](https://haystack.deepset.ai/community/join), [Twitter](https://twitter.com/deepset_ai), [Stack Overflow](https://stackoverflow.com/questions/tagged/haystack), [GitHub Discussions](https://github.com/deepset-ai/haystack/discussions) |\n| :heart: [Contributing](https://github.com/deepset-ai/haystack#heart-contributing)             | We welcome all contributions!                                                                                                                                                                                                                     |\n| :bar_chart: [Benchmarks](https://haystack.deepset.ai/benchmarks/latest)                       | Speed & Accuracy of Retriever, Readers and DocumentStores                                                                                                                                                                                         |\n| :telescope: [Roadmap](https://haystack.deepset.ai/overview/roadmap)                           | Public roadmap of Haystack                                                                                                                                                                                                                        |\n| :newspaper: [Blog](https://medium.com/deepset-ai)                                             | Read our articles on Medium                                                                                                                                                                                                                       |\n| :phone: [Jobs](https://www.deepset.ai/jobs)                                                   | We're hiring! Have a look at our open positions                                                                                                                                                                                                   |\n\n\n## :floppy_disk: Installation\n\n**1. Basic Installation**\n\nYou can install a basic version of Haystack's latest release by using [pip](https://github.com/pypa/pip).\n\n```\n    pip3 install farm-haystack\n```\n\nThis command will install everything needed for basic Pipelines that use an Elasticsearch Document Store.\n\n**2. Full Installation**\n\nIf you plan to be using more advanced features like Milvus, FAISS, Weaviate, OCR or Ray,\nyou will need to install a full version of Haystack.\nThe following command will install the latest version of Haystack from the main branch.\n\n```\ngit clone https://github.com/deepset-ai/haystack.git\ncd haystack\npip install --upgrade pip\npip install -e '.[all]' ## or 'all-gpu' for the GPU-enabled dependencies\n```\n\nIf you cannot upgrade `pip` to version 21.3 or higher, you will need to replace:\n- `'.[all]'` with `'.[sql,only-faiss,only-milvus1,weaviate,graphdb,crawler,preprocessing,ocr,onnx,ray,dev]'`\n- `'.[all-gpu]'` with `'.[sql,only-faiss-gpu,only-milvus1,weaviate,graphdb,crawler,preprocessing,ocr,onnx-gpu,ray,dev]'`\n\nFor an complete list of the dependency groups available, have a look at the `haystack/pyproject.toml` file.\n\nTo install the REST API and UI, run the following from the root directory of the Haystack repo\n\n```\npip install rest_api/\npip install ui/\n```\n\n**3. Installing on Windows**\n\n```\npip install farm-haystack -f https://download.pytorch.org/whl/torch_stable.html\n```\n\n**4. Installing on Apple Silicon (M1)**\n\nM1 Macbooks require some extra dependencies in order to install Haystack.\n\n```\n# some additional dependencies needed on m1 mac\nbrew install postgresql\nbrew install cmake\nbrew install rust\n\n# haystack installation\nGRPC_PYTHON_BUILD_SYSTEM_ZLIB=true pip install git+https://github.com/deepset-ai/haystack.git\n```\n\n**5. Learn More**\n\nSee our [installation guide](https://haystack.deepset.ai/overview/get-started) for more options.\nYou can find out more about our PyPi package on our [PyPi page](https://pypi.org/project/farm-haystack/).\n\n## :mortar_board: Tutorials\n\n![image](https://raw.githubusercontent.com/deepset-ai/haystack/main/docs/img/concepts_haystack_handdrawn.png)\n\nFollow our [introductory tutorial](https://haystack.deepset.ai/tutorials/first-qa-system)\nto setup a question answering system using Python and start performing queries!\nExplore [the rest of our tutorials](https://github.com/deepset-ai/haystack-tutorials)\nto learn how to tweak pipelines, train models and perform evaluation.\n\n## :beginner: Quick Demo\n\n**Hosted**\n\nTry out our hosted [Explore The World](https://haystack-demo.deepset.ai/) live demo here!\nAsk any question on countries or capital cities and let Haystack return the answers to you.\n\n**Local**\n\nStart up a Haystack service via [Docker Compose](https://docs.docker.com/compose/).\nWith this you can begin calling it directly via the REST API or even interact with it using the included Streamlit UI.\n\n<details>\n  <summary>Click here for a step-by-step guide</summary>\n\n**1. Update/install Docker and Docker Compose, then launch Docker**\n\n```\n    apt-get update && apt-get install docker && apt-get install docker-compose\n    service docker start\n```\n\n**2. Clone Haystack repository**\n\n```\n    git clone https://github.com/deepset-ai/haystack.git\n```\n\n**3. Pull images & launch demo app**\n\n```\n    cd haystack\n    docker-compose pull\n    docker-compose up\n\n    # Or on a GPU machine: docker-compose -f docker-compose-gpu.yml up\n```\n\nYou should be able to see the following in your terminal window as part of the log output:\n\n```\n..\nui_1             |   You can now view your Streamlit app in your browser.\n..\nui_1             |   External URL: http://192.168.108.218:8501\n..\nhaystack-api_1   | [2021-01-01 10:21:58 +0000] [17] [INFO] Application startup complete.\n```\n\n**4. Open the Streamlit UI for Haystack by pointing your browser to the \"External URL\" from above.**\n\nYou should see the following:\n\n![image](https://raw.githubusercontent.com/deepset-ai/haystack/main/docs/img/streamlit_ui_screenshot.png)\n\nYou can then try different queries against a pre-defined set of indexed articles related to Game of Thrones.\n\n**Note**: The following containers are started as a part of this demo:\n\n* Haystack API: listens on port 8000\n* DocumentStore (Elasticsearch): listens on port 9200\n* Streamlit UI: listens on port 8501\n\nPlease note that the demo will [publish](https://docs.docker.com/config/containers/container-networking/) the container ports to the outside world. *We suggest that you review the firewall settings depending on your system setup and the security guidelines.*\n\n</details>\n\n## :vulcan_salute: Community\n\nThere is a very vibrant and active community around Haystack which we are regularly interacting with!\nIf you have a feature request or a bug report, feel free to open an [issue in Github](https://github.com/deepset-ai/haystack/issues).\nWe regularly check these and you can expect a quick response.\nIf you'd like to discuss a topic, or get more general advice on how to make Haystack work for your project,\nyou can start a thread in [Github Discussions](https://github.com/deepset-ai/haystack/discussions) or our [Discord channel](https://haystack.deepset.ai/community/join).\nWe also check [Twitter](https://twitter.com/deepset_ai) and [Stack Overflow](https://stackoverflow.com/questions/tagged/haystack).\n\n\n## :heart: Contributing\n\nWe are very open to the community's contributions - be it a quick fix of a typo, or a completely new feature!\nYou don't need to be a Haystack expert to provide meaningful improvements.\nTo learn how to get started, check out our [Contributor Guidelines](https://github.com/deepset-ai/haystack/blob/main/CONTRIBUTING.md) first.\n\nYou can also find instructions to run the tests locally there.\n\nThanks so much to all those who have contributed to our project!\n\n<a href=\"https://github.com/deepset-ai/haystack/graphs/contributors\">\n  <img src=\"https://contrib.rocks/image?repo=deepset-ai/haystack\" />\n</a>\n\n\n## Who uses Haystack\n\nHere's a list of organizations who use Haystack. Don't hesitate to send a PR to let the world know that you use Haystack. Join our growing community!\n\n- [Airbus](https://www.airbus.com/en)\n- [Alcatel-Lucent](https://www.al-enterprise.com/)\n- [BetterUp](https://www.betterup.com/)\n- [Deepset](https://deepset.ai/)\n- [Etalab](https://www.etalab.gouv.fr/)\n- [Infineon](https://www.infineon.com/)\n- [Sooth.ai](https://sooth.ai/)\n"
    },
    {
      "name": "mohamedelnagar1/haystack",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/38164598?s=40&v=4",
      "owner": "mohamedelnagar1",
      "repo_name": "haystack",
      "description": ":mag: Haystack is an open source NLP framework that leverages Transformer models. It enables developers to implement production-ready neural search, question answering, semantic document search and summarization for a wide range of applications.",
      "homepage": "https://deepset.ai/haystack",
      "language": "Python",
      "created_at": "2022-05-04T15:29:17Z",
      "updated_at": "2022-10-19T17:57:15Z",
      "topics": [],
      "readme": "<p align=\"center\">\n  <a href=\"https://www.deepset.ai/haystack/\"><img src=\"https://raw.githubusercontent.com/deepset-ai/haystack/main/docs/img/haystack_logo_colored.png\" alt=\"Haystack\"></a>\n</p>\n\n<p>\n    <a href=\"https://github.com/deepset-ai/haystack/actions/workflows/tests.yml\">\n        <img alt=\"Tests\" src=\"https://github.com/deepset-ai/haystack/workflows/Tests/badge.svg?branch=main\">\n    </a>\n    <a href=\"https://github.com/deepset-ai/haystack/actions/workflows/tutorials_nightly.yml\">\n        <img alt=\"Tutorials\" src=\"https://github.com/deepset-ai/haystack/actions/workflows/tutorials_nightly.yml/badge.svg\">\n    </a>\n    <a href=\"https://haystack.deepset.ai/overview/intro\">\n        <img alt=\"Documentation\" src=\"https://img.shields.io/website/http/haystack.deepset.ai/docs/intromd.svg?down_color=red&down_message=offline&up_message=online\">\n    </a>\n    <a href=\"https://github.com/deepset-ai/haystack/releases\">\n        <img alt=\"Release\" src=\"https://img.shields.io/github/release/deepset-ai/haystack\">\n    </a>\n    <a href=\"https://github.com/deepset-ai/haystack/commits/main\">\n        <img alt=\"Last commit\" src=\"https://img.shields.io/github/last-commit/deepset-ai/haystack\">\n    </a>\n    <a href=\"https://pepy.tech/project/farm-haystack\">\n        <img alt=\"Downloads\" src=\"https://pepy.tech/badge/farm-haystack/month\">\n    </a>\n    <a href=\"https://www.deepset.ai/jobs\">\n        <img alt=\"Jobs\" src=\"https://img.shields.io/badge/Jobs-We're%20hiring-blue\">\n    </a>\n        <a href=\"https://twitter.com/intent/follow?screen_name=deepset_ai\">\n        <img alt=\"Twitter\" src=\"https://img.shields.io/twitter/follow/deepset_ai?style=social\">\n    </a>\n</p>\n\nHaystack is an end-to-end framework that enables you to build powerful and production-ready pipelines for different search use cases.\nWhether you want to perform Question Answering or semantic document search, you can use the State-of-the-Art NLP models in Haystack to provide unique search experiences and allow your users to query in natural language.\nHaystack is built in a modular fashion so that you can combine the best technology from other open-source projects like Huggingface's Transformers, Elasticsearch, or Milvus.\n\n<p align=\"center\"><img src=\"https://raw.githubusercontent.com/deepset-ai/haystack/main/docs/img/main_example.gif\"></p>\n\n## What to build with Haystack\n\n- **Ask questions in natural language** and find granular answers in your documents.\n- Perform **semantic search** and retrieve documents according to meaning, not keywords\n- Use **off-the-shelf models** or **fine-tune** them to your domain.\n- Use **user feedback** to evaluate, benchmark, and continuously improve your live models.\n- Leverage existing **knowledge bases** and better handle the long tail of queries that **chatbots** receive.\n- **Automate processes** by automatically applying a list of questions to new documents and using the extracted answers.\n\n## Core Features\n\n- **Latest models**: Utilize all latest transformer-based models (e.g., BERT, RoBERTa, MiniLM) for extractive QA, generative QA, and document retrieval.\n- **Modular**: Multiple choices to fit your tech stack and use case. Pick your favorite database, file converter, or modeling framework.\n- **Pipelines**: The Node and Pipeline design of Haystack allows for custom routing of queries to only the relevant components.\n- **Open**: 100% compatible with HuggingFace's model hub. Tight interfaces to other frameworks (e.g., Transformers, FARM, sentence-transformers)\n- **Scalable**: Scale to millions of docs via retrievers, production-ready backends like Elasticsearch / FAISS, and a fastAPI REST API\n- **End-to-End**: All tooling in one place: file conversion, cleaning, splitting, training, eval, inference, labeling, etc.\n- **Developer friendly**: Easy to debug, extend and modify.\n- **Customizable**: Fine-tune models to your domain or implement your custom DocumentStore.\n- **Continuous Learning**: Collect new training data via user feedback in production & improve your models continuously\n\n|  |  |\n|-|-|\n| :ledger: [Docs](https://haystack.deepset.ai/overview/intro) | Overview, Components, Guides, API documentation|\n| :floppy_disk: [Installation](https://github.com/deepset-ai/haystack#floppy_disk-installation) | How to install Haystack |\n| :mortar_board: [Tutorials](https://github.com/deepset-ai/haystack#mortar_board-tutorials) | See what Haystack can do with our Notebooks & Scripts |\n| :beginner: [Quick Demo](https://github.com/deepset-ai/haystack#beginner-quick-demo) | Deploy a Haystack application with Docker Compose and a REST API |\n| :vulcan_salute: [Community](https://github.com/deepset-ai/haystack#vulcan_salute-community) | [Discord](https://haystack.deepset.ai/community/join), [Twitter](https://twitter.com/deepset_ai), [Stack Overflow](https://stackoverflow.com/questions/tagged/haystack), [GitHub Discussions](https://github.com/deepset-ai/haystack/discussions) |\n| :heart: [Contributing](https://github.com/deepset-ai/haystack#heart-contributing) | We welcome all contributions! |\n| :bar_chart: [Benchmarks](https://haystack.deepset.ai/benchmarks/latest) | Speed & Accuracy of Retriever, Readers and DocumentStores |\n| :telescope: [Roadmap](https://haystack.deepset.ai/overview/roadmap) | Public roadmap of Haystack |\n| :newspaper: [Blog](https://medium.com/deepset-ai) | Read our articles on Medium |\n| :phone: [Jobs](https://www.deepset.ai/jobs) | We're hiring! Have a look at our open positions |\n\n\n## :floppy_disk: Installation\n\n**1. Basic Installation**\n\nYou can install a basic version of Haystack's latest release by using [pip](https://github.com/pypa/pip).\n\n```\n    pip3 install farm-haystack\n```\n\nThis command will install everything needed for basic Pipelines that use an Elasticsearch Document Store.\n\n**2. Full Installation**\n\nIf you plan to be using more advanced features like Milvus, FAISS, Weaviate, OCR or Ray,\nyou will need to install a full version of Haystack.\nThe following command will install the latest version of Haystack from the main branch.\n\n```\ngit clone https://github.com/deepset-ai/haystack.git\ncd haystack\npip install --upgrade pip\npip install -e '.[all]' ## or 'all-gpu' for the GPU-enabled dependencies\n```\n\nIf you cannot upgrade `pip` to version 21.3 or higher, you will need to replace:\n- `'.[all]'` with `'.[sql,only-faiss,only-milvus1,weaviate,graphdb,crawler,preprocessing,ocr,onnx,ray,dev]'`\n- `'.[all-gpu]'` with `'.[sql,only-faiss-gpu,only-milvus1,weaviate,graphdb,crawler,preprocessing,ocr,onnx-gpu,ray,dev]'`\n\nFor an complete list of the dependency groups available, have a look at the `haystack/pyproject.toml` file.\n\nTo install the REST API and UI, run the following from the root directory of the Haystack repo\n\n```\npip install rest_api/\npip install ui/\n```\n\n**3. Installing on Windows**\n\n```\npip install farm-haystack -f https://download.pytorch.org/whl/torch_stable.html\n```\n\n**4. Installing on Apple Silicon (M1)**\n\nM1 Macbooks require some extra dependencies in order to install Haystack.\n\n```\n# some additional dependencies needed on m1 mac\nbrew install postgresql\nbrew install cmake\nbrew install rust\n\n# haystack installation\nGRPC_PYTHON_BUILD_SYSTEM_ZLIB=true pip install git+https://github.com/deepset-ai/haystack.git\n```\n\n**5. Learn More**\n\nSee our [installation guide](https://haystack.deepset.ai/overview/get-started) for more options.\nYou can find out more about our PyPi package on our [PyPi page](https://pypi.org/project/farm-haystack/).\n\n## :mortar_board: Tutorials\n\n![image](https://raw.githubusercontent.com/deepset-ai/haystack/main/docs/img/concepts_haystack_handdrawn.png)\n\nFollow our [introductory tutorial](https://haystack.deepset.ai/tutorials/first-qa-system)\nto setup a question answering system using Python and start performing queries!\nExplore the rest of our tutorials to learn how to tweak pipelines, train models and perform evaluation.\n\n- Tutorial 1 - Basic QA Pipeline: [Jupyter notebook](https://github.com/deepset-ai/haystack/blob/main/tutorials/Tutorial1_Basic_QA_Pipeline.ipynb)\n    |\n    [Colab](https://colab.research.google.com/github/deepset-ai/haystack/blob/main/tutorials/Tutorial1_Basic_QA_Pipeline.ipynb)\n    |\n    [Python](https://github.com/deepset-ai/haystack/blob/main/tutorials/Tutorial1_Basic_QA_Pipeline.py)\n- Tutorial 2 - Fine-tuning a model on own data: [Jupyter notebook](https://github.com/deepset-ai/haystack/blob/main/tutorials/Tutorial2_Finetune_a_model_on_your_data.ipynb)\n    |\n    [Colab](https://colab.research.google.com/github/deepset-ai/haystack/blob/main/tutorials/Tutorial2_Finetune_a_model_on_your_data.ipynb)\n    |\n    [Python](https://github.com/deepset-ai/haystack/blob/main/tutorials/Tutorial2_Finetune_a_model_on_your_data.py)\n- Tutorial 3 - Basic QA Pipeline without Elasticsearch: [Jupyter notebook](https://github.com/deepset-ai/haystack/blob/main/tutorials/Tutorial3_Basic_QA_Pipeline_without_Elasticsearch.ipynb)\n    |\n    [Colab](https://colab.research.google.com/github/deepset-ai/haystack/blob/main/tutorials/Tutorial3_Basic_QA_Pipeline_without_Elasticsearch.ipynb)\n    |\n    [Python](https://github.com/deepset-ai/haystack/blob/main/tutorials/Tutorial3_Basic_QA_Pipeline_without_Elasticsearch.py)\n- Tutorial 4 - FAQ-style QA: [Jupyter notebook](https://github.com/deepset-ai/haystack/blob/main/tutorials/Tutorial4_FAQ_style_QA.ipynb)\n    |\n    [Colab](https://colab.research.google.com/github/deepset-ai/haystack/blob/main/tutorials/Tutorial4_FAQ_style_QA.ipynb)\n    |\n    [Python](https://github.com/deepset-ai/haystack/blob/main/tutorials/Tutorial4_FAQ_style_QA.py)\n- Tutorial 5 - Evaluation of the whole QA-Pipeline: [Jupyter noteboook](https://github.com/deepset-ai/haystack/blob/main/tutorials/Tutorial5_Evaluation.ipynb)\n    |\n    [Colab](https://colab.research.google.com/github/deepset-ai/haystack/blob/main/tutorials/Tutorial5_Evaluation.ipynb)\n    |\n    [Python](https://github.com/deepset-ai/haystack/blob/main/tutorials/Tutorial5_Evaluation.py)\n- Tutorial 6 - Better Retrievers via \"Dense Passage Retrieval\":\n    [Jupyter noteboook](https://github.com/deepset-ai/haystack/blob/main/tutorials/Tutorial6_Better_Retrieval_via_DPR.ipynb)\n    |\n    [Colab](https://colab.research.google.com/github/deepset-ai/haystack/blob/main/tutorials/Tutorial6_Better_Retrieval_via_DPR.ipynb)\n    |\n    [Python](https://github.com/deepset-ai/haystack/blob/main/tutorials/Tutorial6_Better_Retrieval_via_DPR.py)\n- Tutorial 7 - Generative QA via \"Retrieval-Augmented Generation\":\n    [Jupyter noteboook](https://github.com/deepset-ai/haystack/blob/main/tutorials/Tutorial7_RAG_Generator.ipynb)\n    |\n    [Colab](https://colab.research.google.com/github/deepset-ai/haystack/blob/main/tutorials/Tutorial7_RAG_Generator.ipynb)\n    |\n    [Python](https://github.com/deepset-ai/haystack/blob/main/tutorials/Tutorial7_RAG_Generator.py)\n- Tutorial 8 - Preprocessing:\n    [Jupyter noteboook](https://github.com/deepset-ai/haystack/blob/main/tutorials/Tutorial8_Preprocessing.ipynb)\n    |\n    [Colab](https://colab.research.google.com/github/deepset-ai/haystack/blob/main/tutorials/Tutorial8_Preprocessing.ipynb)\n    |\n    [Python](https://github.com/deepset-ai/haystack/blob/main/tutorials/Tutorial8_Preprocessing.py)\n- Tutorial 9 - DPR Training:\n    [Jupyter noteboook](https://github.com/deepset-ai/haystack/blob/main/tutorials/Tutorial9_DPR_training.ipynb)\n    |\n    [Colab](https://colab.research.google.com/github/deepset-ai/haystack/blob/main/tutorials/Tutorial9_DPR_training.ipynb)\n    |\n    [Python](https://github.com/deepset-ai/haystack/blob/main/tutorials/Tutorial9_DPR_training.py)\n- Tutorial 10 - Knowledge Graph:\n    [Jupyter noteboook](https://github.com/deepset-ai/haystack/blob/main/tutorials/Tutorial10_Knowledge_Graph.ipynb)\n    |\n    [Colab](https://colab.research.google.com/github/deepset-ai/haystack/blob/main/tutorials/Tutorial10_Knowledge_Graph.ipynb)\n    |\n    [Python](https://github.com/deepset-ai/haystack/blob/main/tutorials/Tutorial10_Knowledge_Graph.py)\n- Tutorial 11 - Pipelines:\n    [Jupyter noteboook](https://github.com/deepset-ai/haystack/blob/main/tutorials/Tutorial11_Pipelines.ipynb)\n    |\n    [Colab](https://colab.research.google.com/github/deepset-ai/haystack/blob/main/tutorials/Tutorial11_Pipelines.ipynb)\n    |\n    [Python](https://github.com/deepset-ai/haystack/blob/main/tutorials/Tutorial11_Pipelines.py)\n- Tutorial 12 - Long-Form Question Answering:\n    [Jupyter noteboook](https://github.com/deepset-ai/haystack/blob/main/tutorials/Tutorial12_LFQA.ipynb)\n    |\n    [Colab](https://colab.research.google.com/github/deepset-ai/haystack/blob/main/tutorials/Tutorial12_LFQA.ipynb)\n    |\n    [Python](https://github.com/deepset-ai/haystack/blob/main/tutorials/Tutorial12_LFQA.py)\n- Tutorial 13 - Question Generation:\n    [Jupyter noteboook](https://github.com/deepset-ai/haystack/blob/main/tutorials/Tutorial13_Question_generation.ipynb)\n    |\n    [Colab](https://colab.research.google.com/github/deepset-ai/haystack/blob/main/tutorials/Tutorial13_Question_generation.ipynb)\n    |\n    [Python](https://github.com/deepset-ai/haystack/blob/main/tutorials/Tutorial13_Question_generation.py)\n- Tutorial 14 - Query Classifier:\n    [Jupyter noteboook](https://github.com/deepset-ai/haystack/blob/main/tutorials/Tutorial14_Query_Classifier.ipynb)\n    |\n    [Colab](https://colab.research.google.com/github/deepset-ai/haystack/blob/main/tutorials/Tutorial14_Query_Classifier.ipynb)\n    |\n    [Python](https://github.com/deepset-ai/haystack/blob/main/tutorials/Tutorial14_Query_Classifier.py)\n- Tutorial 15 - TableQA:\n    [Jupyter noteboook](https://github.com/deepset-ai/haystack/blob/main/tutorials/Tutorial15_TableQA.ipynb)\n    |\n    [Colab](https://colab.research.google.com/github/deepset-ai/haystack/blob/main/tutorials/Tutorial15_TableQA.ipynb)\n    |\n    [Python](https://github.com/deepset-ai/haystack/blob/main/tutorials/Tutorial15_TableQA.py)\n- Tutorial 16 - Document Classification at Indexing Time:\n    [Jupyter noteboook](https://github.com/deepset-ai/haystack/blob/main/tutorials/Tutorial16_Document_Classifier_at_Index_Time.ipynb)\n    |\n    [Colab](https://colab.research.google.com/github/deepset-ai/haystack/blob/main/tutorials/Tutorial16_Document_Classifier_at_Index_Time.ipynb)\n    |\n    [Python](https://github.com/deepset-ai/haystack/blob/main/tutorials/Tutorial16_Document_Classifier_at_Index_Time.py)\n- Tutorial 17 - Answers & Documents to Speech:\n    [Jupyter noteboook](https://github.com/deepset-ai/haystack/blob/main/tutorials/Tutorial17_Audio.ipynb)\n    |\n    [Colab](https://colab.research.google.com/github/deepset-ai/haystack/blob/main/tutorials/Tutorial17_Audio.ipynb)\n    |\n    [Python](https://github.com/deepset-ai/haystack/blob/main/tutorials/Tutorial17_Audio.py)\n- Tutorial 18 - Generative Pseudo Labeling:\n    [Jupyter noteboook](https://github.com/deepset-ai/haystack/blob/main/tutorials/Tutorial18_GPL.ipynb)\n    |\n    [Colab](https://colab.research.google.com/github/deepset-ai/haystack/blob/main/tutorials/Tutorial18_GPL.ipynb)\n    |\n    [Python](https://github.com/deepset-ai/haystack/blob/main/tutorials/Tutorial18_GPL.py)\n\n\n## :beginner: Quick Demo\n\n**Hosted**\n\nTry out our hosted [Explore The World](https://haystack-demo.deepset.ai/) live demo here!\nAsk any question on countries or capital cities and let Haystack return the answers to you.\n\n**Local**\n\nStart up a Haystack service via [Docker Compose](https://docs.docker.com/compose/).\nWith this you can begin calling it directly via the REST API or even interact with it using the included Streamlit UI.\n\n<details>\n  <summary>Click here for a step-by-step guide</summary>\n\n**1. Update/install Docker and Docker Compose, then launch Docker**\n\n```\n    apt-get update && apt-get install docker && apt-get install docker-compose\n    service docker start\n```\n\n**2. Clone Haystack repository**\n\n```\n    git clone https://github.com/deepset-ai/haystack.git\n```\n\n**3. Pull images & launch demo app**\n\n```\n    cd haystack\n    docker-compose pull\n    docker-compose up\n\n    # Or on a GPU machine: docker-compose -f docker-compose-gpu.yml up\n```\n\nYou should be able to see the following in your terminal window as part of the log output:\n\n```\n..\nui_1             |   You can now view your Streamlit app in your browser.\n..\nui_1             |   External URL: http://192.168.108.218:8501\n..\nhaystack-api_1   | [2021-01-01 10:21:58 +0000] [17] [INFO] Application startup complete.\n```\n\n**4. Open the Streamlit UI for Haystack by pointing your browser to the \"External URL\" from above.**\n\nYou should see the following:\n\n![image](https://raw.githubusercontent.com/deepset-ai/haystack/main/docs/img/streamlit_ui_screenshot.png)\n\nYou can then try different queries against a pre-defined set of indexed articles related to Game of Thrones.\n\n**Note**: The following containers are started as a part of this demo:\n\n* Haystack API: listens on port 8000\n* DocumentStore (Elasticsearch): listens on port 9200\n* Streamlit UI: listens on port 8501\n\nPlease note that the demo will [publish](https://docs.docker.com/config/containers/container-networking/) the container ports to the outside world. *We suggest that you review the firewall settings depending on your system setup and the security guidelines.*\n\n</details>\n\n## :vulcan_salute: Community\n\nThere is a very vibrant and active community around Haystack which we are regularly interacting with!\nIf you have a feature request or a bug report, feel free to open an [issue in Github](https://github.com/deepset-ai/haystack/issues).\nWe regularly check these and you can expect a quick response.\nIf you'd like to discuss a topic, or get more general advice on how to make Haystack work for your project,\nyou can start a thread in [Github Discussions](https://github.com/deepset-ai/haystack/discussions) or our [Discord channel](https://haystack.deepset.ai/community/join).\nWe also check [Twitter](https://twitter.com/deepset_ai) and [Stack Overflow](https://stackoverflow.com/questions/tagged/haystack).\n\n\n## :heart: Contributing\n\nWe are very open to the community's contributions - be it a quick fix of a typo, or a completely new feature! \nYou don't need to be a Haystack expert to provide meaningful improvements. \nTo learn how to get started, check out our [Contributor Guidelines](https://github.com/deepset-ai/haystack/blob/main/CONTRIBUTING.md) first.\n\nYou can also find instructions to run the tests locally there.\n\nThanks so much to all those who have contributed to our project!\n\n<a href=\"https://github.com/deepset-ai/haystack/graphs/contributors\">\n  <img src=\"https://contrib.rocks/image?repo=deepset-ai/haystack\" />\n</a>\n\n\n## Who uses Haystack\n\nHere's a list of organizations who use Haystack. Don't hesitate to send a PR to let the world know that you use Haystack. Join our growing community!\n\n- [Airbus](https://www.airbus.com/en)\n- [Alcatel-Lucent](https://www.al-enterprise.com/)\n- [BetterUp](https://www.betterup.com/)\n- [Deepset](https://deepset.ai/)\n- [Etalab](https://www.etalab.gouv.fr/)\n- [Infineon](https://www.infineon.com/)\n- [Sooth.ai](https://sooth.ai/)\n"
    },
    {
      "name": "jamescalam/haystack",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/35938317?s=40&v=4",
      "owner": "jamescalam",
      "repo_name": "haystack",
      "description": ":mag: Haystack is an open source NLP framework that leverages Transformer models. It enables developers to implement production-ready neural search, question answering, semantic document search and summarization for a wide range of applications.",
      "homepage": "https://deepset.ai/haystack",
      "language": "Python",
      "created_at": "2022-05-03T21:28:10Z",
      "updated_at": "2022-09-23T20:33:07Z",
      "topics": [],
      "readme": "<p align=\"center\">\n  <a href=\"https://www.deepset.ai/haystack/\"><img src=\"https://raw.githubusercontent.com/deepset-ai/haystack/main/docs/img/haystack_logo_colored.png\" alt=\"Haystack\"></a>\n</p>\n\n<p>\n    <a href=\"https://github.com/deepset-ai/haystack/actions/workflows/tests.yml\">\n        <img alt=\"Tests\" src=\"https://github.com/deepset-ai/haystack/workflows/Tests/badge.svg?branch=main\">\n    </a>\n    <a href=\"https://github.com/deepset-ai/haystack-json-schema/actions/workflows/schemas.yml\">\n        <img alt=\"Schemas\" src=\"https://github.com/deepset-ai/haystack-json-schema/actions/workflows/schemas.yml/badge.svg\">\n    </a>\n    <a href=\"https://docs.haystack.deepset.ai\">\n        <img alt=\"Documentation\" src=\"https://img.shields.io/website?label=documentation&up_message=online&url=https%3A%2F%2Fdocs.haystack.deepset.ai\">\n    </a>\n    <a href=\"https://github.com/deepset-ai/haystack/releases\">\n        <img alt=\"Release\" src=\"https://img.shields.io/github/release/deepset-ai/haystack\">\n    </a>\n    <a href=\"https://github.com/deepset-ai/haystack/commits/main\">\n        <img alt=\"Last commit\" src=\"https://img.shields.io/github/last-commit/deepset-ai/haystack\">\n    </a>\n    <a href=\"https://pepy.tech/project/farm-haystack\">\n        <img alt=\"Downloads\" src=\"https://pepy.tech/badge/farm-haystack/month\">\n    </a>\n    <a href=\"https://www.deepset.ai/jobs\">\n        <img alt=\"Jobs\" src=\"https://img.shields.io/badge/Jobs-We're%20hiring-blue\">\n    </a>\n        <a href=\"https://twitter.com/intent/follow?screen_name=deepset_ai\">\n        <img alt=\"Twitter\" src=\"https://img.shields.io/twitter/follow/deepset_ai?style=social\">\n    </a>\n</p>\n\n[Haystack](https://haystack.deepset.ai) is an end-to-end framework that enables you to build powerful and production-ready pipelines for different search use cases.\nWhether you want to perform Question Answering or semantic document search, you can use the State-of-the-Art NLP models in Haystack to provide unique search experiences and allow your users to query in natural language.\nHaystack is built in a modular fashion so that you can combine the best technology from other open-source projects like Huggingface's Transformers, Elasticsearch, or Milvus.\n\n<p align=\"center\"><img src=\"https://raw.githubusercontent.com/deepset-ai/haystack/main/docs/img/main_example.gif\"></p>\n\n## What to build with Haystack\n\n- **Ask questions in natural language** and find granular answers in your documents.\n- Perform **semantic search** and retrieve documents according to meaning, not keywords\n- Use **off-the-shelf models** or **fine-tune** them to your domain.\n- Use **user feedback** to evaluate, benchmark, and continuously improve your live models.\n- Leverage existing **knowledge bases** and better handle the long tail of queries that **chatbots** receive.\n- **Automate processes** by automatically applying a list of questions to new documents and using the extracted answers.\n\n## Core Features\n\n- **Latest models**: Utilize all latest transformer-based models (e.g., BERT, RoBERTa, MiniLM) for extractive QA, generative QA, and document retrieval.\n- **Modular**: Multiple choices to fit your tech stack and use case. Pick your favorite database, file converter, or modeling framework.\n- **Pipelines**: The Node and Pipeline design of Haystack allows for custom routing of queries to only the relevant components.\n- **Open**: 100% compatible with HuggingFace's model hub. Tight interfaces to other frameworks (e.g., Transformers, FARM, sentence-transformers)\n- **Scalable**: Scale to millions of docs via retrievers, production-ready backends like Elasticsearch / FAISS, and a fastAPI REST API\n- **End-to-End**: All tooling in one place: file conversion, cleaning, splitting, training, eval, inference, labeling, etc.\n- **Developer friendly**: Easy to debug, extend and modify.\n- **Customizable**: Fine-tune models to your domain or implement your custom DocumentStore.\n- **Continuous Learning**: Collect new training data via user feedback in production & improve your models continuously\n\n|                                                                                               |                                                                                                                                                                                                                                                   |\n| --------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| :ledger: [Docs](https://docs.haystack.deepset.ai)                                             | Components, Pipeline Nodes, Guides, API Reference                                                                                                                                                                                                 |\n| :floppy_disk: [Installation](https://github.com/deepset-ai/haystack#floppy_disk-installation) | How to install Haystack                                                                                                                                                                                                                           |\n| :mortar_board: [Tutorials](https://github.com/deepset-ai/haystack#mortar_board-tutorials)     | See what Haystack can do with our Notebooks & Scripts                                                                                                                                                                                             |\n| :beginner: [Quick Demo](https://github.com/deepset-ai/haystack#beginner-quick-demo)           | Deploy a Haystack application with Docker Compose and a REST API                                                                                                                                                                                  |\n| :vulcan_salute: [Community](https://github.com/deepset-ai/haystack#vulcan_salute-community)   | [Discord](https://haystack.deepset.ai/community/join), [Twitter](https://twitter.com/deepset_ai), [Stack Overflow](https://stackoverflow.com/questions/tagged/haystack), [GitHub Discussions](https://github.com/deepset-ai/haystack/discussions) |\n| :heart: [Contributing](https://github.com/deepset-ai/haystack#heart-contributing)             | We welcome all contributions!                                                                                                                                                                                                                     |\n| :bar_chart: [Benchmarks](https://haystack.deepset.ai/benchmarks/)                             | Speed & Accuracy of Retriever, Readers and DocumentStores                                                                                                                                                                                         |\n| :telescope: [Roadmap](https://haystack.deepset.ai/overview/roadmap)                           | Public roadmap of Haystack                                                                                                                                                                                                                        |\n| :newspaper: [Blog](https://medium.com/deepset-ai)                                             | Read our articles on Medium                                                                                                                                                                                                                       |\n| :phone: [Jobs](https://www.deepset.ai/jobs)                                                   | We're hiring! Have a look at our open positions                                                                                                                                                                                                   |\n\n\n## :floppy_disk: Installation\n\n**1. Basic Installation**\n\nYou can install a basic version of Haystack's latest release by using [pip](https://github.com/pypa/pip).\n\n```\n    pip3 install farm-haystack\n```\n\nThis command will install everything needed for basic Pipelines that use an Elasticsearch Document Store.\n\n**2. Full Installation**\n\nIf you plan to be using more advanced features like Milvus, FAISS, Weaviate, OCR or Ray,\nyou will need to install a full version of Haystack.\nThe following command will install the latest version of Haystack from the main branch.\n\n```\ngit clone https://github.com/deepset-ai/haystack.git\ncd haystack\npip install --upgrade pip\npip install -e '.[all]' ## or 'all-gpu' for the GPU-enabled dependencies\n```\n\nIf you cannot upgrade `pip` to version 21.3 or higher, you will need to replace:\n- `'.[all]'` with `'.[sql,only-faiss,only-milvus,weaviate,graphdb,crawler,preprocessing,ocr,onnx,ray,dev]'`\n- `'.[all-gpu]'` with `'.[sql,only-faiss-gpu,only-milvus,weaviate,graphdb,crawler,preprocessing,ocr,onnx-gpu,ray,dev]'`\n\nFor an complete list of the dependency groups available, have a look at the `haystack/pyproject.toml` file.\n\nTo install the REST API and UI, run the following from the root directory of the Haystack repo\n\n```\npip install rest_api/\npip install ui/\n```\n\n**3. Installing on Windows**\n\n```\npip install farm-haystack -f https://download.pytorch.org/whl/torch_stable.html\n```\n\n**4. Installing on Apple Silicon (M1)**\n\nM1 Macbooks require some extra dependencies in order to install Haystack.\n\n```\n# some additional dependencies needed on m1 mac\nbrew install postgresql\nbrew install cmake\nbrew install rust\n\n# haystack installation\nGRPC_PYTHON_BUILD_SYSTEM_ZLIB=true pip install git+https://github.com/deepset-ai/haystack.git\n```\n\n**5. Learn More**\n\nSee our [installation guide](https://haystack.deepset.ai/overview/quick-start) for more options.\nYou can find out more about our PyPi package on our [PyPi page](https://pypi.org/project/farm-haystack/).\n\n## :mortar_board: Tutorials\n\n![image](https://raw.githubusercontent.com/deepset-ai/haystack/main/docs/img/concepts_haystack_handdrawn.png)\n\nFollow our [introductory tutorial](https://haystack.deepset.ai/tutorials/first-qa-system)\nto setup a question answering system using Python and start performing queries!\nExplore [the rest of our tutorials](https://haystack.deepset.ai/tutorials)\nto learn how to tweak pipelines, train models and perform evaluation.\n\n## :beginner: Quick Demo\n\n**Hosted**\n\nTry out our hosted [Explore The World](https://haystack-demo.deepset.ai/) live demo here!\nAsk any question on countries or capital cities and let Haystack return the answers to you.\n\n**Local**\n\nStart up a Haystack service via [Docker Compose](https://docs.docker.com/compose/).\nWith this you can begin calling it directly via the REST API or even interact with it using the included Streamlit UI.\n\n<details>\n  <summary>Click here for a step-by-step guide</summary>\n\n**1. Update/install Docker and Docker Compose, then launch Docker**\n\n```\n    apt-get update && apt-get install docker && apt-get install docker-compose\n    service docker start\n```\n\n**2. Clone Haystack repository**\n\n```\n    git clone https://github.com/deepset-ai/haystack.git\n```\n\n**3. Pull images & launch demo app**\n\n```\n    cd haystack\n    docker-compose pull\n    docker-compose up\n\n    # Or on a GPU machine: docker-compose -f docker-compose-gpu.yml up\n```\n\nYou should be able to see the following in your terminal window as part of the log output:\n\n```\n..\nui_1             |   You can now view your Streamlit app in your browser.\n..\nui_1             |   External URL: http://192.168.108.218:8501\n..\nhaystack-api_1   | [2021-01-01 10:21:58 +0000] [17] [INFO] Application startup complete.\n```\n\n**4. Open the Streamlit UI for Haystack by pointing your browser to the \"External URL\" from above.**\n\nYou should see the following:\n\n![image](https://raw.githubusercontent.com/deepset-ai/haystack/main/docs/img/streamlit_ui_screenshot.png)\n\nYou can then try different queries against a pre-defined set of indexed articles related to Game of Thrones.\n\n**Note**: The following containers are started as a part of this demo:\n\n* Haystack API: listens on port 8000\n* DocumentStore (Elasticsearch): listens on port 9200\n* Streamlit UI: listens on port 8501\n\nPlease note that the demo will [publish](https://docs.docker.com/config/containers/container-networking/) the container ports to the outside world. *We suggest that you review the firewall settings depending on your system setup and the security guidelines.*\n\n</details>\n\n## :vulcan_salute: Community\n\nThere is a very vibrant and active community around Haystack which we are regularly interacting with!\nIf you have a feature request or a bug report, feel free to open an [issue in Github](https://github.com/deepset-ai/haystack/issues).\nWe regularly check these and you can expect a quick response.\nIf you'd like to discuss a topic, or get more general advice on how to make Haystack work for your project,\nyou can start a thread in [Github Discussions](https://github.com/deepset-ai/haystack/discussions) or our [Discord channel](https://haystack.deepset.ai/community).\nWe also check [Twitter](https://twitter.com/deepset_ai) and [Stack Overflow](https://stackoverflow.com/questions/tagged/haystack).\n\n\n## :heart: Contributing\n\nWe are very open to the community's contributions - be it a quick fix of a typo, or a completely new feature!\nYou don't need to be a Haystack expert to provide meaningful improvements.\nTo learn how to get started, check out our [Contributor Guidelines](https://github.com/deepset-ai/haystack/blob/main/CONTRIBUTING.md) first.\n\nYou can also find instructions to run the tests locally there.\n\nThanks so much to all those who have contributed to our project!\n\n<a href=\"https://github.com/deepset-ai/haystack/graphs/contributors\">\n  <img src=\"https://contrib.rocks/image?repo=deepset-ai/haystack\" />\n</a>\n\n\n## Who uses Haystack\n\nHere's a list of organizations who use Haystack. Don't hesitate to send a PR to let the world know that you use Haystack. Join our growing community!\n\n- [Airbus](https://www.airbus.com/en)\n- [Alcatel-Lucent](https://www.al-enterprise.com/)\n- [BetterUp](https://www.betterup.com/)\n- [Deepset](https://deepset.ai/)\n- [Etalab](https://www.etalab.gouv.fr/)\n- [Infineon](https://www.infineon.com/)\n- [Sooth.ai](https://sooth.ai/)\n"
    },
    {
      "name": "davgit/haystack",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/294138?s=40&v=4",
      "owner": "davgit",
      "repo_name": "haystack",
      "description": ":mag: Haystack is an open source NLP framework that leverages Transformer models. It enables developers to implement production-ready neural search, question answering, semantic document search and summarization for a wide range of applications.",
      "homepage": "https://deepset.ai/haystack",
      "language": "Python",
      "created_at": "2022-05-02T18:19:24Z",
      "updated_at": "2022-05-20T19:27:55Z",
      "topics": [],
      "readme": "<p align=\"center\">\n  <a href=\"https://www.deepset.ai/haystack/\"><img src=\"https://raw.githubusercontent.com/deepset-ai/haystack/main/docs/img/haystack_logo_colored.png\" alt=\"Haystack\"></a>\n</p>\n\n<p>\n    <a href=\"https://github.com/deepset-ai/haystack/actions/workflows/tests.yml\">\n        <img alt=\"Tests\" src=\"https://github.com/deepset-ai/haystack/workflows/Tests/badge.svg?branch=main\">\n    </a>\n    <a href=\"https://github.com/deepset-ai/haystack-json-schema/actions/workflows/schemas.yml\">\n        <img alt=\"Schemas\" src=\"https://github.com/deepset-ai/haystack-json-schema/actions/workflows/schemas.yml/badge.svg\">\n    </a>\n    <a href=\"https://docs.haystack.deepset.ai\">\n        <img alt=\"Documentation\" src=\"https://img.shields.io/website?label=documentation&up_message=online&url=https%3A%2F%2Fdocs.haystack.deepset.ai\">\n    </a>\n    <a href=\"https://app.fossa.com/projects/custom%2B24445%2Fgithub.com%2Fdeepset-ai%2Fhaystack?ref=badge_shield\">\n        <img alt=\"FOSSA Status\" src=\"https://app.fossa.com/api/projects/custom%2B24445%2Fgithub.com%2Fdeepset-ai%2Fhaystack.svg?type=shield\"/>\n    </a>\n    <a href=\"https://github.com/deepset-ai/haystack/releases\">\n        <img alt=\"Release\" src=\"https://img.shields.io/github/release/deepset-ai/haystack\">\n    </a>\n    <a href=\"https://github.com/deepset-ai/haystack/commits/main\">\n        <img alt=\"Last commit\" src=\"https://img.shields.io/github/last-commit/deepset-ai/haystack\">\n    </a>\n    <a href=\"https://pepy.tech/project/farm-haystack\">\n        <img alt=\"Downloads\" src=\"https://pepy.tech/badge/farm-haystack/month\">\n    </a>\n    <a href=\"https://www.deepset.ai/jobs\">\n        <img alt=\"Jobs\" src=\"https://img.shields.io/badge/Jobs-We're%20hiring-blue\">\n    </a>\n        <a href=\"https://twitter.com/intent/follow?screen_name=deepset_ai\">\n        <img alt=\"Twitter\" src=\"https://img.shields.io/twitter/follow/deepset_ai?style=social\">\n    </a>\n</p>\n\n[Haystack](https://haystack.deepset.ai) is an end-to-end framework that enables you to build powerful and production-ready pipelines for different search use cases.\nWhether you want to perform Question Answering or semantic document search, you can use the State-of-the-Art NLP models in Haystack to provide unique search experiences and allow your users to query in natural language.\nHaystack is built in a modular fashion so that you can combine the best technology from other open-source projects like Huggingface's Transformers, Elasticsearch, or Milvus.\n\n<p align=\"center\"><img src=\"https://raw.githubusercontent.com/deepset-ai/haystack/main/docs/img/main_example.gif\"></p>\n\n## What to build with Haystack\n\n- **Ask questions in natural language** and find granular answers in your documents.\n- Perform **semantic search** and retrieve documents according to meaning, not keywords\n- Use **off-the-shelf models** or **fine-tune** them to your domain.\n- Use **user feedback** to evaluate, benchmark, and continuously improve your live models.\n- Leverage existing **knowledge bases** and better handle the long tail of queries that **chatbots** receive.\n- **Automate processes** by automatically applying a list of questions to new documents and using the extracted answers.\n\n## Core Features\n\n- **Latest models**: Utilize all latest transformer-based models (e.g., BERT, RoBERTa, MiniLM) for extractive QA, generative QA, and document retrieval.\n- **Modular**: Multiple choices to fit your tech stack and use case. Pick your favorite database, file converter, or modeling framework.\n- **Pipelines**: The Node and Pipeline design of Haystack allows for custom routing of queries to only the relevant components.\n- **Open**: 100% compatible with HuggingFace's model hub. Tight interfaces to other frameworks (e.g., Transformers, FARM, sentence-transformers)\n- **Scalable**: Scale to millions of docs via retrievers, production-ready backends like Elasticsearch / FAISS, and a fastAPI REST API\n- **End-to-End**: All tooling in one place: file conversion, cleaning, splitting, training, eval, inference, labeling, etc.\n- **Developer friendly**: Easy to debug, extend and modify.\n- **Customizable**: Fine-tune models to your domain or implement your custom DocumentStore.\n- **Continuous Learning**: Collect new training data via user feedback in production & improve your models continuously\n\n|                                                                                               |                                                                                                                                                                                                                                                   |\n| --------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| :ledger: [Docs](https://docs.haystack.deepset.ai)                                             | Components, Pipeline Nodes, Guides, API Reference                                                                                                                                                                                                 |\n| :floppy_disk: [Installation](https://github.com/deepset-ai/haystack#floppy_disk-installation) | How to install Haystack                                                                                                                                                                                                                           |\n| :mortar_board: [Tutorials](https://github.com/deepset-ai/haystack#mortar_board-tutorials)     | See what Haystack can do with our Notebooks & Scripts                                                                                                                                                                                             |\n| :beginner: [Quick Demo](https://github.com/deepset-ai/haystack#beginner-quick-demo)           | Deploy a Haystack application with Docker Compose and a REST API                                                                                                                                                                                  |\n| :vulcan_salute: [Community](https://github.com/deepset-ai/haystack#vulcan_salute-community)   | [Discord](https://haystack.deepset.ai/community/join), [Twitter](https://twitter.com/deepset_ai), [Stack Overflow](https://stackoverflow.com/questions/tagged/haystack), [GitHub Discussions](https://github.com/deepset-ai/haystack/discussions) |\n| :heart: [Contributing](https://github.com/deepset-ai/haystack#heart-contributing)             | We welcome all contributions!                                                                                                                                                                                                                     |\n| :bar_chart: [Benchmarks](https://haystack.deepset.ai/benchmarks/)                             | Speed & Accuracy of Retriever, Readers and DocumentStores                                                                                                                                                                                         |\n| :telescope: [Roadmap](https://haystack.deepset.ai/overview/roadmap)                           | Public roadmap of Haystack                                                                                                                                                                                                                        |\n| :newspaper: [Blog](https://medium.com/deepset-ai)                                             | Read our articles on Medium                                                                                                                                                                                                                       |\n| :phone: [Jobs](https://www.deepset.ai/jobs)                                                   | We're hiring! Have a look at our open positions                                                                                                                                                                                                   |\n\n\n## :floppy_disk: Installation\n\n**1. Basic Installation**\n\nYou can install a basic version of Haystack's latest release by using [pip](https://github.com/pypa/pip).\n\n```\n    pip3 install farm-haystack\n```\n\nThis command will install everything needed for basic Pipelines that use an Elasticsearch Document Store.\n\n**2. Full Installation**\n\nIf you plan to be using more advanced features like Milvus, FAISS, Weaviate, OCR or Ray,\nyou will need to install a full version of Haystack.\nThe following command will install the latest version of Haystack from the main branch.\n\n```\ngit clone https://github.com/deepset-ai/haystack.git\ncd haystack\npip install --upgrade pip\npip install -e '.[all]' ## or 'all-gpu' for the GPU-enabled dependencies\n```\n\nIf you cannot upgrade `pip` to version 21.3 or higher, you will need to replace:\n- `'.[all]'` with `'.[sql,only-faiss,only-milvus,weaviate,graphdb,crawler,preprocessing,ocr,onnx,ray,dev]'`\n- `'.[all-gpu]'` with `'.[sql,only-faiss-gpu,only-milvus,weaviate,graphdb,crawler,preprocessing,ocr,onnx-gpu,ray,dev]'`\n\nFor an complete list of the dependency groups available, have a look at the `haystack/pyproject.toml` file.\n\nTo install the REST API and UI, run the following from the root directory of the Haystack repo\n\n```\npip install rest_api/\npip install ui/\n```\n\n**3. Installing on Windows**\n\n```\npip install farm-haystack -f https://download.pytorch.org/whl/torch_stable.html\n```\n\n**4. Installing on Apple Silicon (M1)**\n\nM1 Macbooks require some extra dependencies in order to install Haystack.\n\n```\n# some additional dependencies needed on m1 mac\nbrew install postgresql\nbrew install cmake\nbrew install rust\n\n# haystack installation\nGRPC_PYTHON_BUILD_SYSTEM_ZLIB=true pip install git+https://github.com/deepset-ai/haystack.git\n```\n\n**5. Learn More**\n\nSee our [installation guide](https://haystack.deepset.ai/overview/quick-start) for more options.\nYou can find out more about our PyPi package on our [PyPi page](https://pypi.org/project/farm-haystack/).\n\n## :mortar_board: Tutorials\n\n![image](https://raw.githubusercontent.com/deepset-ai/haystack/main/docs/img/concepts_haystack_handdrawn.png)\n\nFollow our [introductory tutorial](https://haystack.deepset.ai/tutorials/first-qa-system)\nto setup a question answering system using Python and start performing queries!\nExplore [the rest of our tutorials](https://haystack.deepset.ai/tutorials)\nto learn how to tweak pipelines, train models and perform evaluation.\n\n## :beginner: Quick Demo\n\n**Hosted**\n\nTry out our hosted [Explore The World](https://haystack-demo.deepset.ai/) live demo here!\nAsk any question on countries or capital cities and let Haystack return the answers to you.\n\n**Local**\n\nTo run the Explore The World demo on your own machine and customize it to your needs, check out the instructions on [Explore the World repository](https://github.com/deepset-ai/haystack-demos/tree/main/explore_the_world) on GitHub.\n\n## :vulcan_salute: Community\n\nThere is a very vibrant and active community around Haystack which we are regularly interacting with!\nIf you have a feature request or a bug report, feel free to open an [issue in Github](https://github.com/deepset-ai/haystack/issues).\nWe regularly check these and you can expect a quick response.\nIf you'd like to discuss a topic, or get more general advice on how to make Haystack work for your project,\nyou can start a thread in [Github Discussions](https://github.com/deepset-ai/haystack/discussions) or our [Discord channel](https://haystack.deepset.ai/community).\nWe also check [Twitter](https://twitter.com/deepset_ai) and [Stack Overflow](https://stackoverflow.com/questions/tagged/haystack).\n\n\n## :heart: Contributing\n\nWe are very open to the community's contributions - be it a quick fix of a typo, or a completely new feature!\nYou don't need to be a Haystack expert to provide meaningful improvements.\nTo learn how to get started, check out our [Contributor Guidelines](https://github.com/deepset-ai/haystack/blob/main/CONTRIBUTING.md) first.\n\nYou can also find instructions to run the tests locally there.\n\nThanks so much to all those who have contributed to our project!\n\n<a href=\"https://github.com/deepset-ai/haystack/graphs/contributors\">\n  <img src=\"https://contrib.rocks/image?repo=deepset-ai/haystack\" />\n</a>\n\n\n## Who uses Haystack\n\nHere's a list of organizations who use Haystack. Don't hesitate to send a PR to let the world know that you use Haystack. Join our growing community!\n\n- [Airbus](https://www.airbus.com/en)\n- [Alcatel-Lucent](https://www.al-enterprise.com/)\n- [BetterUp](https://www.betterup.com/)\n- [Deepset](https://deepset.ai/)\n- [Etalab](https://www.etalab.gouv.fr/)\n- [Infineon](https://www.infineon.com/)\n- [Sooth.ai](https://sooth.ai/)\n"
    },
    {
      "name": "mathew55/haystack",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/16523777?s=40&v=4",
      "owner": "mathew55",
      "repo_name": "haystack",
      "description": ":mag: Haystack is an open source NLP framework that leverages Transformer models. It enables developers to implement production-ready neural search, question answering, semantic document search and summarization for a wide range of applications.",
      "homepage": "https://deepset.ai/haystack",
      "language": "Python",
      "created_at": "2021-12-23T14:02:37Z",
      "updated_at": "2022-01-22T08:24:59Z",
      "topics": [],
      "readme": "<p align=\"center\">\n  <a href=\"https://www.deepset.ai/haystack/\"><img src=\"https://raw.githubusercontent.com/deepset-ai/haystack/main/docs/img/haystack_logo_colored.png\" alt=\"Haystack\"></a>\n</p>\n\n<p>\n    <a href=\"https://github.com/deepset-ai/haystack/actions/workflows/tests.yml\">\n        <img alt=\"Tests\" src=\"https://github.com/deepset-ai/haystack/workflows/Tests/badge.svg?branch=main\">\n    </a>\n    <a href=\"https://github.com/deepset-ai/haystack-json-schema/actions/workflows/schemas.yml\">\n        <img alt=\"Schemas\" src=\"https://github.com/deepset-ai/haystack-json-schema/actions/workflows/schemas.yml/badge.svg\">\n    </a>\n    <a href=\"https://docs.haystack.deepset.ai\">\n        <img alt=\"Documentation\" src=\"https://img.shields.io/website?label=documentation&up_message=online&url=https%3A%2F%2Fdocs.haystack.deepset.ai\">\n    </a>\n    <a href=\"https://github.com/deepset-ai/haystack/releases\">\n        <img alt=\"Release\" src=\"https://img.shields.io/github/release/deepset-ai/haystack\">\n    </a>\n    <a href=\"https://github.com/deepset-ai/haystack/commits/main\">\n        <img alt=\"Last commit\" src=\"https://img.shields.io/github/last-commit/deepset-ai/haystack\">\n    </a>\n    <a href=\"https://pepy.tech/project/farm-haystack\">\n        <img alt=\"Downloads\" src=\"https://pepy.tech/badge/farm-haystack/month\">\n    </a>\n    <a href=\"https://www.deepset.ai/jobs\">\n        <img alt=\"Jobs\" src=\"https://img.shields.io/badge/Jobs-We're%20hiring-blue\">\n    </a>\n        <a href=\"https://twitter.com/intent/follow?screen_name=deepset_ai\">\n        <img alt=\"Twitter\" src=\"https://img.shields.io/twitter/follow/deepset_ai?style=social\">\n    </a>\n</p>\n\n[Haystack](https://haystack.deepset.ai) is an end-to-end framework that enables you to build powerful and production-ready pipelines for different search use cases.\nWhether you want to perform Question Answering or semantic document search, you can use the State-of-the-Art NLP models in Haystack to provide unique search experiences and allow your users to query in natural language.\nHaystack is built in a modular fashion so that you can combine the best technology from other open-source projects like Huggingface's Transformers, Elasticsearch, or Milvus.\n\n<p align=\"center\"><img src=\"https://raw.githubusercontent.com/deepset-ai/haystack/main/docs/img/main_example.gif\"></p>\n\n## What to build with Haystack\n\n- **Ask questions in natural language** and find granular answers in your documents.\n- Perform **semantic search** and retrieve documents according to meaning, not keywords\n- Use **off-the-shelf models** or **fine-tune** them to your domain.\n- Use **user feedback** to evaluate, benchmark, and continuously improve your live models.\n- Leverage existing **knowledge bases** and better handle the long tail of queries that **chatbots** receive.\n- **Automate processes** by automatically applying a list of questions to new documents and using the extracted answers.\n\n## Core Features\n\n- **Latest models**: Utilize all latest transformer-based models (e.g., BERT, RoBERTa, MiniLM) for extractive QA, generative QA, and document retrieval.\n- **Modular**: Multiple choices to fit your tech stack and use case. Pick your favorite database, file converter, or modeling framework.\n- **Pipelines**: The Node and Pipeline design of Haystack allows for custom routing of queries to only the relevant components.\n- **Open**: 100% compatible with HuggingFace's model hub. Tight interfaces to other frameworks (e.g., Transformers, FARM, sentence-transformers)\n- **Scalable**: Scale to millions of docs via retrievers, production-ready backends like Elasticsearch / FAISS, and a fastAPI REST API\n- **End-to-End**: All tooling in one place: file conversion, cleaning, splitting, training, eval, inference, labeling, etc.\n- **Developer friendly**: Easy to debug, extend and modify.\n- **Customizable**: Fine-tune models to your domain or implement your custom DocumentStore.\n- **Continuous Learning**: Collect new training data via user feedback in production & improve your models continuously\n\n|                                                                                               |                                                                                                                                                                                                                                                   |\n| --------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| :ledger: [Docs](https://docs.haystack.deepset.ai)                                   | Components, Pipeline Nodes, Guides, API Reference                                                                                                                                                                                                   |\n| :floppy_disk: [Installation](https://github.com/deepset-ai/haystack#floppy_disk-installation) | How to install Haystack                                                                                                                                                                                                                           |\n| :mortar_board: [Tutorials](https://github.com/deepset-ai/haystack#mortar_board-tutorials)     | See what Haystack can do with our Notebooks & Scripts                                                                                                                                                                                             |\n| :beginner: [Quick Demo](https://github.com/deepset-ai/haystack#beginner-quick-demo)           | Deploy a Haystack application with Docker Compose and a REST API                                                                                                                                                                                  |\n| :vulcan_salute: [Community](https://github.com/deepset-ai/haystack#vulcan_salute-community)   | [Discord](https://haystack.deepset.ai/community/join), [Twitter](https://twitter.com/deepset_ai), [Stack Overflow](https://stackoverflow.com/questions/tagged/haystack), [GitHub Discussions](https://github.com/deepset-ai/haystack/discussions) |\n| :heart: [Contributing](https://github.com/deepset-ai/haystack#heart-contributing)             | We welcome all contributions!                                                                                                                                                                                                                     |\n| :bar_chart: [Benchmarks](https://haystack.deepset.ai/benchmarks/)                       | Speed & Accuracy of Retriever, Readers and DocumentStores                                                                                                                                                                                         |\n| :telescope: [Roadmap](https://haystack.deepset.ai/overview/roadmap)                           | Public roadmap of Haystack                                                                                                                                                                                                                        |\n| :newspaper: [Blog](https://medium.com/deepset-ai)                                             | Read our articles on Medium                                                                                                                                                                                                                       |\n| :phone: [Jobs](https://www.deepset.ai/jobs)                                                   | We're hiring! Have a look at our open positions                                                                                                                                                                                                   |\n\n\n## :floppy_disk: Installation\n\n**1. Basic Installation**\n\nYou can install a basic version of Haystack's latest release by using [pip](https://github.com/pypa/pip).\n\n```\n    pip3 install farm-haystack\n```\n\nThis command will install everything needed for basic Pipelines that use an Elasticsearch Document Store.\n\n**2. Full Installation**\n\nIf you plan to be using more advanced features like Milvus, FAISS, Weaviate, OCR or Ray,\nyou will need to install a full version of Haystack.\nThe following command will install the latest version of Haystack from the main branch.\n\n```\ngit clone https://github.com/deepset-ai/haystack.git\ncd haystack\npip install --upgrade pip\npip install -e '.[all]' ## or 'all-gpu' for the GPU-enabled dependencies\n```\n\nIf you cannot upgrade `pip` to version 21.3 or higher, you will need to replace:\n- `'.[all]'` with `'.[sql,only-faiss,only-milvus1,weaviate,graphdb,crawler,preprocessing,ocr,onnx,ray,dev]'`\n- `'.[all-gpu]'` with `'.[sql,only-faiss-gpu,only-milvus1,weaviate,graphdb,crawler,preprocessing,ocr,onnx-gpu,ray,dev]'`\n\nFor an complete list of the dependency groups available, have a look at the `haystack/pyproject.toml` file.\n\nTo install the REST API and UI, run the following from the root directory of the Haystack repo\n\n```\npip install rest_api/\npip install ui/\n```\n\n**3. Installing on Windows**\n\n```\npip install farm-haystack -f https://download.pytorch.org/whl/torch_stable.html\n```\n\n**4. Installing on Apple Silicon (M1)**\n\nM1 Macbooks require some extra dependencies in order to install Haystack.\n\n```\n# some additional dependencies needed on m1 mac\nbrew install postgresql\nbrew install cmake\nbrew install rust\n\n# haystack installation\nGRPC_PYTHON_BUILD_SYSTEM_ZLIB=true pip install git+https://github.com/deepset-ai/haystack.git\n```\n\n**5. Learn More**\n\nSee our [installation guide](https://haystack.deepset.ai/overview/get-started) for more options.\nYou can find out more about our PyPi package on our [PyPi page](https://pypi.org/project/farm-haystack/).\n\n## :mortar_board: Tutorials\n\n![image](https://raw.githubusercontent.com/deepset-ai/haystack/main/docs/img/concepts_haystack_handdrawn.png)\n\nFollow our [introductory tutorial](https://haystack.deepset.ai/tutorials/first-qa-system)\nto setup a question answering system using Python and start performing queries!\nExplore [the rest of our tutorials](https://haystack.deepset.ai/tutorials)\nto learn how to tweak pipelines, train models and perform evaluation.\n\n## :beginner: Quick Demo\n\n**Hosted**\n\nTry out our hosted [Explore The World](https://haystack-demo.deepset.ai/) live demo here!\nAsk any question on countries or capital cities and let Haystack return the answers to you.\n\n**Local**\n\nStart up a Haystack service via [Docker Compose](https://docs.docker.com/compose/).\nWith this you can begin calling it directly via the REST API or even interact with it using the included Streamlit UI.\n\n<details>\n  <summary>Click here for a step-by-step guide</summary>\n\n**1. Update/install Docker and Docker Compose, then launch Docker**\n\n```\n    apt-get update && apt-get install docker && apt-get install docker-compose\n    service docker start\n```\n\n**2. Clone Haystack repository**\n\n```\n    git clone https://github.com/deepset-ai/haystack.git\n```\n\n**3. Pull images & launch demo app**\n\n```\n    cd haystack\n    docker-compose pull\n    docker-compose up\n\n    # Or on a GPU machine: docker-compose -f docker-compose-gpu.yml up\n```\n\nYou should be able to see the following in your terminal window as part of the log output:\n\n```\n..\nui_1             |   You can now view your Streamlit app in your browser.\n..\nui_1             |   External URL: http://192.168.108.218:8501\n..\nhaystack-api_1   | [2021-01-01 10:21:58 +0000] [17] [INFO] Application startup complete.\n```\n\n**4. Open the Streamlit UI for Haystack by pointing your browser to the \"External URL\" from above.**\n\nYou should see the following:\n\n![image](https://raw.githubusercontent.com/deepset-ai/haystack/main/docs/img/streamlit_ui_screenshot.png)\n\nYou can then try different queries against a pre-defined set of indexed articles related to Game of Thrones.\n\n**Note**: The following containers are started as a part of this demo:\n\n* Haystack API: listens on port 8000\n* DocumentStore (Elasticsearch): listens on port 9200\n* Streamlit UI: listens on port 8501\n\nPlease note that the demo will [publish](https://docs.docker.com/config/containers/container-networking/) the container ports to the outside world. *We suggest that you review the firewall settings depending on your system setup and the security guidelines.*\n\n</details>\n\n## :vulcan_salute: Community\n\nThere is a very vibrant and active community around Haystack which we are regularly interacting with!\nIf you have a feature request or a bug report, feel free to open an [issue in Github](https://github.com/deepset-ai/haystack/issues).\nWe regularly check these and you can expect a quick response.\nIf you'd like to discuss a topic, or get more general advice on how to make Haystack work for your project,\nyou can start a thread in [Github Discussions](https://github.com/deepset-ai/haystack/discussions) or our [Discord channel](https://haystack.deepset.ai/community/join).\nWe also check [Twitter](https://twitter.com/deepset_ai) and [Stack Overflow](https://stackoverflow.com/questions/tagged/haystack).\n\n\n## :heart: Contributing\n\nWe are very open to the community's contributions - be it a quick fix of a typo, or a completely new feature!\nYou don't need to be a Haystack expert to provide meaningful improvements.\nTo learn how to get started, check out our [Contributor Guidelines](https://github.com/deepset-ai/haystack/blob/main/CONTRIBUTING.md) first.\n\nYou can also find instructions to run the tests locally there.\n\nThanks so much to all those who have contributed to our project!\n\n<a href=\"https://github.com/deepset-ai/haystack/graphs/contributors\">\n  <img src=\"https://contrib.rocks/image?repo=deepset-ai/haystack\" />\n</a>\n\n\n## Who uses Haystack\n\nHere's a list of organizations who use Haystack. Don't hesitate to send a PR to let the world know that you use Haystack. Join our growing community!\n\n- [Airbus](https://www.airbus.com/en)\n- [Alcatel-Lucent](https://www.al-enterprise.com/)\n- [BetterUp](https://www.betterup.com/)\n- [Deepset](https://deepset.ai/)\n- [Etalab](https://www.etalab.gouv.fr/)\n- [Infineon](https://www.infineon.com/)\n- [Sooth.ai](https://sooth.ai/)\n"
    },
    {
      "name": "ArzelaAscoIi/haystack",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/37148029?s=40&v=4",
      "owner": "ArzelaAscoIi",
      "repo_name": "haystack",
      "description": ":mag: End-to-end Python framework for building natural language search interfaces to data. Leverages Transformers and the State-of-the-Art of NLP. Supports DPR, Elasticsearch, Hugging Face’s Hub, and much more!",
      "homepage": "https://deepset.ai/haystack",
      "language": "Python",
      "created_at": "2021-10-14T08:54:05Z",
      "updated_at": "2025-01-10T23:14:28Z",
      "topics": [],
      "readme": "<p align=\"center\">\n  <a href=\"https://www.deepset.ai/haystack/\"><img src=\"https://raw.githubusercontent.com/deepset-ai/haystack/main/docs/img/haystack_logo_colored.png\" alt=\"Haystack\"></a>\n</p>\n\n<p>\n    <a href=\"https://github.com/deepset-ai/haystack/actions/workflows/tests.yml\">\n        <img alt=\"Tests\" src=\"https://github.com/deepset-ai/haystack/workflows/Tests/badge.svg?branch=main\">\n    </a>\n    <a href=\"https://github.com/deepset-ai/haystack-json-schema/actions/workflows/schemas.yml\">\n        <img alt=\"Schemas\" src=\"https://github.com/deepset-ai/haystack-json-schema/actions/workflows/schemas.yml/badge.svg\">\n    </a>\n    <a href=\"https://docs.haystack.deepset.ai\">\n        <img alt=\"Documentation\" src=\"https://img.shields.io/website?label=documentation&up_message=online&url=https%3A%2F%2Fdocs.haystack.deepset.ai\">\n    </a>\n    <a href=\"https://app.fossa.com/projects/custom%2B24445%2Fgithub.com%2Fdeepset-ai%2Fhaystack?ref=badge_shield\">\n        <img alt=\"FOSSA Status\" src=\"https://app.fossa.com/api/projects/custom%2B24445%2Fgithub.com%2Fdeepset-ai%2Fhaystack.svg?type=shield\"/>\n    </a>\n    <a href=\"https://github.com/deepset-ai/haystack/releases\">\n        <img alt=\"Release\" src=\"https://img.shields.io/github/release/deepset-ai/haystack\">\n    </a>\n    <a href=\"https://github.com/deepset-ai/haystack/commits/main\">\n        <img alt=\"Last commit\" src=\"https://img.shields.io/github/last-commit/deepset-ai/haystack\">\n    </a>\n    <a href=\"https://pepy.tech/project/farm-haystack\">\n        <img alt=\"Downloads\" src=\"https://pepy.tech/badge/farm-haystack/month\">\n    </a>\n    <a href=\"https://www.deepset.ai/jobs\">\n        <img alt=\"Jobs\" src=\"https://img.shields.io/badge/Jobs-We're%20hiring-blue\">\n    </a>\n        <a href=\"https://twitter.com/intent/follow?screen_name=deepset_ai\">\n        <img alt=\"Twitter\" src=\"https://img.shields.io/badge/follow-%40deepset_ai-1DA1F2?logo=twitter\">\n    </a>\n    <a href=\"https://discord.com/invite/qZxjM4bAHU\">\n        <img alt=\"chat on Discord\" src=\"https://img.shields.io/discord/993534733298450452?logo=discord\">\n    </a>\n</p>\n\n[Haystack](https://haystack.deepset.ai/) is an end-to-end NLP framework that enables you to build NLP applications powered by LLMs, Transformer models, vector search and more. Whether you want to perform question answering, answer generation, semantic document search, or build tools that are capable of complex decision making and query resolution, you can use the state-of-the-art NLP models with Haystack to build end-to-end NLP applications solving your use case.\n\n## Core Concepts\n\n🏃‍♀️ **[Pipelines](https://docs.haystack.deepset.ai/docs/pipelines):** This is the standard Haystack structure that can connect to your data and perform on it NLP tasks that you define. The data in a Pipeline flows from one Node to the next. You define how Nodes interact with each other, and how one Node pushes data to the next.\n\nAn example pipeline would consist of one `Retriever` Node and one `Reader` Node. When the pipeline runs with a query, the Retriever first retrieves the documents relevant to the query and then the Reader extracts the final answer.\n\n⚛️ **[Nodes](https://docs.haystack.deepset.ai/docs/nodes_overview):** Each Node achieves one thing. Such as preprocessing documents, retrieving documents, using language models to answer questions and so on.\n\n🕵️ **[Agent](https://docs.haystack.deepset.ai/docs/agent):** (since 1.15) An Agent is a component that is powered by an LLM, such as GPT-3. It can decide on the next best course of action so as to get to the result of a query. It uses the Tools available to it to achieve this. While a pipeline has a clear start and end, an Agent is able to decide whether the query has resolved or not. It may also make use of a Pipeline as a Tool.\n\n🛠️ **[Tools](https://docs.haystack.deepset.ai/docs/agent#tools):** You can think of a Tool as an expert, that is able to do something really well. Such as a calculator, good at mathematics. Or a [WebRetriever](https://docs.haystack.deepset.ai/docs/agent#web-tools), good at retrieving pages from the internet. A Node or pipeline in Haystack can also be used as a Tool. A Tool is a component that is used by an Agent, to resolve complex queries.\n\n🗂️ **[DocumentStores](https://docs.haystack.deepset.ai/docs/document_store):** A DocumentStore is database where you store your text data for Haystack to access. Haystack DocumentStores are available with ElasticSearch, Opensearch, Weaviate, Pinecone, FAISS and more. For a full list of available DocumentStores, check out our [documentation](https://docs.haystack.deepset.ai/docs/document_store).\n\n## What to Build with Haystack\n\n-   Perform Question Answering **in natural language** to find granular answers in your documents.\n-   **Generate answers or content** with the use of LLM such as articles, tweets, product descriptions and more, the sky is the limit 🚀\n-   Perform **semantic search** and retrieve documents according to meaning.\n-   Build applications that can do complex decisions making to answer complex queries: such as systems that can resolve complex customer queries, do knowledge search on many disconnected resources and so on.\n-   Use **off-the-shelf models** or **fine-tune** them to your data.\n-   Use **user feedback** to evaluate, benchmark, and continuously improve your models.\n\n## Features\n\n-   **Latest models**: Haystack allows you to use and compare models available from OpenAI, Cohere and Hugging Face, as well as your own local models. Use the latest LLMs or Transformer-based models (for example: BERT, RoBERTa, MiniLM).\n-   **Modular**: Multiple choices to fit your tech stack and use case. A wide choice of DocumentStores to store your data, file conversion tools and more\n-   **Open**: Integrated with Hugging Face's model hub, OpenAI, Cohere and various Azure services.\n-   **Scalable**: Scale to millions of docs using retrievers and production-scale components like Elasticsearch and a fastAPI REST API.\n-   **End-to-End**: All tooling in one place: file conversion, cleaning, splitting, training, eval, inference, labeling, and more.\n-   **Customizable**: Fine-tune models to your domain or implement your custom Nodes.\n-   **Continuous Learning**: Collect new training data from user feedback in production & improve your models continuously.\n\n## Resources\n|                                                                                               |                                                                                                                                                                                                                                                   |\n| --------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| 📒 [Docs](https://docs.haystack.deepset.ai)                                             | Components, Pipeline Nodes, Guides, API Reference                                                                                                                                                                                                 |\n| 💾 [Installation](https://github.com/deepset-ai/haystack#-installation) | How to install Haystack                                                                                                                                                                                                                           |\n| 🎓 [Tutorials](https://haystack.deepset.ai/tutorials)     | See what Haystack can do with our Notebooks & Scripts                                                                                                                                                                                             |\n| 🎉 [Haystack Extras](https://github.com/deepset-ai/haystack-extras)               | A repository that lists extra Haystack packages and components that can be installed separately.                                                                                                                                                                                             |\n| 🔰 [Demos](https://github.com/deepset-ai/haystack-demos)           | A repository containing Haystack demo applications with Docker Compose and a REST API                                                                                                                                                                                  |\n| 🖖 [Community](https://github.com/deepset-ai/haystack#-community)   | [Discord](https://haystack.deepset.ai/community/join), [Twitter](https://twitter.com/deepset_ai), [Stack Overflow](https://stackoverflow.com/questions/tagged/haystack), [GitHub Discussions](https://github.com/deepset-ai/haystack/discussions) |\n| 💙 [Contributing](https://github.com/deepset-ai/haystack#-contributing)             | We welcome all contributions!                                                                                                                                                                                                                     |\n| 📊 [Benchmarks](https://haystack.deepset.ai/benchmarks/)                             | Speed & Accuracy of Retriever, Readers and DocumentStores                                                                                                                                                                                         |\n| 🔭 [Roadmap](https://haystack.deepset.ai/overview/roadmap)                           | Public roadmap of Haystack                                                                                                                                                                                                                        |\n| 📰 [Blog](https://haystack.deepset.ai/blog)                                             | Learn about the latest with Haystack and NLP                                                                                                                                                                   |\n| ☎️ [Jobs](https://www.deepset.ai/jobs)                                                   | We're hiring! Have a look at our open positions                                                                                                                                                                                                   |\n\n\n## 💾 Installation\n\nFor a detailed installation guide see [the official documentation](https://docs.haystack.deepset.ai/docs/installation). There you’ll find instructions for custom installations handling Windows and Apple Silicon.\n\n**Basic Installation**\n\nUse [pip](https://github.com/pypa/pip) to install a basic version of Haystack's latest release:\n\n```\n    pip install farm-haystack\n```\n\nThis command installs everything needed for basic Pipelines that use an Elasticsearch DocumentStore.\n\n**Full Installation**\n\nTo use more advanced features, like certain DocumentStores, FileConverters, OCR, or Ray, install further dependencies. The following command installs the [latest release](https://github.com/deepset-ai/haystack/releases) of Haystack and all its dependencies:\n\n```bash\npip install --upgrade pip\npip install 'farm-haystack[all]' ## or 'all-gpu' for the GPU-enabled dependencies\n```\n\nIf you want to try out the newest features that are not in an official release yet, you can install Haystack from the main branch. The following command installs from `main` branch with `dev` dependencies:\n\n```\npip install git+https://github.com/deepset-ai/haystack.git@main#egg=farm-haystack[dev]\n```\n\nTo be able to make changes to Haystack code, install with the following commands:\n\n```bash\n# Clone the repo\ngit clone https://github.com/deepset-ai/haystack.git\n\n# Move into the cloned folder\ncd haystack\n\n# Upgrade pip\npip install --upgrade pip\n\n# Install Haystack in editable mode\npip install -e '.[all]'\n```\n\nIf you want to contribute to the Haystack repo, check our [Contributor Guidelines](#💙-contributing) first.\n\nSee the list of [dependencies](https://github.com/deepset-ai/haystack/blob/main/pyproject.toml) to check which ones you want to install (for example, `[all]`, `[dev]`, or other).\n\n**Installing the REST API**\n\nHaystack comes packaged with a REST API so that you can deploy it as a service. Run the following command from the root directory of the Haystack repo to install REST_API:\n\n```\npip install rest_api/\n```\n\nYou can find out more about our PyPi package on our [PyPi page](https://pypi.org/project/farm-haystack/).\n\n## 🔰Demos\n\nYou can find some of our hosted demos with instructions to run them locally too on our [haystack-demos](https://github.com/deepset-ai/haystack-demos) repository\n\n🐥 **[Should I follow?](https://huggingface.co/spaces/deepset/should-i-follow) - Twitter demo**\n\n🌎 **[Explore The World](https://haystack-demo.deepset.ai/) demo**\n\n### 🖖 Community\n\nIf you have a feature request or a bug report, feel free to open an [issue in Github](https://github.com/deepset-ai/haystack/issues). We regularly check these and you can expect a quick response. If you'd like to discuss a topic, or get more general advice on how to make Haystack work for your project, you can start a thread in [Github Discussions](https://github.com/deepset-ai/haystack/discussions) or our [Discord channel](https://haystack.deepset.ai/community). We also check [Twitter](https://twitter.com/deepset_ai) and [Stack Overflow](https://stackoverflow.com/questions/tagged/haystack).\n\n### 💙 Contributing\n\nWe are very open to the community's contributions - be it a quick fix of a typo, or a completely new feature! You don't need to be a Haystack expert to provide meaningful improvements. To learn how to get started, check out our [Contributor Guidelines](https://github.com/deepset-ai/haystack/blob/main/CONTRIBUTING.md) first.\n\nYou can also find instructions to run the tests locally there.\n\nThanks so much to all those who have contributed to our project!\n\n<a href=\"[](https://github.com/deepset-ai/haystack/graphs/contributors)[https://github.com/deepset-ai/haystack/graphs/contributors](https://github.com/deepset-ai/haystack/graphs/contributors)\"> <img src=\"[](https://contrib.rocks/image?repo=deepset-ai/haystack)[https://contrib.rocks/image?repo=deepset-ai/haystack](https://contrib.rocks/image?repo=deepset-ai/haystack)\" /> </a>\n\n## Who Uses Haystack\n\nHere's a list of organizations that we know about from our community. Don't hesitate to send a PR to let the world know that you use Haystack. Join our growing community!\n\n-   [Airbus](https://www.airbus.com/en)\n-   [Alcatel-Lucent](https://www.al-enterprise.com/)\n-   [Apple](https://www.apple.com/)\n-   [BetterUp](https://www.betterup.com/)\n-   [Databricks](https://www.databricks.com/)\n-   [Deepset](https://deepset.ai/)\n-   [Etalab](https://www.etalab.gouv.fr/)\n-   [Infineon](https://www.infineon.com/)\n-   [Intelijus](https://www.intelijus.ai/)\n-   [LEGO](https://www.lego.com/)\n-   [Netflix](https://netflix.com)\n-   [Nvidia](https://www.nvidia.com/en-us/)\n-   [PostHog](https://posthog.com/)\n-   [Rakuten](https://www.rakuten.com/)\n-   [Sooth.ai](https://sooth.ai/)\n"
    },
    {
      "name": "cox-j/gamechanger",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/100856227?s=40&v=4",
      "owner": "cox-j",
      "repo_name": "gamechanger",
      "description": null,
      "homepage": null,
      "language": "JavaScript",
      "created_at": "2023-02-02T00:02:06Z",
      "updated_at": "2023-08-22T13:59:38Z",
      "topics": [],
      "readme": "<img src=\"./assets/logos/GAMECHANGER-NoPentagon_RGB@3x.png\" \n     alt=\"GC Logo\" width=\"500\" >\n\n#\nOver 15 thousand documents govern how the Departmentof Defense (DoD) operates. The documents exist in different repositories, often exist on different networks, are discoverable to different communities, are updated independently, and evolve rapidly. No single ability has ever existed that would enable navigation of the vast universe of governing requirements and guidance documents, leaving the Department unable to make evidence-based, data-driven decisions. Today GAMECHANGER offers a scalable solution with an authoritative corpus comprising a single trusted repository of all statutory and policy driven requirements based on Artificial-Intelligence (AI) enabled technologies.\n\n#\n<img src=\"./assets/icons/Brand_Platform.png\" align=\"right\"\n     alt=\"Mission Vision Icons\" width=\"320\" >\n\n### Vision\n\nFundamentally changing the way in which the DoD navigates its universe of requirements and makes decisions\n\n### Mission\nGAMECHANGER aspires to be the Department’s trusted solution for evidence-based, data-driven decision-making across the universe of DoD requirements by:\n\n- Building the DoD’s authoritative corpus of requirements and policy to drive search, discovery, understanding, and analytic capabilities\n- Operationalizing cutting-edge technologies, algorithms, models and interfaces to automate and scale the solution\n- Fusing best practices from industry, academia, and government to advance innovation and research\n- Engaging the open-source community to build generalizable and replicable technology\n\n# Repositories\n\n<p><img src=\"./assets/icons/checkmarks.png\"\n     alt=\"Checkmarks\" width=\"100\" style=\"vertical-align: middle;\" >\n<a href=\"https://github.com/dod-advana/GAMECHANGER\">gamechanger (here)</a>\n</p> \n<p><img src=\"./assets/icons/External%20Data%20Upload.png\" \n     alt=\"Data Engineering\" width=\"100\" style=\"vertical-align: middle;\">\n<a href=\"https://github.com/dod-advana/gamechanger-data\">gamechanger-data</a>\n</p> \n<p><img src=\"./assets/icons/RPA.png\" \n     alt=\"Developer\" width=\"100\" style=\"vertical-align: middle;\">\n<a href=\"https://github.com/dod-advana/gamechanger-ml\">gamechanger-ml</a>\n</p> \n<p><img src=\"./assets/icons/synergy.png\" \n     alt=\"Neo4J Plugin\" width=\"100\" style=\"vertical-align: middle;\">\n<a href=\"https://github.com/dod-advana/gamechanger-neo4j-plugin\">gamechanger-neo4j-plugin</a>\n</p> \n<p><img src=\"./assets/icons/Developers.png\" \n     alt=\"Web Application\" width=\"100\" style=\"vertical-align: middle;\">\n<a href=\"https://github.com/dod-advana/gamechanger-web\">gamechanger-web</a>\n</p> \n<p><img src=\"./assets/icons/chatbot.png\" \n     alt=\"Crawler\" width=\"100\" style=\"vertical-align: middle;\">\n<a href=\"https://github.com/dod-advana/gamechanger-crawlers\">gamechanger-crawler</a>\n</p> \n<p><img src=\"./assets/icons/platform.png\" \n     alt=\"Web Application\" width=\"100\" style=\"vertical-align: middle;\">\ngamechanger-dm-api (Coming Soon!)\n</p> \n\n\n# \n\n## License & Contributions\nSee LICENSE.md (including licensing intent - INTENT.md) and CONTRIBUTING.md\n"
    },
    {
      "name": "cshikai/mirror",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/16132347?s=40&v=4",
      "owner": "cshikai",
      "repo_name": "mirror",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2023-01-21T16:28:15Z",
      "updated_at": "2023-02-28T12:54:01Z",
      "topics": [],
      "readme": "\r\n\r\n# Mirror\r\n![System Overview](overview.png)\r\n\r\nBasic Information Retrieval system designed for multimedia documents and future extensibility. \r\n\r\nThe system allows users to carry out text-based search on a repository of multimedia documents including texts, audio, and images. The search itself has a multi-pronged approach that uses a combination of keyword, semantic and graphical techniques to retrieve the most relevant documents. Several ML services run in the back end to enable important information to be extracted from each document type, and indexed in a manner that is compatible across media types.\r\n\r\n##   Service Description\r\n\r\n### Upload Service\r\nUser-facing service that allows documents to be uploaded by the clients through a web browser. Documents uploaded are stored in the document database and triggers the relevant ML services to extract information for indexing.\r\n\r\n### Inference Service(s)\r\nBack end services that employ ML techniques to extract information from the documents.\r\nCurrent supported service(s) are:\r\n\r\n - Text Embedding Generation - The current version uses a SimCSE RoBERTa model to generate embeddings\r\n - Entity Extraction - Utilizes JEREX model for joint Entity and Relationship Extraction\r\n \r\nFuture services to be implemented\r\n - ASR (Speech-to-Text)\r\n - Translation \r\n - Caption Generation\r\n\r\n### Query\r\nUser-facing service that returns users relevant documents based on search query. Responsible for query transformation, querying the relevant databases for different types of search, and collating the results before generating a unified search result.\r\n\r\n# Individual Service Structure\r\n\r\n\r\n    ├── README.md               <- The top-level README for developers using this project.\r\n    |\r\n    ├── build                   <- Folder that contains files for building the environment \r\n    │   ├── docker-compose.yml  <- docker-compose file for quickly building containers\r\n    │   ├── Makefile            <- Makefile which will be ran when building the docker image\r\n    │   └── requirements.txt    <- The requirements file for reproducing the analysis environment, e.g.\r\n    │                           generated with `pip freeze > requirements.txt`\r\n    │── Dockerfile              <- Dockerfile for building docker image (unfortunately it has to be in root for it to work)\r\n    |\r\n    ├── data                    <- Download data from clearml here\r\n    |   ├── train        \r\n    │   └── valid\r\n    |     \r\n    ├── models                  <- Download/load pretrained model/save trained model locally here\r\n    |   ├── vgg        \r\n    │   └── elmo\r\n    |   └── trained_models      <- Folder that contains the trained model weights\r\n    │       └── model_weights.ckpt     \r\n    |\r\n    ├── src                     <- Source code for use in this project.\r\n    │   │\r\n    │   ├── main.py             <- Code to run for task initialization,  sending to remote, download datasets, starting experimentation\r\n    |   |\r\n    │   ├── experiment.py       <- Experimentation defining the datasets, trainer, epoch behaviour and running training\r\n    |   |\r\n    │   ├── config\r\n    |   │   ├── config.py       <- Boilerplate code for config loading.yaml   \r\n    |   │   └── config.yaml     <- Configfile for parameters\r\n    |   |\r\n    │   ├── data                <- Scripts related to data procesing\r\n    │   │   ├── dataset.py\r\n    │   │   ├── postprocessing.py\r\n    │   │   ├── preprocessing.py\r\n    │   |   ├── transforms.py\r\n    |   |   └── common          <- common reusable transformation modules\r\n    |   │       └── transforms.py \r\n    │   │\r\n    |   ├── model               <- Scripts related to module architecture\r\n    |   │   ├── model.py        <- Main model file chaining together modules \r\n    |   │   └── modules         <- Folder containing model modules\r\n    |   |       ├── common\r\n    |   |       |   └── crf.py \r\n    |   |       ├── encoder.py           \r\n    |   |       └── decoder.py           \r\n    │   │\r\n    │   └── evaluation          <- Scripts to generate evaluations of model e.g. confusion matrix etc.\r\n    |       ├── visualize.py           \r\n    |       └── common \r\n    |           └── metrics.py\r\n    |\r\n    ├── tests                   <- Folder where all the unit-tests are\r\n    |\r\n    ├── notebooks               <- Jupyter notebooks. Naming convention is a number (for ordering),\r\n    │                           the creator's initials, and a short `-` delimited description, e.g.\r\n    │                           `1.0-jqp-initial-data-exploration`.\r\n    |\r\n    ├── docs                    <- A default Sphinx project; see sphinx-doc.org for details\r\n    |\r\n    ├── references              <- Data dictionaries, manuals, and all other explanatory materials.\r\n    │\r\n    │\r\n    └── tox.ini                 <- tox file with settings for running tox; see tox.readthedocs.io\r\n\r\n\r\n# Setting Up\r\nPrerequisites:  Requires docker and docker-compose \r\n\r\n## Docker Compose\r\n\r\n### Steps to commission a service in docker compose\r\n\r\nStep 1: Build docker images for each service \r\n```\r\ncd <repo of service>/build\r\ndocker build -t mirror-<service_name>:latest .\r\n```\r\nStep 2: Spin up containers for all services using docker-compose:\r\n\r\n```\r\ndocker-compose up -d\r\n```\r\n"
    },
    {
      "name": "agent87/IhuguraChatBotUX",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/24498800?s=40&v=4",
      "owner": "agent87",
      "repo_name": "IhuguraChatBotUX",
      "description": "Ihugure Chatbot Streamlit User Interface",
      "homepage": "",
      "language": "Python",
      "created_at": "2022-02-23T06:37:22Z",
      "updated_at": "2022-09-27T22:33:36Z",
      "topics": [
        "document-parser",
        "machine-learning",
        "python",
        "question-answering",
        "streamlit"
      ],
      "readme": "<a name=\"readme-top\"></a>\n\n\n[![Contributors][contributors-shield]][contributors-url]\n[![Forks][forks-shield]][forks-url]\n[![Stargazers][stars-shield]][stars-url]\n[![Issues][issues-shield]][issues-url]\n\n\n\n<!-- PROJECT LOGO -->\n<br />\n<div align=\"center\">\n  <a href=\"https://github.com/MBAZA-NLP/community.website\">\n    <img src=\"assets/img/logo/MBAZA-LOGO-WHITE.png\" alt=\"Logo\" width=\"80\" height=\"80\">\n  </a>\n\n  <h3 align=\"center\">Ihugura Chatbot</h3>\n\n  <p align=\"center\">\n    A guide on how to the Ihugura chatbot!\n    <br />\n    <a href=\"https://github.com/agent87/community.website/issues\">Report Bug</a>\n    ·\n    <a href=\"https://github.com/agent87/community.website/issues\">Request Feature</a>\n  </p>\n</div>\n\n\n\n\n\n\n<!-- ABOUT THE PROJECT -->\n## About The Project\n\n<div align=\"center\">\n    <img src=\"assets/img/frontpage.JPG\" alt=\"Logo\" width=\"1080\" height=\"600\">\n</div>\n\nThe aim of this chatbot is deliver equity in terms of access to legal information.\n\nThis is done by:\n* Voice Enabled interactions\n* SMS Notification\n* Kinyarwanda Support\n\n\n<p align=\"right\">(<a href=\"#readme-top\">back to top</a>)</p>\n\n\n<!-- GETTING STARTED -->\n## Getting Started\n\nWe welcome any contribution to the website and community as a whole. Bellow is an introduction on how to get started.\n\nContribution areas:\n* Content Creator: We look forward to onboarding a content creator for our blog.\n* Content Moderator: We look forward to onboarding a content creator for our blog.\n\nPlease feel free to also suggest any other area you might be intrested to contribute\n\n\n### Built With\n\nThe chatbot is built using the following technologies! We welcome any suggestion on how to improve this items.\n\n* [![Python][python.org]][python-url] \n* [![Linode][linode.com]][linode-url]  \n* [![AWS][aws.amazon.com]][aws-url] \n\n<p align=\"right\">(<a href=\"#readme-top\">back to top</a>)</p>\n\n### Prerequisites\n\nTo be honest, there are no pre-requisite!\n\n\n<!-- ROADMAP -->\n## Roadmap\n\n- [x] Initial Commit\n\n\nSee the [open issues](https://github.com/agent87/ihugura-api/issues) for a full list of proposed features (and known issues).\n\n<p align=\"right\">(<a href=\"#readme-top\">back to top</a>)</p>\n\n\n\n<!-- CONTRIBUTING -->\n## Contributing\n\nContributions are what make this open source repository such an amazing place to learn, inspire, and create. Any contributions you make are **greatly appreciated**.\n\nIf you have a suggestion that would make this better, please fork the repo and create a pull request. You can also simply open an issue with the tag \"enhancement\".\nDon't forget to give the project a star! Thanks again!\n\n1. Fork the Project\n2. Create your Feature Branch (`git checkout -b AmazinFeature`)\n3. Commit your Changes (`git commit -m 'Add some AmazingFeature'`)\n4. Push to the Branch (`git push origin dev`)\n5. Open a Pull Request to the dev branch\n\n<p align=\"right\">(<a href=\"#readme-top\">back to top</a>)</p>\n\n\n\n<!-- CONTACT -->\n## Contact\n\n* Arnaud Kayonga - www.kayarn.com\n\n<p align=\"right\">(<a href=\"#readme-top\">back to top</a>)</p>\n\n\n\n<!-- MARKDOWN LINKS & IMAGES -->\n<!-- https://www.markdownguide.org/basic-syntax/#reference-style-links -->\n[contributors-shield]: https://img.shields.io/github/contributors/agent87/ihugura-api\n[contributors-url]: https://github.com/agent87/ihugura-api/graphs/contributors\n[forks-shield]: https://img.shields.io/github/forks/agent87/ihugura-api\n[forks-url]: agent87/ihugura-api\n[stars-shield]: https://img.shields.io/github/stars/agent87/ihugura-api\n[stars-url]: agent87/ihugura-api\n[issues-shield]: https://img.shields.io/github/issues/agent87/ihugura-api\n[issues-url]: https://github.com/agent87/ihugura-api/issues\n[slack-shield]: https://img.shields.io/badge/Slack-4A154B?style=for-the-badge&logo=slack&logoColor=white\n[slack-url]: https://join.slack.com/t/mbazanlpcommunity/shared_invite/zt-1e5mxv2x2-XH25edKoZ4tFZou4SvLsQA\n[python.org]: https://img.shields.io/badge/Python-14354C?style=for-the-badge&logo=python&logoColor=white\n[python-url]: https://getbootstrap.com\n[linode.com]: https://img.shields.io/badge/Linode-00A95C?style=for-the-badge&logo=Linode&logoColor=white\n[linode-url]: https://linode.com\n[aws.amazon.com]: https://img.shields.io/badge/Amazon_AWS-232F3E?style=for-the-badge&logo=amazon-aws&logoColor=white\n[aws-url]: https://aws.amazon.com"
    },
    {
      "name": "LeaS2/Explainable_NLP",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/105351249?s=40&v=4",
      "owner": "LeaS2",
      "repo_name": "Explainable_NLP",
      "description": null,
      "homepage": null,
      "language": "PHP",
      "created_at": "2022-11-09T11:08:00Z",
      "updated_at": "2024-06-06T00:42:05Z",
      "topics": [],
      "readme": "# Explainable_NLP"
    },
    {
      "name": "AlbertoVilla87/web-file",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/77957428?s=40&v=4",
      "owner": "AlbertoVilla87",
      "repo_name": "web-file",
      "description": "Web-file scrapes information from internet and allows the user to ask and be answered from the data extracted.",
      "homepage": "",
      "language": "Jupyter Notebook",
      "created_at": "2022-12-22T09:09:39Z",
      "updated_at": "2023-01-05T11:59:29Z",
      "topics": [
        "elasticsearch",
        "haystack",
        "question-answering",
        "transformers"
      ],
      "readme": "# web-file\n\nWeb-file scrapes information from internet and allows the user to ask and be answered from the data extracted. \n\n\n\n## Installing\n\n1. Build directories\n\n```sh\nmake build_folders\n```\n\n2. Create virtual env:\n\n```sh\nmake env\n```\n\n3. Activate virual env\n\n```sh\nsource .venv/bin/activate \n```\n\n4. Upgrade pip\n\n```sh\nsource python -m pip install --upgrade pip\n```\n\n5. Install dependencies\n```sh\npython -m pip install -r requirements.txt\n```\n\n## Dependencies\n\n### 1. Installing QA Model\n\nhttps://huggingface.co/deepset/minilm-uncased-squad2\n\n```sh\ngit lfs install\ngit clone https://huggingface.co/deepset/minilm-uncased-squad2\n```\n\n### 2. Installing Docker\n\nhttps://docs.docker.com/desktop/install/mac-install/\n\n### 3. Installing Haystack\n\nhttps://pypi.org/project/farm-haystack/\n\n```sh\ngit clone https://github.com/deepset-ai/haystack.git\ncd haystack\n```\n\nFor Mac M1:\n\n```sh\nbrew install postgresql\nbrew install cmake\nbrew install rust\n\n# haystack installation\nGRPC_PYTHON_BUILD_SYSTEM_ZLIB=true pip install git+https://github.com/deepset-ai/haystack.git\n```\n## Demo\n\n```sh\ndocker-compose up\ndocker start elasticsearch > /dev/null 2>&1 || docker run -d -p 9200:9200 -e \"discovery.type=single-node\" --name elasticsearch elasticsearch:7.17.6\ndocker-compose run webfile python -m scripts.store_transcripts_to_elastic\ndocker-compose up -d\n```\n\nhttps://learn.microsoft.com/es-es/azure/container-instances/tutorial-docker-compose\nhttps://docs.docker.com/cloud/aci-integration/\n\n![gif](demo.gif)\n"
    },
    {
      "name": "KKogaa/jester-bot",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/64268254?s=40&v=4",
      "owner": "KKogaa",
      "repo_name": "jester-bot",
      "description": "Custom discord bot.",
      "homepage": null,
      "language": "Python",
      "created_at": "2022-12-11T02:06:19Z",
      "updated_at": "2024-10-19T22:53:01Z",
      "topics": [],
      "readme": "# Jester\n## _AI Powered discord bot_\n\n![Jester](https://raw.githubusercontent.com/KKogaa/jester-bot/main/resources/logo.jpeg \"Jester\")\n\nJester is a discord bot for a personal project.\n\n## Features\n- Youtube player\n- Admin commands\n- Meme database (pending)\n- Copy pasta database (pending)\n- Image generation (Stable diffusion) (pending)\n- Question answering (small Bert variant) (pending)\n\n## Installation\nCreate a .env file with the discord token.\n```sh\necho \"discord_token = \\\"DISCORD_TOKEN\\\"\" > .env\n```\n\nInstall the dependencies and start the server.\n```sh\npython3 -m venv venv\nsource venv/bin/activate && pip install -r requirements.txt\n```\n\n\n### Works on \nLinux\n\n## License\n\nMIT\n\n**Free Software, Hell Yeah!**\n\n\n\n\n\n\n\n"
    },
    {
      "name": "beamscource/nlp_apps",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/20405070?s=40&v=4",
      "owner": "beamscource",
      "repo_name": "nlp_apps",
      "description": "Simple apps using modern NLP frameworks",
      "homepage": null,
      "language": "Python",
      "created_at": "2022-12-15T11:58:34Z",
      "updated_at": "2023-03-30T11:46:12Z",
      "topics": [],
      "readme": ""
    },
    {
      "name": "trijini/Natural-Language-Processing-with-Transformers-Lewis-Tunstall-Leandro-von-Werra-Thomas-Wolf-z-li",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/110159747?s=40&v=4",
      "owner": "trijini",
      "repo_name": "Natural-Language-Processing-with-Transformers-Lewis-Tunstall-Leandro-von-Werra-Thomas-Wolf-z-li",
      "description": null,
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2022-12-03T13:02:07Z",
      "updated_at": "2023-03-08T00:30:01Z",
      "topics": [],
      "readme": "# Transformers Notebooks\n\nThis repository contains the example code from our O'Reilly book [Natural Language Processing with Transformers](https://www.oreilly.com/library/view/natural-language-processing/9781098136789/):\n\n<img alt=\"book-cover\" height=200 src=\"images/book_cover.jpg\" id=\"book-cover\"/>\n\n## Getting started\n\nYou can run these notebooks on cloud platforms like [Google Colab](https://colab.research.google.com/) or your local machine. Note that most chapters require a GPU to run in a reasonable amount of time, so we recommend one of the cloud platforms as they come pre-installed with CUDA.\n\n### Running on a cloud platform\n\nTo run these notebooks on a cloud platform, just click on one of the badges in the table below:\n\n<!--This table is automatically generated, do not fill manually!-->\n\n\n\n| Chapter                                     | Colab                                                                                                                                                                                               | Kaggle                                                                                                                                                                                                   | Gradient                                                                                                                                                                               | Studio Lab                                                                                                                                                                                                   |\n|:--------------------------------------------|:----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Introduction                                | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/nlp-with-transformers/notebooks/blob/main/01_introduction.ipynb)              | [![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/nlp-with-transformers/notebooks/blob/main/01_introduction.ipynb)              | [![Gradient](https://assets.paperspace.io/img/gradient-badge.svg)](https://console.paperspace.com/github/nlp-with-transformers/notebooks/blob/main/01_introduction.ipynb)              | [![Open In SageMaker Studio Lab](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/nlp-with-transformers/notebooks/blob/main/01_introduction.ipynb)              |\n| Text Classification                         | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/nlp-with-transformers/notebooks/blob/main/02_classification.ipynb)            | [![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/nlp-with-transformers/notebooks/blob/main/02_classification.ipynb)            | [![Gradient](https://assets.paperspace.io/img/gradient-badge.svg)](https://console.paperspace.com/github/nlp-with-transformers/notebooks/blob/main/02_classification.ipynb)            | [![Open In SageMaker Studio Lab](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/nlp-with-transformers/notebooks/blob/main/02_classification.ipynb)            |\n| Transformer Anatomy                         | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/nlp-with-transformers/notebooks/blob/main/03_transformer-anatomy.ipynb)       | [![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/nlp-with-transformers/notebooks/blob/main/03_transformer-anatomy.ipynb)       | [![Gradient](https://assets.paperspace.io/img/gradient-badge.svg)](https://console.paperspace.com/github/nlp-with-transformers/notebooks/blob/main/03_transformer-anatomy.ipynb)       | [![Open In SageMaker Studio Lab](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/nlp-with-transformers/notebooks/blob/main/03_transformer-anatomy.ipynb)       |\n| Multilingual Named Entity Recognition       | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/nlp-with-transformers/notebooks/blob/main/04_multilingual-ner.ipynb)          | [![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/nlp-with-transformers/notebooks/blob/main/04_multilingual-ner.ipynb)          | [![Gradient](https://assets.paperspace.io/img/gradient-badge.svg)](https://console.paperspace.com/github/nlp-with-transformers/notebooks/blob/main/04_multilingual-ner.ipynb)          | [![Open In SageMaker Studio Lab](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/nlp-with-transformers/notebooks/blob/main/04_multilingual-ner.ipynb)          |\n| Text Generation                             | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/nlp-with-transformers/notebooks/blob/main/05_text-generation.ipynb)           | [![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/nlp-with-transformers/notebooks/blob/main/05_text-generation.ipynb)           | [![Gradient](https://assets.paperspace.io/img/gradient-badge.svg)](https://console.paperspace.com/github/nlp-with-transformers/notebooks/blob/main/05_text-generation.ipynb)           | [![Open In SageMaker Studio Lab](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/nlp-with-transformers/notebooks/blob/main/05_text-generation.ipynb)           |\n| Summarization                               | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/nlp-with-transformers/notebooks/blob/main/06_summarization.ipynb)             | [![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/nlp-with-transformers/notebooks/blob/main/06_summarization.ipynb)             | [![Gradient](https://assets.paperspace.io/img/gradient-badge.svg)](https://console.paperspace.com/github/nlp-with-transformers/notebooks/blob/main/06_summarization.ipynb)             | [![Open In SageMaker Studio Lab](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/nlp-with-transformers/notebooks/blob/main/06_summarization.ipynb)             |\n| Question Answering                          | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/nlp-with-transformers/notebooks/blob/main/07_question-answering.ipynb)        | [![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/nlp-with-transformers/notebooks/blob/main/07_question-answering.ipynb)        | [![Gradient](https://assets.paperspace.io/img/gradient-badge.svg)](https://console.paperspace.com/github/nlp-with-transformers/notebooks/blob/main/07_question-answering.ipynb)        | [![Open In SageMaker Studio Lab](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/nlp-with-transformers/notebooks/blob/main/07_question-answering.ipynb)        |\n| Making Transformers Efficient in Production | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/nlp-with-transformers/notebooks/blob/main/08_model-compression.ipynb)         | [![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/nlp-with-transformers/notebooks/blob/main/08_model-compression.ipynb)         | [![Gradient](https://assets.paperspace.io/img/gradient-badge.svg)](https://console.paperspace.com/github/nlp-with-transformers/notebooks/blob/main/08_model-compression.ipynb)         | [![Open In SageMaker Studio Lab](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/nlp-with-transformers/notebooks/blob/main/08_model-compression.ipynb)         |\n| Dealing with Few to No Labels               | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/nlp-with-transformers/notebooks/blob/main/09_few-to-no-labels.ipynb)          | [![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/nlp-with-transformers/notebooks/blob/main/09_few-to-no-labels.ipynb)          | [![Gradient](https://assets.paperspace.io/img/gradient-badge.svg)](https://console.paperspace.com/github/nlp-with-transformers/notebooks/blob/main/09_few-to-no-labels.ipynb)          | [![Open In SageMaker Studio Lab](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/nlp-with-transformers/notebooks/blob/main/09_few-to-no-labels.ipynb)          |\n| Training Transformers from Scratch          | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/nlp-with-transformers/notebooks/blob/main/10_transformers-from-scratch.ipynb) | [![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/nlp-with-transformers/notebooks/blob/main/10_transformers-from-scratch.ipynb) | [![Gradient](https://assets.paperspace.io/img/gradient-badge.svg)](https://console.paperspace.com/github/nlp-with-transformers/notebooks/blob/main/10_transformers-from-scratch.ipynb) | [![Open In SageMaker Studio Lab](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/nlp-with-transformers/notebooks/blob/main/10_transformers-from-scratch.ipynb) |\n| Future Directions                           | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/nlp-with-transformers/notebooks/blob/main/11_future-directions.ipynb)         | [![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/nlp-with-transformers/notebooks/blob/main/11_future-directions.ipynb)         | [![Gradient](https://assets.paperspace.io/img/gradient-badge.svg)](https://console.paperspace.com/github/nlp-with-transformers/notebooks/blob/main/11_future-directions.ipynb)         | [![Open In SageMaker Studio Lab](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/nlp-with-transformers/notebooks/blob/main/11_future-directions.ipynb)         |\n\n<!--End of table-->\n\nNowadays, the GPUs on Colab tend to be K80s (which have limited memory), so we recommend using [Kaggle](https://www.kaggle.com/docs/notebooks), [Gradient](https://gradient.run/notebooks), or [SageMaker Studio Lab](https://studiolab.sagemaker.aws/). These platforms tend to provide more performant GPUs like P100s, all for free!\n\n> Note: some cloud platforms like Kaggle require you to restart the notebook after installing new packages.\n\n### Running on your machine\n\nTo run the notebooks on your own machine, first clone the repository and navigate to it:\n\n```bash\n$ git clone https://github.com/nlp-with-transformers/notebooks.git\n$ cd notebooks\n```\n\nNext, run the following command to create a `conda` virtual environment that contains all the libraries needed to run the notebooks:\n\n```bash\n$ conda env create -f environment.yml\n```\n\n> Note: You'll need a GPU that supports NVIDIA's [CUDA Toolkit](https://developer.nvidia.com/cuda-toolkit) to build the environment. Currently, this means you cannot build locally on Apple silicon 😢.\n\nChapter 7 (Question Answering) has a special set of dependencies, so to run that chapter you'll need a separate environment:\n\n```bash\n$ conda env create -f environment-chapter7.yml\n```\n\nOnce you've installed the dependencies, you can activate the `conda` environment and spin up the notebooks as follows:\n\n```bash\n$ conda activate book # or conda activate book-chapter7\n$ jupyter notebook\n```\n\n## FAQ\n\n### When trying to clone the notebooks on Kaggle I get a message that I am unable to access the book's Github repository. How can I solve this issue?\n\nThis issue is likely due to a missing internet connection. When running your first notebook on Kaggle you need to enable internet access in the settings menu on the right side. \n\n### How do you select a GPU on Kaggle?\n\nYou can enable GPU usage by selecting *GPU* as *Accelerator* in the settings menu on the right side.\n\n## Citations\n\nIf you'd like to cite this book, you can use the following BibTeX entry:\n\n```\n@book{tunstall2022natural,\n  title={Natural Language Processing with Transformers: Building Language Applications with Hugging Face},\n  author={Tunstall, Lewis and von Werra, Leandro and Wolf, Thomas},\n  isbn={1098103246},\n  url={https://books.google.ch/books?id=7hhyzgEACAAJ},\n  year={2022},\n  publisher={O'Reilly Media, Incorporated}\n}\n```\n"
    },
    {
      "name": "Jackiebibili/algo-chatbox-nlp",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/61096708?s=40&v=4",
      "owner": "Jackiebibili",
      "repo_name": "algo-chatbox-nlp",
      "description": "Introduction to Algorithms by CLRS (Prof. Chida's UT Dallas Advanced Algorithms class) chatbot that can answer student questions related to the class syllabus, algorithms in general, and Big-O function comparison",
      "homepage": "",
      "language": "Python",
      "created_at": "2022-11-29T17:44:00Z",
      "updated_at": "2022-12-15T01:23:24Z",
      "topics": [
        "bert-embeddings",
        "chatbot",
        "clrs",
        "elasticsearch",
        "haystack",
        "intro-to-algorithms"
      ],
      "readme": "# Intro to Algorithms Class Chatbot\n\n## Prerequisites for Development\n- python 3.9 and pip\n   - tensorflow\n   - tensorflow_hub\n   - tensorflow-text\n   - (See details in \"How to Run the Project\" about installing other dependencies)\n- Haystack (see installation [instructions](https://docs.haystack.deepset.ai/docs/installation))\n   - Apple M1 has [extra dependencies](https://docs.haystack.deepset.ai/docs/installation#apple-silicon-m1)\n- Elasticsearch host (see details in \"Miscellaneous\" about elasticsearch)\n\n## Prerequisites for Deployment\n- Docker\n- Elasticsearch host (see details in \"Miscellaneous\" about elasticsearch)\n\n## How to Run the Project for Development\n- Change directory to `src/server`\n- Run shell script `run.sh` to prepare and store in your elasticsearch host the information of\n   - i) question and answer (Q&A) pairs,\n   - ii) algorithm general knowledge base and\n   - iii) syllabus knowledge base.\n- Install `pipenv` by `pip install pipenv`\n- Run `pipenv install` to install backend dependencies listed in the Pipfile.lock\n- Run `python -m src.api.app` to start the Flask API, which then initiates the NLP pipeline\n\n## Project Deployment\n- Change directory to `src/server`\n- Run shell script `prod.sh` to build an image of the API and the NLP pipeline. Then, the backend will be running in a docker container. __It has been tested that in Linux x86 system the docker image can be successfully built.__\n- It is suggested that backend server should have a __GPU__ for faster inferencing time.\n\n## Architecture\nNLP pipeline visualization:\\\n\\\n![NLP pipeline visualization](docs/chatbox-nlp-flow.png)\n\n## Performance of QueryClassifier\nPrecision = 1.00\\\nRecall = 0.83\\\nF1 = 0.91\\\nConfusion Matrix:\\\n\\\n![Confusion Matrix](docs/confusion_matrix.png)\n\n## Miscellaneous\n- In Fall 2022, the backend is hosted on the Google Cloud Platform. We utilized a Nvidia Tesla P100 (225W, 16G RAM) and 2 vCPU with 7.5G RAM. This results in 5s response time for each request on average.\n   - A elasticsearch host should be running locally or on the cloud. We used to have a free-trial cloud elasticsearch host provided by [elastic.co](https://www.elastic.co/). Alternatively, elasticsearch instance can also be hosted in a docker container.\n"
    },
    {
      "name": "setren/fastapi",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/70955964?s=40&v=4",
      "owner": "setren",
      "repo_name": "fastapi",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2022-11-22T09:47:18Z",
      "updated_at": "2023-03-07T09:50:00Z",
      "topics": [],
      "readme": ""
    },
    {
      "name": "gziz/question-answer-ai",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/69287011?s=40&v=4",
      "owner": "gziz",
      "repo_name": "question-answer-ai",
      "description": "Deep Learning Transformer Q&A WebApp",
      "homepage": "https://questionanswer.gerardomz.com/",
      "language": "JavaScript",
      "created_at": "2022-08-07T20:30:26Z",
      "updated_at": "2022-12-03T06:53:21Z",
      "topics": [],
      "readme": "# Question Answering AI\n\nWeb application implementing the RoBERTa NLP model to answer questions given text(context) input.\nRoBERTa is an open source transformer model used for various NLP tasks such as text classification and Q&A.\n\n  <p align=\"center\">\n   <img src=\"https://drive.google.com/uc?export=view&id=1yqVP4urxjha7-VrACUHkS7Kh_vumelW4\" width=\"750\" height=\"auto\">\n  </p>\n\n## Requirements\n* Two docker containers part of the same network\n  * Python API container (FastAPI, RoBERTa, Query DB)\n  * Elastic Search container (For paragraph ranking)\n  \nThe approach is simple, given that transformers models are not optimized for a large corpus, such as hundreds of paragraphs, we first pass the text file through Elasticsearch, then, the answer is extracted from the ranked pagraphs.\n\n## Get Started\n  \n1. Create the docker network (connect API with ElasticSearch)\n```\ndocker network create qa-net\n```\n\n2. Build and start the API image\n```\ndocker build -t qa-img -f Dockerfile .\n```\n```\ndocker run -d --network qa-net -e PORT=8000 -p 80:8000 qa-img\n```\n\n\n3. Pull and start the Elastic Search container with Docker\n```\ndocker pull docker.elastic.co/elasticsearch/elasticsearch:7.17.6\n```\n\n```\ndocker run -d --name es01 --network qa-net -d -p 127.0.0.1:9200:9200 -p 127.0.0.1:9300:9300 -e \"discovery.type=single-node\" docker.elastic.co/elasticsearch/elasticsearch:7.17.6\n```\nNote: The name of the ElasticSearch container is the address the API will make requests to.\n\n\n4. Modify the baseURL in apis/axiosManager.js with your local port (where the API is running).\n```\nbaseURL = \"http://127.0.0.1:<your port>\"\n```\n\n\n## How does it work?\n1. Type a context and a question, the latter should be retrieved from the former.\n  * If desired, the app can store your query for metrics purposes.\n  \n  <p align=\"center\">\n   <img src=\"https://drive.google.com/uc?export=view&id=1x_gUWMIJzEy2UiMvFdMxxEmBYnuDTRPJ\"  width=\"550\" height=\"auto\">\n    <img src=\"https://drive.google.com/uc?export=view&id=1OmyXPNkj2dvG5EA6vBoXsbQsp4kt3jrL\"  width=\"550\" height=\"auto\">\n  </p>\n  \n  \n  * The AI understood that asking for Bill is also asking for Smith (same person). Not only that, but the text does not explicitly specifies the age of Bill (no \"Bill is 36 years old...\", no \"Bill's age is ...\"), it understood from the context that 36 is Bill's age.\n  \n  \n2. Additionally, provide a text file (PDF) as context.\n    * When uploading the file, the app will read and process the file, then store the text in ElasticSearch.\n    * The app can now receive questions where the response may be in the text file.\n    * ElasticSearch will rank paragraphs using BM25, then RoBERTa will extract the answer from these.\n\n  <p align=\"center\">\n   <img src=\"https://drive.google.com/uc?export=view&id=1VHj924euaUxzPUdl7sYjMYk_sXKaEuMY\"  width=\"750\" height=\"auto\">\n   <img src=\"https://drive.google.com/uc?export=view&id=1urltXd9zcKOaDYIUBMVC5TAiIMheQN7H\"  width=\"750\" height=\"auto\">\n   <img src=\"https://drive.google.com/uc?export=view&id=13pyeq5lw-IPGH69CHuLqqVH_wax89fKE\"  width=\"750\" height=\"auto\">\n  </p>\n\n\n\n"
    },
    {
      "name": "descentis/contract-review",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/3228021?s=40&v=4",
      "owner": "descentis",
      "repo_name": "contract-review",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2022-07-06T14:38:48Z",
      "updated_at": "2023-10-26T23:37:42Z",
      "topics": [],
      "readme": ""
    },
    {
      "name": "Saranga99/data-science-projects",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/65158515?s=40&v=4",
      "owner": "Saranga99",
      "repo_name": "data-science-projects",
      "description": "This is my main repository for Data Science/Machine Learning. Feel free to use it for your learning. Check out my turning point here: https://www.youtube.com/watch?v=4q9RCT1ouI0",
      "homepage": "",
      "language": "Jupyter Notebook",
      "created_at": "2021-04-28T23:40:03Z",
      "updated_at": "2024-05-01T16:23:44Z",
      "topics": [
        "data-science",
        "jupyter-notebook",
        "machine-learning",
        "project",
        "python3"
      ],
      "readme": "# data-science-projects\nThis is My Data Science / Machine Learning  main learning repository and you also can use this for your learning purpose.\n\nhttps://bmc.link/saranga99\n\n## ML Roadmap\n\n![Image of Yaktocat](https://scikit-learn.org/stable/_static/ml_map.png)\n"
    },
    {
      "name": "doinakis/Real-Time-News-Assistant",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/28433709?s=40&v=4",
      "owner": "doinakis",
      "repo_name": "Real-Time-News-Assistant",
      "description": "Real Time News Asstistant for Greek news.",
      "homepage": "",
      "language": "TeX",
      "created_at": "2022-04-10T10:32:39Z",
      "updated_at": "2025-03-08T20:02:50Z",
      "topics": [
        "bert",
        "haystack",
        "natural-language-processing",
        "nlp",
        "question-answering",
        "rasa"
      ],
      "readme": "# Real Time News Assistant\r\n\r\nReal Time News Assistant for greek news. (Aristotle University of Thessaloniki [Diploma Thesis](https://ikee.lib.auth.gr/record/340592/?ln=el))\r\n\r\n## How to run the project\r\nFirst of all it is advised to use a python virtual environment to run this project. The version of python used was 3.8.10\r\n\r\n### Create Virtual Environment\r\n```\r\npython3 -m venv /path/to/new/virtual/environment\r\n```\r\nMake sure the virtual environment is enabled and then run:\r\n```\r\npip3 install -r requirements.txt\r\n```\r\nThis will install all the required python modules.\r\n\r\n### Set up the database\r\nIn order to set up the elasticsearch database you can use haystacks abstraction that creates and runs a docker container with the default configuration. In a python script just run:\r\n```\r\nfrom haystack.utils import launch_es\r\nlaunch_es()\r\n```\r\nThis will create a docker container with an elasticsearch database running at http://localhost:9200\r\n\r\nYou can then add documents using the API provided in the QASystem as follows:\r\n```\r\ndb = Database()\r\ndb.connect()\r\ndb.add_documents(docs)\r\n```\r\nWhere docs is a list of dictionaries where each dictionary is a document. To connect with custom creadentials or to a different port check the API. After the initialization of the database we can run the action server.\r\n\r\nFor more customization of the database you will need to set up the container as described [here](https://www.elastic.co/guide/en/elasticsearch/reference/current/docker.html)\r\n### Run the Action server\r\nIn order to run the action server open a terminal in the rasa folder with the venv activated and run:\r\n```\r\nrasa run actions\r\n```\r\nMake sure the actions endpoint is uncommented in the endpoint.yml file\r\n\r\n### Set up RASA-X\r\nIn order to set up RASA-X the RASA Ephemeral Installer was used. More details are provided [here](https://github.com/RasaHQ/REI).\r\nRun:\r\n```\r\nbash rei.sh -y\r\nrasactl start rasa-x --values-file values.yml\r\n```\r\nAlso make sure you have set up the existingUrl in the values.yml file to the endpoint of the action server. You will need to port forward the IP of the action server in order for the RASA-X to be able to access the action server we set up earlier.\r\n\r\nThe assistant can be trained and activated using RASA-X.\r\n\r\n## Reference\r\n\r\n```\r\n@article{Artetxe:etal:2019,\r\n  author        = {Mikel Artetxe and Sebastian Ruder and Dani Yogatama},\r\n  title         = {On the cross-lingual transferability of monolingual representations},\r\n  journal       = {CoRR},\r\n  volume        = {abs/1910.11856},\r\n  year          = {2019},\r\n  archivePrefix = {arXiv},\r\n  eprint        = {1910.11856}\r\n}\r\n```\r\n```\r\n@Article{Devlin2019,\r\n  author        = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},\r\n  journal       = {arXiv:1810.04805 [cs]},\r\n  title         = {{BERT}: {Pre}-training of {Deep} {Bidirectional} {Transformers} for {Language} {Understanding}},\r\n  year          = {2019},\r\n  month         = may,\r\n  note          = {arXiv: 1810.04805},\r\n  abstract      = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5\\% (7.7\\% point absolute improvement), MultiNLI accuracy to 86.7\\% (4.6\\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},\r\n  file          = {:Devlin2019 - BERT_ Pre Training of Deep Bidirectional Transformers for Language Understanding.pdf:PDF},\r\n  keywords      = {Computer Science - Computation and Language},\r\n  priority      = {prio1},\r\n  readstatus    = {read},\r\n  shorttitle    = {{BERT}},\r\n  url           = {http://arxiv.org/abs/1810.04805},\r\n  urldate       = {2021-12-09},\r\n}\r\n```\r\n```\r\n @misc{rasa_2021,\r\n  title         = {Open source conversational AI},\r\n  url           = {https://rasa.com/},\r\n  journal       = {Rasa},\r\n  year          = {2021},\r\n  month         = {Nov},\r\n  note          = {Accessed: 2022-04-28}\r\n}\r\n```\r\n```\r\n @misc{haystack docs,\r\n  title         = {Haystack docs},\r\n  url           = {https://haystack.deepset.ai/},\r\n  journal       = {Haystack Docs},\r\n  year          = {2021},\r\n  month         = {Dec},\r\n  note          = {Accessed: 2022-04-28}\r\n}\r\n```\r\n"
    },
    {
      "name": "amitbiswas1999/Covid-QA-system-using-BERT-",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/88819794?s=40&v=4",
      "owner": "amitbiswas1999",
      "repo_name": "Covid-QA-system-using-BERT-",
      "description": "Develop a BERT Q&A system that provides real-time answers from over 200,000 COVID research papers",
      "homepage": "",
      "language": "Jupyter Notebook",
      "created_at": "2022-04-25T18:53:39Z",
      "updated_at": "2022-04-28T07:28:23Z",
      "topics": [],
      "readme": "\n# Build and Deploy a COVID Q&A System using BERT\n\nDevelop a BERT Q&A system that provides real-time answers from over 200,000 COVID research papers\n\nSteps:-\n   \n   1)Perform Exploratory data analysis and Topic modelling on the COVID dataset\n\n   2)Create an indexed database of COVID research text in Elasticsearch\n\n   3)Use haystack that retrieves and ranks candidate documents from Elasticsearch\n\n   4)Develop the BERT Q&A Engine\n\n   5)Building a simple UI with Streamlit\n    \n   6)Dockerize and deploy models\n\n\n## Demo\n![73adfae6e6069b978bff43e336891194b43daa01](https://user-images.githubusercontent.com/88819794/165460571-07460a0b-1721-4d6a-9afc-7bf1fc55bdd1.gif)\n\n## Dataset\n\n[Download Covid Dataset from here](https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/historical_releases/cord-19_2021-11-15.tar.gz\n) \n\n## Tools\nhere I have used haystack which is an open-source framework for building search systems that work intelligently over large document collections.\nRecent advances in NLP have enabled the application of question answering, retrieval and summarization to real world settings and Haystack is designed to be the bridge between research and industry.\nFor more explanation please refer [this docs](https://haystack.deepset.ai/overview/intro)\n![image](https://user-images.githubusercontent.com/88819794/165554750-7be0322b-02b9-4ebf-94af-5231e5e78d31.png)\n\n\n"
    },
    {
      "name": "bhakthil/student-assist",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/12928110?s=40&v=4",
      "owner": "bhakthil",
      "repo_name": "student-assist",
      "description": null,
      "homepage": null,
      "language": "HTML",
      "created_at": "2021-10-21T04:57:45Z",
      "updated_at": "2022-03-01T23:23:46Z",
      "topics": [],
      "readme": "# Ask-CCI Bot\n\nThis repo contains the rasa bot project: **Ask-CCI Student Assist Bot** that was originally developed as part of the course ITIS-6112-8112 and has been further expanded for course DSBA-6345. This project was further modfied for the demo at ATL Dev Con 2022\n\n## Table of Contents\n\n- [Project Setup](https://github.com/bhakthil/student-assist#project-setup)\n- [Installing Dependencies](https://github.com/bhakthil/student-assist#installing-dependencies)\n- [How to Run The Chatbot](https://github.com/bhakthil/student-assist#how-to-run-the-chatbot)\n- [How to Train the Chatbot](https://github.com/bhakthil/student-assist#how-to-train-the-chatbot)\n  - [NLU](https://github.com/bhakthil/student-assist#datanluyml)\n  - [Stories](https://github.com/bhakthil/student-assist#datastoriesyml)\n  - [Domain](https://github.com/bhakthil/student-assist#datadomainyml)\n- [How to Create a Trained Model](https://github.com/bhakthil/student-assist#how-to-create-a-trained-model)\n- [Running Haystack Service](https://github.com/bhakthil/student-assist#running-haystack-service)\n- [Running API Tests](https://github.com/bhakthil/student-assist#running-api-tests)\n\n## Project Setup\n\n1. Clone this project to your local system via:\n\n    ```bash\n    git clone https://github.com/bhakthil/student-assist.git\n    ```\n\n2. Change to the new directory:\n\n    ```bash\n    cd student-assist\n    ```\n\n3. Create a new virtual environment:\n\n    ```bash\n    python3 -m venv .venv\n    ```\n\n4. Activate the virtual environment\n\n    ```bash\n    source .venv/bin/activate\n    ```\n\n## Installing Dependencies\n\n1. Update pip to the latest version\n\n   ```bash\n   pip install -U pip\n   ```\n\n2. Install `requirements.txt`:\n\n    ```bash\n    pip install -r requirements.txt\n    ```\n    **NOTE:** If you encounter any errors please run the command again.\n\n3. Install Docker\n    - Please check Docker site for most recent installation instructions: https://docs.docker.com/desktop/\n\n---\n\n## How to Train The Chatbot\n\nIn order for the bot to understand different intents, it needs to be trained with sample utterances for each new intent.\n\nThere are few places that needs to be changed for a new intent to be added to the system.\n\n## ./data/nlu.yml\n\nPlease add at least ten sample utterances for each of the intent you are responsible for. For an example, below markup is used to define `thank_you` intent.\n\n  ```yml\n  - intent: thank_you\n    examples: |\n    - Thanks\n    - Thank You\n    - thanks\n    - thanks a lot\n    - thank you so much\n  ```\n\n## ./data/stories.yml\n\nEach intent should have at least 2 paths; **happy path** **and unhappy path**. So, please use below example to create paths for your intents.\n\n```yml\n- story: phd info happy path\n  steps:\n  - intent: phd_info\n  - action: utter_phd_info\n  - action: utter_did_that_help\n  - intent: affirm\n  - action: utter_happy\n  - action: utter_continue\n\n- story: phd info unhappy path\n  steps:\n  - intent: phd_info\n  - action: utter_phd_info\n  - action: utter_did_that_help\n  - intent: deny\n  - action: utter_helpdesk\n  .\n  .\n  .\n- story: **YOU_WILL_APPEND_YOUR_STORIES_HERE**\n  steps:\n  - intent: [new intent]\n  - action: [new action]\n  - action: ...\n```\n\n## ./domain.yml\n\nDomain file is where you define the responses for each intent. You will add responses for each of your intent under `responses` section.\n\neach response corrosponding to you utterance should be named in the format `utter_xxxx` where xxxx is the intent name that the respnse corrosponds to.\n\nFor example, the response corrosponding to `phd_info` intent is named as `utter_phd_info`.\n\n```yml\nresponses:\n  utter_phd_info:\n  - text: \"You can get more information about our PhD program at <a href='https://bit.ly/3whwJFL'>Link</a>\"\n\n  utter_msc_info:\n  - text: \"You can get more information about our MSc program at <a href='https://bit.ly/3GMPPIV'>Link</a>\"\n  .\n  .\n  .\n  .\n  utter_<YOUR_NEW_UTTERANCE >:\n  - text: \"YOUR NEW RESPONSE\"\n\n```\n\nIn addition, you will add your intent names under the `intents` section of the domain file.\n\n```yml\nintents:\n  - greet\n  - goodbye\n  - affirm\n  - deny\n  - mood_great\n  - mood_unhappy\n  - bot_challenge\n  - thank_you\n###################\n  - phd_info\n  - msc_info\n  - **YOU_WILL_APPEND_YOUR_INTENT(S)**\n```\n\n#### Training the Model\n\nOnce you finish adding the intents and the strories, you will be able to integrate the new additions/edits to your bot by re-training the bot.\n\nUse below command to re-train the model:\n\n```bash\nrasa train\n```\n\n#### Testing the Model\n\nOnce you finish training your bot, you can perform a quick test by running the bot in interactive mode.\n\nUse below command to start the bot in ineractive mode:\n\n```bash\nrasa shell --debug\n```\n![image](https://user-images.githubusercontent.com/12928110/165818077-81a72ed8-23ef-445c-8f07-6649b76fd03f.png)\n\n---\n## How to Run The Chatbot\n\n\n### One-line operation\n\n1. To launch all services from one script, run the following:\n\n    ```bash\n    . ./run.sh\n    ```\n\n  To run each part of the system indiviually, use the following set(s) of instructions\n\n### Starting separate services at a time\n\n1. First, you must start the Rasa action server:\n  \n   ```bash\n   rasa run actions\n   ```\n\n   - If the action server is running, you should be able to see the below message:\n\n        ```bash\n      (.venv) ➜  student-assist git:(dev) ✗ rasa run actions\n      2022-03-09 14:41:23 INFO     rasa_sdk.endpoint  - Starting action endpoint server...\n      2022-03-09 14:41:23 INFO     rasa_sdk.executor  - Registered function for 'action_info_retrieval'.\n      2022-03-09 14:41:23 INFO     rasa_sdk.executor  - Registered function for 'action_default_ask_affirmation'.\n      2022-03-09 14:41:23 INFO     rasa_sdk.endpoint  - Action endpoint is up and running on http://0.0.0.0:5055\n      ```\n\n2. In a separate terminal, you can start the chatbot server by executing:\n  \n    ```bash\n    rasa run -m models --enable-api --cors \"*\"\n    ```\n\n   - If the server is up and running, you should be able to see the below message:\n\n     ```bash\n     (.venv) ➜  student-assist git:(dev) ✗ rasa run -m models --enable-api --cors \"*\"\n     2022-03-09 14:34:53 INFO     root  - Starting Rasa server on http://0.0.0.0:5005\n     2022-03-09 14:34:54 INFO     rasa.core.processor  - Loading model models/20220309-143217-broad-valid.tar.gz...\n     2022-03-09 14:35:31 INFO     root  - Rasa server is up and running.\n     ```\n\nWhen you open the [AskCCI.html](https://github.com/bhakthil/student-assist/blob/main/Chatbot-Widget/AskCCI.html) file, you will notice chat icon in the bottom-right corner\n\n![bot icon](bot-icon.png)\n\nYou may start communicating with the bot by clicking on the on the bot icon.\n\n![chat](chat-window.png)\n\n### Running Haystack Service\n\n1. Start the elasticsearch document store and initialize question-answer service (requires docker)\n\n    ```bash\n    python3 ./haystack/init_doc_store.py\n    uvicorn app:app --reload --app-dir rest_api/\n    ```\n\n    This will create a new docker container called `elasticsearch` where all of the documents that can be queried will be stored. If this docker container ever stops you can either restart it from within docker dashboard or delete and recreate the container.\n\n    This script also launches the REST API used for querying the Haystack service.\n\n2. To access the Haystack API documentation, go to `http://127.0.0.1:8000/docs` in your browser. You should see the below image:\n\n    ![Haystack api swagger documentation](haystack-api.png)\n\n### Running API Tests\n\n- To test that the API is functioning properly run the following:\n\n    ```bash\n    pytest api_tests/tests.py\n    ```\n\n---\n\n[Back to Top](https://github.com/bhakthil/student-assist#ask-cci-bot)\n"
    },
    {
      "name": "aytugkaya/notebooks",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/45581689?s=40&v=4",
      "owner": "aytugkaya",
      "repo_name": "notebooks",
      "description": null,
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2022-04-18T03:43:02Z",
      "updated_at": "2022-04-18T03:46:21Z",
      "topics": [],
      "readme": "# Transformers Notebooks\n\nThis repository contains the example code from our O'Reilly book [Natural Language Processing with Transformers](https://learning.oreilly.com/library/view/natural-language-processing/9781098103231/):\n\n<img alt=\"book-cover\" height=200 src=\"images/book_cover.jpg\" id=\"book-cover\"/>\n\n## Getting started\n\nYou can run these notebooks on cloud platforms like [Google Colab](https://colab.research.google.com/) or your local machine. Note that most chapters require a GPU to run in a reasonable amount of time, so we recommend one of the cloud platforms as they come pre-installed with CUDA.\n\n### Running on a cloud platform\n\nTo run these notebooks on a cloud platform, just click on one of the badges in the table below:\n\n<!--This table is automatically generated, do not fill manually!-->\n\n\n\n| Chapter                                     | Colab                                                                                                                                                                                               | Kaggle                                                                                                                                                                                                   | Gradient                                                                                                                                                                               | Studio Lab                                                                                                                                                                                                   |\n|:--------------------------------------------|:----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Introduction                                | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/nlp-with-transformers/notebooks/blob/main/01_introduction.ipynb)              | [![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/nlp-with-transformers/notebooks/blob/main/01_introduction.ipynb)              | [![Gradient](https://assets.paperspace.io/img/gradient-badge.svg)](https://console.paperspace.com/github/nlp-with-transformers/notebooks/blob/main/01_introduction.ipynb)              | [![Open In SageMaker Studio Lab](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/nlp-with-transformers/notebooks/blob/main/01_introduction.ipynb)              |\n| Text Classification                         | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/nlp-with-transformers/notebooks/blob/main/02_classification.ipynb)            | [![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/nlp-with-transformers/notebooks/blob/main/02_classification.ipynb)            | [![Gradient](https://assets.paperspace.io/img/gradient-badge.svg)](https://console.paperspace.com/github/nlp-with-transformers/notebooks/blob/main/02_classification.ipynb)            | [![Open In SageMaker Studio Lab](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/nlp-with-transformers/notebooks/blob/main/02_classification.ipynb)            |\n| Transformer Anatomy                         | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/nlp-with-transformers/notebooks/blob/main/03_transformer-anatomy.ipynb)       | [![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/nlp-with-transformers/notebooks/blob/main/03_transformer-anatomy.ipynb)       | [![Gradient](https://assets.paperspace.io/img/gradient-badge.svg)](https://console.paperspace.com/github/nlp-with-transformers/notebooks/blob/main/03_transformer-anatomy.ipynb)       | [![Open In SageMaker Studio Lab](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/nlp-with-transformers/notebooks/blob/main/03_transformer-anatomy.ipynb)       |\n| Multilingual Named Entity Recognition       | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/nlp-with-transformers/notebooks/blob/main/04_multilingual-ner.ipynb)          | [![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/nlp-with-transformers/notebooks/blob/main/04_multilingual-ner.ipynb)          | [![Gradient](https://assets.paperspace.io/img/gradient-badge.svg)](https://console.paperspace.com/github/nlp-with-transformers/notebooks/blob/main/04_multilingual-ner.ipynb)          | [![Open In SageMaker Studio Lab](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/nlp-with-transformers/notebooks/blob/main/04_multilingual-ner.ipynb)          |\n| Text Generation                             | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/nlp-with-transformers/notebooks/blob/main/05_text-generation.ipynb)           | [![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/nlp-with-transformers/notebooks/blob/main/05_text-generation.ipynb)           | [![Gradient](https://assets.paperspace.io/img/gradient-badge.svg)](https://console.paperspace.com/github/nlp-with-transformers/notebooks/blob/main/05_text-generation.ipynb)           | [![Open In SageMaker Studio Lab](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/nlp-with-transformers/notebooks/blob/main/05_text-generation.ipynb)           |\n| Summarization                               | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/nlp-with-transformers/notebooks/blob/main/06_summarization.ipynb)             | [![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/nlp-with-transformers/notebooks/blob/main/06_summarization.ipynb)             | [![Gradient](https://assets.paperspace.io/img/gradient-badge.svg)](https://console.paperspace.com/github/nlp-with-transformers/notebooks/blob/main/06_summarization.ipynb)             | [![Open In SageMaker Studio Lab](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/nlp-with-transformers/notebooks/blob/main/06_summarization.ipynb)             |\n| Question Answering                          | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/nlp-with-transformers/notebooks/blob/main/07_question-answering.ipynb)        | [![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/nlp-with-transformers/notebooks/blob/main/07_question-answering.ipynb)        | [![Gradient](https://assets.paperspace.io/img/gradient-badge.svg)](https://console.paperspace.com/github/nlp-with-transformers/notebooks/blob/main/07_question-answering.ipynb)        | [![Open In SageMaker Studio Lab](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/nlp-with-transformers/notebooks/blob/main/07_question-answering.ipynb)        |\n| Making Transformers Efficient in Production | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/nlp-with-transformers/notebooks/blob/main/08_model-compression.ipynb)         | [![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/nlp-with-transformers/notebooks/blob/main/08_model-compression.ipynb)         | [![Gradient](https://assets.paperspace.io/img/gradient-badge.svg)](https://console.paperspace.com/github/nlp-with-transformers/notebooks/blob/main/08_model-compression.ipynb)         | [![Open In SageMaker Studio Lab](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/nlp-with-transformers/notebooks/blob/main/08_model-compression.ipynb)         |\n| Dealing with Few to No Labels               | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/nlp-with-transformers/notebooks/blob/main/09_few-to-no-labels.ipynb)          | [![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/nlp-with-transformers/notebooks/blob/main/09_few-to-no-labels.ipynb)          | [![Gradient](https://assets.paperspace.io/img/gradient-badge.svg)](https://console.paperspace.com/github/nlp-with-transformers/notebooks/blob/main/09_few-to-no-labels.ipynb)          | [![Open In SageMaker Studio Lab](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/nlp-with-transformers/notebooks/blob/main/09_few-to-no-labels.ipynb)          |\n| Training Transformers from Scratch          | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/nlp-with-transformers/notebooks/blob/main/10_transformers-from-scratch.ipynb) | [![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/nlp-with-transformers/notebooks/blob/main/10_transformers-from-scratch.ipynb) | [![Gradient](https://assets.paperspace.io/img/gradient-badge.svg)](https://console.paperspace.com/github/nlp-with-transformers/notebooks/blob/main/10_transformers-from-scratch.ipynb) | [![Open In SageMaker Studio Lab](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/nlp-with-transformers/notebooks/blob/main/10_transformers-from-scratch.ipynb) |\n| Future Directions                           | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/nlp-with-transformers/notebooks/blob/main/11_future-directions.ipynb)         | [![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/nlp-with-transformers/notebooks/blob/main/11_future-directions.ipynb)         | [![Gradient](https://assets.paperspace.io/img/gradient-badge.svg)](https://console.paperspace.com/github/nlp-with-transformers/notebooks/blob/main/11_future-directions.ipynb)         | [![Open In SageMaker Studio Lab](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/nlp-with-transformers/notebooks/blob/main/11_future-directions.ipynb)         |\n\n<!--End of table-->\n\nNowadays, the GPUs on Colab tend to be K80s (which have limited memory), so we recommend using [Kaggle](https://www.kaggle.com/docs/notebooks), [Gradient](https://gradient.run/notebooks), or [SageMaker Studio Lab](https://studiolab.sagemaker.aws/). These platforms tend to provide more performant GPUs like P100s, all for free!\n\n> Note: some cloud platforms like Kaggle require you to restart the notebook after installing new packages.\n\n### Running on your machine\n\nTo run the notebooks on your own machine, first clone the repository and navigate to it:\n\n```bash\n$ git clone https://github.com/nlp-with-transformers/notebooks.git\n$ cd notebooks\n```\n\nNext, run the following command to create a `conda` virtual environment that contains all the libraries needed to run the notebooks:\n\n```bash\n$ conda env create -f environment.yml\n```\n\nChapter 7 (Question Answering) has a special set of dependencies, so to run that chapter you'll need a separate environment:\n\n```bash\n$ conda env create -f environment-chapter7.yml\n```\n\nOnce you've installed the dependencies, you can activate the `conda` environment and spin up the notebooks as follows:\n\n```bash\n$ conda activate book # or conda activate book-chapter7\n$ jupyter notebook\n```\n\n## FAQ\n\n### When trying to clone the notebooks on Kaggle I get a message that I am unable to access the book's Github repository. How can I solve this issue?\n\nThis issue is likely due to a missing internet connection. When running your first notebook on Kaggle you need to enable internet access in the settings menu on the right side. \n\n### How do you select a GPU on Kaggle?\n\nYou can enable GPU usage by selecting *GPU* as *Accelerator* in the settings menu on the right side.\n\n## Citations\n\nIf you'd like to cite this book, you can use the following BibTeX entry:\n\n```\n@book{tunstall2022natural,\n  title={Natural Language Processing with Transformers: Building Language Applications with Hugging Face},\n  author={Tunstall, Lewis and von Werra, Leandro and Wolf, Thomas},\n  isbn={1098103246},\n  url={https://books.google.ch/books?id=7hhyzgEACAAJ},\n  year={2022},\n  publisher={O'Reilly Media, Incorporated}\n}\n```\n"
    },
    {
      "name": "Bijay2305/sentence_similarity2",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/52318917?s=40&v=4",
      "owner": "Bijay2305",
      "repo_name": "sentence_similarity2",
      "description": null,
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2022-04-14T16:32:07Z",
      "updated_at": "2022-05-02T16:10:44Z",
      "topics": [],
      "readme": "# Learn Transformers\n\nRepo for the *Natural Language Processing: NLP With Transformers in Python* course, you can get 70% off the price with [this link!](https://www.udemy.com/course/nlp-with-transformers/?couponCode=70SEP2021) If the link stops working raise an issue or drop me a message somewhere:\n\n[YouTube](https://www.youtube.com/c/jamesbriggs) \n| [Discord](https://discord.gg/c5QtDB9RAP)\n\nI also have plenty of free material on YouTube 😊\n"
    },
    {
      "name": "sbadecker/ask_me_anything",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/19605053?s=40&v=4",
      "owner": "sbadecker",
      "repo_name": "ask_me_anything",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2021-03-10T09:05:02Z",
      "updated_at": "2024-01-09T09:55:19Z",
      "topics": [],
      "readme": "# ask_me_anything"
    },
    {
      "name": "ekmixon/gamechanger-ml",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/6691194?s=40&v=4",
      "owner": "ekmixon",
      "repo_name": "gamechanger-ml",
      "description": "GAMECHANGER Machine Learning Repo",
      "homepage": "",
      "language": null,
      "created_at": "2021-06-30T22:18:54Z",
      "updated_at": "2023-03-04T04:10:21Z",
      "topics": [],
      "readme": "# GC - Machine Learning\n## Table of Contents\n1. [Directory](##Directory)\n2. [Development Rules](#Development-Rules)\n3. [Train Models](#Train-Models)\n4. [ML API](#ML-API)\n\n## Directory\n```\ngamechangerml\n├── api\n│   ├── fastapi\n│   └── utils\n├── configs\n├── data\n├── experimental\n│   ├── notebooks\n│   │   ├── evaluation\n│   │   ├── portion_marking_demo\n│   │   └── sentence-transformer\n├── mlflow\n├── models\n│   ├── qexp_*\n│   ├── sent_index_*\n│   ├── topic_models\n│   └── transformers\n├── scripts\n├── src\n│   ├── featurization\n│   │   ├── data\n│   │   ├── keywords\n│   │   │   └── qe_mlm\n│   │   ├── term_extract\n│   ├── model_testing\n│   ├── search\n│   │   ├── QA\n│   │   ├── embed_reader\n│   │   ├── evaluation\n│   │   ├── query_expansion\n│   │   ├── ranking\n│   │   ├── semantic\n│   │   └── sent_transformer\n│   ├── text_classif\n│   ├── text_handling\n│   │   └── assets\n│   └── utilities\n│       └── numpy_encoder\n├── train\n│   └── scripts\n└── unittest\n```\n\n## Development Rules\n- Everything in gamechangerml/src should be independent of things outside of that structure (should not need to import from dataPipeline, common, etc).\n- Where ever possible, code should be modular and broken down into smallest logical pieces and placed in the most logical subfolder.\n- Include README.md file and/or example scripts demonstrating the functionality of your code.\n- Models/large files should not be stored on Github.\n- Data should not be stored on Github, there is a script in the `gamechangerml/scripts` folder to download a corpus from s3.\n- File paths in gamechangerml/configs config files should be relative to gamechangerml and only used for local testing purposes (feel free to change on your local machine, but do not commit to repo with system specific paths).\n- A config should not be required as an input parameter to a function; however a config can be used to provide parameters to a function (`foo(path=Config.path)`, rather than `foo(Config)`).\n- If a config is used for a piece of code (such as training a model), the config should be placed in the relevant section of the repo (dataPipeline, api, etc.) and should clearly designate which environment the config is for (if relevant).\n\n## Getting Started\n### To use gamechangerml as a python module\n- `pip install .`\n- you should now be able to import gamechangerml anywhere python is available.\n\n\n## Train Models\n1. Setup your environment, and make any changes to configs: \n- `source ./gamechangerml/setup_env.sh DEV`\n2. Ensure your AWS enviroment is setup (you have a default profile)\n3. Get dependencies\n- `source ./gamechangerml/scripts/download_dependencies.sh`\n4. For query expansion:\n- `python -m gamechangerml.train.scripts.run_train_models --flag {MODEL_NAME_SUFFIX} --saveremote {True or False} --model_dest {FILE_PATH_MODEL_OUTPUT} --corpus {CORPUS_DIR}`\n5. For sentence embeddings:\n- `python -m gamechangerml.train.scripts.create_embeddings -c {CORPUS LOCATION} --gpu True --em msmarco-distilbert-base-v2`\n\n## ML API\n1. Setup your environment, make any changes to configs: \n- `source ./gamechangerml/setup_env.sh DEV`\n2. Ensure your AWS enviroment is setup (you have a default profile)\n3. Dependencies will be automatically downloaded and extracted.\n4. `cd gamechangerml/api`\n5. `docker-compose build`\n6. `docker-compose up`\n7. visit `localhost:5000/docs`\n\n## FAQ\n- Do I need to train models to use the API?\n  - No, you can use the pretrained models within the dependencies. \n- The API is crashing when trying to load the models.\n  - Likely your machine does not have enough resources (RAM or CPU) to load all models. Try to exclude models from the model folder.\n- Do I need a machine with a GPU?\n  - No, but it will make training or inferring faster.\n- What if I can't download the dependencies since I am external?\n  - We are working on making models publically available. However you can use download pretrained transformers from HuggingFace to include in the models/transformers directory, which will enable you to use some functionality of the API. Without any models, there is still functionality available like text extraction avaiable. \n"
    },
    {
      "name": "augustodn/NLP_with_Transformers",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/1857412?s=40&v=4",
      "owner": "augustodn",
      "repo_name": "NLP_with_Transformers",
      "description": null,
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2022-03-03T12:37:26Z",
      "updated_at": "2022-03-11T12:11:47Z",
      "topics": [],
      "readme": "# Learn Transformers\n\nRepo for the *Natural Language Processing: NLP With Transformers in Python* course, you can get 70% off the price with [this link!](https://www.udemy.com/course/nlp-with-transformers/?couponCode=70SEP2021) If the link stops working raise an issue or drop me a message somewhere:\n\n[YouTube](https://www.youtube.com/c/jamesbriggs) \n| [Discord](https://discord.gg/c5QtDB9RAP)\n\nI also have plenty of free material on YouTube 😊\n"
    },
    {
      "name": "axirestech/cm_project",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/87490464?s=40&v=4",
      "owner": "axirestech",
      "repo_name": "cm_project",
      "description": null,
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2022-02-15T19:17:13Z",
      "updated_at": "2022-02-26T13:02:03Z",
      "topics": [],
      "readme": "# The Contract Manager Project\n- Repo name : cm_project\n- Description: Create an NLP engine that parses contracts to retrieve relevant information\n- Data Source: Contract Understanding Atticut Dataset\n- Type of analysis: NLP\n\n# Using Azure Board to track the projects' tasks\n## Task planning is done here\nhttps://dev.azure.com/axirestech/Contract%20Manager/_boards/board/t/Contract%20Manager%20Team/Backlog%20items\n## Validating our tasks when you commit\nThere are two ways to link the work you commit to Github to the Azure Board tasks :\n- Mention the task number in your commit message like this 'Updates AB#32'\n- or mention the task number the same way when you create a pull request in Github \nYou will then see a little Github icon on the task card linking back to your commits...👍 Ain't that cool or what ? 😜\nIf you mention 'Fixes' as in 'Fixes AB#32', the card will automatically be moved to the 'Done' section of the board.\nHow's that for tracking work that gets done ?\n\n# Startup the project\n\nThe initial setup.\n\nCreate virtualenv and install the project:\n```bash\nsudo apt-get install virtualenv python-pip python-dev\ndeactivate; virtualenv ~/venv ; source ~/venv/bin/activate ;\\\n    pip install pip -U; pip install -r requirements.txt\n```\n\nUnittest test:\n```bash\nmake clean install test\n```\n\nCheck for cm_project in gitlab.com/{group}.\nIf your project is not set please add it:\n\n- Create a new project on `gitlab.com/{group}/cm_project`\n- Then populate it:\n\n```bash\n##   e.g. if group is \"{group}\" and project_name is \"cm_project\"\ngit remote add origin git@github.com:{group}/cm_project.git\ngit push -u origin master\ngit push -u origin --tags\n```\n\nFunctionnal test with a script:\n\n```bash\ncd\nmkdir tmp\ncd tmp\ncm_project-run\n```\n\n# Install\n\nGo to `https://github.com/{group}/cm_project` to see the project, manage issues,\nsetup you ssh public key, ...\n\nCreate a python3 virtualenv and activate it:\n\n```bash\nsudo apt-get install virtualenv python-pip python-dev\ndeactivate; virtualenv -ppython3 ~/venv ; source ~/venv/bin/activate\n```\n\nClone the project and install it:\n\n```bash\ngit clone git@github.com:{group}/cm_project.git\ncd cm_project\npip install -r requirements.txt\nmake clean install test                # install and test\n```\nFunctionnal test with a script:\n\n```bash\ncd\nmkdir tmp\ncd tmp\ncm_project-run\n```\n"
    },
    {
      "name": "JungeAlexander/kbase_search",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/6056731?s=40&v=4",
      "owner": "JungeAlexander",
      "repo_name": "kbase_search",
      "description": null,
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2021-02-25T16:37:31Z",
      "updated_at": "2022-04-25T16:10:35Z",
      "topics": [],
      "readme": ""
    },
    {
      "name": "manuelyhvh/nlp-with-transformers",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/55846269?s=40&v=4",
      "owner": "manuelyhvh",
      "repo_name": "nlp-with-transformers",
      "description": null,
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2022-02-02T23:24:27Z",
      "updated_at": "2022-09-10T10:18:26Z",
      "topics": [],
      "readme": "# Transformers Notebooks\n\nThis repository contains the example code from our O'Reilly book [Natural Language Processing with Transformers](https://learning.oreilly.com/library/view/natural-language-processing/9781098103231/):\n\n<img alt=\"book-cover\" height=200 src=\"images/book_cover.jpg\" id=\"book-cover\"/>\n\n## Getting started\n\nYou can run these notebooks on cloud platforms like [Google Colab](https://colab.research.google.com/) or your local machine. Note that most chapters require a GPU to run in a reasonable amount of time, so we recommend one of the cloud platforms as they come pre-installed with CUDA.\n\n### Running on a cloud platform\n\nTo run these notebooks on a cloud platform, just click on one of the badges in the table below:\n\n<!--This table is automatically generated, do not fill manually!-->\n\n\n\n| Chapter                                     | Colab                                                                                                                                                                                               | Kaggle                                                                                                                                                                                                   | Gradient                                                                                                                                                                               | Studio Lab                                                                                                                                                                                                   |\n|:--------------------------------------------|:----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Introduction                                | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/nlp-with-transformers/notebooks/blob/main/01_introduction.ipynb)              | [![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/nlp-with-transformers/notebooks/blob/main/01_introduction.ipynb)              | [![Gradient](https://assets.paperspace.io/img/gradient-badge.svg)](https://console.paperspace.com/github/nlp-with-transformers/notebooks/blob/main/01_introduction.ipynb)              | [![Open In SageMaker Studio Lab](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/nlp-with-transformers/notebooks/blob/main/01_introduction.ipynb)              |\n| Text Classification                         | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/nlp-with-transformers/notebooks/blob/main/02_classification.ipynb)            | [![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/nlp-with-transformers/notebooks/blob/main/02_classification.ipynb)            | [![Gradient](https://assets.paperspace.io/img/gradient-badge.svg)](https://console.paperspace.com/github/nlp-with-transformers/notebooks/blob/main/02_classification.ipynb)            | [![Open In SageMaker Studio Lab](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/nlp-with-transformers/notebooks/blob/main/02_classification.ipynb)            |\n| Transformer Anatomy                         | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/nlp-with-transformers/notebooks/blob/main/03_transformer-anatomy.ipynb)       | [![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/nlp-with-transformers/notebooks/blob/main/03_transformer-anatomy.ipynb)       | [![Gradient](https://assets.paperspace.io/img/gradient-badge.svg)](https://console.paperspace.com/github/nlp-with-transformers/notebooks/blob/main/03_transformer-anatomy.ipynb)       | [![Open In SageMaker Studio Lab](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/nlp-with-transformers/notebooks/blob/main/03_transformer-anatomy.ipynb)       |\n| Multilingual Named Entity Recognition       | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/nlp-with-transformers/notebooks/blob/main/04_multilingual-ner.ipynb)          | [![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/nlp-with-transformers/notebooks/blob/main/04_multilingual-ner.ipynb)          | [![Gradient](https://assets.paperspace.io/img/gradient-badge.svg)](https://console.paperspace.com/github/nlp-with-transformers/notebooks/blob/main/04_multilingual-ner.ipynb)          | [![Open In SageMaker Studio Lab](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/nlp-with-transformers/notebooks/blob/main/04_multilingual-ner.ipynb)          |\n| Text Generation                             | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/nlp-with-transformers/notebooks/blob/main/05_text-generation.ipynb)           | [![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/nlp-with-transformers/notebooks/blob/main/05_text-generation.ipynb)           | [![Gradient](https://assets.paperspace.io/img/gradient-badge.svg)](https://console.paperspace.com/github/nlp-with-transformers/notebooks/blob/main/05_text-generation.ipynb)           | [![Open In SageMaker Studio Lab](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/nlp-with-transformers/notebooks/blob/main/05_text-generation.ipynb)           |\n| Summarization                               | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/nlp-with-transformers/notebooks/blob/main/06_summarization.ipynb)             | [![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/nlp-with-transformers/notebooks/blob/main/06_summarization.ipynb)             | [![Gradient](https://assets.paperspace.io/img/gradient-badge.svg)](https://console.paperspace.com/github/nlp-with-transformers/notebooks/blob/main/06_summarization.ipynb)             | [![Open In SageMaker Studio Lab](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/nlp-with-transformers/notebooks/blob/main/06_summarization.ipynb)             |\n| Question Answering                          | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/nlp-with-transformers/notebooks/blob/main/07_question-answering.ipynb)        | [![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/nlp-with-transformers/notebooks/blob/main/07_question-answering.ipynb)        | [![Gradient](https://assets.paperspace.io/img/gradient-badge.svg)](https://console.paperspace.com/github/nlp-with-transformers/notebooks/blob/main/07_question-answering.ipynb)        | [![Open In SageMaker Studio Lab](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/nlp-with-transformers/notebooks/blob/main/07_question-answering.ipynb)        |\n| Making Transformers Efficient in Production | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/nlp-with-transformers/notebooks/blob/main/08_model-compression.ipynb)         | [![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/nlp-with-transformers/notebooks/blob/main/08_model-compression.ipynb)         | [![Gradient](https://assets.paperspace.io/img/gradient-badge.svg)](https://console.paperspace.com/github/nlp-with-transformers/notebooks/blob/main/08_model-compression.ipynb)         | [![Open In SageMaker Studio Lab](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/nlp-with-transformers/notebooks/blob/main/08_model-compression.ipynb)         |\n| Dealing with Few to No Labels               | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/nlp-with-transformers/notebooks/blob/main/09_few-to-no-labels.ipynb)          | [![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/nlp-with-transformers/notebooks/blob/main/09_few-to-no-labels.ipynb)          | [![Gradient](https://assets.paperspace.io/img/gradient-badge.svg)](https://console.paperspace.com/github/nlp-with-transformers/notebooks/blob/main/09_few-to-no-labels.ipynb)          | [![Open In SageMaker Studio Lab](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/nlp-with-transformers/notebooks/blob/main/09_few-to-no-labels.ipynb)          |\n| Training Transformers from Scratch          | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/nlp-with-transformers/notebooks/blob/main/10_transformers-from-scratch.ipynb) | [![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/nlp-with-transformers/notebooks/blob/main/10_transformers-from-scratch.ipynb) | [![Gradient](https://assets.paperspace.io/img/gradient-badge.svg)](https://console.paperspace.com/github/nlp-with-transformers/notebooks/blob/main/10_transformers-from-scratch.ipynb) | [![Open In SageMaker Studio Lab](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/nlp-with-transformers/notebooks/blob/main/10_transformers-from-scratch.ipynb) |\n| Future Directions                           | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/nlp-with-transformers/notebooks/blob/main/11_future-directions.ipynb)         | [![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/nlp-with-transformers/notebooks/blob/main/11_future-directions.ipynb)         | [![Gradient](https://assets.paperspace.io/img/gradient-badge.svg)](https://console.paperspace.com/github/nlp-with-transformers/notebooks/blob/main/11_future-directions.ipynb)         | [![Open In SageMaker Studio Lab](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/nlp-with-transformers/notebooks/blob/main/11_future-directions.ipynb)         |\n\n<!--End of table-->\n\nNowadays, the GPUs on Colab tend to be K80s (which have limited memory), so we recommend using [Kaggle](https://www.kaggle.com/docs/notebooks), [Gradient](https://gradient.run/notebooks), or [SageMaker Studio Lab]((https://studiolab.sagemaker.aws/)). These platforms tend to provide more performant GPUs like P100s, all for free!\n\n### Running on your machine\n\nTo run the notebooks on your own machine, first clone the repository:\n\n```\ngit clone https://github.com/nlp-with-transformers/notebooks\ncd notebooks-test\n```\n\nNext, you'll need to install a few packages that depend on your operating system and hardware:\n\n* [PyTorch](https://pytorch.org/get-started/locally/)\n* [TensorFlow](https://www.tensorflow.org/install/) (optional, since only used in a few chapters)\n* [PyTorch Scatter](https://github.com/rusty1s/pytorch_scatter) (only used in Chapter 11)\n* [librosa](https://librosa.org/) (only used in Chapter 11)\n* [libsndfile](http://www.mega-nerd.com/libsndfile/) (only used in Chapter 11)\n\nOnce you have install the above requirements, create a virtual environment and install the remaining Python dependencies:\n\n```\nconda create -n book python=3.8 -y && conda activate book\nfrom install import *\ninstall_requirements()\n# Use the following to run Chapter 7\n# install_requirements(is_chapter7)\n```\n"
    },
    {
      "name": "cgleone/gui_v0",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/45293022?s=40&v=4",
      "owner": "cgleone",
      "repo_name": "gui_v0",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2021-11-03T12:44:12Z",
      "updated_at": "2022-01-16T21:32:08Z",
      "topics": [],
      "readme": ""
    },
    {
      "name": "Kau5h1K/ds5500-userprivacy",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/40333107?s=40&v=4",
      "owner": "Kau5h1K",
      "repo_name": "ds5500-userprivacy",
      "description": "DS5500 Capstone Project",
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2021-10-19T03:19:52Z",
      "updated_at": "2021-12-22T05:51:48Z",
      "topics": [],
      "readme": "# Detecting Textual Saliency in Privacy Policy\n### DS5500 Capstone Project\n\n## Summary \n\nWebsites, mobile apps, and other product and service providers share how they gather, use and manage customers' data in the form of privacy policy documents. However, due to their lengthy and complex nature, a majority of people tend to ignore these documents. To counteract the risks posed by this ignorance, we aim to develop language models to extract and deliver salient policy information to end-users. To that end, we propose two use case-specific modules, namely the INFORM module and the QUERY module, that collectively enable users to understand their rights better. \n\n\nThe primary objective of the INFORM module lies in communicating key insights by exploiting a dataset containing 115 privacy policies defined by US companies. Additionally, we seek to build a selection of models that can annotate pre-defined categories to privacy policy paragraphs using supervised machine learning. The QUERY module can enable users to ask policy-related questions using a freeform question-answering system for privacy policies.  By retrieving the most relevant information from the policy document given a question, this system alleviates the burden of searching the target information from a lengthy policy document. To this purpose, we make use of a Question-Answer dataset that provides 714 human-annotated questions written for a wide range of privacy practices. Finally, we attempt to build on earlier studies in the literature and make the functionality publicly available using an interactive web framework. \n"
    },
    {
      "name": "huygensravel/hotelreviews_sentiment_analysis",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/5883285?s=40&v=4",
      "owner": "huygensravel",
      "repo_name": "hotelreviews_sentiment_analysis",
      "description": "Collect hotel reviews data from the web using web-scrapping. Create and train a transformer model for sentiment analysis on the collected data.",
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2021-12-17T20:59:47Z",
      "updated_at": "2022-07-04T18:36:39Z",
      "topics": [],
      "readme": "# hotelreviews_sentiment_analysis\nSentiment Analysis on hotel reviews from google travel using BERT transformer.\n* We collect dataset via webscrapping with selenium from google travel hotel.\n* We explore, clean, and transform the data for it to be used by Pytorch or Tensorflow.\n* We build, train, and test a binary sentiment analysis model using BERT architecture with the data.\n\n## Requirements\n+ Python version: python 3.8.5. You can create a new conda environment with the command\n\t```\n\tconda create -n <yourEnvironmentName> python=3.8.5 anaconda\n\t```\n\n+ All  packages are in requirements.txt\n\t* If installing locally, run in terminal\n\t\t```bash\n\t\tpip install -r requirements.txt\n\t\t```\n\t\t\n\t* If using google colab, follow the steps in `BinaryHotelReviews.ipynb` notebook\n\t\t1. upload the requirement.txt file in your session. You car run the cell\n\t\t\t```python\n\t\t\tfrom google.colab import files\n\t\t\tuploaded = files.upload()\n\t\t\t```\n\t\t2. Mount google drive and specify an installation path by running the cell containing\n\t\t\t```python\n\t\t\timport os, sys\n\t\t\tfrom google.colab import drive\n\t\t\tdrive.mount('/content/drive')\n\t\t\tnb_path = '/content/notebooks'\n\t\t\tos.symlink('<where in your doogle drive you want to store the env>', nb_path)\n\t\t\tsys.path.insert(0, nb_path)\n\t\t\t```\n        3. un-comment the following command and run the cell\n\t\t\t```\n\t\t\t!pip install --target=$nb_path -r requirements.txt\n\t\t\t```\n\t\t4. Restart the runtime\n\t\t5. Since the requirements are now installed, after closing the session, if you reopen and want to run the notebook again just comment the cell in step 1 and 3, and run only step 2.\n\t\t6. Run the cells containing the following command\n\n\t\t\t```\n\t\t\t!pip install transformers -U \n\t\t\t```\n\n\t\t\t```\n\t\t\t!pip install datasets\n\t\t\t```\n\n\t\t\t```\n\t\t\t!pip install --upgrade pyarrow\n\t\t\t```\n\n## Structure of the project\n\n### Data collection\nWe used  web scraping with selenium to collect hotel reviews from google travel. The scarping was done using the `ScrapHotelReviews.py` script.\n\nThe raw data collected consists of hotel names, reviews and rating.\nWe got the reviews for hotels in:\n* Hanoi\n* Macao\n* Kuala Lumpur\n* Hong Kong\n* different cities of Madagascar.\n\nWe made sure that the data collected is balanced between positive and negative reviews.\n\nAll data are saved in the `data/`  directory.\n\n### Data Eploration Dataset creation\n* We explored, cleaned and transform the different collected data.\n* Using the data collected from different cities. We created two single ready to use `csv` file, one for multi-class classification and one for binary classification. These two files are saved in the `data/` directory.\n\n### Binary Sentiment Classification\nWe use BERT transformer with Pytorch to classify positive and negative reviews. We trained only the classification head part of the architecture and froze the lower layers of the BERT model.\n"
    },
    {
      "name": "SGrannemann/Star-Trek-Scripts-NLP-Playground",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/64766572?s=40&v=4",
      "owner": "SGrannemann",
      "repo_name": "Star-Trek-Scripts-NLP-Playground",
      "description": "Toy project for text data cleaning, pandas DataFrame handling, training bigram models and word2vec models. Also contains search and question answering capabilities based on Haystack.",
      "homepage": "",
      "language": "Jupyter Notebook",
      "created_at": "2021-03-27T15:01:25Z",
      "updated_at": "2022-03-10T21:07:04Z",
      "topics": [
        "gensim",
        "haystack",
        "nlp",
        "python",
        "question-answering",
        "startrek",
        "word2vec"
      ],
      "readme": "# Star-Trek-Scripts-NLP-Playground\n## Project as a training ground for different aspects of NLP\nThis project is intended as a toy project to practice different aspects of NLP, from collecting and preprocessing data to the options for search and question answering. I was especially interested in trying out the Haystack library.\nYou can use the provided Python scripts to create your own corpus of Memory Alpha plot summaries and afterwards query the corpus. For now, I have focussed more on collecting and cleaning the data instead of using advanced NLP methods. Available query options are thus limited to the Haystack implementation.\n\n## Getting Started\nIn case you want to use the code available here, you can most easily proceed as follows:\n- Use the requirements.txt to setup your VENV with the necessary libraries.\n- Download the Memory Alpha articles via the provided scraping script (data folder: scrape_episodes_articles.py). \n- Use the script extract_plots.py from the data folder to write the plot descriptions from the Wiki articles to text files.\n- Train the bigram model and save a corpus for word2vec via collocation_creation.py.\n- Run word2vec_creation.py to create the word2vec vector embedding model of the corpus and try it out if you want ;).\n- You can now use the combined_search_qa.py script to query the corpus based on TFIDF or DPR methods and ask questions. Have fun - and do not hesitate to give feedback :)\n\n\n## Project structure\n\nThis project comes with multiple folders to organize all the files. These folders are:\n- Data\n- Preprocessing\n\n### Data\nIn this folder, you can find the raw data as well as artifacts generated for the different steps of the projects: Serialized DataFrames, collocation models etc.\nScripts for collecting additional data can be found here as well, such as scraping episode articles from Wikipedia.\n\n\n### Preprocessing\nIn this folder, you can find scripts that preprocess the data, either for extracting the text from the original files or for cleaning up text data etc. \nAdditionally, modules that create collocations or word2vec embeddings reside in this folder.\n\n## Contact and contribution\nShould you want to contribute or to contact me, feel free to send me an email.\nLive long and prosper!\n\n\n\n\n\n## Data collection and cleaning\nThis playground currently uses the following data:\n- A dataset with the scripts of the episodes, taken from here: https://www.kaggle.com/gjbroughton/start-trek-scripts\nFor this part of the data, functions that can extract the episode title from the scripts as well as clean up (remove speakers for example) the text are available.\n\n- A dataset of plot descriptions that I scraped from the Wikipedia articles. The scripts for downloading the files and extracting the actual descriptions of the plots can be found in the data folder.\nThis part of the data was scraped from Wikipedia. The Plot Descriptions of the episodes were extracted/parsed with a simple script.\n\n\n## \n\n\n\n"
    },
    {
      "name": "byukan/movie-rec",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/12012907?s=40&v=4",
      "owner": "byukan",
      "repo_name": "movie-rec",
      "description": "MovieRec is a website with a recommender system for movies.",
      "homepage": "",
      "language": "Jupyter Notebook",
      "created_at": "2021-09-11T05:50:09Z",
      "updated_at": "2022-02-16T03:22:33Z",
      "topics": [],
      "readme": "<h1 align=\"center\">MovieRec\n  <sub>\n    <img src=\"https://c.pxhere.com/images/6d/90/07af6a31003f44671b90e75d50a4-1566355.jpg!d\" height=\"50\" width=\"90\">\n  </sub>\n</h1>\n\n<p align=\"center\">\n  <sup>\n    MovieRec is a website with a recommender system for movies.\n    https://dvrba-movierec.herokuapp.com/\n  </sup>\n</p>\n\n***\n\n* [Purpose](#purpose)\n* [Motivation](#motivation)\n* [Resources](#resources)\n* [Instructions](#instructions)\n* [YouTube Video Link](#youtube-video-link)\n* [Team](#team)\n\n## Purpose\n\nThis website allows visitors to search for movies and view movie details, and will recommend similar movies based on the text in the movie overview.\n\n## Motivation\n\nWe've chosen this project because recommender systems are very common; almost every business wants or uses them. Some of us are also interested in working with text data to find semantic similarities. Beneficiaries of this website include anyone who wants to save time finding movies they will enjoy or appreciate.\n\n## Resources\n- Data Source: <https://www.kaggle.com/harshitshankhdhar/imdb-dataset-of-top-1000-movies-and-tv-shows>\n\n## Instructions\n\n### How to clone this repo\n- Make sure to use a password-protected SSH key. Instructions to generate an SSH key and add it to your GitHub account are available at: [https://github.com/settings/keys](https://github.com/settings/keys)\n- In your terminal, navigate to the directory where you want the project to live.\n- Type the following command: `git clone git@github.com:byukan/movie-rec.git`\n\n### Setup\nPlease do the following so that you can run Flask and run our unit and integration tests.\n- Install the project dependencies by running the following commands:\n  - `pip install -r MovieRecommender/requirements.txt`\n  - `pip install -r model/requirements.txt`\n- Install node.js by visiting this website: <https://nodejs.org/en/download/>\n- Download Cypress using the following command: `npm install cypress`\n  - If this does not work, use one of the other download methods described here: <https://docs.cypress.io/guides/getting-started/installing-cypress>\n- Navigate to the top-level folder of this project (it will be *movie-rec* unless you renamed it)\n- Run our setup script, which will set a couple Flask variables. Type the following: `source flask_setup` \n\n### How to run Flask and use the website\n- In your terminal, navigate to the *MovieRecommender* directory\n- Run *either* of the following commands (both should work):\n  - `flask run`\n  - `python3 serve.py`\n- Open up your web browser and visit: **localhost:5000**\n\n### How to run our unit tests\n- To test our text similarity model:\n  - Navigate to the *model* directory\n  - Type the following command: `python3 -m pytest`\n- To test access to our MongoDB collections:\n  - Navigate to the *mongo* directory\n  - Type the following command: `python3 -m pytest`\n- To test our Flask code:\n  - Navigate to the *MovieRecommender* directory\n  - Type the following command: `python3 -m unittest`\n\n### How to run our integration tests with Cypress\n- Run flask first (see instructions above)\n- Navigate up one level to the top-level project folder (it will be *movie-rec* unless you renamed it)\n- Open Cypress\n- Within the Cypress window, select the file to run: *cypress/integration/test.js*\n- A new browser window will open and run the tests\n\n## YouTube Video Link\nhttps://youtu.be/eyzZRgSytEM\n[![Alt text](https://img.youtube.com/vi/eyzZRgSytEM/maxresdefault.jpg)](https://www.youtube.com/watch?v=eyzZRgSytEM)\n\n\n## Notes\n12/20/21: Mongo password changed and repo made public.\n\n\n## Team\n\n[Aybike Bayraktar](https://github.com/aybikke), [Annat Koren](https://github.com/a-kor-en), & [Brant Yukan](https://github.com/byukan)\n\n## License\n\n[BSD 3-Clause License](https://github.com/byukan/movie-rec/blob/main/LICENSE)\n"
    },
    {
      "name": "dhruv2600/flask_app",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/56503628?s=40&v=4",
      "owner": "dhruv2600",
      "repo_name": "flask_app",
      "description": "flask app for running ML models used in capstone",
      "homepage": null,
      "language": "Python",
      "created_at": "2021-10-18T14:11:37Z",
      "updated_at": "2023-10-05T18:49:09Z",
      "topics": [],
      "readme": ""
    },
    {
      "name": "caplincapture/NLP",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/18247673?s=40&v=4",
      "owner": "caplincapture",
      "repo_name": "NLP",
      "description": null,
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2021-08-30T17:28:38Z",
      "updated_at": "2021-10-24T03:12:52Z",
      "topics": [],
      "readme": ""
    },
    {
      "name": "fptUniversityMCQSS/ModelRoberta",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/86480070?s=40&v=4",
      "owner": "fptUniversityMCQSS",
      "repo_name": "ModelRoberta",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2021-08-27T17:13:13Z",
      "updated_at": "2021-10-07T14:36:58Z",
      "topics": [],
      "readme": "#Install and Start Milvus Store\n\n1.Confirm that the Docker daemon is running in the background:\n```\nsudo docker info\n```\n2.Pull Docker Image\n```\nsudo docker pull milvusdb/milvus:1.0.0-cpu-d030521-1ea92e\n```\n3.Download Configuration Files\n```\nmkdir -p /home/$USER/milvus/conf\ncd /home/$USER/milvus/conf\nwget https://raw.githubusercontent.com/milvus-io/milvus/v1.0.0/core/conf/demo/server_config.yaml\n```\n4.Start Docker Container\n```\nsudo docker run -d --name milvus_cpu_1.0.0 \\\n-p 19530:19530 \\\n-p 19121:19121 \\\n-v /home/$USER/milvus/db:/var/lib/milvus/db \\\n-v /home/$USER/milvus/conf:/var/lib/milvus/conf \\\n-v /home/$USER/milvus/logs:/var/lib/milvus/logs \\\n-v /home/$USER/milvus/wal:/var/lib/milvus/wal \\\nmilvusdb/milvus:1.0.0-cpu-d030521-1ea92e\n```\n5.Confirm the running state of Milvus:\n```\nsudo docker ps\n```\n\n\n=======================\nkiểm tra xem Milvus đã running ở port localhost:19530\n=======================\n\n"
    },
    {
      "name": "AhmedYounes94/MLM-NSP",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/68699166?s=40&v=4",
      "owner": "AhmedYounes94",
      "repo_name": "MLM-NSP",
      "description": null,
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2021-06-20T04:51:20Z",
      "updated_at": "2022-07-30T07:28:52Z",
      "topics": [],
      "readme": "# Learn Transformers\n\n[Course](https://www.udemy.com/course/nlp-with-transformers/?couponCode=MEDIUM)\n\n[Docs](https://jamescalam.github.io/transformers/)\n"
    },
    {
      "name": "AetherPrior/QA-BERT",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/39939017?s=40&v=4",
      "owner": "AetherPrior",
      "repo_name": "QA-BERT",
      "description": "A program for question answering using the haystack api",
      "homepage": null,
      "language": "Python",
      "created_at": "2021-03-22T21:19:28Z",
      "updated_at": "2023-04-26T10:57:32Z",
      "topics": [],
      "readme": "# QA-BERT\n\nA Question-Answering model based off of HuggingFace's transformer models, using the [haystack](https://github.com/deepset-ai/haystack) library.  \nThis is a rewrite and improvement upon the original code written using [cdqa](https://github.com/cdqa-suite/cdQA), a now outdated library.\n\n# Method\n\n- I have used a Dense-passage-retrieval System as a retriever for QA, and and existing BERT Models that function as document embedders and query embedders\n\n- We did _not_ initially train the model at all, but I'm researching into ways to be able to train it on several similar datasets to improve the performance.\n\n  - For example, there are ways to [train the DPR](https://haystack.deepset.ai/docs/latest/tutorial9md) using [medical QA datasets](https://github.com/abachaa/Existing-Medical-QA-Datasets).\n\n  - I'm trying to find one that's more relevant to our field.\n\n- Colab is NOT the way to go IMO for any decent code, but I'm constrained by the lack of a GPU on my system for speed.\n  - The initial code was written in colab, after which I've exported it, and changed it\n  - Python code can actually be run on colab by mounting it to the drive and calling it from colab\n  - The model still works on the cpu however, but parallelism appears to be an issue.\n\n# Setup\n\nClone the repository:\n\n```\ngit clone git@github.com:AetherPrior/qa-bert-haystack.git\nOR\ngit clone https://github.com/AetherPrior/qa-bert-haystack.git\n```\n\nCreate a new `virtualenv`\n\n```\npython -m venv /my/venv\nsource /my/venv/bin/activate\n```\n\nInstall the required libraries\n\n```\npip install -r requirements.txt\n```\n\n**NOTE:** There is no support for `python3.9` as of yet from `haystack`. Recommend against using any version `>=python3.8` for running the code\n\n# Running\n\n```\ncd src\npython main.py --text-dir /path/to/textbooks\n```\n\nplease be patient! First runs can take a huge amount of time.\nCurrently, the whole program is command line based.\n\n# Contributing\n\nSee [CONTRIBUTING.md](./CONTRIBUTING.md)\n\nAdditionally, the docs for `haystack` are quite terse. A lot of breaking changes happen over a very small time-frame, leaving many questions on their issue-tracker no longer valid. Recommend downloading the source code with the release that you're using to resolve most errors.\n"
    },
    {
      "name": "omerlevi2/NLP_Project",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/73997702?s=40&v=4",
      "owner": "omerlevi2",
      "repo_name": "NLP_Project",
      "description": null,
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2021-02-09T10:00:33Z",
      "updated_at": "2021-04-20T09:46:44Z",
      "topics": [],
      "readme": "\n"
    },
    {
      "name": "aaronbriel/jugglechat",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/5034032?s=40&v=4",
      "owner": "aaronbriel",
      "repo_name": "jugglechat",
      "description": "An Eclectic and Malleable Multi-Chatbot Framework",
      "homepage": "",
      "language": "Python",
      "created_at": "2020-11-19T21:39:43Z",
      "updated_at": "2021-12-22T19:38:20Z",
      "topics": [
        "absum",
        "chatbot",
        "multiagent",
        "nlp",
        "question-answering",
        "transformers"
      ],
      "readme": "![JuggleChat](https://github.com/aaronbriel/jugglechat/blob/master/logo.png?raw=true)\n\nAn Eclectic and Malleable Multi-Chatbot Framework. [Link](http://doi.org/10.1002/cae.22449) to publication.\n\n![JuggleChat](https://github.com/aaronbriel/jugglechat/blob/master/architecture.png?raw=true)\n\n## Installation\nUbuntu:\n\n\tsudo apt install python3\n\tsudo apt install python3-pip\n\tsudo apt install virtualenv (or sudo apt install python3-virtualenv)\n\nClone repo and install dependencies:\n\n    git clone https://github.com/aaronbriel/jugglechat.git\n    cd jugglechat\n    make setup\n    \nInstall redis.\n\n    sudo apt install redis\n\nInstall docker.\n\n    sudo apt install docker\n\nUpdate .env BASE_PATH ie \"/home/aaron/jugglechat/\"\n\n## Configuration\n\n### Configure Celery for Redis instance:\nSee config.py: BROKER_URL, BACKEND_URL\n\n### Start Celery:\n    make start_celery\n    \nView redis (celery task) data in browser:\n\n    npm install -g redis-commander\n    redis-commander\n\n### Starting rasa server: \n    make start_rasa\n    \n### Starting haystack (QA and FAQ chatbots):\nStart docker:\n\n    sudo systemctl start docker\n    \nStart docker container:\n\n    make elastic_start\n\nStore FAQ and QA retrieval content into ElasticSearch:\n    \n    make store_faq \n    make store_faq_experiment       \n    make store_qa\n    make store_qa_experiment\n    \nYou may need to run the following to resolve a Permission Denied issue on Ubuntu:\n\n    sudo chmod 777 /tmp/tika*\n    \nIf you run into the following error you will need to either increase disc size or free up the index:\n\n    elasticsearch.exceptions.AuthorizationException: AuthorizationException(403, 'cluster_block_exception', 'index [qa_\n    default] blocked by: [FORBIDDEN/12/index read-only / allow delete (api)];')\n    \nOn Ubuntu, you also may need to do the following, then log out and back in:\n\n\tsudo groupadd docker\n\tsudo usermod -aG docker ${USER} \n\t\nA viable solution can be found here: \nhttps://stackoverflow.com/questions/50609417/elasticsearch-error-cluster-block-exception-forbidden-12-index-read-only-all\n\nFor example: \n\n    curl -XPUT -H \"Content-Type: application/json\" https://localhost:9200/_all/_settings -d '{\"index.blocks.read_only_allow_delete\": null}'\n\n\nView content of ElasticSearch (default port: 9200, default index_names: faq_default, qa_default):\n\n    curl https://localhost:PORT/INDEX_NAME/_search?pretty\nEx:\n\n    curl https://localhost:9200/faq/_search?pretty\n    \nStart Haystack REST API:\n\n    make start_haystack\n\nDo sanity check of haystack:\n    \n    curl --request POST --url 'http://127.0.0.1:8000/models/1/doc-faq' --data '{\"questions\": [\"can covid19 pass to pets?\"]}' \n\nView API documentation: http://127.0.0.1:8000/docs\n\n### Calling celery task that passes data to rasa core:\n    from celery_app.tasks import call_rasa\n    result = call_rasa_core.delay(\"hello\")\n    result.get()\n    \n### Calling celery task that passes data to rasa nlp:\n    from celery_app.tasks import call_rasa\n    result = call_rasa_nlp.delay(\"hello\")\n    result.get()\n\n### Calling celery task that passes data to haystack faq:\n    from celery_app.tasks import call_faq\n    result = call_faq.delay(\"How is the virus spreading?\")\n    result.get()\n    \n### Calling celery task that passes data to haystack qa:\n    from celery_app.tasks import call_qa\n    result = call_qa.delay(\"Where was COVID19 first discovered?\")\n    result.get()\n    \n### Calling celery task that does single chat with dialogpt:\n    from celery_app.tasks import call_convai\n    result = call_convai.delay(\"Who is mort?\")\n    result.get()\n\n### Calling celery task that does Abstractive Summarization:\n    from celery_app.tasks import call_summarizer\n    result = call_summarizer.delay(\"Lets do a summary of this overly long sentence please.\")\n    result.get()\n    \n### Calling celery tasks via flower API calls (expects rasa, haystack, elasticsearch, celery, and flower to be running):\n    curl -X POST -u user1:password1 -d '{\"args\":[\"can covid19 pass to pets?\"]}' http://localhost:5555/api/task/apply/celery_app.tasks.call_rasa_nlp\n    curl -X POST -u user1:password1 -d '{\"args\":[\"can covid19 pass to pets?\"]}' http://localhost:5555/api/task/apply/celery_app.tasks.call_qa\n    curl -X POST -u user1:password1 -d '{\"args\":[\"how long does the coronavirus survive on surfaces\"]}' http://localhost:5555/api/task/apply/celery_app.tasks.call_rasa_nlp\n    curl -X POST -u user1:password1 -d '{\"args\":[\"how long does the coronavirus survive on surfaces\"]}' http://localhost:5555/api/task/apply/celery_app.tasks.call_faq\n    curl -X POST -u user1:password1 -d '{\"args\":[\"can I get a summary of the lesson?\"]}' http://localhost:5555/api/task/apply/celery_app.tasks.call_rasa_nlp\n    curl -X POST -u user1:password1 -d '{\"args\":[\"can I get a summary of the lesson?\"]}' http://localhost:5555/api/task/apply/celery_app.tasks.call_summarizer\n\n    \n## Configuring production instance\n\n### Setting up supervisor\n\nFollow: https://www.digitalocean.com/community/tutorials/how-to-install-and-manage-supervisor-on-ubuntu-and-debian-vps\n\nFirst confirm redis is installed and running for the celery command.\n\nInstall supervisor:\n\n    sudo apt install supervisor\n\nCopy supervisord conf file to local installation:\n\n    sudo cp supervisor/supervisord.conf /etc/supervisor\n\nStart supervisor:\n\n    sudo supervisord\n\nRestart ctl to reflect changes:\n\n    sudo supervisorctl update\n    sudo supervisorctl reload\n\nCheck that running:\n\n    sudo supervisorctl\n\nview log for celery\nvi /home/aaron/jugglechat/celery_app/celery_app.log\n\nkill all celery workers:\n\n    sudo pkill -f \"celery_app worker\"\n\n### Exposing Flower API on GCP\n\n1. Reserve a static IP and assign to VM. \n\n2. Allow HTTP traffic\n\n3. Whitelist jugglechat-experiment GCP cloud app URL\n\n### Running a Control Group for the Experiment\n\n## Experiment Configuration and Deployment\n\nFor the non-chat control group, simply complete step 4 with experimentalGroup set to \"control\". For the rest, \nfollow all listed steps.\n\n1. Modify celery_app > tasks.py call_rasa_nlp to pass in control_group (ie \"qa\" or \"faq\") to Allocator instantiation. This \nforces allocator to return that control group every time. There is no need for control_group with JuggleChat, as it is \nthe default.\n\n2. For QA or FAQ, modify supervisord.conf haystack variables to RUN_TYPE=experiment, READER_MODEL_PATH=\"models/qa_experiment\" . \nThe former  results in the elastic_search index used for retrieval to either be the faq_experiment (full FAQ dataset) or the \nqa_experiment and the latter points the qa chatbot's model path to the full QA set. For JuggleChat use RUN_TYPE=default,\nREADER_MODEL_PATH=\"models/qa_default\"\n\nNOTE: For the added experimental run to get results with deepset's extractive-QA model, RUN_TYPE=deepset, \nREADER_MODEL_PATH=\"deepset/roberta-base-squad2-covid\", with control_group=\"qa\" for (1)\n\n3. Restart all supervisor processes:\n\n\n    ```\n    sudo supervisorctl\n    restart all\n    ```\n\n4. In jugglechat-experiment > client > src > index.tsx change experimentalGroup to the experimental group of:\njugglechat (JuggleChat framework), qa (QA bot only), faq (FAQ bot only), or control (no chatbot). Update the \ncompletionCode if needed.\n\n5. Rebuild the jugglechat-experiment app by deleting all files under build then rebuilding:\n\n\n    ```\n    cd client\n    npm run build\n    cd ..\n    npm run build\n    ```\n    \n6. Redeploy the jugglechat-experiment app:\n\n\n    ```\n    gcloud app deploy\n    ```\n\nNOTE: The Worker IDs extracted by jugglechat-experiment > client/src/pages/GetId.tsx are stored in the worker_ids \ndatabase table and can be exported to use for exclusion lists in subsequent experiment runs.\n\n## Experiment Results Data Processing and Visualization\n\n    source .venv/bin/activate \n    cd experiment\n\nGenerates results.json:\n\n    python data_processing.py\n\nGenerates graphs comparing mean values of test scores, sentiment, percieved usefulness and accuracy, and (optionally) \nwordcloud images:\n\n    python data_visualization.py\n\n## Running Statistical Tests on Results Data\n\nRuns One-Way ANOVA test along with Tukey Honestly Significant Difference (HSD) post-hoc tests, generating sentiment, \nperceived accuracy, and perceived usefulness confidence interval comparison graphs for the latter:\n\n    source .venv/bin/activate \n    cd experiment\n    python statistical_tests.py\n"
    },
    {
      "name": "Accuro-Lab/Data-ML",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/65883023?s=40&v=4",
      "owner": "Accuro-Lab",
      "repo_name": "Data-ML",
      "description": "Repository for Data processing and AI/ML/NLP Code",
      "homepage": null,
      "language": "Python",
      "created_at": "2020-06-19T20:57:07Z",
      "updated_at": "2024-01-28T11:59:37Z",
      "topics": [],
      "readme": "# Data-ML\n\nOpen source repository for Accurolab and Data4Good ML scripts :)\n"
    },
    {
      "name": "Paeaede/taschenhirn",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/16348173?s=40&v=4",
      "owner": "Paeaede",
      "repo_name": "taschenhirn",
      "description": "follows",
      "homepage": null,
      "language": "Python",
      "created_at": "2020-01-12T17:00:29Z",
      "updated_at": "2021-04-18T15:28:44Z",
      "topics": [],
      "readme": "# taschenhirn\n\n\n### bad_scraper  \nFirst version of a scraper to only scrape data tables\n\n\n### qna_webapp  \nFirst version of a haystack + FastAPI based webapp runnable using uvicorn haystack_qna:app and Swagger UI under 127.0.0.1:8000/docs.\nMore details follow"
    },
    {
      "name": "AKJUS/opik",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/173074602?s=40&v=4",
      "owner": "AKJUS",
      "repo_name": "opik",
      "description": "Debug, evaluate, and monitor your LLM applications, RAG systems, and agentic workflows with comprehensive tracing, automated evaluations, and production-ready dashboards.",
      "homepage": "https://www.comet.com/docs/opik/",
      "language": "Python",
      "created_at": "2025-04-02T14:16:04Z",
      "updated_at": "2025-04-21T20:11:28Z",
      "topics": [],
      "readme": "<div align=\"center\"><b><a href=\"README.md\">English</a> | <a href=\"readme_CN.md\">简体中文</a> | <a href=\"readme_JP.md\">日本語</a> | <a href=\"readme_KO.md\">한국어</a></b></div>\n\n<h1 align=\"center\" style=\"border-bottom: none\">\n    <div>\n        <a href=\"https://www.comet.com/site/products/opik/?from=llm&utm_source=opik&utm_medium=github&utm_content=header_img&utm_campaign=opik\"><picture>\n            <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://raw.githubusercontent.com/comet-ml/opik/refs/heads/main/apps/opik-documentation/documentation/static/img/logo-dark-mode.svg\">\n            <source media=\"(prefers-color-scheme: light)\" srcset=\"https://raw.githubusercontent.com/comet-ml/opik/refs/heads/main/apps/opik-documentation/documentation/static/img/opik-logo.svg\">\n            <img alt=\"Comet Opik logo\" src=\"https://raw.githubusercontent.com/comet-ml/opik/refs/heads/main/apps/opik-documentation/documentation/static/img/opik-logo.svg\" width=\"200\" />\n        </picture></a>\n        <br>\n        Opik\n    </div>\n    Open source LLM evaluation framework<br>\n</h1>\n\n<p align=\"center\">\nFrom RAG chatbots to code assistants to complex agentic pipelines and beyond, build LLM systems that run better, faster, and cheaper with tracing, evaluations, and dashboards.\n</p>\n\n<div align=\"center\">\n\n[![Python SDK](https://img.shields.io/pypi/v/opik)](https://pypi.org/project/opik/)\n[![License](https://img.shields.io/github/license/comet-ml/opik)](https://github.com/comet-ml/opik/blob/main/LICENSE)\n[![Build](https://github.com/comet-ml/opik/actions/workflows/build_apps.yml/badge.svg)](https://github.com/comet-ml/opik/actions/workflows/build_apps.yml)\n<a target=\"_blank\" href=\"https://colab.research.google.com/github/comet-ml/opik/blob/master/apps/opik-documentation/documentation/docs/cookbook/opik_quickstart.ipynb\">\n\n  <!-- <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open Quickstart In Colab\"/> -->\n</a>\n\n</div>\n\n<p align=\"center\">\n    <a href=\"https://www.comet.com/site/products/opik/?from=llm&utm_source=opik&utm_medium=github&utm_content=website_button&utm_campaign=opik\"><b>Website</b></a> •\n    <a href=\"https://chat.comet.com\"><b>Slack community</b></a> •\n    <a href=\"https://x.com/Cometml\"><b>Twitter</b></a> •\n    <a href=\"https://www.comet.com/docs/opik/?from=llm&utm_source=opik&utm_medium=github&utm_content=docs_button&utm_campaign=opik\"><b>Documentation</b></a>\n</p>\n\n![Opik thumbnail](readme-thumbnail.png)\n\n## Important change on version 1.7.0\n**Please check the change log [here](CHANGELOG.md).**\n\n## 🚀 What is Opik?\n\nOpik is an open-source platform for evaluating, testing and monitoring LLM applications. Built by [Comet](https://www.comet.com?from=llm&utm_source=opik&utm_medium=github&utm_content=what_is_opik_link&utm_campaign=opik).\n\n<br>\n\nYou can use Opik for:\n* **Development:**\n\n  * **Tracing:** Track all LLM calls and traces during development and production ([Quickstart](https://www.comet.com/docs/opik/quickstart/?from=llm&utm_source=opik&utm_medium=github&utm_content=quickstart_link&utm_campaign=opik), [Integrations](https://www.comet.com/docs/opik/tracing/integrations/overview/?from=llm&utm_source=opik&utm_medium=github&utm_content=integrations_link&utm_campaign=opik))\n\n  * **Annotations:** Annotate your LLM calls by logging feedback scores using the [Python SDK](https://www.comet.com/docs/opik/tracing/annotate_traces/#annotating-traces-and-spans-using-the-sdk?from=llm&utm_source=opik&utm_medium=github&utm_content=sdk_link&utm_campaign=opik) or the [UI](https://www.comet.com/docs/opik/tracing/annotate_traces/#annotating-traces-through-the-ui?from=llm&utm_source=opik&utm_medium=github&utm_content=ui_link&utm_campaign=opik).\n\n  * **Playground:** Try out different prompts and models in the [prompt playground](https://www.comet.com/docs/opik/prompt_engineering/playground).\n\n* **Evaluation**: Automate the evaluation process of your LLM application:\n\n    * **Datasets and Experiments**: Store test cases and run experiments ([Datasets](https://www.comet.com/docs/opik/evaluation/manage_datasets/?from=llm&utm_source=opik&utm_medium=github&utm_content=datasets_link&utm_campaign=opik), [Evaluate your LLM Application](https://www.comet.com/docs/opik/evaluation/evaluate_your_llm/?from=llm&utm_source=opik&utm_medium=github&utm_content=eval_link&utm_campaign=opik))\n\n    * **LLM as a judge metrics**: Use Opik's LLM as a judge metric for complex issues like [hallucination detection](https://www.comet.com/docs/opik/evaluation/metrics/hallucination/?from=llm&utm_source=opik&utm_medium=github&utm_content=hallucination_link&utm_campaign=opik), [moderation](https://www.comet.com/docs/opik/evaluation/metrics/moderation/?from=llm&utm_source=opik&utm_medium=github&utm_content=moderation_link&utm_campaign=opik) and RAG evaluation ([Answer Relevance](https://www.comet.com/docs/opik/evaluation/metrics/answer_relevance/?from=llm&utm_source=opik&utm_medium=github&utm_content=alex_link&utm_campaign=opik), [Context Precision](https://www.comet.com/docs/opik/evaluation/metrics/context_precision/?from=llm&utm_source=opik&utm_medium=github&utm_content=context_link&utm_campaign=opik)\n\n    * **CI/CD integration**: Run evaluations as part of your CI/CD pipeline using our [PyTest integration](https://www.comet.com/docs/opik/testing/pytest_integration/?from=llm&utm_source=opik&utm_medium=github&utm_content=pytest_link&utm_campaign=opik)\n\n* **Production Monitoring**:\n    \n    * **Log all your production traces**: Opik has been designed to support high volumes of traces, making it easy to monitor your production applications. Even small deployments can ingest more than 40 million traces per day!\n    \n    * **Monitoring dashboards**: Review your feedback scores, trace count and tokens over time in the [Opik Dashboard](https://www.comet.com/docs/opik/production/production_monitoring/?from=llm&utm_source=opik&utm_medium=github&utm_content=dashboard_link&utm_campaign=opik).\n\n    * **Online evaluation metrics**: Easily score all your production traces using LLM as a Judge metrics and identify any issues with your production LLM application thanks to [Opik's online evaluation metrics](https://www.comet.com/docs/opik/production/rules/?from=llm&utm_source=opik&utm_medium=github&utm_content=dashboard_link&utm_campaign=opik)\n\n> [!TIP]  \n> If you are looking for features that Opik doesn't have today, please raise a new [Feature request](https://github.com/comet-ml/opik/issues/new/choose) 🚀\n\n<br>\n\n## 🛠️ Installation\nOpik is available as a fully open source local installation or using Comet.com as a hosted solution.\nThe easiest way to get started with Opik is by creating a free Comet account at [comet.com](https://www.comet.com/signup?from=llm&utm_source=opik&utm_medium=github&utm_content=install&utm_campaign=opik).\n\nIf you'd like to self-host Opik, you can do so by cloning the repository and starting the platform using Docker Compose:\n\nOn Linux or Mac do:\n```bash\n# Clone the Opik repository\ngit clone https://github.com/comet-ml/opik.git\n\n# Navigate to the repository\ncd opik\n\n# Start the Opik platform\n./opik.sh\n```\n\nOn Windows do:\n```powershell\n# Clone the Opik repository\ngit clone https://github.com/comet-ml/opik.git\n\n# Navigate to the repository\ncd opik\n\n# Start the Opik platform\npowershell -ExecutionPolicy ByPass -c \".\\opik.ps1\"\n```\n\nUse the `--help` or `--info` options to troubleshoot issues.\n\nOnce all is up and running, you can now visit [localhost:5173](http://localhost:5173) on your browser!\n\nFor more information about the different deployment options, please see our deployment guides:\n\n| Installation methods | Docs link |\n| ------------------- | --------- |\n| Local instance | [![Local Deployment](https://img.shields.io/badge/Local%20Deployments-%232496ED?style=flat&logo=docker&logoColor=white)](https://www.comet.com/docs/opik/self-host/local_deployment?from=llm&utm_source=opik&utm_medium=github&utm_content=self_host_link&utm_campaign=opik)\n| Kubernetes | [![Kubernetes](https://img.shields.io/badge/Kubernetes-%23326ce5.svg?&logo=kubernetes&logoColor=white)](https://www.comet.com/docs/opik/self-host/kubernetes/#kubernetes-installation?from=llm&utm_source=opik&utm_medium=github&utm_content=kubernetes_link&utm_campaign=opik)\n\n\n## 🏁 Get Started\n\nTo get started, you will need to first install the Python SDK:\n\n```bash\npip install opik\n```\n\nOnce the SDK is installed, you can configure it by running the `opik configure` command:\n\n```bash\nopik configure\n```\n\nThis will allow you to configure Opik locally by setting the correct local server address or if you're using the Cloud platform by setting the API Key\n\n> [!TIP]  \n> You can also call the `opik.configure(use_local=True)` method from your Python code to configure the SDK to run on the local installation.\n\nYou are now ready to start logging traces using the [Python SDK](https://www.comet.com/docs/opik/python-sdk-reference/?from=llm&utm_source=opik&utm_medium=github&utm_content=sdk_link2&utm_campaign=opik).\n\n### 📝 Logging Traces\n\nThe easiest way to get started is to use one of our integrations. Opik supports:\n\n| Integration | Description                                                                  | Documentation                                                                                                                                                      | Try in Colab                                                                                                                                                                                                                      |\n|-------------|------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| OpenAI      | Log traces for all OpenAI LLM calls                                          | [Documentation](https://www.comet.com/docs/opik/tracing/integrations/openai/?utm_source=opik&utm_medium=github&utm_content=openai_link&utm_campaign=opik)          | [![Open Quickstart In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/comet-ml/opik/blob/master/apps/opik-documentation/documentation/docs/cookbook/openai.ipynb)      |\n| LiteLLM     | Call any LLM model using the OpenAI format                                   | [Documentation](https://www.comet.com/docs/opik/tracing/integrations/litellm/?utm_source=opik&utm_medium=github&utm_content=openai_link&utm_campaign=opik)                                                                                                                  | [![Open Quickstart In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/comet-ml/opik/blob/master/apps/opik-documentation/documentation/docs/cookbook/litellm.ipynb)     |\n| LangChain   | Log traces for all LangChain LLM calls                                       | [Documentation](https://www.comet.com/docs/opik/tracing/integrations/langchain/?utm_source=opik&utm_medium=github&utm_content=langchain_link&utm_campaign=opik)    | [![Open Quickstart In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/comet-ml/opik/blob/master/apps/opik-documentation/documentation/docs/cookbook/langchain.ipynb)   |\n| Haystack    | Log traces for all Haystack calls                                            | [Documentation](https://www.comet.com/docs/opik/tracing/integrations/haystack/?utm_source=opik&utm_medium=github&utm_content=haystack_link&utm_campaign=opik)      | [![Open Quickstart In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/comet-ml/opik/blob/master/apps/opik-documentation/documentation/docs/cookbook/haystack.ipynb)    |\n| Anthropic   | Log traces for all Anthropic LLM calls                                       | [Documentation](https://www.comet.com/docs/opik/tracing/integrations/anthropic?utm_source=opik&utm_medium=github&utm_content=anthropic_link&utm_campaign=opik)     | [![Open Quickstart In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/comet-ml/opik/blob/master/apps/opik-documentation/documentation/docs/cookbook/anthropic.ipynb)   |\n| Bedrock     | Log traces for all Bedrock LLM calls                                         | [Documentation](https://www.comet.com/docs/opik/tracing/integrations/bedrock?utm_source=opik&utm_medium=github&utm_content=bedrock_link&utm_campaign=opik)         | [![Open Quickstart In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/comet-ml/opik/blob/master/apps/opik-documentation/documentation/docs/cookbook/bedrock.ipynb)     |\n| CrewAI      | Log traces for all CrewAI calls                                              | [Documentation](https://www.comet.com/docs/opik/tracing/integrations/crewai?utm_source=opik&utm_medium=github&utm_content=crewai_link&utm_campaign=opik)           | [![Open Quickstart In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/comet-ml/opik/blob/master/apps/opik-documentation/documentation/docs/cookbook/crewai.ipynb)      |\n| DeepSeek    | Log traces for all DeepSeek LLM calls                                        | [Documentation](https://www.comet.com/docs/opik/tracing/integrations/deepseek?utm_source=opik&utm_medium=github&utm_content=deepseek_link&utm_campaign=opik)       | |\n| DSPy        | Log traces for all DSPy runs                                                 | [Documentation](https://www.comet.com/docs/opik/tracing/integrations/dspy?utm_source=opik&utm_medium=github&utm_content=dspy_link&utm_campaign=opik)               | [![Open Quickstart In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/comet-ml/opik/blob/master/apps/opik-documentation/documentation/docs/cookbook/dspy.ipynb)        |\n| Gemini      | Log traces for all Gemini LLM calls                                          | [Documentation](https://www.comet.com/docs/opik/tracing/integrations/gemini?utm_source=opik&utm_medium=github&utm_content=gemini_link&utm_campaign=opik)           | [![Open Quickstart In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/comet-ml/opik/blob/master/apps/opik-documentation/documentation/docs/cookbook/gemini.ipynb)      |\n| Groq        | Log traces for all Groq LLM calls                                            | [Documentation](https://www.comet.com/docs/opik/tracing/integrations/groq?utm_source=opik&utm_medium=github&utm_content=groq_link&utm_campaign=opik)               | [![Open Quickstart In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/comet-ml/opik/blob/master/apps/opik-documentation/documentation/docs/cookbook/groq.ipynb)        |\n| Guardrails  | Log traces for all Guardrails validations                                    | [Documentation](https://www.comet.com/docs/opik/tracing/integrations/guardrails/?utm_source=opik&utm_medium=github&utm_content=guardrails_link&utm_campaign=opik)    | [![Open Quickstart In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/comet-ml/opik/blob/master/apps/opik-documentation/documentation/docs/cookbook/guardrails-ai.ipynb)   |\n| Instructor  | Log traces for all LLM calls made with Instructor                            | [Documentation](https://www.comet.com/docs/opik/tracing/integrations/instructor/?utm_source=opik&utm_medium=github&utm_content=instructor_link&utm_campaign=opik)    | [![Open Quickstart In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/comet-ml/opik/blob/master/apps/opik-documentation/documentation/docs/cookbook/instructor.ipynb)   |\n| LangGraph   | Log traces for all LangGraph executions                                      | [Documentation](https://www.comet.com/docs/opik/tracing/integrations/langgraph/?utm_source=opik&utm_medium=github&utm_content=langchain_link&utm_campaign=opik)    | [![Open Quickstart In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/comet-ml/opik/blob/master/apps/opik-documentation/documentation/docs/cookbook/langgraph.ipynb)   |\n| LlamaIndex  | Log traces for all LlamaIndex LLM calls                                      | [Documentation](https://www.comet.com/docs/opik/tracing/integrations/llama_index?utm_source=opik&utm_medium=github&utm_content=llama_index_link&utm_campaign=opik) | [![Open Quickstart In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/comet-ml/opik/blob/master/apps/opik-documentation/documentation/docs/cookbook/llama-index.ipynb) |\n| Ollama      | Log traces for all Ollama LLM calls                                          | [Documentation](https://www.comet.com/docs/opik/tracing/integrations/ollama?utm_source=opik&utm_medium=github&utm_content=ollama_link&utm_campaign=opik)           | [![Open Quickstart In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/comet-ml/opik/blob/master/apps/opik-documentation/documentation/docs/cookbook/ollama.ipynb)      |\n| Predibase   | Fine-tune and serve open-source Large Language Models                        | [Documentation](https://www.comet.com/docs/opik/tracing/integrations/predibase?utm_source=opik&utm_medium=github&utm_content=predibase_link&utm_campaign=opik)     | [![Open Quickstart In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/comet-ml/opik/blob/master/apps/opik-documentation/documentation/docs/cookbook/predibase.ipynb)   |\n| Pydantic AI | Fine-tune and serve open-source Large Language Models                        | [Documentation](https://www.comet.com/docs/opik/tracing/integrations/predibase?utm_source=opik&utm_medium=github&utm_content=predibase_link&utm_campaign=opik)     | [![Open Quickstart In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/comet-ml/opik/blob/master/apps/opik-documentation/documentation/docs/cookbook/predibase.ipynb)   |\n| Ragas       | PydanticAI is a Python agent framework designed to build production apps     | [Documentation](https://www.comet.com/docs/opik/tracing/integrations/pydantic-ai?utm_source=opik&utm_medium=github&utm_content=pydantic_ai_link&utm_campaign=opik) | [![Open Quickstart In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/comet-ml/opik/blob/master/apps/opik-documentation/documentation/docs/cookbook/pydantic-ai.ipynb) |\n| watsonx     | Log traces for all watsonx LLM calls                                         | [Documentation](https://www.comet.com/docs/opik/tracing/integrations/watsonx?utm_source=opik&utm_medium=github&utm_content=watsonx_link&utm_campaign=opik)         | [![Open Quickstart In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/comet-ml/opik/blob/master/apps/opik-documentation/documentation/docs/cookbook/watsonx.ipynb)     |\n\n> [!TIP]  \n> If the framework you are using is not listed above, feel free to [open an issue](https://github.com/comet-ml/opik/issues) or submit a PR with the integration.\n\nIf you are not using any of the frameworks above, you can also use the `track` function decorator to [log traces](https://www.comet.com/docs/opik/tracing/log_traces/?from=llm&utm_source=opik&utm_medium=github&utm_content=traces_link&utm_campaign=opik):\n\n```python\nimport opik\n\nopik.configure(use_local=True) # Run locally\n\n@opik.track\ndef my_llm_function(user_question: str) -> str:\n    # Your LLM code here\n\n    return \"Hello\"\n```\n\n> [!TIP]  \n> The track decorator can be used in conjunction with any of our integrations and can also be used to track nested function calls.\n\n### 🧑‍⚖️ LLM as a Judge metrics\n\nThe Python Opik SDK includes a number of LLM as a judge metrics to help you evaluate your LLM application. Learn more about it in the [metrics documentation](https://www.comet.com/docs/opik/evaluation/metrics/overview/?from=llm&utm_source=opik&utm_medium=github&utm_content=metrics_2_link&utm_campaign=opik).\n\nTo use them, simply import the relevant metric and use the `score` function:\n\n```python\nfrom opik.evaluation.metrics import Hallucination\n\nmetric = Hallucination()\nscore = metric.score(\n    input=\"What is the capital of France?\",\n    output=\"Paris\",\n    context=[\"France is a country in Europe.\"]\n)\nprint(score)\n```\n\nOpik also includes a number of pre-built heuristic metrics as well as the ability to create your own. Learn more about it in the [metrics documentation](https://www.comet.com/docs/opik/evaluation/metrics/overview?from=llm&utm_source=opik&utm_medium=github&utm_content=metrics_3_link&utm_campaign=opik).\n\n### 🔍 Evaluating your LLM Application\n\nOpik allows you to evaluate your LLM application during development through [Datasets](https://www.comet.com/docs/opik/evaluation/manage_datasets/?from=llm&utm_source=opik&utm_medium=github&utm_content=datasets_2_link&utm_campaign=opik) and [Experiments](https://www.comet.com/docs/opik/evaluation/evaluate_your_llm/?from=llm&utm_source=opik&utm_medium=github&utm_content=experiments_link&utm_campaign=opik).\n\nYou can also run evaluations as part of your CI/CD pipeline using our [PyTest integration](https://www.comet.com/docs/opik/testing/pytest_integration/?from=llm&utm_source=opik&utm_medium=github&utm_content=pytest_2_link&utm_campaign=opik).\n\n## ⭐ Star Us on GitHub\n\nIf you find Opik useful, please consider giving us a star! Your support helps us grow our community and continue improving the product.\n\n<img src=\"https://github.com/user-attachments/assets/ffc208bb-3dc0-40d8-9a20-8513b5e4a59d\" alt=\"Opik GitHub Star History\" width=\"600\"/>\n\n\n\n## 🤝 Contributing\n\nThere are many ways to contribute to Opik:\n\n* Submit [bug reports](https://github.com/comet-ml/opik/issues) and [feature requests](https://github.com/comet-ml/opik/issues)\n* Review the documentation and submit [Pull Requests](https://github.com/comet-ml/opik/pulls) to improve it\n* Speaking or writing about Opik and [letting us know](https://chat.comet.com)\n* Upvoting [popular feature requests](https://github.com/comet-ml/opik/issues?q=is%3Aissue+is%3Aopen+label%3A%22enhancement%22) to show your support\n\nTo learn more about how to contribute to Opik, please see our [contributing guidelines](CONTRIBUTING.md).\n"
    },
    {
      "name": "opea-project/Haystack-OPEA",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/165422283?s=40&v=4",
      "owner": "opea-project",
      "repo_name": "Haystack-OPEA",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2025-03-21T00:53:46Z",
      "updated_at": "2025-04-18T02:21:16Z",
      "topics": [],
      "readme": "# Haystack-OPEA\n\nThis package contains the Haystack integrations for OPEA Compatible [OPEA](https://opea.dev/) Microservices.\n\n## Installation\n\nYou can install Haystack OPEA package in several ways:\n\n### Install from Source\n\nTo install the package from the source, run:\n\n```bash\npip install poetry && poetry install --with test\n```\n\n### Install from Wheel Package\n\nTo install the package from a pre-built wheel, run:\n\n1. **Build the Wheels**: Ensure the wheels are built using Poetry.\n    ```bash\n    poetry build\n    ```\n2. **Install via Wheel File**: Install the package using the generated wheel file.\n    ```bash\n    pip install dist/haystack_opea-0.1.0-py3-none-any.whl\n    ```\n\n## Embeddings\n\nThe classes `OPEADocumentEmbedder` and `OPEATextEmbedder` are introduced.\n\n```python\nfrom haystack_opea.embedders.tei import OPEATextEmbedder\n\ntext_to_embed = \"I love pizza!\"\n\ntext_embedder = OPEATextEmbedder(api_url=\"http://localhost:6006\")\ntext_embedder.warm_up()\n\nprint(text_embedder.run(text_to_embed)\n```\n\nAnd similarly:\n\n```python\nfrom haystack import Document\nfrom haystack_opea.embedders.tei import OPEADocumentEmbedder\n\ndoc = Document(content=\"I love pizza!\")\n\ndocument_embedder = OPEADocumentEmbedder(api_url=\"http://localhost:6006\")\ndocument_embedder.warm_up()\n\nresult = document_embedder.run([doc])\nprint(result[\"documents\"][0].embedding)\n```\n\n## LLMs\n\nThe class `OPEAGenerator` is introduced:\n\n```python\nfrom haystack_opea.generators import OPEAGenerator\n\ngenerator = OPEAGenerator(\n    \"http://localhost:9009\",\n    model_arguments={\n        \"temperature\": 0.2,\n        \"top_p\": 0.7,\n        \"max_tokens\": 1024,\n    },\n)\ngenerator.warm_up()\nresult = generator.run(prompt=\"What is the answer?\")\n```\n\nFor more information, see [Haystack Docs](https://docs.haystack.deepset.ai/docs/intro) and [OPEA](https://opea.dev).\n"
    },
    {
      "name": "andremmfaria/rexis",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/400238?s=40&v=4",
      "owner": "andremmfaria",
      "repo_name": "rexis",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2025-03-26T21:57:12Z",
      "updated_at": "2025-04-19T19:38:35Z",
      "topics": [],
      "readme": "# 🔍 REXIS — Retrieval-Enhanced eXploration of Infected Software\n\n**REXIS** is an experimental framework designed to enhance static malware analysis using Large Language Models (LLMs) integrated with Retrieval-Augmented Generation (RAG). This project explores how contextual retrieval from external knowledge sources can improve the accuracy, interpretability, and justifiability of LLM-based malware classification.\n\nBuilt for cybersecurity research, **REXIS** focuses on analyzing static features (e.g., bytecode, file structure, API calls) and comparing its performance against traditional static analysis techniques.\n\n---\n\n## ✨ Key Features\n\n- 📦 Static malware analysis with LLMs  \n- 🔍 Context-aware insights using Retrieval-Augmented Generation  \n- 📊 Benchmarking against traditional detection techniques  \n- 🧠 Emphasis on explainability and contextual reasoning  \n\n---\n\n## 🛠️ Toolchain\n\n- **Code Retrieval & RAG Pipeline:**  \n  - [Haystack](https://github.com/deepset-ai/haystack) — used to build the pipeline between decompiled malware samples and the LLM\n\n- **AI Engine:**  \n  - [OpenAI](https://platform.openai.com/) — for general-purpose, high-accuracy LLM queries  \n  - [DeepSeek](https://github.com/deepseek-ai) — for code-centric language understanding and reasoning\n\n- **Static Analysis Input:**  \n  - Decompiled source code and structural features from known malware datasets  \n  - Recommended decompilation tools include:  \n    - [IDA Pro](https://hex-rays.com/ida-pro/)  \n    - [Ghidra](https://ghidra-sre.org/)  \n    - Any tool producing readable code or bytecode representations suitable for static analysis\n\n- **Datastore (Vector Database):**  \n  - [PostgreSQL](https://www.postgresql.org/) with [pgvector](https://github.com/pgvector/pgvector) extension  \n  - Used to store and query embeddings for Retrieval-Augmented Generation (RAG)  \n  - Integrated with the Haystack pipeline for vector-based semantic search and context retrieval\n\n\n---\n\n## 📂 Project Structure _(Coming Soon)_\n\n> This section will outline the repo structure, including modules for data ingestion, RAG querying, LLM prompts, and evaluation.\n\n---\n\n## 📈 Evaluation & Benchmarks _(Planned)_\n\n> REXIS will be tested against traditional static analysis tools and scored based on:\n- Accuracy of classification\n- Justifiability of output\n- Contextual relevance of LLM explanations\n- Efficiency of the analysis pipeline\n\n---\n\n## ⚙️ Installation & Setup\n\nREXIS uses Python `3.13+` and is managed using [PDM](https://pdm.fming.dev/).  \nEnsure you have Python 3.13 installed and [PostgreSQL](https://www.postgresql.org/) running with the [pgvector](https://github.com/pgvector/pgvector) extension enabled.\n\n### 📦 Prerequisites\n\n- Python 3.13+\n- PDM (`pip install pdm`)\n- PostgreSQL with pgvector extension\n- OpenAI and/or DeepSeek API credentials\n\n### 🚀 Setup Steps\n\n```bash\n# Clone the repo\ngit clone https://github.com/andremmfaria/rexis\ncd rexis\n\n# Install dependencies\npdm install\n\n# Create a ./config/.secrets.toml file for your API keys and database config\ncp ./config/.secrets_template.toml ./config/.secrets.toml\n```\n\n---\n\n## 🧪 Usage\n\nREXIS is containerized for reproducibility and ease of development. The project uses `docker-compose` to manage two main services:\n\n- `app`: The main application (e.g. RAG pipeline, interface, analysis logic)\n- `db`: PostgreSQL with the `pgvector` extension for vector-based semantic search\n\n---\n\n### 🐳 Step-by-Step Instructions\n\n1. **Create your `.env` file** in the root of the project by copying from the template file (`.env-template`):\n\n```dotenv\nPOSTGRES_USER=postgres\nPOSTGRES_PASSWORD=super_secret_password\nPOSTGRES_DB=rexis\n```\n\n2. **Build and start the containers**:\n\n```bash\ndocker compose up --build\n```\n\n3. **App source code and configuration**:\n   - Application code lives in `./src/`\n   - Configuration files (via Dynaconf) are in `./config/`\n\n4. **Stopping the containers**:\n\n```bash\ndocker compose down\n```\n\n5. **Persistent data**:  \nPostgreSQL data is stored in a Docker volume named `pgdata` and will persist between restarts.\n\n---\n\n## 📜 License\n\nThis project is licensed under the [MIT License](LICENSE).\n\n---\n\n## 👤 Author\n\n**Andre Faria**  \nMSc in Applied Cybersecurity  \nTechnological University Dublin — School of Informatics and Cyber Security  \nResearch Project: *Enhancing Static Malware Analysis with Large Language Models and Retrieval-Augmented Generation*\n"
    },
    {
      "name": "NoorFathima14/multimodal-chatbot",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/146946527?s=40&v=4",
      "owner": "NoorFathima14",
      "repo_name": "multimodal-chatbot",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2025-03-23T09:25:25Z",
      "updated_at": "2025-04-10T14:50:20Z",
      "topics": [],
      "readme": "Multimodal Chatbot\n===============\n\nA multimodal chatbot designed to simplify file querying, processing, and intelligent response generation. Buckman Chatbot supports diverse file types (PDF, CSV, DOCX, PNG, JPEG, MP4), processes text, images, and videos, and enhances responses with web search validation and caching for performance. Built with open-source technologies, it’s a user-friendly solution for students, researchers, educators, and professionals.\n\nOverview\n--------\n\nThis Chatbot is a web-based application that allows users to upload, process, and query files in various formats, including PDFs, images, and videos. It leverages multimodal processing to handle text, images, and videos, and uses natural language querying to provide intelligent, Markdown-formatted responses with syntax highlighting. The chatbot validates responses using web search (Google Custom Search API) and caches them with Redis for faster retrieval of repeated queries. It’s designed for academic research, content analysis, lecture summarization, and more, targeting students, researchers, educators, and professionals.\n\nKey Features\n------------\n\n*   **Diverse File Support**: Processes PDFs, images, videos (PDF, CSV, DOCX, PNG, JPEG, MP4).\n    \n*   **Efficient Querying**: Replaces manual searching with fast, automated extraction.\n    \n*   **Multimodal Processing**: Handles multiple media types with intelligent querying.\n    \n*   **Web Search Validation**: Enhances responses with Google Custom Search API.\n    \n*   **Response Caching**: Caches query responses in Redis for instant retrieval of previously used queries.\n    \n*   **Markdown Rendering**: Displays responses with syntax-highlighted code blocks using React-Markdown and React-Syntax-Highlighter.\n    \n*   **File Management**: Upload, list, and delete files with ease.\n    \n*   **User-Friendly Interface**: Straightforward and intuitive design built with React.\n    \n*   **Optimization**: Parallelizes file processing and web search, saving 5.61s (from 25.52s to 19.91s) for efficient resource utilization and faster responses.\n    \n*   **Containerization**: Uses Docker for seamless deployment and scalability across services.\n    \n\nTechnologies Used\n-----------------\n\n*   **Frontend**: React, Vite, React-Markdown, React-Syntax-Highlighter; Nginx for serving.\n    \n*   **Backend**: FastAPI, Tesseract OCR; Uvicorn for high-performance API serving.\n    \n*   **Database & Caching**: ChromaDB for vector storage; Redis container for caching.\n    \n*   **Containerization**: Docker and Dockerfile for scalability and consistency.\n    \n*   **NLP**: Sentence Transformers for embeddings and text processing.\n    \n*   **RAG Pipeline**: Haystack for retrieval-augmented generation, enabling efficient document querying.\n    \n*   **Web Search**: Google Custom Search API for response validation and enhancement.\n    \n*   **LLM**: Mistral-7B via Ollama container for local inference, enhancing multimodal processing.\n    \n\nInstallation\n------------\n\n### Prerequisites\n\n*   Python 3.8+\n    \n*   Node.js 16+\n    \n*   Docker and Docker Compose\n    \n*   Git\n    \n\n### Steps\n    \n1.  **Set Up Environment Variables**:\n    \n    *   bashCollapseWrapCopycp .env.example .env\n        \n    *   Update .env with your API keys and configuration (e.g., Google Custom Search API key, Redis settings).\n        \n2.  bash cd backend pip install -r requirements.txt\n    \n    \n3.  **Run with Docker**:\n    \n    *   Ensure Docker is running.\n        \n    *   This will start the backend (FastAPI), frontend (React), Redis, and Ollama containers.\n        \n\nUsage\n-----\n\n1.  **Upload Files**:\n    \n    *   Use the web interface to upload files (PDF, CSV, DOCX, PNG, JPEG, MP4).\n        \n    *   Supported file types are processed automatically (e.g., text extraction from PDFs, transcripts from videos).\n        \n2.  **Query the Chatbot**:\n    \n    *   Enter a natural language query, such as “What are the key findings in the file, and are they consistent with recent studies?”\n        \n    *   The chatbot processes the query, retrieves relevant content, validates with web search, and generates a response.\n        \n3.  **View Responses**:\n    \n    *   Responses are displayed in Markdown format with syntax-highlighted code blocks.\n        \n    *   Repeated queries are served instantly from the Redis cache.\n        \n4.  **Manage Files**:\n    \n    *   List and delete uploaded files as needed through the interface.\n        \n\nWorkflow\n--------\n### Workflow Steps\n\n1.  **User Interaction**: Upload files and submit a query via the React frontend.\n    \n2.  **File Processing**: Extract text/audio using FastAPI and Tesseract OCR.\n    \n3.  **Parallel Operations**: Process files and perform web search in parallel.\n    \n4.  **Sequential Embedding**: Generate embeddings with Sentence Transformers, stored in ChromaDB.\n    \n5.  **RAG Pipeline**: Retrieve relevant content using Haystack and ChromaDB.\n    \n6.  **Response Generation**: Generate a response with Mistral-7B via Ollama.\n    \n7.  **Caching**: Cache the response in Redis.\n    \n8.  **Response Delivery**: Display the Markdown-formatted response on the frontend.\n    \n\nContact\n-------\n\n*   **Author**: Noor Fathima\n    \n*   **GitHub**: [NoorFathima14](https://github.com/NoorFathima14)\n    \n\nFor questions, suggestions, or collaboration, feel free to open an issue or reach out directly!\n\nThis README provides a professional and detailed overview of your project, making it easy for others to understand, use, and contribute to Buckman Chatbot. Let me know if you’d like to adjust or add more sections!\n"
    },
    {
      "name": "ernanhughes/mars",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/155574808?s=40&v=4",
      "owner": "ernanhughes",
      "repo_name": "mars",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2025-03-24T15:58:38Z",
      "updated_at": "2025-03-27T04:14:11Z",
      "topics": [],
      "readme": "# mars"
    },
    {
      "name": "jenabesaman/LLM",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/94444692?s=40&v=4",
      "owner": "jenabesaman",
      "repo_name": "LLM",
      "description": "QR persian English",
      "homepage": null,
      "language": "Python",
      "created_at": "2024-10-07T17:22:29Z",
      "updated_at": "2025-03-24T09:23:49Z",
      "topics": [],
      "readme": "# chatbot\nhaystack version 2\nchatbot: persian English support\ndocument\nchat history for each seassion id with own database\nload local models\nqueue for requests\nusing elasticsearch as databse\nsupport delete and view elasticsearch indexes of chat history\ninclude open ai api version\ngood performance\n"
    },
    {
      "name": "bigdata-org/agentic-rag",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/196103522?s=40&v=4",
      "owner": "bigdata-org",
      "repo_name": "agentic-rag",
      "description": null,
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2025-03-24T04:53:26Z",
      "updated_at": "2025-03-28T16:06:34Z",
      "topics": [],
      "readme": "# AI-Powered Integrated Research Assistant (Agentic RAG)\n\n## Overview\nThis Proof of Concept (PoC) demonstrates the feasibility of an AI-powered Integrated Research Assistant designed to generate comprehensive research reports on NVIDIA. The system consolidates structured financial data, historical performance insights, and real-time industry trends using a multi-agent architecture.\n\n## System Architecture\nThe solution is built using a multi-agent system orchestrated with LangGraph, integrating:\n\n### Agents\n- **Snowflake Agent** – Queries and summarizes structured financial data (valuation metrics from Yahoo Finance stored in Snowflake).\n- **RAG Agent (Pinecone-powered)** – Retrieves historical insights from NVIDIA's quarterly reports using metadata-filtered search (Year, Quarter).\n- **Web Search Agent** – Fetches real-time industry news from APIs like SerpAPI, Tavily, or Bing.\n\nThese agents interact to synthesize a well-rounded research report, incorporating textual summaries, visualizations, and real-time insights.\n\n## Implementation Approach\n### Data Ingestion & Storage\n- **Structured Data:** Extracted from Yahoo Finance and stored in Snowflake.\n- **Unstructured Data:** NVIDIA's quarterly reports are chunked and indexed in Pinecone with metadata.\n- **Web Data:** Retrieved dynamically using a search API.\n\n### Processing & Analysis\n- **Pinecone-based RAG** for historical trend retrieval.\n- **Snowflake queries** for financial metrics and visualizations.\n- **LLM-based response generation** for summarizing results.\n\n### User Interaction\n- **Streamlit Interface** enables user queries with filtering options (Year/Quarter).\n- **Users can trigger** individual or combined agent responses for tailored insights.\n\n## Project Links\n- **GitHub Repository:** [Agentic RAG](https://github.com/bigdata-org/agentic-rag)\n- **Streamlit Frontend:** [Live Demo](https://nvidia-agentic-rag.streamlit.app/)\n- **Project Demonstration Video:** Call with Big Data-20250228_050702-Meeting Recording.mp4\n- **Backend API Endpoint:** [Agentic RAG Backend](https://agentic-rag-451496260635.us-central1.run.app/heartbeat)\n\n## Getting Started\n### Prerequisites\n- Python 3.8+\n- Snowflake Account\n- Pinecone API Key\n- Search API Key (SerpAPI, Tavily, or Bing)\n\n### Installation\nClone the repository:\n```bash\ngit clone https://github.com/bigdata-org/agentic-rag.git\ncd agentic-rag\n```\n\nInstall dependencies:\n```bash\npip install -r requirements.txt\n```\n\n### Running the Project\n1. Set up environment variables for API keys and database credentials.\n2. Start the backend service:\n   ```bash\n   python app.py\n   ```\n3. Run the Streamlit frontend:\n   ```bash\n   streamlit run app.py\n   ```\n\n## Contributing\nContributions are welcome! Feel free to submit issues or pull requests to enhance the project.\n\n## License\nThis project is licensed under the MIT License.\n\n"
    },
    {
      "name": "VCharrua/free-genai-bootcamp-2025",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/13697673?s=40&v=4",
      "owner": "VCharrua",
      "repo_name": "free-genai-bootcamp-2025",
      "description": "ExamPro GenAI Bootcamp 2025 - Vitor Repo",
      "homepage": null,
      "language": "Python",
      "created_at": "2025-02-08T12:38:19Z",
      "updated_at": "2025-04-11T20:35:55Z",
      "topics": [],
      "readme": "<!-- free-genai-bootcamp-2025 -->\n# ExamPro GenAI Bootcamp 2025 - AI Language Assistant\n\n![GitHub Repo stars](https://img.shields.io/github/stars/VCharrua/free-genai-bootcamp-2025) \n![GitHub Issues or Pull Requests](https://img.shields.io/github/issues/VCharrua/free-genai-bootcamp-2025) \n![GitHub last commit](https://img.shields.io/github/last-commit/VCharrua/free-genai-bootcamp-2025) \n![GitHub Release](https://img.shields.io/github/v/release/VCharrua/free-genai-bootcamp-2025) \n![GitHub Downloads (all assets, all releases)](https://img.shields.io/github/downloads/VCharrua/free-genai-bootcamp-2025/total)\n\n\n## Project Overview <!-- ✨ -->\n\nThis AI Language Assistant aims to extend the language offering and also augment the learning experience for students between instructor-led classes in a fictitious school website for the purpose of participating in the ExamPro GenAI Bootcamp 2025.\n\nThis project has the following main objectives:\n- Build a collection of learning apps using various different use-cases of AI\n- Maintain the learning experience the learning portal using AI developer tools\n- Extend the platform to support various different languages\n\n<br />\nAfter the conclusion of this bootcamp, the results and concepts materialized during the implementation of this project will be applied in a real solution to support the teaching and learning of national languages for the educational system in Angola.\n\n\n### Main Features\n\n## Usage Instructions <!-- 🚀 -->\n\nThe installation and execution of this project codebase will be directly linked to future choices about the system architecture to be adopted.\n\nWith the main options being cloud implementation or on-permissions environment, the installation and usage instructions will be updated to allow the correct use of the system.\n\nTo clone this repo, use the followin code:\n\n```sh\ngit clone https://github.com/VCharrua/free-genai-bootcamp-2025.git\ncd free-genai-bootcamp-2025\n``` \n\nFuture contributions may be allowed in forked versions of this repo.\n\n## Documentation <!-- 📚 -->\n\n> **Note:** The documentation and code contained in this repository will be updated during the event, thus detailing the various phases and concepts implemented in this solution.\n\nCheck the individual project documentation for more details.\n\n## Bootcamp Projects\n\nThis `Free GenAI Bootcamp 2025` repo contains the following projects developed during the event:\n\n| Project name | Description | Link | Deployment |\n|--------------|-------------|------|------------|\n| GenAI Architecting | Comprehensive architectural planning for the language learning platform, covering business requirements, data strategy, model selection criteria, infrastructure design, and governance considerations for supporting multiple languages and AI use-cases | [readme](./genai-architecting/readme.md) | N/A |\n| Language Portal | A comprehensive language learning platform for Portuguese and Kimbundu vocabulary acquisition serving as a vocabulary repository, learning record store, and unified launchpad for various language learning applications | [readme](./lang-portal/Readme.md) | [docker](./lang-portal/deployment/docker_compose) |\n| Language Portal - *Backend* | Backend component of the Language Portal project providing a Flask API for data management of vocabulary words, study sessions, and learning activities | [readme](./lang-portal/backend-flask/readme.md) | [docker](./lang-portal/backend-flask/Dockerfile) |\n| Language Portal - *Frontend* | Frontend component of the Language Portal project built with React to provide an intuitive user interface for vocabulary exploration and study session management | [readme](./lang-portal/frontend-react/README.md) | [docker](./lang-portal/frontend-react/Dockerfile) |\n| Sentence Construction | AI-powered tools for sentence construction using different LLM models including ChatGPT, Claude, and Meta AI | [readme](./sentence-construction/readme.md) | N/A |\n| Vocabulary Importer | Web application for generating thematic vocabulary categories for language learning using AI, expanding the Language Portal's vocabulary repository | [readme](./vocabulary-importer/README.md) | [docker](./vocabulary-importer/deployment/docker_compose) |\n| Ollama Mega-service | Implementation guide for running large language models locally using Ollama's efficient runtime with Open-WebUI frontend | [readme](./opea-comps/ollama/README.md) | [docker](./opea-comps/ollama/deployment/docker_compose) |\n| Translation Mega-service | OPEA Translation Mega-service architecture implementation guide providing powerful translation capabilities powered by large language models | [readme](./opea-comps/mega-service/readme.md) | [docker](./opea-comps/mega-service/translation/deployment/docker_compose) |\n\n\n## Resources \n\nThis section details the project resources and data used during the development of the solution and will be updated during the course of the bootcamp.\n\n## Bug Tracking and Feedback\n\nBugs will be tracked on [GitHub Issues](https://github.com/VCharrua/free-genai-bootcamp-2025/issues). In case of trouble, please check there if your issue has already been reported. If you spotted it first, help me fix-it it by providing a detailed and welcomed feedback.\n\n\n## Contributing <!-- 🤝 -->\n\n> **Note** : During the course of the bootcamp, contributions will be welcomed in the form of feedback on the [issues page](https://github.com/VCharrua/free-genai-bootcamp-2025/issues). Future collaboration may be allowed and detailed in the [contribution guide](https://github.com/VCharrua/free-genai-bootcamp-2025/CONTRIBUTING.md).\n\nPlease ⭐️ this repository if this project helped you!\n\n## License <!-- 📃 -->\n\nDuring this phase, this codebase licensing model will be based on the [original repo](https://github.com/ExamProCo/free-genai-bootcamp-2025) conditions, subject to future changes. \n\n## Credits\nThanks to the [ExamPro](https://exampro.co/) Training Team, Super Gest Instructors and all the Sponsors for this incredible Bootcamp and all the effort involved in this project.\n\n"
    },
    {
      "name": "Jackela/mcp-academic-rag-system",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/27112787?s=40&v=4",
      "owner": "Jackela",
      "repo_name": "mcp-academic-rag-system",
      "description": "Model Context Protocol (MCP) 服务器，提供学术文献OCR处理、智能检索与RAG功能",
      "homepage": null,
      "language": "Python",
      "created_at": "2025-03-15T22:39:36Z",
      "updated_at": "2025-03-25T05:11:28Z",
      "topics": [],
      "readme": "# MCP学术文献RAG服务器\n\n基于Model Context Protocol (MCP)的学术文献检索增强生成(RAG)服务器，提供文献OCR处理、自动分类、智能检索与AI交互功能。\n\n## ⚠️ 开发状态警告\n\n**当前项目状态：原型开发中 - 尚未完成**\n\n本项目目前处于积极开发阶段，尚未准备好用于生产环境。API、功能和结构可能会发生重大变化。欢迎贡献和反馈，但请注意当前的不稳定性。\n\n## MCP集成功能\n\n作为MCP服务器，本项目将提供以下MCP功能：\n\n### 工具 (Tools)\n\n- **文献搜索工具**：通过关键词、主题或语义查询查找相关文献\n- **文献处理工具**：上传、OCR处理和结构化文献内容\n- **聊天会话工具**：管理基于文献内容的对话交互\n\n### 资源 (Resources)\n\n- **文献资源**：访问已处理文献的结构化内容\n- **会话历史**：查看和继续之前的交互记录\n- **文献集合**：管理主题相关的文献分组\n\n### 提示模板 (Prompts)\n\n- **文献分析提示**：用于分析和总结文献内容\n- **比较研究提示**：比较多篇文献的内容和观点\n- **论文撰写辅助提示**：帮助构建论文结构和引用\n\n## 系统功能\n\n本系统是一个基于API的学术文献OCR电子化、自动分类与智能检索平台，采用流水线架构处理学术文献，将扫描文档转换为结构化电子格式，并提供基于向量数据库的智能检索与自然语言对话功能。\n\n- **文档OCR处理**：将扫描的学术文献转换为可搜索文本\n- **文档结构识别**：自动识别标题、摘要、章节等结构元素\n- **内容自动分类**：基于内容对文献进行主题分类和标签标注\n- **格式转换**：生成Markdown和PDF输出，保留原文排版\n- **向量化存储**：将文档内容转换为向量表示并存入向量数据库\n- **智能检索**：通过自然语言查询检索相关文献内容\n- **知识对话**：基于文献内容回答用户问题，提供引用来源\n\n## 开发路线图\n\n- [x] 基础文档处理流水线实现\n- [x] 命令行工具开发\n- [x] 基本RAG功能实现\n- [ ] **MCP服务器接口实现**\n- [ ] MCP工具 (Tools) 功能开发\n- [ ] MCP资源 (Resources) 功能开发\n- [ ] MCP提示 (Prompts) 功能开发\n- [ ] Web界面开发\n- [ ] 高级RAG功能增强\n- [ ] 安全性和性能优化\n- [ ] 文档与教程完善"
    },
    {
      "name": "MuhammadAbdullah95/Hotel_Receptionist_AI_Agent",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/147532804?s=40&v=4",
      "owner": "MuhammadAbdullah95",
      "repo_name": "Hotel_Receptionist_AI_Agent",
      "description": null,
      "homepage": null,
      "language": "HTML",
      "created_at": "2025-03-14T20:41:40Z",
      "updated_at": "2025-03-24T08:10:08Z",
      "topics": [],
      "readme": "# 🏨 Hotel Receptionist Chatbot\n\nWelcome to the **Hotel Receptionist Chatbot** project — an AI-powered virtual receptionist designed to handle frequently asked questions (FAQs) from hotel guests. Built with **Haystack** for natural language processing (NLP) and **FastAPI** for backend services, this chatbot offers a seamless and interactive experience for users, mimicking a real hotel concierge.\n\n---\n\n## 📁 Table of Contents\n\n1. [Project Overview](#project-overview)  \n2. [Technical Architecture](#technical-architecture)  \n3. [Tech Stack](#tech-stack)  \n4. [Features](#features)  \n5. [Setup and Installation](#setup-and-installation)  \n6. [Usage](#usage)  \n7. [Contributing](#contributing)  \n8. [Acknowledgments](#acknowledgments)  \n\n---\n\n## 🚀 Project Overview\n\nThe **Hotel Receptionist Chatbot** uses **Retrieval-Augmented Generation (RAG)** to provide accurate, context-aware answers to user queries. It combines **retrieval** and **generation** techniques for optimal responses.\n\n### Key Components:\n- **Hugging Face Embeddings Model**: Generates embeddings from a predefined dataset (`FAQs.txt`) to retrieve relevant information.\n- **Gemini Model**: Refines and generates human-like, contextually accurate responses.\n- **Pinecone**: Pinecone vector database to store and retrieve embeddings.\n\nThis chatbot is designed to be both **intelligent** and **visually appealing**, reflecting the ambiance of a luxurious hotel environment.\n\n---\n\n## 🛠 Technical Architecture\n\nThe system is structured as follows:\n\n### 1. **Frontend**:\n- Responsive and elegant chat interface using **HTML**, **CSS**, and **JavaScript**.\n- **Bootstrap** for responsive UI components.\n- **SweetAlert2** for attractive and user-friendly alert messages.\n\n### 2. **Backend**:\n- Built on **FastAPI** for high-performance API management.\n- Handles chat queries, invokes NLP pipeline, and returns refined answers.\n\n### 3. **AI Pipeline**:\n- **Hugging Face Embeddings Model**: Retrieves relevant data from FAQs.\n- **Gemini Model**: Generates natural and polished responses.\n- **Haystack RAG pipeline** to integrate retrieval and generation seamlessly.\n\n---\n\n## ⚙️ Tech Stack\n\n| Technology      | Description                                     |\n|-----------------|-------------------------------------------------|\n| **Haystack**    | NLP framework for document retrieval & QA.      |\n| **Gemini Model** | For interating with the user in Conversational way. |\n| **Pinecone** | A vector Database for as a RAG Storage          |\n| **FastAPI**     | High-performance Python backend framework.     |\n| **HTML/CSS/JS** | Frontend structure and interactivity.           |\n| **Bootstrap**   | Responsive, modern UI components.               |\n| **SweetAlert2** | Beautiful and responsive alert modals.          |\n\n\n---\n\n## ✨ Features\n\n- **Interactive Chat Interface**: Real-time conversations with a chatbot.\n- **AI-Powered, Context-Aware Responses**: High-quality, intelligent replies using RAG.\n- **Luxurious Design**: Chat interface mimicking hotel lobby ambiance.\n- **Error Handling**: Graceful error messages for unrecognized queries.\n- **Fully Responsive**: Works seamlessly across devices.\n\n---\n\n## ⚙️ Setup and Installation\n\n### ✅ Prerequisites\n- **Python 3.7+**\n- **pip** (Python package manager)\n\n---\n\n### 📥 Installation Steps\n\n1. **Clone the Repository**:\n```bash\ngit clone https://github.com/MuhammadAbdullah95/Hotel_Receptionist_AI_Agent.git\ncd Hotel_Receptionist_AI_Agent\n```\n\n2. **Set Up a Virtual Environment**:\n```bash\npython -m venv venv\nsource venv/bin/activate  # On Windows: venv\\Scripts\\activate\n```\n\n3. **Install Dependencies**:\n```bash\npip install -r requirements.txt\n```\n\n4. **Run the FastAPI Server**:\n```bash\nuvicorn app:app --reload\n```\n\n5. **Open the Chatbot Interface**:\n- Open `index.html` located in the `templates` folder using any browser.\n- Or serve via local server:\n```bash\npython -m http.server\n```\n- Visit `http://localhost:8000` in your browser.\n\n---\n\n## 💬 Usage\n\n### 1. **Open the Chatbot Interface**:\n- Launch `index.html` or navigate to the hosted URL.\n\n### 2. **Start Chatting**:\n- Type your question in the chatbox and hit **Send** or press **Enter**.\n\n### 3. **Sample Queries**:\n- \"What time is check-in?\"\n- \"Do you offer room service?\"\n- \"Is breakfast included in the stay?\"\n\n### 4. **View Responses**:\n- AI-generated responses appear instantly in a conversational format.\n\n---\n\n\n## 🔑 API Keys Setup\n\nBefore running the project, make sure you have the following API keys:\n\n1. **Google API Key** - for accessing Google services (e.g., search, knowledge retrieval).\n2. **Pinecone API Key** - for vector database operations (e.g., storing and retrieving embeddings).\n3. **Hugging Face Token (HF_TOKEN)** - for accessing hosted LLM models and pipelines.\n\n### ⚙️ How to Set API Keys\n\nYou can set these API keys as **environment variables** to keep them secure and avoid hardcoding them into your code.\n\n#### 🖥️ Option 1: Temporary (per session)\n\n```bash\nexport GOOGLE_API_KEY='your_google_api_key_here'\nexport PINECONE_API_KEY='your_pinecone_api_key_here'\nexport HF_TOKEN='your_huggingface_token_here'\n```\n\n#### 📁 Option 2: Permanent (via `.env` file)\n\n1. Create a `.env` file in the root directory of your project:\n    ```bash\n    touch .env\n    ```\n\n2. Add your keys inside the `.env` file:\n    ```ini\n    GOOGLE_API_KEY=your_google_api_key_here\n    PINECONE_API_KEY=your_pinecone_api_key_here\n    HF_TOKEN=your_huggingface_token_here\n    ```\n\n3. Make sure to load this `.env` file in your Python app (if using something like `python-dotenv`):\n    ```python\n    from dotenv import load_dotenv\n    load_dotenv()\n    ```\n\n4. ⚠️ **Important**: Add `.env` to your `.gitignore` to avoid exposing keys:\n    ```\n    # .gitignore\n    .env\n    ```\n\n### 🚨 Notes:\n- Never share or commit your API keys publicly.\n- Regenerate API keys if you suspect they are compromised.\n\nFeel free to contribute or suggest improvements! ✨\n\n\n\n\n\n## 🤝 Contributing\n\nWe welcome contributions! Follow these steps to contribute:\n\n1. **Fork** the repository.\n2. **Create a branch** for your feature or bugfix:\n```bash\ngit checkout -b feature-name\n```\n3. **Commit** your changes:\n```bash\ngit commit -m \"Add feature\"\n```\n4. **Push** your branch:\n```bash\ngit push origin feature-name\n```\n5. **Open a Pull Request** for review.\n\n---\n\n## 🙏 Acknowledgments\n\n- **Haystack**: NLP framework for retrieval and generation.\n- **FastAPI**: Backend API framework.\n- **Bootstrap**: Beautiful and responsive UI toolkit.\n- **SweetAlert2**: For stunning alert dialogs.\n\n---\n\n\n**Enjoy a seamless hotel experience with our AI Receptionist!** 🌐✨\n\n"
    },
    {
      "name": "AdeptTechSolutions/aging-res",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/61491605?s=40&v=4",
      "owner": "AdeptTechSolutions",
      "repo_name": "aging-res",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2025-03-13T09:42:27Z",
      "updated_at": "2025-04-22T20:42:31Z",
      "topics": [],
      "readme": ""
    },
    {
      "name": "danielfleischer/haystack_opea",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/22022514?s=40&v=4",
      "owner": "danielfleischer",
      "repo_name": "haystack_opea",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2025-03-12T13:38:38Z",
      "updated_at": "2025-03-20T10:07:05Z",
      "topics": [],
      "readme": "# Haystack-OPEA\n\nThis package contains the Haystack integrations for OPEA Compatible [OPEA](https://opea.dev/) Microservices.\n\n## Installation\n\nYou can install Haystack OPEA package in several ways:\n\n### Install from Source\n\nTo install the package from the source, run:\n\n```bash\npip install poetry && poetry install --with test\n```\n\n### Install from Wheel Package\n\nTo install the package from a pre-built wheel, run:\n\n1. **Build the Wheels**: Ensure the wheels are built using Poetry.\n    ```bash\n    poetry build\n    ```\n2. **Install via Wheel File**: Install the package using the generated wheel file.\n    ```bash\n    pip install dist/haystack_opea-0.1.0-py3-none-any.whl\n    ```\n\n## Embeddings\n\nThe classes `OPEADocumentEmbedder` and `OPEATextEmbedder` are introduced.\n\n```python\nfrom haystack_opea.embedders.tei import OPEATextEmbedder\n\ntext_to_embed = \"I love pizza!\"\n\ntext_embedder = OPEATextEmbedder(api_url=\"http://localhost:6006\")\ntext_embedder.warm_up()\n\nprint(text_embedder.run(text_to_embed)\n```\n\nAnd similarly:\n\n```python\nfrom haystack import Document\nfrom haystack_opea.embedders.tei import OPEADocumentEmbedder\n\ndoc = Document(content=\"I love pizza!\")\n\ndocument_embedder = OPEADocumentEmbedder(api_url=\"http://localhost:6006\")\ndocument_embedder.warm_up()\n\nresult = document_embedder.run([doc])\nprint(result[\"documents\"][0].embedding)\n```\n\n## LLMs\n\nThe class `OPEAGenerator` is introduced:\n\n```python\nfrom haystack_opea.generators import OPEAGenerator\n\ngenerator = OPEAGenerator(\n    \"http://localhost:9009\",\n    model_arguments={\n        \"temperature\": 0.2,\n        \"top_p\": 0.7,\n        \"max_tokens\": 1024,\n    },\n)\ngenerator.warm_up()\nresult = generator.run(prompt=\"What is the answer?\")\n```\n\nFor more information, see [Haystack Docs](https://docs.haystack.deepset.ai/docs/intro) and [OPEA](https://opea.dev).\n"
    },
    {
      "name": "bigdata-org/data-extraction-and-standardization",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/196103522?s=40&v=4",
      "owner": "bigdata-org",
      "repo_name": "data-extraction-and-standardization",
      "description": null,
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2025-01-22T19:39:26Z",
      "updated_at": "2025-03-14T03:08:01Z",
      "topics": [],
      "readme": "# Data Extraction and Standardization\n\n## Quick Start Guide\n\n### Setup\n\n1. **Clone the Repository**\ngit clone <repository-url>\ncd data-extraction-and-standardization\ntext\n\n2. **Configure Python Environment**\npython -m venv .venv\nFor Unix/macOS\nsource .venv/bin/activate\nFor Windows\n.venv\\Scripts\\activate\ntext\n\n3. **Install Dependencies**\npip install -r requirements.txt\ntext\n\n### Launch Applications\n\n**Backend Service**\ncd backend\nuvicorn app:app --reload --port 8000\ntext\n\n**Frontend Interface**\ncd frontend\nstreamlit run streamlit-app.py --server.port=8501\ntext\n\n## Access Points\n\n| Service | URL | Description |\n|---------|-----|-------------|\n| FastAPI Docs | http://localhost:8000/docs | API documentation and testing interface |\n| Streamlit App | http://localhost:8501 | User-friendly web interface |\n\n---\n"
    },
    {
      "name": "md-nobir-hasan/fastapi-personality-test",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/96966076?s=40&v=4",
      "owner": "md-nobir-hasan",
      "repo_name": "fastapi-personality-test",
      "description": "Make a admin panel step by step",
      "homepage": null,
      "language": "HTML",
      "created_at": "2025-03-06T06:20:13Z",
      "updated_at": "2025-04-22T08:11:21Z",
      "topics": [],
      "readme": "\n"
    },
    {
      "name": "HarshNevse/RAG_Agent",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/110392719?s=40&v=4",
      "owner": "HarshNevse",
      "repo_name": "RAG_Agent",
      "description": "This project implements a Retrieval-Augmented Generation (RAG) system that enhances generative AI models with information retrieval capabilities.",
      "homepage": "",
      "language": "HTML",
      "created_at": "2025-03-01T06:00:45Z",
      "updated_at": "2025-03-13T10:29:26Z",
      "topics": [
        "artificial-intelligence",
        "flask",
        "llm",
        "python",
        "retrieval-augmented-generation"
      ],
      "readme": "# Retrieval-Augmented Generation (RAG) Agent Project\n\n## Overview\nThis project implements a Retrieval-Augmented Generation (RAG) system that enhances generative AI models with information retrieval capabilities. It allows users to upload documents, query the system, and receive responses based on both stored knowledge and real-time retrieval.\n\n## Retrieval Augmented Generation\n![image](https://github.com/user-attachments/assets/e7b2ca31-00da-4464-b97a-fbf224a03466)\n![image](https://github.com/user-attachments/assets/09d4fdff-b58e-4340-b890-ab315092695b)\n\n\n- **Text Embedder**: Embeds the user query into a vector representation. --->**Retriever**: Takes the query embedding, compares it with the stored document embeddings, and retrieves relevant documents.​\n- **Retriever**: Relevant documents retrieved ---> **Prompt Builder**: Uses the retrieved documents and the original query to create a structured prompt for the language model. ​\n- **Prompt Builder**: sends structured prompt to the llm ---> **LLM**: Uses the generated prompt to generate a response based on the context provided by the retrieved documents.​\n\n\n## Features\n- **File Upload**: Users can upload documents to populate the knowledge base.\n- **Chat Assistant**: A conversational interface powered by RAG.\n- **Prompt Management**: Save and manage prompts for better interactions.\n- **Backend API Integration**: Uses a backend to handle file processing and retrieval.\n- **User-Friendly Interface**: Simple and intuitive UI built with HTML, CSS, and JavaScript.\n\n## Tech Stack\n- **Frontend**: HTML, CSS, JavaScript\n- **Backend**: Python (Flask)\n- **Database**: InMemory\n- **AI Model**: Mistral Instruct 7b v0.3 via HuggingFace\n- **Vector Search**: sentence-transformers/all-MiniLM-L6-v2\n\n## 📂 Project Structure\n```\n📁 project-root\n ├── 📂 templates       # Contains index.html\n ├── 📄 app.py            # core python code wrapped in flask.\n ├── 📄 requirements.txt  # Dependencies\n ├── 📄 README.md      # Project Documentation\n```\n\n## Installation\n1. Clone the repository:\n   ```bash\n   git clone https://github.com/HarshNevse/RAG_Agent.git\n   cd rag-project\n   ```\n2. Create a virtual environment and activate it:\n   ```bash\n   python -m venv env\n   source env/bin/activate  # For macOS/Linux\n   env\\Scripts\\activate  # For Windows\n   ```\n3. Install dependencies:\n   ```bash\n   pip install -r requirements.txt\n   ```\n4. Set your HuggingFace API key:\n    ```bash\n   set HF_API_KEY=hf_your_api_key_here\n   ```\n5. Run the backend server:\n   ```bash\n   python app.py  # Flask example\n   ```\n\n## Usage\n- **Upload Files**: Click on the upload section to add documents.\n- **Ask Questions**: Type queries in the chat interface.\n- **Retrieve Information**: The AI model will fetch and generate responses based on the documents.\n\n## Frontend Preview\n![9_](https://github.com/user-attachments/assets/788d2d64-c677-4996-9074-35cb717a914a)\n\n\n## Changelog\n- 12/03/2025: Added support for multiple file types (PDF, CSV, Markdown etc.)\n\n\n## Future Enhancements\n- Multi-user authentication & access control\n- Cloud storage support (AWS S3 / Firebase)\n- Advanced document parsing with NLP techniques\n- Support for multiple languages\n\n\n---\n**Author:** Harsh Nevse \n**GitHub:** [HarshNevse](https://github.com/HarshNevse)  \n**Email:** harshnevse29@gmail.com\n\n"
    },
    {
      "name": "hek1412/JupyterHub_v1",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/183311011?s=40&v=4",
      "owner": "hek1412",
      "repo_name": "JupyterHub_v1",
      "description": "Юпитер хаб с образом quay.io/jupyter/pytorch-notebook:cuda12-python-3.11.8 и доп библиотеками(9гб)",
      "homepage": null,
      "language": "Python",
      "created_at": "2025-02-28T22:50:42Z",
      "updated_at": "2025-03-01T17:40:05Z",
      "topics": [],
      "readme": "# JupyterHub_v1\n## Описание проекта JupyterHub с Docker и GitHub OAuth\n\nЭтот проект представляет собой создание JupyterHub, работающего в контейнере Docker, с поддержкой GPU через NVIDIA runtime, аутентификации пользователей через GitHub OAuth и использования DockerSpawner для динамического создания окружений для каждого пользователя с кастомным образом \n ноутбука юпитера.\n\n---\n\n## Содержание\n1. [Структура проекта](#структура-проекта)\n2. [Создание Docker-образа](#создание-docker-образа)\n3. [Docker Compose](#docker-compose)\n4. [Dockerfile](#dockerfile)\n5. [Конфигурация JupyterHub](#конфигурация-jupyterhub)\n6. [Переменные окружения](#переменные-окружения)\n7. [Запуск JupyterHub](#запуск-JupyterHub)\n\n---\n\n## Структура проекта\n\n```\njupyterhub\n│\n├── docker-compose.yaml\n│\n├── Dockerfile.jupyterhub\n│\n├──jupyterhub_config.py\n│\n└── jupytercastom/\n      ├── Dockerfile.jupyterlab\n      └── requirements.txt\n```\n\n---\n\n## Создание Docker-образа\n\nСначалоа необходимо создать Docker-образ с необходимыми зависимостями для работы пользовательских контейнеров в JupyterHub, а затем опубликовать его в Docker Hub для дальнейшего использования.\nПереходим в директорию `jupytercastom/` и собираем образ на базе  `quay.io/jupyter/pytorch-notebook:cuda12-python-3.11.8` и установкой дополнительных библиотек.\n\n```\ndocker build -t jupyterlab-v2 -f Dockerfile.jupyterlab .\n```\nпроверяем наличие образа\n```\ndocker images\n```\nЛогинимся\n```\ndocker login\n```\nПрисваиваем тэг\n```\ndocker tag jupyterlab-v2  hek1412/dockerfile.jupyterlab_v2:latest \n```\nОтправляем собранный образ, тепеь он доступен из репозитория\n```\ndocker push hek1412/dockerfile.jupyterlab_v2:latest\n```\n![image](https://github.com/user-attachments/assets/9bc522e4-2671-42e6-9b98-bb0ae2ea784b)\n\n\n---\n\n## Docker Compose\n\n### Создаем `docker-compose.yaml` для разворачивания jupyterhub \n(предварительно переходим в директорию jupyterhub) \n```\nservices:\n  jupyterhub:\n    build:\n      context: .\n      dockerfile: Dockerfile.jupyterhub\n    restart: always\n    image: jupyterhub\n    container_name: jupyterhub\n    networks:\n      - jupyterhub-network\n    volumes:\n      - \"./jupyterhub_config.py:/srv/jupyterhub/jupyterhub_config.py\" # Монтируем конфигурационный файл\n      - \"/var/run/docker.sock:/var/run/docker.sock:rw\"               # Даем доступ к Docker daemon\n      - \"jupyterhub-data:/srv/jupyterhub/data\"                       # Общий том для данных\n    ports:\n      - \"35001:8000\"                                                 # Открываем порт для JupyterHub\n    environment:\n      - JUPYTERHUB_BASE_URL=/hub                                     # Базовый URL для JupyterHub\n      - JUPYTERHUB_URL=http://skayfaks.keenetic.pro:35001            # Полный URL JupyterHub\n      - GITHUB_CLIENT_ID=${GITHUB_CLIENT_ID}                         # ID клиента GitHub OAuth\n      - GITHUB_TOKEN=${GITHUB_TOKEN}                                 # Токен GitHub OAuth\n    runtime: nvidia                                                  # Используем NVIDIA runtime\n    deploy:\n      resources:\n        reservations:\n          devices:\n            - driver: nvidia                                         # Резервируем все доступные GPU\n              count: all\n              capabilities: [gpu]\n\nvolumes:\n  jupyterhub-data:                                                   # Общий том для хранения данных\n\nnetworks:\n  jupyterhub-network:\n    name: jupyterhub-network                                         # Создаем сеть для контейнеров\n```\n\n---\n\n## Dockerfile\n\n### Создаем `Dockerfile.jupyterhub` для сборки образа нашего jupyterhub\n\n```\nFROM jupyterhub/jupyterhub:latest\nWORKDIR /srv/jupyterhub\n\n# Обновляем пакеты и устанавливаем необходимые зависимости\nRUN apt-get update -y && \\\n    python3 -m pip install --no-cache-dir \\\n    dockerspawner \\\n    python-dotenv \\\n    oauthenticator\n\n# Копируем конфигурационный файл\nCOPY jupyterhub_config.py /srv/jupyterhub/jupyterhub_config.py\n\n# Запускаем JupyterHub с указанным конфигурационным файлом\nCMD [\"jupyterhub\", \"-f\", \"/srv/jupyterhub/jupyterhub_config.py\"]\n```\n\n---\n\n## Конфигурация JupyterHub\n\n### Создаем `jupyterhub_config.py` с параметрами:\n\n```\nc.JupyterHub.bind_url = 'http://:8000'                           # URL, на котором будет запущен JupyterHub\nc.JupyterHub.hub_bind_url = 'http://0.0.0.0:8081'                # URL для внутреннего общения между компонентами\nc.JupyterHub.start_timeout = 360                                 # Максимальное время ожидания старта процесса (секунды)\nc.JupyterHub.shutdown_no_activity_timeout = 600                  # Автоматическое завершение работы при простоях (секунды)\nc.JupyterHub.shutdown_on_logout = True                           # Автоматическая остановка сервера при выходе из системы\nc.DockerSpawner.use_internal_ip = True                           # Использование внутренних IP-адресов Docker\nc.JupyterHub.authenticator_class = GitHubOAuthenticator          # Использование GitHub OAuth\nc.GitHubOAuthenticator.client_id = os.getenv('GITHUB_CLIENT_ID') # ID клиента GitHub OAuth\nc.GitHubOAuthenticator.client_secret = os.getenv('GITHUB_TOKEN') # Секретный токен GitHub OAuth\nc.GitHubOAuthenticator.oauth_callback_url = 'https://jupiter45.skayfaks.keenetic.pro/hub/oauth_callback'\nc.OAuthenticator.admin_users = {'hek1412'}                       # Администраторы JupyterHub\nc.GitHubOAuthenticator.allowed_organizations = {'1T45git'}       # Разрешенные организации GitHub\nc.GitHubOAuthenticator.scope = ['read:user', 'user:email', 'read:org']\nc.JupyterHub.spawner_class = 'dockerspawner.DockerSpawner'       # Использование DockerSpawner\nc.DockerSpawner.image = 'hek1412/dockerfile.jupyterlab_v2:latest' # Образ Docker для пользовательских контейнеров\nc.DockerSpawner.network_name = \"jupyterhub-network\"              # Имя сети Docker\nc.DockerSpawner.notebook_dir = '/home/jovyan/work'               # Рабочая директория внутри контейнера\nc.DockerSpawner.volumes = { 'jupyterhub-data_{username}': '/home/jovyan/work' } # Монтирование томов для пользователей\nc.DockerSpawner.debug = True                                     # Включение режима отладки\nc.DockerSpawner.extra_create_kwargs = { 'runtime': 'nvidia' }    # Настройка NVIDIA runtime\nc.DockerSpawner.extra_host_config = {\n    'device_requests': [\n        {\n            'Driver': 'nvidia',\n            'Count': -1,\n            'Capabilities': [['gpu']],\n        }\n    ]\n}\nc.JupyterHub.cookie_secret_file = os.path.join(data_dir, 'jupyterhub_cookie_secret')\nc.JupyterHub.db_url = \"sqlite:////srv/jupyterhub/data/jupyterhub.sqlite\"\nc.JupyterHub.log_level = 'DEBUG'\nc.JupyterHub.metrics_enabled = True                              # Включение сбора метрик\nc.JupyterHub.metrics_port = 8000                                 # Порт для метрик\nc.JupyterHub.authenticate_prometheus = False                     # Отключаем аутентификацию Prometheus\n```\n\n---\n\n## Переменные окружения\n\nСоздаём файл `.env` с следующими переменными:\n\n```\nGITHUB_CLIENT_ID=your_github_client_id\nGITHUB_TOKEN=your_github_token\n```\n\nЗначения `your_github_client_id` и `your_github_token` нужно получить в GitHub.\n\n---\n\n## Запуск JupyterHub\n\n```\ndocker compose build\ndocker-compose up -d\n```\n\nТеперь у нас работает JupyterHub `http://skayfaks.keenetic.pro:35001/hub` с поддержкой GPU, аутентификацией через GitHub и динамическим созданием окружений для пользователей!\n\n"
    },
    {
      "name": "HuzaifaAnsari/Intelligent-Confluence-Knowledge-Assistant",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/70470083?s=40&v=4",
      "owner": "HuzaifaAnsari",
      "repo_name": "Intelligent-Confluence-Knowledge-Assistant",
      "description": "The Intelligent Confluence Knowledge Assistant is an advanced system designed to facilitate seamless querying of company documentation within Confluence. The assistant integrates a Retrieval-Augmented Generation (RAG) system, enabling users to retrieve and generate accurate responses from Confluence content using natural language queries.",
      "homepage": null,
      "language": "Python",
      "created_at": "2025-02-18T10:15:29Z",
      "updated_at": "2025-03-16T17:08:16Z",
      "topics": [],
      "readme": "# Intelligent Confluence Knowledge Assistant\n\nThe **Intelligent Confluence Knowledge Assistant** is an advanced system designed to facilitate seamless querying of company documentation within Confluence. The assistant integrates a **Retrieval-Augmented Generation (RAG)** system, enabling users to retrieve and generate accurate responses from Confluence content using natural language queries.\n\n## Features\n- **RAG-Based Querying:** Retrieve relevant information from Confluence using advanced retrieval techniques.\n- **Elasticsearch Integration:** Store and search indexed Confluence content efficiently.\n- **FastAPI Backend:** API-based architecture for seamless integration.\n- **Streamlit UI:** User-friendly interface for querying documents.\n- **Together.AI Support:** Leverage AI-powered LLMs for enhanced search performance.\n\n## Installation Guide\n\n### Clone the Repository\n```bash\ngit clone https://github.com/HuzaifaAnsari/Intelligent-Confluence-Knowledge-Assistant.git\ncd Intelligent-Confluence-Knowledge-Assistant\n```\n\n### Install Elasticsearch using Docker\n\n1. **Install Docker:** Follow the official Docker installation guide for your system: [Get Docker](https://docs.docker.com/get-started/get-docker/).\n\n   - If using Docker Desktop, allocate at least **4GB of memory** in `Settings > Resources`.\n\n2. **Create a Docker network for Elasticsearch:**\n```bash\ndocker network create elastic\n```\n\n3. **Pull the Elasticsearch Docker image:**\n```bash\ndocker pull docker.elastic.co/elasticsearch/elasticsearch:8.17.2\n```\n\n4. **Start an Elasticsearch container:**\n```bash\ndocker run --restart always --name es01 --net elastic -p 9200:9200 -it -m 6GB \\\n-e \"xpack.ml.use_auto_machine_memory_percent=true\" \\\ndocker.elastic.co/elasticsearch/elasticsearch:8.17.2\n```\n   - The command will print the **Elasticsearch user password** and an **enrollment token**.\n   - Copy the **username** and **password**, and paste them into the `.env` file.\n\n### Configure Environment Variables\nUpdate the `.env` file with the required credentials:\n- **Elasticsearch Credentials** (username & password from Elasticsearch setup)\n- **Confluence API Credentials**\n  - `CONFLUENCE_API_TOKEN`\n  - `CONFLUENCE_USERNAME`\n  - `CONFLUENCE_EMAIL`\n  - `CONFLUENCE_SPACE_KEY`\n- **Together.AI API Key** (Sign up at [Together.AI](https://www.together.ai/) and store the key in `.env`)\n\n### Install Dependencies\n```bash\npip install -r requirements.txt\n```\n\n### Store Embeddings into Elasticsearch\nRun the embedding script to index Confluence data into Elasticsearch.\n```bash\npython embedding.py\n```\n\n### Start the FastAPI Backend\n```bash\nuvicorn main:app --host 0.0.0.0 --port 80\n```\n\n### Start the Streamlit Application\n```bash\nstreamlit run Home.py\n```\n\n## Usage\n1. Access the **Streamlit UI** to enter your query and retrieve information from Confluence.\n2. The **FastAPI backend** processes the query using the RAG system and fetches the relevant information.\n3. Elasticsearch indexes and retrieves relevant content efficiently.\n\n\n## Contributions\nContributions are welcome! Feel free to submit a pull request or open an issue.\n\n## Contact\nFor questions or support, reach out to [Email](huzaifamuqeem@gmail.com) or [GitHub profile](https://github.com/HuzaifaAnsari).\n\n"
    },
    {
      "name": "PixelWelt/ThalamOS",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/33161332?s=40&v=4",
      "owner": "PixelWelt",
      "repo_name": "ThalamOS",
      "description": "Thalamos is a powerful Flask web application designed to enhance your storage management. It utilizes the WLED-API to light up the item you are looking for.",
      "homepage": "https://pixelwelt.github.io/ThalamOS/",
      "language": "Python",
      "created_at": "2025-01-04T17:37:18Z",
      "updated_at": "2025-04-16T15:18:27Z",
      "topics": [
        "cloud",
        "flask",
        "free-software",
        "mangement-system",
        "python",
        "self-hosted",
        "storage",
        "web",
        "wled"
      ],
      "readme": "![Issues](https://img.shields.io/github/issues/PixelWelt/ThalamOS)\n![Forks](https://img.shields.io/github/forks/PixelWelt/ThalamOS)\n![Stars](https://img.shields.io/github/stars/PixelWelt/ThalamOS)\n![Commits since latest release](https://img.shields.io/github/commits-since/PixelWelt/ThalamOS/latest)\n![Contributors](https://img.shields.io/github/contributors/PixelWelt/ThalamOS)\n![created at](https://img.shields.io/github/created-at/PixelWelt/ThalamOS)\n![last commit](https://img.shields.io/github/last-commit/PixelWelt/ThalamOS)\n![Banner](img/banner.png)\n# ThalamOS\nThalamos is a powerful Flask web application designed to enhance your storage management. It utilizes the WLED-API to light up the item you are looking for.\n## Etymology\n\nThe name **ThalamOS** is derived from the Latin word *thalamus*, which means \"inner chamber\" or \"storage room.\" This reflects the application's purpose of managing and organizing storage spaces efficiently.\n## Features\n\n- **LED Integration**: Controls an addressable LED strip via WLED to highlight the correct location of your items.\n- **Custom Properties**: Save any property with your stored items using the info field, allowing for infinite key-value pairs.\n- **Search Functionality**: Easily search for items and see their location light up on your storage shelf.\n- **Lightweight**: Built with SQLite and Flask, ensuring minimal resource usage and easy deployment.\n- **Easy Deployment**: Deploy effortlessly using a Docker container.\n\n## Documentation\nThe ThalamOS code documentation, generated with Doxygen, is available at: [https://pixelwelt.github.io/ThalamOS/](https://pixelwelt.github.io/ThalamOS/)\n\nCredits to [jothepro/doxygen-awesome-css](https://github.com/jothepro/doxygen-awesome-css) for providing the CSS files for the documentation.\n\n## Installation\n### Pull the latest build from Docker Hub (the easy way)\n\n1. Pull the latest image from Docker Hub:\n    ```bash\n    docker pull pixelwelt/thalamos:latest\n    ```\n2. setup your data directory and add  the `WLED_HOST` enviroment variable:\n    ```/data/.env\n    WLED_HOST=\"ip-adress\"\n    ```\n3. Setup your compose file\n    ```docker-compose\n    services:\n        thalamos:\n            container_name: ThalamOS\n            image: pixelwelt/thalamos:latest\n            restart: always\n            ports:\n            - \"8000:8000\"\n            volumes:\n            - ./data:/app/app/data\n    ```\n4. Start the service\n    ```bash\n    docker-compose up\n    ```\n### Build it yourself\n\n1. Clone the repository:\n    ```bash\n    git clone https://github.com/yourusername/thalamos.git\n    ```\n2. Navigate to the project directory:\n    ```bash\n    cd thalamos\n    ```\n3. Build the Docker image:\n    ```bash\n    docker build -t PixelWelt/thalamos .\n    ```\n4. Insert your WLED_HOST into the `.env.example` file and rename it to `.env`.\n5. Run the application using the Docker Compose file:\n    ```bash\n    docker-compose up\n    ```\n1. Clone the repository:\n    ```bash\n    git clone https://github.com/yourusername/thalamos.git\n    ```\n2. Navigate to the project directory:\n    ```bash\n    cd thalamos\n    ```\n3. Build the container:\n    ```bash\n    docker build -t PixelWelt/thalamos .\n    ```\n4. insert your WLED_HOST into the .env.example file and rename it to .env\n5. Run the application using the docker compose file:\n    ```bash\n    docker-compose up\n    ```\n\n## Usage\n\n1. Open your web browser and go to `http://localhost:8000`.\n2. Add your stored items along with their properties.\n3. Use the search functionality to find items and see their location light up on your storage shelf.\n\n## optional Modules\n### Weighfi - Where pounds meet packets\nWeigh-fi is a WiFi-enabled scale with API access, allowing users to easily add weight measurements when using ThalamOS. To enable it:\n\n1. Add the IP address of your Weigh-fi device to the `.env` file.\n2. Read more about how to build your own Weigh-fi device [here](https://github.com/PixelWelt/Weigh-fi).\n\n#### To-Dos\n- [x] Implement software\n- [ ] Create a 3D model\n- [ ] Create a working PCB\n\n### ThalamOS AI assistant\nWhy should I use AI in my storage management? - Because you can!\nThe ThalamOS AI assistant helps you manage your storage more efficiently. It uses Retrieval-Augmented Generation (RAG) to access your database in read-only mode, helping you find your items faster. It can also answer simple questions.\n\n**Note:** The AI assistant is still in development and may not work as expected. Deepseek Models are not yet supported.\n\nTo enable the ThalamOS AI assistant:\n1. Enable it in the `.env` file.\n2. Add the IP address of your Ollama Server.\n\nFramework: [Haystack](https://github.com/deepset-ai/haystack)\n\nThe specific pipeline steps are shown in the picture below:\n![pipeline](img/pipeline.png)\n![ai-demo](img/ai-demo.png)\n### To-Dos\n- [x] Implement basic ai usage\n- [ ] Implement Deepseek Models\n- [ ] Improve pipeline to prevent errors from happening\n- [ ] make the AI even smarter, by adding more context\n\n## Images\n![home page](img/home-page.png)\n![createItem.png](img/createItem.png)\n## Contributing\n\nContributions are welcome! Please fork the repository and submit a pull request.\n\n## License\n\nThis project is licensed under the MIT License.\n\n## Contact\n\nFor any questions or suggestions, please open an issue or contact the repository owner.\n"
    },
    {
      "name": "jazielloureiro/Coruja",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/69116273?s=40&v=4",
      "owner": "jazielloureiro",
      "repo_name": "Coruja",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2024-11-10T01:09:43Z",
      "updated_at": "2025-02-15T14:22:08Z",
      "topics": [],
      "readme": ""
    },
    {
      "name": "jbkoh/not-a-recruiter",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/1572627?s=40&v=4",
      "owner": "jbkoh",
      "repo_name": "not-a-recruiter",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2025-02-07T19:49:18Z",
      "updated_at": "2025-02-16T01:06:49Z",
      "topics": [],
      "readme": "Not A Recruiter\n===============\n\n\n# Installation\n```bash\npip install git+https://github.com/jbkoh/not-a-recruiter\n```\n\n# Usage\n\n1. Prepare `.env` based on `.sample.env` first.\n2. `not-a-recruiter screen-multiple --config config/sample_config.json # Update the config file for your JD`\n"
    },
    {
      "name": "deepset-ai/opea-haystack-demo",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/51827949?s=40&v=4",
      "owner": "deepset-ai",
      "repo_name": "opea-haystack-demo",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2025-02-05T11:57:54Z",
      "updated_at": "2025-03-20T17:43:23Z",
      "topics": [],
      "readme": "# Demopalooza: Practical RAG with OPEA and Haystack\n\n![image](https://github.com/user-attachments/assets/96b77b58-0c2e-49f1-9900-df4ec06152ae)\n\n\nThis [Streamlit](https://docs.streamlit.io/) app is set up for simple [Haystack](https://haystack.deepset.ai/) and [OPEA](https://opea.dev/) applications. The template is ready to perform **Retrieval Augmented Generation** with three different retrieval methods on example files.\n\nSee the ['How to use this template'](#how-to-use-this-template) instructions below to create a simple UI for your own Haystack search pipelines.\n\n## Installation and Running\nTo run the bare application:\n1. Install requirements: `pip install -r requirements.txt`\n2. Make sure that your OPEA microservices are running\n3. Run the streamlit app: `streamlit run app.py`\n\nThis will start the app on `localhost:8501`, where you will find a simple search bar. \n\n## How to use this template\n1. Create a new repository from this template or simply open it in a codespace to start playing around 💙\n2. Make sure your `requirements.txt` file includes the Haystack (`haystack-ai`) and Streamlit versions you would like to use.\n3. Change the code in `utils/haystack.py` if you would like a different pipeline. \n4. Create a `.env` file with all of your configuration settings.\n5. Make any UI edits if you'd like to.\n6. Run the app as show in [installation and running](#installation-and-running)\n\n### Repo structure\n- `./utils`: This is where we have 2 files: \n    - `haystack.py`: Here you will find some functions already set up for you to start creating your Haystack search pipeline. It includes 2 main functions called `start_haystack_pipeline()` which is what we use to create a pipeline and cache it, and `query()` which is the function called by `app.py` once a user query is received.\n    - `ui.py`: Use this file for any UI and initial value setups.\n- `app.py`: This is the main Streamlit application file that we will run. In its current state it has a sidebar, a simple search bar, a 'Run' button, and a response.\n- `./files`: You can use this folder to store files to be indexed.\n\n### What to edit?\nThere are default pipelines both in `start_document_store()` and `start_haystack_pipeline()`. Change the pipelines to use different document stores, embedding and generative models or update the pipelines as you need. Check out [📚 Useful Resources](#-useful-resources) section for details.\n\n### 📚 Useful Resources\n* [Get Started](https://haystack.deepset.ai/overview/quick-start)\n* [Docs](https://docs.haystack.deepset.ai/docs/intro)\n* [Tutorials](https://haystack.deepset.ai/tutorials)\n* [Integrations](https://haystack.deepset.ai/integrations)\n"
    },
    {
      "name": "snjax/savant",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/1750575?s=40&v=4",
      "owner": "snjax",
      "repo_name": "savant",
      "description": "Smart Contract auditing agent for Agentic Ethereum Hackathon",
      "homepage": null,
      "language": "Svelte",
      "created_at": "2025-01-29T13:23:32Z",
      "updated_at": "2025-02-12T11:33:44Z",
      "topics": [],
      "readme": "# savant 🤖\nSmart Contract auditing agent for Agentic Ethereum Hackathon\n\n> ⚠️ **Important Notice**: This repository contains limited version of the final product. We are currently withholding the full version due to its exceptional effectiveness at discovering critical vulnerabilities in smart contracts. The full version will be published after careful consideration of responsible disclosure practices.\n\nLive demo is available at [savant.chat](https://savant.chat/)\n\n## Bug finding example 🐛\n\n![Bug finding example](assets/2025-02-08_02-17.png)\n\n## System Architecture 🏗️\n\n![Savant AI Auditor Architecture](assets/diagram.svg)\n\n## How it works 🔍\n\nThe Savant AI auditor operates through a sophisticated multi-stage process to identify and validate potential vulnerabilities in smart contracts:\n\n1. **Input Processing** 📥\n   - Project Codebase: The system accepts Solidity smart contract code as input\n   - Project Documentation: Supporting documentation helps provide context for the analysis\n\n2. **Analysis Pipeline** ⚡\n   - **Hypothesis Generator** 🧪: The first AI agent (powered by GPT) analyzes the smart contract code to identify potential security vulnerabilities. It uses a carefully crafted prompt that focuses on critical security issues.\n\n   - **Hypothesis Critic** 🔍: A second AI agent acts as a critic, independently validating each potential vulnerability identified by the generator. This helps reduce false positives by providing an additional layer of verification.\n\n3. **Output** 📊\n   - **Vulnerability Report** 📝: The final output is a detailed security report that includes:\n     - Validated vulnerabilities (confirmed by both generator and critic) ✅\n     - Rejected findings (identified by generator but rejected by critic) ❌\n     - Severity assessments 🚨\n     - Code context for each issue 📄\n     - Detailed descriptions and explanations 📋\n\n## Features ⭐\n\n- Concurrent analysis of multiple security aspects 🔄\n- Two-stage validation process to minimize false positives ✔️\n- Detailed logging and audit trail 📝\n- Exponential backoff retry mechanism for API reliability 🔁\n- Structured output with clear differentiation between confirmed and rejected findings 📊\n\n## Usage 🛠️\n\n```bash\npython simple-agent.py <solidity_file>\n```\n\nThe tool will analyze the provided smart contract and generate a comprehensive security report highlighting any identified vulnerabilities.\n"
    },
    {
      "name": "JackLeeJM/rag-medication-ner",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/49935126?s=40&v=4",
      "owner": "JackLeeJM",
      "repo_name": "rag-medication-ner",
      "description": "An LLM-powered RAG application for medication Named Entity Recognition (NER)",
      "homepage": "",
      "language": "Python",
      "created_at": "2025-01-17T03:33:18Z",
      "updated_at": "2025-04-19T11:14:43Z",
      "topics": [
        "docker",
        "docker-compose",
        "healthcare",
        "hybrid-search",
        "llm",
        "ner",
        "ollama",
        "qdrant",
        "rag",
        "reranking"
      ],
      "readme": "# Advanced RAG for Medication NER\n\nThis project demonstrates a novel application of Retrieval-Augmented Generation (RAG) for Named Entity Recognition (NER) in healthcare. The solution combines advanced search techniques and large language models (LLMs) to accurately extract medication-related entities, including drug names, dosages, quantities, administration types, and brands, from unstructured text. By leveraging hybrid search and reranking, the system achieves state-of-the-art accuracy in processing complex medical datasets.\n\n> [!NOTE]\n> Check out the accompanying [article](https://jackleejm.com/rag-medication-ner/) for more in-depth details on how and why this project was developed.\n\n## Table of Content\n\n1. **[Features](#features)**\n2. **[Architecture Overview](#architecture-overview)**\n3. **[Tech Stack](#tech-stack)**\n4. **[Installation](#installation)**\n5. **[Usage](#usage)**\n6. **[Dataset](#dataset)**\n\n## Features\n\n- **Accurate Medication NER**: Extracts medication-related entities such as drug names, quantites, dosages, administration types, and brands from unstructured text.\n- **Hybrid Search and Reranking**: Combines dense and sparse retrieval methods to boost accuracy by fetching contextually similar few-shot examples.\n- **Small Language Model**: Utilizes small GenAI LLM such as Llama3.2 3B Parameters for efficient processing and low latency.\n- **Customizable Framework**: Supports customization to adapt to different use cases and requirements.\n\n## Architecture Overview\n\n![RAG Diagram](docs/assets/rag_diagram.png)\n\n- Two-Stage Retrieval\n  - Hybrid Search\n    - Dense (Semantic Search)\n    - Sparse (Keyword Search)\n  - Reranking Module\n    - Transformers-based\n- Local LLM (i.e. Llama 3.2 3B)\n- Indexing and Querying Haystack Pipelines\n- Custom Prompt Template\n\n## Tech Stack\n\n- Ollama\n- Qdrant\n- Haystack\n- Transformers\n- Python\n- FastAPI\n- Asyncio\n- Docker\n\n## Performance\n\n### Entity Extraction\n\nTested on an evaluation dataset consisting of 335 medication texts. \n\n- Accuracy: 92.2 %\n- Precision: 100 %\n- Recall: 92.2 %\n- F1-Score: 95.9 %\n\n### Latency\n- Average processing time per medication text: 1.33 seconds\n\nThe RAG application is more suited for batch processing tasks that runs in the background, not for real-time tasks as seen from the latency results.\n\n## Installation\n\nThere are 2 main approaches to setup this project for local development when customizing the framework to adapt to other use cases:\n- Rapid Prototyping\n- Standard Development\n\n**Rapid Prototyping** involves running only the Qdrant docker container (i.e. `docker-compose.local.yml`) and rely on locally installed Ollama for text-generation and inferences. One that uses Poetry virtual environment to run the RAG application. Reliant on the environment variables from `.env.local` as the Qdrant and Ollama HOST is different from the standard development process.\n\n**Standard Development** involves running the complete Docker setup (i.e. `docker-compose.yml`) that includes building the RAG app docker image, spinning up both Qdrant and Ollama docker containers. During development, the RAG app docker image will have to be built with every changes to the codebase, which may incur additional disk space to be used during the build process even when the image layers are being cached.\n\n**Quick Test** involves running the bare minimum Docker setup for testing purposes, pulling the docker image from Docker Hub, which is only reliant on Docker Compose and the ENV file to spin up the containers to access the API endpoints and test the application locally.\n\nFor better developer experience, it is recommended to go with **Rapid Prototyping** as it is less time-consuming and resource-intensive, though it does take a considerable effort for the initial setup.\n\n### System Setup\n\n1. Check if `en_US.UTF-8` is set as the default locale.\n2. Otherwise, set locale with `sudo locale-gen en_US.UTF-8` if you are running on Linux/WSL ecosystem.\n3. This is to circumvent the error of running the pre-commit hooks when committing code to GitHub.\n4. Clone the repository with `git clone https://github.com/JackLeeJM/rag-medication-ner.git`.\n\n### Rapid Prototyping Setup\n\n1. Install [Just](https://github.com/casey/just?tab=readme-ov-file#installation).\n2. Install [Poetry](https://python-poetry.org/docs/#installation).\n3. Install [Ollama](https://ollama.com/download).\n4. Install [Docker Desktop](https://www.docker.com/).\n5. Setup **Poetry** and install dependencies with `just install`.\n6. Create a local ENV file with `just env-local`.\n7. Spin up docker containers with `just up-local`.\n8. Activate the virtual environment with `poetry shell`.\n9. Start FastAPI server with `just run`.\n10. Go to `http://localhost:8000/docs` to access the API docs to test the endpoints.\n11. Use keyboard shortcut `Ctrl + C` to exit the FastAPI server.\n12. Any changes to the codebase will be reflected immediately, run `just run` again to boot up the FastAPI server again.\n\n### Standard Development Setup\n\n> [!NOTE]\n> EXTERNAL_PORT values for FastAPI, Qdrant, and Ollama are customizable in the `.env` file.\n\n1. Install [Just](https://github.com/casey/just?tab=readme-ov-file#installation).\n2. Install [Docker Desktop](https://www.docker.com/).\n3. Create a local ENV file with `just env`.\n4. Spin up docker containers with `just up`, or `just up-cpu` in favor of using CPU instead of GPU.\n5. Go to `http://localhost:8040/docs` to access the API docs and test the endpoints.\n6. Run `just down` to stop and remove all docker containers.\n7. Start customizing the prompt template and data to suit your specific use case.\n8. Once the custom changes are complete, version bump the app service's image (1.0.0 > 1.0.1) and run `just up` to update the docker image.\n\n### Quick Test\n\n1. Install [Docker Desktop](https://www.docker.com/).\n2. Create a new file `.env`, and copy content of `.env.sample` into it.\n3. Pull the docker image from Docker Hub with `docker pull jackleejm/rag-medication-ner:v1.0.0`.\n4. Spin up docker containers with `docker-compose up -d`.\n5. Go to `http://localhost:8040/docs` to access the API docs and test the endpoints.\n6. Run `docker-compose down` to stop and remove all docker containers.\n\n\n## Usage\n\n### API Endpoint for Entity Extraction\n\n- **Method**: POST\n- **Path**: `/extract`\n- **Description**: Extracts entities from a given list of unstructured text\n- **Request Body**:\n  ```json\n  {\n    \"text\": [\n        \"Acetaminophen 325 MG Oral Tablet\",\n        \"Ibuprofen 100 MG Oral Tablet\"\n    ]\n  }\n  ```\n- **Response Example**:\n  ```json\n  {\n    \"results\": [\n        {\n            \"original_text\": \"Acetaminophen 325 MG Oral Tablet\",\n            \"quantity\": [],\n            \"drug_name\": [\n                \"Acetaminophen\"\n            ],\n            \"dosage\": [\n                \"325 MG\"\n            ],\n            \"administration_type\": [\n                \"Oral Tablet\"\n            ],\n            \"brand\": []\n        },\n        {\n            \"original_text\": \"Ibuprofen 100 MG Oral Tablet\",\n            \"quantity\": [],\n            \"drug_name\": [\n                \"Ibuprofen\"\n            ],\n            \"dosage\": [\n                \"100 MG\"\n            ],\n            \"administration_type\": [\n                \"Oral Tablet\"\n            ],\n            \"brand\": []\n        }\n    ],\n    \"processing_time\": 1.5\n  }\n  ```\n\n### API Endpoint for Indexing Medications\n\n- **Method**: POST\n- **Path**: `/index`\n- **Description**: Index medications into vector database, to add more few-shot examples for better in-context learning performance.\n- **Request Body**:\n  ```json\n  {\n    \"medications\": [\n        {\n            \"original_text\": \"1 ML medroxyprogesterone acetate 150 MG/ML Injection\",\n            \"quantity\": [\n                \"1 ML\"\n            ],\n            \"drug_name\": [\n                \"medroxyprogesterone acetate\"\n            ],\n            \"dosage\": [\n                \"150 MG/ML\"\n            ],\n            \"administration_type\": [\n                \"Injection\"\n            ],\n            \"brand\": []\n        },\n        {\n            \"original_text\": \"budesonide 0.125 MG/ML Inhalation Suspension [Pulmicort]\",\n            \"quantity\": [],\n            \"drug_name\": [\n                \"budesonide\"\n            ],\n            \"dosage\": [\n                \"0.125 MG/ML\"\n            ],\n            \"administration_type\": [\n                \"Inhalation Suspension\"\n            ],\n            \"brand\": [\n                \"Pulmicort\"\n            ]\n        }\n    ]\n  }\n  ```\n- **Response Example**:\n  ```json\n  {\n    \"message\": \"Successfully indexed 2 entities\",\n    \"processing_time\": 0.05,\n  }\n  ```\n\n## Dataset\n\nThe training dataset is generated using an open-source project called [Healthcare Data Generator](https://github.com/JackLeeJM/healthcare-data-generator) that is based on [Synthea](https://github.com/synthetichealth/synthea), which is a synthetic healthcare data generator that creates realistic patient records. The raw dataset is then manually annotated and validated for accuracy and completeness by checking the original_text to the extracted entities.\n"
    },
    {
      "name": "plog/learn_llm_slowly",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/26974?s=40&v=4",
      "owner": "plog",
      "repo_name": "learn_llm_slowly",
      "description": "The correct way to learn LLM and LangChain",
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2025-01-21T07:41:13Z",
      "updated_at": "2025-01-22T14:37:54Z",
      "topics": [],
      "readme": "# LangChain LLM Tutorial bit by bit...\n\nWelcome to the LangChain LLM tutorial! This guide is designed to help you learn LangChain step by step, at a slow and deliberate pace. Rushing often leads to mistakes, and we want to ensure you have a solid understanding at every stage.\n\n# About This Tutorial\nLangChain is a powerful framework for building applications using large language models (LLMs). \nHowever, due to its complexity, it’s easy to become overwhelmed if you try to move too quickly. \nThis tutorial is intentionally structured to:\n\n- Emphasize gradual learning.\n- Break down complex topics into manageable steps.\n- Provide clear examples and exercises.\n\n**We will use LangChain only, no LangGraph nor LangSmith** \n\n*(...at the time of writing this I don't even know what they do 😂)*\n\n---\n\n# Structure of the Tutorial\n\n1. Talk to the LLM<br>\n[01.llm_template.ipynb](01.llm_template.ipynb): Ask simple questions to a LLM\n\n2. Vectors and embedding<br>\n[02.embeddings_vectors.ipynb](02.embeddings_vectors.ipynb): How to use Embedding we can use for RAG's\n\n3. Classification<br>\n[03.classify.ipynb](03.classify.ipynb): Classify a message using pydantic object and ChatPromptTemplate\n\n4. Extraction<br>\n[04.extraction.ipynb](04.extraction.ipynb): Same but for structured data extraction\n\n5. Tools calling<br>\n[05.tool_calling.ipynb](05.tool_calling.ipynb): Tool calling, also known as function calling, enables AI models to interact with systems like APIs or databases by responding in a schema-specific format\n\n6. Short-Term Memory<br>\n[06.short_memory.ipynb](06.short_memory.ipynb): The code creates a chatbot with short-term memory using ChatOllam.\n\n7. Long-Term Memory<br>\n[07.long_memory.ipynb](07.long_memory.ipynb): This code builds a chatbot that remembers and retrieves past conversations using a database, allowing it to provide personalized and context-aware responses.\n\n8. Runnable<br>\n[08.runnable.ipynb](08.runnable.ipynb): This code demonstrates the Runnable concept by building a pipeline of chained transformations—adding context, generating a response with a language model, and post-processing the output—showcasing how LangChain organizes and executes workflows efficiently.\n\n9. Agent<br>\n[09.agent.ipynb](09.agent.ipynb): An agent dynamically selects tools and actions based on input, offering flexibility, efficiency, and scalability, while a pipeline processes inputs linearly through predefined steps without decision-making.\n---\n\n# How to Install and run the Notebooks on your Docker\n\n## Docker\n\nTo be fully independant and be able to practice anywhere, I'm using two small Docker containers:\n1) One for [**Ollama**](https://ollama.com/search) with a couple of models (llama,)\n2) One as my **development container** with Python 3.12 and the LangChain libraries I need\n\n## Ollama\nThe models I use:\n```\nNAME                        ID              SIZE      MODIFIED    \nphi3:14b                    cf611a26b048    7.9 GB    2 days ago     \nmxbai-embed-large:latest    468836162de7    669 MB    5 days ago     \nmistral:latest              f974a74358d6    4.1 GB    9 days ago     \nllama3.2:latest             a80c4f17acd5    2.0 GB    11 days ago  \n```\n\n## VSCode\n\nThen, using VSCode, you can [**\"Attach Visual Studio Code\"**](https://code.visualstudio.com/docs/devcontainers/containers) and run all examples with Jupyter plugins \n\n<p align=\"center\"><img src=\"docs/vscode.gif\" width=\"500\"></p>\n\n... and you're the king of the world\n\n## Notes\n- We'll use Ollama to avoid expenses associated with LLM like GPT. No need API keys nor Credit Card. \n- Results may be a bit less accurate than GPT but we wanna learn first.\n- The version of LangChain installed should be 0.3+ (important)\n- Ollama [is easy to install using Docker](https://github.com/ollama/ollama/blob/main/docs/docker.md). \n(Better to have a GPU, even a small one)\n- I give an example of Dockerfile and docker-compose.yaml using Python 3.12 (I have to test this...)\n- You can then use an evironment variable OLLAMA_SERVER (ex. OLLAMA_SERVER='http://ollama:11434') from this container [running Jupyter](https://jupyterlab.readthedocs.io/en/4.1.x/getting_started/installation.html)\n- Required Python packages from `requirements.txt` file should be build with the Dockerfile.\n\nA simple `docker-compose up -d` should do the trick\n\n---\n\n## Tips for Success\n- **Take Your Time**: Resist the urge to skip steps or rush through sections.\n- **Experiment**: Modify the examples and test your ideas.\n- **Ask Questions**: Engage with the LangChain community if you encounter issues.\n\n---\n\n## Contribution\nThis is a community-driven effort, and your feedback is valuable. If you find errors or have suggestions for improvement, please submit an issue or pull request.\n\nHappy Learning!"
    },
    {
      "name": "mr-rakesh-ranjan/PDF_RAG_Haystack",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/90750228?s=40&v=4",
      "owner": "mr-rakesh-ranjan",
      "repo_name": "PDF_RAG_Haystack",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2025-01-21T04:07:52Z",
      "updated_at": "2025-02-22T07:26:46Z",
      "topics": [],
      "readme": "# Haystack POC\n\nThis repository contains a proof of concept (POC) for using the Haystack framework to build and evaluate a Retrieval-Augmented Generation (RAG) pipeline. The project includes components for reading PDF content, storing documents, retrieving relevant documents, generating answers using an LLM, and evaluating the generated answers.\n\n## Project Structure\n\n```\n.\n├── .env\n├── .gitignore\n├── app.py\n├── evaluating_model.py\n├── main.py\n├── questions_test.py\n├── questions.py\n├── replicate_POC.py\n└── requirements.txt\n```\n\n## Files\n\n- **app.py**: Contains a simple pipeline for generating answers using OpenAI's GPT model.\n- **evaluating_model.py**: Builds and evaluates a RAG pipeline using the Haystack framework.\n- **main.py**: Main script for building and running the RAG pipeline.\n- **questions_test.py**: Contains a list of questions and their corresponding ground truth answers for evaluation.\n- **questions.py**: Contains a list of questions and their corresponding ground truth answers and predicted answers.\n- **replicate_POC.py**: Example script for generating images using the Replicate API.\n- **requirements.txt**: Lists the dependencies required for the project.\n\n## Setup\n\n1. **Clone the repository**:\n    ```sh\n    git clone <https://github.com/mr-rakesh-ranjan/PDF_RAG_Haystack>\n    cd <PDF_RAG_Haystack>\n    ```\n\n2. **Create a virtual environment**:\n    ```sh\n    python -m venv venv\n    source venv/bin/activate  # On Windows, use `venv\\Scripts\\activate`\n    ```\n\n3. **Install the dependencies**:\n    ```sh\n    pip install -r requirements.txt\n    ```\n\n4. **Set up environment variables**:\n    - Create a [`.env`](.env ) file in the root directory and add your API keys:\n      ```\n      OPENAI_API_KEY=\"your_openai_api_key\"\n      REPLICATE_API_TOKEN=\"your_replicate_api_token\"\n      ```\n\n## Usage\n\n### Running the RAG Pipeline\n\n1. **Run the main script**:\n    ```sh\n    python main.py\n    ```\n\n2. **Run the evaluation script**:\n    ```sh\n    python evaluating_model.py\n    ```\n\n### Generating Images with Replicate API\n\n1. **Run the replicate POC script**:\n    ```sh\n    python replicate_POC.py\n    ```\n\n## License\n\nThis project is licensed under the MIT License. See the LICENSE file for details.\n\n## Acknowledgements\n\n- [Haystack](https://github.com/deepset-ai/haystack) - An open-source framework for building search systems.\n- [OpenAI](https://openai.com/) - For providing the GPT model used in this project.\n- [Replicate](https://replicate.com/) - For providing the API used to generate images.\n\n## Contact\n\nFor any questions or inquiries, please contact [Rakesh Ranjan Kumar](mailto:rakeshranjan.java1.8@gmail.com).\n"
    },
    {
      "name": "Shrijeeth/QuickRAG",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/58306412?s=40&v=4",
      "owner": "Shrijeeth",
      "repo_name": "QuickRAG",
      "description": "Sample Production Ready Agentic RAG application",
      "homepage": null,
      "language": "Python",
      "created_at": "2025-01-18T05:38:30Z",
      "updated_at": "2025-01-19T18:45:27Z",
      "topics": [],
      "readme": "# QuickRAG\nSample Production Ready Agentic RAG application\n"
    },
    {
      "name": "jd-coderepos/chat-QA",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/15185300?s=40&v=4",
      "owner": "jd-coderepos",
      "repo_name": "chat-QA",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2025-01-15T16:06:05Z",
      "updated_at": "2025-04-11T22:51:16Z",
      "topics": [],
      "readme": "# Local Llama\n\nThis project enables you to chat with your PDFs, TXT files, or Docx files entirely offline, free from OpenAI dependencies. \n\n## Demo\n\n[![Watch the Video](https://img.youtube.com/vi/nHJqqcTSMbY/0.jpg)](https://youtu.be/nHJqqcTSMbY)\n\n## Requirements\n\n- Python >= 3.10\n- [Ollama](https://ollama.ai/download)\n\n## Features\n\n- Local LLM integration: Uses Ollama for improved performance\n- Multiple file format support: PDF, TXT, DOCX, MD\n- Persistent vector database: Reusable indexed documents\n- Streamlit-based user interface\n\n## New Updates\n\n- Ollama integration for significant performance improvements\n- Uses nomic-embed-text and llama3.2:3b models (can be changed to your liking)\n- Upgraded to Haystack 2.0\n- Persistent Chroma vector database to enable re-use of previously updloaded docs\n\n## Installation\n\n1. Install Ollama from https://ollama.ai/download\n2. Clone this repository\n3. Install dependencies:\n   ```\n   pip install -r requirements.txt\n   ```\n4. Pull required Ollama models:\n   ```\n   ollama pull nomic-embed-text\n   ollama pull llama3.2:3b\n   ```\n5. After cloning the repository, create two folders named \"uploads\" and \"vec-index\" in the code directory.\n\n## Usage\n\n1. Run the Streamlit app:\n\n   ```sh\n   ollama run llama3.2\n   ```\n\n2. Run the Streamlit app:\n   ```\n   python -m streamlit run local_llama_v3.py\n   ```\n2. Upload your documents and start chatting!\n\n## How It Works\n\n1. Document Indexing: Uploaded files are processed, split, and embedded using Ollama.\n2. Vector Storage: Embeddings are stored in a local Chroma vector database.\n3. Query Processing: User queries are embedded and relevant document chunks are retrieved.\n4. Response Generation: Ollama generates responses based on the retrieved context and chat history.\n\n## License\n\nThis project is licensed under the [MIT License](LICENSE).\n\nYou are free to use, modify, and distribute this software in accordance with the terms of the license.\n"
    },
    {
      "name": "maxonary/Spotter",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/62939182?s=40&v=4",
      "owner": "maxonary",
      "repo_name": "Spotter",
      "description": "Share your location. Get real-time notifications of your favourite meme spots. AI analyses social media videos with reverse image search, puts them onto a map and enables users to get push-notified once they walk by a categorised spot.",
      "homepage": "https://meme-map.streamlit.app",
      "language": "Swift",
      "created_at": "2025-01-18T10:47:26Z",
      "updated_at": "2025-02-10T14:13:39Z",
      "topics": [
        "fastapi",
        "ios-swift",
        "streamlit"
      ],
      "readme": "# Spotter - Redefine Sightseeing\n<img width=\"531\" alt=\"Spotter Logo\" src=\"https://github.com/user-attachments/assets/56000cbd-a825-4552-bdc5-460cefae324f\" />\n\nTurn your digital memories into real-world moments. Share your location, and let Spotter do the rest. Our AI analyzes posts, videos, and images, mapping your digital memories to the places they were captured. As you move through the world, Spotter notifies you when you pass a meaningful spot, letting you relive, reconnect, and rediscover.\nSpotter takes your digital life beyond the screen.\n\nLet’s bring your memories to life.\n\n--- \n\n## iOS App Overview\nSpotter is a location-based app that allows users to discover and interact with links (e.g., memes, resources) on a map. The app integrates a FastAPI backend and a Streamlit frontend for a seamless user experience.\n\n---\n\n## Features\n\n1. **Map View**: Display links on a map with markers, allowing users to explore links based on location.\n2. **Link Management**: Add, update, and delete links with geolocation data and optional descriptions.\n3. **Location-Based Notifications**: Receive notifications when nearby links are detected.\n4. **Tabbed Interface**: Switch between different views, including a list of collected spots, a discover view, a friends view, and a me view.\n\n--- \n\n## API and Map Overview\nThe Meme Map application is an interactive platform for geolocating and displaying links (e.g., memes, resources) on a map. It integrates a FastAPI backend and a Streamlit frontend for a seamless user experience. The application also supports MongoDB for database storage.\n\n---\n\n## Features\n1. **FastAPI Backend**:\n   - Provides RESTful API endpoints for managing and querying geolocated links.\n   - Auto-generated Swagger documentation available at `/docs`.\n\n2. **Streamlit Frontend**:\n   - Interactive web interface for adding, updating, and visualizing links on a map.\n   - Communicates with the FastAPI backend.\n\n3. **MongoDB Integration**:\n   - Stores link information, geolocation data, and optional descriptions.\n   - Allows querying and filtering of stored links.\n\n4. **Geocoding Support**:\n   - Converts street addresses to latitude and longitude coordinates using OpenStreetMap's Nominatim API.\n\n5. **Interactive Map**:\n   - Displays stored links with markers.\n   - Allows users to explore links based on location.\n\n---\n\n## Installation\n\n### Prerequisites\n- Python 3.10 or higher.\n- MongoDB instance (local or cloud-based).\n- Node.js and npm (optional, for advanced development).\n\n### Setup\n1. **Clone the Repository:**\n   ```bash\n   git clone https://github.com/yourusername/meme-map.git\n   cd meme-map\n   ```\n\n2. **Set Up the Environment:**\n   - Create a virtual environment:\n     ```bash\n     python -m venv venv\n     source venv/bin/activate  # On Windows: venv\\Scripts\\activate\n     ```\n   - Install dependencies:\n     ```bash\n     pip install -r requirements.txt\n     ```\n\n3. **Configure Secrets:**\n   - Create a `.env` file in the root directory with the following content:\n     ```env\n     MONGO_URI=<your_mongo_connection_string>\n     ```\n   - For Streamlit, configure `secrets.toml` in the `.streamlit` folder:\n     ```toml\n     [default]\n     MONGO_URI = \"<your_mongo_connection_string>\"\n     ```\n\n4. **Run the Application:**\n   - Start the FastAPI server:\n     ```bash\n     uvicorn api_meme_map:app --host 0.0.0.0 --port 8000\n     ```\n   - Start the Streamlit frontend:\n     ```bash\n     streamlit run meme_map.py\n     ```\n\n---\n\n## Deployment\n\n### Hosting FastAPI\n- Use platforms like **Render**, **Railway**, or **Heroku** for hosting the FastAPI backend.\n- Example `Procfile` for Heroku:\n  ```\n  web: uvicorn api_meme_map:app --host 0.0.0.0 --port $PORT\n  ```\n\n### Hosting Streamlit\n- Use **Streamlit Cloud** for deploying the frontend.\n- Push your code to GitHub and link the repository to Streamlit Cloud.\n\n---\n\n## Usage\n\n### FastAPI Endpoints\n- **Swagger Documentation**: Navigate to `/docs` for auto-generated API documentation.\n- **Key Endpoints**:\n  - `POST /add-or-update-link`: Add or update a link.\n  - `GET /all-links`: Fetch all stored links.\n  - `GET /nearby-links`: Fetch links within a specific radius.\n  - `DELETE /delete-link`: Delete a link by URL.\n\n### Streamlit Features\n1. Add a link with geolocation:\n   - Enter a valid URL, address, or coordinates.\n   - Provide an optional description.\n2. View and interact with links on the map:\n   - Click markers to view link details.\n   - Filter links by location and distance.\n\n---\n\n## Contributing\n1. Fork the repository.\n2. Create a feature branch:\n   ```bash\n   git checkout -b feature-name\n   ```\n3. Commit your changes:\n   ```bash\n   git commit -m \"Add feature description\"\n   ```\n4. Push to the branch:\n   ```bash\n   git push origin feature-name\n   ```\n5. Open a pull request.\n\n---\n\n## License\nThis project is licensed under the [Apache 2.0 License](LICENSE).\n\n---\n\n## Acknowledgments\n- [Streamlit](https://streamlit.io/)\n- [FastAPI](https://fastapi.tiangolo.com/)\n- [MongoDB](https://www.mongodb.com/)\n- [Nominatim API](https://nominatim.openstreetmap.org/)\n  \n"
    },
    {
      "name": "Teaching-and-Learning-in-Computing/dRAG",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/166765208?s=40&v=4",
      "owner": "Teaching-and-Learning-in-Computing",
      "repo_name": "dRAG",
      "description": "dRAG is a local, privacy-focused tool designed to help educators answer discussion questions efficiently using their course materials, such as syllabi and lecture notes.",
      "homepage": "https://tlc.ics.uci.edu/",
      "language": "Python",
      "created_at": "2024-02-09T23:59:28Z",
      "updated_at": "2025-01-17T00:37:19Z",
      "topics": [],
      "readme": "# Discussion Retrieval Augmented Generation (dRAG)\n\n**dRAG** is a local, privacy-focused tool designed to help educators answer discussion questions efficiently using their course materials, such as syllabi and lecture notes. By leveraging Retrieval-Augmented Generation (RAG) techniques, dRAG ensures accurate and context-aware responses, all while keeping sensitive data secure by running offline. The project uses UV for dependency management and Ollama for both LLM and embedding models.\n\n## Features\n\n- **PDF Indexing**: Converts and indexes course materials for efficient retrieval.\n- **Customizable Prompt**: Allows users to specify LLM prompt for tailored results.\n- **Ollama Integration**: Uses `qwen2.5:latest` for generation and `snowflake-arctic-embed2` for embeddings.\n- **Offline Privacy**: Ensures data security by running entirely offline.\n\n## Requirements\n\n- Python 3.12 or higher\n- [UV](https://astral.sh/uv)\n- [Ollama](https://ollama.com/download) installed and running\n\n## Installation\n\n### Install UV\n\nOn macOS and Linux:\n```bash\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n```\n\nOn Windows:\n```powershell\npowershell -ExecutionPolicy ByPass -c \"irm https://astral.sh/uv/install.ps1 | iex\"\n```\n\n### Clone the Repository\n\n```bash\ngit clone https://github.com/Teaching-and-Learning-in-Computing/dRAG\ncd dRAG\n```\n\n### Set Up the Environment\n\nUV will handle dependencies defined in the `pyproject.toml`:\n```bash\nuv sync\n```\n\n## Usage\n\n### Step 1: Place the PDF File to Index\n\nPlace the PDF file you want to index in the `/documents/source_documents/` folder. For example:\n\n```\ndocuments/source_documents/your_file_name.pdf\n```\n\n### Step 2: Download and Start Ollama Models\n\nEnsure Ollama is installed and running. Pull the required models by running the following commands:\n\n```bash\nollama pull qwen2.5:latest\nollama pull snowflake-arctic-embed2\n```\n\nStart the Ollama server to enable model usage.\n\n### Step 3: Prepare Input Questions\n\nUpdate the `documents/input/input.json` file with the questions and answers you want to use. The JSON file should follow this structure:\n\n```json\n[\n    {\n        \"questions\": \"What are the course prerequisites?\",\n        \"answer\": \"Prerequisites include introductory programming and basic statistics.\"\n    }\n]\n```\n\nThe `answer` field should contain a \"ground truth\" response provided by a professor or TA for evaluation purposes.\n\n### Step 4: Run the Project\n\nRun the `main.py` script to process documents, generate answers, and evaluate results:\n\n```bash\npython main.py\n```\n\nThe script executes the following steps in order:\n1. Indexes the specified PDF files using `index.py`.\n2. Processes input questions and generates answers using `generate.py`.\n3. Evaluates generated answers against the provided ground truth using `evaluate.py`.\n\n## Notes for Subsequent Runs\n\n- **Adding Documents**: If you add new documents, place them in the `/documents/source_documents/` folder and rerun `main.py`.\n- **Modifying Inputs**: To change questions or prompts, update the `input.json` file and rerun `main.py`.\n- **Model Updates**: If you want to switch models, ensure they are pulled into Ollama and update each file.\n\n## Example Workflow\n\n1. Place a syllabus PDF in `documents/source_documents/`.\n2. Add your questions and ground truth answers to `input.json`.\n3. Run `main.py` to index, generate, and evaluate results.\n\n"
    },
    {
      "name": "Vivek-Lahole/custom-model-runpod",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/82323706?s=40&v=4",
      "owner": "Vivek-Lahole",
      "repo_name": "custom-model-runpod",
      "description": "Used cognitivecomputations/dolphin-2.9-llama3-8b Open source model deployed on Serverless GPUs using Runpod ",
      "homepage": "",
      "language": "TypeScript",
      "created_at": "2025-01-09T19:21:03Z",
      "updated_at": "2025-02-07T14:51:24Z",
      "topics": [
        "fastapi",
        "haystack-ai",
        "llama3",
        "llms",
        "nextjs"
      ],
      "readme": "# custom-model-runpod\n\nRun `docker compose up` to start the application.\n"
    },
    {
      "name": "Asugawara/WhatTheFuck",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/47840708?s=40&v=4",
      "owner": "Asugawara",
      "repo_name": "WhatTheFuck",
      "description": "WhatTheFuck is a command-line tool that leverages LLMs to correct and enhance previously executed terminal commands. ",
      "homepage": "https://github.com/Asugawara/WhatTheFuck",
      "language": "Python",
      "created_at": "2025-01-03T10:54:33Z",
      "updated_at": "2025-02-06T13:53:01Z",
      "topics": [],
      "readme": "[![Test wtf](https://github.com/Asugawara/WhatTheFuck/actions/workflows/run_test.yml/badge.svg)](https://github.com/Asugawara/WhatTheFuck/actions/workflows/run_test.yml)\n![PyPI](https://img.shields.io/pypi/v/wtf-cli?color=green)\n![PyPI - Python Version](https://img.shields.io/pypi/pyversions/wtf-cli)\n![GitHub](https://img.shields.io/github/license/Asugawara/WhatTheFuck)\n\n# WhatTheFuck\n\n**WhatTheFuck** is a command-line tool that leverages LLMs to correct and enhance previously executed terminal commands. Inspired by [`wut`](https://github.com/shobrook/wut) and [`The Fuck`](https://github.com/nvbn/thefuck), it’s designed to save your time and sanity by fixing errors, summarizing logs, and more.\n\n# Example\n```bash\n$ (wtf): git brnch\ngit: 'brnch' is not a git command. See 'git --help'.\n\nThe most similar command is\n\tbranch\n$ (wtf): wtf\nThe error message indicates that 'brnch' is not recognized as a valid Git command. The correct command is 'branch', which is likely what you intended to use to list, create,\nor delete branches in your repository. The suggestion for 'branch' highlights this common typo. To avoid such errors, ensure command spelling is accurate.\nRun `git branch`? [y/N]: y\n* main\n```\n※LLM's streamed outputs are omitted for simplicity\n\n# Features\n\n- **LLM Switching**: Easily switch between different llms.(OpanAI, Anthropic, Vertex)\n- **Instant Command Fixes**: Detect and fix incorrect terminal commands effortlessly.\n- **One-Click Execution**: Run corrected commands seamlessly.\n- **Log Summarization**: Summarize logs from previous terminal commands for quick insights.\n- **Web Search Integration**: Perform real-time web searches to gather relevant information or troubleshoot issues directly from the tool.\n\n> [!NOTE]\n> WTF utilizes [`script(Unix)`](https://en.wikipedia.org/wiki/Script_(Unix)) to log and analyze the previously executed command.\n\n# Installation\n```bash\n$ pip install wtf-cli\n# or\n$ uv add wtf-cli\n# or\n$ poetry add wtf-cli\n```\n\n\n\n# Versioning\nThis repo uses [Semantic Versioning](https://semver.org/).\n\n# License\n**WhatTheFuck** is released under the MIT License. See [LICENSE](/LICENSE) for additional details.\n"
    },
    {
      "name": "cuongpiger/chatgpt-like-clone",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/40521173?s=40&v=4",
      "owner": "cuongpiger",
      "repo_name": "chatgpt-like-clone",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2023-11-24T08:39:30Z",
      "updated_at": "2025-01-15T08:43:10Z",
      "topics": [],
      "readme": "# ChatGPT-like clone\n\n_This is just a small project for lab purposes. NOT for production._\n<hr>\n\n###### 🌈 Table of Contents\n\n- [Environment](#environment)\n- [Next plans](#next-plans)\n- [Overview](#overview)\n- [Installation](#installation)\n  - [Local](#local)\n  - [Kubernetes](#kubernetes)\n\n<hr>\n\n# Environment\n- Ubuntu 22.04 LTS (Desktop/Cloud Server)\n- GPU NVIDIA RTX 3070Ti or RTX 4090Ti _(tested both)_\n- Your server has been installed `nvidia-smi`, using `sudo ubuntu-drivers autoinstall` to install the driver. I am using the latest driver version:\n  ```bash\n  stackops@cuongdm3-voldemort:~$ nvidia-smi --version\n  NVIDIA-SMI version  : 550.120\n  NVML version        : 550.120\n  DRIVER version      : 550.120\n  CUDA Version        : 12.4\n  ```\n\n# Next plans\n\n- [ ] Integrated with Triton Inference Server.\n- [x] Deploy it on Kubernetes.\n- [ ] Streaming messages.\n- [x] Multi-session support.\n\n# Overview\n\n- The docker image located at `vcr.vngcloud.vn/60108-cuongdm3/chatgpt-like-clone:base`.\n- **ChatGPT-like clone** is a web-chat application using Gradio and Haystack.\n- Currently, I am using `llama3.1:8b` model as the default model.\n- The model is running in a **Ollama** Docker container with **GPU support**.\n  ![](./assets/01.png)\n\n# Installation\n- Until this step, this application support running on **Local** and **Kubernetes**. Depending on your environment, you can choose the suitable way to deploy it.\n## Local\n- To run it:\n    - Enable GPU in\n      Docker [https://github.com/ollama/ollama/blob/main/docs/docker.md](https://github.com/ollama/ollama/blob/main/docs/docker.md):\n      ```bash\n      curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey \\\n          | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg\n      curl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list \\\n          | sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' \\\n          | sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list\n      sudo apt-get update\n  \n      sudo apt-get install -y nvidia-container-toolkit\n  \n      sudo nvidia-ctk runtime configure --runtime=docker\n  \n      sudo systemctl restart docker\n      ```\n\n    - Run Ollama in Docker integrated with GPU:\n      ```bash\n      docker run -d --gpus=all -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama\n      docker exec ollama ollama pull llama3.1:8b\n      ```\n    - Run the Gradio ChatApp:\n      ```bash\n      python3 main.py\n      ```\n    - Visit [http://localhost:7860](http://localhost:7860) to chat with the model.\n    \n\n## Kubernetes\n- If you tend to deploy this application on Kubernetes, apply the manifest [app.yaml](./app.yaml), then access the application by the public IP of the worker nodes on port `30007`.\n  ```yaml\n  # File app.yaml\n  apiVersion: apps/v1\n  kind: Deployment\n  metadata:\n    name: chatgpt-clone-deployment\n  spec:\n    replicas: 1\n    selector:\n      matchLabels:\n        app: chatgpt-clone\n    template:\n      metadata:\n        labels:\n          app: chatgpt-clone\n      spec:\n        containers:\n        - name: chatgpt-clone\n          image: vcr.vngcloud.vn/60108-cuongdm3/chatgpt-like-clone:base\n          command: [\"python\"]\n          args: [\"main.py\", \"http://ollama33.ollama\"]  # please chaage the ollama server address\n          ports:\n          - containerPort: 7860\n        nodeSelector:\n          vks.vngcloud.vn/nodegroup: nodegroup-27051 \n  ---\n  apiVersion: v1\n  kind: Service\n  metadata:\n    name: chatgpt-clone-service\n  spec:\n    selector:\n      app: chatgpt-clone\n    type: NodePort\n    ports:\n    - protocol: TCP\n      port: 80\n      targetPort: 7860\n      nodePort: 30007 # Specify a fixed NodePort (optional) or let Kubernetes choose\n  ```\n\n  ![](./assets/02.png)"
    },
    {
      "name": "Cosmian/cosmian-ai-runner",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/37953047?s=40&v=4",
      "owner": "Cosmian",
      "repo_name": "cosmian-ai-runner",
      "description": "Confidential computing demo on how to use LLMs on GWS encrypted docs",
      "homepage": null,
      "language": "Python",
      "created_at": "2023-11-03T09:09:54Z",
      "updated_at": "2025-04-16T09:48:29Z",
      "topics": [],
      "readme": "# Cosmian AI runner\n\nConfidential computing backend to run AI models\n\n## Structure\n\nThe AI Runner is a Flask-based application that provides endpoints for performing inference across various AI tasks and pipelines, such as text summarization, translation, text querying, and retrieval-augmented generation (RAG) over document databases.\n\nThese pipelines are constructed using the Haystack library (https://haystack.deepset.ai/) as a foundation. Users have the flexibility to customize the pipelines by modifying the loaded models, selecting a preferred vector database, and tailoring the setup to meet specific requirements.\n\n## Usage\n\n- Build and install the [app](./app/README.md)\n\n- Edit the config file ([more info](./app/README.md#config-file))\n\n- Run the app\n\n```bash\nCONFIG_PATH=\"./run/config.json\" cosmian-ai-runner --port 5001\nUsing current model, you need to add your HuggingFace token from your config file.\n```\n\nDetails of the API Endpoints are explained in the `app/` folder of the repository.\n"
    },
    {
      "name": "d-kleine/Advent_of_HayStack",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/53251018?s=40&v=4",
      "owner": "d-kleine",
      "repo_name": "Advent_of_HayStack",
      "description": "Solutions from the Advent of Haystack 2024 event, exploring Haystack framework fundamentals through advanced RAG pipelines and intelligent agents",
      "homepage": "https://haystack.deepset.ai/advent-of-haystack",
      "language": "Jupyter Notebook",
      "created_at": "2024-12-06T12:20:35Z",
      "updated_at": "2025-01-30T02:35:34Z",
      "topics": [
        "agentic",
        "haystack",
        "information-retrieval-system",
        "retrieval-augmented-generation",
        "vector-database"
      ],
      "readme": "# **Advent of Haystack**\n\nThis project highlights the capabilities of [Haystack](https://github.com/deepset-ai/haystack) in solving real-world problems through a set of ten unique challenges. Developed by [*deepset*](https://www.deepset.ai/), Haystack is a robust framework for building pipelines that integrate retrieval-augmented generation (RAG), metadata processing, and other advanced NLP techniques. The event takes place during December 2024, blending technical problem-solving with the festive theme of Christmas.\n\nhttps://haystack.deepset.ai\n\n## **Project Description**\n\nThis project is a series of ten challenges designed to explore and enhance skills in using the Haystack framework for building advanced pipelines. Each challenge presents a scenario requiring the design, implementation, and optimization of pipelines using Haystack's tools and capabilities. The tasks focus on leveraging components from HayStack to solve practical problems.\n\nIn addition to showcasing Haystack's features, the challenges explore its application in realistic environments, working with technologies such as [Weaviate](https://weaviate.io/), [AssemblyAI](https://www.assemblyai.com/), [NVIDIA](https://www.nvidia.com/en-us/ai/), [Arize](https://arize.com/), and [MongoDB](https://www.mongodb.com/), demonstrating how Haystack integrates seamlessly with these systems to create powerful, end-to-end solutions for information retrieval and natural language processing tasks.\n\n> [!IMPORTANT]  \n> Please note that this project has used `haystack-ai` package version 2.8.0 and functionalities might (have) change(d) over time.\n\n## **Challenges**\n\n### **1. Fetching Knowledge**\nThe first challenge involves building a pipeline capable of fetching content from URLs, processing it for relevance, and enabling a seamless Q&A system. The objective was to configure the pipeline to identify the ten most relevant chunks of information from the content and ensure efficient query handling.\n\n[Notebook](./01_Enhancing_Pipeline.ipynb)\n\n### **2. Creating a RAG Pipeline and Filter Information using Weaviate**\nThis challenge focuses on utilizing the integration *Weaviate*, a vector database optimized for semantic search, to solve a fictional mystery. Goal is to design and implement a pipeline using Haystack and *Weaviate* that enables efficient retrieval of relevant information from a dataset answering the key question of the mystery.\n\n[Notebook](./02_Weaviate.ipynb)\n\n### **3. Enhancing Retrieval with Multi-Query RAG**\nThis task focuses on creating a Retrieval-Augmented Generation (RAG) pipeline that integrates multi-query retrieval techniques. The goal was to improve recall by retrieving highly relevant answers from external data sources, such as news feeds. Custom components were implemented to enhance the pipeline's performance.\n\n[Notebook](./03_Multi_Query_Retrieval.ipynb)\n\n### **4. Transcribing, Summarizing, and Rewriting using AssemblyAI**\nThis challenge concentrates on combining the capabilities of Haystack with AssemblyAI to process audio data and transform it into meaningful text outputs. The goal is to build a pipeline that can handle the following tasks: transcribing an audio file into text, summarizing the content for simplicity, and rewriting it in a creative style tailored for a specific audience.\n\n[Notebook](./04_AssemblyAI.ipynb)\n\n### **5. Accelerating Development with deepset Studio**\nThis project involves leveraging *deepset Studio*, a user-friendly platform for building and managing Haystack pipelines, to streamline the development of a Retrieval-Augmented Generation (RAG) pipeline. The task involves utilizing the platform's features, such as its drag-and-drop interface, pipeline templates, and deployment tools, to create an efficient indexing and query pipeline.\n\n[Notebook](./05_deepset_Cloud.ipynb)\n\n### **6. Integration with NVIDIA Inference Microservices**\nThis challenge focuses on leveraging *NVIDIA Inference Microservices (NIMs)* with Haystack to build two key functionalities: task delegation optimization and multilingual document organization. It demonstrates the practical application of NVIDIA's AI models through microservices for efficient workflow management.\n\n[Notebook](./06_NVIDIA_NIM.ipynb)\n\n### **7. Automated Matching and Evaluation with Arize Phoenix**\nThis challenge involves building an end-to-end system that automates matching and evaluation processes using advanced NLP techniques. It integrates Haystack for retrieval and generation, an LLM-based judge for evaluation, and *Arize Phoenix* for monitoring and tracing.\n\n[Notebook](./07_Arize_Phoenix.ipynb)\n\n### **8. Building an Inventory Management Agent**\nThis task demonstrates the implementation of an intelligent agent system using Haystack's experimental components, specifically designed for automated inventory management. Along with custom-built tools, the system enables real-time inventory tracking, automated price comparisons, and natural language interactions. The solution showcases how Haystack's tool-calling capabilities can be leveraged to create sophisticated AI agents that handle complex operational tasks while maintaining user-friendly interfaces.\n\n[Notebook](./08_Agents_Tools.ipynb)\n\n### **9. Self-Reflecting Recommendation Agent with MongoDB Integration**\nThis project demonstrates the implementation of a self-reflecting AI agent using Haystack's RAG capabilities integrated with *MongoDB Atlas* vector search. The system combines advanced retrieval techniques with recursive self-evaluation to optimize recommendations based on multiple criteria. By leveraging MongoDB's vector search capabilities and Haystack's RAG pipeline architecture, the solution enables semantic matching while incorporating budget constraints, age appropriateness, and preference optimization. The implementation showcases how Haystack can be used to create sophisticated recommendation systems that continuously improve their suggestions through self-reflection mechanisms.\n\n[Notebook](./09_MongoDB.ipynb)\n\n### **10. RAG Pipeline Evaluation**\nThis challenge demonstrates the implementation of systematic evaluation methodologies for RAG pipelines using Haystack's EvaluationHarness. By integrating specialized evaluators for faithfulness, context relevance, and overall performance metrics, the system enables automated assessment and comparison of different pipeline configurations. The implementation showcases how Haystack's evaluation framework can be used to measure and optimize RAG pipeline performance in production environments.\n\n[Notebook](./10_Evaluation_Harness.ipynb)\n\n## Setting Up the Project Environment on Windows or Linux\n\nFollow these steps to set up the project environment with Python and necessary tools:\n\n### **1. Install Python 3.11 with Anaconda**\n- Download the Anaconda installer from the official Anaconda website.\n- Install Anaconda following the graphical or command-line instructions for your operating system and add it to the PATH.\n- Verify the installation by running the following commands in the terminal (Linux/macOS) or Anaconda Prompt (Windows):\n\n  ```bash\n  python --version\n  ```\n\n### **2. Create and Activate a Conda Environment**\n- Create a new Conda environment named `haystack` with Python (verified for version 3.11):\n\n  ```bash\n  conda create --n haystack python=3.11\n  ```\n\n- Activate the environment:\n\n  ```bash\n  conda activate haystack\n  ```\n\n### **3. Install PyTorch with CUDA Support**\n- Ensure you have a CUDA-compatible device if using GPU acceleration.\n- Install PyTorch with CUDA (verified for version 12.1) support using pip:\n\n  ```bash\n  pip install torch --index-url https://download.pytorch.org/whl/cu121\n  ```\n\n### **4. Install Jupyter Notebook Support for VS Code**\n- Install the necessary packages to work with Jupyter notebooks in Visual Studio Code:\n\n  ```bash\n  pip install notebook jupyterlab ipykernel\n  ```\n\n- If you haven't already, install the Jupyter extension in Visual Studio Code from the Extensions Marketplace.\n\n### **5. Install Additional Dependencies**\n- Install the additional dependencies listed in a `requirements.txt` file by running:\n\n  ```bash\n  pip install -r requirements.txt\n  ```\n  \n- The `requirements.txt` file will download the latest versions of the defined packages. \n- After installing packages. open Visual Studio Code, select your Conda environment as the interpreter (*Python: Select Interpreter* from the Command Palette).\n- Open the Jupyter notebook corresponding to the specific project.\n\n### **6. Save Your OpenAI Secret Key**\n- To use OpenAI's API, you need to save your secret key securely in a configuration file.\n- Create a `config.json` file in your project directory and add the following content:\n\n  ```json\n  \"sk-proj-....\"\n  ```\n\n- Replace `\"sk-proj-....\"` with your actual OpenAI secret key, but make sure it is contained in the string `\"...\"`\n- Ensure this file is not shared or pushed to version control systems like Git by adding it to `.gitignore`.\n\n### **7. Verify Setup**\n- Open Visual Studio Code, select your Conda environment as the interpreter (*Python: Select Interpreter* from the Command Palette).\n- Open the Jupyter notebook corresponding to the specific project.\n"
    },
    {
      "name": "speakeasy-api/openapi-agent-examples",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/91446104?s=40&v=4",
      "owner": "speakeasy-api",
      "repo_name": "openapi-agent-examples",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2024-12-11T07:59:41Z",
      "updated_at": "2025-02-10T15:46:27Z",
      "topics": [],
      "readme": "<div align=\"center\">\n <a href=\"https://www.speakeasy.com/\" target=\"_blank\">\n   <picture>\n       <source media=\"(prefers-color-scheme: light)\" srcset=\"https://github.com/user-attachments/assets/21dd5d3a-aefc-4cd3-abee-5e17ef1d4dad\">\n       <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://github.com/user-attachments/assets/0a747f98-d228-462d-9964-fd87bf93adc5\">\n       <img width=\"100px\" src=\"https://github.com/user-attachments/assets/21dd5d3a-aefc-4cd3-abee-5e17ef1d4dad#gh-light-mode-only\" alt=\"Speakeasy\">\n   </picture>\n </a>\n  <h1>Speakeasy</h1>\n  <p>Build APIs your users love ❤️ with Speakeasy</p>\n  <div>\n   <a href=\"https://speakeasy.com/docs/create-client-sdks/\" target=\"_blank\"><b>Docs Quickstart</b></a>&nbsp;&nbsp;//&nbsp;&nbsp;<a href=\"https://join.slack.com/t/speakeasy-dev/shared_invite/zt-1cwb3flxz-lS5SyZxAsF_3NOq5xc8Cjw\" target=\"_blank\"><b>Join us on Slack</b></a>\n  </div>\n<h1>\n OpenAPI agent examples\n</h1>\n\n\n</div>\n\n\n\nThis repository demonstrates how to build OpenAPI-powered AI agents using two popular frameworks, [LangChain](https://python.langchain.com/docs/introduction/) and [Haystack](https://haystack.deepset.ai/). These agents can process user queries, interact with APIs described by OpenAPI documents, and generate intelligent responses.\n\nFor an in-depth comparison of LangChain and Haystack, see [this article](https://www.speakeasy.com/openapi).\n\n## Repository structure\n\nThe repository is organized into the following directories:\n\n### 1. `f1-fastapi-server`\n\nA mock Formula One (F1) API server built using FastAPI. This API serves as the backend for the agents.\n\n- **Files:**\n  - `main.py`: FastAPI application defining endpoints for querying F1 race winners.\n  - `openapi.yaml`: OpenAPI specification describing the API's endpoints, request parameters, and response formats.\n  - `README.md`: Instructions for setting up and running the F1 API server.\n  - `requirements.txt`: Dependencies required to run the server.\n\n### 2. `langchain-agent`\n\nAn AI agent built with LangChain that interacts with the F1 API to fetch race data based on user queries.\n\n- **Files:**\n  - `langchain_agent.py`: Script defining the LangChain agent.\n  - `openapi.json`: OpenAPI document (converted to JSON format) for the F1 API.\n  - `README.md`: Instructions for setting up and running the LangChain agent.\n  - `requirements.txt`: Dependencies required for the LangChain agent.\n\n### 3. `haystack-agent`\n\nAn AI agent built with Haystack that performs similar tasks as the LangChain agent.\n\n- **Files:**\n  - `haystack_agent.py`: Script defining the Haystack agent.\n  - `openapi.yaml`: OpenAPI document for the F1 API.\n  - `README.md`: Instructions for setting up and running the Haystack agent.\n  - `requirements.txt`: Dependencies required for the Haystack agent.\n\n## Setup instructions\n\n### Prerequisites\n\n- Python 3.9+\n- A valid API key from [OpenAI](https://platform.openai.com/) or [Anthropic](https://anthropic.com/).\n\n### Steps\n\n1. **Clone the repository:**\n\n   ```bash\n   git clone https://github.com/ritza-co/openapi-agent-examples.git\n   cd openapi-agent-examples\n   ```\n\n2. **Set up the F1 API server:**\n   - Navigate to the `f1-fastapi-server` directory:\n\n     ```bash\n     cd f1-fastapi-server\n     ```\n\n   - Install dependencies:\n\n     ```bash\n     pip install -r requirements.txt\n     ```\n\n   - Run the server:\n\n     ```bash\n     uvicorn main:app --reload\n     ```\n\n   The server will be available at `http://127.0.0.1:8000`.\n\n   To get the OpenAPI document for the API server, navigate to `http://127.0.0.1:8000/openapi.json`.\n\n3. **Run the LangChain agent:**\n   - Navigate to the `langchain-agent` directory:\n\n     ```bash\n     cd langchain-agent\n     ```\n\n   - Install dependencies:\n\n     ```bash\n     pip install -r requirements.txt\n     ```\n\n   - Set your Anthropic API key as an environment variable:\n\n     ```bash\n     export ANTHROPIC_API_KEY=\"your_api_key\"\n     ```\n\n   - Run the agent with a query:\n\n     ```bash\n     python langchain_agent.py \"Who won the Monaco Grand Prix in 2024?\"\n     ```\n\n4. **Run the Haystack agent:**\n   - Navigate to the `haystack-agent` directory:\n\n     ```bash\n     cd haystack-agent\n     ```\n\n   - Install dependencies:\n\n     ```bash\n     pip install -r requirements.txt\n     ```\n\n   - Set your OpenAI API key as an environment variable:\n\n     ```bash\n     export OPENAI_API_KEY=\"your_api_key\"\n     ```\n\n   - Run the agent with a query:\n\n     ```bash\n     python haystack_agent.py \"Who won the Monaco Grand Prix in 2024?\"\n     ```\n\n## Comparison of LangChain and Haystack\n\nBoth LangChain and Haystack are capable frameworks for building OpenAPI-powered AI agents. See [this article](https://www.speakeasy.com/openapi) for an in-depth comparison.\n"
    },
    {
      "name": "Maeret/ask-the-doc",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/62506259?s=40&v=4",
      "owner": "Maeret",
      "repo_name": "ask-the-doc",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2024-11-10T11:38:24Z",
      "updated_at": "2024-12-19T09:56:41Z",
      "topics": [],
      "readme": "# Ask the Doc\n\nAsk the Doc makes possible to use natural langage requests to your knowlege base or any documentation site.\n\nIt is a basic Retrieval-Augmented Generation (RAG) pipeline with an integrated web scraper. It allows you to fetch any documentation site, store its content, and query it using an LLM (Language Model) for precise answers. \n\n## Features\n- Fetch documentation websites and store their content.\n- Query the stored content using a pre-configured Question & Answers pipeline.\n- Ready-to-use FastAPI server for easy integration.\n\n---\n\n## Quick Start\n\n### Prerequisites\n1. Install Python 3.8+.\n2. Add your [Hugging Face token](https://huggingface.co/docs/hub/security-tokens) to the `.env` file:\n\n   ```plaintext\n   HUGGINGFACEHUB_API_TOKEN=your_huggingface_token\n   ```\n\n### Install dependencies\n```bash\npip install -r requirements.txt\n```\n\n### Run the server\n```bash\nuvicorn app.main:app --reload\n```\nYou can add the path to your SSL certificate in the `.env` file to enable requests to secure or private networks:\n\n```txt\nSSL_CRT = \"c:\\path\\to\\your\\CERT.pem\"\n```\n---\n\n## API Reference\n\n### Fetch a website\n\nStore the content of a website for querying:\n```bash\ncurl -X POST http://localhost:8000/fetch \\\n     -d '{\"base_url\": \"https://example.com\"}' \\\n     -H \"Content-Type: application/json\"\n```\n\n### Get stored documents\n\nRetrieve all stored documents:\n```bash\ncurl -X GET http://localhost:8000/documents\n```\n\n### Ask a question\n\nQuery the stored documents for an answer:\n```bash\ncurl -X POST http://localhost:8000/query \\\n     -d '{\"question\": \"What is wallet\"}' \\\n     -H \"Content-Type: application/json\"\n```\n\n---\n\n## Swagger Documentation\n\nInteractive API documentation is available at:\n[http://localhost:8000/docs](http://localhost:8000/docs)\n\n---\n\n## How It Works\n\n1. **Fetching:**\n   - The `/fetch` endpoint scrapes a website and stores its content in the document store. It doesn't fetch external links from the site, so it's useful for fetching your technical documentation or API.\n2. **Document Storage:**\n   - All content is stored in an in-memory document store or can be connected to a database for persistence.\n3. **Question Answering:**\n   - The `/query` endpoint retrieves relevant documents and uses a Hugging Face LLM to answer the question.\n\n---\n\n## Contributing\n\nI welcome contributions to improve the project! Please see the [CONTRIBUTING.md](./CONTRIBUTING.md) file for guidelines.\n\n## Dependecies\n\nThis application uses Hugging Face's APIs and requires adherence to their [Terms of Service](https://huggingface.co/terms).\n\n## License\n\nThis project is licensed under the **Apache 2.0 License**. See the [LICENSE](./LICENSE) file for details.\n"
    },
    {
      "name": "jianjungki/haystack-rag",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/1188643?s=40&v=4",
      "owner": "jianjungki",
      "repo_name": "haystack-rag",
      "description": "a rag toolkit with haystack and chainlit",
      "homepage": null,
      "language": "Python",
      "created_at": "2024-12-03T07:57:16Z",
      "updated_at": "2025-03-20T04:53:34Z",
      "topics": [],
      "readme": "# Intelligent Knowledge Base\n\nAn advanced RAG (Retrieval-Augmented Generation) system that automatically selects optimal parsing methods and embedding models based on document types and content.\n\n## Features\n\n- **Intelligent Content Analysis**: Automatically detects document type and content characteristics\n- **Adaptive Processing**: Selects optimal document splitters and embedding models based on content type\n- **Format-Specific Optimization**: Special handling for PDFs, code files, technical documents, and more\n- **Robust Conversion**: Handles a wide variety of document formats (PDF, DOCX, HTML, TXT, etc.)\n- **Interactive Interface**: User-friendly Chainlit interface for uploading documents and asking questions\n\n## Document Type Specialization\n\nThe system automatically optimizes processing based on content type:\n\n- **Code**: Uses code-optimized embedding models and word-based splitting\n- **PDF**: Uses PDF-specific processing with passage-based splitting\n- **Technical**: Uses models optimized for technical content with denser splitting\n- **Text**: General purpose processing for standard text documents\n\n## Installation\n\n1. Clone the repository\n2. Install dependencies:\n   ```\n   pip install -r requirements.txt\n   ```\n3. Set up your API key:\n   ```\n   export OPENAI_API_KEY=your_api_key_here\n   ```\n\n## Usage\n\nRun the knowledge base application:\n\n```\nchainlit run knowlage.py\n```\n\nThen:\n1. Upload any document (PDF, TXT, HTML, DOCX, etc.)\n2. Wait for processing (the system will analyze and index the document)\n3. Ask questions about the document content\n\n## How It Works\n\n1. **Content Analysis**: Analyzes documents using MIME types and extensions\n2. **Converter Selection**: Chooses optimal converter based on file type\n3. **Embedding Model Selection**: Selects specialized embedding models for different content types\n4. **Document Splitting**: Configures splitters optimally for different content types\n5. **RAG Pipeline**: Builds a specialized RAG pipeline tailored to the content\n6. **Query Processing**: Processes user queries with content-aware retrieval and generation\n\n## Supported File Types\n\n- Text files (.txt)\n- PDF documents (.pdf)\n- Microsoft Word (.docx)\n- HTML (.html, .htm)\n- Markdown (.md)\n- JSON (.json)\n- CSV (.csv)\n- PowerPoint (.pptx)\n- Excel (.xlsx)\n- Programming code files (.py, .js, .java, etc.)\n- And many more formats...\n\n## Advanced Configuration\n\nTo customize the embedding models or splitter configurations, modify the `EMBEDDING_MODELS` and `SPLITTER_CONFIG` dictionaries in `knowlage.py`.\n"
    },
    {
      "name": "DataScienceHamburg/GenerativeKImitPython_Material",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/59503425?s=40&v=4",
      "owner": "DataScienceHamburg",
      "repo_name": "GenerativeKImitPython_Material",
      "description": "Das ist unser Repo für den Bildungsurlaubskurs \"Generative KI mit Python\"",
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2024-11-17T10:53:23Z",
      "updated_at": "2025-03-28T12:52:03Z",
      "topics": [],
      "readme": "# Installation\n\n## Environment\n\n### Environment using `venv`\n\n#### Installation und Aktivierung auf Mac/Linux:\n\n```bash\npython -m venv .venv\nsource .venv/bin/activate\n```\n\n#### Installation und Aktivierung auf Windows:\n\n```bash\npython -m venv .venv\n.venv\\Scripts\\activate\n```\n\n#### Pakete installieren\n\nDie benötigten Pakete sind in der requirements.txt Datei aufgelistet. Diese kann mit folgendem Befehl installiert werden:\n\n```bash\npip install -r requirements.txt\n```\n\nSofern die Pakete nicht installiert werden können, kann man sie auch einzeln installieren.\n\n```bash\npip install <paketname>\n```\n\n### Environment using `uv`\n\n```bash\npip install uv\n```\n\n## Install dependencies\n\n```bash\nuv sync\n```\n\n## PyTorch special treatment\n\nPyTorch is not yet fully supported for Python 3.13.\n\n[PyTorch Issue #130249](https://github.com/pytorch/pytorch/issues/130249)\n\nThus, we need to install it manually.\n\n```bash\npip3 install --pre torch --index-url https://download.pytorch.org/whl/nightly/cpu\n```\n\n# Weitere Materialien\n"
    },
    {
      "name": "airndlab/hackathon-hacks-ai-mediawise-qna",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/125554461?s=40&v=4",
      "owner": "airndlab",
      "repo_name": "hackathon-hacks-ai-mediawise-qna",
      "description": "Международный хакатон ИИ 2024 | Media Wise | Поиск информации по библиотеке знаний",
      "homepage": null,
      "language": "HTML",
      "created_at": "2024-11-08T14:20:15Z",
      "updated_at": "2025-02-11T14:41:56Z",
      "topics": [],
      "readme": "# AI поисковая система и QnA бот по библиотеке знаний \n\n[![Docker Image Version (latest semver)](https://img.shields.io/docker/v/airndlab/perplexica-frontend?label=perplexica-frontend)](https://hub.docker.com/r/airndlab/perplexica-frontend)\n[![Docker Image Version (latest semver)](https://img.shields.io/docker/v/airndlab/perplexica-backend?label=perplexica-backend)](https://hub.docker.com/r/airndlab/perplexica-backend)\n[![Docker Image Version (latest semver)](https://img.shields.io/docker/v/airndlab/mediawise-rag?label=mediawise-rag)](https://hub.docker.com/r/airndlab/mediawise-rag)\n\nРешение команды **НейроДрайв**\nпо [кейсу](docs/media-wise.pdf) созданию поисковая система и чат-бот по библиотеке знаний медийного агентства\n[Media Wise](https://mediadirectiongroup.ru/agency/mediawise/)\nна хакатоне Цифровой прорыв сезон: ИИ.\n\n![avatar](docs/images/avatar.jpg)\n## Описание задачи\nВ рамках хакатона мы разработали гибридную ИИ-поисковую систему для маркетингового агентства, направленное на повышение эффективности стратегий за счет анализа данных и оптимизации работы с контентом. Цель — автоматизация поиска, обработки информации и построение рекомендаций, основанных на накопленных материалах и данных заказчика.\n\n## Архитектура\n\n### 1. **vllm**\n- **Задача:** Обработка запросов на основе модели `Qwen2.5-14B-Instruct-GPTQ-Int4`.\n- **Технологии:** vllm с поддержкой GPU для быстрой обработки запросов.\n- **Детали:**\n    - Модель загружается с сервиса Hugging Face.\n    - Используется с квантованием GPTQ для повышения производительности на GPU.\n    - Для haystack используется генератор openai - но запросы отправляются в локальный vllm \n\n### 2. **AI поисковый сервис и QnA**\n- **Задача:** Обеспечивает обработку вопросов и генерацию данных с использованием LLM\n- **Технологии:** Python, FastAPI.\n\n### 3. **Интерфейс**\n- **Задача:** Обеспечивает интерфейс общения с пользователями\n- **Технологии:** Perplexica, React.\n- **Детали:**\n    - Основной интерфейс на основе fork [Perplexica](https://github.com/airndlab/Perplexica).\n    - Ведение и хранение диалогов\n    - Загрузка документов в s3 совместимое хранилище\n\n### 4. **Просмотр базы знаний и аналитика**\n- **Задача:** Предоставляет интерфейс для аналитики\n- **Технологии:** Metabase.\n- **Детали:**\n    - Используется для анализа запросов и улучшения работы поисковой системы\n\n## [Описание RAG-пайплайна](rag/pipline/pipeline.ipynb)\n\n## Стек технологий\n- **Интерфейс:** `Perplexica`\n- **Языки программирования:** `Python`, `JavaScript`.\n- **Базы данных:** `SQLite`, `Metabase`, `s3`.\n- **RAG-пайплайны:** `Haystack`.\n- **Модели ИИ:** `Qwen2.5-14B-Instruct-GPTQ-Int4`.\n- **Инфраструктура:** `Docker Compose`, `vLLM`\n\n![Technologies](docs/images/technologies.svg)\n\n## Установка\n\nДля запуска проекта на локальной машине необходимо:\n1. Убедиться, что установлен Docker и docker-compose.\n2. Настроить файл `.env` с необходимыми переменными окружения.\n3. Выполнить команду:\n   ```bash\n   docker-compose up\n   ```\n## Сборка\n\n```\ndocker build -t <название вашего образа>:<тег вашего образа> .\n```"
    },
    {
      "name": "ATANKERA/byoLLM",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/57043781?s=40&v=4",
      "owner": "ATANKERA",
      "repo_name": "byoLLM",
      "description": "Bring your own LLM, local UI for your purposes.",
      "homepage": null,
      "language": "Python",
      "created_at": "2024-11-07T05:11:21Z",
      "updated_at": "2025-01-20T02:26:39Z",
      "topics": [],
      "readme": "# byoLLM\nBring Your Own LLM, local UI for your purposes.\n\nThis Project is targeted to allow peopl\n\n\n# Local Llama\n\nThis tool lets you interact with your PDFs, TXT, or Docx files & LLMs entirely offline, removing the need for OpenAI or cloud dependencies. It uses local Language Models for improved privacy and fully offline capabilities.\n\n\n## Features\n\n- Full Offline Capability: Works even without an internet connection\n- Local LLM Support: Uses Ollama for high-efficiency local processing\n- Versatile File Support: Handles PDF, TXT, DOCX, and MD files\n- Persistent Document Indexing: Stores and reuses embedded documents\n- User-Friendly Interface: Accessible via Streamlit\n\n\n## New Updates\n\n- Ollama integration for significant performance improvements\n- Uses `nomic-embed-text` (Ollama Default) and `llama3.2:1b` models (models are customizable)\n    - ref `text_embedder.py`:\n    ```\n        model: str = \"nomic-embed-text\",\n        url: str = \"http://localhost:11434\",\n    ```\n- Upgraded to Haystack 2.0 for improved processing\n- Integrated a persistent Chroma vector database for reusing uploaded documents\n\n\n## Setup Instructions\n\n1. Download Ollama from [here](https://ollama.ai/download)\n2. Clone this repository\n3. Install required dependencies:\n   ```\n   pip install -r requirements.txt\n   ```\n4. Load the necessary Ollama models:\n   ```\n   ollama pull nomic-embed-text\n   ollama pull llama3.2:1b\n   ```\n\n## Running the Application\n\n1. Start the Ollama server:\n   ```\n   ollama serve\n   ```\n2. Launch the Streamlit application:\n   ```\n   python -m streamlit run LLM_Chat_Interface.py\n   ```\n   1. **Note**\n      If you get error 403 when uploading files, redeploy the app like so, do disable CORS:\n      ```\n      python -m streamlit run LLM_Chat_Interface.py --server.enableXsrfProtection false\n      ```\n3. Upload your documents and start chatting!\n\n\n\n\n## How It Works\n\n1. Document Indexing: Uploaded files are processed, split, and embedded using Ollama.\n2. Vector Storage: Embeddings are stored in a local Chroma vector database.\n3. Query Matching: User inputs are embedded, and related document sections are identified and retrieved.\n4. Response Generation: Ollama generates responses based on the retrieved context and chat history.\n"
    },
    {
      "name": "JaegerP/web-app-rag",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/7987899?s=40&v=4",
      "owner": "JaegerP",
      "repo_name": "web-app-rag",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2024-10-26T12:45:49Z",
      "updated_at": "2024-11-01T14:08:40Z",
      "topics": [],
      "readme": "# RAG für ZaPF und jDPG Beschlüsse\n\nKurzanleitung zum Deployment der App:\n\n* Python (<=3.11) mit installierten Requirements\n* Azure OpenAI Service\n* Azure App Service Plan für Web App\n\n## Lokal starten:\n\n```bash\nexport AZURE_OPENAI_API_KEY=<YOUR API KEY>\n\npython -m streamlit run main-jdpg.py #bzw\npython -m streamlit run main-zapf.py\n```\nim jeweiligen Verzeichnis unter `./environments`\n\n## Deployment in Azure\n\nOn the webapp, the environment variable `AZURE_OPENAI_API_KEY` to your API key. This can be done through the Azure Portal or az cli.\n\n**Commands for deploying the app:**\n\n```bash\naz webapp up --name <WEB APP NAME> --plan <ASP NAME> --sku B1 --resource-group <RG NAME> --runtime \"PYTHON|3.11\" --location WestEurope\n\naz webapp config set --resource-group <RG NAME> --name <WEB APP NAME> --startup-file \"streamlit run main-<ENVIRONMENT>.py --server.port=8000 --server.address=0.0.0.0\"\n```\n\nLocation and SKU can be changed as needed\n\n"
    },
    {
      "name": "gelbal/wordpress-author-style-imitate",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/2027598?s=40&v=4",
      "owner": "gelbal",
      "repo_name": "wordpress-author-style-imitate",
      "description": "A tool that analyzes an author's writing style from their WordPress.com blog posts and helps generate new content that mimics their unique voice.",
      "homepage": "",
      "language": "Jupyter Notebook",
      "created_at": "2024-10-24T18:54:54Z",
      "updated_at": "2025-02-04T13:04:03Z",
      "topics": [],
      "readme": "# WordPress Author Style Imitator\n\nA tool that analyzes an author's writing style from their WordPress.com blog posts and helps generate new content that mimics their unique voice.\n\n![Project Pipeline](mimic_author_style.png)\n*Pipeline diagram showing the flow from WordPress.com API through style analysis to final content generation*\n\n## How It Works\n\nThe tool follows a four-stage pipeline as shown in the diagram above:\n\n1. **Data Collection**: Fetches posts and metadata from WordPress.com API\n2. **Content Classification**: Uses local LLM to filter out confidential content\n3. **Style Analysis**: Leverages Claude to generate comprehensive style instructions\n4. **Style Application**: Applies captured style patterns to new content\n\n\n## Project Structure\n\n- `src/`: Contains the source code for the project\n  - `auth/`: Authentication related code.\n  - `notebooks/`: Jupyter notebooks for the main pipeline\n  - `utils/`: Utility functions and helpers\n- `data/`: Contains the processed data (not included in repo)\n  - `author_posts/`: Raw posts by author\n  - `classified_posts/`: Posts after confidentiality classification\n  - `author_instructions/`: Generated writing style instructions\n  - `post_instructions/`: Style-applied drafts\n\n## Setup\n\n1. Clone this repository\n2. Create a `.env` file in the root directory with:\n   - `WPCOM_CLIENT_SECRET`: WordPress.com API client secret\n   - `WPCOM_ACCESS_TOKEN`: WordPress.com API access token\n   - `ANTHROPIC_API_KEY`: Anthropic API key for Claude\n3. Install required packages using Poetry: `poetry install`\n\n## Notebooks\n\n1. Obtain WordPress.com API credentials and set up an app (see [WordPress.com API documentation](https://developer.wordpress.com/docs/))\n2. Get an Anthropic API key for Claude 3.5\n3. Run `auth_get_token.py` to obtain an access token\n4. Execute notebooks in the following order:\n  - `retrieve_posts.ipynb`\n  - `llm_classify_posts.ipynb`\n  - `llm_generate_author_prompts.ipynb`\n  - `llm_apply_author_style.ipynb`\n\n### retrieve_posts.ipynb\nFetches blog posts and engagement metrics from WordPress.com API. The notebook:\n- Retrieves posts using WordPress.com REST API\n- Collects metadata (views, likes, comments)\n- Handles rate limiting and pagination\n- Saves posts organized by domain and author\n- Processes cross-posts between different WordPress.com sites\n\n### llm_classify_posts.ipynb\nUses a local LLM to identify and filter out posts containing confidential information. The notebook:\n- Processes the top 50 posts by engagement score\n- Uses Ollama with Qwen 2.5 model for classification\n- Identifies sensitive/confidential content\n- Saves classification results with reasoning\n- Filters out confidential posts from further processing\n\n### llm_generate_author_prompts.ipynb\nAnalyzes non-confidential posts to generate comprehensive writing style instructions. The notebook:\n- Takes filtered posts from previous step\n- Uses Claude 3.5 to analyze writing patterns\n- Generates detailed style instructions covering:\n  - Tone and voice\n  - Content structure\n  - Language patterns\n  - Technical depth\n  - Engagement techniques\n- Saves instructions for use in content generation\n\n### llm_apply_author_style.ipynb\nApplies the generated style instructions to new content. The notebook:\n- Takes a draft post as input\n- Uses style instructions and example posts\n- Leverages Claude 3.5 to rewrite content\n- Maintains technical accuracy while matching author's voice\n- Saves the style-applied output\n\n## Use Cases\n\n### Content Creation and Editing\n- Writing assistants that adapt to your team's voice\n- Draft refinement while maintaining consistent style\n- Technical documentation with consistent tone across teams\n- Blog post generation matching established author voices\n\n### Marketing and Communications\n- Content generation for different brand personas\n- Consistent messaging across multiple channels\n- Newsletter and marketing email writing\n- Campaign content that maintains brand voice\n\n### Learning and Development\n- Style analysis for writing improvement\n- Understanding different writing approaches\n- Learning from experienced writers' patterns\n- Continuous feedback on writing habits\n\n### Localization and Accessibility\n- Style-aware content adaptation\n- Maintaining voice consistency in translations\n- Adapting technical content for different audiences\n- Accessibility-focused content rewrites\n\n## Benefits and Insights\n\n- **Style Preservation**: Captures and maintains unique writing voices while generating new content\n- **Learning Tool**: Helps understand and learn from different writing styles\n- **Efficiency**: Streamlines content creation while maintaining consistency\n- **Flexibility**: Adapts to different writing styles and content types\n- **Quality Control**: Ensures consistent voice across multiple pieces of content\n\n## Note on Data Privacy\n\nThis repository intentionally excludes input and output data as the development used internal blog posts from Automattic. When using this tool, ensure you have appropriate permissions for the blog posts you analyze.\n\n## License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n"
    },
    {
      "name": "carlo-airaghi/chat-widget-project",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/60608923?s=40&v=4",
      "owner": "carlo-airaghi",
      "repo_name": "chat-widget-project",
      "description": null,
      "homepage": null,
      "language": "JavaScript",
      "created_at": "2024-09-16T13:05:04Z",
      "updated_at": "2025-04-21T09:12:46Z",
      "topics": [],
      "readme": "# THEAPESHAPE AI Chat Widget\n\nThe **THEAPESHAPE AI Chat Widget** is an interactive AI-powered chatbot designed to provide personalized fitness coaching and guidance to advanced fitness enthusiasts. It leverages a retrieval-augmented generation (RAG) system using OpenAI's GPT models and Haystack to deliver relevant and contextual responses.\n\n## Table of Contents\n\n- [Overview](#overview)\n- [Features](#features)\n- [Tech Stack](#tech-stack)\n- [Installation](#installation)\n- [Usage](#usage)\n- [Project Structure](#project-structure)\n- [Future Enhancements](#future-enhancements)\n- [License](#license)\n- [Contribution](#contribution)\n\n## Overview\n\nThe project is part of **The Ape Shape** app, a platform designed for fitness enthusiasts looking to optimize their workout routines and diet plans. This widget is embedded into the app's interface, allowing users to interact with an AI-based coach for personalized advice.\n\n## Features\n\n- **Personalized Responses**: Provides advice based on the user's fitness data, including weight, height, body fat percentage, and caloric expenditure.\n- **Retrieval-Augmented Generation**: Combines a document retriever with OpenAI's GPT to deliver highly relevant responses.\n- **Privacy and Survey Integration**: Includes a privacy policy acknowledgment and a post-chat survey for user feedback.\n- **Dynamic Chat Interface**: Fully customizable widget with an intuitive interface.\n- **PDF Document Indexing**: Supports knowledge retrieval from PDF documents.\n\n## Tech Stack\n\n- **Backend**: Flask, Haystack, OpenAI API\n- **Frontend**: JavaScript, HTML, CSS\n- **Containerization**: Docker\n- **Document Processing**: PyPDF\n- **Database**: In-memory document store (Haystack)\n\n## Installation\n\n### Prerequisites\n\n- Docker and Docker Compose installed on your system.\n- OpenAI API key set as an environment variable (`OPENAI_API_KEY`).\n\n### Steps\n\n1. Clone the repository:\n   ```bash\n   git clone https://github.com/yourusername/the-ape-shape-ai-chat-widget.git\n   cd the-ape-shape-ai-chat-widget\n2. Build the Docker image:\n```\ndocker build -t the-ape-shape-chat-widget .\n```\n3. Run the container:\n```\ndocker run -p 5000:5000 -e OPENAI_API_KEY=your_openai_api_key the-ape-shape-chat-widget\nAccess the widget at http://127.0.0.1:5000.\n```\n4. Open the test page (test1.html or test2.html) with the vs code extension \"Live Server\"  \n\n### Usage\n1. Open the test.html file in a browser to interact with the chat widget locally.\n2. Update the embedded script in the test.html file to include user data.\n3. Use the chat interface to send messages and receive AI-generated responses.\n4. Provide feedback via the post-chat survey to test survey functionality.\n\n### Project Structure\n```\n.\n├── Dockerfile\n├── app.py\n├── directory_tree.txt\n├── requirements.txt\n├── static_bmw\n│   ├── chat-widget.css\n│   ├── chat-widget.js\n│   ├── fonts\n│   └── images\n├── static_theapeshape\n│   ├── chat-widget.css\n│   ├── chat-widget.js\n│   ├── documents\n│   ├── fonts\n│   └── images\n├── test1.html\n└── test2.html\n```\n### Future Enhancements\n\n- Database Integration: Replace the in-memory store with a persistent database.\n- Advanced Analytics: Incorporate analytics to track user interactions and improve performance."
    },
    {
      "name": "SyedAffan10/Document-QA-Chatbot",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/63011097?s=40&v=4",
      "owner": "SyedAffan10",
      "repo_name": "Document-QA-Chatbot",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2024-10-17T18:56:36Z",
      "updated_at": "2024-10-26T16:33:55Z",
      "topics": [],
      "readme": "---\n\n# Document QA Chatbot\n\nThis project implements a Document Question-Answering (QA) chatbot using Streamlit and Haystack. The application allows users to upload and query `.docx` documents. It processes the documents, splits them into manageable chunks, and uses embedding techniques to provide accurate answers to user queries.\n\n## Features\n\n- Upload multiple `.docx` documents.\n- Ask questions related to the content of the uploaded documents.\n- Provides relevant answers based on the document's content.\n- Error handling for unsupported or corrupt document files.\n\n## Technologies Used\n\n- **Streamlit**: For building the web application interface.\n- **Haystack**: For implementing the document processing and QA pipeline.\n- **Langchain Community**: For loading `.docx` files.\n- **Sentence Transformers**: For embedding documents and queries.\n- **UUID**: For generating unique document IDs.\n\n## Installation\n\n1. Clone the repository or download the code files.\n2. Ensure you have Python installed on your machine.\n3. Create a virtual environment and activate it:\n   ```bash\n   python -m venv env\n   source env/bin/activate  # On Windows use `env\\Scripts\\activate`\n   ```\n4. Install the required packages:\n   ```bash\n   pip install -r requirements.txt\n   ```\n\n## Usage\n\n1. Run the Streamlit application:\n   ```bash\n   streamlit run app.py\n   ```\n2. Open your web browser and navigate to the URL provided (typically `http://localhost:8501`).\n3. Upload your `.docx` files using the interface.\n4. Ask questions about the content in the uploaded documents using the chat interface.\n\n## File Structure\n\n```\n/your_project_directory/\n│\n├── app.py                     # Main application file\n└── /GEN_AI/                   # Directory for .docx files\n    └── your_document.docx     # Example document file\n```\n\n## How It Works\n\n1. **Loading Documents**: The application uses `Docx2txtLoader` from Langchain to load `.docx` files and extract their content.\n2. **Text Splitting**: The content is split into chunks using `RecursiveCharacterTextSplitter`, which allows for efficient processing and retrieval of information.\n3. **Embedding Pipeline**: The documents are embedded using the `SentenceTransformersDocumentEmbedder`. The embedded documents are stored in an in-memory document store.\n4. **Retrieval and QA Pipeline**: A retrieval pipeline is created to find relevant chunks based on the user's queries. The `ExtractiveReader` component is used to provide answers to the queries.\n5. **User Interface**: Streamlit is used to create a user-friendly interface for uploading documents and interacting with the chatbot.\n---\n"
    },
    {
      "name": "shrimantasatpati/fastapi_LLM_bot_Streamlit",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/82357659?s=40&v=4",
      "owner": "shrimantasatpati",
      "repo_name": "fastapi_LLM_bot_Streamlit",
      "description": null,
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2024-10-10T19:15:16Z",
      "updated_at": "2024-11-26T05:50:34Z",
      "topics": [],
      "readme": ""
    },
    {
      "name": "NERC-CEH/llm-eval",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/5802506?s=40&v=4",
      "owner": "NERC-CEH",
      "repo_name": "llm-eval",
      "description": "Scripts and data for LLM evaluation.",
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2024-08-19T13:05:53Z",
      "updated_at": "2024-12-20T14:23:22Z",
      "topics": [],
      "readme": "# llm-eval\nThis repository contains a reproducible workflow setup using [DVC](https://dvc.org/) backed by a [JASMIN object store](https://help.jasmin.ac.uk/docs/short-term-project-storage/using-the-jasmin-object-store/). Before working with the repository please contact [Matt Coole](mailto:matcoo@ceh.ac.uk) to request access to the Jasmin object store `llm-eval-o`. Then follow the instructions below.\n\n## Requirements\n- [Ollama](https://ollama.com/download) ([`llama3.1`](https://ollama.com/library/llama3.1) and [`mistral-nemo`](https://ollama.com/library/mistral-nemo) models)\n- [Python 3.9+](https://www.python.org/downloads/)\n- [uv](https://docs.astral.sh/uv/)\n\n## Getting started\n### Setup\nThis project uses `uv` to manage python version and dependency. If you haven't got `uv` installed, the easiest way to it is using `pip`:\n```\npip install uv\n```\nThis will allow you to use uv across projects. You can verify that the installation is successful by running:\n```\nuv --version\n```\n\nOnce `uv` is installed you can use it to automatically download the appropriate version of python and create a virtual environment for running the project code. This can be done using:\n```\nuv sync\n```\nThis will create a virtual environment in `.venv` and installed the necessary dependencies from `pyproject.toml`. Within the project, any commands that you wish to run can be preceeded by `uv run` to ensure that they run with the correct version of python and using the correct virtual environment.\n> **Note:** The remainder of this readme assume you have either activated the virtual environment created using `source .venv/bin/activate` or that you are prepending all commands with `uv run`.\n\n### Configuration\nNext setup your local DVC configuration with your [Jasmin object store access key](https://help.jasmin.ac.uk/docs/short-term-project-storage/using-the-jasmin-object-store/#creating-an-access-key-and-secret):\n```shell\ndvc remote modify --local jasmin access_key_id '<ACCES_KEY_ID>'\ndvc remote modify --local jasmin secret_access_key '<KEY_SECRET>'\n```\n### Getting the data\nPull the data from the object store using DVC:\n```shell\ndvc pull\n```\n### Working with the pipeline\nYou should now be ready to run the pipeline:\n```shell\ndvc repro\n```\nThis should only reproduce the pipeline, but only stages that have been modified will actually be re-run (see output whilst running). If you want to check that all stages of the pipeline are running correctly you can either user the `-f` flag with the above command to force DVC to re-run all stages of the pipeline or (as re-running with all the data can take several hours) run the convenience script `test-pipeline.sh`. This script will run the pipeline with a tiny subset of data as an experiment which should only take a copule of minutes:\n```shell\n./test-pipeline.sh\n```\n\nThis pipeline is defined in [`dvc.yaml`](dvc.yaml) and can be viewed with the command:\n```shell\ndvc dag\n```\nor it can be output to mermaid format to display in markdown:\n```shell\ndvc dag -md\n```\n```mermaid\nflowchart TD\n\tnode1[\"chunk-data\"]\n\tnode2[\"create-embeddings\"]\n\tnode3[\"evaluate\"]\n\tnode4[\"extract-metadata\"]\n\tnode5[\"fetch-metadata\"]\n\tnode6[\"fetch-supporting-docs\"]\n\tnode7[\"generate-testset\"]\n\tnode8[\"run-rag-pipeline\"]\n\tnode9[\"upload-to-docstore\"]\n\tnode1-->node2\n\tnode2-->node9\n\tnode4-->node1\n\tnode5-->node4\n\tnode5-->node6\n\tnode6-->node1\n\tnode7-->node8\n\tnode8-->node3\n\tnode9-->node8\n\tnode10[\"data/evaluation-sets.dvc\"]\n\tnode11[\"data/synthetic-datasets.dvc\"]\n```\n> Note: To re-run the `fetch-supporting-docs` stage of the pipeline you will need to request access to the [Legilo](https://legilo.eds-infra.ceh.ac.uk/) service from the EDS dev team and provide your `username` and `password` in a `.env` file.\n\n## Running Experiments\nThe pipeline by default will run using the parameters defind in [`params.yaml`](params.yaml). To experiment with varying these paramaters you can change them directly, or use [DVC experiments](). \n\nTo run an experiment varying a particual parameter:\n```shell\ndvc exp run -S hp.chunk-size=1000\n```\nThis will re-run the pipeline but override the value of the `hp.chunk-size` parameter in [`params.yaml`](params.yaml) and set it to `1000`. Only the necessary stages of the pipeline should be re-run and the result should appear in your workspace.\n\nYou can compare the results of your experiment to the results of the baseline run of the pipeline using:\n```shell\ndvc exp diff\n```\n```shell\nPath               Metric              HEAD      workspace    Change\ndata/metrics.json  answer_correctness  0.049482  0.043685     -0.0057974\ndata/metrics.json  answer_similarity   0.19793   0.17474      -0.02319\ndata/metrics.json  context_recall      0.125     0            -0.125\ndata/metrics.json  faithfulness        0.75      0.69375      -0.05625\n\nPath         Param          HEAD    workspace    Change\nparams.yaml  hp.chunk-size  300     1000         700\n```\n\nIt is also possible to compare the results of all experiments:\n```shell\ndvc exp show --only-changed\n```\nExperiments can be remove using (`-A` flag removes all experiment, but individually experiment can be removed using their name or ID):\n```shell\ndvc exp remove -A\n```\n### Experiment Runner\nThe repository includes a simple shell script that can be used as an experiment runner to test various different models:\n```shell\n./run-experiments.sh\n```\nThis will run the dvc pipeline with various different llm model (check the shell scripts for details) and save the results as experiments. \n\nAn experiment for each model defined will be queued and run in the background. To check the status of the experiments:\n```shell\ndvc queue status\n```\nTo check the output for an experiment currently running use:\n```shell\ndvc queue log $EXPERIMENT_NAME\n```\n## Other Notes\n\n### DVC and CML\nNotes on the use of Data Version Control and Continuous Machine Learning:\n- [DVC](dvc.md)\n- [CML](cml.md)\n\n### vLLM\nNotes on running models with vLLM:\n- [vLLM](vllm.md)\n"
    },
    {
      "name": "yusyel/rag-wiki-helpers",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/25446622?s=40&v=4",
      "owner": "yusyel",
      "repo_name": "rag-wiki-helpers",
      "description": null,
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2024-10-07T23:24:48Z",
      "updated_at": "2024-10-09T19:45:39Z",
      "topics": [],
      "readme": "# /R/Germany Wiki Pages Helper\n\n![Reddit Aliens with Germany Theme](./reddit.webp)\n\n\nThe /r/Germany subreddit is a community on Reddit focused on discussions related to Germany in English.\n\nThe /r/Germany wiki pages contain a number of useful resources to help users navigate key topics related to Germany. The topics range from education (university, Studienkolleg) and housing to the tax system, work and visa process.\n\n\"Shortcuts\" bot in /r/Germany provides quick links to frequently referenced info like FAQs or wiki pages.\n\nFor example, if someone asks a question about insurance, the `!insurance` shortcut will return links to the relevant wiki page. The problem with this is that some wiki pages are very long, and even if the wiki page contains the answer to a specific question, you may have to read the whole page or navigate to another wiki page. Wiki helpers makes it easier to find specific answers without having to read through long pages or follow multiple links.\n\n\n[Project Google Cloud Service App link](https://wikis-653995409450.europe-west10.run.app/)\n\n\n\n## Evaluation\n\n\n### Question Vector\n\n384 is `multi-qa-MiniLM-L6-cos-v1` model.\n\n768 is `multi-qa-distilbert-cos-v1` model.\n\n\n\n| Model            | Hit Rate              |         MRR          |\n|------------------|-----------------------|-----------------------|\n| 384_docs_4o_mini | 0.822087745839637     | 0.6860010085728689    |\n| 384_docs_llama   | 0.822087745839637     | 0.6860010085728689    |\n| 768_docs_4o_mini | 0.8311649016641453    | 0.6961018658598076    |\n| 768_docs_llama   | 0.8311649016641453    | 0.6961018658598076    |\n\n\n### Content Vector\n\n| Model            | Hit Rate              |         MRR          |\n|------------------|-----------------------|-----------------------|\n| 384_docs_4o_mini | 0.8717095310136157    | 0.717297024710035     |\n| 384_docs_llama   | 0.8720121028744326    | 0.7173726676752392    |\n| 768_docs_4o_mini | 0.8925869894099848    | 0.7341805345436204    |\n| 768_docs_llama   | 0.8925869894099848    | 0.7341805345436204    |\n\n\n### Content + Question Vector\n\n| Model            | Hit Rate              |         MRR          |\n|------------------|-----------------------|-----------------------|\n| 384_docs_4o_mini | 0.9128593040847202    | 0.7873424104891578    |\n| 384_docs_llama   | 0.9131618759455371    | 0.787418053454362     |\n| 768_docs_4o_mini | 0.9231467473524962    | 0.7947503782148263    |\n| 768_docs_llama   | 0.9228441754916793    | 0.7948562783661121    |\n\n### Haystack Ragas Evaluator\n\n| Model                          | Doc MRR Evaluator     | Doc Rec(hit rate) Evaluator     | MAP                   |\n|--------------------------------|-----------------------|-----------------------|-----------------------|\n| model_mini_lm_docs_4o_mini    | 0.7834947049924357    | 0.9122541603630863    | 0.7839544461253997    |\n| model_mini_lm_docs_llama      | 0.7829803328290469    | 0.9113464447806354    | 0.7834400739620109    |\n| model_distilbert_docs_4o_mini | 0.7919616742309635    | 0.9222390317700454    | 0.7923861153134987    |\n| model_distilbert_docs_llama    | 0.7912455874936968    | 0.9213313161875946    | 0.791670028576232     |\n\n\n### Haystack Ragas Evaluator Answer relevancy\n\n[Haystack RagasEvaluator](https://haystack.deepset.ai/cookbook/rag_eval_ragas#evaluate-the-rag-pipeline)\n\nRandom selected 50 sample scores:\n\n\n| Statistic | Value       |  * | Statistic | Value       |  * | Statistic | Value       |  * | Statistic | Value       |\n|-----------|-------------|---|-----------|-------------|---|-----------|-------------|---|-----------|-------------|\n| Count     | 50.000000   |   | Count     | 50.000000   |   | Count     | 50.000000   |   | Count     | 50.000000   |\n| Mean      | 0.930749    |   | Mean      | 0.934164    |   | Mean      | 0.910511    |   | Mean      | 0.883394    |\n| Std Dev   | 0.194594    |   | Std Dev   | 0.194037    |   | Std Dev   | 0.234884    |   | Std Dev   | 0.266121    |\n| Min       | 0.000000    |   | Min       | 0.000000    |   | Min       | 0.000000    |   | Min       | 0.000000    |\n| 25%       | 0.951582    |   | 25%       | 0.955874    |   | 25%       | 0.931460    |   | 25%       | 0.928617    |\n| 50%       | 0.978830    |   | 50%       | 0.975092    |   | 50%       | 0.981040    |   | 50%       | 0.971967    |\n| 75%       | 0.992876    |   | 75%       | 0.996937    |   | 75%       | 0.996109    |   | 75%       | 0.989022    |\n| Max       | 1.000000    |   | Max       | 1.000000    |   | Max       | 1.000000    |   | Max       | 1.000000    |\n\n\n### Monitoring\n\nFor monitoring, each session is stored in Bigquery and a Looker dashboard is created.\n\n\n![Reddit Aliens with Germany Theme](./dashboard.png)"
    },
    {
      "name": "JeanJean-rxl/random-retrieval-plugin",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/33363603?s=40&v=4",
      "owner": "JeanJean-rxl",
      "repo_name": "random-retrieval-plugin",
      "description": null,
      "homepage": null,
      "language": "TypeScript",
      "created_at": "2024-10-02T11:58:07Z",
      "updated_at": "2024-12-28T09:20:57Z",
      "topics": [],
      "readme": "# Obsidian Random Retrieval Plugin\n\n**Note**: This feature is only supported on macOS so far.\n\nThis plugin was developped to open a random note based on local LLMs with Obsidian.\n- Random for Random Note in Obsidian\n- Retrieval for Retrieval Component in Haystack\n\nI developped this plugin because .. I have a bad memory... and I enjoy the pleasure of treasure Hunt. While\n- Search provides multiple way to reveal those sporadic ideas hidden in the vault,\n- Random Note, as a core plugin, provides a complete random way to dig it out...\n\nI choose LLM to be a retriver + ranker, so that this plugin can function like a fuzzy-search / half-random-note.\n\n\n## Installation\n\n- at the first time \n\nconda create --name rr-env\n\npip install -r requirements.txt\n\n- and then (for the most users)\n\nopen plugin in obsidian (a terminal will be activated automatically) \n\nclick ribbon button, and enjoy the nightwalk💡\n\n(please keep the terminal open while you run the plugin)\n\n- and then (for the developpers)\n\nfeel free to replace rr_app.py with your models \n\n\n\n## Future Plan\n- support more platform, such as: Windows, mobile device\n- multi-language support\n- speed up\n\n\n## Version History\n\n#### 1.0.7\n- complete the changes and minor changes requested\n\n\n#### 1.0.6\n- support language added: Chinese\n\n\n#### 1.0.4\n- update styles.css\n- allow user to configure the configuration directory\n\n\n#### 1.0.3\n- once conda env is set, open terminal automatically while activate the plugin\n- manually set the num of new pages (default open 3 new pages, ranking by retriever)\n\n\n#### 1.0.0\n- basic version: only support English for now.\n"
    },
    {
      "name": "CodeAKrome/bootcupboard",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/44734955?s=40&v=4",
      "owner": "CodeAKrome",
      "repo_name": "bootcupboard",
      "description": "It's bigger on the inside than the outside!",
      "homepage": null,
      "language": "Cypher",
      "created_at": "2023-09-16T20:26:33Z",
      "updated_at": "2024-10-11T12:37:27Z",
      "topics": [],
      "readme": "# bootcupboard\nIt's bigger on the inside than the outside!\n"
    },
    {
      "name": "Areopaguaworkshop/wenbi",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/138816467?s=40&v=4",
      "owner": "Areopaguaworkshop",
      "repo_name": "wenbi",
      "description": "wenbi",
      "homepage": "",
      "language": "Python",
      "created_at": "2024-09-15T07:52:41Z",
      "updated_at": "2025-02-27T20:32:11Z",
      "topics": [
        "ai-editor",
        "audio-text",
        "editor",
        "rewrite",
        "video-text"
      ],
      "readme": "# Wenbi\n\nA simple tool to make the video, audio, subtitle and video-url (especially youtube) content into a written markdown files with the ability to rewritten the oral expression into written ones, or translating the content into a target language by using LLM. \n\nInitally, this porject is just serving to my website [GCDFL](https://www.gcdfl.org/). We do a service to turn its lectures into a written files for easier further editing. \n\nWenbi is a Chinese name 文笔, meaning a good writting. \n\n:warning: LLM can make mistakes (of course, human make mistakes too) and cannot be fully trusted. LLM can only be used for preliminary processing of data, some elementary work, and in this sense, LLM does greatly improve editing efficiency. \n\n\n### you can try the [demo](https://archive.gcdfl.org/). \n\n## Features\n\n- **100% Open source and totally free of use**. I love open source, I learned a lot from it. \n\n- :100: Accept most popular audio, video, subtitle files and url--mainly using yt-dlp as input. \n\n- :100: Editing the files by using LLM to rewriting and translating the content into a readable written markdown files. \n\n- :100: Support input with multiple languages.\n\n- :100: offer an commandline and gradio GUI with multiple options for further personal settings \n\n- :100: The support provider is Ollama, you can use most of the models from ollama. \n\n- :construction: other provider supporting, such as OpenAi, Google and Others. \n\n- :construction: fine-tuned model for specific job, for example for my personal project from [GCDFL](https://www.gcdfl.org/), introducing the eastern churches to Chinese audience through academic lectures; [CTCFOL](https://www.ctcfol.org/), The Chinese Translation of Church Fathers from Original Languages. \n\n:warning: the default translating and rewritten language are Chinese, however, you can choose other Ollama models, and languages through our commandline options. \n\n## Install\n- You can install through pip (or other tools as uv or rye) and from source. \n\n### prerequest\n- Install [Ollama](https://ollama.com/) and dowload a model. The default model for this project is qwen2.5. \n\n### Install through pip\n\n1. build a virtue environment through [uv](https://docs.astral.sh/uv/guides/install-python/)--recommened or [conda](https://docs.conda.io/projects/conda/en/latest/user-guide/install/index.html).\n\n-for uv: `uv venv --python 3.12`\n\n2. `uv pip install wenbi` or `uv add wenbi`\n\n:warning: due to some package issues, it is better to install llvmlite and numba first by `uv add llvmlite numba` , then install wenbi. \n\nAfter install, you can simply using wenbi commandline. if you want a gradio GUI, you can run `wenbi --gui`. the commandline options as follow: \n```\nwenbi: Convert video, audio, URL, or subtitle files to CSV and Markdown outputs.\n\npositional arguments:\n  input                 Path to input file or URL\n\noptions:\n  -h, --help            show this help message and exit\n  --output-dir OUTPUT_DIR\n                        Output directory (optional)\n  --gui                 Launch Gradio GUI\n  --rewrite-llm REWRITE_LLM\n                        Rewrite LLM model identifier (optional)\n  --translate-llm TRANSLATE_LLM\n                        Translation LLM model identifier (optional)\n  --transcribe-lang TRANSCRIBE_LANG\n                        Transcribe language (optional)\n  --translate-lang TRANSLATE_LANG\n                        Target translation language (default: Chinese)\n  --rewrite-lang REWRITE_LANG\n                        Target language for rewriting (default: Chinese)\n  --multi-language      Enable multi-language processing\n  --chunk-length CHUNK_LENGTH\n                        the chunk of Number of sentences per paragraph for llm to tranlsate or rewrite.\n                        (default: 8)\n  --max-tokens MAX_TOKENS\n                        Maximum tokens for LLM output (default: 50000)\n  --timeout TIMEOUT     LLM request timeout in seconds (default: 3600)\n  --temperature TEMPERATURE\n                        LLM temperature parameter (default: 0.1)\n  --base-url BASE_URL   Base URL for LLM API (default: http://localhost:11434)\n\n```\n\n### Install from Source\n\n1. install [rye](https://rye.astral.sh/)\n\n2. `\ngit clone https://github.com/Areopaguaworkshop/wenbi.git\n` \n3. \n```\ncd wenbi \n\nmv pyproject.toml pyproject-bk.toml\n\nrye init \n\n```\n\n4. `\ncopy whole content of the pyproject-bk.toml into pyproject.toml\n` \n5. \n`source .venv/bin/activate` \n\n`rye pin 3.12` \n\n`rye sync`\n\n:warning: Again, you may face some package issues. you can either install the depencies one by one in the pyproject-bk.toml, or  add llvmlite and numba first by `rye add llvmlite numba` , then install wenbi. \n\n6. You can choose commandline or webGUI through gradio.\n\n- gradio\n\n`python wenbi/main.py`\n\nThen go to http://localhost:7860. \n\n- commandline \n\n'python wenbi/cli.py --help'\n\n:warning: if you want to convert the audio file of multi-language, you should set multi-language as True. for commandline is --multi-language. you nedd a HUGGINGFACE_TOKEN in you environment. by `export HUGGINGFACE_TOKEN=\"you HUGGINGFACE_TOKEN here\"`. \n\n\nEnjoy! \n\n### Buy me a [Cofee](https://www.gcdfl.org/donate/). \n\n## License:\nAI-Subtitle-Editor is licensed under the Apache License 2.0 found in the [LICENSE](https://github.com/Areopaguaworkshop/AI-Subtitle-Editor/blob/main/license.md) file in the root directory of this repository.\n\n## Citation:\n```@article{Areopaguaworkshop/wenbi\n  title = {wenbi},\n  author = {Ephrem, Yuan},\n  year = {2024},\n}\n\n```\n\n"
    },
    {
      "name": "MadisonEvans94/splade",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/100099375?s=40&v=4",
      "owner": "MadisonEvans94",
      "repo_name": "splade",
      "description": "splade retrieval example with milvus",
      "homepage": null,
      "language": "Python",
      "created_at": "2024-09-03T11:43:02Z",
      "updated_at": "2024-10-11T03:04:05Z",
      "topics": [],
      "readme": ""
    },
    {
      "name": "nsubordin81/haystack_learning",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/4014893?s=40&v=4",
      "owner": "nsubordin81",
      "repo_name": "haystack_learning",
      "description": null,
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2024-09-12T03:59:12Z",
      "updated_at": "2025-01-24T21:13:11Z",
      "topics": [],
      "readme": ""
    },
    {
      "name": "vhidvz/language-identification",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/54132701?s=40&v=4",
      "owner": "vhidvz",
      "repo_name": "language-identification",
      "description": "Language identification microservice powered by the FastText language detection model",
      "homepage": "https://vhidvz.github.io/language-identification/",
      "language": "Python",
      "created_at": "2024-09-08T06:27:11Z",
      "updated_at": "2024-12-20T09:20:10Z",
      "topics": [
        "ai",
        "fastapi",
        "fasttext",
        "language-identification"
      ],
      "readme": "# Quick Start\n\nLanguage identification microservice powered by the FastText language detection model, providing fast and accurate recognition of over 170 languages.\n\n```sh\ngit clone git@github.com:vhidvz/language-identification.git\ncd language-identification && docker-compose up -d\n```\n\n**Docker Hub:**\n\n```sh\ndocker run -p 8000:8000 vhidvz/language-identification:latest\n```\n\nEndpoints are fully documented using OpenAPI Specification 3 (OAS3) at:\n\n- ReDoc: <http://localhost:8000/redoc>\n- Swagger: <http://localhost:8000/docs>\n\n## Documentation\n\nTo generate the documentation for the python model, execute the following command:\n\n```sh\npdoc --output-dir docs model.py\n```\n"
    },
    {
      "name": "useless-ai-tinfoil-hats/sf-conspiracies",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/180780952?s=40&v=4",
      "owner": "useless-ai-tinfoil-hats",
      "repo_name": "sf-conspiracies",
      "description": null,
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2024-09-07T18:24:34Z",
      "updated_at": "2024-09-08T19:16:42Z",
      "topics": [],
      "readme": "# SF Conspiracy Generator\n\n## Requirements\n\nTo run this app, you will need:\n\n- Python 3.x\n- Streamlit\n- `streamlit_navigation_bar` (for the navigation bar component)\n\n## Setting Up the Environment\n\nFollow these steps to set up a virtual environment and install the required packages:\n\n1. **Clone the repository**:\n2. **Navigate to the project directory**:\n3. **Add OpenAI API Key to .env file**:\n4. **Create a virtual environment**:\n\n    ```bash\n    python -m venv venv\n    ```\n\n5. **Activate the virtual environment**:\n\n    - **On Windows**:\n\n      ```bash\n      venv\\Scripts\\activate\n      ```\n\n    - **On macOS/Linux**:\n\n      ```bash\n      source venv/bin/activate\n      ```\n\n6s. **Install the required libraries**:\n\n    ```bash\n    pip install -r requirements.txt\n    ```\n\n\n## Running the App\n\n1. **Ensure the virtual environment is activated** (refer to the activation instructions above).\n\n2. **Run the Streamlit app**:\n\n    ```bash\n    streamlit run main.py\n    ```\n\n3. **Open your browser** and go to the URL provided in the terminal to view the app."
    },
    {
      "name": "Sanket12122003/RAG-With-Haystack-MistralAI-Pinecone",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/132454353?s=40&v=4",
      "owner": "Sanket12122003",
      "repo_name": "RAG-With-Haystack-MistralAI-Pinecone",
      "description": null,
      "homepage": null,
      "language": "HTML",
      "created_at": "2024-09-02T19:06:58Z",
      "updated_at": "2024-09-02T19:09:35Z",
      "topics": [],
      "readme": ""
    },
    {
      "name": "Sibgat-Ul/food_diet",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/61459751?s=40&v=4",
      "owner": "Sibgat-Ul",
      "repo_name": "food_diet",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2024-08-23T14:42:56Z",
      "updated_at": "2024-10-07T12:41:16Z",
      "topics": [],
      "readme": "### About:\n\nThis is a RAG application to generate diet plan. It searches the web to generate diet plans according to the prompt and needs of the user. \n\n<br>\nIt utilizes,\n<br>\n\n1. Microsoft Phi-3 model to generate the replies (quantized)\n2. Huggingface pipeline to infer\n3. Chroma-db vector store as retriever\n4. Mini LM embedding model \n5. Tavily search api to search the web\n\n### Install:\n\n```bash\npip install -r 'requirements.txt'\n```\n\n### To run:\n\n1. Create a .env file and add\n   1. TAVILY_API_KEY=your_tavily_api_key_(check their website, its free)\n\n2. Firstly start the fastapi server:\n```bash\nuvicorn main:app --reload --reload-exclude ./front_end/*\n```\n\n3. Start the streamlit server:\n```bash\nstreamlit run front_end/streamlit_ui.py\n```\n\n### Future work:\n\n1. Cloud based vector store\n2. SQL based Chat history retrieving \n3. Modify the whole structure so that any model can be used."
    },
    {
      "name": "dashinja/CalarmHelp",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/25598950?s=40&v=4",
      "owner": "dashinja",
      "repo_name": "CalarmHelp",
      "description": "Simplify Calendar Alarm Creation",
      "homepage": null,
      "language": "Python",
      "created_at": "2024-04-18T03:26:47Z",
      "updated_at": "2024-11-08T04:28:43Z",
      "topics": [],
      "readme": "# CalarmHelp\n\n## Why\n\nPerhaps you like to use google calendar to time block both events and tasks - however you find the 'notifications' from those calendar events to be lack luster. It just doesn't get your attention enough to be in your face, and thus actionable.\n<br><br>\nPerhaps then you'd like to supplement each calendar event with a separate alarm - which also means double work.\n<br><br>\nThen maybe you found an alarm application like (AMDroid on Android devices) which can look at selected calendars for tags of a given name (i.e. `#work` or `#home`) paired with a reminder syntax (i.e. `[10m]` or `[120m]`) and create an alarm to pair with the specified event.\n\nA calendar title nomenclature of say: `\"Take out the trash #home [10m]\"` and the event itself with a start time of 8:00AM will produce an alarm titled \"Take out the trash\" and be set for 7:50AM.\n\nCalarmHelp is a word jumble coming from `\"Calendar\"` + `\"Alarm\"` + `\"Help\"`. Calendar Alarm Help.\n\nCalarmHelp contains an API used to trigger `GPT4o` to translate normal human speech (or an API call) with content `\"At 8AM remind me to take out the trash 10 minutes early at home\"` into a JSON object which contains a structured title with a value such as\n`\"Take out the trash @ 8AM #home [10m]\"`.\n<br>\n<br>\n\nThis output (simplified in this readme) is passed as input to a google calendar API service, and calls the create calendar event method - and provides the needed meta data for that event's creation.\n<br>\n\nAfter the calendar event is created - the Android application (or application of your choice) can look at the calendar and create an alarm on your behalf based on the tags in that title + the syntax for lead time for a reminder of that alarm.\n\n## Setting up your project\n\n### Requirements\n- [Python >= 3.10](https://www.python.org/downloads/)\n- [Poetry](https://python-poetry.org/docs/)\n<br>\n\n### Optional Requirements\n- The following is required for working with docker and deploying to Google Cloud:\n  - [Docker](https://docs.docker.com/get-docker/)\n  - [Docker Compose](https://docs.docker.com/compose/install/)\n  - [gcloud CLI](https://cloud.google.com/sdk/docs/install)\n\n_________________\n### Installation\n\n1. Make sure you have Python and Poetry installed on your system. If not, follow the instructions in the links above.\n<br>\n\n2. Clone this repository to your local machine:\n\n```shell\ngit clone https://github.com/your-username/calarmHelp.git\n```\n<br>\n\n3. Navigate to the project directory:\n\n```shell\ncd calarmHelp\n```\n<br>\n\n4. Install the project dependencies using Poetry:\n\n```shell\npoetry install\n```\n<br>\n\n5. Create an environment file\n\n```shell\ntouch .env\n```\n<br>\n\n6. Add the following environment variables, with values, to the `.env` file.\n\n```.env\n  OPENAI_API_KEY=<Refer to OpenAI Docs>\n  ORIGINS=<A string which defines origins for CORS middleware>\n  CALENDAR_ID=<The calendar ID you want to use>\n  GOOGLE_APPLICATION_CREDENTIALS=<to the location of your google credentials file>\n```\n\nSee [How Application Default Credentials Work](https://cloud.google.com/docs/authentication/application-default-credentials)\n\nSee [OpenAI API Keys](https://platform.openai.com/docs/quickstart/step-2-set-up-your-api-key)\n\n### IMPORTANT: Accessing the Calendar with a Service Account\nFind your `CALENDAR_ID` by going to your Google Calendar, clicking on the three dots next to the calendar you want to use, and selecting `Settings and Sharing`. The `CALENDAR_ID` will be listed under `Integrate Calendar`.\n\nEnsure the calendar is shared with the service account by following these steps:\n\nIn the same Settings and sharing section, under \"Share with specific people or groups,\" add the service account's email.\nAssign the \"Make changes and manage sharing\" permissions.\n\nOtherwise, the service account will not be able to access the calendar. And this application will not be able to create events on your behalf.\n\n<br>\n\n_________________\n\n## Building and running your application\n### Locally on your machine\nThe advantage of running your application locally is that you can make changes to your application and see the changes immediately.\n\nWhen you're ready to run locally:\nEnsure you're in the root directory of your project and run the following commands:\n\n```\npoetry run start\n\n# alias for: `poetry run python calarmhelp/main.py`\n```\n\nYour application will be available at http://localhost:8000.\n_________________\n\n### Locally with Docker\nThe advantage of running your application in Docker is that you can ensure that your application runs the same way locally as it does in the cloud.\n\nWhen you're ready to run locally, start your application by running:\n`docker compose up --build`.\n\nYour application will be available at http://localhost:8000.\n_________________\n\n## Scripts\nThe commands are defined in the `tool.poetry.scripts` section of the `pyproject.toml` file for inclusion in the application. \n\nThe scripts themselves are defined in the `calarmhelp/scripts` directory.\n\n##### To create a new script:\n1. Create a new file (named after the script) in the `calarmhelp/scripts` directory.\n2. Add the script to the `tool.poetry.scripts` section of the `pyproject.toml` file.\n3. Run `poetry install` to install the new script.\n4. Run the script by running `poetry run <script-name>`.\n\n### Commands:\n\n#### Start:\nTo start your application, run:\n\n`poetry run start`\n\nFor Running locally with Docker [click here](README.Docker.md#locally-with-docker) or see the [Docker-Start](#Docker-Start) section below.\n<br>\n<br>\n\n#### Docker-Start:\nTo start your application with Docker, run:\n\n`poetry run docker-start`\n<br>\n<br>\n\n#### Docker-Build:\nTo build your application with Docker, run:\n\n`poetry run docker-build <image-tag>`\n<br>\n<br>\n\n#### Docker-Push:\nTo push your application with Docker, run:\n\n`poetry run docker-push <image-tag>`\n<br>\n<br>\n\n#### Deploy-App\nTo deploy your application to Google Cloud, run:\n\n`poetry run deploy-app <image-tag>`\n##### Important\nThis command is comprehensive and will build, push, and deploy your application to Google Cloud Run as well as update the traffic to the new revision.\n<br>\n<br>\n\n#### Linting:\nTo lint your code, run:\n\n`poetry run precommit`\n\nThe linting step will also happen automatically before you commit your code.\n\n_________________\n\n### References\n- [Run locally with Docker](README.Docker.md#locally-with-docker)\n- [Deploy to Docker to Google Cloud](README.Docker.md#deploying-to-google-cloud)\n"
    },
    {
      "name": "sebhoron/food-chatbot",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/92864754?s=40&v=4",
      "owner": "sebhoron",
      "repo_name": "food-chatbot",
      "description": "Personal assistant bot able to find recipes and weather details",
      "homepage": "",
      "language": "Python",
      "created_at": "2024-08-07T09:44:09Z",
      "updated_at": "2024-09-06T17:38:33Z",
      "topics": [
        "haystack",
        "python"
      ],
      "readme": "# Food Chatbot\n\n\n## Description\n\nThis project demonstrates a tooling capabilities for LLMs using the Haystack framework and Azure OpenAI. It showcases how to create a basic question-answering system.\n\n## Features\n\n- Weather checker and recipe finder\n- Custom prompt template\n- Integration with Azure OpenAI for text generation\n- Visualizes the pipeline structure\n\n## Prerequisites\n\n- [Git](https://git-scm.com/downloads)\n- [Python 3.10+](https://www.python.org/downloads/)\n- [Poetry](https://python-poetry.org/docs/)\n- An Azure OpenAI account with API access\n- A Spoonacular API key\n- Visual Crossing Weather API key\n\n## Installation\n\n1. Clone this repository:\n\n* `git clone https://github.com/sebhoron/food-chatbot.git`\n* `cd food-chatbot`\n\n2. Install the required packages:\n\n* `poetry shell`\n* `poetry install`\n\n3. Create a `.env` file in the project root and add your Azure OpenAI credentials:\n\n* `AZURE_OPENAI_KEY=your_api_key`\n* `AZURE_OPENAI_INSTANCE_NAME=your_instance_name`\n* `AZURE_OPENAI_ENDPOINT0=your_instance_endpoint`\n* `AZURE_OPENAI_DEPLOYMENT_NAME=your_model_deployment_name`\n* `VISUAL_CROSSING_WEATHER_API_KEY=your_weather_api_key`\n* `SPOONACULAR_API_KEY=your_food_api_key`\n\n## Usage\n\nRun the main script:\n`poetry run python -m food-chatbot`\n\n## License\n\n[MIT License](LICENSE)\n\n## Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.\n\n## Acknowledgements\n\nThis project uses the [Haystack](https://github.com/deepset-ai/haystack) framework by deepset.\n"
    },
    {
      "name": "nicolorosso/ensemble_llms",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/98038058?s=40&v=4",
      "owner": "nicolorosso",
      "repo_name": "ensemble_llms",
      "description": null,
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2024-08-25T15:10:12Z",
      "updated_at": "2024-09-29T14:53:01Z",
      "topics": [],
      "readme": "# Ensemble LLMs for data labelling\nOllama is used to install and manage the LLM models, while Haystack provides the framework for building the RAG pipeline. The two are separate but complementary: first, you'll install the LLM models via Ollama, and then use them through Haystack's pipeline structure. All required Python libraries are listed in the requirements.txt file, except for Ollama itself and the LLM models, which need to be installed separately as per the instructions below\n\n## Ollama Set-up\n\n[Ollama](https://ollama.ai/) can be installed on Windows. Alternatively, it can be installed on Linux with:\n```bash\ncurl -fsSL https://ollama.com/install.sh | sh\n```\n\n## Haystack and Ollama set up\n1. Install the haystack and Ollama-haystack framework necessary to run the RAG pipeline.\n\n```python\npip install haystack-ai ollama-haystack\n```\n2. Pull the necessary LLMs model from Ollama\n```python\nollama pull llama3.1:8b\nollama pull mistral\nollama pull phi3:mini\nollama pull gemma2\n```\n## Data Preparation\n\nThe following CSV files are the one used: \n- `scientific_papers.csv`: Contains the (reduced) scientific papers for context\n- `antiscience-withlanguage-all-tweets.csv`: Contains tweets for classification (this one has been sent via e-mail)\n\n## Configuration\n\nIn `main.py`, you can adjust the `sample_size` variable to set the number of tweets to be classified. Ideally, this would set between 50000 and 7000\n\n## Running the Script\n\nExecute the main script:\n\nThe script will process the specified number of tweets and output the classification results. Results will also be saved to `classification_results.csv`.\n\n## Output\n\nThe script will print classification results for each tweet, including:\n- Original tweet\n- Preprocessed tweet\n- Final classification\n- Confidence score\n- Individual model results\n\nA summary of execution time will also be displayed. \n\n"
    },
    {
      "name": "WaqasAli-Munawar/Building-AI-Applications-With-Haystack",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/48343525?s=40&v=4",
      "owner": "WaqasAli-Munawar",
      "repo_name": "Building-AI-Applications-With-Haystack",
      "description": null,
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2024-08-22T10:19:15Z",
      "updated_at": "2024-08-22T13:23:43Z",
      "topics": [],
      "readme": ""
    },
    {
      "name": "mobiusml/aana_chat_with_video",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/34684912?s=40&v=4",
      "owner": "mobiusml",
      "repo_name": "aana_chat_with_video",
      "description": "A multimodal chat application that allows users to upload a video and ask questions about the video content based on the visual and audio information.",
      "homepage": "",
      "language": "Python",
      "created_at": "2024-07-09T12:07:04Z",
      "updated_at": "2024-12-06T12:59:52Z",
      "topics": [],
      "readme": "# Chat with Video App\n\n**Chat with Video App** is a multimodal chat application that allows users to upload a video and ask questions about the video content based on the visual and audio information. See [Chat with Video Demo notebook](notebooks/chat_with_video_demo.ipynb) for more information.\n\n## Installation\n\nTo install the project, follow these steps:\n\n1. Clone the repository.\n\n2. Install additional libraries.\n\nFor optimal performance, you should also install [PyTorch](https://pytorch.org/get-started/locally/) version >=2.1 appropriate for your system. You can continue directly to the next step, but it will install a default version that may not make optimal use of your system's resources, for example, a GPU or even some SIMD operations. Therefore we recommend choosing your PyTorch package carefully and installing it manually.\n\nSome models use Flash Attention. Install Flash Attention library for better performance. See [flash attention installation instructions](https://github.com/Dao-AILab/flash-attention?tab=readme-ov-file#installation-and-features) for more details and supported GPUs.\n\n3. Install the package with poetry.\n\nThe project is managed with [Poetry](https://python-poetry.org/docs/). See the [Poetry installation instructions](https://python-poetry.org/docs/#installation) on how to install it on your system.\n\nIt will install the package and all dependencies in a virtual environment.\n\n```bash\npoetry install\n```\n\n4. Run the app.\n\n```bash\nCUDA_VISIBLE_DEVICES=\"0\" aana deploy aana_chat_with_video.app:aana_app\n```\n\n## Usage\n\nTo use the project, follow these steps:\n\n1. Run the app as described in the installation section.\n\n```bash\nCUDA_VISIBLE_DEVICES=\"0\" aana deploy aana_chat_with_video.app:aana_app\n```\n\nOnce the application is running, you will see the message `Deployed successfully.` in the logs. It will also show the URL for the API documentation.\n\n> **⚠️ Warning**\n>\n> The applications require 1 large GPUs to run. GPU should have at least 48GB of memory.\n>\n> The applications will detect the available GPU automatically but you need to make sure that `CUDA_VISIBLE_DEVICES` is set correctly.\n> \n> Sometimes `CUDA_VISIBLE_DEVICES` is set to an empty string and the application will not be able to detect the GPU. Use `unset CUDA_VISIBLE_DEVICES` to unset the variable.\n> \n> You can also set the `CUDA_VISIBLE_DEVICES` environment variable to the GPU index you want to use: `export CUDA_VISIBLE_DEVICES=0`.\n\n2. Send a POST request to the app.\n\nSee [Chat with Video Demo notebook](notebooks/chat_with_video_demo.ipynb) for more information.\n\n## Running with Docker\n\nWe provide a docker-compose configuration to run the application in a Docker container.\n\nRequirements:\n\n- Docker Engine >= 26.1.0\n- Docker Compose >= 1.29.2\n- NVIDIA Driver >= 525.60.13\n\nTo run the application, simply run the following command:\n\n```bash\ndocker-compose up\n```\n\nThe application will be accessible at `http://localhost:8000` on the host server.\n\n\n> **⚠️ Warning**\n>\n> The applications require 1 GPUs to run.\n>\n> The applications will detect the available GPU automatically but you need to make sure that `CUDA_VISIBLE_DEVICES` is set correctly.\n> \n> Sometimes `CUDA_VISIBLE_DEVICES` is set to an empty string and the application will not be able to detect the GPU. Use `unset CUDA_VISIBLE_DEVICES` to unset the variable.\n> \n> You can also set the `CUDA_VISIBLE_DEVICES` environment variable to the GPU index you want to use: `CUDA_VISIBLE_DEVICES=0 docker-compose up`.\n\n\n> **💡Tip**\n>\n> Some models use Flash Attention for better performance. You can set the build argument `INSTALL_FLASH_ATTENTION` to `true` to install Flash Attention. \n>\n> ```bash\n> INSTALL_FLASH_ATTENTION=true docker-compose build\n> ```\n>\n> After building the image, you can use `docker-compose up` command to run the application.\n>\n> You can also set the `INSTALL_FLASH_ATTENTION` environment variable to `true` in the `docker-compose.yaml` file.\n"
    },
    {
      "name": "mobiusml/aana_summarize_video",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/34684912?s=40&v=4",
      "owner": "mobiusml",
      "repo_name": "aana_summarize_video",
      "description": "An Aana application that summarizes a video by extracting transcription from the audio and generating a summary using a Language Model (LLM).",
      "homepage": null,
      "language": "Python",
      "created_at": "2024-07-30T11:33:30Z",
      "updated_at": "2024-08-19T08:28:48Z",
      "topics": [],
      "readme": "# Summarize Video App\n\n**Summarize Video App** is an Aana application that summarizes a video by extracting transcription from the audio and generating a summary using a Language Model (LLM). This application is a part of the [tutorial](https://mobiusml.github.io/aana_sdk/pages/tutorial/) on how to build multimodal applications with [Aana SDK](https://github.com/mobiusml/aana_sdk).\n\n## Installation\n\nTo install the project, follow these steps:\n\n1. Clone the repository.\n\n2. Install the package with Poetry.\n\nThe project is managed with [Poetry](https://python-poetry.org/docs/). See the [Poetry installation instructions](https://python-poetry.org/docs/#installation) on how to install it on your system.\n\nIt will install the package and all dependencies in a virtual environment.\n\n```bash\npoetry install\n```\n\n3. Install additional libraries.\n\nFor optimal performance, you should also install [PyTorch](https://pytorch.org/get-started/locally/) version >=2.1 appropriate for your system. You can continue directly to the next step, but it will install a default version that may not make optimal use of your system's resources, for example, a GPU or even some SIMD operations. Therefore we recommend choosing your PyTorch package carefully and installing it manually.\n\nSome models use Flash Attention. Install Flash Attention library for better performance. See [flash attention installation instructions](https://github.com/Dao-AILab/flash-attention?tab=readme-ov-file#installation-and-features) for more details and supported GPUs.\n\n\n4. Activate the Poetry environment.\n\nTo activate the Poetry environment, run the following command:\n\n```bash\npoetry shell\n```\n\nAlternatively, you can run commands in the Poetry environment by prefixing them with `poetry run`. For example:\n\n```bash\npoetry run aana deploy aana_summarize_video.app:aana_app\n```\n\n5. Run the app.\n\n```bash\naana deploy aana_summarize_video.app:aana_app\n```\n\n## Usage\n\nTo use the project, follow these steps:\n\n1. Run the app as described in the installation section.\n\n```bash\naana deploy aana_summarize_video.app:aana_app\n```\n\nOnce the application is running, you will see the message `Deployed successfully.` in the logs. It will also show the URL for the API documentation.\n\n> **⚠️ Warning**\n>\n> The applications require 1 GPUs to run.\n>\n> The applications will detect the available GPU automatically but you need to make sure that `CUDA_VISIBLE_DEVICES` is set correctly.\n> \n> Sometimes `CUDA_VISIBLE_DEVICES` is set to an empty string and the application will not be able to detect the GPU. Use `unset CUDA_VISIBLE_DEVICES` to unset the variable.\n> \n> You can also set the `CUDA_VISIBLE_DEVICES` environment variable to the GPU index you want to use: `export CUDA_VISIBLE_DEVICES=0`.\n\n2. Send a POST request to the app.\n\n```bash\ncurl -X POST http://127.0.0.1:8000/video/summarize -Fbody='{\"video\":{\"url\":\"https://www.youtube.com/watch?v=VhJFyyukAzA\"}}'\n```\n\nSee [Tutorial](https://mobiusml.github.io/aana_sdk/pages/tutorial/) for more information.\n\n## Running with Docker\n\nWe provide a docker-compose configuration to run the application in a Docker container.\n\nRequirements:\n\n- Docker Engine >= 26.1.0\n- Docker Compose >= 1.29.2\n- NVIDIA Driver >= 525.60.13\n\nTo run the application, simply run the following command:\n\n```bash\ndocker-compose up\n```\n\nThe application will be accessible at `http://localhost:8000` on the host server.\n\n\n> **⚠️ Warning**\n>\n> The applications require 1 GPUs to run.\n>\n> The applications will detect the available GPU automatically but you need to make sure that `CUDA_VISIBLE_DEVICES` is set correctly.\n> \n> Sometimes `CUDA_VISIBLE_DEVICES` is set to an empty string and the application will not be able to detect the GPU. Use `unset CUDA_VISIBLE_DEVICES` to unset the variable.\n> \n> You can also set the `CUDA_VISIBLE_DEVICES` environment variable to the GPU index you want to use: `CUDA_VISIBLE_DEVICES=0 docker-compose up`.\n\n\n> **💡Tip**\n>\n> Some models use Flash Attention for better performance. You can set the build argument `INSTALL_FLASH_ATTENTION` to `true` to install Flash Attention. \n>\n> ```bash\n> INSTALL_FLASH_ATTENTION=true docker-compose build\n> ```\n>\n> After building the image, you can use `docker-compose up` command to run the application.\n>\n> You can also set the `INSTALL_FLASH_ATTENTION` environment variable to `true` in the `docker-compose.yaml` file.\n"
    },
    {
      "name": "fpreiss/talk_rag_scheduler",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/17441607?s=40&v=4",
      "owner": "fpreiss",
      "repo_name": "talk_rag_scheduler",
      "description": "Code that accompanies the talk \"Mastering RAG: Techniques for Improved Retrieval and Generation\" first held at 8. Bergisches Entwicklerforum",
      "homepage": null,
      "language": "Python",
      "created_at": "2024-08-08T10:04:13Z",
      "updated_at": "2024-08-08T10:46:57Z",
      "topics": [],
      "readme": "# Mastering RAG: Techniques for Improved Retrieval and Generation\n\nCode that accompanies the talk \"Mastering RAG: Techniques for Improved Retrieval and Generation\" first\nheld at\n[Bergisches Entwicklerforum](https://www.meetup.com/bergisches-entwicklerforum/events/300781375).\nThis repository showcases steps involved in building a RAG pipeline for a QA system on the\n[python scheduler library by Digon.IO](https://github.com/digonio/scheduler).\n\nThe components covered for the RAG pipeline are build upon the\n[haystack](https://github.com/deepset-ai/haystack) framework and\n[ollama](https://github.com/ollama/ollama) backend. By default the 4bit quantized version of\ngemma2:2b is used, therefore the hardware requirements are minimal and no external API keys\nare needed.\n\n## Structure\n\nThe implementation of the individual components can be found in `src/talk_rag_scheduler`.\nMost of the files can be run as a script and offer command line arguments for configuration.\nGet information about the available run-time parameters with:\n\n```bash\npython -m talk_rag_scheduler.indexing_pipeline --help\npython -m talk_rag_scheduler.retrieval_pipeline --help\npython -m talk_rag_scheduler.rag_pipeline --help\npython -m talk_rag_scheduler.rag_pipeline_concurrent --help\npython -m talk_rag_scheduler.bench_retrieval --help\n```\n\nThe individual steps come with additional jupyter notebooks.\nStart with [01_start_here.ipynb](notebooks/00_start_here.ipynb).\n\n## Installation\n\nThis project has only been tested with Python 3.12, it might work with 3.11 as well.\n\n```console\npip install -r requirements.txt\npip install -e .\n```\n\n* Make sure you have ollama running and listening as specified in `const.py`\n* You can get pretty formatted markdown output in the terminal with\n  [rich-cli](https://github.com/Textualize/rich-cli)\n  `pip install rich-cli`.\n\n  Try it out:\n\n  ```bash\n  python -m talk_rag_scheduler.rag_pipeline \"What are the features of the scheduler?\"  | rich - --markdown\n  ```\n\n## Topics for future work\n\nRAG system\n\n* Multi-turn chat (possibly using haystack's ChatMessage object and streaming callback)\n* Document preprocessing\n* persistent databases & indexes\n\nCoding\n\n* Use environment variables for configuration instead of hardcoded `const.py`\n"
    },
    {
      "name": "mayankkapoor/ecom-agent-with-llama3",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/1268053?s=40&v=4",
      "owner": "mayankkapoor",
      "repo_name": "ecom-agent-with-llama3",
      "description": "AI shopping agent which researches products and compares prices",
      "homepage": "",
      "language": "Python",
      "created_at": "2024-08-04T07:43:11Z",
      "updated_at": "2024-08-24T11:35:37Z",
      "topics": [],
      "readme": "# AI Purchase Assistant\n\nWelcome to the AI Purchase Assistant project! This application leverages a Llama 3 powered Retrieval-Augmented Generation (RAG) pipeline to create an AI-powered purchase assistant that helps users find products on Amazon and suggests the most cost-friendly options.\n\nThe application performs several steps to achieve its functionality:\n\n1. **Data Indexing**: Reads a CSV file containing Amazon product data and indexes it in an in-memory vector database.\n2. **User Query Analysis**: Analyzes user queries to identify products of interest.\nProduct Retrieval: Retrieves similar products from the indexed database based on the user's query.\n3. **Tool Functions**: Provides functionalities such as identifying products and finding budget-friendly options.\n4. **Chat Interface**: Uses a chat interface with function calling capabilities to interact with the user and provide recommendations.\n\n## Table of Contents\n- [Overview](#overview)\n- [Features](#features)\n- [Installation](#installation)\n- [Usage](#usage)\n- [Configuration](#configuration)\n- [Components](#components)\n- [License](#license)\n\n## Overview\n\nThe AI Purchase Assistant uses Large language models and machine learning techniques to understand user queries, search for relevant products, and provide budget-friendly recommendations. The application is built using Python, Gradio, and the Haystack framework, and it integrates with Llama 3 language models for generating responses.\n\n## Features\n\n- **Product Search**: Understands user queries and identifies relevant products.\n- **Price Comparison**: Retrieves similar products and compares their prices.\n- **Budget-Friendly Suggestions**: Finds and suggests the most cost-effective products.\n- **Interactive Chat Interface**: Users can interact with the assistant via a web-based chat interface powered by Gradio.\n\n## Installation\n\n### Prerequisites\n\n- Python 3.12 or higher\n- Docker (for containerized development)\n- Access to Groq Llama 3 model\n\n### Steps\n\n1. Clone the repository:\n    ```bash\n    git clone https://github.com/mayankkapoor/ecom-agent-with-llama3.git\n    cd ecom-agent-with-llama3\n    ```\n2. Set up a virtual environment:\n    ```bash\n    python -m venv venv\n    source venv/bin/activate   # On Windows: venv\\Scripts\\activate\n    pip install -r requirements.txt\n    ```\n3. Create a .env file in the project root directory and add your OpenAI API key and model \n    ```bash\n    GROQ_API_KEY=your_groq_api_key\n    HF_API_TOKEN=your_hugging_face_api_token\n    GROQ_LLM_MODEL=llama3-groq-70b-8192-tool-use-preview\n    ```\n4. Run the application:\n    ```bash\n    python main.py\n    ```\n5. Access the web interface:\n    Open your web browser and go to http://localhost:7860 to interact with the AI Purchase Assistant.\n\n\n# Detailed explanations\n## Data Indexing:\n\nThe application reads a CSV file into a Pandas DataFrame.\nIt creates an in-memory document store and indexes the documents using the SentenceTransformersDocumentEmbedder.\nThis step ensures that the product data is transformed into embeddings that can be efficiently searched.\n\n## User Query Analysis:\n\nA product_identifier pipeline is created to analyze user queries.\nThe pipeline uses a PromptBuilder to generate a prompt and an OpenAIGenerator to process the prompt.\nThe template ensures the returned result is a Python list of products.\n\n## Product Retrieval:\n\nA RAG pipeline is created to retrieve similar products from the vector database.\nThe pipeline uses SentenceTransformersTextEmbedder to generate query embeddings and InMemoryEmbeddingRetriever to find similar documents.\nA PromptBuilder formats the retrieved documents and a second OpenAIGenerator generates the final response.\n\n## Tool Functions:\n\nTwo primary tool functions are defined:\nproduct_identifier_func(query: str): Identifies products from the user's query and retrieves similar products.\nfind_budget_friendly_option(selected_product_details): Finds the most cost-friendly option from a list of products.\n\n## Chat Interface:\n\nThe chat interface is built using the gradio library.\nThe chat template includes XML tags for tool calls, allowing the AI to call predefined functions to process user queries.\nThe chatbot_with_fc function handles the conversation, checking for tool calls and executing the corresponding functions.\nThe extract_tool_calls function parses tool call information from the AI's responses.\nThe chatbot_interface function integrates the chatbot logic with Gradio's interface elements.\n\n# Key Components and Their Roles\n\n## Gradio for User Interface:\n\ngradio provides a web-based interface for interacting with the chatbot.\nIt allows users to input queries and view responses in a user-friendly manner.\n\n## Haystack for Pipelines:\n\nhaystack is used to create and manage the various pipelines.\nIt provides components for embedding, retrieval, and generation tasks.\n\n## Groq Llama 3 Language Model:\n\nOpenAIGenerator and OpenAIChatGenerator utilize Groq's Llama models to generate responses and handle chat interactions.\n\n## Environment Variables:\n\nEnvironment variables are managed using dotenv to keep sensitive information secure and configurable.\n\n## Error Handling and Data Parsing:\n\nThe use of literal_eval and JSON parsing ensures that responses are correctly interpreted and handled.\n\n# Future Roadmap & Improvements\n\n## Error Handling:\n\nEnhancing error handling to cover more edge cases and provide informative error messages.\nFor example, logging errors for debugging and improve user feedback in case of failures.\n\n## Scalability:\n\nScaling the document store and retrieval components to handle larger datasets and more simultaneous queries.\nUsing a more scalable document store like Elasticsearch for production environments.\n\n## Performance Optimization:\n\nOptimizing the embedding and retrieval steps to reduce latency.\nCaching frequently retrieved results can improve response times.\n\n## User Experience:\n\nImproving the UI/UX of the Chat interface to make it more intuitive and visually appealing.\nAdding features like conversational history, context retention, and more interactive elements.\n\n## Security:\n\nEnsuring that the application is secure, especially when handling user inputs and API keys.\nImplementing rate limiting and input validation to prevent abuse and injection attacks.\n\n# Conclusion\nThis RAG pipeline-based LLM agentic application combines various components to create a sophisticated AI-powered purchase assistant. By leveraging document embedding, retrieval, and generation, it provides users with relevant product recommendations and budget-friendly options. The integration with Gradio ensures a smooth and interactive user experience, while the use of environment variables and structured templates maintains flexibility and security.\n\n# License\nThis project is licensed under the MIT License. See the LICENSE file for details.\n"
    },
    {
      "name": "hiimbach/Financial-Statement-Reader",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/90375638?s=40&v=4",
      "owner": "hiimbach",
      "repo_name": "Financial-Statement-Reader",
      "description": "Financial Statement Chatbot is a Retrieval-Augmented Generation (RAG) system designed to interact with financial statement data. This project leverages OCR, summarization, and embedding techniques to enable users to query and retrieve information from financial statements effectively.",
      "homepage": "",
      "language": "Python",
      "created_at": "2024-07-19T11:21:36Z",
      "updated_at": "2024-08-20T09:18:12Z",
      "topics": [],
      "readme": "# Financial Statement Reader\nFinancial Statement Chatbot is a Retrieval-Augmented Generation (RAG) system designed to interact with financial \nstatement data. This project leverages OCR, summarization, and embedding techniques to enable users to query and \nretrieve information from financial statements effectively.\n\n## About the Project\nKey features include:\n\n- Converting PDF OCR files to JSON using Tesseract.\n- Summarizing PDFs for efficient data retrieval.\n- Utilizing sentence-transformers/all-MiniLM-L6-v2 as an embedder for RAG.\n- A user-friendly interface built with Streamlit.\n\n## Quick Start with Docker\nRun the following command to build the Docker image:\n```\ndocker build -t finread . \n```\n\nRun the following command to start the Docker container:\n```\ndocker run -p 8501:8501 finread\n```\n\n## Installation\nRun the following commands to install the required packages:\n\n### Install Tesseract in Brew with Vietnamese language\n```\nbrew install tesseract\nbrew install tesseract-lang\n```\n\n### Install Poppler in Brew\n```\nbrew install poppler\n```\n\n### Install Python packages\n```\npip install -r requirements.txt\n```\n\n## Usage\nRun the following command to start the Streamlit app:\n```\nstreamlit run app.py\n```\nIn the app you can choose Demo if you dont have a financial statement file to upload.\n\n## Contact \nIf you have any questions, feel free to reach out to me at: \n- Email: lenhobach@gmail.com\n- GitHub: hiimbach\n"
    },
    {
      "name": "oscar066/FeynmanAI",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/71583548?s=40&v=4",
      "owner": "oscar066",
      "repo_name": "FeynmanAI",
      "description": null,
      "homepage": "https://pypi.org/project/FeynmanAI/",
      "language": "Jupyter Notebook",
      "created_at": "2024-07-03T13:50:13Z",
      "updated_at": "2024-12-23T13:45:09Z",
      "topics": [],
      "readme": "# FeynmanAI\n\nFeynmanAI is a powerful command-line tool that leverages the Retrieval-Augmented Generation (RAG) model to interact with local documents. Whether you want to query your documents for specific information or quiz yourself on the contents, FeynmanAi offers an intuitive interface to get answers and enhance your understanding.\n\n## Features\n\n- **Interactive Query Loop**: Ask questions and get answers based on your documents.\n- **Quiz Mode**: Test your knowledge with random questions from your documents.\n- **Multi-format Support**: Works with PDF, TXT, PPT, and DOCX files.\n- **Additional Options**: Clear the screen after each query or enable text-to-speech for answers.\n\n## Installation\n\nYou can install FeynmanAi using pip:\n\n```bash\npip install FeynmanAI\n```\n\n## Usage\n\nThe `FeynmanAI` package provides a command-line interface (CLI) that you can use to interact with the RAG model and your Local documents.\n\n### Interactive Query Loop\n\nTo start the interactive query loop, run the following command:\n\nYou can use either `-d` or `--document` flag:\n\n```bash\nFeynmanAI -d /path/to/your/document.pdf \n```\n\n#### Example :\n\nThe package includes sample documents to get you started :\n\n```bash\nFeynmanAI -d Sample_documents/Distributed.pdf\n```\nThis command will load the specified document into the model's knowledge base and initiate the interactive query loop. In this mode, you can ask questions and receive answers derived from the loaded documents. Available commands include:\n\n- `Enter your question (or 'quit' to exit, 'clear' to clear screen):`: Type your question, and the tool will retrieve an answer from the RAG model.\n- `'quit'`: Exit the interactive query loop.\n- `'clear'`: Clear the screen and continue the interactive session.\n\n### Quiz Mode\n\nYou can also enter quiz mode by using the `-qz` or `--quiz` flag:\n\nIn quiz mode, the tool will randomly select a topic from your documents and ask you a question related to that topic. You can type your answer, and the tool will provide feedback.\n\nTo exit quiz mode, type `'quit'`.\n\n#### Example \n\n```bash \nFeynmanAI -d Sample_documents/Distributed.pdf -qz\n```\n\n### Other Options\n\nThe `FeynmanAI` package also provides the following additional options:\n\n- `-cls` or `--clear`: Clear the screen after each query in the interactive mode.\n- `-rd` or `--read`: Read the answer out loud (requires text-to-speech capabilities on your system).\n\n## Troubleshooting\n\nIf you encounter any issues, please check the error messages and make sure you have the required dependencies installed. If the problem persists, feel free to open an issue on the project's GitHub repository.\n\n## Contributing\n\nIf you would like to contribute to the FeynmanAi project, please follow these steps:\n\n1. Fork the repository.\n2. Create a new branch for your feature or bug fix.\n3. Make your changes and commit them.\n4. Push your changes to your fork.\n5. Submit a pull request.\n\nPlease make sure to write tests for any new functionality you add, and ensure that all existing tests pass before submitting your pull request.\n\n## License\n\nFeynmanAi is released under the [MIT License](LICENSE)."
    },
    {
      "name": "ChrisPappalardo/agent_fred",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/3485539?s=40&v=4",
      "owner": "ChrisPappalardo",
      "repo_name": "agent_fred",
      "description": "LLM agent interface for Federal Reserve Economic Data",
      "homepage": null,
      "language": "Python",
      "created_at": "2024-05-31T18:01:39Z",
      "updated_at": "2024-06-17T22:07:27Z",
      "topics": [],
      "readme": "# Agent FRED\n\n**Documentation**: [https://ChrisPappalardo.github.io/agent_fred](https://ChrisPappalardo.github.io/agent_fred)\n\n**Source Code**: [https://github.com/ChrisPappalardo/agent_fred](https://github.com/ChrisPappalardo/agent_fred)\n\n---\n\n[![source: https://fred.stlouisfed.org/](docs/fred.png)](https://fred.stlouisfed.org/)\n\nAgentic LLM interface for [Federal Reserve Economic Data](https://fred.stlouisfed.org/) (\"FRED\") using a locally-hosted open-source Large Language Model.\n\nFRED is hosted and maintained by the [St. Louis Federal Reserve Bank](https://www.stlouisfed.org/).  Please see the [FRED Terms of Use](https://fred.stlouisfed.org/legal/#contact) for permissible use, copyright and other legal restrictions, and additional information.\n\n## Prerequisites\n\nThis project assumes you have Linux or Windows running WSL2.  I have not tested it with macOS.  You do <u>not</u> need a GPU to run this application.\n\nThis python app requires ollama to host and serve LLM API. Select your platform and follow the [ollama installation directions](https://ollama.com/download).\n\nYou will also need to download whatever model(s) you intend to use. The default is [Microsoft's 3B phi3 LLM](https://ollama.com/library/phi3) for both the agent and chat pipelines. After installing ollama you can install phi3 with:\n\n```sh\nollama pull phi3\n```\n\nYou will need [Poetry](https://python-poetry.org/) and at least python 3.10.\n\n## Installation\n\nClone this repository:\n\n```sh\ngit clone https://github.com/ChrisPappalardo/agent_fred.git\n```\n\nCreate a poetry shell inside the directory and install the requirements:\n\n```sh\ncd agent_fred\npoetry shell\npoetry install\n```\n\n## Quickstart\n\n### Configuration\n\nThis python app uses environment variables to configure the AI backend and other variables. See `src/agent_fred/config.py`.\n\nIf you use the phi3 LLM you don't need to set anything but the `FRED_API_KEY`, which you can obtain [here](https://fred.stlouisfed.org/docs/api/api_key.html):\n\n```sh\nexport FRED_API_KEY=1234\n```\n\nYou can also set your config environment in `.env` in the project root:\n\n```sh\nFRED_API_KEY=123\n```\n\nThe configurable options are as follows:\n- `FRED_API_KEY` API key to use when making calls to the FRED\n- `FRED_LLM` LLM to use with Agent FRED (default=`phi3`)\n- `FRED_TEMPERATURE` verbosity of the model (default=`0.0`)\n\n### Run\n\nAgent FRED has both a command-line interface and a web application.\n\nTo use the web application (recommended):\n\n```sh\npython app.py\n```\n\nNavigate to [http://127.0.0.1:7860](http://127.0.0.1:7860) to view the app:\n\n![web app](docs/app.png)\n\nYou can also use the CLI, which returns API results in JSON:\n\n```sh\npython src/agent_fred/chat.py\n```\n\n![cli](docs/cli.png)\n\n### Usage\n\nIt typically helps to have an understanding of the [data available](https://fred.stlouisfed.org/docs/api/fred/series_search.html) from the FRED API.\n\nAn example script you can use to test the application:\n\n- `What data is available from the FRED?`\n- `What is GDP and how is it calculated?`\n- `What was GDP from 2000-01-01 through 2023-12-31?`\n- `Why did it dip in 2020?`\n\n![web app demo](docs/web.gif)\n\nIn the web app you can clear the LLM's memory and start a new Q&A session with the `Clear` button.\n\nEnjoy!\n\n## Testing\n\nIf you're developing with Agent FRED, you can run the unit tests using the `Makefile` like so:\n\n```sh\nmake test\n```\n\nYou can run the linters using pre-commit:\n\n```sh\nmake pre-commit\n```\n\n## Documentation\n\nThe documentation is automatically generated from the content of the [docs directory](https://github.com/ChrisPappalardo/agent_fred/tree/master/docs) and from the docstrings\n of the public signatures of the source code. The documentation is updated and published as a [Github Pages page](https://pages.github.com/) automatically as part each release.\n\n---\n"
    },
    {
      "name": "sunithalv/MedicalQA_Haystack_Pubmed",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/28974154?s=40&v=4",
      "owner": "sunithalv",
      "repo_name": "MedicalQA_Haystack_Pubmed",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2024-07-10T14:04:56Z",
      "updated_at": "2025-04-02T16:32:45Z",
      "topics": [],
      "readme": "# Healthcare-Chatbot\nHealthcare Chatbot using augmented pubmed which is a biomedical literature search engine.The LLM used is Mixtral-8X7B which is a MOE (Mixture of Experts) model.The framework used is haystack since it has a pubmed fetcher . Gradio is used as UI\n\n![Screenshot 2024-07-10 181120](https://github.com/sunithalv/MedicalQA_Haystack_Pubmed/assets/28974154/826eda2b-e34d-4c3b-8efe-8a58e25b215f)\n\n\n\n"
    },
    {
      "name": "Morpheus-An/IoT-Agent",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/110734442?s=40&v=4",
      "owner": "Morpheus-An",
      "repo_name": "IoT-Agent",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2024-03-29T07:03:11Z",
      "updated_at": "2024-12-11T14:19:04Z",
      "topics": [],
      "readme": "```\nIoT-Agent/\n├── data/\n│   ├── ECG\n│   ├── IMU\n│   └── machine_detect\n├── common/\n│   ├── args.py\n│   ├── generate_prompt.py\n│   ├── model.py\n│   ├── read_data.py\n│   └── utils.py\n├── knowledge/\n│   ├── ecg_detection/\n│   |   ├── demo-knowledge/\n│   |   └── domain-knowledge/\n│   ├── imu_HAR/\n│   |   ├── demo-knowledge/\n│   |   └── domain-knowledge/\n│   └── machine_detection/\n│       ├── demo-knowledge/\n│       └── domain-knowledge/\n├── models/\n|   ├── LLaMa2-7b-32k/\n|   └── Mistral-7b-instruct-v0.3/\n├── results/\n├── imports.py\n├── requirements.txt\n├── openAI_API_key.py\n├── main.py\n├── run.sh\n└── README.md\n```\n\n"
    },
    {
      "name": "Rusteam/vlm-rag-chat",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/22130831?s=40&v=4",
      "owner": "Rusteam",
      "repo_name": "vlm-rag-chat",
      "description": "Chat with your own internal data using vision-language models",
      "homepage": null,
      "language": "Python",
      "created_at": "2024-06-27T10:03:58Z",
      "updated_at": "2024-07-01T07:38:06Z",
      "topics": [],
      "readme": "# VLM-RAG app\n\nThe goal of this project is to create a quick deployment of\nRAG pipelines for textual and visual data.\n\n## Features\n\n- \\[x\\] Index files for chatting\n- \\[x\\] Vector database for persistence\n- \\[x\\] Answer user questions with a LLM\n- \\[x\\] REST api\n- \\[ \\] Reference original documents in the answers\n- \\[ \\] Guardrails to prevent inappropriate and irrelevant answers\n- \\[ \\] Webpage\n- \\[ \\] Docker-compose to run all services\n\n## Usage (dev version)\n\n### Installation\n\n1. Install [ poetry ](https://python-poetry.org/docs/basic-usage/) and use system env (in case using conda or other):\n\n```\npoetry env use system\n```\n\n2. Install torch and torchvision with conda (or other):\n\n```\nconda install pytorch torchvision -c pytorch\n```\n\n3. Install dependencies:\n\n```\npoetry install\n```\n\n4. Install [ollama](ollama.ai) to run LLMs locally\n   by downloading the installer from their website.\n   Pull a model and test it:\n\n```\nollama run llama3:latest\n```\n\n### System overview\n\n- [Haystack](https://haystack.deepset.ai/tutorials/30_file_type_preprocessing_index_pipeline)\n  is used as the main backend for building indexing and RAG pipelines.\n- [Qdrant](https://qdrant.tech/documentation/guides/installation/)\n  is used as a vector database for storing chunk embeddings.\n- [Fire](https://google.github.io/python-fire/guide/) is the main entrypoint\n  to run commands from cli.\n- User inputs are validated with\n  [pydantic](https://docs.pydantic.dev/latest/concepts/models/).\n\nThe main pipelines are defined in the `__init__` file as following:\n\n```\nfrom .pipelines import RAG as rag\nfrom .pipelines import IndexingPipeline as index\n```\n\nThey can be invoked in the cli as following:\n\n```\npython main.py <pipeline_name> --pipeline-args <command> --command-args\n```\n\nTo find all arguments use `--help` argument of `fire`:\n\n### Running commands\n\nFollow the steps below to index documents and chat with them.\n\n#### Vector database\n\nFirst, we run a qdrant vector storage with docker in order\nto store document reference:\n\n```\ndocker compose up -d qdrant\n```\n\nNow qdrant's api is available at the 6333 port.\n\n#### Indexing documents\n\nOnce `qdrant` is running, we can index our documents from local filesystem.\nCurrently supported text formats are:\n\n- plain text\n- markdown\n- pdf\n\nIn order to index a folder with documents, run the following command:\n\n```\n❯ python main.py index \\\n        --store_params '{location:localhost,index:recipe_files}' \\\n        run --path=tests/data/recipe_files\n\nOutputs:\n5 documents have been written to 'localhost'\n```\n\nAs a result, we have new documents indexed to the qdrant vector db.\nLater on, we will be able to interact with these documents using an LLM.\n\nTo add new documents without overwriting existing documents,\nensure to skip recreating a collection by adding an extra parameter.\n\n```\n❯ python main.py index \\\n        --recreate-index False \\\n        --store_params '{location:localhost,index:recipe_files}' \\\n        run --path=tests/data/extra_files\n```\n\n#### Ask questions (RAG)\n\nOnce documents have been indexed into a qdrant collection,\nit is time to start chatting with them. This can be\naccomplished by invoking the RAG pipeline through cli:\n\n> NOTE: this step assumes `ollama` is running on the localhost.\n\n```\n❯ python main.py rag \\\n      --store_params '{location:localhost,index:recipe_files}' \\\n      run --query \"how do you make a vegan lasagna?\"\n```\n\nThe command is supposed to output the following:\n\n```\nBased on the given context, to make a vegan lasagna, follow these steps:\n\n1. Slice eggplants into 1/4 inch thick slices and rub both sides with salt.\n2. Let the eggplant slices sit in a colander for half an hour to draw out excess wa\nter.\n3. Roast the eggplant slices at 400°F (200°C) for about 20 minutes, or until they'r\ne soft and lightly browned.\n4. Meanwhile, make the pesto by blending together basil leaves, almond meal, nutrit\nional yeast, garlic powder, lemon juice, and salt to taste.\n5. Make the macadamia nut cheese by blending cooked spinach, steamed tofu, drained\nwater from the tofu, macadamia nuts until smooth, and adjusting seasonings with gar\nlic, lemon juice, and salt to taste.\n6. Assemble the lasagna by layering roasted eggplant, pesto, and vegan macadamia nu\nt cheese in a casserole dish. Top with additional cheese if desired (optional).\n7. Bake at 350°F (180°C) for about 25 minutes, or until the cheese is melted and bu\nbbly.\n8. Serve and enjoy!\n```\n\nContinue by asking different questions.\n\n#### Serialize pipelines\n\nIn order to re-use pipelines with the REST api,\nsave them as yaml files using the `export` command:\n\n```\n❯ python main.py index --store_params '{location:localhost,index:recipe_files}' export --write-path tests/data/pipelines/index_recipe_files.yaml\n\n❯ python main.py rag --store_params '{location:localhost,index:recipe_files}' export --write-path tests/data/pipelines/rag_recipe_files.yaml\n```\n\nThese commands will create two yaml files (one per each pipeline)\nat the specified filepath location. Any storage/llm/chunking parameters\ncan be passed on the cli command or updated inside a yaml file.\n\n#### REST api\n\nREST api is implemented using [hayhooks](https://github.com/deepset-ai/hayhooks),\nwhich is tightly integrated with `haystack`.\n\n> NOTE: use [my fork](https://github.com/Rusteam/hayhooks/tree/fix-typing)\n> of hayhooks until [this PR](https://github.com/deepset-ai/hayhooks/pull/31) is merged.\n\nRun a fastapi sever and load the exported pipelines:\n\n```\n❯ hayhooks run --pipelines-dir tests/data/pipelines                             ─╯\n\nOutputs:\nINFO:     Pipelines dir set to: tests/data/pipelines\nINFO:     Deployed pipeline: index_recipe_files\nINFO:     Deployed pipeline: rag_recipe_files\nINFO:     Started server process [44078]\nINFO:     Waiting for application startup.\nINFO:     Application startup complete.\nINFO:     Uvicorn running on http://localhost:1416 (Press CTRL+C to quit)\n```\n\nNavigate to [localhost swagger](http://localhost:1416/docs#/)\nand try out pipeline endpoints.\n\n## Usage (docker)\n\nTo run with docker, start services from the docker-compose file\n(this expects `ollama` running on localhost):\n\n```\ndocker compose up\n```\n\nNavigate to swagger and try invoking pipeline endpoints.\n"
    },
    {
      "name": "TuanaCelik/ai-dev-haystack",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/15802862?s=40&v=4",
      "owner": "TuanaCelik",
      "repo_name": "ai-dev-haystack",
      "description": "📓 Notebook and instructions for AI_dev livecoding session, Paris 2024",
      "homepage": "",
      "language": "Jupyter Notebook",
      "created_at": "2024-06-19T20:54:03Z",
      "updated_at": "2025-01-22T12:47:44Z",
      "topics": [],
      "readme": "# AI_dev Live Coding Session\n\nThis is an accompanying reposiyory to the 'Composing AI Applications as a Graph with Haystack' talk at AI_dev, Paris, 2024\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/TuanaCelik/ai-dev-haystack/blob/main/ai-dev-haystack.ipynb)\n\nLearn the basics about:\n- Approaching AI applications as a graph\n- Learn how to use and build with Haystack components\n- Create Haystack pipelines for:\n    - Indexing\n    - RAG\n    - Self reflection for entity extraction\n\n### To run the examples:\n- Create a `.env` file and add your `OPENAI_API_KEY`\n- `pip install -r requirements.txt`\n- Go ahead and play around with the code in `ai-dev-haystack.ipynb`\n"
    },
    {
      "name": "heysourin/RAG-App-Using-Haystack-MistralAI-Pinecone-FastAPI-End-to-End",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/98945276?s=40&v=4",
      "owner": "heysourin",
      "repo_name": "RAG-App-Using-Haystack-MistralAI-Pinecone-FastAPI-End-to-End",
      "description": "End-to-End-RAG-Application-Using-Haystack-MistralAI-Pinecone-FastAPI",
      "homepage": null,
      "language": "HTML",
      "created_at": "2024-06-24T11:49:14Z",
      "updated_at": "2025-04-05T20:00:09Z",
      "topics": [],
      "readme": "# heysourin-RAG-Application-Using-Haystack-MistralAI-Pinecone-FastAPI-End-to-End\n### Credits: https://www.youtube.com/watch?v=5topvo0a4uY&t=3040s (YT Channel: Sunny Savita)\n## UI:\n  - I am using a PDF (Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks) for this project\n  -   ![image](https://github.com/heysourin/RAG-Application-Using-Haystack-MistralAI-Pinecone-FastAPI-End-to-End/assets/98945276/517229e2-3dc2-4b35-94ff-b0de3872c693)\n"
    },
    {
      "name": "samuelw1w/ResumeBuilder",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/111531999?s=40&v=4",
      "owner": "samuelw1w",
      "repo_name": "ResumeBuilder",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2024-06-01T03:27:44Z",
      "updated_at": "2024-06-06T18:23:34Z",
      "topics": [],
      "readme": ""
    },
    {
      "name": "salakhovilia/coworker",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/47942484?s=40&v=4",
      "owner": "salakhovilia",
      "repo_name": "coworker",
      "description": null,
      "homepage": null,
      "language": "TypeScript",
      "created_at": "2024-05-29T16:46:38Z",
      "updated_at": "2025-01-08T16:03:47Z",
      "topics": [],
      "readme": "<p align=\"center\">\n  <a href=\"http://nestjs.com/\" target=\"blank\"><img src=\"https://nestjs.com/img/logo-small.svg\" width=\"200\" alt=\"Nest Logo\" /></a>\n</p>\n\n[circleci-image]: https://img.shields.io/circleci/build/github/nestjs/nest/master?token=abc123def456\n[circleci-url]: https://circleci.com/gh/nestjs/nest\n\n  <p align=\"center\">A progressive <a href=\"http://nodejs.org\" target=\"_blank\">Node.js</a> framework for building efficient and scalable server-side applications.</p>\n    <p align=\"center\">\n<a href=\"https://www.npmjs.com/~nestjscore\" target=\"_blank\"><img src=\"https://img.shields.io/npm/v/@nestjs/core.svg\" alt=\"NPM Version\" /></a>\n<a href=\"https://www.npmjs.com/~nestjscore\" target=\"_blank\"><img src=\"https://img.shields.io/npm/l/@nestjs/core.svg\" alt=\"Package License\" /></a>\n<a href=\"https://www.npmjs.com/~nestjscore\" target=\"_blank\"><img src=\"https://img.shields.io/npm/dm/@nestjs/common.svg\" alt=\"NPM Downloads\" /></a>\n<a href=\"https://circleci.com/gh/nestjs/nest\" target=\"_blank\"><img src=\"https://img.shields.io/circleci/build/github/nestjs/nest/master\" alt=\"CircleCI\" /></a>\n<a href=\"https://coveralls.io/github/nestjs/nest?branch=master\" target=\"_blank\"><img src=\"https://coveralls.io/repos/github/nestjs/nest/badge.svg?branch=master#9\" alt=\"Coverage\" /></a>\n<a href=\"https://discord.gg/G7Qnnhy\" target=\"_blank\"><img src=\"https://img.shields.io/badge/discord-online-brightgreen.svg\" alt=\"Discord\"/></a>\n<a href=\"https://opencollective.com/nest#backer\" target=\"_blank\"><img src=\"https://opencollective.com/nest/backers/badge.svg\" alt=\"Backers on Open Collective\" /></a>\n<a href=\"https://opencollective.com/nest#sponsor\" target=\"_blank\"><img src=\"https://opencollective.com/nest/sponsors/badge.svg\" alt=\"Sponsors on Open Collective\" /></a>\n  <a href=\"https://paypal.me/kamilmysliwiec\" target=\"_blank\"><img src=\"https://img.shields.io/badge/Donate-PayPal-ff3f59.svg\"/></a>\n    <a href=\"https://opencollective.com/nest#sponsor\"  target=\"_blank\"><img src=\"https://img.shields.io/badge/Support%20us-Open%20Collective-41B883.svg\" alt=\"Support us\"></a>\n  <a href=\"https://twitter.com/nestframework\" target=\"_blank\"><img src=\"https://img.shields.io/twitter/follow/nestframework.svg?style=social&label=Follow\"></a>\n</p>\n  <!--[![Backers on Open Collective](https://opencollective.com/nest/backers/badge.svg)](https://opencollective.com/nest#backer)\n  [![Sponsors on Open Collective](https://opencollective.com/nest/sponsors/badge.svg)](https://opencollective.com/nest#sponsor)-->\n\n## Description\n\n[Nest](https://github.com/nestjs/nest) framework TypeScript starter repository.\n\n## Installation\n\n```bash\n$ npm install\n```\n\n## Running the app\n\n```bash\n# development\n$ npm run start\n\n# watch mode\n$ npm run start:dev\n\n# production mode\n$ npm run start:prod\n```\n\n## Test\n\n```bash\n# unit tests\n$ npm run test\n\n# e2e tests\n$ npm run test:e2e\n\n# test coverage\n$ npm run test:cov\n```\n\n## Support\n\nNest is an MIT-licensed open source project. It can grow thanks to the sponsors and support by the amazing backers. If you'd like to join them, please [read more here](https://docs.nestjs.com/support).\n\n## Stay in touch\n\n- Author - [Kamil Myśliwiec](https://kamilmysliwiec.com)\n- Website - [https://nestjs.com](https://nestjs.com/)\n- Twitter - [@nestframework](https://twitter.com/nestframework)\n\n## License\n\nNest is [MIT licensed](LICENSE).\n"
    },
    {
      "name": "RidhimaGupta4/sparrow",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/172059959?s=40&v=4",
      "owner": "RidhimaGupta4",
      "repo_name": "sparrow",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2024-06-16T08:00:32Z",
      "updated_at": "2024-06-19T06:54:03Z",
      "topics": [],
      "readme": "# Sparrow\n\nData processing with ML and LLM\n\n<p align=\"center\">\n  <img width=\"300\" height=\"300\" src=\"https://github.com/katanaml/sparrow/blob/main/sparrow-ui/assets/sparrow_logo_5.png\">\n</p>\n\n## The Principle\n\nSparrow is an innovative open-source solution for efficient data extraction and processing from various documents and images. It seamlessly handles forms, invoices, receipts, and other unstructured data sources. Sparrow stands out with its modular architecture, offering independent services and pipelines all optimized for robust performance. One of the critical functionalities of Sparrow - pluggable architecture. You can easily integrate and run data extraction pipelines using tools and frameworks like LlamaIndex, Haystack, or Unstructured. Sparrow enables local LLM data extraction pipelines through Ollama or Apple MLX. With Sparrow solution you get API, which helps to process and transform your data into structured output, ready to be integrated with custom workflows.\n\nSparrow Agents - with Sparrow you can build independent LLM agents, and use API to invoke them from your system.\n\n**List of available agents:**\n\n- **llamaindex** - RAG pipeline with LlamaIndex for PDF processing\n- **vllamaindex** - RAG pipeline with LLamaIndex multimodal for image processing\n- **vprocessor** - RAG pipeline with OCR and LlamaIndex for image processing\n- **haystack** - RAG pipeline with Haystack for PDF processing\n- **fcall** - Function call pipeline\n- **unstructured-light** - RAG pipeline with Unstructured and LangChain, supports PDF and image processing\n- **unstructured** - RAG pipeline with Weaviate vector DB query, Unstructured and LangChain, supports PDF and image processing\n- **instructor** - RAG pipeline with Unstructured and Instructor libraries, supports PDF and image processing. Works great for JSON response generation\n\n![Sparrow](https://github.com/katanaml/sparrow/blob/main/sparrow-ui/assets/sparrow_architecture.png)\n\n### Services\n\n* **[sparrow-data-ocr](https://github.com/katanaml/sparrow/tree/main/sparrow-data/ocr)** - OCR service, providing optical character recognition as part of the Sparrow.\n* **[sparrow-data-parse](https://github.com/katanaml/sparrow/tree/main/sparrow-data/parse)** - Sparrow library with helpful methods for data pre-processing for LLM.\n* **[sparrow-ml-llm](https://github.com/katanaml/sparrow/tree/main/sparrow-ml/llm)** - LLM RAG pipeline, Sparrow service for data extraction and document processing.\n* **[sparrow-ui](https://github.com/katanaml/sparrow/tree/main/sparrow-ui/)** - Dashboard UI for LLM RAG pipeline.\n\nSparrow implementation with Donut ML model - <a href=\"https://github.com/katanaml/sparrow-donut\">sparrow-donut</a>\n\n## Quickstart\n\n1. Install Weaviate vector DB, if you are planning to use Sparrow agent, which runs Weaviate (for example `llamaindex` or `unstructured`)\n2. Install `pyenv` and then install Python into your environment\n3. Create virtual environment for the Sparrow agent you want to run\n4. Install requirements for the Sparrow agent you want to use. Keep in mind, depending on OS, it could be required to do additional install steps for some of the libraries (for example PaddleOCR or Unstructured)\n5. Run Sparrow either from CLI or from API. You need to start API endpoint\n6. Pass field names and types you want to extract from the document\n7. Some of the Sparrow agents (`vprocessor`, `instructor`, etc.) support both PDF and image formats\n\nSee detailed instructions below.\n\n## Installation\n\n### LLM\n\n- **Install Weaviate local DB with Docker:**\n   \n```\ndocker compose up -d\n```\n\n- **Sparrow setup** \n\n*Setup Python Environment (Sparrow is tested with Python 3.10.4) with `pyenv`:*\n\n1. Install `pyenv`:\n\nIf you haven't already installed `pyenv`, you can do so using Homebrew with the following command:\n\n```\nbrew update\nbrew install pyenv\n\n```\n\n2. Install the desired Python version:\n\nWith `pyenv` installed, you can now install a specific version of Python. For example, to install Python 3.10.4, you would use:\n\n```\npyenv install 3.10.4\n```\n\nYou can check available Python versions by running `pyenv install --list`.\n\n3. Set the global Python version:\n\nOnce the installation is complete, you can set the desired Python version as the default (global) version on your system:\n\n```\npyenv global 3.10.4\n```\n\nThis command sets Python 3.10.4 as the default version for all shells.\n\n4. Verify the change:\n\nTo ensure the change was successful, you can verify the current Python version by running:\n\n```\npython --version\n```\n\nIf the output doesn’t reflect the change, you may need to restart your terminal or add `pyenv` to your shell's initialization script as follows:\n\n5. Configure your shell's initialization script:\n\nAdd `pyenv` to your shell by adding the following lines to your `~/.bash_profile`, `~/.zprofile`, `~/.bashrc`, or `~/.zshrc` file:\n\n```\nexport PATH=\"$HOME/.pyenv/bin:$PATH\"\neval \"$(pyenv init --path)\"\neval \"$(pyenv init -)\"\n```\n\nAfter adding these lines, restart your terminal or source your profile script with `source ~/.bash_profile` (or the appropriate file for your shell).\n\n*Create Virtual Environments to Run Sparrow Agents*\n\n1. Create virtual environments in `sparrow-ml/llm` folder:\n\n```\npython -m venv .env_llamaindex\npython -m venv .env_haystack\npython -m venv .env_instructor\npython -m venv .env_unstructured\n```\n\n`.env_llamaindex` is used for LLM RAG with `llamaindex`, `vllamaindex` and `vprocessor` agents, `.env_haystack` is used for LLM RAG with `haystack` agent, and `.env_instructor` is used for LLM function calling with `fcall` agent and for `instructor` RAG agent. `.env_unstructured` is used for `unstructured-light` and `unstructured` agents.\n\n2. Create virtual environment in `sparrow-data/ocr` folder:\n\n```\npython -m venv .env_ocr\n```\n\n*Activate Virtual Environments and Install Dependencies*\n\nActivate each environment and install its dependencies using the corresponding `requirements.txt` file.\n\nFor `llamaindex` environment:\n\n1. Activate the environment:\n\n```\nsource .env_llamaindex/bin/activate\n```\n\n2. Install dependencies:\n\n```\npip install -r requirements_llamaindex.txt\n```\n\nRepeat the same for `haystack`, `instructor` and `unstructured` environments.\n\n*Run Sparrow*\n\nYou can run Sparrow on CLI or through API. To run on CLI, use `sparrow.sh` script. Run it from corresponding virtual environment, depending which agent you want to execute.\n\n- **Install <a href=\"https://ollama.ai\">Ollama</a> and pull LLM model specified in config.yml**\n\n### OCR\n\nFollow the install steps outlined here:\n\n1. Sparrow OCR services <a href=\"https://github.com/katanaml/sparrow/tree/main/sparrow-data/ocr\">install steps</a>\n\n## Usage\n\nCopy text PDF files to the `data` folder or use sample data provided in the `data` folder.\n\n### Ingest\n\nThis step is required for `llamaindex` or `haystack` agents only.\n\nRun the script, to convert text to vector embeddings and save in Weaviate. By default it will use `llamaindex` agent. Example with `llamaindex` agent:  \n\n```\n./sparrow.sh ingest --file-path /data/invoice_1.pdf --agent llamaindex --index-name Sparrow_llamaindex_doc1\n```\n\nExample with `haystack` agent:\n\n```\n./sparrow.sh ingest --file-path /data/invoice_1.pdf --agent haystack --index-name Sparrow_haystack_doc1\n```\n\n### Inference\n\nRun the script, to process data with LLM RAG and return the answer. By default, it will use `llamaindex` agent. You can specify other agents (see ingest example), such as `haystack`: \n\n```\n./sparrow.sh \"invoice_number, invoice_date, client_name, client_address, client_tax_id, seller_name, seller_address,\nseller_tax_id, iban, names_of_invoice_items, gross_worth_of_invoice_items, total_gross_worth\" \"int, str, str, str, str,\nstr, str, str, str, List[str], List[float], str\" --agent llamaindex --index-name Sparrow_llamaindex_doc1\n```\n\nAnswer:\n\n```json\n{\n    \"invoice_number\": 61356291,\n    \"invoice_date\": \"09/06/2012\",\n    \"client_name\": \"Rodriguez-Stevens\",\n    \"client_address\": \"2280 Angela Plain, Hortonshire, MS 93248\",\n    \"client_tax_id\": \"939-98-8477\",\n    \"seller_name\": \"Chapman, Kim and Green\",\n    \"seller_address\": \"64731 James Branch, Smithmouth, NC 26872\",\n    \"seller_tax_id\": \"949-84-9105\",\n    \"iban\": \"GB50ACIE59715038217063\",\n    \"names_of_invoice_items\": [\n        \"Wine Glasses Goblets Pair Clear Glass\",\n        \"With Hooks Stemware Storage Multiple Uses Iron Wine Rack Hanging Glass\",\n        \"Replacement Corkscrew Parts Spiral Worm Wine Opener Bottle Houdini\",\n        \"HOME ESSENTIALS GRADIENT STEMLESS WINE GLASSES SET OF 4 20 FL OZ (591 ml) NEW\"\n    ],\n    \"gross_worth_of_invoice_items\": [\n        66.0,\n        123.55,\n        8.25,\n        14.29\n    ],\n    \"total_gross_worth\": \"$212,09\"\n}\n```\n\nExample with `haystack` agent:\n\n```\n./sparrow.sh \"invoice_number, invoice_date, client_name, client_address, client_tax_id, seller_name, seller_address,\nseller_tax_id, iban, names_of_invoice_items, gross_worth_of_invoice_items, total_gross_worth\" \"int, str, str, str, str,\nstr, str, str, str, List[str], List[float], str\" --agent haystack --index-name Sparrow_haystack_doc1\n```\n\nTo run multimodal agent, use `vllamaindex` flag:\n\n```\n./sparrow.sh \"guest_no, cashier_name\" \"int, str\" --agent vllamaindex --file-path /data/inout-20211211_001.jpg\n```\n\nUse `vprocessor` agent to run OCR + LLM, this works best to process scanned docs\n\n```\n./sparrow.sh \"guest_no, cashier_name, transaction_number, names_of_receipt_items, authorized_amount, receipt_date\" \"int, str, int, List[str], str, str\" --agent vprocessor --file-path /data/inout-20211211_001.jpg\n```\n\nLLM function call example:\n\n```\n./sparrow.sh assistant --agent \"fcall\" --query \"Exxon\"\n```\n\nAnswer:\n\n```json\n{\n  \"company\": \"Exxon\",\n  \"ticker\": \"XOM\"\n}\n```\n\n```\nThe stock price of the Exxon is 111.2699966430664. USD\n```\n\nUse `unstructured-light` agent to run RAG pipeline with Unstructured library. It helps to improve data pre-processing for LLM. This agent supports PDF, JPG and PNG files\n\n```\n./sparrow.sh \"invoice_number, invoice_date, total_gross_worth\" \"int, str, str\" --agent unstructured-light --file-path /data/invoice_1.pdf\n```\n\nWith `unstructured-light` it is possible to specify option for table data processing only. This agent supports PDF, JPG and PNG files\n\n```\n./sparrow.sh \"names_of_invoice_items, gross_worth_of_invoice_items, total_gross_worth\" \"List[str], List[str], str\"\n--agent unstructured-light --file-path /data/invoice_1.pdf --options tables\n```\n\nUse `unstructured` agent to run RAG pipeline with Weaviate query (no separate step to ingest data is required) and Unstructured library. This agent supports PDF, JPG and PNG files\n\n```\n./sparrow.sh \"invoice_number, invoice_date, total_gross_worth\" \"int, str, str\" --agent unstructured --file-path /data/invoice_1.pdf\n```\n\nUse `instructor` agent to run RAG pipeline with Unstructured and Instructor libraries. Unstructured helps to pre-process data for better LLM responses. Instructor simplifies RAG pipeline and ensures JSON responses. This agent supports both PDF, JPG and PNG files\n\n```\n./sparrow.sh \"names_of_invoice_items, gross_worth_of_invoice_items, total_gross_worth\" \"List[str], List[str], str\" --agent instructor --file-path /data/invoice_1.pdf\n```\n\n## FastAPI Endpoint for Local LLM RAG\n\nSparrow enables you to run a local LLM RAG as an API using FastAPI, providing a convenient and efficient way to interact with our services. You can pass the name of the plugin to be used for the inference. By default, `llamaindex` agent is used.\n\nTo set this up:\n\n1. Start the Endpoint\n\nLaunch the endpoint by executing the following command in your terminal:\n\n```\npython api.py\n```\n\nIf you want to run agents from different Python virtual environments simultaneously, you can specify port, to avoid conflicts:\n\n```\npython api.py --port 8001\n```\n\n2. Access the Endpoint Documentation\n\nYou can view detailed documentation for the API by navigating to:\n\n```\nhttp://127.0.0.1:8000/api/v1/sparrow-llm/docs\n```\n\nFor visual reference, a screenshot of the FastAPI endpoint\n\n![FastAPI endpoint](https://github.com/katanaml/sparrow/blob/main/sparrow-ui/assets/lemming_2_.png)\n\nAPI calls:\n\n### Ingest\n\nIngest call with `llamaindex` agent:\n\n```\ncurl -X 'POST' \\\n  'http://127.0.0.1:8000/api/v1/sparrow-llm/ingest' \\\n  -H 'accept: application/json' \\\n  -H 'Content-Type: multipart/form-data' \\\n  -F 'agent=llamaindex' \\\n  -F 'index_name=Sparrow_llamaindex_doc2' \\\n  -F 'file=@invoice_1.pdf;type=application/pdf'\n```\n\nIngest call with `haystack` agent:\n\n```\ncurl -X 'POST' \\\n  'http://127.0.0.1:8000/api/v1/sparrow-llm/ingest' \\\n  -H 'accept: application/json' \\\n  -H 'Content-Type: multipart/form-data' \\\n  -F 'agent=haystack' \\\n  -F 'index_name=Sparrow_haystack_doc2' \\\n  -F 'file=@invoice_1.pdf;type=application/pdf'\n```\n\n### Inference\n\nInference call with `llamaindex` agent:\n\n```\ncurl -X 'POST' \\\n  'http://127.0.0.1:8000/api/v1/sparrow-llm/inference' \\\n  -H 'accept: application/json' \\\n  -H 'Content-Type: multipart/form-data' \\\n  -F 'fields=invoice_number' \\\n  -F 'types=int' \\\n  -F 'agent=llamaindex' \\\n  -F 'index_name=Sparrow_llamaindex_doc2' \\\n  -F 'file='\n```\n\nInference call with `haystack` agent:\n\n```\ncurl -X 'POST' \\\n  'http://127.0.0.1:8000/api/v1/sparrow-llm/inference' \\\n  -H 'accept: application/json' \\\n  -H 'Content-Type: multipart/form-data' \\\n  -F 'fields=invoice_number' \\\n  -F 'types=int' \\\n  -F 'agent=haystack' \\\n  -F 'index_name=Sparrow_haystack_doc2' \\\n  -F 'file='\n```\n\nInference call with multimodal agent `vllamaindex`:\n\n```\ncurl -X 'POST' \\\n  'http://127.0.0.1:8000/api/v1/sparrow-llm/inference' \\\n  -H 'accept: application/json' \\\n  -H 'Content-Type: multipart/form-data' \\\n  -F 'fields=guest_no, cashier_name' \\\n  -F 'types=int, str' \\\n  -F 'agent=vllamaindex' \\\n  -F 'index_name=' \\\n  -F 'file=@inout-20211211_001.jpg;type=image/jpeg'\n```\n\nInference call with OCR + LLM agent `vprocessor`:\n\n```\ncurl -X 'POST' \\\n  'http://127.0.0.1:8000/api/v1/sparrow-llm/inference' \\\n  -H 'accept: application/json' \\\n  -H 'Content-Type: multipart/form-data' \\\n  -F 'fields=guest_no, cashier_name, transaction_number, names_of_receipt_items, authorized_amount, receipt_date' \\\n  -F 'types=int, str, int, List[str], str, str' \\\n  -F 'agent=vprocessor' \\\n  -F 'index_name=' \\\n  -F 'file=@inout-20211211_001.jpg;type=image/jpeg'\n```\n\nInference call with `unstructured-light` agent, this agent supports also JPG and PNG files:\n\n```\ncurl -X 'POST' \\\n  'http://127.0.0.1:8000/api/v1/sparrow-llm/inference' \\\n  -H 'accept: application/json' \\\n  -H 'Content-Type: multipart/form-data' \\\n  -F 'fields=invoice_number, invoice_date, total_gross_worth' \\\n  -F 'types=int, str, str' \\\n  -F 'agent=unstructured-light' \\\n  -F 'index_name=' \\\n  -F 'options=' \\\n  -F 'file=@invoice_1.pdf;type=application/pdf'\n```\n\nInference call with `unstructured-light` agent, using only tables option. This agent supports also JPG and PNG files:\n\n```\ncurl -X 'POST' \\\n  'http://127.0.0.1:8000/api/v1/sparrow-llm/inference' \\\n  -H 'accept: application/json' \\\n  -H 'Content-Type: multipart/form-data' \\\n  -F 'fields=names_of_invoice_items, gross_worth_of_invoice_items, total_gross_worth' \\\n  -F 'types=List[str], List[str], str' \\\n  -F 'agent=unstructured-light' \\\n  -F 'index_name=' \\\n  -F 'options=tables' \\\n  -F 'file=@invoice_1.pdf;type=application/pdf'\n```\n\nInference call with `unstructured` agent, this agent supports also JPG and PNG files:\n\n```\ncurl -X 'POST' \\\n  'http://127.0.0.1:8000/api/v1/sparrow-llm/inference' \\\n  -H 'accept: application/json' \\\n  -H 'Content-Type: multipart/form-data' \\\n  -F 'fields=names_of_invoice_items, gross_worth_of_invoice_items, total_gross_worth' \\\n  -F 'types=List[str], List[str], str' \\\n  -F 'agent=unstructured' \\\n  -F 'index_name=' \\\n  -F 'options=' \\\n  -F 'file=@invoice_1.pdf;type=application/pdf'\n```\n\nInference call with `instructor` agent, this agent supports also JPG and PNG files:\n\n```\ncurl -X 'POST' \\\n  'http://127.0.0.1:8000/api/v1/sparrow-llm/inference' \\\n  -H 'accept: application/json' \\\n  -H 'Content-Type: multipart/form-data' \\\n  -F 'fields=names_of_invoice_items, gross_worth_of_invoice_items, total_gross_worth' \\\n  -F 'types=List[str], List[str], str' \\\n  -F 'agent=instructor' \\\n  -F 'index_name=' \\\n  -F 'options=' \\\n  -F 'file=@invoice_1.pdf;type=application/pdf'\n```\n\n## Examples\n\n### Inference with local LLM RAG\n\nDocument:\n\n![document RAG](https://github.com/katanaml/sparrow/blob/main/sparrow-ui/assets/invoice_1.jpg)\n\nRequest:\n\n```\n./sparrow.sh \"invoice_number, invoice_date, client_name, client_address, client_tax_id, seller_name, seller_address,\nseller_tax_id, iban, names_of_invoice_items, gross_worth_of_invoice_items, total_gross_worth\" \"int, str, str, str, str,\nstr, str, str, str, List[str], List[float], str\" --agent llamaindex --index-name Sparrow_llamaindex_doc1\n```\n\nResponse:\n\n```json\n{\n    \"invoice_number\": 61356291,\n    \"invoice_date\": \"09/06/2012\",\n    \"client_name\": \"Rodriguez-Stevens\",\n    \"client_address\": \"2280 Angela Plain, Hortonshire, MS 93248\",\n    \"client_tax_id\": \"939-98-8477\",\n    \"seller_name\": \"Chapman, Kim and Green\",\n    \"seller_address\": \"64731 James Branch, Smithmouth, NC 26872\",\n    \"seller_tax_id\": \"949-84-9105\",\n    \"iban\": \"GB50ACIE59715038217063\",\n    \"names_of_invoice_items\": [\n        \"Wine Glasses Goblets Pair Clear Glass\",\n        \"With Hooks Stemware Storage Multiple Uses Iron Wine Rack Hanging Glass\",\n        \"Replacement Corkscrew Parts Spiral Worm Wine Opener Bottle Houdini\",\n        \"HOME ESSENTIALS GRADIENT STEMLESS WINE GLASSES SET OF 4 20 FL OZ (591 ml) NEW\"\n    ],\n    \"gross_worth_of_invoice_items\": [\n        66.0,\n        123.55,\n        8.25,\n        14.29\n    ],\n    \"total_gross_worth\": \"$212,09\"\n}\n```\n\n### Inference with OCR + local LLM RAG\n\nDocument:\n\n![document OCR RAG](https://github.com/katanaml/sparrow/blob/main/sparrow-ui/assets/ross-20211211_010.jpg)\n\nRequest:\n\n```\n./sparrow.sh \"store_name, receipt_id, receipt_item_names, receipt_item_prices, receipt_date, receipt_store_id,\nreceipt_sold, receipt_returned, receipt_total\" \"str, str, List[str], List[str], str, int, int,\nint, str\" --agent vprocessor --file-path /data/ross-20211211_010.jpg\n```\n\nResponse:\n\n```json\n{\n    \"store_name\": \"Ross\",\n    \"receipt_id\": \"Receipt # 0421-01-1602-1330-0\",\n    \"receipt_item_names\": [\n        \"400226513665 x hanes b1ue 4pk\",\n        \"400239602790 fruit premium 4pk\"\n    ],\n    \"receipt_item_prices\": [\n        \"$9.99R\",\n        \"$12.99R\"\n    ],\n    \"receipt_date\": \"11/26/21 10:35:05 AM\",\n    \"receipt_store_id\": 421,\n    \"receipt_sold\": 2,\n    \"receipt_returned\": 0,\n    \"receipt_total\": \"$25.33\"\n}\n```\n\n## Commercial usage\n\nSparrow is available under the GPL 3.0 license, promoting freedom to use, modify, and distribute the software while ensuring any modifications remain open source under the same license. This aligns with our commitment to supporting the open-source community and fostering collaboration.\n\nAdditionally, we recognize the diverse needs of organizations, including small to medium-sized enterprises (SMEs). Therefore, Sparrow is also offered for free commercial use to organizations with gross revenue below $5 million USD in the past 12 months, enabling them to leverage Sparrow without the financial burden often associated with high-quality software solutions.\n\nFor businesses that exceed this revenue threshold or require usage terms not accommodated by the GPL 3.0 license—such as integrating Sparrow into proprietary software without the obligation to disclose source code modifications—we offer dual licensing options. Dual licensing allows Sparrow to be used under a separate proprietary license, offering greater flexibility for commercial applications and proprietary integrations. This model supports both the project's sustainability and the business's needs for confidentiality and customization.\n"
    },
    {
      "name": "padmapria/Interactive-URL-Based-LLM-RAG-Question-Answering-System",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/31624929?s=40&v=4",
      "owner": "padmapria",
      "repo_name": "Interactive-URL-Based-LLM-RAG-Question-Answering-System",
      "description": null,
      "homepage": null,
      "language": null,
      "created_at": "2024-05-22T08:35:10Z",
      "updated_at": "2024-06-14T03:54:47Z",
      "topics": [],
      "readme": ""
    },
    {
      "name": "MuhammadTalmeez337/Healthcare-chatbot-using-mixtral-LLM-and-Pubmed",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/46068141?s=40&v=4",
      "owner": "MuhammadTalmeez337",
      "repo_name": "Healthcare-chatbot-using-mixtral-LLM-and-Pubmed",
      "description": "PubMed Healthcare Chatbot. LLM Augmented Q&A over PubMed Search Engine. ",
      "homepage": null,
      "language": "Python",
      "created_at": "2024-05-21T10:13:09Z",
      "updated_at": "2024-07-09T04:21:30Z",
      "topics": [],
      "readme": "# Healthcare-chatbot-using-mixtral-LLM-and-Pubmed\nPubMed Healthcare Chatbot. LLM Augmented Q&A over PubMed Search Engine.\n\nCreating a healthcare chatbot that leverages the power of Mixtral 8x7b (Mixtral-8x7B-Instruct-v0.1), a state-of-the-art Large Language Model (LLM) developed by Mistral AI. \nThis chatbot is designed to help users efficiently search biomedical literature from PubMed.\n\nExploring the capabilities of Mixtral 8x7b and how it can be integrated with the Haystack orchestration framework to create a sophisticated and responsive chatbot. \nThe Haystack framework is instrumental in simplifying the search process, allowing for quick and accurate retrieval of biomedical information.\n"
    },
    {
      "name": "samvardhan777/opensearch_search_engine",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/69216650?s=40&v=4",
      "owner": "samvardhan777",
      "repo_name": "opensearch_search_engine",
      "description": null,
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2024-04-28T12:02:40Z",
      "updated_at": "2024-05-18T13:56:52Z",
      "topics": [],
      "readme": "# OpenSearch Docker Setup\n\nThis repository provides Docker Compose files for easily running OpenSearch and OpenSearch Dashboards. It's designed to help users quickly set up a local search and visualization environment for managing and exploring data.\n\n## Prerequisites\n\nBefore setting up, ensure you have the following installed on your system:\n\n- **Docker**: You can install Docker by following the instructions here: [Install Docker](https://docs.docker.com/get-docker/)\n- **Docker Compose**: You can install Docker Compose by following the instructions here: [Install Docker Compose](https://docs.docker.com/compose/install/)\n\n## Installation\n\nTo get started, clone this repository to your local machine:\n\n```bash\ngit clone https://github.com/your-username/opensearch-docker-setup.git\n```\n\n## Running OpenSearch\n\nTo start OpenSearch and OpenSearch Dashboards, execute the following command:\n\n```bash\ndocker-compose up -d\n```\n\n## Accessing OpenSearch Dashboards\n\nOnce the services are running, OpenSearch Dashboards can be accessed at:\n\n[http://localhost:5601/app/home#/](http://localhost:5601/app/home#/)\n\nHere you can create and manage your visualizations, dashboards, and explore your data.\n\n\n# Stopping OpenSearch\n\n```\ndocker-compose down\n```\n\n"
    },
    {
      "name": "last-brain-cell/know-better",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/109949049?s=40&v=4",
      "owner": "last-brain-cell",
      "repo_name": "know-better",
      "description": "A super smart content assistant",
      "homepage": null,
      "language": "TypeScript",
      "created_at": "2024-05-04T09:25:01Z",
      "updated_at": "2024-06-26T13:25:32Z",
      "topics": [],
      "readme": ""
    },
    {
      "name": "Dhilip2002/Augmented-Question-Answering-System-for-Biomedical-Literature-Retrieval-using-LLM-and-PubMed",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/71542489?s=40&v=4",
      "owner": "Dhilip2002",
      "repo_name": "Augmented-Question-Answering-System-for-Biomedical-Literature-Retrieval-using-LLM-and-PubMed",
      "description": "A Chatbot used to fetch articles from PubMed based on user queries and displays the queries and responses in a user friendly interface built using Streamlit. It fetches the Title, Abstract and Keywords based on user's input and then stores the Question Answer pair in Firebase. ",
      "homepage": "",
      "language": "Python",
      "created_at": "2024-05-09T14:38:37Z",
      "updated_at": "2024-09-17T07:32:05Z",
      "topics": [
        "artificial-intelligence",
        "chatbot",
        "firebase",
        "firebase-database",
        "haystack",
        "haystack-ai",
        "haystack-pipeline",
        "large-language-models",
        "llm",
        "machine-learning",
        "mistral-7b",
        "ml",
        "natural-language-processing",
        "nlp",
        "pubmed",
        "pymed",
        "qasystem",
        "streamlit"
      ],
      "readme": "## Augmented Question-Answering System for Biomedical Literature Retrieval using LLM and PubMed\n\n## Overview\nThe Biomedical Q&A with Language Models project is an innovative approach to accessing and understanding vast amounts of biomedical literature using state-of-the-art language models. Leveraging advanced natural language processing (NLP) techniques, this project enables users to ask questions about various biomedical topics and receive accurate and relevant answers in real-time.\n\n## Table of Contents\n1. [Description](#Description)\n2. [Prerequisites](#Prerequisites)\n3. [Setting up the Environment](#SettinguptheEnvironment)\n4. [Setting up the Database Folder](#SettinguptheDatabaseFolder)\n5. [Running the Scripts](#RunningtheScripts)\n6. [Usage](#Usage)\n7. [License](#License)\n\n## Description\nThe Biomedical Q&A with Language Models project is a cutting-edge solution designed to facilitate easy access to biomedical information. By combining advanced language models with integration capabilities with PubMed, users can simply input their queries into the Streamlit interface and receive accurate, evidence-based responses in real-time. The system also incorporates features like face detection and recognition, ensuring a comprehensive user experience. With its ability to fetch relevant articles from PubMed, interact with powerful language models, and store responses for future reference, this project represents a significant advancement in biomedical research and information retrieval.\n\n## Prerequisites\nBefore you can start using the face detection and recognition scripts, make sure you have the following prerequisites installed on your system:\n\n1. ## Python:\n    * Python is the programming language used for this project. If you don't have Python installed on your system, you can download and install it from here.\n\n2. ## Visual Studio C++ Build Tools:\n   * These tools are necessary for compiling certain dependencies required by the libraries used in this project. Follow the steps below to download and install\n     Visual Studio along with the required workload for \"Desktop Development with C++\":\n\n   ## Step 1: Download Visual Studio\n      * Navigate to Visual Studio Downloads website.\n      * Click on the \"Download Visual Studio\" button.\n      * Follow the on-screen instructions to download the Visual Studio installer.\n      * Once the installer is downloaded, run it.\n        \n   ## Step 2: Download and install the workload: \"Desktop Development with C++\"\n      * Open the Visual Studio Installer.\n      * Select \"Visual Studio C++ Build Tools\" from the list of available products.\n      * Click on \"Install\" to download and install the workload.\n      * Follow the on-screen instructions to complete the installation.\n      \n   ## Step 3: Customize Installation (Optional)\n      * If you want to customize the installation or select additional components, you can do so by clicking on the \"Individual components\" tab.\n      * Here, you can browse and select additional components based on your requirements.\n      \n   ## Step 4: Install\n      * After selecting the desired workload and components, click on the \"Install\" button to begin the installation process.\n      * Follow the on-screen instructions to complete the installation.\n\n## Setting up the Environment\n   * To ensure a smooth running of the scripts, it's recommended to set up a Python virtual environment. Follow these steps to create the environment:\n\n1. ## Clone the Github Repository\n      * Open your terminal or command prompt\n      * Run the following command to download the folder\n##\n      git clone https://github.com/Dhilip2002/Augmented-Question-Answering-System-for-Biomedical-Literature-Retrieval-using-LLM-and-PubMed.git\n   \n2. ## Create a Python Environment\n      * Open your terminal or command prompt and navigate to your project directory.\n      * Run the following command to create a virtual environment named myenv:\n##\n    python -m venv myenv\n    \n3. ## Activate the Environment\n      * Depending on your operating system, use the appropriate command to activate the virtual environment:\n  On Windows:\n  ##\n    myenv\\Scripts\\activate\n  On macOS/Linux:\n  ##\n    source myenv/bin/activate\n    \n4. ## Install Necessary Packages\n      * Once the virtual environment is activated, install the required packages using pip:\n  ##\n    pip install -r requirements.txt\n\n## Setting up the Database Folder\nSetting up the database folder is an essential step in ensuring the smooth functioning of the Biomedical Q&A with Language Models project. With Firebase integration in place, storing and managing responses becomes streamlined. Here's how you can set up the database folder:\n\n1. ## Firebase Configuration \n   * Begin by configuring Firebase with the necessary credentials. \n   * Ensure you have the API key, authentication domain, database URL, project ID, storage bucket, messaging sender ID, and app ID handy. \n   * These details are vital for initializing Firebase and establishing a connection with the database.\n\n2. ## Initialize Firebase \n   * Once you have the configuration parameters, initialize Firebase using the pyrebase library. \n   * This step establishes a connection to the Firebase project, enabling interaction with the database.\n\n3. ## Access Authentication \n   * If your project requires authentication for accessing the database, you can utilize Firebase's authentication functionality. \n   * This ensures that only authorized users can perform operations like reading and writing data to the database.\n\n4. ## Database Interaction \n   * With Firebase initialized, you can now interact with the database.\n   * In the provided code, a Firebase database instance (db) is created, allowing you to push responses to the database.\n   * Each response is stored as a new entry under the \"responses\" node, containing the question and corresponding answer.\n     \n## Running the Scripts\n* Now that you've set up your environment and installed the necessary packages, you can run the app.py scripts.\n* Run the script app.py to fetch responses from PubMed for users query and store it in Firebase.\n##\n      python app.py\n\n## Usage\n* app.py: To fetch responses from PubMed for users query and store it in Firebase.\n\n## License\nThis project is licensed under the MIT License. You can find more details in the LICENSE file included in the repository.\n\nThat's it! You're now ready to retrieve and store biomedical literatures using the provided scripts. If you have any questions or encounter any issues, feel free to reach out to the project maintainers. \n\nHappy literature retrieving and viewing!\n"
    },
    {
      "name": "ROHITH-M10/aws-invoice-generation",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/93328109?s=40&v=4",
      "owner": "ROHITH-M10",
      "repo_name": "aws-invoice-generation",
      "description": null,
      "homepage": null,
      "language": "JavaScript",
      "created_at": "2024-02-29T15:42:20Z",
      "updated_at": "2025-01-13T08:15:32Z",
      "topics": [],
      "readme": "# Invoice extractor and expense calculator\r\n\r\nTO use this project just clone this repo and do : \r\n\r\nnpm install\r\n\r\nnpm run dev \r\n\r\nto run and check the website \r\n\r\nuserlogin :\r\n\r\nUsername : Ajay@gmail.com\r\n\r\npass: bonda\r\n\r\nAs the services might be inactivity while non usage kindly wait for few mins after clicking login and refresh or wait again until it shows some alert or logs in\r\n\r\nNote:\r\n\r\nYou dont have to worry about the backend as everything is hosted in separate instances and been used as APIs here following microarchitecture services \r\n\r\n\r\n## Contributors\r\n\r\n- Barath Kumar J\r\n- Rohith M\r\n- Harish T.S\r\n- Guhanesh G\r\n- Ajayraj M\r\n- Adhvaith Sankar\r\n"
    },
    {
      "name": "browserbase/haystack",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/158221360?s=40&v=4",
      "owner": "browserbase",
      "repo_name": "haystack",
      "description": "Browserbase Haystack Fetcher",
      "homepage": "https://pypi.org/project/browserbase-haystack/",
      "language": "Python",
      "created_at": "2024-04-18T21:06:59Z",
      "updated_at": "2025-01-14T07:55:48Z",
      "topics": [],
      "readme": "# Browserbase Haystack Fetcher\n\n[Browserbase](https://browserbase.com) is a developer platform to reliably run, manage, and monitor headless browsers.\n\nPower your AI data retrievals with:\n- [Serverless Infrastructure](https://docs.browserbase.com/under-the-hood) providing reliable browsers to extract data from complex UIs\n- [Stealth Mode](https://docs.browserbase.com/features/stealth-mode) with included fingerprinting tactics and automatic captcha solving\n- [Session Debugger](https://docs.browserbase.com/features/sessions) to inspect your Browser Session with networks timeline and logs\n- [Live Debug](https://docs.browserbase.com/guides/session-debug-connection/browser-remote-control) to quickly debug your automation\n\n## Installation and setup\n\n- Get an API key and Project ID from [browserbase.com](https://browserbase.com) and set it in environment variables (`BROWSERBASE_API_KEY`, `BROWSERBASE_PROJECT_ID`).\n- Install the required dependencies:\n\n```\npip install browserbase-haystack\n```\n\n## Usage\n\nYou can load webpages into Haystack using `BrowserbaseFetcher`. Optionally, you can set `text_content` parameter to convert the pages to text-only representation.\n\n### Standalone\n\n```py\nfrom browserbase_haystack import BrowserbaseFetcher\n\nbrowserbase_fetcher = BrowserbaseFetcher()\nbrowserbase_fetcher.run(urls=[\"https://example.com\"], text_content=False)\n```\n\n### In a pipeline\n\n```py\nfrom haystack import Pipeline\nfrom haystack.components.generators import OpenAIGenerator\nfrom haystack.components.builders import PromptBuilder\nfrom browserbase_haystack import BrowserbaseFetcher\n\nprompt_template = (\n    \"Tell me the titles of the given pages. Pages: {{ documents }}\"\n)\nprompt_builder = PromptBuilder(template=prompt_template)\nllm = OpenAIGenerator()\n\nbrowserbase_fetcher = BrowserbaseFetcher()\n\npipe = Pipeline()\npipe.add_component(\"fetcher\", browserbase_fetcher)\npipe.add_component(\"prompt_builder\", prompt_builder)\npipe.add_component(\"llm\", llm)\n\npipe.connect(\"fetcher.documents\", \"prompt_builder.documents\")\npipe.connect(\"prompt_builder.prompt\", \"llm.prompt\")\nresult = pipe.run(data={\"fetcher\": {\"urls\": [\"https://example.com\"]}})\n```\n\n### Parameters\n\n- `urls` Required. A list of URLs to fetch.\n- `text_content` Retrieve only text content. Default is `False`.\n- `session_id` Optional. Provide an existing Session ID.\n- `proxy` Optional. Enable/Disable Proxies.\n"
    },
    {
      "name": "silvhua/pubmed-search",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/111714881?s=40&v=4",
      "owner": "silvhua",
      "repo_name": "pubmed-search",
      "description": "Semantic search engine for PubMed database",
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2024-02-04T06:10:41Z",
      "updated_at": "2024-07-25T23:21:01Z",
      "topics": [],
      "readme": "# Pubmed Search and Recommender System\nSemantic search engine & article recommender system for PubMed database\n"
    },
    {
      "name": "justus-positivo/groq-edu",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/121574860?s=40&v=4",
      "owner": "justus-positivo",
      "repo_name": "groq-edu",
      "description": null,
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2024-03-23T19:16:03Z",
      "updated_at": "2024-04-08T06:42:43Z",
      "topics": [],
      "readme": ""
    },
    {
      "name": "craigsdennis/haystack-llamaguard-workers-ai",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/8494909?s=40&v=4",
      "owner": "craigsdennis",
      "repo_name": "haystack-llamaguard-workers-ai",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2024-03-13T04:54:21Z",
      "updated_at": "2024-04-14T14:18:24Z",
      "topics": [],
      "readme": "# Haystack + LlamaGuard - Cloudflare Workers\n\nThis is an exploration of the deepset orchestration framework [Haystack](https://haystack.deepset.ai/) using [Cloudflare Workers AI](https://ai.cloudflare.com) to provide LLM Guardrails using Meta's [Llama Guard](https://ai.meta.com/research/publications/llama-guard-llm-based-input-output-safeguard-for-human-ai-conversations/).\n\n\n## Installation\n\nCopy [.env.example](./.env.example) to .env and add your values.\n\n```python\npython -m venv venv\nsource ./venv/bin/activate\npython -m pip install -r requirements.txt\n```\n\n## Run\n\n```python\npython -m streamlit run app.py\n```\n\nGive it something safe like \"I want to airpunch\"\n\nThen give it something unsafe like \"I want to punch myself\""
    },
    {
      "name": "SameerAhamed25/Telegram_AI_Bot",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/145002118?s=40&v=4",
      "owner": "SameerAhamed25",
      "repo_name": "Telegram_AI_Bot",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2024-03-08T16:31:22Z",
      "updated_at": "2024-03-16T17:31:25Z",
      "topics": [],
      "readme": "## Overview\n\nThis is a chatbot created using Python, Haystack, pydub, and the Hugging Face model `mistralai/Mistral-7B-Instruct-v0.2`. It supports text and voice messages, allowing users to interact with the chatbot in their preferred format.\nThe **Sania Telegram Bot** is a conversational AI bot built using Python and the Telegram Bot API. It allows users to interact with the Mistral model to get responses to their messages, including text, voice and send timed messages.\n\n**Unlinke Other Bots this Bot can Remember the conversations within the sessions. We can change how many conversations it can save in the code.**\n**It can also send timed messages specified by the user using chat id of the recipient**\n\n## Prerequisites\n\n- Hugging Face Token\n- Telegram Bot Token\n\n## Installation\n\n1. Clone the repository:\n    ```bash\n    git clone https://github.com/renuka010/Mistral-Telegram-Bot.git\n    ```\n2. Change into the project directory:\n    ```bash\n    cd Mistral-Telegram-Bot\n    ```\n3. Create and Activate the virtual environment:\n    ```bash\n    python3 -m venv venv  # Create\n    venv\\Scripts\\activate    # Activate\n    ```\n4. Install the dependencies in the virtual environment:\n    ```bash\n    pip install -r requirements.txt\n    ' and others specified in the notion doc.'\n    ```\n5. Create a `.env` file with your Huggingface token and Telegram token:\n    ```\n    HF_API_TOKEN=your_huggingface_token\n    TELEGRAM_BOT_TOKEN=your_telegram_token\n    ```\n6. Run the server:\n    ```bash\n    python chatbot.py\n    ```\n\n## Features\n\n- **Text Input**: Users can send text messages to the bot to initiate a conversation or ask questions. The Bot excels in maintaining continuity within conversations, seamlessly remembering previous interactions within the same session to provide users with a personalized and coherent experience.\n- **Voice Input**: Users can send voice messages to the bot, which will be transcribed to text for processing.\n- **Timed Messages**: The bot also has the ability to send timed messages to users. This can be set up to provide periodic reminders or updates, enhancing the interaction experience.\n\n## Inputs\n\nThe Bot accepts the following inputs:\n\n1. **Text Messages**: Users can send text messages containing their queries or messages.\n2. **Voice Messages**: Users can send voice messages containing their queries or messages.\n\n## Outputs\n\nThe Bot provides the following outputs:\n\n1. **Text Responses**: The bot responds to user messages with text-based replies.\n2. **Voice Responses**: The bot may provide voice responses for voice inputs or text inputs, allowing for a conversational experience.\n\n## Usage\n\nTo use the Mistral Telegram Bot, follow these steps:\n\n1. **Start the Bot**: Start a conversation with the bot by searching for it on Telegram and sending a message.\n2. **Send Messages**: Send text messages, voice messages to the bot to initiate conversations or ask questions.\n3. **Receive Responses**: The bot will respond to your messages with relevant information or responses.\n\n\nThis bot is created on the basis of -> https://github.com/renuka010/Mistral-Telegram-Bot.git, basicaly this an updated and more effective version.\nFor this to work we need to install and import more packages other than that mentioned in the requirements.txt!\n\nCheck Out Notion Notes For More Details:\nhttps://www.notion.so/Telegram-AI-Bot-Sam-57802ddc637147008e1fb0d6bd7a8bbd?pvs=4\n"
    },
    {
      "name": "nish700/RailBot-With-Haystack-and-Mistral",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/39549411?s=40&v=4",
      "owner": "nish700",
      "repo_name": "RailBot-With-Haystack-and-Mistral",
      "description": "Creating a Railway Chatbot with the aid of Haystack 2.0 framework and using Mistral 7B Instruct LLM for chat Question Answering",
      "homepage": null,
      "language": "HTML",
      "created_at": "2024-02-09T06:36:39Z",
      "updated_at": "2024-03-27T21:58:29Z",
      "topics": [],
      "readme": "# RailBot-With-Haystack-and-Mistral\nCreating a Railway Chatbot with the aid of Haystack 2.0 framework and using Mistral 7B Instruct LLM for chat Question Answering\n\nHaystack is an open-source Python framework developed by deepset for building custom applications with large language models (LLMs). It allows you to quickly experiment with the latest NLP models while maintaining flexibility and ease of use. Haystack 2.0 allows to integrate open source model to the pipeline with the LLamaCppGenerator.\n\nOpen Source Document store - Qdrant has been used to store the text embedding. Qdrant has the adantage of ease of scalabitlity.\n\nThe Qdrant Document Store is hosted on the Docker Container, so that it can be easily used across various environments.\n\nSentence Trsnaformer has been used to vectorize the document and generate the embeddings.\n\nThe following Libraries used for developing the chatbot:\n\n- haystack-ai\n- fastapi\n- uvicorn\n- sentence-transformers\n- docker\n- qdrant\n- llamacpp\n\n"
    },
    {
      "name": "sfgrahman/Healtcare_chatbot_PubMed",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/4463215?s=40&v=4",
      "owner": "sfgrahman",
      "repo_name": "Healtcare_chatbot_PubMed",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2024-01-29T05:26:13Z",
      "updated_at": "2024-01-30T19:35:04Z",
      "topics": [],
      "readme": "HealthCare chatbot for keyword and answer generation <a href=\"https://healhcare-pubmed.streamlit.app/\" target=\"_blank\">live demo</a>\n\n\n\n![alt text](https://github.com/sfgrahman/Healtcare_chatbot_PubMed/blob/main/HealthCare%20chatbot%20for%20keyword%20and%20answer%20generation.png)"
    },
    {
      "name": "textomatic/Finetuning-LLMs-for-Question-Answering",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/102237745?s=40&v=4",
      "owner": "textomatic",
      "repo_name": "Finetuning-LLMs-for-Question-Answering",
      "description": "Fine-tuning LLMs to answer questions about Tesla Model 3",
      "homepage": "",
      "language": "Python",
      "created_at": "2023-12-14T16:40:57Z",
      "updated_at": "2023-12-15T22:03:09Z",
      "topics": [],
      "readme": "# Fine-tuning Large Language Models for Question-Answering\n\n## Motivation\nThis project was part of the independent studies course about Large Language Models (LLMs) and Generative AI (GenAI). The aim was to pick one area of LLMs/GenAI to focus on and gain deeper theoretical understanding or application skills. My choice of focus was to learn more about supervised fine-tuning of LLMs and I decided to approach that by working on a problem of manageable magnitude: cars.\n\nCars are everywhere in this country, but how well do people know their car? If they encounter an issue with their vehicle or have a doubt about some settings in their automobile, do they have to dig out their owners' manual and flip through physical pages of books? Or is there a better way to help car owners in such situations?\n\nI decided to experiment with fine-tuning open-source LLMs such that they are able to answer questions related to a particular make of car. The fine-tuned model would ideally be the virtual assistant that a car owner would like to first approach before attempting anything.\n\n## Data\nThe car make that I settled on for this project was the Tesla Model 3, which has become increasingly popular on the roads in recent years, thanks to their quality and generous electric vehicle tax credits from the government. \n\nThe [Owner's Manual](https://www.tesla.com/ownersmanual/model3/en_us/) of Tesla Model 3 is available online in web pages or as a Portable Document Format ([PDF](https://www.tesla.com/ownersmanual/model3/en_us/Owners_Manual.pdf)) file.There were thus two ways to obtain data about Model 3 - web scraping and optical character recognition (OCR). I chose the former.\n\nSome manual post-processing of the collected data was done. They include removal of non-textual information, removal of non-ASCII characters, removal of excessive white spaces, and converting the documents into markdown format. There was a total of 128 documents upon completion of post-processing.\n\n## Approach\nThere are multiple ways to perform supervised fine-tuning of LLMs. I chose to limit the scope of this project by focusing on Parameter Efficient Fine-Tuning (PEFT) methods, particularly the Low-Rank Adapter (LoRA) approach. \n\nThere is a multitude of open-source LLMs nowadays. So, I was spoilt for choice when I had to make a decision on which models to work on. I eventually chose to experiment with Meta's Llama 2 and Mistral AI's Mistral models. For ease of comparison and deployment, the 7-billion-parameter variant of the models were used. \n\nIn order to perform supervised fine-tuning (SFT), a labeled dataset is required. I could either come up with the questions and answers manually or enlist the help of advanced LLMs like GPT-3.5/4 to speed up the process. For the latter, the general idea was to leverage prompt engineering and pass documents to LLMs to extract questions and answers. There are a number of libraries and packages which could help us out on this. The one that I tried out was called [question_extractor](https://github.com/nestordemeure/question_extractor). I had to make some modifications to their scripts in order to work with Azure OpenAI endpoints as their scripts were intended for use with OpenAI endpoints only. A total of 2078 question-answer pairs were extracted.\n\nUpon getting the question-answer data, the next step was to ensure they are in the appropriate format to be used for SFT. Some additional work needed to be done to fit every question-answer pair into a format that looked like this:\n```\n<s>[INST] question [/INST] answer </s>\n```\n\nThe Llama 2 and Mistral models contained more than 7-billion parameters and were gigantic in size. In order to perform SFT on them realistically, I had to resort to using techniques like LoRA or QLoRA (which refers to quantized LoRA). As HuggingFace has the weights of these open-source models, I decided to leverage their ecosystem of libraries which included `transformers`, `accelerate`, `peft`, `trl`, and `bitsandbytes`. The `SFTTrainer` was the main class used for performing supervised fine-tuning.\n\nThere are a lot of parameter values that needed to be decided on when using LoRA/QLoRA. Some of the key ones are rank, alpha scaling parameter, dropout probability, quantization type, etc. Trial and error was the approach taken in this stage to learn and find out what worked and what did not, especially when a specific dataset was involved. For details about the parameter values used in training the models, please refer to the config json files in the repository.\n\n## Deployment\nBoth models were trained for a total of 10 epochs each. And upon completion of training, the adapter weights were merged with the weight of their base models. The merged models were then pushed to HuggingFace hub for ease of subsequent deployment in various public cloud platforms. \n\nThe two fine-tuned models are:\n- [llama-2-7b-tsla-qna-4](https://huggingface.co/textomatic/llama-2-7b-tsla-qna-4)\n- [mistral-7b-tsla-qna-1](https://huggingface.co/textomatic/mistral-7b-tsla-qna-1)\n\nSome other models were trained along the way as well, but the most promising candidates are the above two. \n\nThe models were subsequently deployed on AWS Sagemaker as inference endpoints. Virtual machine instances were stood up in Azure to host Streamlit web applications that acted as the front end for interacting with the fine-tuned models. In the backend, the instances were communicating with the AWS endpoints to obtain answers to user questions. \n\nTo provide a point of comparison, a Retrieval Augmented Generation (RAG) solution was developed and deployed in Azure. [Haystack](https://github.com/deepset-ai/haystack) was used for orchestrating the RAG pipeline while Streamlit served as the frontend application. The RAG solution finds the most promising candidate text(s) from the document store and pass them together with the user's query to LLMs to obtain an answer. The LLMs used in this RAG pipeline are the pre-trained variants of Llama 2 7B and Mistral 7B. \n\n## Results\nAs there does not yet exist any industrial standard for systematic evaluation of text generation models, only qualitative testing and manual evaluation were performed in this project. The results were promising as both Llama 2 7B and Mistral 7B had less than 1% of their total parameters trained but they were able to retain new knowledge. Check out the demo video to see the fine-tuned models in action!\n\n## Future work\nThere are certainly many areas of improvement that this project could take on moving forward:\n- Fine-tuning models that are smaller than 7-billion parameters\n- Quantization and deployment on edge devices\n- Fine-tuning of multi-modal models such that questions and answers could be conveyed not only through words but also visuals (this would be particularly helpful as a lot of tips and instructions shared in the owners' manual were in the form of a picture or animation)\n\n## Sitemap\n\n```\n.\n|____fine-tune\n| |____deploy\n| | |____requirements.txt\n| | |____.streamlit\n| | | |____config.toml\n| | |____.gitignore\n| | |____app.py\n| | |____assets\n| | | |____icon-tesla-48x48.png\n| |____scripts\n| | |____config_mistral_7b.json\n| | |____requirements.txt\n| | |____config_llama_7b.json\n| | |____fine-tune_llm.py\n|____data_prep\n| |____scripts\n| | |____combine_qna.py\n| | |____generate_jsonl.py\n| |____data\n| | |____question_answer_pairs.jsonl\n| | |____docs\n| | | |____identification_labels.md\n| | | |____cold_weather_best_practices.md\n.......\n| | | |____troubleshooting_alerts_p1.md\n| | | |____track_mode.md\n| | |____questions.json\n|____rag\n| |____deploy\n| | |____requirements.txt\n| | |____.streamlit\n| | | |____config.toml\n| | |____.gitignore\n| | |____utils.py\n| | |____app.py\n| | |____data\n| | | |____identification_labels.md\n| | | |____cold_weather_best_practices.md\n| | | |____installing_phone_charging_cable.md\n.......\n| | | |____troubleshooting_alerts_p1.md\n| | | |____track_mode.md\n| | |____assets\n| | | |____icon-tesla-48x48.png\n|____README.md\n|____.gitignore\n```"
    },
    {
      "name": "michaelromagne/advent-of-code-submissions-2023",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/44051774?s=40&v=4",
      "owner": "michaelromagne",
      "repo_name": "advent-of-code-submissions-2023",
      "description": "Advent of Code - Open Source by Zilliz",
      "homepage": null,
      "language": "HTML",
      "created_at": "2023-12-03T16:44:13Z",
      "updated_at": "2024-06-26T23:58:42Z",
      "topics": [],
      "readme": "# Advent of Code for Open Source\n\n![christmas](img/santa_boat.png)\n\nDiscovery of open source libraries, mostly related to LLMs. [AOC organized by Zilliz](https://zilliz.com/blog/advent-of-code-for-open-source).\n\nBased on the libraries I discovered in December, I wrote a [blog post](https://medium.com/@michael.romagne/a-gentle-journey-to-llmops-zilliz-advent-of-code-293fe166926a) and created a tool map with the most interesting projects I know so far regarding LLMOps :\n\n![tool_map](img/mlops_tools.png)\n\n\n### Details of tools explored\n\n- Day 1 : [Milvus](https://github.com/milvus-io/milvus)\n- Day 2 : [FiftyOne](https://github.com/voxel51/fiftyone)\n- Day 3 : [Quivr](https://github.com/StanGirard/quivr)\n- Day 4 : [Haystack](https://haystack.deepset.ai/)\n- Day 5 : [Proton](https://docs.timeplus.com/proton)\n- Day 6 : [YData-synthetic](https://ydata.ai/)\n- Day 7 : [Apache Flink](https://flink.apache.org/)\n- Day 8 : [Langchain.rb](https://github.com/andreibondarev/langchainrb)\n- Day 9 : [Flyte](https://github.com/flyteorg/flyte)\n- Day 10 : [DVC 💚](https://github.com/iterative/dvc)\n- Day 11 : [Phoenix - Arize AI](https://github.com/Arize-ai/phoenix)\n- Day 12 : [Trulens](https://github.com/truera/trulens)\n- Day 13 : [OpenLLM](https://github.com/bentoml/OpenLLM)\n- Day 14: [Label Studio](https://github.com/HumanSignal/label-studio/)\n- Day 15: [Llama Index](https://github.com/run-llama/llama_index)\n- Day 16: [LLMWare](https://github.com/llmware-ai/llmware)\n- Day 17: [Determined AI](https://github.com/determined-ai/determined)\n- Day 18: [Apache Paimon](https://github.com/apache/incubator-paimon)\n- Day 19: [Vectorflow](https://github.com/dgarnitz/vectorflow)\n- Day 20 : [Pachyderm](https://github.com/pachyderm/pachyderm)\n- Day 21: [GPTCache](https://github.com/zilliztech/gptcache)\n- Day 22: [Ray](https://github.com/ray-project/ray)\n- Day 23: [SuperGradients](https://github.com/Deci-AI/super-gradients)\n- Day 24: [Temporian](https://github.com/google/temporian)\n\n- Additional tools : [Giskard AI](https://github.com/Giskard-AI/giskard)\n"
    }
  ],
  "total_dependents_number": 580,
  "public_dependents_number": 580,
  "private_dependents_number": -580,
  "public_dependents_stars": 389,
  "badges": {
    "total_doc_url": "[![Generated by github-dependents-info](https://img.shields.io/static/v1?label=Used%20by&message=580&color=informational&logo=slickpic)](https://github.com/nvuillam/github-dependents-info)",
    "total": "[![Generated by github-dependents-info](https://img.shields.io/static/v1?label=Used%20by&message=580&color=informational&logo=slickpic)](https://github.com/deepset-ai/Haystack/network/dependents)",
    "public": "[![Generated by github-dependents-info](https://img.shields.io/static/v1?label=Used%20by%20(public)&message=580&color=informational&logo=slickpic)](https://github.com/deepset-ai/Haystack/network/dependents)",
    "private": "[![Generated by github-dependents-info](https://img.shields.io/static/v1?label=Used%20by%20(private)&message=-580&color=informational&logo=slickpic)](https://github.com/deepset-ai/Haystack/network/dependents)",
    "stars": "[![Generated by github-dependents-info](https://img.shields.io/static/v1?label=Used%20by%20(stars)&message=389&color=informational&logo=slickpic)](https://github.com/deepset-ai/Haystack/network/dependents)"
  },
  "packages": [
    [
      {
        "id": "UGFja2FnZS0zNjMxNDIzNDYy",
        "name": "haystack-ai",
        "url": "https://github.com/deepset-ai/Haystack/network/dependents?package_id=UGFja2FnZS0zNjMxNDIzNDYy",
        "public_dependent_stars": 367,
        "public_dependents": [
          {
            "name": "raga-ai-hub/RagaAI-Catalyst",
            "stars": 16170,
            "img": "https://avatars.githubusercontent.com/u/161833182?s=40&v=4",
            "owner": "raga-ai-hub",
            "repo_name": "RagaAI-Catalyst"
          },
          {
            "name": "explodinggradients/ragas",
            "stars": 8907,
            "img": "https://avatars.githubusercontent.com/u/122604797?s=40&v=4",
            "owner": "explodinggradients",
            "repo_name": "ragas"
          },
          {
            "name": "Canner/WrenAI",
            "stars": 7582,
            "img": "https://avatars.githubusercontent.com/u/7250217?s=40&v=4",
            "owner": "Canner",
            "repo_name": "WrenAI"
          },
          {
            "name": "comet-ml/opik",
            "stars": 6705,
            "img": "https://avatars.githubusercontent.com/u/31487821?s=40&v=4",
            "owner": "comet-ml",
            "repo_name": "opik"
          },
          {
            "name": "traceloop/openllmetry",
            "stars": 5691,
            "img": "https://avatars.githubusercontent.com/u/125419530?s=40&v=4",
            "owner": "traceloop",
            "repo_name": "openllmetry"
          },
          {
            "name": "posit-dev/positron",
            "stars": 3176,
            "img": "https://avatars.githubusercontent.com/u/107264312?s=40&v=4",
            "owner": "posit-dev",
            "repo_name": "positron"
          },
          {
            "name": "instructlab/instructlab",
            "stars": 1250,
            "img": "https://avatars.githubusercontent.com/u/160199024?s=40&v=4",
            "owner": "instructlab",
            "repo_name": "instructlab"
          },
          {
            "name": "aws-samples/amazon-bedrock-samples",
            "stars": 987,
            "img": "https://avatars.githubusercontent.com/u/8931462?s=40&v=4",
            "owner": "aws-samples",
            "repo_name": "amazon-bedrock-samples"
          },
          {
            "name": "TilmanGriesel/chipper",
            "stars": 442,
            "img": "https://avatars.githubusercontent.com/u/1132745?s=40&v=4",
            "owner": "TilmanGriesel",
            "repo_name": "chipper"
          },
          {
            "name": "Arize-ai/openinference",
            "stars": 385,
            "img": "https://avatars.githubusercontent.com/u/59858760?s=40&v=4",
            "owner": "Arize-ai",
            "repo_name": "openinference"
          },
          {
            "name": "jlonge4/local_llama",
            "stars": 267,
            "img": "https://avatars.githubusercontent.com/u/91354480?s=40&v=4",
            "owner": "jlonge4",
            "repo_name": "local_llama"
          },
          {
            "name": "deepset-ai/haystack-core-integrations",
            "stars": 144,
            "img": "https://avatars.githubusercontent.com/u/51827949?s=40&v=4",
            "owner": "deepset-ai",
            "repo_name": "haystack-core-integrations"
          },
          {
            "name": "jlonge4/mychatGPT",
            "stars": 139,
            "img": "https://avatars.githubusercontent.com/u/91354480?s=40&v=4",
            "owner": "jlonge4",
            "repo_name": "mychatGPT"
          },
          {
            "name": "opea-project/GenAIComps",
            "stars": 137,
            "img": "https://avatars.githubusercontent.com/u/165422283?s=40&v=4",
            "owner": "opea-project",
            "repo_name": "GenAIComps"
          },
          {
            "name": "teremterem/MiniAgents",
            "stars": 125,
            "img": "https://avatars.githubusercontent.com/u/5508002?s=40&v=4",
            "owner": "teremterem",
            "repo_name": "MiniAgents"
          },
          {
            "name": "CodingWithLewis/MemeGenerator",
            "stars": 104,
            "img": "https://avatars.githubusercontent.com/u/142752183?s=40&v=4",
            "owner": "CodingWithLewis",
            "repo_name": "MemeGenerator"
          },
          {
            "name": "parkervg/blendsql",
            "stars": 95,
            "img": "https://avatars.githubusercontent.com/u/44219290?s=40&v=4",
            "owner": "parkervg",
            "repo_name": "blendsql"
          },
          {
            "name": "AIAnytime/Haystack-and-Mistral-7B-RAG-Implementation",
            "stars": 79,
            "img": "https://avatars.githubusercontent.com/u/30049737?s=40&v=4",
            "owner": "AIAnytime",
            "repo_name": "Haystack-and-Mistral-7B-RAG-Implementation"
          },
          {
            "name": "deepset-ai/hayhooks",
            "stars": 74,
            "img": "https://avatars.githubusercontent.com/u/51827949?s=40&v=4",
            "owner": "deepset-ai",
            "repo_name": "hayhooks"
          },
          {
            "name": "tuhinsharma121/ai-playground",
            "stars": 69,
            "img": "https://avatars.githubusercontent.com/u/6801780?s=40&v=4",
            "owner": "tuhinsharma121",
            "repo_name": "ai-playground"
          },
          {
            "name": "katanaml/llm-rag-invoice-cpu",
            "stars": 68,
            "img": "https://avatars.githubusercontent.com/u/49202856?s=40&v=4",
            "owner": "katanaml",
            "repo_name": "llm-rag-invoice-cpu"
          },
          {
            "name": "anakin87/autoquizzer",
            "stars": 62,
            "img": "https://avatars.githubusercontent.com/u/44616784?s=40&v=4",
            "owner": "anakin87",
            "repo_name": "autoquizzer"
          },
          {
            "name": "dzlab/deeplearning.ai",
            "stars": 58,
            "img": "https://avatars.githubusercontent.com/u/1645304?s=40&v=4",
            "owner": "dzlab",
            "repo_name": "deeplearning.ai"
          },
          {
            "name": "NJUDeepEngine/llm-course-lecture",
            "stars": 50,
            "img": "https://avatars.githubusercontent.com/u/184465282?s=40&v=4",
            "owner": "NJUDeepEngine",
            "repo_name": "llm-course-lecture"
          },
          {
            "name": "mobiusml/aana_sdk",
            "stars": 47,
            "img": "https://avatars.githubusercontent.com/u/34684912?s=40&v=4",
            "owner": "mobiusml",
            "repo_name": "aana_sdk"
          },
          {
            "name": "Ransaka/ai-agents-with-llama3",
            "stars": 46,
            "img": "https://avatars.githubusercontent.com/u/48125060?s=40&v=4",
            "owner": "Ransaka",
            "repo_name": "ai-agents-with-llama3"
          },
          {
            "name": "deepset-ai/haystack-experimental",
            "stars": 44,
            "img": "https://avatars.githubusercontent.com/u/51827949?s=40&v=4",
            "owner": "deepset-ai",
            "repo_name": "haystack-experimental"
          },
          {
            "name": "TuanaCelik/should-i-follow",
            "stars": 44,
            "img": "https://avatars.githubusercontent.com/u/15802862?s=40&v=4",
            "owner": "TuanaCelik",
            "repo_name": "should-i-follow"
          },
          {
            "name": "zazencodes/zazencodes-season-1",
            "stars": 34,
            "img": "https://avatars.githubusercontent.com/u/13775567?s=40&v=4",
            "owner": "zazencodes",
            "repo_name": "zazencodes-season-1"
          },
          {
            "name": "ploomber/doc",
            "stars": 34,
            "img": "https://avatars.githubusercontent.com/u/60114551?s=40&v=4",
            "owner": "ploomber",
            "repo_name": "doc"
          },
          {
            "name": "gilad-rubin/modular-rag",
            "stars": 32,
            "img": "https://avatars.githubusercontent.com/u/6134299?s=40&v=4",
            "owner": "gilad-rubin",
            "repo_name": "modular-rag"
          },
          {
            "name": "deepset-ai/rag-with-nvidia-nims",
            "stars": 31,
            "img": "https://avatars.githubusercontent.com/u/51827949?s=40&v=4",
            "owner": "deepset-ai",
            "repo_name": "rag-with-nvidia-nims"
          },
          {
            "name": "bcgov/mds",
            "stars": 28,
            "img": "https://avatars.githubusercontent.com/u/916280?s=40&v=4",
            "owner": "bcgov",
            "repo_name": "mds"
          },
          {
            "name": "yip-kl/llm_function_calling_demo",
            "stars": 25,
            "img": "https://avatars.githubusercontent.com/u/85790832?s=40&v=4",
            "owner": "yip-kl",
            "repo_name": "llm_function_calling_demo"
          },
          {
            "name": "prosto/neo4j-haystack",
            "stars": 24,
            "img": "https://avatars.githubusercontent.com/u/706952?s=40&v=4",
            "owner": "prosto",
            "repo_name": "neo4j-haystack"
          },
          {
            "name": "NanGePlus/HaystackTest",
            "stars": 23,
            "img": "https://avatars.githubusercontent.com/u/178549638?s=40&v=4",
            "owner": "NanGePlus",
            "repo_name": "HaystackTest"
          },
          {
            "name": "AIAnytime/PubMed-Healthcare-Chatbot",
            "stars": 22,
            "img": "https://avatars.githubusercontent.com/u/30049737?s=40&v=4",
            "owner": "AIAnytime",
            "repo_name": "PubMed-Healthcare-Chatbot"
          },
          {
            "name": "wsargent/groundedllm",
            "stars": 21,
            "img": "https://avatars.githubusercontent.com/u/71236?s=40&v=4",
            "owner": "wsargent",
            "repo_name": "groundedllm"
          },
          {
            "name": "TuanaCelik/hackernews-summaries",
            "stars": 21,
            "img": "https://avatars.githubusercontent.com/u/15802862?s=40&v=4",
            "owner": "TuanaCelik",
            "repo_name": "hackernews-summaries"
          },
          {
            "name": "aws-samples/rag-workshop-amazon-bedrock-knowledge-bases",
            "stars": 20,
            "img": "https://avatars.githubusercontent.com/u/8931462?s=40&v=4",
            "owner": "aws-samples",
            "repo_name": "rag-workshop-amazon-bedrock-knowledge-bases"
          },
          {
            "name": "cmin764/cmiN",
            "stars": 20,
            "img": "https://avatars.githubusercontent.com/u/709053?s=40&v=4",
            "owner": "cmin764",
            "repo_name": "cmiN"
          },
          {
            "name": "danilop/oss-for-generative-ai",
            "stars": 20,
            "img": "https://avatars.githubusercontent.com/u/1550395?s=40&v=4",
            "owner": "danilop",
            "repo_name": "oss-for-generative-ai"
          },
          {
            "name": "Hoanganhvu123/ShoppingGPT",
            "stars": 20,
            "img": "https://avatars.githubusercontent.com/u/89352572?s=40&v=4",
            "owner": "Hoanganhvu123",
            "repo_name": "ShoppingGPT"
          },
          {
            "name": "docling-project/docling-haystack",
            "stars": 19,
            "img": "https://avatars.githubusercontent.com/u/188446108?s=40&v=4",
            "owner": "docling-project",
            "repo_name": "docling-haystack"
          },
          {
            "name": "waifu-lab/anyknowage",
            "stars": 19,
            "img": "https://avatars.githubusercontent.com/u/167647828?s=40&v=4",
            "owner": "waifu-lab",
            "repo_name": "anyknowage"
          },
          {
            "name": "prosto/ray-haystack",
            "stars": 18,
            "img": "https://avatars.githubusercontent.com/u/706952?s=40&v=4",
            "owner": "prosto",
            "repo_name": "ray-haystack"
          },
          {
            "name": "vanrohan/ai-bookmark-organizer",
            "stars": 18,
            "img": "https://avatars.githubusercontent.com/u/22074683?s=40&v=4",
            "owner": "vanrohan",
            "repo_name": "ai-bookmark-organizer"
          },
          {
            "name": "Not-Diamond/notdiamond-examples",
            "stars": 17,
            "img": "https://avatars.githubusercontent.com/u/136373660?s=40&v=4",
            "owner": "Not-Diamond",
            "repo_name": "notdiamond-examples"
          },
          {
            "name": "Guest400123064/bbm25-haystack",
            "stars": 17,
            "img": "https://avatars.githubusercontent.com/u/31812718?s=40&v=4",
            "owner": "Guest400123064",
            "repo_name": "bbm25-haystack"
          },
          {
            "name": "PacktPublishing/Building-Natural-Language-Pipelines",
            "stars": 17,
            "img": "https://avatars.githubusercontent.com/u/10974906?s=40&v=4",
            "owner": "PacktPublishing",
            "repo_name": "Building-Natural-Language-Pipelines"
          },
          {
            "name": "jfagan/llm-rag-invoice-haystack",
            "stars": 16,
            "img": "https://avatars.githubusercontent.com/u/133829?s=40&v=4",
            "owner": "jfagan",
            "repo_name": "llm-rag-invoice-haystack"
          },
          {
            "name": "milvus-io/milvus-haystack",
            "stars": 16,
            "img": "https://avatars.githubusercontent.com/u/51735404?s=40&v=4",
            "owner": "milvus-io",
            "repo_name": "milvus-haystack"
          },
          {
            "name": "TuanaCelik/unstructuredio-haystack",
            "stars": 16,
            "img": "https://avatars.githubusercontent.com/u/15802862?s=40&v=4",
            "owner": "TuanaCelik",
            "repo_name": "unstructuredio-haystack"
          },
          {
            "name": "Mouez-Yazidi/WhisperMesh",
            "stars": 14,
            "img": "https://avatars.githubusercontent.com/u/56160970?s=40&v=4",
            "owner": "Mouez-Yazidi",
            "repo_name": "WhisperMesh"
          },
          {
            "name": "KBVE/kbve",
            "stars": 12,
            "img": "https://avatars.githubusercontent.com/u/5571997?s=40&v=4",
            "owner": "KBVE",
            "repo_name": "kbve"
          },
          {
            "name": "fahminlb33/bogor-house-price",
            "stars": 11,
            "img": "https://avatars.githubusercontent.com/u/8360880?s=40&v=4",
            "owner": "fahminlb33",
            "repo_name": "bogor-house-price"
          },
          {
            "name": "EdAbati/outlines-haystack",
            "stars": 11,
            "img": "https://avatars.githubusercontent.com/u/29585319?s=40&v=4",
            "owner": "EdAbati",
            "repo_name": "outlines-haystack"
          },
          {
            "name": "dkm1006/news-graph-rag",
            "stars": 11,
            "img": "https://avatars.githubusercontent.com/u/23480813?s=40&v=4",
            "owner": "dkm1006",
            "repo_name": "news-graph-rag"
          },
          {
            "name": "CdC-SI/ZAS-EAK-CopilotGPT",
            "stars": 11,
            "img": "https://avatars.githubusercontent.com/u/150324891?s=40&v=4",
            "owner": "CdC-SI",
            "repo_name": "ZAS-EAK-CopilotGPT"
          },
          {
            "name": "bytewax/real-time-rag-workshop",
            "stars": 11,
            "img": "https://avatars.githubusercontent.com/u/72483929?s=40&v=4",
            "owner": "bytewax",
            "repo_name": "real-time-rag-workshop"
          },
          {
            "name": "NLPerWS/KMatrix",
            "stars": 10,
            "img": "https://avatars.githubusercontent.com/u/114089542?s=40&v=4",
            "owner": "NLPerWS",
            "repo_name": "KMatrix"
          },
          {
            "name": "LLM-Projects/haystack-book",
            "stars": 10,
            "img": "https://avatars.githubusercontent.com/u/140273596?s=40&v=4",
            "owner": "LLM-Projects",
            "repo_name": "haystack-book"
          },
          {
            "name": "AccessibleAI/ailibrary",
            "stars": 9,
            "img": "https://avatars.githubusercontent.com/u/13865104?s=40&v=4",
            "owner": "AccessibleAI",
            "repo_name": "ailibrary"
          },
          {
            "name": "sunnysavita10/RAG-With-Haystack-MistralAI-Pinecone",
            "stars": 9,
            "img": "https://avatars.githubusercontent.com/u/56354186?s=40&v=4",
            "owner": "sunnysavita10",
            "repo_name": "RAG-With-Haystack-MistralAI-Pinecone"
          },
          {
            "name": "mixedbread-ai/mixedbread-ai-haystack",
            "stars": 9,
            "img": "https://avatars.githubusercontent.com/u/153227975?s=40&v=4",
            "owner": "mixedbread-ai",
            "repo_name": "mixedbread-ai-haystack"
          },
          {
            "name": "AssemblyAI/assemblyai-haystack",
            "stars": 9,
            "img": "https://avatars.githubusercontent.com/u/24515738?s=40&v=4",
            "owner": "AssemblyAI",
            "repo_name": "assemblyai-haystack"
          },
          {
            "name": "Rightpoint/ai-discipline-ollama-rag",
            "stars": 8,
            "img": "https://avatars.githubusercontent.com/u/621479?s=40&v=4",
            "owner": "Rightpoint",
            "repo_name": "ai-discipline-ollama-rag"
          },
          {
            "name": "masci/chroma-haystack",
            "stars": 8,
            "img": "https://avatars.githubusercontent.com/u/7241?s=40&v=4",
            "owner": "masci",
            "repo_name": "chroma-haystack"
          },
          {
            "name": "deepset-ai/document-store",
            "stars": 8,
            "img": "https://avatars.githubusercontent.com/u/51827949?s=40&v=4",
            "owner": "deepset-ai",
            "repo_name": "document-store"
          },
          {
            "name": "000haoji/deep-student",
            "stars": 7,
            "img": "https://avatars.githubusercontent.com/u/54060703?s=40&v=4",
            "owner": "000haoji",
            "repo_name": "deep-student"
          },
          {
            "name": "RaamRaam/GrowthSchoolRAGWorkshop",
            "stars": 7,
            "img": "https://avatars.githubusercontent.com/u/12892059?s=40&v=4",
            "owner": "RaamRaam",
            "repo_name": "GrowthSchoolRAGWorkshop"
          },
          {
            "name": "pkweitai/hummingbotAI",
            "stars": 7,
            "img": "https://avatars.githubusercontent.com/u/157328927?s=40&v=4",
            "owner": "pkweitai",
            "repo_name": "hummingbotAI"
          },
          {
            "name": "renuka010/Mistral-Telegram-Bot",
            "stars": 7,
            "img": "https://avatars.githubusercontent.com/u/72569696?s=40&v=4",
            "owner": "renuka010",
            "repo_name": "Mistral-Telegram-Bot"
          },
          {
            "name": "afontana1/Data-Engineering",
            "stars": 6,
            "img": "https://avatars.githubusercontent.com/u/46588040?s=40&v=4",
            "owner": "afontana1",
            "repo_name": "Data-Engineering"
          },
          {
            "name": "ptbdnr/snippets",
            "stars": 6,
            "img": "https://avatars.githubusercontent.com/u/32688266?s=40&v=4",
            "owner": "ptbdnr",
            "repo_name": "snippets"
          },
          {
            "name": "avnlp/llm-blender",
            "stars": 6,
            "img": "https://avatars.githubusercontent.com/u/123800833?s=40&v=4",
            "owner": "avnlp",
            "repo_name": "llm-blender"
          },
          {
            "name": "relari-ai/examples",
            "stars": 6,
            "img": "https://avatars.githubusercontent.com/u/135984758?s=40&v=4",
            "owner": "relari-ai",
            "repo_name": "examples"
          },
          {
            "name": "GovML/retriever",
            "stars": 6,
            "img": "https://avatars.githubusercontent.com/u/167251131?s=40&v=4",
            "owner": "GovML",
            "repo_name": "retriever"
          },
          {
            "name": "deepset-ai/haystack-rest-api",
            "stars": 6,
            "img": "https://avatars.githubusercontent.com/u/51827949?s=40&v=4",
            "owner": "deepset-ai",
            "repo_name": "haystack-rest-api"
          },
          {
            "name": "silvanocerza/search-all-the-docs",
            "stars": 6,
            "img": "https://avatars.githubusercontent.com/u/3314350?s=40&v=4",
            "owner": "silvanocerza",
            "repo_name": "search-all-the-docs"
          },
          {
            "name": "deepset-ai/github-agent",
            "stars": 5,
            "img": "https://avatars.githubusercontent.com/u/51827949?s=40&v=4",
            "owner": "deepset-ai",
            "repo_name": "github-agent"
          },
          {
            "name": "jbcodeforce/ML-studies",
            "stars": 5,
            "img": "https://avatars.githubusercontent.com/u/8050643?s=40&v=4",
            "owner": "jbcodeforce",
            "repo_name": "ML-studies"
          },
          {
            "name": "Health-Informatics-UoN/lettuce",
            "stars": 5,
            "img": "https://avatars.githubusercontent.com/u/156185347?s=40&v=4",
            "owner": "Health-Informatics-UoN",
            "repo_name": "lettuce"
          },
          {
            "name": "Pash10g/allcr-ai",
            "stars": 5,
            "img": "https://avatars.githubusercontent.com/u/16643200?s=40&v=4",
            "owner": "Pash10g",
            "repo_name": "allcr-ai"
          },
          {
            "name": "marqo-ai/marqo-haystack",
            "stars": 5,
            "img": "https://avatars.githubusercontent.com/u/103185353?s=40&v=4",
            "owner": "marqo-ai",
            "repo_name": "marqo-haystack"
          },
          {
            "name": "PacktPublishing/Building-Neo4j-Powered-Applications-with-LLMs",
            "stars": 4,
            "img": "https://avatars.githubusercontent.com/u/10974906?s=40&v=4",
            "owner": "PacktPublishing",
            "repo_name": "Building-Neo4j-Powered-Applications-with-LLMs"
          },
          {
            "name": "2456868764/LiteRAG",
            "stars": 4,
            "img": "https://avatars.githubusercontent.com/u/108045855?s=40&v=4",
            "owner": "2456868764",
            "repo_name": "LiteRAG"
          },
          {
            "name": "podolskyDavid/h-genai",
            "stars": 4,
            "img": "https://avatars.githubusercontent.com/u/63861055?s=40&v=4",
            "owner": "podolskyDavid",
            "repo_name": "h-genai"
          },
          {
            "name": "FloTeu/mr-injector",
            "stars": 4,
            "img": "https://avatars.githubusercontent.com/u/37250993?s=40&v=4",
            "owner": "FloTeu",
            "repo_name": "mr-injector"
          },
          {
            "name": "deepset-ai/haystack-streamlit-app",
            "stars": 4,
            "img": "https://avatars.githubusercontent.com/u/51827949?s=40&v=4",
            "owner": "deepset-ai",
            "repo_name": "haystack-streamlit-app"
          },
          {
            "name": "tituslhy/literate-octo-tribble",
            "stars": 4,
            "img": "https://avatars.githubusercontent.com/u/7207877?s=40&v=4",
            "owner": "tituslhy",
            "repo_name": "literate-octo-tribble"
          },
          {
            "name": "dribia/deepl-haystack",
            "stars": 4,
            "img": "https://avatars.githubusercontent.com/u/41189616?s=40&v=4",
            "owner": "dribia",
            "repo_name": "deepl-haystack"
          },
          {
            "name": "xtreamsrl/genai-for-engineers-class",
            "stars": 4,
            "img": "https://avatars.githubusercontent.com/u/38501645?s=40&v=4",
            "owner": "xtreamsrl",
            "repo_name": "genai-for-engineers-class"
          },
          {
            "name": "davidberenstein1957/paper-prowler",
            "stars": 4,
            "img": "https://avatars.githubusercontent.com/u/25269220?s=40&v=4",
            "owner": "davidberenstein1957",
            "repo_name": "paper-prowler"
          },
          {
            "name": "katimanova/advanced_rag",
            "stars": 4,
            "img": "https://avatars.githubusercontent.com/u/73062632?s=40&v=4",
            "owner": "katimanova",
            "repo_name": "advanced_rag"
          },
          {
            "name": "domenicocinque/web-rag",
            "stars": 4,
            "img": "https://avatars.githubusercontent.com/u/72546227?s=40&v=4",
            "owner": "domenicocinque",
            "repo_name": "web-rag"
          },
          {
            "name": "awinml/llama-cpp-haystack",
            "stars": 4,
            "img": "https://avatars.githubusercontent.com/u/97467100?s=40&v=4",
            "owner": "awinml",
            "repo_name": "llama-cpp-haystack"
          },
          {
            "name": "awinml/voyage-embedders-haystack",
            "stars": 4,
            "img": "https://avatars.githubusercontent.com/u/97467100?s=40&v=4",
            "owner": "awinml",
            "repo_name": "voyage-embedders-haystack"
          },
          {
            "name": "amphilagus/Haystack-RAG_Assistant",
            "stars": 3,
            "img": "https://avatars.githubusercontent.com/u/82392488?s=40&v=4",
            "owner": "amphilagus",
            "repo_name": "Haystack-RAG_Assistant"
          },
          {
            "name": "avnlp/dataloaders",
            "stars": 3,
            "img": "https://avatars.githubusercontent.com/u/123800833?s=40&v=4",
            "owner": "avnlp",
            "repo_name": "dataloaders"
          },
          {
            "name": "malte-b/gaming_copilot",
            "stars": 3,
            "img": "https://avatars.githubusercontent.com/u/27922183?s=40&v=4",
            "owner": "malte-b",
            "repo_name": "gaming_copilot"
          },
          {
            "name": "avnlp/rag-pipelines",
            "stars": 3,
            "img": "https://avatars.githubusercontent.com/u/123800833?s=40&v=4",
            "owner": "avnlp",
            "repo_name": "rag-pipelines"
          },
          {
            "name": "analitiq-ai/analitiq",
            "stars": 3,
            "img": "https://avatars.githubusercontent.com/u/165009794?s=40&v=4",
            "owner": "analitiq-ai",
            "repo_name": "analitiq"
          },
          {
            "name": "shxntanu/lesa",
            "stars": 3,
            "img": "https://avatars.githubusercontent.com/u/97496261?s=40&v=4",
            "owner": "shxntanu",
            "repo_name": "lesa"
          },
          {
            "name": "GitXpresso/CLODSH",
            "stars": 3,
            "img": "https://avatars.githubusercontent.com/u/126926699?s=40&v=4",
            "owner": "GitXpresso",
            "repo_name": "CLODSH"
          },
          {
            "name": "nlpkeg/KMatrix",
            "stars": 3,
            "img": "https://avatars.githubusercontent.com/u/186663451?s=40&v=4",
            "owner": "nlpkeg",
            "repo_name": "KMatrix"
          },
          {
            "name": "alphavector/all",
            "stars": 3,
            "img": "https://avatars.githubusercontent.com/u/11805788?s=40&v=4",
            "owner": "alphavector",
            "repo_name": "all"
          },
          {
            "name": "KushagraSikka/RAG_Microservice-",
            "stars": 3,
            "img": "https://avatars.githubusercontent.com/u/54061496?s=40&v=4",
            "owner": "KushagraSikka",
            "repo_name": "RAG_Microservice-"
          },
          {
            "name": "GivAlz/duckduckgo-api-haystack",
            "stars": 3,
            "img": "https://avatars.githubusercontent.com/u/9434511?s=40&v=4",
            "owner": "GivAlz",
            "repo_name": "duckduckgo-api-haystack"
          },
          {
            "name": "Hoanganhvu123/BookingGPT",
            "stars": 3,
            "img": "https://avatars.githubusercontent.com/u/89352572?s=40&v=4",
            "owner": "Hoanganhvu123",
            "repo_name": "BookingGPT"
          },
          {
            "name": "ArmanTunga/5-levels-of-building-chatbot-apps",
            "stars": 3,
            "img": "https://avatars.githubusercontent.com/u/30978203?s=40&v=4",
            "owner": "ArmanTunga",
            "repo_name": "5-levels-of-building-chatbot-apps"
          },
          {
            "name": "hemhemoh/DocLing",
            "stars": 3,
            "img": "https://avatars.githubusercontent.com/u/70777991?s=40&v=4",
            "owner": "hemhemoh",
            "repo_name": "DocLing"
          },
          {
            "name": "apify/apify-haystack",
            "stars": 3,
            "img": "https://avatars.githubusercontent.com/u/24586296?s=40&v=4",
            "owner": "apify",
            "repo_name": "apify-haystack"
          },
          {
            "name": "baldpanda/pydata-manchester-talk",
            "stars": 3,
            "img": "https://avatars.githubusercontent.com/u/37364932?s=40&v=4",
            "owner": "baldpanda",
            "repo_name": "pydata-manchester-talk"
          },
          {
            "name": "Lyn4ever29/pipy_server",
            "stars": 3,
            "img": "https://avatars.githubusercontent.com/u/25952589?s=40&v=4",
            "owner": "Lyn4ever29",
            "repo_name": "pipy_server"
          },
          {
            "name": "avnlp/rrf",
            "stars": 3,
            "img": "https://avatars.githubusercontent.com/u/123800833?s=40&v=4",
            "owner": "avnlp",
            "repo_name": "rrf"
          },
          {
            "name": "sausheong/programming_ai",
            "stars": 3,
            "img": "https://avatars.githubusercontent.com/u/5962?s=40&v=4",
            "owner": "sausheong",
            "repo_name": "programming_ai"
          },
          {
            "name": "PagalavanPagal66/Medical_chatbot",
            "stars": 3,
            "img": "https://avatars.githubusercontent.com/u/125946240?s=40&v=4",
            "owner": "PagalavanPagal66",
            "repo_name": "Medical_chatbot"
          },
          {
            "name": "bytewax/real-time-job-posting",
            "stars": 3,
            "img": "https://avatars.githubusercontent.com/u/72483929?s=40&v=4",
            "owner": "bytewax",
            "repo_name": "real-time-job-posting"
          },
          {
            "name": "EdAbati/dataframes-haystack",
            "stars": 3,
            "img": "https://avatars.githubusercontent.com/u/29585319?s=40&v=4",
            "owner": "EdAbati",
            "repo_name": "dataframes-haystack"
          },
          {
            "name": "contextco/context-haystack",
            "stars": 3,
            "img": "https://avatars.githubusercontent.com/u/133496075?s=40&v=4",
            "owner": "contextco",
            "repo_name": "context-haystack"
          },
          {
            "name": "rti/gbnc",
            "stars": 3,
            "img": "https://avatars.githubusercontent.com/u/80712?s=40&v=4",
            "owner": "rti",
            "repo_name": "gbnc"
          },
          {
            "name": "dlt-hub/llm_adapter",
            "stars": 3,
            "img": "https://avatars.githubusercontent.com/u/89419010?s=40&v=4",
            "owner": "dlt-hub",
            "repo_name": "llm_adapter"
          },
          {
            "name": "jwiegley/rag-server",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/8460?s=40&v=4",
            "owner": "jwiegley",
            "repo_name": "rag-server"
          },
          {
            "name": "Open-Community-Building/hackathon-council-analytics",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/173253715?s=40&v=4",
            "owner": "Open-Community-Building",
            "repo_name": "hackathon-council-analytics"
          },
          {
            "name": "oryx1729/mcp-haystack",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/78848855?s=40&v=4",
            "owner": "oryx1729",
            "repo_name": "mcp-haystack"
          },
          {
            "name": "ammar-s847/CV-Drone-Detection",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/40363389?s=40&v=4",
            "owner": "ammar-s847",
            "repo_name": "CV-Drone-Detection"
          },
          {
            "name": "avnlp/rankers",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/123800833?s=40&v=4",
            "owner": "avnlp",
            "repo_name": "rankers"
          },
          {
            "name": "lichingngamba/talk_to_page",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/63799239?s=40&v=4",
            "owner": "lichingngamba",
            "repo_name": "talk_to_page"
          },
          {
            "name": "superleesa/haystack-pydantic",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/88019950?s=40&v=4",
            "owner": "superleesa",
            "repo_name": "haystack-pydantic"
          },
          {
            "name": "rajs1006/AIDataAnalysisAgent",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/39014867?s=40&v=4",
            "owner": "rajs1006",
            "repo_name": "AIDataAnalysisAgent"
          },
          {
            "name": "simiyu-dess/WrenAI",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/38722733?s=40&v=4",
            "owner": "simiyu-dess",
            "repo_name": "WrenAI"
          },
          {
            "name": "avnlp/vectordb",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/123800833?s=40&v=4",
            "owner": "avnlp",
            "repo_name": "vectordb"
          },
          {
            "name": "BlutzerZ/chatbot-akademik",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/29569667?s=40&v=4",
            "owner": "BlutzerZ",
            "repo_name": "chatbot-akademik"
          },
          {
            "name": "huxiaolongyin/Chat2RAG",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/92241411?s=40&v=4",
            "owner": "huxiaolongyin",
            "repo_name": "Chat2RAG"
          },
          {
            "name": "codingtodeath/REA",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/101001793?s=40&v=4",
            "owner": "codingtodeath",
            "repo_name": "REA"
          },
          {
            "name": "andychert/elevenlabs-haystack",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/184323448?s=40&v=4",
            "owner": "andychert",
            "repo_name": "elevenlabs-haystack"
          },
          {
            "name": "deepset-ai/dc-custom-component-template",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/51827949?s=40&v=4",
            "owner": "deepset-ai",
            "repo_name": "dc-custom-component-template"
          },
          {
            "name": "mikhail-w/bonsai",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/119347826?s=40&v=4",
            "owner": "mikhail-w",
            "repo_name": "bonsai"
          },
          {
            "name": "Couchbase-Ecosystem/couchbase-haystack",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/167254978?s=40&v=4",
            "owner": "Couchbase-Ecosystem",
            "repo_name": "couchbase-haystack"
          },
          {
            "name": "bytewax/bytewax-azure-ai-search",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/72483929?s=40&v=4",
            "owner": "bytewax",
            "repo_name": "bytewax-azure-ai-search"
          },
          {
            "name": "apuslabs/ao-competition-playground-service",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/146177720?s=40&v=4",
            "owner": "apuslabs",
            "repo_name": "ao-competition-playground-service"
          },
          {
            "name": "JANHMS/needle-haystack",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/45521680?s=40&v=4",
            "owner": "JANHMS",
            "repo_name": "needle-haystack"
          },
          {
            "name": "iceray00/LLM_RAG_Clanguage",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/108821718?s=40&v=4",
            "owner": "iceray00",
            "repo_name": "LLM_RAG_Clanguage"
          },
          {
            "name": "kyrillschmid/SEGym",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/12870793?s=40&v=4",
            "owner": "kyrillschmid",
            "repo_name": "SEGym"
          },
          {
            "name": "reaganlo/ai-safety",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/12738189?s=40&v=4",
            "owner": "reaganlo",
            "repo_name": "ai-safety"
          },
          {
            "name": "PhamTrinhDuc/retrieval-augmented-generation-with-Langchain",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/127647215?s=40&v=4",
            "owner": "PhamTrinhDuc",
            "repo_name": "retrieval-augmented-generation-with-Langchain"
          },
          {
            "name": "Blizarre/microProjects",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/66079?s=40&v=4",
            "owner": "Blizarre",
            "repo_name": "microProjects"
          },
          {
            "name": "Futyn-Maker/wb_project",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/54239175?s=40&v=4",
            "owner": "Futyn-Maker",
            "repo_name": "wb_project"
          },
          {
            "name": "Redna/GenerativeAgents",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/18450828?s=40&v=4",
            "owner": "Redna",
            "repo_name": "GenerativeAgents"
          },
          {
            "name": "elvanselvano/streamlit-whisper",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/49674061?s=40&v=4",
            "owner": "elvanselvano",
            "repo_name": "streamlit-whisper"
          },
          {
            "name": "prapooskur/SlugCourses",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/61389919?s=40&v=4",
            "owner": "prapooskur",
            "repo_name": "SlugCourses"
          },
          {
            "name": "alanmeeson/lancedb-haystack",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/10676989?s=40&v=4",
            "owner": "alanmeeson",
            "repo_name": "lancedb-haystack"
          },
          {
            "name": "NechbaMohammed/ai-chat-bot",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/108382119?s=40&v=4",
            "owner": "NechbaMohammed",
            "repo_name": "ai-chat-bot"
          },
          {
            "name": "Nikunj3masarani/DocAI",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/18481030?s=40&v=4",
            "owner": "Nikunj3masarani",
            "repo_name": "DocAI"
          },
          {
            "name": "exowanderer/WikidataChat",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/11221046?s=40&v=4",
            "owner": "exowanderer",
            "repo_name": "WikidataChat"
          },
          {
            "name": "alanmeeson/sqlite-haystack",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/10676989?s=40&v=4",
            "owner": "alanmeeson",
            "repo_name": "sqlite-haystack"
          },
          {
            "name": "longluu/Medical-QA-LLM",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/33553089?s=40&v=4",
            "owner": "longluu",
            "repo_name": "Medical-QA-LLM"
          },
          {
            "name": "sebastiaan-dev/haiki",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/50165934?s=40&v=4",
            "owner": "sebastiaan-dev",
            "repo_name": "haiki"
          },
          {
            "name": "ravipratap366/RAG-Mistral7B-Haystack-Weaviate-FastAPI",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/53921865?s=40&v=4",
            "owner": "ravipratap366",
            "repo_name": "RAG-Mistral7B-Haystack-Weaviate-FastAPI"
          },
          {
            "name": "AKJUS/opik",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/173074602?s=40&v=4",
            "owner": "AKJUS",
            "repo_name": "opik"
          },
          {
            "name": "opea-project/Haystack-OPEA",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/165422283?s=40&v=4",
            "owner": "opea-project",
            "repo_name": "Haystack-OPEA"
          },
          {
            "name": "andremmfaria/rexis",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/400238?s=40&v=4",
            "owner": "andremmfaria",
            "repo_name": "rexis"
          },
          {
            "name": "NoorFathima14/multimodal-chatbot",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/146946527?s=40&v=4",
            "owner": "NoorFathima14",
            "repo_name": "multimodal-chatbot"
          },
          {
            "name": "ernanhughes/mars",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/155574808?s=40&v=4",
            "owner": "ernanhughes",
            "repo_name": "mars"
          },
          {
            "name": "jenabesaman/LLM",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/94444692?s=40&v=4",
            "owner": "jenabesaman",
            "repo_name": "LLM"
          },
          {
            "name": "bigdata-org/agentic-rag",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/196103522?s=40&v=4",
            "owner": "bigdata-org",
            "repo_name": "agentic-rag"
          },
          {
            "name": "VCharrua/free-genai-bootcamp-2025",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/13697673?s=40&v=4",
            "owner": "VCharrua",
            "repo_name": "free-genai-bootcamp-2025"
          },
          {
            "name": "Jackela/mcp-academic-rag-system",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/27112787?s=40&v=4",
            "owner": "Jackela",
            "repo_name": "mcp-academic-rag-system"
          },
          {
            "name": "MuhammadAbdullah95/Hotel_Receptionist_AI_Agent",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/147532804?s=40&v=4",
            "owner": "MuhammadAbdullah95",
            "repo_name": "Hotel_Receptionist_AI_Agent"
          },
          {
            "name": "AdeptTechSolutions/aging-res",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/61491605?s=40&v=4",
            "owner": "AdeptTechSolutions",
            "repo_name": "aging-res"
          },
          {
            "name": "danielfleischer/haystack_opea",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/22022514?s=40&v=4",
            "owner": "danielfleischer",
            "repo_name": "haystack_opea"
          },
          {
            "name": "bigdata-org/data-extraction-and-standardization",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/196103522?s=40&v=4",
            "owner": "bigdata-org",
            "repo_name": "data-extraction-and-standardization"
          },
          {
            "name": "md-nobir-hasan/fastapi-personality-test",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/96966076?s=40&v=4",
            "owner": "md-nobir-hasan",
            "repo_name": "fastapi-personality-test"
          },
          {
            "name": "HarshNevse/RAG_Agent",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/110392719?s=40&v=4",
            "owner": "HarshNevse",
            "repo_name": "RAG_Agent"
          },
          {
            "name": "hek1412/JupyterHub_v1",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/183311011?s=40&v=4",
            "owner": "hek1412",
            "repo_name": "JupyterHub_v1"
          },
          {
            "name": "HuzaifaAnsari/Intelligent-Confluence-Knowledge-Assistant",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/70470083?s=40&v=4",
            "owner": "HuzaifaAnsari",
            "repo_name": "Intelligent-Confluence-Knowledge-Assistant"
          },
          {
            "name": "PixelWelt/ThalamOS",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/33161332?s=40&v=4",
            "owner": "PixelWelt",
            "repo_name": "ThalamOS"
          },
          {
            "name": "jazielloureiro/Coruja",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/69116273?s=40&v=4",
            "owner": "jazielloureiro",
            "repo_name": "Coruja"
          },
          {
            "name": "jbkoh/not-a-recruiter",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/1572627?s=40&v=4",
            "owner": "jbkoh",
            "repo_name": "not-a-recruiter"
          },
          {
            "name": "deepset-ai/opea-haystack-demo",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/51827949?s=40&v=4",
            "owner": "deepset-ai",
            "repo_name": "opea-haystack-demo"
          },
          {
            "name": "snjax/savant",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/1750575?s=40&v=4",
            "owner": "snjax",
            "repo_name": "savant"
          },
          {
            "name": "JackLeeJM/rag-medication-ner",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/49935126?s=40&v=4",
            "owner": "JackLeeJM",
            "repo_name": "rag-medication-ner"
          },
          {
            "name": "plog/learn_llm_slowly",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/26974?s=40&v=4",
            "owner": "plog",
            "repo_name": "learn_llm_slowly"
          },
          {
            "name": "mr-rakesh-ranjan/PDF_RAG_Haystack",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/90750228?s=40&v=4",
            "owner": "mr-rakesh-ranjan",
            "repo_name": "PDF_RAG_Haystack"
          },
          {
            "name": "Shrijeeth/QuickRAG",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/58306412?s=40&v=4",
            "owner": "Shrijeeth",
            "repo_name": "QuickRAG"
          },
          {
            "name": "jd-coderepos/chat-QA",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/15185300?s=40&v=4",
            "owner": "jd-coderepos",
            "repo_name": "chat-QA"
          },
          {
            "name": "maxonary/Spotter",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/62939182?s=40&v=4",
            "owner": "maxonary",
            "repo_name": "Spotter"
          },
          {
            "name": "Teaching-and-Learning-in-Computing/dRAG",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/166765208?s=40&v=4",
            "owner": "Teaching-and-Learning-in-Computing",
            "repo_name": "dRAG"
          },
          {
            "name": "Vivek-Lahole/custom-model-runpod",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/82323706?s=40&v=4",
            "owner": "Vivek-Lahole",
            "repo_name": "custom-model-runpod"
          },
          {
            "name": "Asugawara/WhatTheFuck",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/47840708?s=40&v=4",
            "owner": "Asugawara",
            "repo_name": "WhatTheFuck"
          },
          {
            "name": "cuongpiger/chatgpt-like-clone",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/40521173?s=40&v=4",
            "owner": "cuongpiger",
            "repo_name": "chatgpt-like-clone"
          },
          {
            "name": "Cosmian/cosmian-ai-runner",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/37953047?s=40&v=4",
            "owner": "Cosmian",
            "repo_name": "cosmian-ai-runner"
          },
          {
            "name": "d-kleine/Advent_of_HayStack",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/53251018?s=40&v=4",
            "owner": "d-kleine",
            "repo_name": "Advent_of_HayStack"
          },
          {
            "name": "speakeasy-api/openapi-agent-examples",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/91446104?s=40&v=4",
            "owner": "speakeasy-api",
            "repo_name": "openapi-agent-examples"
          },
          {
            "name": "Maeret/ask-the-doc",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/62506259?s=40&v=4",
            "owner": "Maeret",
            "repo_name": "ask-the-doc"
          },
          {
            "name": "jianjungki/haystack-rag",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/1188643?s=40&v=4",
            "owner": "jianjungki",
            "repo_name": "haystack-rag"
          },
          {
            "name": "DataScienceHamburg/GenerativeKImitPython_Material",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/59503425?s=40&v=4",
            "owner": "DataScienceHamburg",
            "repo_name": "GenerativeKImitPython_Material"
          },
          {
            "name": "airndlab/hackathon-hacks-ai-mediawise-qna",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/125554461?s=40&v=4",
            "owner": "airndlab",
            "repo_name": "hackathon-hacks-ai-mediawise-qna"
          },
          {
            "name": "ATANKERA/byoLLM",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/57043781?s=40&v=4",
            "owner": "ATANKERA",
            "repo_name": "byoLLM"
          },
          {
            "name": "YashBaravaliya/MedGuide-AI",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/110822197?s=40&v=4",
            "owner": "YashBaravaliya",
            "repo_name": "MedGuide-AI"
          },
          {
            "name": "JaegerP/web-app-rag",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/7987899?s=40&v=4",
            "owner": "JaegerP",
            "repo_name": "web-app-rag"
          },
          {
            "name": "gelbal/wordpress-author-style-imitate",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/2027598?s=40&v=4",
            "owner": "gelbal",
            "repo_name": "wordpress-author-style-imitate"
          },
          {
            "name": "AayushSalvi/Finance-Chatbot-using-Mistral-7B",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/65705434?s=40&v=4",
            "owner": "AayushSalvi",
            "repo_name": "Finance-Chatbot-using-Mistral-7B"
          },
          {
            "name": "carlo-airaghi/chat-widget-project",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/60608923?s=40&v=4",
            "owner": "carlo-airaghi",
            "repo_name": "chat-widget-project"
          },
          {
            "name": "SyedAffan10/Document-QA-Chatbot",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/63011097?s=40&v=4",
            "owner": "SyedAffan10",
            "repo_name": "Document-QA-Chatbot"
          },
          {
            "name": "shrimantasatpati/fastapi_LLM_bot_Streamlit",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/82357659?s=40&v=4",
            "owner": "shrimantasatpati",
            "repo_name": "fastapi_LLM_bot_Streamlit"
          },
          {
            "name": "NERC-CEH/llm-eval",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/5802506?s=40&v=4",
            "owner": "NERC-CEH",
            "repo_name": "llm-eval"
          },
          {
            "name": "yusyel/rag-wiki-helpers",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/25446622?s=40&v=4",
            "owner": "yusyel",
            "repo_name": "rag-wiki-helpers"
          },
          {
            "name": "JeanJean-rxl/random-retrieval-plugin",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/33363603?s=40&v=4",
            "owner": "JeanJean-rxl",
            "repo_name": "random-retrieval-plugin"
          },
          {
            "name": "CodeAKrome/bootcupboard",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/44734955?s=40&v=4",
            "owner": "CodeAKrome",
            "repo_name": "bootcupboard"
          },
          {
            "name": "Areopaguaworkshop/wenbi",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/138816467?s=40&v=4",
            "owner": "Areopaguaworkshop",
            "repo_name": "wenbi"
          },
          {
            "name": "MadisonEvans94/splade",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/100099375?s=40&v=4",
            "owner": "MadisonEvans94",
            "repo_name": "splade"
          },
          {
            "name": "nsubordin81/haystack_learning",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/4014893?s=40&v=4",
            "owner": "nsubordin81",
            "repo_name": "haystack_learning"
          },
          {
            "name": "vhidvz/language-identification",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/54132701?s=40&v=4",
            "owner": "vhidvz",
            "repo_name": "language-identification"
          },
          {
            "name": "useless-ai-tinfoil-hats/sf-conspiracies",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/180780952?s=40&v=4",
            "owner": "useless-ai-tinfoil-hats",
            "repo_name": "sf-conspiracies"
          },
          {
            "name": "Sanket12122003/RAG-With-Haystack-MistralAI-Pinecone",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/132454353?s=40&v=4",
            "owner": "Sanket12122003",
            "repo_name": "RAG-With-Haystack-MistralAI-Pinecone"
          },
          {
            "name": "Sibgat-Ul/food_diet",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/61459751?s=40&v=4",
            "owner": "Sibgat-Ul",
            "repo_name": "food_diet"
          },
          {
            "name": "dashinja/CalarmHelp",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/25598950?s=40&v=4",
            "owner": "dashinja",
            "repo_name": "CalarmHelp"
          },
          {
            "name": "sebhoron/food-chatbot",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/92864754?s=40&v=4",
            "owner": "sebhoron",
            "repo_name": "food-chatbot"
          },
          {
            "name": "nicolorosso/ensemble_llms",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/98038058?s=40&v=4",
            "owner": "nicolorosso",
            "repo_name": "ensemble_llms"
          },
          {
            "name": "WaqasAli-Munawar/Building-AI-Applications-With-Haystack",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/48343525?s=40&v=4",
            "owner": "WaqasAli-Munawar",
            "repo_name": "Building-AI-Applications-With-Haystack"
          },
          {
            "name": "mobiusml/aana_chat_with_video",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/34684912?s=40&v=4",
            "owner": "mobiusml",
            "repo_name": "aana_chat_with_video"
          },
          {
            "name": "5ud21/Weaviate-Document-Store",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/88098618?s=40&v=4",
            "owner": "5ud21",
            "repo_name": "Weaviate-Document-Store"
          },
          {
            "name": "mobiusml/aana_summarize_video",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/34684912?s=40&v=4",
            "owner": "mobiusml",
            "repo_name": "aana_summarize_video"
          },
          {
            "name": "fpreiss/talk_rag_scheduler",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/17441607?s=40&v=4",
            "owner": "fpreiss",
            "repo_name": "talk_rag_scheduler"
          },
          {
            "name": "mayankkapoor/ecom-agent-with-llama3",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/1268053?s=40&v=4",
            "owner": "mayankkapoor",
            "repo_name": "ecom-agent-with-llama3"
          },
          {
            "name": "hiimbach/Financial-Statement-Reader",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/90375638?s=40&v=4",
            "owner": "hiimbach",
            "repo_name": "Financial-Statement-Reader"
          },
          {
            "name": "oscar066/FeynmanAI",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/71583548?s=40&v=4",
            "owner": "oscar066",
            "repo_name": "FeynmanAI"
          },
          {
            "name": "ChrisPappalardo/agent_fred",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/3485539?s=40&v=4",
            "owner": "ChrisPappalardo",
            "repo_name": "agent_fred"
          },
          {
            "name": "sunithalv/MedicalQA_Haystack_Pubmed",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/28974154?s=40&v=4",
            "owner": "sunithalv",
            "repo_name": "MedicalQA_Haystack_Pubmed"
          },
          {
            "name": "Morpheus-An/IoT-Agent",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/110734442?s=40&v=4",
            "owner": "Morpheus-An",
            "repo_name": "IoT-Agent"
          },
          {
            "name": "Rusteam/vlm-rag-chat",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/22130831?s=40&v=4",
            "owner": "Rusteam",
            "repo_name": "vlm-rag-chat"
          },
          {
            "name": "TuanaCelik/ai-dev-haystack",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/15802862?s=40&v=4",
            "owner": "TuanaCelik",
            "repo_name": "ai-dev-haystack"
          },
          {
            "name": "heysourin/RAG-App-Using-Haystack-MistralAI-Pinecone-FastAPI-End-to-End",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/98945276?s=40&v=4",
            "owner": "heysourin",
            "repo_name": "RAG-App-Using-Haystack-MistralAI-Pinecone-FastAPI-End-to-End"
          },
          {
            "name": "samuelw1w/ResumeBuilder",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/111531999?s=40&v=4",
            "owner": "samuelw1w",
            "repo_name": "ResumeBuilder"
          },
          {
            "name": "salakhovilia/coworker",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/47942484?s=40&v=4",
            "owner": "salakhovilia",
            "repo_name": "coworker"
          },
          {
            "name": "RidhimaGupta4/sparrow",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/172059959?s=40&v=4",
            "owner": "RidhimaGupta4",
            "repo_name": "sparrow"
          },
          {
            "name": "padmapria/Interactive-URL-Based-LLM-RAG-Question-Answering-System",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/31624929?s=40&v=4",
            "owner": "padmapria",
            "repo_name": "Interactive-URL-Based-LLM-RAG-Question-Answering-System"
          },
          {
            "name": "MuhammadTalmeez337/Healthcare-chatbot-using-mixtral-LLM-and-Pubmed",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/46068141?s=40&v=4",
            "owner": "MuhammadTalmeez337",
            "repo_name": "Healthcare-chatbot-using-mixtral-LLM-and-Pubmed"
          },
          {
            "name": "samvardhan777/opensearch_search_engine",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/69216650?s=40&v=4",
            "owner": "samvardhan777",
            "repo_name": "opensearch_search_engine"
          },
          {
            "name": "last-brain-cell/know-better",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/109949049?s=40&v=4",
            "owner": "last-brain-cell",
            "repo_name": "know-better"
          },
          {
            "name": "Dhilip2002/Augmented-Question-Answering-System-for-Biomedical-Literature-Retrieval-using-LLM-and-PubMed",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/71542489?s=40&v=4",
            "owner": "Dhilip2002",
            "repo_name": "Augmented-Question-Answering-System-for-Biomedical-Literature-Retrieval-using-LLM-and-PubMed"
          },
          {
            "name": "ekimkk/CMU-Advanced-NLP-Assignment-2-End-to-end-NLP-System-Building",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/142109497?s=40&v=4",
            "owner": "ekimkk",
            "repo_name": "CMU-Advanced-NLP-Assignment-2-End-to-end-NLP-System-Building"
          },
          {
            "name": "ROHITH-M10/aws-invoice-generation",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/93328109?s=40&v=4",
            "owner": "ROHITH-M10",
            "repo_name": "aws-invoice-generation"
          },
          {
            "name": "browserbase/haystack",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/158221360?s=40&v=4",
            "owner": "browserbase",
            "repo_name": "haystack"
          },
          {
            "name": "silvhua/pubmed-search",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/111714881?s=40&v=4",
            "owner": "silvhua",
            "repo_name": "pubmed-search"
          },
          {
            "name": "justus-positivo/groq-edu",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/121574860?s=40&v=4",
            "owner": "justus-positivo",
            "repo_name": "groq-edu"
          },
          {
            "name": "craigsdennis/haystack-llamaguard-workers-ai",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/8494909?s=40&v=4",
            "owner": "craigsdennis",
            "repo_name": "haystack-llamaguard-workers-ai"
          },
          {
            "name": "SameerAhamed25/Telegram_AI_Bot",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/145002118?s=40&v=4",
            "owner": "SameerAhamed25",
            "repo_name": "Telegram_AI_Bot"
          },
          {
            "name": "srikanthpl/MY_PROJECTS",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/124775319?s=40&v=4",
            "owner": "srikanthpl",
            "repo_name": "MY_PROJECTS"
          },
          {
            "name": "nish700/RailBot-With-Haystack-and-Mistral",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/39549411?s=40&v=4",
            "owner": "nish700",
            "repo_name": "RailBot-With-Haystack-and-Mistral"
          },
          {
            "name": "sfgrahman/Healtcare_chatbot_PubMed",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/4463215?s=40&v=4",
            "owner": "sfgrahman",
            "repo_name": "Healtcare_chatbot_PubMed"
          },
          {
            "name": "pks20iitk/Haystack-and-Mistral-7B-RAG-Implementation",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/78561615?s=40&v=4",
            "owner": "pks20iitk",
            "repo_name": "Haystack-and-Mistral-7B-RAG-Implementation"
          },
          {
            "name": "Achaarya-AI/Acharya-core",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/155107177?s=40&v=4",
            "owner": "Achaarya-AI",
            "repo_name": "Acharya-core"
          },
          {
            "name": "greymini/Miss7B",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/79689279?s=40&v=4",
            "owner": "greymini",
            "repo_name": "Miss7B"
          },
          {
            "name": "textomatic/Finetuning-LLMs-for-Question-Answering",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/102237745?s=40&v=4",
            "owner": "textomatic",
            "repo_name": "Finetuning-LLMs-for-Question-Answering"
          },
          {
            "name": "bogdankostic/notion-haystack",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/48713846?s=40&v=4",
            "owner": "bogdankostic",
            "repo_name": "notion-haystack"
          },
          {
            "name": "michaelromagne/advent-of-code-submissions-2023",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/44051774?s=40&v=4",
            "owner": "michaelromagne",
            "repo_name": "advent-of-code-submissions-2023"
          },
          {
            "name": "kndeepak/LLM-RAG-invoice-Local_CPU",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/55972417?s=40&v=4",
            "owner": "kndeepak",
            "repo_name": "LLM-RAG-invoice-Local_CPU"
          }
        ],
        "public_dependents_number": 260,
        "private_dependents_number": -260,
        "total_dependents_number": 260,
        "badges": {
          "total": "[![Generated by github-dependents-info](https://img.shields.io/static/v1?label=Used%20by&message=260&color=informational&logo=slickpic)](https://github.com/deepset-ai/Haystack/network/dependents?package_id=UGFja2FnZS0zNjMxNDIzNDYy)",
          "public": "[![Generated by github-dependents-info](https://img.shields.io/static/v1?label=Used%20by%20(public)&message=260&color=informational&logo=slickpic)](https://github.com/deepset-ai/Haystack/network/dependents?package_id=UGFja2FnZS0zNjMxNDIzNDYy)",
          "private": "[![Generated by github-dependents-info](https://img.shields.io/static/v1?label=Used%20by%20(private)&message=-260&color=informational&logo=slickpic)](https://github.com/deepset-ai/Haystack/network/dependents?package_id=UGFja2FnZS0zNjMxNDIzNDYy)",
          "stars": "[![Generated by github-dependents-info](https://img.shields.io/static/v1?label=Used%20by%20(stars)&message=367&color=informational&logo=slickpic)](https://github.com/deepset-ai/Haystack/network/dependents?package_id=UGFja2FnZS0zNjMxNDIzNDYy)"
        }
      },
      {
        "id": "UGFja2FnZS03MTQ5NzEwOTc%3D",
        "name": "farm-haystack",
        "url": "https://github.com/deepset-ai/Haystack/network/dependents?package_id=UGFja2FnZS03MTQ5NzEwOTc%3D",
        "public_dependent_stars": 22,
        "public_dependents": [
          {
            "name": "Chainlit/chainlit",
            "stars": 9382,
            "img": "https://avatars.githubusercontent.com/u/128686189?s=40&v=4",
            "owner": "Chainlit",
            "repo_name": "chainlit"
          },
          {
            "name": "weaviate/Verba",
            "stars": 7052,
            "img": "https://avatars.githubusercontent.com/u/37794290?s=40&v=4",
            "owner": "weaviate",
            "repo_name": "Verba"
          },
          {
            "name": "traceloop/openllmetry",
            "stars": 5691,
            "img": "https://avatars.githubusercontent.com/u/125419530?s=40&v=4",
            "owner": "traceloop",
            "repo_name": "openllmetry"
          },
          {
            "name": "nlp-with-transformers/notebooks",
            "stars": 4294,
            "img": "https://avatars.githubusercontent.com/u/93219893?s=40&v=4",
            "owner": "nlp-with-transformers",
            "repo_name": "notebooks"
          },
          {
            "name": "Chainlit/cookbook",
            "stars": 1061,
            "img": "https://avatars.githubusercontent.com/u/128686189?s=40&v=4",
            "owner": "Chainlit",
            "repo_name": "cookbook"
          },
          {
            "name": "jamescalam/transformers",
            "stars": 585,
            "img": "https://avatars.githubusercontent.com/u/35938317?s=40&v=4",
            "owner": "jamescalam",
            "repo_name": "transformers"
          },
          {
            "name": "Vasanthengineer4949/NLP-Projects-NHV",
            "stars": 432,
            "img": "https://avatars.githubusercontent.com/u/64586431?s=40&v=4",
            "owner": "Vasanthengineer4949",
            "repo_name": "NLP-Projects-NHV"
          },
          {
            "name": "jlonge4/local_llama",
            "stars": 267,
            "img": "https://avatars.githubusercontent.com/u/91354480?s=40&v=4",
            "owner": "jlonge4",
            "repo_name": "local_llama"
          },
          {
            "name": "maxent-ai/ocrpy",
            "stars": 223,
            "img": "https://avatars.githubusercontent.com/u/34341850?s=40&v=4",
            "owner": "maxent-ai",
            "repo_name": "ocrpy"
          },
          {
            "name": "ocdevel/gnothi",
            "stars": 186,
            "img": "https://avatars.githubusercontent.com/u/103905581?s=40&v=4",
            "owner": "ocdevel",
            "repo_name": "gnothi"
          },
          {
            "name": "sambanova/toolbench",
            "stars": 150,
            "img": "https://avatars.githubusercontent.com/u/80118584?s=40&v=4",
            "owner": "sambanova",
            "repo_name": "toolbench"
          },
          {
            "name": "rickiepark/nlp-with-transformers",
            "stars": 140,
            "img": "https://avatars.githubusercontent.com/u/18256853?s=40&v=4",
            "owner": "rickiepark",
            "repo_name": "nlp-with-transformers"
          },
          {
            "name": "jlonge4/mychatGPT",
            "stars": 139,
            "img": "https://avatars.githubusercontent.com/u/91354480?s=40&v=4",
            "owner": "jlonge4",
            "repo_name": "mychatGPT"
          },
          {
            "name": "li-ronghui/FineDance",
            "stars": 138,
            "img": "https://avatars.githubusercontent.com/u/20925701?s=40&v=4",
            "owner": "li-ronghui",
            "repo_name": "FineDance"
          },
          {
            "name": "aws-samples/semantic-search-aws-docs",
            "stars": 125,
            "img": "https://avatars.githubusercontent.com/u/8931462?s=40&v=4",
            "owner": "aws-samples",
            "repo_name": "semantic-search-aws-docs"
          },
          {
            "name": "foxminchan/LawKnowledge",
            "stars": 110,
            "img": "https://avatars.githubusercontent.com/u/56079798?s=40&v=4",
            "owner": "foxminchan",
            "repo_name": "LawKnowledge"
          },
          {
            "name": "flairNLP/fabricator",
            "stars": 108,
            "img": "https://avatars.githubusercontent.com/u/59021421?s=40&v=4",
            "owner": "flairNLP",
            "repo_name": "fabricator"
          },
          {
            "name": "larsbaunwall/bricky",
            "stars": 105,
            "img": "https://avatars.githubusercontent.com/u/3168816?s=40&v=4",
            "owner": "larsbaunwall",
            "repo_name": "bricky"
          },
          {
            "name": "AIAnytime/Haystack-and-Mistral-7B-RAG-Implementation",
            "stars": 79,
            "img": "https://avatars.githubusercontent.com/u/30049737?s=40&v=4",
            "owner": "AIAnytime",
            "repo_name": "Haystack-and-Mistral-7B-RAG-Implementation"
          },
          {
            "name": "katanaml/llm-rag-invoice-cpu",
            "stars": 68,
            "img": "https://avatars.githubusercontent.com/u/49202856?s=40&v=4",
            "owner": "katanaml",
            "repo_name": "llm-rag-invoice-cpu"
          },
          {
            "name": "PostHog/max-ai",
            "stars": 65,
            "img": "https://avatars.githubusercontent.com/u/60330232?s=40&v=4",
            "owner": "PostHog",
            "repo_name": "max-ai"
          },
          {
            "name": "AIAnytime/YouTube-Video-Summarization-App",
            "stars": 56,
            "img": "https://avatars.githubusercontent.com/u/30049737?s=40&v=4",
            "owner": "AIAnytime",
            "repo_name": "YouTube-Video-Summarization-App"
          },
          {
            "name": "TuanaCelik/should-i-follow",
            "stars": 44,
            "img": "https://avatars.githubusercontent.com/u/15802862?s=40&v=4",
            "owner": "TuanaCelik",
            "repo_name": "should-i-follow"
          },
          {
            "name": "qdrant/qdrant-haystack",
            "stars": 44,
            "img": "https://avatars.githubusercontent.com/u/73504361?s=40&v=4",
            "owner": "qdrant",
            "repo_name": "qdrant-haystack"
          },
          {
            "name": "Karthik-Bhaskar/Context-Based-Question-Answering",
            "stars": 43,
            "img": "https://avatars.githubusercontent.com/u/13200370?s=40&v=4",
            "owner": "Karthik-Bhaskar",
            "repo_name": "Context-Based-Question-Answering"
          },
          {
            "name": "ploomber/doc",
            "stars": 34,
            "img": "https://avatars.githubusercontent.com/u/60114551?s=40&v=4",
            "owner": "ploomber",
            "repo_name": "doc"
          },
          {
            "name": "Prismadic/magnet",
            "stars": 31,
            "img": "https://avatars.githubusercontent.com/u/153389283?s=40&v=4",
            "owner": "Prismadic",
            "repo_name": "magnet"
          },
          {
            "name": "dod-advana/gamechanger-data",
            "stars": 31,
            "img": "https://avatars.githubusercontent.com/u/82123680?s=40&v=4",
            "owner": "dod-advana",
            "repo_name": "gamechanger-data"
          },
          {
            "name": "bcgov/mds",
            "stars": 28,
            "img": "https://avatars.githubusercontent.com/u/916280?s=40&v=4",
            "owner": "bcgov",
            "repo_name": "mds"
          },
          {
            "name": "anakin87/fact-checking-rocks",
            "stars": 28,
            "img": "https://avatars.githubusercontent.com/u/44616784?s=40&v=4",
            "owner": "anakin87",
            "repo_name": "fact-checking-rocks"
          },
          {
            "name": "deepset-ai/haystack-search-pipeline-streamlit",
            "stars": 27,
            "img": "https://avatars.githubusercontent.com/u/51827949?s=40&v=4",
            "owner": "deepset-ai",
            "repo_name": "haystack-search-pipeline-streamlit"
          },
          {
            "name": "redhat-et/foundation-models-for-documentation",
            "stars": 27,
            "img": "https://avatars.githubusercontent.com/u/66332970?s=40&v=4",
            "owner": "redhat-et",
            "repo_name": "foundation-models-for-documentation"
          },
          {
            "name": "lfunderburk/llmops-with-haystack",
            "stars": 26,
            "img": "https://avatars.githubusercontent.com/u/13559004?s=40&v=4",
            "owner": "lfunderburk",
            "repo_name": "llmops-with-haystack"
          },
          {
            "name": "restful3/ds4th_study",
            "stars": 25,
            "img": "https://avatars.githubusercontent.com/u/2604066?s=40&v=4",
            "owner": "restful3",
            "repo_name": "ds4th_study"
          },
          {
            "name": "prosto/neo4j-haystack",
            "stars": 24,
            "img": "https://avatars.githubusercontent.com/u/706952?s=40&v=4",
            "owner": "prosto",
            "repo_name": "neo4j-haystack"
          },
          {
            "name": "dod-advana/gamechanger-ml",
            "stars": 24,
            "img": "https://avatars.githubusercontent.com/u/82123680?s=40&v=4",
            "owner": "dod-advana",
            "repo_name": "gamechanger-ml"
          },
          {
            "name": "AIAnytime/RAG-Tool-using-Haystack-Mistral-and-Chainlit",
            "stars": 23,
            "img": "https://avatars.githubusercontent.com/u/30049737?s=40&v=4",
            "owner": "AIAnytime",
            "repo_name": "RAG-Tool-using-Haystack-Mistral-and-Chainlit"
          },
          {
            "name": "miranthajayatilake/nanoQA",
            "stars": 23,
            "img": "https://avatars.githubusercontent.com/u/11708057?s=40&v=4",
            "owner": "miranthajayatilake",
            "repo_name": "nanoQA"
          },
          {
            "name": "rolandtannous/haystack-memory",
            "stars": 22,
            "img": "https://avatars.githubusercontent.com/u/115670425?s=40&v=4",
            "owner": "rolandtannous",
            "repo_name": "haystack-memory"
          },
          {
            "name": "PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition",
            "stars": 21,
            "img": "https://avatars.githubusercontent.com/u/10974906?s=40&v=4",
            "owner": "PacktPublishing",
            "repo_name": "Python-Natural-Language-Processing-Cookbook-Second-Edition"
          },
          {
            "name": "mathislucka/pycon-berlin-2023",
            "stars": 19,
            "img": "https://avatars.githubusercontent.com/u/16985509?s=40&v=4",
            "owner": "mathislucka",
            "repo_name": "pycon-berlin-2023"
          },
          {
            "name": "MedBot-team/NaBot",
            "stars": 19,
            "img": "https://avatars.githubusercontent.com/u/88496278?s=40&v=4",
            "owner": "MedBot-team",
            "repo_name": "NaBot"
          },
          {
            "name": "EikeKohl/paperqa-web-app",
            "stars": 18,
            "img": "https://avatars.githubusercontent.com/u/57795397?s=40&v=4",
            "owner": "EikeKohl",
            "repo_name": "paperqa-web-app"
          },
          {
            "name": "CogniQ/CogniQ",
            "stars": 18,
            "img": "https://avatars.githubusercontent.com/u/132189118?s=40&v=4",
            "owner": "CogniQ",
            "repo_name": "CogniQ"
          },
          {
            "name": "intel/open-domain-question-and-answer",
            "stars": 17,
            "img": "https://avatars.githubusercontent.com/u/17888862?s=40&v=4",
            "owner": "intel",
            "repo_name": "open-domain-question-and-answer"
          },
          {
            "name": "LLukas22/Retrieval-Augmented-QA",
            "stars": 17,
            "img": "https://avatars.githubusercontent.com/u/65088241?s=40&v=4",
            "owner": "LLukas22",
            "repo_name": "Retrieval-Augmented-QA"
          },
          {
            "name": "johnnygreco/hpqa",
            "stars": 17,
            "img": "https://avatars.githubusercontent.com/u/10998105?s=40&v=4",
            "owner": "johnnygreco",
            "repo_name": "hpqa"
          },
          {
            "name": "jamescalam/aurelius",
            "stars": 17,
            "img": "https://avatars.githubusercontent.com/u/35938317?s=40&v=4",
            "owner": "jamescalam",
            "repo_name": "aurelius"
          },
          {
            "name": "milvus-io/milvus-haystack",
            "stars": 16,
            "img": "https://avatars.githubusercontent.com/u/51735404?s=40&v=4",
            "owner": "milvus-io",
            "repo_name": "milvus-haystack"
          },
          {
            "name": "ShuHuang/chemdatawriter",
            "stars": 14,
            "img": "https://avatars.githubusercontent.com/u/44169687?s=40&v=4",
            "owner": "ShuHuang",
            "repo_name": "chemdatawriter"
          },
          {
            "name": "ArzelaAscoIi/haystack-github-bot",
            "stars": 14,
            "img": "https://avatars.githubusercontent.com/u/37148029?s=40&v=4",
            "owner": "ArzelaAscoIi",
            "repo_name": "haystack-github-bot"
          },
          {
            "name": "Kohimax/qna-api",
            "stars": 14,
            "img": "https://avatars.githubusercontent.com/u/20102354?s=40&v=4",
            "owner": "Kohimax",
            "repo_name": "qna-api"
          },
          {
            "name": "KhanhHua2102/Monetize.ai",
            "stars": 13,
            "img": "https://avatars.githubusercontent.com/u/46571355?s=40&v=4",
            "owner": "KhanhHua2102",
            "repo_name": "Monetize.ai"
          },
          {
            "name": "anakin87/who-killed-laura-palmer",
            "stars": 11,
            "img": "https://avatars.githubusercontent.com/u/44616784?s=40&v=4",
            "owner": "anakin87",
            "repo_name": "who-killed-laura-palmer"
          },
          {
            "name": "giguru/converse",
            "stars": 11,
            "img": "https://avatars.githubusercontent.com/u/5540787?s=40&v=4",
            "owner": "giguru",
            "repo_name": "converse"
          },
          {
            "name": "karthik19967829/InferDoc",
            "stars": 11,
            "img": "https://avatars.githubusercontent.com/u/35610230?s=40&v=4",
            "owner": "karthik19967829",
            "repo_name": "InferDoc"
          },
          {
            "name": "GURPREETKAURJETHRA/Youtube-Video-Transcribe-Summarizer-LLM-App",
            "stars": 10,
            "img": "https://avatars.githubusercontent.com/u/100081334?s=40&v=4",
            "owner": "GURPREETKAURJETHRA",
            "repo_name": "Youtube-Video-Transcribe-Summarizer-LLM-App"
          },
          {
            "name": "VectorInstitute/NAA",
            "stars": 10,
            "img": "https://avatars.githubusercontent.com/u/40637123?s=40&v=4",
            "owner": "VectorInstitute",
            "repo_name": "NAA"
          },
          {
            "name": "naynco/nayn.ml",
            "stars": 10,
            "img": "https://avatars.githubusercontent.com/u/41342599?s=40&v=4",
            "owner": "naynco",
            "repo_name": "nayn.ml"
          },
          {
            "name": "LOH-puzik/LegalEase-AI",
            "stars": 9,
            "img": "https://avatars.githubusercontent.com/u/36806008?s=40&v=4",
            "owner": "LOH-puzik",
            "repo_name": "LegalEase-AI"
          },
          {
            "name": "yordanoswuletaw/covid19-qa-system",
            "stars": 9,
            "img": "https://avatars.githubusercontent.com/u/101830198?s=40&v=4",
            "owner": "yordanoswuletaw",
            "repo_name": "covid19-qa-system"
          },
          {
            "name": "fvanlitsenburg/promptbox",
            "stars": 9,
            "img": "https://avatars.githubusercontent.com/u/68997090?s=40&v=4",
            "owner": "fvanlitsenburg",
            "repo_name": "promptbox"
          },
          {
            "name": "eea/nlp-service",
            "stars": 9,
            "img": "https://avatars.githubusercontent.com/u/1176627?s=40&v=4",
            "owner": "eea",
            "repo_name": "nlp-service"
          },
          {
            "name": "deepset-ai/biqa-llm",
            "stars": 8,
            "img": "https://avatars.githubusercontent.com/u/51827949?s=40&v=4",
            "owner": "deepset-ai",
            "repo_name": "biqa-llm"
          },
          {
            "name": "AIAnytime/Fashion-Search-App-using-AI",
            "stars": 8,
            "img": "https://avatars.githubusercontent.com/u/30049737?s=40&v=4",
            "owner": "AIAnytime",
            "repo_name": "Fashion-Search-App-using-AI"
          },
          {
            "name": "bilgeyucel/haystack-chainlit-demo",
            "stars": 8,
            "img": "https://avatars.githubusercontent.com/u/25897116?s=40&v=4",
            "owner": "bilgeyucel",
            "repo_name": "haystack-chainlit-demo"
          },
          {
            "name": "deepset-ai/document-store",
            "stars": 8,
            "img": "https://avatars.githubusercontent.com/u/51827949?s=40&v=4",
            "owner": "deepset-ai",
            "repo_name": "document-store"
          },
          {
            "name": "raynerz/pdf-search",
            "stars": 8,
            "img": "https://avatars.githubusercontent.com/u/38153565?s=40&v=4",
            "owner": "raynerz",
            "repo_name": "pdf-search"
          },
          {
            "name": "WangXII/BEEDS",
            "stars": 8,
            "img": "https://avatars.githubusercontent.com/u/23039011?s=40&v=4",
            "owner": "WangXII",
            "repo_name": "BEEDS"
          },
          {
            "name": "etalab-ia/piaf-ml",
            "stars": 8,
            "img": "https://avatars.githubusercontent.com/u/59837095?s=40&v=4",
            "owner": "etalab-ia",
            "repo_name": "piaf-ml"
          },
          {
            "name": "apache/openserverless-runtimes",
            "stars": 6,
            "img": "https://avatars.githubusercontent.com/u/47359?s=40&v=4",
            "owner": "apache",
            "repo_name": "openserverless-runtimes"
          },
          {
            "name": "concaption/text2img-search",
            "stars": 6,
            "img": "https://avatars.githubusercontent.com/u/43001234?s=40&v=4",
            "owner": "concaption",
            "repo_name": "text2img-search"
          },
          {
            "name": "cabustillo13/RAG_Haystack_Chatbot",
            "stars": 6,
            "img": "https://avatars.githubusercontent.com/u/52437136?s=40&v=4",
            "owner": "cabustillo13",
            "repo_name": "RAG_Haystack_Chatbot"
          },
          {
            "name": "sebastianschramm/german-qa-rag",
            "stars": 6,
            "img": "https://avatars.githubusercontent.com/u/16114609?s=40&v=4",
            "owner": "sebastianschramm",
            "repo_name": "german-qa-rag"
          },
          {
            "name": "LLukas22/vLLM-haystack-adapter",
            "stars": 6,
            "img": "https://avatars.githubusercontent.com/u/65088241?s=40&v=4",
            "owner": "LLukas22",
            "repo_name": "vLLM-haystack-adapter"
          },
          {
            "name": "norahollenstein/copco-processing",
            "stars": 6,
            "img": "https://avatars.githubusercontent.com/u/17772234?s=40&v=4",
            "owner": "norahollenstein",
            "repo_name": "copco-processing"
          },
          {
            "name": "TuanaCelik/what-would-mother-say",
            "stars": 6,
            "img": "https://avatars.githubusercontent.com/u/15802862?s=40&v=4",
            "owner": "TuanaCelik",
            "repo_name": "what-would-mother-say"
          },
          {
            "name": "dtaivpp/opensearch-haystack-demo",
            "stars": 6,
            "img": "https://avatars.githubusercontent.com/u/17506770?s=40&v=4",
            "owner": "dtaivpp",
            "repo_name": "opensearch-haystack-demo"
          },
          {
            "name": "ArzelaAscoIi/haystack-keda-indexing",
            "stars": 6,
            "img": "https://avatars.githubusercontent.com/u/37148029?s=40&v=4",
            "owner": "ArzelaAscoIi",
            "repo_name": "haystack-keda-indexing"
          },
          {
            "name": "gravityml/haystack-human-tool",
            "stars": 6,
            "img": "https://avatars.githubusercontent.com/u/129120892?s=40&v=4",
            "owner": "gravityml",
            "repo_name": "haystack-human-tool"
          },
          {
            "name": "MysterionRise/boost-search",
            "stars": 6,
            "img": "https://avatars.githubusercontent.com/u/956191?s=40&v=4",
            "owner": "MysterionRise",
            "repo_name": "boost-search"
          },
          {
            "name": "baochi0212/tourxQA",
            "stars": 6,
            "img": "https://avatars.githubusercontent.com/u/77192945?s=40&v=4",
            "owner": "baochi0212",
            "repo_name": "tourxQA"
          },
          {
            "name": "okara83/Becoming-a-Data-Scientist",
            "stars": 6,
            "img": "https://avatars.githubusercontent.com/u/2750953?s=40&v=4",
            "owner": "okara83",
            "repo_name": "Becoming-a-Data-Scientist"
          },
          {
            "name": "flexudy-pipe/qugeev",
            "stars": 6,
            "img": "https://avatars.githubusercontent.com/u/63919993?s=40&v=4",
            "owner": "flexudy-pipe",
            "repo_name": "qugeev"
          },
          {
            "name": "gariciodaro/arXiv-haystack-app",
            "stars": 6,
            "img": "https://avatars.githubusercontent.com/u/9937662?s=40&v=4",
            "owner": "gariciodaro",
            "repo_name": "arXiv-haystack-app"
          },
          {
            "name": "TuanaCelik/milvus-documentation-qa",
            "stars": 5,
            "img": "https://avatars.githubusercontent.com/u/15802862?s=40&v=4",
            "owner": "TuanaCelik",
            "repo_name": "milvus-documentation-qa"
          },
          {
            "name": "bobcastaldeli/QA_B3",
            "stars": 5,
            "img": "https://avatars.githubusercontent.com/u/32075635?s=40&v=4",
            "owner": "bobcastaldeli",
            "repo_name": "QA_B3"
          },
          {
            "name": "hyunsir/Cross-language-tutorial-search",
            "stars": 5,
            "img": "https://avatars.githubusercontent.com/u/46249154?s=40&v=4",
            "owner": "hyunsir",
            "repo_name": "Cross-language-tutorial-search"
          },
          {
            "name": "WIET-QA-SYSTEM/quap",
            "stars": 5,
            "img": "https://avatars.githubusercontent.com/u/104865888?s=40&v=4",
            "owner": "WIET-QA-SYSTEM",
            "repo_name": "quap"
          },
          {
            "name": "tanchangsheng/covid19-search-and-qa",
            "stars": 5,
            "img": "https://avatars.githubusercontent.com/u/20267562?s=40&v=4",
            "owner": "tanchangsheng",
            "repo_name": "covid19-search-and-qa"
          },
          {
            "name": "lambdaofgod/pytorch_hackathon",
            "stars": 5,
            "img": "https://avatars.githubusercontent.com/u/3647577?s=40&v=4",
            "owner": "lambdaofgod",
            "repo_name": "pytorch_hackathon"
          },
          {
            "name": "Blacksujit/ArogyaKrishi",
            "stars": 4,
            "img": "https://avatars.githubusercontent.com/u/148805811?s=40&v=4",
            "owner": "Blacksujit",
            "repo_name": "ArogyaKrishi"
          },
          {
            "name": "harshal-pathak/Gemini1.5-mp3-Summerization",
            "stars": 4,
            "img": "https://avatars.githubusercontent.com/u/124377711?s=40&v=4",
            "owner": "harshal-pathak",
            "repo_name": "Gemini1.5-mp3-Summerization"
          },
          {
            "name": "shuoli90/TRAQ",
            "stars": 4,
            "img": "https://avatars.githubusercontent.com/u/19268975?s=40&v=4",
            "owner": "shuoli90",
            "repo_name": "TRAQ"
          },
          {
            "name": "THUSIGSICLAB/FineDance",
            "stars": 4,
            "img": "https://avatars.githubusercontent.com/u/100828931?s=40&v=4",
            "owner": "THUSIGSICLAB",
            "repo_name": "FineDance"
          },
          {
            "name": "SeungHunHan11/FooTball-LLM",
            "stars": 4,
            "img": "https://avatars.githubusercontent.com/u/95485958?s=40&v=4",
            "owner": "SeungHunHan11",
            "repo_name": "FooTball-LLM"
          },
          {
            "name": "argilla-io/argilla-haystack",
            "stars": 4,
            "img": "https://avatars.githubusercontent.com/u/18415507?s=40&v=4",
            "owner": "argilla-io",
            "repo_name": "argilla-haystack"
          },
          {
            "name": "cuya26/hbd-demo",
            "stars": 4,
            "img": "https://avatars.githubusercontent.com/u/105236184?s=40&v=4",
            "owner": "cuya26",
            "repo_name": "hbd-demo"
          },
          {
            "name": "sunilkumardash9/chat-arxiv",
            "stars": 4,
            "img": "https://avatars.githubusercontent.com/u/47926185?s=40&v=4",
            "owner": "sunilkumardash9",
            "repo_name": "chat-arxiv"
          },
          {
            "name": "ksmin23/rag-with-haystack-and-amazon-opensearch",
            "stars": 4,
            "img": "https://avatars.githubusercontent.com/u/4167885?s=40&v=4",
            "owner": "ksmin23",
            "repo_name": "rag-with-haystack-and-amazon-opensearch"
          },
          {
            "name": "beltran-oscar/ETL-pipeline-ML",
            "stars": 4,
            "img": "https://avatars.githubusercontent.com/u/84318636?s=40&v=4",
            "owner": "beltran-oscar",
            "repo_name": "ETL-pipeline-ML"
          },
          {
            "name": "rubenkruiper/IRReC",
            "stars": 4,
            "img": "https://avatars.githubusercontent.com/u/7871378?s=40&v=4",
            "owner": "rubenkruiper",
            "repo_name": "IRReC"
          },
          {
            "name": "datasciencecampus/consultation_nlp",
            "stars": 4,
            "img": "https://avatars.githubusercontent.com/u/25666867?s=40&v=4",
            "owner": "datasciencecampus",
            "repo_name": "consultation_nlp"
          },
          {
            "name": "XpiritBV/Generative-AI-PoC",
            "stars": 4,
            "img": "https://avatars.githubusercontent.com/u/9567984?s=40&v=4",
            "owner": "XpiritBV",
            "repo_name": "Generative-AI-PoC"
          },
          {
            "name": "recrudesce/haystack_lemmatize_node",
            "stars": 4,
            "img": "https://avatars.githubusercontent.com/u/6450799?s=40&v=4",
            "owner": "recrudesce",
            "repo_name": "haystack_lemmatize_node"
          },
          {
            "name": "ByteSpiritGit/fdet",
            "stars": 4,
            "img": "https://avatars.githubusercontent.com/u/125552933?s=40&v=4",
            "owner": "ByteSpiritGit",
            "repo_name": "fdet"
          },
          {
            "name": "kmcleste/oracle-of-ammon",
            "stars": 4,
            "img": "https://avatars.githubusercontent.com/u/47370125?s=40&v=4",
            "owner": "kmcleste",
            "repo_name": "oracle-of-ammon"
          },
          {
            "name": "KhalilMrini/Medical-Question-Answering",
            "stars": 4,
            "img": "https://avatars.githubusercontent.com/u/11042966?s=40&v=4",
            "owner": "KhalilMrini",
            "repo_name": "Medical-Question-Answering"
          },
          {
            "name": "aminzayer/notebooks",
            "stars": 3,
            "img": "https://avatars.githubusercontent.com/u/7605327?s=40&v=4",
            "owner": "aminzayer",
            "repo_name": "notebooks"
          },
          {
            "name": "CTXY/table_retrieval",
            "stars": 3,
            "img": "https://avatars.githubusercontent.com/u/33929617?s=40&v=4",
            "owner": "CTXY",
            "repo_name": "table_retrieval"
          },
          {
            "name": "GitXpresso/CLODSH",
            "stars": 3,
            "img": "https://avatars.githubusercontent.com/u/126926699?s=40&v=4",
            "owner": "GitXpresso",
            "repo_name": "CLODSH"
          },
          {
            "name": "alphavector/all",
            "stars": 3,
            "img": "https://avatars.githubusercontent.com/u/11805788?s=40&v=4",
            "owner": "alphavector",
            "repo_name": "all"
          },
          {
            "name": "hemhemoh/DocLing",
            "stars": 3,
            "img": "https://avatars.githubusercontent.com/u/70777991?s=40&v=4",
            "owner": "hemhemoh",
            "repo_name": "DocLing"
          },
          {
            "name": "Lyn4ever29/pipy_server",
            "stars": 3,
            "img": "https://avatars.githubusercontent.com/u/25952589?s=40&v=4",
            "owner": "Lyn4ever29",
            "repo_name": "pipy_server"
          },
          {
            "name": "Finboost/finboost-ml",
            "stars": 3,
            "img": "https://avatars.githubusercontent.com/u/169436477?s=40&v=4",
            "owner": "Finboost",
            "repo_name": "finboost-ml"
          },
          {
            "name": "dlt-hub/llm_adapter",
            "stars": 3,
            "img": "https://avatars.githubusercontent.com/u/89419010?s=40&v=4",
            "owner": "dlt-hub",
            "repo_name": "llm_adapter"
          },
          {
            "name": "Tsadoq/haystack-documentation-agent",
            "stars": 3,
            "img": "https://avatars.githubusercontent.com/u/28503469?s=40&v=4",
            "owner": "Tsadoq",
            "repo_name": "haystack-documentation-agent"
          },
          {
            "name": "UW-Madison-DSI/ask-xDD",
            "stars": 3,
            "img": "https://avatars.githubusercontent.com/u/62457943?s=40&v=4",
            "owner": "UW-Madison-DSI",
            "repo_name": "ask-xDD"
          },
          {
            "name": "machinelearningzuu/mastering-llm-apps",
            "stars": 3,
            "img": "https://avatars.githubusercontent.com/u/41842488?s=40&v=4",
            "owner": "machinelearningzuu",
            "repo_name": "mastering-llm-apps"
          },
          {
            "name": "zhiyiyi/stock-price-predition",
            "stars": 3,
            "img": "https://avatars.githubusercontent.com/u/36960178?s=40&v=4",
            "owner": "zhiyiyi",
            "repo_name": "stock-price-predition"
          },
          {
            "name": "c0ntradicti0n/system",
            "stars": 3,
            "img": "https://avatars.githubusercontent.com/u/46457469?s=40&v=4",
            "owner": "c0ntradicti0n",
            "repo_name": "system"
          },
          {
            "name": "riadibadulla/SmartRedBox",
            "stars": 3,
            "img": "https://avatars.githubusercontent.com/u/33466778?s=40&v=4",
            "owner": "riadibadulla",
            "repo_name": "SmartRedBox"
          },
          {
            "name": "BobMerkus/ADS-LLM-QA",
            "stars": 3,
            "img": "https://avatars.githubusercontent.com/u/23738320?s=40&v=4",
            "owner": "BobMerkus",
            "repo_name": "ADS-LLM-QA"
          },
          {
            "name": "kevin-pek/document-semantic-search",
            "stars": 3,
            "img": "https://avatars.githubusercontent.com/u/76090938?s=40&v=4",
            "owner": "kevin-pek",
            "repo_name": "document-semantic-search"
          },
          {
            "name": "ekimetrics/climategpt",
            "stars": 3,
            "img": "https://avatars.githubusercontent.com/u/2689898?s=40&v=4",
            "owner": "ekimetrics",
            "repo_name": "climategpt"
          },
          {
            "name": "Rami-Ismael/UTD-chat-bot",
            "stars": 3,
            "img": "https://avatars.githubusercontent.com/u/39520903?s=40&v=4",
            "owner": "Rami-Ismael",
            "repo_name": "UTD-chat-bot"
          },
          {
            "name": "jonas-nothnagel/SDSN-tool",
            "stars": 3,
            "img": "https://avatars.githubusercontent.com/u/41116329?s=40&v=4",
            "owner": "jonas-nothnagel",
            "repo_name": "SDSN-tool"
          },
          {
            "name": "prateekralhan/Haystack-based-QA-system",
            "stars": 3,
            "img": "https://avatars.githubusercontent.com/u/29462447?s=40&v=4",
            "owner": "prateekralhan",
            "repo_name": "Haystack-based-QA-system"
          },
          {
            "name": "ugm2/neural-search-demo",
            "stars": 3,
            "img": "https://avatars.githubusercontent.com/u/25923343?s=40&v=4",
            "owner": "ugm2",
            "repo_name": "neural-search-demo"
          },
          {
            "name": "UCLComputerScience/COMP0016_2020_21_Team8",
            "stars": 3,
            "img": "https://avatars.githubusercontent.com/u/26434653?s=40&v=4",
            "owner": "UCLComputerScience",
            "repo_name": "COMP0016_2020_21_Team8"
          },
          {
            "name": "Hisarlik/simpleTextCLEF",
            "stars": 3,
            "img": "https://avatars.githubusercontent.com/u/1486603?s=40&v=4",
            "owner": "Hisarlik",
            "repo_name": "simpleTextCLEF"
          },
          {
            "name": "byukan/bookrec",
            "stars": 3,
            "img": "https://avatars.githubusercontent.com/u/12012907?s=40&v=4",
            "owner": "byukan",
            "repo_name": "bookrec"
          },
          {
            "name": "daniel-gomm/document-corpus-generation-pipelines",
            "stars": 3,
            "img": "https://avatars.githubusercontent.com/u/63717948?s=40&v=4",
            "owner": "daniel-gomm",
            "repo_name": "document-corpus-generation-pipelines"
          },
          {
            "name": "UrosOgrizovic/FitBot",
            "stars": 3,
            "img": "https://avatars.githubusercontent.com/u/25843402?s=40&v=4",
            "owner": "UrosOgrizovic",
            "repo_name": "FitBot"
          },
          {
            "name": "MesumRaza/qa_quran_heroku",
            "stars": 3,
            "img": "https://avatars.githubusercontent.com/u/22402962?s=40&v=4",
            "owner": "MesumRaza",
            "repo_name": "qa_quran_heroku"
          },
          {
            "name": "venuraja79/qa-api",
            "stars": 3,
            "img": "https://avatars.githubusercontent.com/u/10090460?s=40&v=4",
            "owner": "venuraja79",
            "repo_name": "qa-api"
          },
          {
            "name": "hassanjawwad12/text-to-image-search",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/113373773?s=40&v=4",
            "owner": "hassanjawwad12",
            "repo_name": "text-to-image-search"
          },
          {
            "name": "tl2309/SRAG",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/144661509?s=40&v=4",
            "owner": "tl2309",
            "repo_name": "SRAG"
          },
          {
            "name": "MubasharSiddique/LLM-YouTube-Transcript-Summarizer",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/35160292?s=40&v=4",
            "owner": "MubasharSiddique",
            "repo_name": "LLM-YouTube-Transcript-Summarizer"
          },
          {
            "name": "nutco21ris/ai-magic-recipe-generator",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/130265036?s=40&v=4",
            "owner": "nutco21ris",
            "repo_name": "ai-magic-recipe-generator"
          },
          {
            "name": "Rishav-Paramhans/Video_Summarization_Using_AI",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/65668108?s=40&v=4",
            "owner": "Rishav-Paramhans",
            "repo_name": "Video_Summarization_Using_AI"
          },
          {
            "name": "dmitrimahayana/Py-Haystack-Semantic",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/10221496?s=40&v=4",
            "owner": "dmitrimahayana",
            "repo_name": "Py-Haystack-Semantic"
          },
          {
            "name": "shortfastgood/AI-Lab",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/615499?s=40&v=4",
            "owner": "shortfastgood",
            "repo_name": "AI-Lab"
          },
          {
            "name": "0xRy4n/airistotle",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/4227041?s=40&v=4",
            "owner": "0xRy4n",
            "repo_name": "airistotle"
          },
          {
            "name": "dtaivpp/opensearch-conversational-search",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/17506770?s=40&v=4",
            "owner": "dtaivpp",
            "repo_name": "opensearch-conversational-search"
          },
          {
            "name": "ravipratap366/RAG-Mistral7B-Haystack-Weaviate-FastAPI",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/53921865?s=40&v=4",
            "owner": "ravipratap366",
            "repo_name": "RAG-Mistral7B-Haystack-Weaviate-FastAPI"
          },
          {
            "name": "im-sanka/magicalytics",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/61604177?s=40&v=4",
            "owner": "im-sanka",
            "repo_name": "magicalytics"
          },
          {
            "name": "ploomber/hacktoberfest-2023-project",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/60114551?s=40&v=4",
            "owner": "ploomber",
            "repo_name": "hacktoberfest-2023-project"
          },
          {
            "name": "balqaasem/youlit-ai",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/15086345?s=40&v=4",
            "owner": "balqaasem",
            "repo_name": "youlit-ai"
          },
          {
            "name": "NOVA-IMS-Innovation-and-Analytics-Lab/MapIntel",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/73640439?s=40&v=4",
            "owner": "NOVA-IMS-Innovation-and-Analytics-Lab",
            "repo_name": "MapIntel"
          },
          {
            "name": "devilteo911/privateGPT",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/15840439?s=40&v=4",
            "owner": "devilteo911",
            "repo_name": "privateGPT"
          },
          {
            "name": "Subodh7976/OnePiece-Question-Answering",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/97154559?s=40&v=4",
            "owner": "Subodh7976",
            "repo_name": "OnePiece-Question-Answering"
          },
          {
            "name": "anakin87/haystack-entailment-checker",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/44616784?s=40&v=4",
            "owner": "anakin87",
            "repo_name": "haystack-entailment-checker"
          },
          {
            "name": "StrangeNPC/HaystackChatbotStreamlit",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/95240891?s=40&v=4",
            "owner": "StrangeNPC",
            "repo_name": "HaystackChatbotStreamlit"
          },
          {
            "name": "Lewington-pitsos/oopscover",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/24558325?s=40&v=4",
            "owner": "Lewington-pitsos",
            "repo_name": "oopscover"
          },
          {
            "name": "davidberenstein1957/memory-palace",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/25269220?s=40&v=4",
            "owner": "davidberenstein1957",
            "repo_name": "memory-palace"
          },
          {
            "name": "abhiamishra/RamayanaGPT",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/52009380?s=40&v=4",
            "owner": "abhiamishra",
            "repo_name": "RamayanaGPT"
          },
          {
            "name": "renyuanL/_JosephLin_2023",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/6368761?s=40&v=4",
            "owner": "renyuanL",
            "repo_name": "_JosephLin_2023"
          },
          {
            "name": "Forbu/LoiLibre",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/11457947?s=40&v=4",
            "owner": "Forbu",
            "repo_name": "LoiLibre"
          },
          {
            "name": "recrudesce/haystack_threshold_node",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/6450799?s=40&v=4",
            "owner": "recrudesce",
            "repo_name": "haystack_threshold_node"
          },
          {
            "name": "Bubbalubagus/semanticsearch",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/16597445?s=40&v=4",
            "owner": "Bubbalubagus",
            "repo_name": "semanticsearch"
          },
          {
            "name": "recrudesce/haystack_translate_node",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/6450799?s=40&v=4",
            "owner": "recrudesce",
            "repo_name": "haystack_translate_node"
          },
          {
            "name": "aaalexlit/cc-evidences-api",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/116374290?s=40&v=4",
            "owner": "aaalexlit",
            "repo_name": "cc-evidences-api"
          },
          {
            "name": "danielbichuetti/haystack",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/90110233?s=40&v=4",
            "owner": "danielbichuetti",
            "repo_name": "haystack"
          },
          {
            "name": "MartinTheDoge/fDet",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/123597562?s=40&v=4",
            "owner": "MartinTheDoge",
            "repo_name": "fDet"
          },
          {
            "name": "rkaunismaa/NaturalLanguageProcessingWithTransformers",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/11901059?s=40&v=4",
            "owner": "rkaunismaa",
            "repo_name": "NaturalLanguageProcessingWithTransformers"
          },
          {
            "name": "Shingo-Kamata/japanese_qa_demo_with_haystack_and_es",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/19622710?s=40&v=4",
            "owner": "Shingo-Kamata",
            "repo_name": "japanese_qa_demo_with_haystack_and_es"
          },
          {
            "name": "gizdatalab/haystack_utils",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/71088041?s=40&v=4",
            "owner": "gizdatalab",
            "repo_name": "haystack_utils"
          },
          {
            "name": "deeplearn-ai/covid-qa-udemy",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/88691020?s=40&v=4",
            "owner": "deeplearn-ai",
            "repo_name": "covid-qa-udemy"
          },
          {
            "name": "zbw/Funder-NER",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/376765?s=40&v=4",
            "owner": "zbw",
            "repo_name": "Funder-NER"
          },
          {
            "name": "lambdaofgod/github_search",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/3647577?s=40&v=4",
            "owner": "lambdaofgod",
            "repo_name": "github_search"
          },
          {
            "name": "arulpugazh/pubmedbot",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/1398646?s=40&v=4",
            "owner": "arulpugazh",
            "repo_name": "pubmedbot"
          },
          {
            "name": "xxxsergxxx/Social_Networks_Comments_Prioritization_System",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/39562636?s=40&v=4",
            "owner": "xxxsergxxx",
            "repo_name": "Social_Networks_Comments_Prioritization_System"
          },
          {
            "name": "Blacksujit/Data-thon-2025",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/148805811?s=40&v=4",
            "owner": "Blacksujit",
            "repo_name": "Data-thon-2025"
          },
          {
            "name": "Erwin2307-py/Paper",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/197731009?s=40&v=4",
            "owner": "Erwin2307-py",
            "repo_name": "Paper"
          },
          {
            "name": "MisioMusic08/Semisizer",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/129396003?s=40&v=4",
            "owner": "MisioMusic08",
            "repo_name": "Semisizer"
          },
          {
            "name": "artefactory/redis-player-one",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/86767100?s=40&v=4",
            "owner": "artefactory",
            "repo_name": "redis-player-one"
          },
          {
            "name": "shs1018/iM_DiGital_Banker_academy",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/90491841?s=40&v=4",
            "owner": "shs1018",
            "repo_name": "iM_DiGital_Banker_academy"
          },
          {
            "name": "sukeshrs/machine-learning-utils",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/9681508?s=40&v=4",
            "owner": "sukeshrs",
            "repo_name": "machine-learning-utils"
          },
          {
            "name": "tianyi-gu/phillipian",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/61768110?s=40&v=4",
            "owner": "tianyi-gu",
            "repo_name": "phillipian"
          },
          {
            "name": "NI3singh/AI-Compilance-Checker",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/115877777?s=40&v=4",
            "owner": "NI3singh",
            "repo_name": "AI-Compilance-Checker"
          },
          {
            "name": "NI3singh/IITGN-and-Odoo-Hackathon",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/115877777?s=40&v=4",
            "owner": "NI3singh",
            "repo_name": "IITGN-and-Odoo-Hackathon"
          },
          {
            "name": "Nevy11/therapistGpt",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/125488481?s=40&v=4",
            "owner": "Nevy11",
            "repo_name": "therapistGpt"
          },
          {
            "name": "YashBaravaliya/MedGuide-AI",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/110822197?s=40&v=4",
            "owner": "YashBaravaliya",
            "repo_name": "MedGuide-AI"
          },
          {
            "name": "NI3singh/Stock_Price_Forcasting",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/115877777?s=40&v=4",
            "owner": "NI3singh",
            "repo_name": "Stock_Price_Forcasting"
          },
          {
            "name": "AayushSalvi/Finance-Chatbot-using-Mistral-7B",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/65705434?s=40&v=4",
            "owner": "AayushSalvi",
            "repo_name": "Finance-Chatbot-using-Mistral-7B"
          },
          {
            "name": "NI3singh/FINAL_YEAR_PROJECT-1",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/115877777?s=40&v=4",
            "owner": "NI3singh",
            "repo_name": "FINAL_YEAR_PROJECT-1"
          },
          {
            "name": "otaviosoaresp/rag_bot_ollama",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/38701225?s=40&v=4",
            "owner": "otaviosoaresp",
            "repo_name": "rag_bot_ollama"
          },
          {
            "name": "rohithekm/AI-ChatBot---opensourse",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/160238864?s=40&v=4",
            "owner": "rohithekm",
            "repo_name": "AI-ChatBot---opensourse"
          },
          {
            "name": "vhidvz/question-answering",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/54132701?s=40&v=4",
            "owner": "vhidvz",
            "repo_name": "question-answering"
          },
          {
            "name": "tandangwang/ai-powered-customer-service-system",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/43236091?s=40&v=4",
            "owner": "tandangwang",
            "repo_name": "ai-powered-customer-service-system"
          },
          {
            "name": "opscidia/science-checker",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/57664197?s=40&v=4",
            "owner": "opscidia",
            "repo_name": "science-checker"
          },
          {
            "name": "alecrimi/chainlit_mistral_nonlocal",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/16406658?s=40&v=4",
            "owner": "alecrimi",
            "repo_name": "chainlit_mistral_nonlocal"
          },
          {
            "name": "5ud21/Weaviate-Document-Store",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/88098618?s=40&v=4",
            "owner": "5ud21",
            "repo_name": "Weaviate-Document-Store"
          },
          {
            "name": "adityaraj1105/RAGs-vs-Fine-tuning",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/119999602?s=40&v=4",
            "owner": "adityaraj1105",
            "repo_name": "RAGs-vs-Fine-tuning"
          },
          {
            "name": "Progressive-Insurance/chainlit",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/33731436?s=40&v=4",
            "owner": "Progressive-Insurance",
            "repo_name": "chainlit"
          },
          {
            "name": "DJIMYDAMESSE/Transformers",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/101936118?s=40&v=4",
            "owner": "DJIMYDAMESSE",
            "repo_name": "Transformers"
          },
          {
            "name": "knaig/stitch",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/2597693?s=40&v=4",
            "owner": "knaig",
            "repo_name": "stitch"
          },
          {
            "name": "joybratasarkar/Ai-inteview",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/72306307?s=40&v=4",
            "owner": "joybratasarkar",
            "repo_name": "Ai-inteview"
          },
          {
            "name": "ai4africagroup/telcom_llm",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/171547595?s=40&v=4",
            "owner": "ai4africagroup",
            "repo_name": "telcom_llm"
          },
          {
            "name": "Ravikiran0303/YouTube_summarizer_",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/147860071?s=40&v=4",
            "owner": "Ravikiran0303",
            "repo_name": "YouTube_summarizer_"
          },
          {
            "name": "viveknair6915/Video-Summarization-System-LLM",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/117741254?s=40&v=4",
            "owner": "viveknair6915",
            "repo_name": "Video-Summarization-System-LLM"
          },
          {
            "name": "AIbyMLcom/Chainlit-examples",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/149811768?s=40&v=4",
            "owner": "AIbyMLcom",
            "repo_name": "Chainlit-examples"
          },
          {
            "name": "ekimkk/CMU-Advanced-NLP-Assignment-2-End-to-end-NLP-System-Building",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/142109497?s=40&v=4",
            "owner": "ekimkk",
            "repo_name": "CMU-Advanced-NLP-Assignment-2-End-to-end-NLP-System-Building"
          },
          {
            "name": "exploringweirdmachines/chat-with-a-pdf",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/87807046?s=40&v=4",
            "owner": "exploringweirdmachines",
            "repo_name": "chat-with-a-pdf"
          },
          {
            "name": "SunilKumarPradhan/Actions",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/89890497?s=40&v=4",
            "owner": "SunilKumarPradhan",
            "repo_name": "Actions"
          },
          {
            "name": "Oldentomato/elasticsearch_test",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/26248635?s=40&v=4",
            "owner": "Oldentomato",
            "repo_name": "elasticsearch_test"
          },
          {
            "name": "p-vbordei/RAG-SOTA",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/66299953?s=40&v=4",
            "owner": "p-vbordei",
            "repo_name": "RAG-SOTA"
          },
          {
            "name": "ami-zou/document-viewer-chatbot",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/15065020?s=40&v=4",
            "owner": "ami-zou",
            "repo_name": "document-viewer-chatbot"
          },
          {
            "name": "Eduds007/LanguageModels",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/63738051?s=40&v=4",
            "owner": "Eduds007",
            "repo_name": "LanguageModels"
          },
          {
            "name": "srikanthpl/MY_PROJECTS",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/124775319?s=40&v=4",
            "owner": "srikanthpl",
            "repo_name": "MY_PROJECTS"
          },
          {
            "name": "ioniccommerce/ionic_haystack",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/140454189?s=40&v=4",
            "owner": "ioniccommerce",
            "repo_name": "ionic_haystack"
          },
          {
            "name": "VivekSai07/Text2Image-Search-System",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/113660738?s=40&v=4",
            "owner": "VivekSai07",
            "repo_name": "Text2Image-Search-System"
          },
          {
            "name": "pks20iitk/Haystack-and-Mistral-7B-RAG-Implementation",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/78561615?s=40&v=4",
            "owner": "pks20iitk",
            "repo_name": "Haystack-and-Mistral-7B-RAG-Implementation"
          },
          {
            "name": "seok-hee97/Transformers-for-Natural-Language-Processing",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/98581131?s=40&v=4",
            "owner": "seok-hee97",
            "repo_name": "Transformers-for-Natural-Language-Processing"
          },
          {
            "name": "Achaarya-AI/Acharya-core",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/155107177?s=40&v=4",
            "owner": "Achaarya-AI",
            "repo_name": "Acharya-core"
          },
          {
            "name": "NgoDangKhoauit/Rocks-checking",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/131435439?s=40&v=4",
            "owner": "NgoDangKhoauit",
            "repo_name": "Rocks-checking"
          },
          {
            "name": "peterkchung/logos-ai",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/77805646?s=40&v=4",
            "owner": "peterkchung",
            "repo_name": "logos-ai"
          },
          {
            "name": "peterkchung/quickchat",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/77805646?s=40&v=4",
            "owner": "peterkchung",
            "repo_name": "quickchat"
          },
          {
            "name": "alaeddinehamroun/Scholar-s-Ally",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/71340201?s=40&v=4",
            "owner": "alaeddinehamroun",
            "repo_name": "Scholar-s-Ally"
          },
          {
            "name": "greymini/Miss7B",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/79689279?s=40&v=4",
            "owner": "greymini",
            "repo_name": "Miss7B"
          },
          {
            "name": "samroj95/GenAI---Recommeder-App-main",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/92337579?s=40&v=4",
            "owner": "samroj95",
            "repo_name": "GenAI---Recommeder-App-main"
          },
          {
            "name": "guptashrey/LLM-Training-and-Fine-tuning",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/17765731?s=40&v=4",
            "owner": "guptashrey",
            "repo_name": "LLM-Training-and-Fine-tuning"
          },
          {
            "name": "philiphess1/CustomerServiceChatbot",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/106563229?s=40&v=4",
            "owner": "philiphess1",
            "repo_name": "CustomerServiceChatbot"
          },
          {
            "name": "Oushesh/Impact_Nexus005",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/5883845?s=40&v=4",
            "owner": "Oushesh",
            "repo_name": "Impact_Nexus005"
          },
          {
            "name": "Siddhartha082/Text_to_Image_Fashion_Search_Streamlit",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/110781138?s=40&v=4",
            "owner": "Siddhartha082",
            "repo_name": "Text_to_Image_Fashion_Search_Streamlit"
          },
          {
            "name": "a-romero/qrage",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/7581333?s=40&v=4",
            "owner": "a-romero",
            "repo_name": "qrage"
          },
          {
            "name": "Vinayak-HUB1/Rag-Implementation-using-haystack-and-chainlit",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/77567868?s=40&v=4",
            "owner": "Vinayak-HUB1",
            "repo_name": "Rag-Implementation-using-haystack-and-chainlit"
          },
          {
            "name": "RedTachyon/tutor-at-home",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/19414946?s=40&v=4",
            "owner": "RedTachyon",
            "repo_name": "tutor-at-home"
          },
          {
            "name": "raaasin/Drillie",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/97795923?s=40&v=4",
            "owner": "raaasin",
            "repo_name": "Drillie"
          },
          {
            "name": "cnvrg/FastRAG",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/44049243?s=40&v=4",
            "owner": "cnvrg",
            "repo_name": "FastRAG"
          },
          {
            "name": "blur0b0t/mh_shell",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/143605527?s=40&v=4",
            "owner": "blur0b0t",
            "repo_name": "mh_shell"
          },
          {
            "name": "kndeepak/LLM-RAG-invoice-Local_CPU",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/55972417?s=40&v=4",
            "owner": "kndeepak",
            "repo_name": "LLM-RAG-invoice-Local_CPU"
          },
          {
            "name": "Qing145/QA_Web",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/68147254?s=40&v=4",
            "owner": "Qing145",
            "repo_name": "QA_Web"
          },
          {
            "name": "asabade/Chat-Summarization",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/28676938?s=40&v=4",
            "owner": "asabade",
            "repo_name": "Chat-Summarization"
          },
          {
            "name": "Ankush-Chander/cricket-ama",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/4995297?s=40&v=4",
            "owner": "Ankush-Chander",
            "repo_name": "cricket-ama"
          },
          {
            "name": "sumanghosh1234/haystack-streamlit-weaviate-mistral",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/28214681?s=40&v=4",
            "owner": "sumanghosh1234",
            "repo_name": "haystack-streamlit-weaviate-mistral"
          },
          {
            "name": "MentoriaPloomber/RAG_HayStack_QA",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/145875106?s=40&v=4",
            "owner": "MentoriaPloomber",
            "repo_name": "RAG_HayStack_QA"
          },
          {
            "name": "NevyrIO/haystack-demo",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/107774787?s=40&v=4",
            "owner": "NevyrIO",
            "repo_name": "haystack-demo"
          },
          {
            "name": "abdullahnizami77/NLP-Miners",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/86009804?s=40&v=4",
            "owner": "abdullahnizami77",
            "repo_name": "NLP-Miners"
          },
          {
            "name": "bogdankostic/notion-haystack",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/48713846?s=40&v=4",
            "owner": "bogdankostic",
            "repo_name": "notion-haystack"
          },
          {
            "name": "delatorrena2016/sql-etl-analytics",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/37124283?s=40&v=4",
            "owner": "delatorrena2016",
            "repo_name": "sql-etl-analytics"
          },
          {
            "name": "Edgar-Pacheco/Team4HacktoberFest2023",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/87995679?s=40&v=4",
            "owner": "Edgar-Pacheco",
            "repo_name": "Team4HacktoberFest2023"
          },
          {
            "name": "Rakib-data-scientist/GenAI-Recommeder-App",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/137823730?s=40&v=4",
            "owner": "Rakib-data-scientist",
            "repo_name": "GenAI-Recommeder-App"
          },
          {
            "name": "Landlucas/content-relevance-score-api",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/20175697?s=40&v=4",
            "owner": "Landlucas",
            "repo_name": "content-relevance-score-api"
          },
          {
            "name": "SamthinkGit/llama2-terminal",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/92941012?s=40&v=4",
            "owner": "SamthinkGit",
            "repo_name": "llama2-terminal"
          },
          {
            "name": "BobbyLumpkin/docs2chat",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/50218116?s=40&v=4",
            "owner": "BobbyLumpkin",
            "repo_name": "docs2chat"
          },
          {
            "name": "adborroto/transformers",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/2942602?s=40&v=4",
            "owner": "adborroto",
            "repo_name": "transformers"
          },
          {
            "name": "Azizkhaled/NLP-Concepts-and-Transformers",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/35635341?s=40&v=4",
            "owner": "Azizkhaled",
            "repo_name": "NLP-Concepts-and-Transformers"
          },
          {
            "name": "hanzotaz/smart_receptionist_system",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/55962807?s=40&v=4",
            "owner": "hanzotaz",
            "repo_name": "smart_receptionist_system"
          },
          {
            "name": "bhattarai333/AI-Resume",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/10687373?s=40&v=4",
            "owner": "bhattarai333",
            "repo_name": "AI-Resume"
          },
          {
            "name": "yasyf/haystack-hybrid-embedding",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/709645?s=40&v=4",
            "owner": "yasyf",
            "repo_name": "haystack-hybrid-embedding"
          },
          {
            "name": "leomaurodesenv/qasports-dataset-scripts",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/11961778?s=40&v=4",
            "owner": "leomaurodesenv",
            "repo_name": "qasports-dataset-scripts"
          },
          {
            "name": "cyberspyde/jbnu",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/52118091?s=40&v=4",
            "owner": "cyberspyde",
            "repo_name": "jbnu"
          },
          {
            "name": "LoyumM/Machine-Learning-and-Deep-learning",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/90868002?s=40&v=4",
            "owner": "LoyumM",
            "repo_name": "Machine-Learning-and-Deep-learning"
          },
          {
            "name": "Hossamster/QA-System-Covid19-Bert",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/89027268?s=40&v=4",
            "owner": "Hossamster",
            "repo_name": "QA-System-Covid19-Bert"
          },
          {
            "name": "simondanielsson/SEB-OpenQA",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/70206058?s=40&v=4",
            "owner": "simondanielsson",
            "repo_name": "SEB-OpenQA"
          },
          {
            "name": "ankitshaw/CSE635_NLP_Project",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/25771394?s=40&v=4",
            "owner": "ankitshaw",
            "repo_name": "CSE635_NLP_Project"
          },
          {
            "name": "maximuslee1226/NLP",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/17769293?s=40&v=4",
            "owner": "maximuslee1226",
            "repo_name": "NLP"
          },
          {
            "name": "aiswaryasankar/dbrief",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/7874177?s=40&v=4",
            "owner": "aiswaryasankar",
            "repo_name": "dbrief"
          },
          {
            "name": "gjreda/haystack-pdf-bot",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/921995?s=40&v=4",
            "owner": "gjreda",
            "repo_name": "haystack-pdf-bot"
          },
          {
            "name": "krishika-r/semantic_search",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/36788439?s=40&v=4",
            "owner": "krishika-r",
            "repo_name": "semantic_search"
          },
          {
            "name": "ameliekong609/NLP-foundation",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/97581675?s=40&v=4",
            "owner": "ameliekong609",
            "repo_name": "NLP-foundation"
          },
          {
            "name": "pmarkun/redeiro-bot",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/682827?s=40&v=4",
            "owner": "pmarkun",
            "repo_name": "redeiro-bot"
          },
          {
            "name": "TuanaCelik/find-the-animal",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/15802862?s=40&v=4",
            "owner": "TuanaCelik",
            "repo_name": "find-the-animal"
          },
          {
            "name": "mlevitt-deloitte/Policy_Recon_HDSI",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/114176092?s=40&v=4",
            "owner": "mlevitt-deloitte",
            "repo_name": "Policy_Recon_HDSI"
          },
          {
            "name": "rgs2151/GraphWelder",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/26141934?s=40&v=4",
            "owner": "rgs2151",
            "repo_name": "GraphWelder"
          },
          {
            "name": "textomatic/nutrition-bot",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/102237745?s=40&v=4",
            "owner": "textomatic",
            "repo_name": "nutrition-bot"
          },
          {
            "name": "besson/ml-lab",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/516122?s=40&v=4",
            "owner": "besson",
            "repo_name": "ml-lab"
          },
          {
            "name": "HaiCuCai-00/nlp_haystack",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/69022895?s=40&v=4",
            "owner": "HaiCuCai-00",
            "repo_name": "nlp_haystack"
          },
          {
            "name": "Ok3ks/haystack",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/80680311?s=40&v=4",
            "owner": "Ok3ks",
            "repo_name": "haystack"
          },
          {
            "name": "Python-Repository-Hub/haystack",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/76585909?s=40&v=4",
            "owner": "Python-Repository-Hub",
            "repo_name": "haystack"
          },
          {
            "name": "RwGrid/haystack",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/33643615?s=40&v=4",
            "owner": "RwGrid",
            "repo_name": "haystack"
          },
          {
            "name": "Funkmyster/haystack",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/401350?s=40&v=4",
            "owner": "Funkmyster",
            "repo_name": "haystack"
          },
          {
            "name": "tamanna18/haystack",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/57901189?s=40&v=4",
            "owner": "tamanna18",
            "repo_name": "haystack"
          },
          {
            "name": "hsm207/haystack",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/2398765?s=40&v=4",
            "owner": "hsm207",
            "repo_name": "haystack"
          },
          {
            "name": "creatorrr/haystack",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/931887?s=40&v=4",
            "owner": "creatorrr",
            "repo_name": "haystack"
          },
          {
            "name": "mohamedelnagar1/haystack",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/38164598?s=40&v=4",
            "owner": "mohamedelnagar1",
            "repo_name": "haystack"
          },
          {
            "name": "jamescalam/haystack",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/35938317?s=40&v=4",
            "owner": "jamescalam",
            "repo_name": "haystack"
          },
          {
            "name": "davgit/haystack",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/294138?s=40&v=4",
            "owner": "davgit",
            "repo_name": "haystack"
          },
          {
            "name": "mathew55/haystack",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/16523777?s=40&v=4",
            "owner": "mathew55",
            "repo_name": "haystack"
          },
          {
            "name": "ArzelaAscoIi/haystack",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/37148029?s=40&v=4",
            "owner": "ArzelaAscoIi",
            "repo_name": "haystack"
          },
          {
            "name": "cox-j/gamechanger",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/100856227?s=40&v=4",
            "owner": "cox-j",
            "repo_name": "gamechanger"
          },
          {
            "name": "cshikai/mirror",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/16132347?s=40&v=4",
            "owner": "cshikai",
            "repo_name": "mirror"
          },
          {
            "name": "agent87/IhuguraChatBotUX",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/24498800?s=40&v=4",
            "owner": "agent87",
            "repo_name": "IhuguraChatBotUX"
          },
          {
            "name": "LeaS2/Explainable_NLP",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/105351249?s=40&v=4",
            "owner": "LeaS2",
            "repo_name": "Explainable_NLP"
          },
          {
            "name": "AlbertoVilla87/web-file",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/77957428?s=40&v=4",
            "owner": "AlbertoVilla87",
            "repo_name": "web-file"
          },
          {
            "name": "KKogaa/jester-bot",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/64268254?s=40&v=4",
            "owner": "KKogaa",
            "repo_name": "jester-bot"
          },
          {
            "name": "beamscource/nlp_apps",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/20405070?s=40&v=4",
            "owner": "beamscource",
            "repo_name": "nlp_apps"
          },
          {
            "name": "trijini/Natural-Language-Processing-with-Transformers-Lewis-Tunstall-Leandro-von-Werra-Thomas-Wolf-z-li",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/110159747?s=40&v=4",
            "owner": "trijini",
            "repo_name": "Natural-Language-Processing-with-Transformers-Lewis-Tunstall-Leandro-von-Werra-Thomas-Wolf-z-li"
          },
          {
            "name": "Jackiebibili/algo-chatbox-nlp",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/61096708?s=40&v=4",
            "owner": "Jackiebibili",
            "repo_name": "algo-chatbox-nlp"
          },
          {
            "name": "setren/fastapi",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/70955964?s=40&v=4",
            "owner": "setren",
            "repo_name": "fastapi"
          },
          {
            "name": "gziz/question-answer-ai",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/69287011?s=40&v=4",
            "owner": "gziz",
            "repo_name": "question-answer-ai"
          },
          {
            "name": "descentis/contract-review",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/3228021?s=40&v=4",
            "owner": "descentis",
            "repo_name": "contract-review"
          },
          {
            "name": "Saranga99/data-science-projects",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/65158515?s=40&v=4",
            "owner": "Saranga99",
            "repo_name": "data-science-projects"
          },
          {
            "name": "doinakis/Real-Time-News-Assistant",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/28433709?s=40&v=4",
            "owner": "doinakis",
            "repo_name": "Real-Time-News-Assistant"
          },
          {
            "name": "amitbiswas1999/Covid-QA-system-using-BERT-",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/88819794?s=40&v=4",
            "owner": "amitbiswas1999",
            "repo_name": "Covid-QA-system-using-BERT-"
          },
          {
            "name": "bhakthil/student-assist",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/12928110?s=40&v=4",
            "owner": "bhakthil",
            "repo_name": "student-assist"
          },
          {
            "name": "aytugkaya/notebooks",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/45581689?s=40&v=4",
            "owner": "aytugkaya",
            "repo_name": "notebooks"
          },
          {
            "name": "Bijay2305/sentence_similarity2",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/52318917?s=40&v=4",
            "owner": "Bijay2305",
            "repo_name": "sentence_similarity2"
          },
          {
            "name": "sbadecker/ask_me_anything",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/19605053?s=40&v=4",
            "owner": "sbadecker",
            "repo_name": "ask_me_anything"
          },
          {
            "name": "ekmixon/gamechanger-ml",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/6691194?s=40&v=4",
            "owner": "ekmixon",
            "repo_name": "gamechanger-ml"
          },
          {
            "name": "augustodn/NLP_with_Transformers",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/1857412?s=40&v=4",
            "owner": "augustodn",
            "repo_name": "NLP_with_Transformers"
          },
          {
            "name": "axirestech/cm_project",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/87490464?s=40&v=4",
            "owner": "axirestech",
            "repo_name": "cm_project"
          },
          {
            "name": "JungeAlexander/kbase_search",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/6056731?s=40&v=4",
            "owner": "JungeAlexander",
            "repo_name": "kbase_search"
          },
          {
            "name": "manuelyhvh/nlp-with-transformers",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/55846269?s=40&v=4",
            "owner": "manuelyhvh",
            "repo_name": "nlp-with-transformers"
          },
          {
            "name": "cgleone/gui_v0",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/45293022?s=40&v=4",
            "owner": "cgleone",
            "repo_name": "gui_v0"
          },
          {
            "name": "Kau5h1K/ds5500-userprivacy",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/40333107?s=40&v=4",
            "owner": "Kau5h1K",
            "repo_name": "ds5500-userprivacy"
          },
          {
            "name": "huygensravel/hotelreviews_sentiment_analysis",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/5883285?s=40&v=4",
            "owner": "huygensravel",
            "repo_name": "hotelreviews_sentiment_analysis"
          },
          {
            "name": "SGrannemann/Star-Trek-Scripts-NLP-Playground",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/64766572?s=40&v=4",
            "owner": "SGrannemann",
            "repo_name": "Star-Trek-Scripts-NLP-Playground"
          },
          {
            "name": "byukan/movie-rec",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/12012907?s=40&v=4",
            "owner": "byukan",
            "repo_name": "movie-rec"
          },
          {
            "name": "dhruv2600/flask_app",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/56503628?s=40&v=4",
            "owner": "dhruv2600",
            "repo_name": "flask_app"
          },
          {
            "name": "caplincapture/NLP",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/18247673?s=40&v=4",
            "owner": "caplincapture",
            "repo_name": "NLP"
          },
          {
            "name": "fptUniversityMCQSS/ModelRoberta",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/86480070?s=40&v=4",
            "owner": "fptUniversityMCQSS",
            "repo_name": "ModelRoberta"
          },
          {
            "name": "AhmedYounes94/MLM-NSP",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/68699166?s=40&v=4",
            "owner": "AhmedYounes94",
            "repo_name": "MLM-NSP"
          },
          {
            "name": "AetherPrior/QA-BERT",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/39939017?s=40&v=4",
            "owner": "AetherPrior",
            "repo_name": "QA-BERT"
          },
          {
            "name": "omerlevi2/NLP_Project",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/73997702?s=40&v=4",
            "owner": "omerlevi2",
            "repo_name": "NLP_Project"
          },
          {
            "name": "aaronbriel/jugglechat",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/5034032?s=40&v=4",
            "owner": "aaronbriel",
            "repo_name": "jugglechat"
          },
          {
            "name": "Accuro-Lab/Data-ML",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/65883023?s=40&v=4",
            "owner": "Accuro-Lab",
            "repo_name": "Data-ML"
          },
          {
            "name": "Paeaede/taschenhirn",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/16348173?s=40&v=4",
            "owner": "Paeaede",
            "repo_name": "taschenhirn"
          }
        ],
        "public_dependents_number": 320,
        "private_dependents_number": -320,
        "total_dependents_number": 320,
        "badges": {
          "total": "[![Generated by github-dependents-info](https://img.shields.io/static/v1?label=Used%20by&message=320&color=informational&logo=slickpic)](https://github.com/deepset-ai/Haystack/network/dependents?package_id=UGFja2FnZS03MTQ5NzEwOTc%3D)",
          "public": "[![Generated by github-dependents-info](https://img.shields.io/static/v1?label=Used%20by%20(public)&message=320&color=informational&logo=slickpic)](https://github.com/deepset-ai/Haystack/network/dependents?package_id=UGFja2FnZS03MTQ5NzEwOTc%3D)",
          "private": "[![Generated by github-dependents-info](https://img.shields.io/static/v1?label=Used%20by%20(private)&message=-320&color=informational&logo=slickpic)](https://github.com/deepset-ai/Haystack/network/dependents?package_id=UGFja2FnZS03MTQ5NzEwOTc%3D)",
          "stars": "[![Generated by github-dependents-info](https://img.shields.io/static/v1?label=Used%20by%20(stars)&message=22&color=informational&logo=slickpic)](https://github.com/deepset-ai/Haystack/network/dependents?package_id=UGFja2FnZS03MTQ5NzEwOTc%3D)"
        }
      }
    ]
  ]
}