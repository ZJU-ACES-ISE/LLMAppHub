{
  "all_public_dependent_repos": [
    {
      "name": "microsoft/autogen",
      "stars": 43573,
      "img": "https://avatars.githubusercontent.com/u/6154722?s=40&v=4",
      "owner": "microsoft",
      "repo_name": "autogen",
      "description": "A programming framework for agentic AI 🤖 PyPi: autogen-agentchat Discord: https://aka.ms/autogen-discord Office Hour: https://aka.ms/autogen-officehour",
      "homepage": "https://microsoft.github.io/autogen/",
      "language": "Python",
      "created_at": "2023-08-18T11:43:45Z",
      "updated_at": "2025-04-23T08:58:20Z",
      "topics": [
        "agentic",
        "agentic-agi",
        "agents",
        "ai",
        "autogen",
        "autogen-ecosystem",
        "chatgpt",
        "framework",
        "llm-agent",
        "llm-framework"
      ],
      "readme": "<a name=\"readme-top\"></a>\n\n<div align=\"center\">\n<img src=\"https://microsoft.github.io/autogen/0.2/img/ag.svg\" alt=\"AutoGen Logo\" width=\"100\">\n\n[![Twitter](https://img.shields.io/twitter/url/https/twitter.com/cloudposse.svg?style=social&label=Follow%20%40pyautogen)](https://twitter.com/pyautogen)\n[![LinkedIn](https://img.shields.io/badge/LinkedIn-Company?style=flat&logo=linkedin&logoColor=white)](https://www.linkedin.com/company/105812540)\n[![Discord](https://img.shields.io/badge/discord-chat-green?logo=discord)](https://aka.ms/autogen-discord)\n[![Documentation](https://img.shields.io/badge/Documentation-AutoGen-blue?logo=read-the-docs)](https://microsoft.github.io/autogen/)\n[![Blog](https://img.shields.io/badge/Blog-AutoGen-blue?logo=blogger)](https://devblogs.microsoft.com/autogen/)\n\n</div>\n\n<div align=\"center\" style=\"background-color: rgba(255, 235, 59, 0.5); padding: 10px; border-radius: 5px; margin: 20px 0;\">\n  <strong>Important:</strong> This is the official project. We are not affiliated with any fork or startup. See our <a href=\"https://x.com/pyautogen/status/1857264760951296210\">statement</a>.\n</div>\n\n# AutoGen\n\n**AutoGen** is a framework for creating multi-agent AI applications that can act autonomously or work alongside humans.\n\n## Installation\n\nAutoGen requires **Python 3.10 or later**.\n\n```bash\n# Install AgentChat and OpenAI client from Extensions\npip install -U \"autogen-agentchat\" \"autogen-ext[openai]\"\n```\n\nThe current stable version is v0.4. If you are upgrading from AutoGen v0.2, please refer to the [Migration Guide](https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/migration-guide.html) for detailed instructions on how to update your code and configurations.\n\n```bash\n# Install AutoGen Studio for no-code GUI\npip install -U \"autogenstudio\"\n```\n\n## Quickstart\n\n### Hello World\n\nCreate an assistant agent using OpenAI's GPT-4o model. See [other supported models](https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/tutorial/models.html).\n\n```python\nimport asyncio\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n\nasync def main() -> None:\n    model_client = OpenAIChatCompletionClient(model=\"gpt-4o\")\n    agent = AssistantAgent(\"assistant\", model_client=model_client)\n    print(await agent.run(task=\"Say 'Hello World!'\"))\n    await model_client.close()\n\nasyncio.run(main())\n```\n\n### Web Browsing Agent Team\n\nCreate a group chat team with a web surfer agent and a user proxy agent\nfor web browsing tasks. You need to install [playwright](https://playwright.dev/python/docs/library).\n\n```python\n# pip install -U autogen-agentchat autogen-ext[openai,web-surfer]\n# playwright install\nimport asyncio\nfrom autogen_agentchat.agents import UserProxyAgent\nfrom autogen_agentchat.conditions import TextMentionTermination\nfrom autogen_agentchat.teams import RoundRobinGroupChat\nfrom autogen_agentchat.ui import Console\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\nfrom autogen_ext.agents.web_surfer import MultimodalWebSurfer\n\nasync def main() -> None:\n    model_client = OpenAIChatCompletionClient(model=\"gpt-4o\")\n    # The web surfer will open a Chromium browser window to perform web browsing tasks.\n    web_surfer = MultimodalWebSurfer(\"web_surfer\", model_client, headless=False, animate_actions=True)\n    # The user proxy agent is used to get user input after each step of the web surfer.\n    # NOTE: you can skip input by pressing Enter.\n    user_proxy = UserProxyAgent(\"user_proxy\")\n    # The termination condition is set to end the conversation when the user types 'exit'.\n    termination = TextMentionTermination(\"exit\", sources=[\"user_proxy\"])\n    # Web surfer and user proxy take turns in a round-robin fashion.\n    team = RoundRobinGroupChat([web_surfer, user_proxy], termination_condition=termination)\n    try:\n        # Start the team and wait for it to terminate.\n        await Console(team.run_stream(task=\"Find information about AutoGen and write a short summary.\"))\n    finally:\n        await web_surfer.close()\n        await model_client.close()\n\nasyncio.run(main())\n```\n\n### AutoGen Studio\n\nUse AutoGen Studio to prototype and run multi-agent workflows without writing code.\n\n```bash\n# Run AutoGen Studio on http://localhost:8080\nautogenstudio ui --port 8080 --appdir ./my-app\n```\n\n## Why Use AutoGen?\n\n<div align=\"center\">\n  <img src=\"autogen-landing.jpg\" alt=\"AutoGen Landing\" width=\"500\">\n</div>\n\nThe AutoGen ecosystem provides everything you need to create AI agents, especially multi-agent workflows -- framework, developer tools, and applications.\n\nThe _framework_ uses a layered and extensible design. Layers have clearly divided responsibilities and build on top of layers below. This design enables you to use the framework at different levels of abstraction, from high-level APIs to low-level components.\n\n- [Core API](./python/packages/autogen-core/) implements message passing, event-driven agents, and local and distributed runtime for flexibility and power. It also support cross-language support for .NET and Python.\n- [AgentChat API](./python/packages/autogen-agentchat/) implements a simpler but opinionated API for rapid prototyping. This API is built on top of the Core API and is closest to what users of v0.2 are familiar with and supports common multi-agent patterns such as two-agent chat or group chats.\n- [Extensions API](./python/packages/autogen-ext/) enables first- and third-party extensions continuously expanding framework capabilities. It support specific implementation of LLM clients (e.g., OpenAI, AzureOpenAI), and capabilities such as code execution.\n\nThe ecosystem also supports two essential _developer tools_:\n\n<div align=\"center\">\n  <img src=\"https://media.githubusercontent.com/media/microsoft/autogen/refs/heads/main/python/packages/autogen-studio/docs/ags_screen.png\" alt=\"AutoGen Studio Screenshot\" width=\"500\">\n</div>\n\n- [AutoGen Studio](./python/packages/autogen-studio/) provides a no-code GUI for building multi-agent applications.\n- [AutoGen Bench](./python/packages/agbench/) provides a benchmarking suite for evaluating agent performance.\n\nYou can use the AutoGen framework and developer tools to create applications for your domain. For example, [Magentic-One](./python/packages/magentic-one-cli/) is a state-of-the-art multi-agent team built using AgentChat API and Extensions API that can handle a variety of tasks that require web browsing, code execution, and file handling.\n\nWith AutoGen you get to join and contribute to a thriving ecosystem. We host weekly office hours and talks with maintainers and community. We also have a [Discord server](https://aka.ms/autogen-discord) for real-time chat, GitHub Discussions for Q&A, and a blog for tutorials and updates.\n\n## Where to go next?\n\n<div align=\"center\">\n\n|               | [![Python](https://img.shields.io/badge/AutoGen-Python-blue?logo=python&logoColor=white)](./python)                                                                                                                                                                                                                                                                                                                | [![.NET](https://img.shields.io/badge/AutoGen-.NET-green?logo=.net&logoColor=white)](./dotnet) | [![Studio](https://img.shields.io/badge/AutoGen-Studio-purple?logo=visual-studio&logoColor=white)](./python/packages/autogen-studio)                     |\n| ------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | ---------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| Installation  | [![Installation](https://img.shields.io/badge/Install-blue)](https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/installation.html)                                                                                                                                                                                                                                                            | [![Install](https://img.shields.io/badge/Install-green)](https://microsoft.github.io/autogen/dotnet/dev/core/installation.html) | [![Install](https://img.shields.io/badge/Install-purple)](https://microsoft.github.io/autogen/stable/user-guide/autogenstudio-user-guide/installation.html) |\n| Quickstart    | [![Quickstart](https://img.shields.io/badge/Quickstart-blue)](https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/quickstart.html#)                                                                                                                                                                                                                                                            | [![Quickstart](https://img.shields.io/badge/Quickstart-green)](https://microsoft.github.io/autogen/dotnet/dev/core/index.html) | [![Usage](https://img.shields.io/badge/Quickstart-purple)](https://microsoft.github.io/autogen/stable/user-guide/autogenstudio-user-guide/usage.html#)        |\n| Tutorial      | [![Tutorial](https://img.shields.io/badge/Tutorial-blue)](https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/tutorial/index.html)                                                                                                                                                                                                                                                            | [![Tutorial](https://img.shields.io/badge/Tutorial-green)](https://microsoft.github.io/autogen/dotnet/dev/core/tutorial.html) | [![Usage](https://img.shields.io/badge/Tutorial-purple)](https://microsoft.github.io/autogen/stable/user-guide/autogenstudio-user-guide/usage.html#)        |\n| API Reference | [![API](https://img.shields.io/badge/Docs-blue)](https://microsoft.github.io/autogen/stable/reference/index.html#)                                                                                                                                                                                                                                                                                                    | [![API](https://img.shields.io/badge/Docs-green)](https://microsoft.github.io/autogen/dotnet/dev/api/Microsoft.AutoGen.Contracts.html) | [![API](https://img.shields.io/badge/Docs-purple)](https://microsoft.github.io/autogen/stable/user-guide/autogenstudio-user-guide/usage.html)               |\n| Packages      | [![PyPi autogen-core](https://img.shields.io/badge/PyPi-autogen--core-blue?logo=pypi)](https://pypi.org/project/autogen-core/) <br> [![PyPi autogen-agentchat](https://img.shields.io/badge/PyPi-autogen--agentchat-blue?logo=pypi)](https://pypi.org/project/autogen-agentchat/) <br> [![PyPi autogen-ext](https://img.shields.io/badge/PyPi-autogen--ext-blue?logo=pypi)](https://pypi.org/project/autogen-ext/) | [![NuGet Contracts](https://img.shields.io/badge/NuGet-Contracts-green?logo=nuget)](https://www.nuget.org/packages/Microsoft.AutoGen.Contracts/) <br> [![NuGet Core](https://img.shields.io/badge/NuGet-Core-green?logo=nuget)](https://www.nuget.org/packages/Microsoft.AutoGen.Core/) <br> [![NuGet Core.Grpc](https://img.shields.io/badge/NuGet-Core.Grpc-green?logo=nuget)](https://www.nuget.org/packages/Microsoft.AutoGen.Core.Grpc/) <br> [![NuGet RuntimeGateway.Grpc](https://img.shields.io/badge/NuGet-RuntimeGateway.Grpc-green?logo=nuget)](https://www.nuget.org/packages/Microsoft.AutoGen.RuntimeGateway.Grpc/) | [![PyPi autogenstudio](https://img.shields.io/badge/PyPi-autogenstudio-purple?logo=pypi)](https://pypi.org/project/autogenstudio/)                       |\n\n</div>\n\n\nInterested in contributing? See [CONTRIBUTING.md](./CONTRIBUTING.md) for guidelines on how to get started. We welcome contributions of all kinds, including bug fixes, new features, and documentation improvements. Join our community and help us make AutoGen better!\n\nHave questions? Check out our [Frequently Asked Questions (FAQ)](./FAQ.md) for answers to common queries. If you don't find what you're looking for, feel free to ask in our [GitHub Discussions](https://github.com/microsoft/autogen/discussions) or join our [Discord server](https://aka.ms/autogen-discord) for real-time support. You can also read our [blog](https://devblogs.microsoft.com/autogen/) for updates.\n\n## Legal Notices\n\nMicrosoft and any contributors grant you a license to the Microsoft documentation and other content\nin this repository under the [Creative Commons Attribution 4.0 International Public License](https://creativecommons.org/licenses/by/4.0/legalcode),\nsee the [LICENSE](LICENSE) file, and grant you a license to any code in the repository under the [MIT License](https://opensource.org/licenses/MIT), see the\n[LICENSE-CODE](LICENSE-CODE) file.\n\nMicrosoft, Windows, Microsoft Azure, and/or other Microsoft products and services referenced in the documentation\nmay be either trademarks or registered trademarks of Microsoft in the United States and/or other countries.\nThe licenses for this project do not grant you rights to use any Microsoft names, logos, or trademarks.\nMicrosoft's general trademark guidelines can be found at <http://go.microsoft.com/fwlink/?LinkID=254653>.\n\nPrivacy information can be found at <https://go.microsoft.com/fwlink/?LinkId=521839>\n\nMicrosoft and any contributors reserve all other rights, whether under their respective copyrights, patents,\nor trademarks, whether by implication, estoppel, or otherwise.\n\n<p align=\"right\" style=\"font-size: 14px; color: #555; margin-top: 20px;\">\n  <a href=\"#readme-top\" style=\"text-decoration: none; color: blue; font-weight: bold;\">\n    ↑ Back to Top ↑\n  </a>\n</p>\n"
    },
    {
      "name": "microsoft/ai-agents-for-beginners",
      "stars": 16610,
      "img": "https://avatars.githubusercontent.com/u/6154722?s=40&v=4",
      "owner": "microsoft",
      "repo_name": "ai-agents-for-beginners",
      "description": "10 Lessons to Get Started Building AI Agents",
      "homepage": "https://microsoft.github.io/ai-agents-for-beginners/",
      "language": "Jupyter Notebook",
      "created_at": "2024-11-28T10:42:52Z",
      "updated_at": "2025-04-23T09:28:05Z",
      "topics": [
        "agentic-ai",
        "agentic-framework",
        "agentic-rag",
        "ai-agents",
        "ai-agents-framework",
        "autogen",
        "generative-ai",
        "semantic-kernel"
      ],
      "readme": "# AI Agents for Beginners - A Course\n\n![Generative AI For Beginners](./images/repo-thumbnail.png)\n\n## 10 Lessons teaching everything you need to know to start building AI Agents\n\n[![GitHub license](https://img.shields.io/github/license/microsoft/ai-agents-for-beginners.svg)](https://github.com/microsoft/ai-agents-for-beginners/blob/master/LICENSE?WT.mc_id=academic-105485-koreyst)\n[![GitHub contributors](https://img.shields.io/github/contributors/microsoft/ai-agents-for-beginners.svg)](https://GitHub.com/microsoft/ai-agents-for-beginners/graphs/contributors/?WT.mc_id=academic-105485-koreyst)\n[![GitHub issues](https://img.shields.io/github/issues/microsoft/ai-agents-for-beginners.svg)](https://GitHub.com/microsoft/ai-agents-for-beginners/issues/?WT.mc_id=academic-105485-koreyst)\n[![GitHub pull-requests](https://img.shields.io/github/issues-pr/microsoft/ai-agents-for-beginners.svg)](https://GitHub.com/microsoft/ai-agents-for-beginners/pulls/?WT.mc_id=academic-105485-koreyst)\n[![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat-square)](http://makeapullrequest.com?WT.mc_id=academic-105485-koreyst)\n\n### Language Support\n[![English](https://img.shields.io/badge/English-brightgreen.svg?style=flat-square)](README.md)\n[![Chinese Simplified](https://img.shields.io/badge/Chinese_Simplified-brightgreen.svg?style=flat-square)](./translations/zh/README.md)\n[![Chinese Traditional](https://img.shields.io/badge/Chinese_Traditional-brightgreen.svg?style=flat-square)](./translations/tw/README.md)     \n[![Chinese Hong Kong](https://img.shields.io/badge/Chinese_Hong_Kong-brightgreen.svg?style=flat-square)](./translations/hk/README.md) \n[![French](https://img.shields.io/badge/French-brightgreen.svg?style=flat-square)](./translations/fr/README.md)\n[![Japanese](https://img.shields.io/badge/Japanese-brightgreen.svg?style=flat-square)](./translations/ja/README.md) \n[![Korean](https://img.shields.io/badge/Korean-brightgreen.svg?style=flat-square)](./translations/ko/README.md)\n[![Portuguese Brazilian](https://img.shields.io/badge/Portuguese_Brazilian-brightgreen.svg?style=flat-square)](./translations/pt/README.md)\n[![Spanish](https://img.shields.io/badge/Spanish-brightgreen.svg?style=flat-square)](./translations/es/README.md)\n[![German](https://img.shields.io/badge/German-brightgreen.svg?style=flat-square)](./translations/de/README.md)  \n[![Persian](https://img.shields.io/badge/Persian-brightgreen.svg?style=flat-square)](./translations/fa/README.md) \n[![Polish](https://img.shields.io/badge/Polish-brightgreen.svg?style=flat-square)](./translations/pl/README.md) \n[![Hindi](https://img.shields.io/badge/Hindi-brightgreen.svg?style=flat-square)](./translations/hi/README.md)\n\n[![GitHub watchers](https://img.shields.io/github/watchers/microsoft/ai-agents-for-beginners.svg?style=social&label=Watch)](https://GitHub.com/microsoft/ai-agents-for-beginners/watchers/?WT.mc_id=academic-105485-koreyst)\n[![GitHub forks](https://img.shields.io/github/forks/microsoft/ai-agents-for-beginners.svg?style=social&label=Fork)](https://GitHub.com/microsoft/ai-agents-for-beginners/network/?WT.mc_id=academic-105485-koreyst)\n[![GitHub stars](https://img.shields.io/github/stars/microsoft/ai-agents-for-beginners.svg?style=social&label=Star)](https://GitHub.com/microsoft/ai-agents-for-beginners/stargazers/?WT.mc_id=academic-105485-koreyst)\n\n[![Azure AI Discord](https://dcbadge.limes.pink/api/server/kzRShWzttr)](https://discord.gg/kzRShWzttr)\n\n\n## 🌱 Getting Started\n\nThis course has 10 lessons covering the fundamentals of building AI Agents. Each lesson covers its own topic so start wherever you like!\n\nThere is multi-language support for this course. Go to our [available languages here](#-multi-language-support). \n\nIf this is your first time building with Generative AI models, check out our [Generative AI For Beginners](https://aka.ms/genai-beginners) course, which includes 21 lessons on building with GenAI.\n\nDon't forget to [star (🌟) this repo](https://docs.github.com/en/get-started/exploring-projects-on-github/saving-repositories-with-stars?WT.mc_id=academic-105485-koreyst) and [fork this repo](https://github.com/microsoft/ai-agents-for-beginners/fork) to run the code.\n\n### What You Need \n\nEach lesson in this course includes code examples, which can be found in the code_samples folder. You can [fork this repo](https://github.com/microsoft/ai-agents-for-beginners/fork) to create your own copy.  \n\nThe code example in these exercises, utilize Azure AI Foundry and GitHub Model Catalogs for interacting with Language Models:\n\n- [Github Models](https://aka.ms/ai-agents-beginners/github-models) - Free / Limited\n- [Azure AI Foundry](https://aka.ms/ai-agents-beginners/ai-foundry) - Azure Account Required\n\nThis course also uses the following AI Agent frameworks and services from Microsoft:\n\n- [Azure AI Agent Service](https://aka.ms/ai-agents-beginners/ai-agent-service)\n- [Semantic Kernel](https://aka.ms/ai-agents-beginners/semantic-kernel)\n- [AutoGen](https://aka.ms/ai-agents/autogen)\n\nFor more information on running the code for this course, go to the [Course Setup](./00-course-setup/README.md).\n\n## 🙏 Want to help?\n\nDo you have suggestions or found spelling or code errors? [Raise an issue](https://github.com/microsoft/ai-agents-for-beginners/issues?WT.mc_id=academic-105485-koreyst) or [Create a pull request](https://github.com/microsoft/ai-agents-for-beginners/pulls?WT.mc_id=academic-105485-koreyst)\n\nIf you get stuck or have any questions about building AI Agents, join our [Azure AI Community Discord](https://discord.gg/kzRShWzttr).\n\n## 📂 Each lesson includes\n\n- A written lesson located in the README and a short video\n- Python code samples supporting Azure AI Foundry and Github Models (Free)\n- Links to extra resources to continue your learning\n\n\n## 🗃️ Lessons\n\n| **Lesson**                               | **Text & Code**                                    | **Video**                                                  | **Extra Learning**                                                                     |\n|------------------------------------------|----------------------------------------------------|------------------------------------------------------------|----------------------------------------------------------------------------------------|\n| Intro to AI Agents and Agent Use Cases   | [Link](./01-intro-to-ai-agents/README.md)          | [Video](https://youtu.be/3zgm60bXmQk?si=z8QygFvYQv-9WtO1)  | [Link](https://aka.ms/ai-agents-beginners/collection?WT.mc_id=academic-105485-koreyst) |\n| Exploring AI Agentic Frameworks          | [Link](./02-explore-agentic-frameworks/README.md)  | [Video](https://youtu.be/ODwF-EZo_O8?si=Vawth4hzVaHv-u0H)  | [Link](https://aka.ms/ai-agents-beginners/collection?WT.mc_id=academic-105485-koreyst) |\n| Understanding AI Agentic Design Patterns | [Link](./03-agentic-design-patterns/README.md)     | [Video](https://youtu.be/m9lM8qqoOEA?si=BIzHwzstTPL8o9GF)  | [Link](https://aka.ms/ai-agents-beginners/collection?WT.mc_id=academic-105485-koreyst) |\n| Tool Use Design Pattern                  | [Link](./04-tool-use/README.md)                    | [Video](https://youtu.be/vieRiPRx-gI?si=2z6O2Xu2cu_Jz46N)  | [Link](https://aka.ms/ai-agents-beginners/collection?WT.mc_id=academic-105485-koreyst) |\n| Agentic RAG                              | [Link](./05-agentic-rag/README.md)                 | [Video](https://youtu.be/WcjAARvdL7I?si=gKPWsQpKiIlDH9A3)  | [Link](https://aka.ms/ai-agents-beginners/collection?WT.mc_id=academic-105485-koreyst) |\n| Building Trustworthy AI Agents           | [Link](./06-building-trustworthy-agents/README.md) | [Video](https://youtu.be/iZKkMEGBCUQ?si=jZjpiMnGFOE9L8OK ) | [Link](https://aka.ms/ai-agents-beginners/collection?WT.mc_id=academic-105485-koreyst) |\n| Planning Design Pattern                  | [Link](./07-planning-design/README.md)             | [Video](https://youtu.be/kPfJ2BrBCMY?si=6SC_iv_E5-mzucnC)  | [Link](https://aka.ms/ai-agents-beginners/collection?WT.mc_id=academic-105485-koreyst) |\n| Multi-Agent Design Pattern               | [Link](./08-multi-agent/README.md)                 | [Video](https://youtu.be/V6HpE9hZEx0?si=rMgDhEu7wXo2uo6g)  | [Link](https://aka.ms/ai-agents-beginners/collection?WT.mc_id=academic-105485-koreyst) |\n| Metacognition Design Pattern             | [Link](./09-metacognition/README.md)               | [Video](https://youtu.be/His9R6gw6Ec?si=8gck6vvdSNCt6OcF)  | [Link](https://aka.ms/ai-agents-beginners/collection?WT.mc_id=academic-105485-koreyst) |\n| AI Agents in Production                  | [Link](./10-ai-agents-production/README.md)        | [Video](https://youtu.be/l4TP6IyJxmQ?si=31dnhexRo6yLRJDl)  | [Link](https://aka.ms/ai-agents-beginners/collection?WT.mc_id=academic-105485-koreyst) |\n\n## 🌐 Multi-Language Support\n\n| Language             | Code | Link to Translated README                               | Last Updated |\n|----------------------|------|---------------------------------------------------------|--------------|\n| Chinese (Simplified) | zh   | [Chinese Translation](./translations/zh/README.md)      | 2025-03-24   |\n| Chinese (Traditional)| tw   | [Chinese Translation](./translations/tw/README.md)      | 2025-03-28   |\n| Chinese (Hong Kong)  | hk   | [Chinese (Hong Kong) Translation](./translations/hk/README.md) | 2025-03-28   |\n| French               | fr   | [French Translation](./translations/fr/README.md)       | 2025-03-28   |\n| Japanese             | ja   | [Japanese Translation](./translations/ja/README.md)     | 2025-03-28   |\n| Korean               | ko   | [Korean Translation](./translations/ko/README.md)       | 2025-03-28   |\n| Portuguese           | pt   | [Portuguese Translation](./translations/pt/README.md)   | 2025-03-28   |\n| Spanish              | es   | [Spanish Translation](./translations/es/README.md)      | 2025-03-28   |\n| German               | de   | [German Translation](./translations/de/README.md)       | 2025-03-28   |\n| Persian              | fa   | [Persian Translation](./translations/fa/README.md)       | 2025-03-28   |\n| Polish               | pl   | [Polish Translation](./translations/pl/README.md)       | 2025-03-28  |\n| Hindi               | hi   | [Hindi Translation](./translations/hi/README.md)       | 2025-04-05   |\n\n\n## 🎒 Other Courses\n\nOur team produces other courses! Check out:\n\n- [**NEW** Generative AI for Beginners using .NET](https://github.com/microsoft/Generative-AI-for-beginners-dotnet?WT.mc_id=academic-105485-koreyst)\n- [Generative AI for Beginners](https://github.com/microsoft/generative-ai-for-beginners?WT.mc_id=academic-105485-koreyst)\n- [ML for Beginners](https://aka.ms/ml-beginners?WT.mc_id=academic-105485-koreyst)\n- [Data Science for Beginners](https://aka.ms/datascience-beginners?WT.mc_id=academic-105485-koreyst)\n- [AI for Beginners](https://aka.ms/ai-beginners?WT.mc_id=academic-105485-koreyst)\n- [Cybersecurity for Beginners](https://github.com/microsoft/Security-101??WT.mc_id=academic-96948-sayoung)\n- [Web Dev for Beginners](https://aka.ms/webdev-beginners?WT.mc_id=academic-105485-koreyst)\n- [IoT for Beginners](https://aka.ms/iot-beginners?WT.mc_id=academic-105485-koreyst)\n- [XR Development for Beginners](https://github.com/microsoft/xr-development-for-beginners?WT.mc_id=academic-105485-koreyst)\n- [Mastering GitHub Copilot for AI Paired Programming](https://aka.ms/GitHubCopilotAI?WT.mc_id=academic-105485-koreyst)\n- [Mastering GitHub Copilot for C#/.NET Developers](https://github.com/microsoft/mastering-github-copilot-for-dotnet-csharp-developers?WT.mc_id=academic-105485-koreyst)\n- [Choose Your Own Copilot Adventure](https://github.com/microsoft/CopilotAdventures?WT.mc_id=academic-105485-koreyst)\n\n## 🌟 Community Thanks\n\nThanks to [Shivam Goyal](https://www.linkedin.com/in/shivam2003/) for contributing important code samples demonstrating Agentic RAG. \n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit <https://cla.opensource.microsoft.com>.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft\ntrademarks or logos is subject to and must follow\n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos is subject to those third-parties' policies.\n"
    },
    {
      "name": "dotnet/machinelearning",
      "stars": 9161,
      "img": "https://avatars.githubusercontent.com/u/9141961?s=40&v=4",
      "owner": "dotnet",
      "repo_name": "machinelearning",
      "description": "ML.NET is an open source and cross-platform machine learning framework for .NET.",
      "homepage": "https://dot.net/ml",
      "language": "C#",
      "created_at": "2018-05-03T16:20:42Z",
      "updated_at": "2025-04-23T02:32:15Z",
      "topics": [
        "algorithms",
        "dotnet",
        "machine-learning",
        "ml"
      ],
      "readme": "# Machine Learning for .NET\n\n[ML.NET](https://dotnet.microsoft.com/apps/machinelearning-ai/ml-dotnet) is a cross-platform open-source machine learning (ML) framework for .NET.\n\nML.NET allows developers to easily build, train, deploy, and consume custom models in their .NET applications without requiring prior expertise in developing machine learning models or experience with other programming languages like Python or R. The framework provides data loading from files and databases, enables data transformations, and includes many ML algorithms.\n\nWith ML.NET, you can train models for a [variety of scenarios](https://docs.microsoft.com/dotnet/machine-learning/resources/tasks), like classification, forecasting, and anomaly detection.\n\nYou can also consume both TensorFlow and ONNX models within ML.NET which makes the framework more extensible and expands the number of supported scenarios.\n\n## Getting started with machine learning and ML.NET\n\n- Learn more about the [basics of ML.NET](https://dotnet.microsoft.com/apps/machinelearning-ai/ml-dotnet).\n- Build your first ML.NET model by following our [ML.NET Getting Started tutorial](https://dotnet.microsoft.com/learn/ml-dotnet/get-started-tutorial/intro).\n- Check out our [documentation and tutorials](https://docs.microsoft.com/dotnet/machine-learning/).\n- See the [API Reference documentation](https://docs.microsoft.com/dotnet/api/?view=ml-dotnet).\n- Clone our [ML.NET Samples GitHub repo](https://github.com/dotnet/machinelearning-samples) and run some sample apps.\n- Take a look at some [ML.NET Community Samples](https://github.com/dotnet/machinelearning-samples/blob/main/docs/COMMUNITY-SAMPLES.md).\n- Watch some videos on the [ML.NET videos YouTube playlist](https://aka.ms/mlnetyoutube).\n\n## Roadmap\n\nTake a look at ML.NET's [Roadmap](ROADMAP.md) to see what the team plans to work on in the next year.\n\n## Operating systems and processor architectures supported by ML.NET\n\nML.NET runs on Windows, Linux, and macOS using .NET Core, or Windows using .NET Framework.\n\nML.NET also runs on ARM64, Apple M1, and Blazor Web Assembly. However, there are some [limitations](docs/project-docs/platform-limitations.md).\n\n64-bit is supported on all platforms. 32-bit is supported on Windows, except for TensorFlow and LightGBM related functionality.\n\n## ML.NET NuGet packages status\n\n[![NuGet Status](https://img.shields.io/nuget/v/Microsoft.ML.svg?style=flat)](https://www.nuget.org/packages/Microsoft.ML/)\n\n[![Latest NuGet Status](https://img.shields.io/nuget/vpre/Microsoft.ML.svg?style=flat)](https://www.nuget.org/packages/Microsoft.ML/absoluteLatest)\n\n## Release notes\n\nCheck out the [release notes](docs/release-notes) to see what's new. You can also read the [blog posts](https://devblogs.microsoft.com/dotnet/category/ml-net/) for more details about each release.\n\n## Using ML.NET packages\n\nFirst, ensure you have installed [.NET Core 2.1](https://www.microsoft.com/net/learn/get-started) or later. ML.NET also works on the .NET Framework 4.6.1 or later, but 4.7.2 or later is recommended.\n\nOnce you have an app, you can install the ML.NET NuGet package from the .NET Core CLI using:\n```\ndotnet add package Microsoft.ML\n```\n\nor from the NuGet Package Manager:\n```\nInstall-Package Microsoft.ML\n```\n\nAlternatively, you can add the Microsoft.ML package from within Visual Studio's NuGet package manager or via [Paket](https://github.com/fsprojects/Paket).\n\nDaily NuGet builds of the project are also available in our Azure DevOps feed:\n\n> [https://pkgs.dev.azure.com/dnceng/public/_packaging/dotnet-libraries/nuget/v3/index.json](https://pkgs.dev.azure.com/dnceng/public/_packaging/dotnet-libraries/nuget/v3/index.json)\n\n## Building ML.NET (For contributors building ML.NET open source code)\n\nTo build ML.NET from source please visit our [developer guide](docs/project-docs/developer-guide.md).\n\n[![codecov](https://codecov.io/gh/dotnet/machinelearning/branch/main/graph/badge.svg?flag=production)](https://codecov.io/gh/dotnet/machinelearning)\n\n|    | Debug | Release |\n|:---|----------------:|------------------:|\n|**CentOS**|[![Build Status](https://dev.azure.com/dnceng/public/_apis/build/status/dotnet/machinelearning/MachineLearning-CI?branchName=main&jobName=Centos_x64_Net60&configuration=Centos_x64_Net60%20Debug_Build)](https://dev.azure.com/dnceng/public/_build/latest?definitionId=104&branchName=main)|[![Build Status](https://dev.azure.com/dnceng/public/_apis/build/status/dotnet/machinelearning/MachineLearning-CI?branchName=main&jobName=Centos_x64_Net60&configuration=Centos_x64_Net60%20Release_Build)](https://dev.azure.com/dnceng/public/_build/latest?definitionId=104&branchName=main)|\n|**Ubuntu**|[![Build Status](https://dev.azure.com/dnceng/public/_apis/build/status/dotnet/machinelearning/MachineLearning-CI?branchName=main&jobName=Ubuntu_x64_Net60&configuration=Ubuntu_x64_Net60%20Debug_Build)](https://dev.azure.com/dnceng/public/_build/latest?definitionId=104&branchName=main)|[![Build Status](https://dev.azure.com/dnceng/public/_apis/build/status/dotnet/machinelearning/MachineLearning-CI?branchName=main&jobName=Ubuntu_x64_Net60&configuration=Ubuntu_x64_Net60%20Release_Build)](https://dev.azure.com/dnceng/public/_build/latest?definitionId=104&branchName=main)|\n|**macOS**|[![Build Status](https://dev.azure.com/dnceng/public/_apis/build/status/dotnet/machinelearning/MachineLearning-CI?branchName=main&jobName=MacOS_x64_Net60&configuration=MacOS_x64_Net60%20Debug_Build)](https://dev.azure.com/dnceng/public/_build/latest?definitionId=104&branchName=main)|[![Build Status](https://dev.azure.com/dnceng/public/_apis/build/status/dotnet/machinelearning/MachineLearning-CI?branchName=main&jobName=MacOS_x64_Net60&configuration=MacOS_x64_Net60%20Release_Build)](https://dev.azure.com/dnceng/public/_build/latest?definitionId=104&branchName=main)|\n|**Windows x64**|[![Build Status](https://dev.azure.com/dnceng/public/_apis/build/status/dotnet/machinelearning/MachineLearning-CI?branchName=main&jobName=Windows_x64_Net60&configuration=Windows_x64_Net60%20Debug_Build)](https://dev.azure.com/dnceng/public/_build/latest?definitionId=104&branchName=main)|[![Build Status](https://dev.azure.com/dnceng/public/_apis/build/status/dotnet/machinelearning/MachineLearning-CI?branchName=main&jobName=Windows_x64_Net60&configuration=Windows_x64_Net60%20Release_Build)](https://dev.azure.com/dnceng/public/_build/latest?definitionId=104&branchName=main)|\n|**Windows FullFramework**|[![Build Status](https://dev.azure.com/dnceng/public/_apis/build/status/dotnet/machinelearning/MachineLearning-CI?branchName=main&jobName=Windows_x64_NetFx461&configuration=Windows_x64_NetFx461%20Debug_Build)](https://dev.azure.com/dnceng/public/_build/latest?definitionId=104&branchName=main)|[![Build Status](https://dev.azure.com/dnceng/public/_apis/build/status/dotnet/machinelearning/MachineLearning-CI?branchName=main&jobName=Windows_x64_NetFx461&configuration=Windows_x64_NetFx461%20Release_Build)](https://dev.azure.com/dnceng/public/_build/latest?definitionId=104&branchName=main)|\n|**Windows x86**|[![Build Status](https://dev.azure.com/dnceng/public/_apis/build/status/dotnet/machinelearning/MachineLearning-CI?branchName=main&jobName=Windows_x86_Net60&configuration=Windows_x86_Net60%20Debug_Build)](https://dev.azure.com/dnceng/public/_build/latest?definitionId=104&branchName=main)|[![Build Status](https://dev.azure.com/dnceng/public/_apis/build/status/dotnet/machinelearning/MachineLearning-CI?branchName=main&jobName=Windows_x86_Net60&configuration=Windows_x86_Net60%20Release_Build)](https://dev.azure.com/dnceng/public/_build/latest?definitionId=104&branchName=main)|\n|**Windows NetCore3.1**|[![Build Status](https://dev.azure.com/dnceng/public/_apis/build/status/dotnet/machinelearning/MachineLearning-CI?branchName=main&jobName=Windows_x64_Net60&configuration=Windows_x64_Net60%20Debug_Build)](https://dev.azure.com/dnceng/public/_build/latest?definitionId=104&branchName=main)|[![Build Status](https://dev.azure.com/dnceng/public/_apis/build/status/dotnet/machinelearning/MachineLearning-CI?branchName=main&jobName=Windows_x64_Net60&configuration=Windows_x64_Net60%20Release_Build)](https://dev.azure.com/dnceng/public/_build/latest?definitionId=104&branchName=main)|\n\n## Release process and versioning\n\nMajor releases of ML.NET are shipped once a year with the major .NET releases, starting with ML.NET 1.7 in November 2021 with .NET 6, then ML.NET 2.0 with .NET 7, etc. We will maintain release branches to optionally service ML.NET with bug fixes and/or minor features on the same cadence as .NET servicing.\n\nCheck out the [Release Notes](docs/release-notes) to see all of the past ML.NET releases.\n\n## Contributing\n\nWe welcome contributions! Please review our [contribution guide](CONTRIBUTING.md).\n\n## Community\n\n- Join our community on [Discord](https://aka.ms/dotnet-discord).\n- Tune into the [.NET Machine Learning Community Standup](https://dotnet.microsoft.com/live/community-standup) every other Wednesday at 10AM Pacific Time.\n\nThis project has adopted the code of conduct defined by the [Contributor Covenant](https://contributor-covenant.org/) to clarify expected behavior in our community.\nFor more information, see the [.NET Foundation Code of Conduct](https://dotnetfoundation.org/code-of-conduct).\n\n## Code examples\n\nHere is a code snippet for training a model to predict sentiment from text samples. You can find complete samples in the [samples repo](https://github.com/dotnet/machinelearning-samples).\n\n```C#\nvar dataPath = \"sentiment.csv\";\nvar mlContext = new MLContext();\nvar loader = mlContext.Data.CreateTextLoader(new[]\n    {\n        new TextLoader.Column(\"SentimentText\", DataKind.String, 1),\n        new TextLoader.Column(\"Label\", DataKind.Boolean, 0),\n    },\n    hasHeader: true,\n    separatorChar: ',');\nvar data = loader.Load(dataPath);\nvar learningPipeline = mlContext.Transforms.Text.FeaturizeText(\"Features\", \"SentimentText\")\n        .Append(mlContext.BinaryClassification.Trainers.FastTree());\nvar model = learningPipeline.Fit(data);\n```\n\nNow from the model we can make inferences (predictions):\n\n```C#\nvar predictionEngine = mlContext.Model.CreatePredictionEngine<SentimentData, SentimentPrediction>(model);\nvar prediction = predictionEngine.Predict(new SentimentData\n{\n    SentimentText = \"Today is a great day!\"\n});\nConsole.WriteLine(\"prediction: \" + prediction.Prediction);\n```\n\n## License\n\nML.NET is licensed under the [MIT license](LICENSE), and it is free to use commercially.\n\n## .NET Foundation\n\nML.NET is a part of the [.NET Foundation](https://www.dotnetfoundation.org/projects).\n"
    },
    {
      "name": "unitycatalog/unitycatalog",
      "stars": 2806,
      "img": "https://avatars.githubusercontent.com/u/171874451?s=40&v=4",
      "owner": "unitycatalog",
      "repo_name": "unitycatalog",
      "description": "Open, Multi-modal Catalog for Data & AI",
      "homepage": "https://unitycatalog.io/",
      "language": "Python",
      "created_at": "2024-06-13T14:39:25Z",
      "updated_at": "2025-04-23T08:07:10Z",
      "topics": [],
      "readme": "<img src=\"./docs/assets/images/uc-logo.png\" width=\"600px\" />\n\n# Unity Catalog: Open, Multimodal Catalog for Data & AI\n\nUnity Catalog is the industry’s only universal catalog for data and AI.\n\n- **Multimodal interface supports any format, engine, and asset**\n  - Multi-format support: It is extensible and supports Delta Lake, Apache Iceberg and Apache Hudi via UniForm, Apache Parquet, JSON, CSV, and many others.\n  - Multi-engine support: With its open APIs, data cataloged in Unity can be read by many leading compute engines.\n  - Multimodal: It supports all your data and AI assets, including tables, files, functions, AI models.\n- **Open source API and implementation** - OpenAPI spec and OSS implementation (Apache 2.0 license). It is also compatible with Apache Hive's metastore API and Apache Iceberg's REST catalog API. Unity Catalog is currently a sandbox project with LF AI and Data Foundation (part of the Linux Foundation).\n- **Unified governance** for data and AI - Govern and secure tabular data, unstructured assets, and AI assets with a single interface.\n\nThe first release of Unity Catalog focuses on a core set of APIs for tables, unstructured data, and AI assets - with more to come soon on governance, access, and client interoperability. This is just the beginning!\n\n![UC Hero Image](./docs/assets/images/uc.png)\n\n### Vibrant ecosystem\n\nThis is a community effort. Unity Catalog is supported by\n\n- [Amazon Web Services](https://aws.amazon.com/)\n- [Confluent](https://www.confluent.io/)\n- [Daft (Eventual)](https://github.com/Eventual-Inc/Daft)\n- [dbt Labs](https://www.getdbt.com/)\n- [DuckDB](https://duckdblabs.com/)\n- [Fivetran](https://www.fivetran.com/)\n- [Google Cloud](https://cloud.google.com/)\n- [Granica](https://granica.ai/)\n- [Immuta](https://www.immuta.com/)\n- [Informatica](https://www.informatica.com/)\n- [LanceDB](https://lancedb.com/)\n- [LangChain](https://www.langchain.com/)\n- [LlamaIndex](https://www.llamaindex.ai/)\n- [Microsoft Azure](https://azure.microsoft.com)\n- [NVIDIA](https://www.nvidia.com/)\n- [Onehouse](https://www.onehouse.ai/)\n- [PuppyGraph](https://www.puppygraph.com/)\n- [Salesforce](https://www.salesforce.com/)\n- [StarRocks (CelerData)](https://celerdata.com/)\n- [Spice AI](https://github.com/spiceai/spiceai)\n- [Tecton](https://www.tecton.ai/)\n- [Unstructured](https://unstructured.io/)\n\nUnity Catalog is proud to be hosted by the LF AI & Data Foundation.\n\n<a href=\"https://lfaidata.foundation/projects\">\n  <img src=\"./docs/assets/images/lfaidata-project-badge-sandbox-color.png\" width=\"200px\" />\n</a>\n\n## Quickstart - Hello UC!\n\nLet's take Unity Catalog for spin. In this guide, we are going to do the following:\n\n- In one terminal, run the UC server.\n- In another terminal, we will explore the contents of the UC server using a CLI.\n  An example project is provided to demonstrate how to use the UC SDK for various assets\n  as well as provide a convenient way to explore the content of any UC server implementation.\n\n> If you prefer to run Unity Catalog in Docker use `docker\n> compose up`. See the [Docker Compose docs](./docs/docker_compose.md) for more details.\n\n### Prerequisites\n\nYou have to ensure that your local environment has the following:\n\n- Clone this repository.\n- Ensure the `JAVA_HOME` environment variable your terminal is configured to point to JDK17.\n- Compile the project using `build/sbt package`\n\n\n### Run the UC Server\n\nIn a terminal, in the cloned repository root directory, start the UC server.\n\n```sh\nbin/start-uc-server\n```\n\nFor the remaining steps, continue in a different terminal.\n\n### Operate on Delta tables with the CLI\n\nLet's list the tables.\n\n```sh\nbin/uc table list --catalog unity --schema default\n```\n\nYou should see a few tables. Some details are truncated because of the nested nature of the data.\nTo see all the content, you can add `--output jsonPretty` to any command.\n\nNext, let's get the metadata of one of those tables.\n\n```sh\nbin/uc table get --full_name unity.default.numbers\n```\n\nYou can see that it is a Delta table. Now, specifically for Delta tables, this CLI can\nprint a snippet of the contents of a Delta table (powered by the [Delta Kernel Java](https://delta.io/blog/delta-kernel/) project).\nLet's try that.\n\n```sh\nbin/uc table read --full_name unity.default.numbers\n```\n\n### Operate on Delta tables with DuckDB\n\nFor operating on tables with DuckDB, you will have to [install it](https://duckdb.org/docs/installation/) (version 1.0).\nLet's start DuckDB and install a couple of extensions. To start DuckDB, run the command `duckdb` in the terminal.\nThen, in the DuckDB shell, run the following commands:\n\n```sql\ninstall uc_catalog from core_nightly;\nload uc_catalog;\ninstall delta;\nload delta;\n```\n\nIf you have installed these extensions before, you may have to run `update extensions` and restart DuckDB\nfor the following steps to work.\n\nNow that we have DuckDB all set up, let's try connecting to UC by specifying a secret.\n\n```sql\nCREATE SECRET (\n      TYPE UC,\n      TOKEN 'not-used',\n      ENDPOINT 'http://127.0.0.1:8080',\n      AWS_REGION 'us-east-2'\n );\n```\n\nYou should see it print a short table saying `Success` = `true`. Then we attach the `unity` catalog to DuckDB.\n\n```sql\nATTACH 'unity' AS unity (TYPE UC_CATALOG);\n```\n\nNow we are ready to query. Try the following:\n\n```sql\nSHOW ALL TABLES;\nSELECT * from unity.default.numbers;\n```\n\nYou should see the tables listed and the contents of the `numbers` table printed.\nTo quit DuckDB, press `Ctrl`+`D` (if your platform supports it), press `Ctrl`+`C`, or use the `.exit` command in the DuckDB shell.\n\n### Interact with the Unity Catalog UI\n\n![UC UI](./docs/assets/images/uc-ui.png)\n\nTo use the Unity Catalog UI, start a new terminal and ensure you have already started the UC server (e.g., `./bin/start-uc-server`)\n\n**Prerequisites**\n* Node: https://nodejs.org/en/download/package-manager\n* Yarn: https://classic.yarnpkg.com/lang/en/docs/install\n\n**How to start the UI through yarn**\n```\ncd /ui\nyarn install\nyarn start\n```\n\n\n## CLI tutorial\n\nYou can interact with a Unity Catalog server to create and manage catalogs, schemas and tables,\noperate on volumes and functions from the CLI, and much more.\nSee the [cli usage](docs/usage/cli.md) for more details.\n\n## APIs and Compatibility\n\n- Open API specification: See the [Unity Catalog Rest API](https://docs.unitycatalog.io/swagger-docs/).\n- Compatibility and stability: The APIs are currently evolving and should not be assumed to be stable.\n\n## Building Unity Catalog\n\nUnity Catalog can be built using [sbt](https://www.scala-sbt.org/).\n\nTo build UC (incl. [Spark Integration](./connectors/spark) module), run the following command:\n\n```sh\nbuild/sbt clean package publishLocal spark/publishLocal\n```\n\nRefer to [sbt docs](https://www.scala-sbt.org/1.x/docs/) for more commands.\n\n## Deployment\n\n- To create a tarball that can be used to deploy the UC server or run the CLI, run the following:\n  ```sh\n  build/sbt createTarball\n  ```\n  This will create a tarball in the `target` directory. See the full [deployment guide](docs/deployment.md) for more details.\n\n## Compiling and testing\n\n- Install JDK 17 by whatever mechanism is appropriate for your system, and\n  set that version to be the default Java version (e.g. via the env variable `JAVA_HOME`)\n- To compile all the code without running tests, run the following:\n  ```sh\n  build/sbt clean compile\n  ```\n- To compile and execute tests, run the following:\n  ```sh\n  build/sbt -J-Xmx2G clean test\n  ```\n- To execute tests with coverage, run the following:\n  ```sh\n  build/sbt -J-Xmx2G jacoco\n  ```\n- To update the API specification, just update the `api/all.yaml` and then run the following:\n  ```sh\n  build/sbt generate\n  ```\n  This will regenerate the OpenAPI data models in the UC server and data models + APIs in the client SDK.\n- To format the code, run the following:\n  ```sh\n  build/sbt javafmtAll\n  ```\n\n## Setting up IDE\n\nIntelliJ is the recommended IDE to use when developing Unity Catalog. The below steps outline how to add the project to IntelliJ:\n\n1. Clone Unity Catalog into a local folder, such as `~/unitycatalog`.\n2. Select `File` > `New Project` > `Project from Existing Sources...` and select `~/unitycatalog`.\n3. Under `Import project from external model` select `sbt`. Click `Next`.\n4. Click `Finish`.\n\nJava code adheres to the [Google style](https://google.github.io/styleguide/javaguide.html), which is verified via `build/sbt javafmtCheckAll` during builds.\nIn order to automatically fix Java code style issues, please use `build/sbt javafmtAll`.\n\n### Configuring Code Formatter for Eclipse/IntelliJ\n\nFollow the instructions for [Eclipse](https://github.com/google/google-java-format#eclipse) or\n[IntelliJ](https://github.com/google/google-java-format#intellij-android-studio-and-other-jetbrains-ides) to install the **google-java-format** plugin (note the required manual actions for IntelliJ).\n\n### Using more recent JDKs\n\nThe build script [checks for a lower bound on the JDK](./build.sbt#L14) but the [current SBT version](./project/build.properties)\nimposes an upper bound. Please check the [JDK compatibility](https://docs.scala-lang.org/overviews/jdk-compatibility/overview.html) documentation for more information\n\n### Serving the documentation with mkdocs\n\nFor an overview of how to contribute to the documentation, please see our introduction [here](./docs/README.md).\nFor the official documentation, please take a look at [https://docs.unitycatalog.io/](https://docs.unitycatalog.io/).\n"
    },
    {
      "name": "microsoft/msticpy",
      "stars": 1844,
      "img": "https://avatars.githubusercontent.com/u/6154722?s=40&v=4",
      "owner": "microsoft",
      "repo_name": "msticpy",
      "description": "Microsoft Threat Intelligence Security Tools",
      "homepage": null,
      "language": "Python",
      "created_at": "2019-02-21T17:30:03Z",
      "updated_at": "2025-04-22T19:03:28Z",
      "topics": [],
      "readme": "# MSTIC Jupyter and Python Security Tools\r\n\r\n![GitHub Actions build](https://github.com/microsoft/msticpy/actions/workflows/python-package.yml/badge.svg)\r\n[![Azure Pipelines build](https://dev.azure.com/mstic-detections/mstic-jupyter/_apis/build/status/microsoft.msticpy?branchName=main)](https://dev.azure.com/mstic-detections/mstic-jupyter/_build/latest?definitionId=14&branchName=main)\r\n[![Downloads](https://pepy.tech/badge/msticpy)](https://pepy.tech/project/msticpy)\r\n[![BlackHat Arsenal 2020](https://raw.githubusercontent.com/toolswatch/badges/master/arsenal/usa/2020.svg)](https://www.blackhat.com/us-20/arsenal/schedule/#msticpy-the-security-analysis-swiss-army-knife-19872)\r\n\r\nMicrosoft Threat Intelligence Python Security Tools.\r\n\r\n**msticpy** is a library for InfoSec investigation and hunting\r\nin Jupyter Notebooks. It includes functionality to:\r\n\r\n- query log data from multiple sources\r\n- enrich the data with Threat Intelligence, geolocations and Azure\r\n  resource data\r\n- extract Indicators of Activity (IoA) from logs and unpack encoded data\r\n- perform sophisticated analysis such as anomalous session detection and\r\n  time series decomposition\r\n- visualize data using interactive timelines, process trees and\r\n  multi-dimensional Morph Charts\r\n\r\nIt also includes some time-saving notebook tools such as widgets to\r\nset query time boundaries, select and display items from lists, and\r\nconfigure the notebook environment.\r\n\r\n<img src=\"./docs/source/visualization/_static/Timeline-08.png\"\r\nalt=\"Timeline\" title=\"Msticpy Timeline Control\" height=\"300\" />\r\n\r\nThe **msticpy** package was initially developed to support\r\n[Jupyter Notebooks](https://jupyter-notebook-beginner-guide.readthedocs.io/en/latest/)\r\nauthoring for\r\n[Azure Sentinel](https://azure.microsoft.com/en-us/services/azure-sentinel/).\r\nWhile Azure Sentinel is still a big focus of our work, we are\r\nextending the data query/acquisition components to pull log data from\r\nother sources (currently Splunk, Microsoft Defender for Endpoint and\r\nMicrosoft Graph are supported but we\r\nare actively working on support for data from other SIEM platforms).\r\nMost of the components can also be used with data from any source. Pandas\r\nDataFrames are used as the ubiquitous input and output format of almost\r\nall components. There is also a data provider to make it easy to and process\r\ndata from local CSV files and pickled DataFrames.\r\n\r\nThe package addresses three central needs for security investigators\r\nand hunters:\r\n\r\n- Acquiring and enriching data\r\n- Analyzing data\r\n- Visualizing data\r\n\r\nWe welcome feedback, bug reports, suggestions for new features and contributions.\r\n\r\n## Installing\r\n\r\nFor core install:\r\n\r\n`pip install msticpy`\r\n\r\nIf you are using *MSTICPy* with Azure Sentinel you should install with\r\nthe \"azsentinel\" extra package:\r\n\r\n`pip install msticpy[azsentinel]`\r\n\r\nor for the latest dev build\r\n\r\n`pip install git+https://github.com/microsoft/msticpy`\r\n\r\n## Upgrading\r\n\r\nTo upgrade msticpy to the latest public non-beta release, run:\r\n\r\n`pip install --upgrade msticpy`\r\n\r\nNote it is good practice to copy your msticpyconfig.yaml and store it on your disk but outside of your msticpy folder, referencing it in an environment variable. This prevents you from losing your configurations every time you update your msticpy installation.\r\n\r\n## Documentation\r\n\r\nFull documentation is at [ReadTheDocs](https://msticpy.readthedocs.io/en/latest/)\r\n\r\nSample notebooks for many of the modules are in the\r\n[docs/notebooks](https://github.com/microsoft/msticpy/blob/master/docs/notebooks)\r\nfolder and accompanying notebooks.\r\n\r\nYou can also browse through the sample notebooks referenced at the end of this document\r\nto see some of the functionality used in context. You can play with some of the package\r\nfunctions in this interactive demo on mybinder.org.\r\n\r\n[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/Azure/Azure-Sentinel-Notebooks/master?filepath=A%20Tour%20of%20Cybersec%20notebook%20features.ipynb)\r\n\r\n---\r\n\r\n## Log Data Acquisition\r\n\r\nQueryProvider is an extensible query library targeting Azure Sentinel/Log Analytics,\r\nSplunk, OData\r\nand other log data sources. It also has special support for\r\n[Mordor](https://github.com/OTRF/mordor) data sets and using local data.\r\n\r\nBuilt-in parameterized queries allow complex queries to be run\r\nfrom a single function call. Add your own queries using a simple YAML\r\nschema.\r\n\r\n[Data Queries Notebook](https://github.com/microsoft/msticpy/blob/master/docs/notebooks/Data_Queries.ipynb)\r\n\r\n## Data Enrichment\r\n\r\n### Threat Intelligence providers\r\n\r\nThe TILookup class can lookup IoCs across multiple TI providers. built-in\r\nproviders include AlienVault OTX, IBM XForce, VirusTotal and Azure Sentinel.\r\n\r\nThe input can be a single IoC observable or a pandas DataFrame containing\r\nmultiple observables. Depending on the provider, you may require an account\r\nand an API key. Some providers also enforce throttling (especially for free\r\ntiers), which might affect performing bulk lookups.\r\n\r\n[TIProviders](https://msticpy.readthedocs.io/en/latest/data_acquisition/TIProviders.html)\r\nand\r\n[TILookup Usage Notebook](https://github.com/microsoft/msticpy/blob/master/docs/notebooks/TIProviders.ipynb)\r\n\r\n### GeoLocation Data\r\n\r\nThe GeoIP lookup classes allow you to match the geo-locations of IP addresses\r\nusing either:\r\n\r\n- GeoLiteLookup - Maxmind Geolite (see <https://www.maxmind.com>)\r\n- IPStackLookup  - IPStack (see <https://ipstack.com>)\r\n\r\n<img src=\"./docs/source/visualization/_static/folium_sf_zoom.png\"\r\n  alt=\"Folium map\"\r\n  title=\"Plotting Geo IP Location\" height=\"200\" />\r\n\r\n[GeoIP Lookup](https://msticpy.readthedocs.io/en/latest/data_acquisition/GeoIPLookups.html)\r\nand\r\n[GeoIP Notebook](https://github.com/microsoft/msticpy/blob/master/docs/notebooks/GeoIPLookups.ipynb)\r\n\r\n### Azure Resource Data, Storage and Azure Sentinel API\r\n\r\nThe AzureData module contains functionality for enriching data regarding Azure host\r\ndetails with additional host details exposed via the Azure API. The AzureSentinel\r\nmodule allows you to query incidents, retrieve detector and hunting\r\nqueries. AzureBlogStorage lets you read and write data from blob storage.\r\n\r\n[Azure Resource APIs](https://msticpy.readthedocs.io/en/latest/data_acquisition/AzureData.html),\r\n[Azure Sentinel APIs](https://msticpy.readthedocs.io/en/latest/data_acquisition/Sentinel.html),\r\n[Azure Storage](https://msticpy.readthedocs.io/en/latest/data_acquisition/AzureBlobStorage.html)\r\n\r\n## Security Analysis\r\n\r\nThis subpackage contains several modules helpful for working on security investigations and hunting:\r\n\r\n### Anomalous Sequence Detection\r\n\r\nDetect unusual sequences of events in your Office, Active Directory or other log data.\r\nYou can extract sessions (e.g. activity initiated by the same account) and identify and\r\nvisualize unusual sequences of activity. For example, detecting an attacker setting\r\na mail forwarding rule on someone's mailbox.\r\n\r\n[Anomalous Sessions](https://msticpy.readthedocs.io/en/latest/data_analysis/AnomalousSequence.html)\r\nand\r\n[Anomalous Sequence Notebook](https://github.com/microsoft/msticpy/blob/master/docs/notebooks/AnomalousSequence.ipynb)\r\n\r\n### Time Series Analysis\r\n\r\nTime series analysis allows you to identify unusual patterns in your log data\r\ntaking into account normal seasonal variations (e.g. the regular ebb and flow of\r\nevents over hours of the day, days of the week, etc.). Using both analysis and\r\nvisualization highlights unusual traffic flows or event activity for any data\r\nset.\r\n\r\n<img src=\"./docs/source/visualization/_static/TimeSeriesAnomalieswithRangeTool.png\"\r\nalt=\"Time Series anomalies\" title=\"Time Series anomalies\" height=\"300\" />\r\n\r\n[Time Series](https://msticpy.readthedocs.io/en/latest/visualization/TimeSeriesAnomalies.html)\r\n\r\n## Visualization\r\n\r\n### Event Timelines\r\n\r\nDisplay any log events on an interactive timeline. Using the\r\n[Bokeh Visualization Library](https://bokeh.org/) the timeline control enables\r\nyou to visualize one or more event streams, interactively zoom into specific time\r\nslots and view event details for plotted events.\r\n\r\n<img src=\"./docs/source/visualization/_static/TimeLine-01.png\"\r\nalt=\"Timeline\" title=\"Msticpy Timeline Control\" height=\"300\" />\r\n\r\n[Timeline](https://msticpy.readthedocs.io/en/latest/visualization/EventTimeline.html)\r\nand\r\n[Timeline Notebook](https://github.com/microsoft/msticpy/blob/master/docs/notebooks/EventTimeline.ipynb)\r\n\r\n### Process Trees\r\n\r\nThe process tree functionality has two main components:\r\n\r\n- Process Tree creation - taking a process creation log from a host and building\r\n  the parent-child relationships between processes in the data set.\r\n- Process Tree visualization - this takes the processed output displays an interactive process tree using Bokeh plots.\r\n\r\nThere are a set of utility functions to extract individual and partial trees from the processed data set.\r\n\r\n<img src=\"./docs/source/visualization/_static/process_tree3.png\"\r\nalt=\"Process Tree\"\r\ntitle=\"Interactive Process Tree\" height=\"400\" />\r\n\r\n[Process Tree](https://msticpy.readthedocs.io/en/latest/visualization/ProcessTree.html)\r\nand\r\n[Process Tree Notebook](https://github.com/microsoft/msticpy/blob/master/docs/notebooks/ProcessTree.ipynb)\r\n\r\n## Data Manipulation and Utility functions\r\n\r\n### Pivot Functions\r\n\r\nLets you use *MSTICPy* functionality in an \"entity-centric\" way.\r\nAll functions, queries and lookups that relate to a particular entity type\r\n(e.g. Host, IpAddress, Url) are collected together as methods of that\r\nentity class. So, if you want to do things with an IP address, just load\r\nthe IpAddress entity and browse its methods.\r\n\r\n[Pivot Functions](https://msticpy.readthedocs.io/en/latest/data_analysis/PivotFunctions.html)\r\nand\r\n[Pivot Functions Notebook](https://github.com/microsoft/msticpy/blob/master/docs/notebooks/PivotFunctions.ipynb)\r\n\r\n### base64unpack\r\n\r\nBase64 and archive (gz, zip, tar) extractor. It will try to identify any base64 encoded\r\nstrings and try decode them. If the result looks like one of the supported archive types it\r\nwill unpack the contents. The results of each decode/unpack are rechecked for further\r\nbase64 content and up to a specified depth.\r\n\r\n[Base64 Decoding](https://msticpy.readthedocs.io/en/latest/data_analysis/Base64Unpack.html)\r\nand\r\n[Base64Unpack Notebook](https://github.com/microsoft/msticpy/blob/master/docs/notebooks/Base64Unpack.ipynb)\r\n\r\n### iocextract\r\n\r\nUses regular expressions to look for Indicator of Compromise (IoC) patterns - IP Addresses, URLs,\r\nDNS domains, Hashes, file paths.\r\nInput can be a single string or a pandas dataframe.\r\n\r\n[IoC Extraction](https://msticpy.readthedocs.io/en/latest/data_analysis/IoCExtract.html)\r\nand\r\n[IoCExtract Notebook](https://github.com/microsoft/msticpy/blob/master/docs/notebooks/IoCExtract.ipynb)\r\n\r\n### eventcluster (experimental)\r\n\r\nThis module is intended to be used to summarize large numbers of\r\nevents into clusters of different patterns. High volume repeating\r\nevents can often make it difficult to see unique and interesting items.\r\n\r\n<img src=\"./docs/source/data_analysis/_static/EventClustering_2a.png\"\r\n  alt=\"Clustering\"\r\n  title=\"Clustering based on command-line variability\" height=\"400\" />\r\n\r\nThis is an unsupervised learning module implemented using SciKit Learn DBScan.\r\n\r\n[Event Clustering](https://msticpy.readthedocs.io/en/latest/data_analysis/EventClustering.html)\r\nand\r\n[Event Clustering Notebook](https://github.com/microsoft/msticpy/blob/master/docs/notebooks/EventClustering.ipynb)\r\n\r\n### auditdextract\r\n\r\nModule to load and decode Linux audit logs. It collapses messages sharing the same\r\nmessage ID into single events, decodes hex-encoded data fields and performs some\r\nevent-specific formatting and normalization (e.g. for process start events it will\r\nre-assemble the process command line arguments into a single string).\r\n\r\n### syslog_utils\r\n\r\nModule to support an investigation of a Linux host with only syslog logging enabled.\r\nThis includes functions for collating host data, clustering logon events and detecting\r\nuser sessions containing suspicious activity.\r\n\r\n### cmd_line\r\n\r\nA module to support the detection of known malicious command line activity or suspicious\r\npatterns of command line activity.\r\n\r\n### domain_utils\r\n\r\nA module to support investigation of domain names and URLs with functions to\r\nvalidate a domain name and screenshot a URL.\r\n\r\n### Notebook widgets\r\n\r\nThese are built from the [Jupyter ipywidgets](https://ipywidgets.readthedocs.io/) collection\r\nand group common functionality useful in InfoSec tasks such as list pickers,\r\nquery time boundary settings and event display into an easy-to-use format.\r\n\r\n<img src=\"./docs/source/visualization/_static/Widgets1.png\"\r\n  alt=\"Time span Widget\"\r\n  title=\"Query time setter\" height=\"100\" />\r\n\r\n<img src=\"./docs/source/visualization/_static/Widgets4.png\"\r\n  alt=\"Alert browser\"\r\n  title=\"Alert browser\" height=\"300\" />\r\n\r\n---\r\n\r\n## Example MSTICPy notebooks\r\n\r\n[MSTICPy Notebooks](https://github.com/microsoft/msticpy/blob/master/docs/notebooks/)\r\n\r\n## More Notebooks on Azure Sentinel Notebooks GitHub\r\n\r\n[Azure Sentinel Notebooks](https://github.com/Azure/Azure-Sentinel-Notebooks)\r\n\r\nExample notebooks:\r\n\r\n- [Tour of MSTICPy Features](https://github.com/Azure/Azure-Sentinel-Notebooks/blob/master/A%20Tour%20of%20Cybersec%20notebook%20features.ipynb)\r\n- [Account Explorer](https://github.com/Azure/Azure-Sentinel-Notebooks/blob/master/Entity%20Explorer%20-%20Account.ipynb)\r\n- [Domain and URL Explorer](https://github.com/Azure/Azure-Sentinel-Notebooks/blob/master/Entity%20Explorer%20-%20Domain%20and%20URL.ipynb)\r\n- [IP Explorer](https://github.com/Azure/Azure-Sentinel-Notebooks/blob/master/Entity%20Explorer%20-%20IP%20Address.ipynb)\r\n- [Linux Host Explorer](https://github.com/Azure/Azure-Sentinel-Notebooks/blob/master/Entity%20Explorer%20-%20Linux%20Host.ipynb)\r\n- [Windows Host Explorer](https://github.com/Azure/Azure-Sentinel-Notebooks/blob/master/Entity%20Explorer%20-%20Windows%20Host.ipynb)\r\n- [Getting started in Azure Sentinel notebooks](https://github.com/Azure/Azure-Sentinel-Notebooks/blob/master/A%20Getting%20Started%20Guide%20For%20Azure%20Sentinel%20ML%20Notebooks.ipynb)\r\n\r\nView directly on GitHub or copy and paste the link into [nbviewer.org](https://nbviewer.jupyter.org/)\r\n\r\n## Notebook examples with saved data\r\n\r\nSee the following notebooks for more examples of the use of this package in practice:\r\n\r\n- [Simple Machine Learning](https://github.com/Azure/Azure-Sentinel-Notebooks/blob/master/Machine%20Learning%20in%20Notebooks%20Examples.ipynb)\r\n\r\n## Supported Platforms and Packages\r\n\r\n- msticpy is OS-independent\r\n- Requires [Python 3.8 or later](https://www.python.org/dev/peps/pep-0494/)\r\n- See [requirements.txt](requirements.txt) for more details and version requirements.\r\n\r\n---\r\n\r\n## Contributing\r\n\r\nFor (brief) developer guidelines, see this wiki article\r\n[Contributor Guidelines](https://github.com/microsoft/msticpy/wiki/Contributor-guidelines)\r\n\r\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\r\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\r\nthe rights to use your contribution. For details, visit <https://cla.microsoft.com>.\r\n\r\nWhen you submit a pull request, a CLA-bot will automatically determine whether you need to provide\r\na CLA and decorate the PR appropriately (e.g., label, comment). Simply follow the instructions\r\nprovided by the bot. You will only need to do this once across all repos using our CLA.\r\n\r\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\r\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\r\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\r\n"
    },
    {
      "name": "panaversity/learn-agentic-ai",
      "stars": 1330,
      "img": "https://avatars.githubusercontent.com/u/98460328?s=40&v=4",
      "owner": "panaversity",
      "repo_name": "learn-agentic-ai",
      "description": "Learn Agentic AI using Dapr Agentic Cloud Ascent (DACA) Design Pattern: OpenAI Agents SDK, Memory, MCP, A2A, Knowledge Graphs, Rancher Desktop, and Kubernetes.",
      "homepage": "",
      "language": "Jupyter Notebook",
      "created_at": "2024-06-12T07:50:04Z",
      "updated_at": "2025-04-23T04:53:57Z",
      "topics": [
        "a2a",
        "agentic-ai",
        "dapr",
        "dapr-pub-sub",
        "dapr-service-invocation",
        "dapr-sidecar",
        "dapr-workflow",
        "docker",
        "kafka",
        "kubernetes",
        "langmem",
        "mcp",
        "openai",
        "openai-agents-sdk",
        "openai-api",
        "postgresql-database",
        "rabbitmq",
        "rancher-desktop",
        "redis",
        "serverless-containers"
      ],
      "readme": "# Learn Agentic AI using Dapr Agentic Cloud Ascent (DACA) Design Pattern: From Start to Scale\n\nThis repo is part of the [Panaversity Certified Agentic & Robotic AI Engineer](https://docs.google.com/document/d/15usu1hkrrRLRjcq_3nCTT-0ljEcgiC44iSdvdqrCprk/edit?usp=sharing) program. It covers AI-201, AI-202 and AI-301 courses.\n\n## This Panaversity Initiative Tackles the Critical Challenge: \n\n**“How do we design AI Agents that can handle 10 million concurrent users without failing?”**\n\nNote: The challenge is intensified as we must guide our students to solve this issue with minimal financial resources available during training.\n\n<p align=\"center\">\n<img src=\"./img/cover.png\" width=\"600\">\n</p>\n\nKubernetes with Dapr can theoretically handle 10 million concurrent users in an agentic AI system without failing, but achieving this requires extensive optimization, significant infrastructure, and careful engineering. While direct evidence at this scale is limited, logical extrapolation from existing benchmarks, Kubernetes’ scalability, and Dapr’s actor model supports feasibility, especially with rigorous tuning and resource allocation.\n\n**Condensed Argument with Proof and Logic**:\n\n1. **Kubernetes Scalability**:\n   - **Evidence**: Kubernetes supports up to 5,000 nodes and 150,000 pods per cluster (Kubernetes docs), with real-world examples like PayPal scaling to 4,000 nodes and 200,000 pods (InfoQ, 2023) and KubeEdge managing 100,000 edge nodes and 1 million pods (KubeEdge case studies). OpenAI’s 2,500-node cluster for AI workloads (OpenAI blog, 2022) shows Kubernetes can handle compute-intensive tasks.\n   - **Logic**: For 10 million users, a cluster of 5,000–10,000 nodes (e.g., AWS g5 instances with GPUs) can distribute workloads. Each node can run hundreds of pods, and Kubernetes’ horizontal pod autoscaling (HPA) dynamically adjusts to demand. Bottlenecks (e.g., API server, networking) can be mitigated by tuning etcd, using high-performance CNIs like Cilium, and optimizing DNS.\n\n2. **Dapr’s Efficiency for Agentic AI**:\n   - **Evidence**: Dapr’s actor model supports thousands of virtual actors per CPU core with double-digit millisecond latency (Dapr docs, 2024). Case studies show Dapr handling millions of events, e.g., Tempestive’s IoT platform processing billions of messages (Dapr blog, 2023) and DeFacto’s system managing 3,700 events/second (320 million daily) on Kubernetes with Kafka (Microsoft case study, 2022).\n   - **Logic**: Agentic AI relies on stateful, low-latency agents. Dapr Agents, built on the actor model, can represent 10 million users as actors, distributed across a Kubernetes cluster. Dapr’s state management (e.g., Redis) and pub/sub messaging (e.g., Kafka) ensure efficient coordination and resilience, with automatic retries preventing failures. Sharding state stores and message brokers scales to millions of operations/second.\n\n3. **Handling AI Workloads**:\n   - **Evidence**: LLM inference frameworks like vLLM and TGI serve thousands of requests/second per GPU (vLLM benchmarks, 2024). Kubernetes orchestrates GPU workloads effectively, as seen  Kubernetes manages GPU workloads, as seen in NVIDIA’s AI platform scaling to thousands of GPUs (NVIDIA case study, 2023).\n   - **Logic**: Assuming each user generates 1 request/second requiring 0.01 GPU, 10 million users need ~100,000 GPUs. Batching, caching, and model parallelism reduce this to a feasible ~10,000–20,000 GPUs, achievable in hyperscale clouds (e.g., AWS). Kubernetes’ resource scheduling ensures optimal GPU utilization.\n\n4. **Networking and Storage**:\n   - **Evidence**: EMQX on Kubernetes handled 1 million concurrent connections with tuning (EMQX blog, 2024). C10M benchmarks (2013) achieved 10 million connections using optimized stacks. Dapr’s state stores (e.g., Redis) support millions of operations/second (Redis benchmarks, 2024).\n   - **Logic**: 10 million connections require ~100–1,000 Gbps bandwidth, supported by modern clouds. High-throughput databases (e.g., CockroachDB) and caching (e.g., Redis Cluster) handle 10 TB of state data for 10 million users (1 KB/user). Kernel bypass (e.g., DPDK) and eBPF-based CNIs (e.g., Cilium) minimize networking latency.\n\n5. **Resilience and Monitoring**:\n   - **Evidence**: Dapr’s resiliency policies (retries, circuit breakers) and Kubernetes’ self-healing (pod restarts) ensure reliability (Dapr docs, 2024). Dapr’s OpenTelemetry integration scales monitoring for millions of agents (Prometheus case studies, 2023).\n   - **Logic**: Real-time metrics (e.g., latency, error rates) and distributed tracing prevent cascading failures. Kubernetes’ liveness probes and Dapr’s workflow engine recover from crashes, ensuring 99.999% uptime.\n\n**Feasibility with Constraints**:\n- **Challenge**: No direct benchmark exists for 10 million concurrent users with Dapr/Kubernetes in an agentic AI context. Infrastructure costs (e.g., $10M–$100M for 10,000 nodes) are prohibitive for low-budget scenarios.\n- **Solution**: Use open-source tools (e.g., Minikube, kind) for local testing and cloud credits (e.g., AWS Educate) for students. Simulate 10 million users with tools like Locust on smaller clusters (e.g., 100 nodes), extrapolating results. Optimize Dapr’s actor placement and Kubernetes’ resource quotas to maximize efficiency on limited hardware. Leverage free-tier databases (e.g., MongoDB Atlas) and message brokers (e.g., RabbitMQ).\n\n**Conclusion**: Kubernetes with Dapr can handle 10 million concurrent users in an agentic AI system, supported by their proven scalability, real-world case studies, and logical extrapolation. For students with minimal budgets, small-scale simulations, open-source tools, and cloud credits make the problem tractable, though production-scale deployment requires hyperscale resources and expertise.\n\n\n**Agentic AI Top Trend of 2025**\n\n<p align=\"center\">\n<img src=\"./img/toptrend.webp\" width=\"200\">\n</p>\n\n\n## The Dapr Agentic Cloud Ascent (DACA) Design Pattern Addresses 10 Million Concurrent Users Challenge \n\nLet's understand and learn about \"Dapr Agentic Cloud Ascent (DACA)\", our winning design pattern for developing and deploying planet scale multi-agent systems.\n\n### Executive Summary: Dapr Agentic Cloud Ascent (DACA)\n\nThe Dapr Agentic Cloud Ascent (DACA) guide introduces a strategic design pattern for building and deploying sophisticated, scalable, and resilient agentic AI systems. Addressing the complexities of modern AI development, DACA integrates the OpenAI Agents SDK for core agent logic with the Model Context Protocol (MCP) for standardized tool use and the Agent2Agent (A2A) protocol for seamless inter-agent communication, all underpinned by the distributed capabilities of Dapr. **Grounded in AI-first and cloud-first principles**, DACA promotes the use of stateless, containerized applications deployed on platforms like Azure Container Apps (Serverless Containers) or Kubernetes, enabling efficient scaling from local development to planetary-scale production, potentially leveraging free-tier cloud services and self-hosted LLMs for cost optimization. The pattern emphasizes modularity, context-awareness, and standardized communication, envisioning an **Agentia World** where diverse AI agents collaborate intelligently. Ultimately, DACA offers a robust, flexible, and cost-effective framework for developers and architects aiming to create complex, cloud-native agentic AI applications that are built for scalability and resilience from the ground up.\n\n\n**[Comprehensive Guide to Dapr Agentic Cloud Ascent (DACA) Design Pattern](https://github.com/panaversity/learn-agentic-ai/blob/main/comprehensive_guide_daca.md)**\n\n<p align=\"center\">\n<img src=\"./img/ascent.png\" width=\"500\">\n</p>\n\n<p align=\"center\">\n<img src=\"./img/architecture1.png\" width=\"400\">\n</p>\n\n\n\n\n### Target User\n- **Agentic AI Developer and AgentOps Professionals**\n\n### Why OpenAI Agents SDK should be the main framework for agentic development for most use cases?\n\n**Table 1: Comparison of Abstraction Levels in AI Agent Frameworks**\n\n| **Framework**         | **Abstraction Level** | **Key Characteristics**                                                                 | **Learning Curve** | **Control Level** | **Simplicity** |\n|-----------------------|-----------------------|-----------------------------------------------------------------------------------------|--------------------|-------------------|----------------|\n| **OpenAI Agents SDK** | Minimal              | Python-first, core primitives (Agents, Handoffs, Guardrails), direct control           | Low               | High             | High           |\n| **CrewAI**            | Moderate             | Role-based agents, crews, tasks, focus on collaboration                                | Low-Medium        | Medium           | Medium         |\n| **AutoGen**           | High                 | Conversational agents, flexible conversation patterns, human-in-the-loop support       | Medium            | Medium           | Medium         |\n| **Google ADK**        | Moderate             | Multi-agent hierarchies, Google Cloud integration (Gemini, Vertex AI), rich tool ecosystem, bidirectional streaming | Medium            | Medium-High      | Medium         |\n| **LangGraph**         | Low-Moderate         | Graph-based workflows, nodes, edges, explicit state management                        | Very High         | Very High        | Low            |\n| **Dapr Agents**       | Moderate             | Stateful virtual actors, event-driven multi-agent workflows, Kubernetes integration, 50+ data connectors, built-in resiliency | Medium            | Medium-High      | Medium         |\n\n\nThe table clearly identifies why OpenAI Agents SDK should be the main framework for agentic development for most use cases:\n- It excels in **simplicity** and **ease of use**, making it the best choice for rapid development and broad accessibility.\n- It offers **high control** with **minimal abstraction**, providing the flexibility needed for agentic development without the complexity of frameworks like LangGraph.\n- It outperforms most alternatives (CrewAI, AutoGen, Google ADK, Dapr Agents) in balancing usability and power, and while LangGraph offers more control, its complexity makes it less practical for general use.\n\nIf your priority is ease of use, flexibility, and quick iteration in agentic development, OpenAI Agents SDK is the clear winner based on the table. However, if your project requires enterprise-scale features (e.g., Dapr Agents) or maximum control for complex workflows (e.g., LangGraph), you might consider those alternatives despite their added complexity. \n\n## Core DACA Agentic AI Courses:\n\n### AI-201:  Fundamentals of Agentic AI and DACA AI-First Development (14 weeks)\n\n- ⁠Agentic & DACA Theory - 1 week\n- UV & ⁠OpenAI Agents SDK - 5 weeks\n- ⁠Agentic Design Patterns - 2 weeks \n- ⁠Memory [LangMem & mem0] 1 week\n- Postgres/Redis (Managed Cloud) - 1 week\n- FastAPI (Basic)  - 2 weeks\n- ⁠Containerization (Rancher Desktop) - 1 week\n- Hugging Face Docker Spaces - 1 week\n\n\n**[AI-201 Video Playlist](https://www.youtube.com/playlist?list=PL0vKVrkG4hWovpr0FX6Gs-06hfsPDEUe6)**\n\nNote: These videos are for additional learning, and do not cover all the material taught in the onsite classes.\n\nPrerequisite: Successful completion of [AI-101: Modern AI Python Programming - Your Launchpad into Intelligent Systems](https://github.com/panaversity/learn-modern-ai-python)\n\n### AI-202: DACA Cloud-First Agentic AI Development (14 weeks)\n- Rancher Desktop with Local Kubernetes - 4 weeks\n- Advanced FastAPI with Kubernetes - 2 weeks\n- Dapr [workflows, state, pubsub, secrets] - 3 Week\n- CockRoachdb & RabbitMQ Managed Services - 2 weeks\n- ⁠Model Context Protocol -  2 weeks\n- ⁠Serverless Containers Deployment (ACA) - 2 weeks\n\nPrerequisite: Successful completion of AI-201\n\n### AI-301 DACA Planet-Scale Distributed AI Agents (14 Weeks)\n- ⁠Certified Kubernetes Application Developer (CKAD) - 4 weeks\n- ⁠A2A Protocol - 2 weeks\n- ⁠Voice Agents - 2 weeks\n- ⁠Dapr Agents/Google ADK - 2 weeks\n- ⁠Self-LLMs Hosting - 1 week\n- Finetuning LLMs - 3 weeks\n\nPrerequisite: Successful completion of AI-201 & AI-202\n\n\n\n\n\n\n\n"
    },
    {
      "name": "EmergenceAI/Agent-E",
      "stars": 1093,
      "img": "https://avatars.githubusercontent.com/u/162732084?s=40&v=4",
      "owner": "EmergenceAI",
      "repo_name": "Agent-E",
      "description": "Agent driven automation starting with the web. Try it: https://www.emergence.ai/web-automation-api",
      "homepage": "",
      "language": "Python",
      "created_at": "2024-03-28T21:54:39Z",
      "updated_at": "2025-04-22T09:29:43Z",
      "topics": [],
      "readme": "# Agent-E\n**Free Trial: Managed Web Agent & Orchestrator**<br>\nTry our Web Agent (Agent-E with enterprise enhancements) and multi-agent orchestrator. Access features such as advanced logging, role-based access, and cloud-hosted scalable infrastructure, backed by expert support. Sign up [here](https://www.emergence.ai/orchestrator)\n\n\n[Discord](https://discord.gg/wgNfmFuqJF) &nbsp;&nbsp; [Cite paper](https://arxiv.org/abs/2407.13032) _Note: The WebVoyager validation used [nested_chat_for_hierarchial_planning branch](https://github.com/EmergenceAI/Agent-E/tree/nested_chat_for_hierarchial_planning) and GPT4-Turbo_\n\n\nAgent-E is an agent based system that aims to automate actions on the user's computer. At the moment it focuses on automation within the browser. The system is based on on [AG2 agent framework](https://docs.ag2.ai/docs/Home).\n\nThis provides a natural language way to interacting with a web browser:\n- Fill out forms (web forms not PDF yet) using information about you or from another site\n- Search and sort products on e-commerce sites like Amazon based on various criteria, such as bestsellers or price.\n- Locate specific content and details on websites, from sports scores on ESPN to contact information on university pages.\n- Navigate to and interact with web-based media, including playing YouTube videos and managing playback settings like full-screen and mute.\n- Perform comprehensive web searches to gather information on a wide array of topics, from historical sites to top local restaurants.\n- Manage and automate tasks on project management platforms (like JIRA) by filtering issues, easing the workflow for users.\n- Provide personal shopping assistance, suggesting products based on the user's needs, such as storage options for game cards.\n\nWhile Agent-E is growing, it is already equipped to handle a versatile range of tasks, but the best task is the one that you come up with. So, take it for a spin and tell us what you were able to do with it. For more information see our [blog article](https://www.emergence.ai/blog/distilling-the-web-for-multi-agent-automation).\n\n\n## Quick Start Using Scripts\nTo get started with Agent-E, follow the steps below to install dependencies and configure your environment.\n#### 1. Run the Installation Script\n\n- **macOS/Linux**:\n  - From the project root, run the following command to set up the environment and install all dependencies:\n    ```bash\n    ./install.sh\n    ```\n    - For **Playwright support**, you can pass the `-p` flag to install Playwright without further prompting:\n      ```bash\n      ./install.sh -p\n      ```\n\n- **Windows**:\n  - From the project root, execute the following command in PowerShell:\n    ```powershell\n    .\\win_install.ps1\n    ```\n    - To install Playwright without further prompting, add the `-p` flag:\n      ```powershell\n      .\\win_install.ps1 -p\n      ```\n#### 2. Configure Environment Variables\n- Go to the newly created `.env` and `agents_llm_config.json` and follow the instructions to set the fields\n\n#### 3. Run Agent-E\nOnce you have set up the environment and installed all the dependencies, you can run Agent-E using the following command:\n```bash\npython -m ae.main\n```\n\n**For macOS Users**\n```bash\npython -u -m ae.main\n```\n\n\n## Manual Setup\n\n### 1. Install `uv`\nAgent-E uses `uv` to manage the Python virtual environment and package dependencies.\n\n- **macOS/Linux**:\n  ```bash\n  curl -LsSf https://astral.sh/uv/install.sh | sh\n\n- **Windows**:\n  ```bash\n  powershell -c \"irm https://astral.sh/uv/install.ps1 | iex\"\n  ```\n\n- Alternatively, you can install `uv` using `pip`: `pip install uv`\n\n### 2. Set up the virtual environment\nUse `uv` to create and activate a virtual environment for the project.\n```bash\nuv venv --python 3.11  # 3.10+ should also work\nsource .venv/bin/activate  # On Windows: .venv\\Scripts\\activate\n```\n\n### 3. Install dependencies\nGenerate the `requirements.txt` file from the `pyproject.toml` and install dependencies.\n```bash\nuv pip compile pyproject.toml -o requirements.txt\nuv pip install -r requirements.txt\n```\n\nTo install extras for development, run:\n```bash\nuv pip install -r pyproject.toml --extra dev\n```\n\n### 4.(Optional) Install Playwright Drivers\nIf you do not have Google Chrome installed locally and don’t want to install it, you can use Playwright for browser automation.\n```bash\nplaywright install\n```\n\n### 5. Configure the environment\nCreate a `.env` file by copying the provided example file.\n```bash\ncp .env-example .env\n```\n- Edit the `.env` file and set the following variables:\n    - `AUTOGEN_MODEL_NAME` (e.g., `gpt-4-turbo` for optimal performance).\n    - `AUTOGEN_MODEL_API_KEY` LLM API key.\n    - If using a model other than OpenAI, configure `AUTOGEN_MODEL_BASE_URL` (url where the completion endpoint is hosted, but don't put `/completion` in it), `AUTOGEN_MODEL_API_TYPE`, and `AUTOGEN_MODEL_API_VERSION`.\n- Optionally configure `AUTOGEN_LLM_TEMPERATURE` and `AUTOGEN_LLM_TOP_P`.\n- If you want to use local chrome browser over playwright browser, go to chrome://version/ in chrome, find the path to your profile and set `BROWSER_STORAGE_DIR` to the path value\n\n## Environment Variables\n\nAgent-E relies on several environment variables for its configuration. You need to define these in a `.env` file in the project root. A sample `.env-example` file is provided for convenience.\n\n### Key Variables:\n\n- **`AUTOGEN_MODEL_NAME`**  \n  Name of the LLM model you want to use (e.g., `gpt-4-turbo`). This is required for most setups.\n  \n- **`AUTOGEN_MODEL_API_KEY`**  \n  Your API key for accessing the LLM model (e.g., OpenAI API key).\n  \n- **`AUTOGEN_MODEL_BASE_URL`** *(optional)*  \n  Base URL for the model if it's hosted on a service other than OpenAI (e.g., Azure OpenAI services). Example:  \n  `https://api.groq.com/openai/v1`  \n  or  \n  `https://<YOUR_AZURE_ENDPOINT>.openai.azure.com`\n\n- **`AUTOGEN_MODEL_API_TYPE`** *(optional)*  \n  Type of model API (e.g., `azure` for Azure-hosted models).\n\n- **`AUTOGEN_MODEL_API_VERSION`** *(optional)*  \n  Version of the model API to use, typically needed for Azure models (e.g., `2023-03-15-preview`).\n\n- **`AUTOGEN_LLM_TEMPERATURE`** *(optional)*  \n  Sets the temperature for the LLM. Controls randomness in output. Defaults to `0.0` for `gpt-*` models.\n\n- **`AUTOGEN_LLM_TOP_P`** *(optional)*  \n  Sets the top-p value, which controls the diversity of token sampling. Defaults to `0.001` for `gpt-*` models.\n\n- **`BROWSER_STORAGE_DIR`** *(optional)*  \n  Path to your local Chrome browser profile, required if using a local Chrome instance instead of Playwright.\n\n- **`SAVE_CHAT_LOGS_TO_FILE`**  \n  Set to `true` or `false` (Default: `true`). Indicates whether to save chat logs to a file or print them to stdout.\n\n- **`LOG_MESSAGES_FORMAT`**  \n  Set to `json` or `text` (Default: `text`). Specifies the format for logging messages.\n\n- **`ADDITIONAL_SKILL_DIRS`** *(optional)*\n  A comma-separated list of directories or `.py` files where additional skills can be loaded from. This is used to dynamically load skills from specified directories or files.\n  Example: `ADDITIONAL_SKILL_DIRS=\"./private_skills,./extra_skills/my_custom_skill.py\"` would be added to the `.env` file (or equivalent)\n\n- **`PLANNER_USER_INPUT_SKILL_ENABLED`** *(optional)*\n  Set to `true` or `false` (Default: `false`). Specifies whether to allow the planner agent to get user input or not.\n  \n## Running the Code\n\nOnce you have set up the environment and installed all the dependencies, you can run Agent-E using `./run.sh` script or using the following command:\n```bash\npython -m ae.main\n```\n\n### For macOS Users\nIf you encounter `BlockingIOError` (Errno 35) when running the program on macOS, execute the following command to avoid the issue:\n```bash\npython -u -m ae.main\n```\n\n### Expected Behavior\nOnce Agent-E is running, you should see an icon in the browser interface. Clicking on this icon will open a chat-like interface where you can input natural language commands. Example commands you can try:\n- `open youtube and search for funny cat videos`\n- `find iPhone 14 on Amazon and sort by best seller`\n\n## Advanced Usage\n\n### Launch via Web Endpoint\n\nAgent-E provides a FastAPI wrapper, allowing you to send commands via HTTP and receive streaming results. This feature is useful for programmatic task automation or integrating Agent-E into larger systems.\n\n#### To launch the FastAPI server:\n\n1. On Linux/macOS, run the following command:\n   ```bash\n   uvicorn ae.server.api_routes:app --reload --loop asyncio\n   ```\n2. On Windows, run the same command but without ```--reload``` (Python still has different async implementations across OSes, removing --reload helping finding a workaround, see this [answer on  StackOverflow](https://stackoverflow.com/a/78795990)):\n   ```cmd\n   uvicorn ae.server.api_routes:app --loop asyncio\n   ```\n\n2. Send POST requests to execute tasks. For example, to execute a task using cURL:\n```bash\ncurl --location 'http://127.0.0.1:8000/execute_task' \\\n--header 'Content-Type: application/json' \\\n--data '{\n    \"command\": \"go to espn, look for soccer news, report the names of the most recent soccer champs\"\n}'\n```\nOptionally, the API request can include an llm_config object if you want to apply a different configuration during API request execution. The llm_config object should have configuration seperately for planner_agent and browser_nav_agent. See  `agents_llm_config-example.json` for an exmaple.\n\n```bash\ncurl --location 'http://127.0.0.1:8000/execute_task' \\\n--header 'Content-Type: application/json' \\\n--data '{\n    \"command\": \"go to espn, look for soccer news, report the names of the most recent soccer champs\",\n    \"llm_config\":{\"planner_agent\":{...}, \"browser_nav_agent\":{...}}\n}'\n```\n### Customizing LLM Parameters\nAgent-E supports advanced LLM configurations using environment variables or JSON-based configuration files. This allows users to customize how the underlying model behaves, such as setting temperature, top-p, and model API base URLs.\n\nTo configure Agent-E using a JSON file, add the following to your `.env` file:\n```makefile\nAGENTS_LLM_CONFIG_FILE=agents_llm_config.json\nAGENTS_LLM_CONFIG_FILE_REF_KEY=openai_gpt\n```\nA sample JSON config file is provided in the project root: `agents_llm_config-example.json`.\n\n\n#### Default Values for LLM Parameters\nIf you do not set `temperature`, `top_p`, or `seed` in your `.env` file or JSON configuration, Agent-E will use the following default values:\n- For `gpt-*` models:\n    - `\"temperature\": 0.0`\n    - `\"top_p\": 0.001`\n    - `\"seed\": 12345`\n- For other models:\n    -  `\"temperature\": 0.1`\n    - `\"top_p\": 0.1`\n\n## Open-source Models\n\nAgent-E supports the use of open-source models through LiteLLM and Ollama. This allows users to run language models locally on their machines, with LiteLLM translating OpenAI-format inputs to local models' endpoints.\n\n### Steps to Use Open-source Models:\n\n1. **Install LiteLLM**:\n    ```bash\n    pip install 'litellm[proxy]'\n    ```\n\n2. **Install Ollama**:\n    - For Mac and Windows, download [Ollama](https://ollama.com/download).\n    - For Linux:\n        ```bash\n        curl -fsSL https://ollama.com/install.sh | sh\n        ```\n\n3. **Pull Ollama Models**:\n    Before using a model, download it from the library. The list of available models is [here](https://ollama.com/library). For example, to pull the Mistral v0.3 model:\n    ```bash\n    ollama pull mistral:v0.3\n    ```\n\n4. **Run LiteLLM**:\n    Start the LiteLLM proxy using the downloaded model:\n    ```bash\n    litellm --model ollama_chat/mistral:v0.3\n    ```\n\n5. **Configure Model in AutoGen**:\n    Modify your `.env` file as follows. No model name or API keys are required since the model is running locally.\n    ```bash\n    AUTOGEN_MODEL_NAME=NotRequired\n    AUTOGEN_MODEL_API_KEY=NotRequired\n    AUTOGEN_MODEL_BASE_URL=http://0.0.0.0:400\n    ```\n\n### Notes:\n- Running local Large Language Models (LLMs) with Agent-E is possible, but has not been thoroughly tested. Use this feature with caution.\n\n\n## Troubleshooting\n\nBelow are some common issues you may encounter when setting up or running Agent-E, along with steps to resolve them.\n\n### 1. pip not installed in the virtual environment\n\nIf you encounter an issue where `pip` is not installed in the virtual environment after setup, follow these steps:\n\n1. Activate the virtual environment:\n   ```bash\n   source .venv/bin/activate  # On Windows: .venv\\Scripts\\activate\n\n2. Install `pip`:\n```bash\npython -m ensurepip --upgrade\n```\n\n3. Deactivate the virtual environment:\n```bash\ndeactivate\n```\n\n4. Reactivate the virtual environment:\n```bash\nsource .venv/bin/activate  # On Windows: .venv\\Scripts\\activate\n```\n\n5. Check for `pip` in the `.venv/bin` directory. You should now have `pip` installed.\n\n### 2. BlockingIOError on macOS\nIf you are on `macOS` and encounter the following error:\n```\nBlockingIOError: [Errno 35] write could not complete without blocking\n```\nThis happens when AutoGen tries to print large amounts of text to the terminal. To fix this, run the following command with the `-u` flag to make output unbuffered:\n```bash\npython -u -m ae.main\n```\nNote: Using unbuffered output may result in some output not appearing in the terminal.\n\n### 3. Playwright driver issues\nIf you do not have Google Chrome installed locally and run into issues with browser automation, install the Playwright drivers:\n```bash\nplaywright install\n```\nPlaywright will install the necessary browser binaries to run the automation tasks without needing Chrome locally installed.\n\n### 4. Chrome profile not found\nIf you want to use your local Chrome browser instead of Playwright and encounter issues finding the browser profile path, follow these steps:\n\n1. Open Chrome and go to `chrome://version/`.\n2. Locate the `Profile Path`.\n3. Set the `BROWSER_STORAGE_DIR` environment variable in the `.env` file to this path:\n```\nBROWSER_STORAGE_DIR=/path/to/your/chrome/profile\n```\n\nIf you encounter other issues, please refer to the project’s [GitHub issues](https://github.com/EmergenceAI/Agent-E/issues) or reach out on [Discord](https://discord.gg/wgNfmFuqJF) for assistance.\n\n\n## Demos\n\n| Video | Command | Description |\n|-----------|-------------|-------------|\n| [![Oppenheimer Video](docs/images/play-video-on-youtube-thumbnail.png)](https://www.youtube.com/embed/v4BgYiDHNZs) | There is an Oppenheimer video on youtube by Veritasium, can you find it and play it? | <ul> <li>Navigates to www.youtube.com </li> <li>Searches for Oppenheimer Veritasium using the searchbar </li> <li> Plays the correct video </li></ul>|\n| [![Example 2: Use information to fill forms](docs/images/form-filling-thumbnail.png)](https://www.youtube.com/embed/uyE7tfKkB0E) | Can you do this task? Wait for me to review before submitting. | Takes the highlighted text from the email as part of the instruction. <ul> <li>Navigates to the form URL </li> <li>Identifies elements in the form to fill </li> <li> Fills the form using information from memory defined in user preferences.txt </li> <li>Waits for user to review before submitting the form </li> |\n| [![Example 3: Find and add specific product to amazon cart](docs/images/amazon-add-to-cart-thumbnail.png)](https://www.youtube.com/embed/CiKZwU_F6TQ) | Find Finish dishwasher detergent tablets on amazon, sort by best seller and add the first one to my cart | <ul> <li> Navigates to www.amazon.com </li> <li>Searches for Finish dishwasher detergent tablets using amazon search feature </li> <li> Sorts the search results by best seller </li> <li>Selects the first product to navigate to the the product page of the first product. </li> <li> Adds the product to cart </li></ul> |\n| [![Example 4: Compare flight prices on Google Flights](docs/images/compare-flights-thumbnail.png)](https://www.youtube.com/embed/JDtnMx0pTmQ) | Compare business class flight options from Lisbon to Singapore for a one-way trip on September 15, 2024 on Google Flights? | <ul><li>  </li> <li> Sets Journey type to one-way. </li> <li> Sets number of passengers to one. </li> <li> Sets departure date to 15 September </li> <li> Sets date to September 15 2024 </li> <li> Sets ticket type to business class </li> <li> Executes search </li> <li> Sets departure date to 15 September </li> Extracts flight information</ul>|\n\n\n\n## Architecture\n\n![Agent-E system view](docs/images/agent-e-system-architecture.png?raw=true \"Agent-E system view\")\n\nBuilding on the foundation provided by the [AG2 agent framework](https://docs.ag2.ai/docs/Home) (Formerly AutoGen), Agent-E's architecture leverages the interplay between skills and agents. Each skill embodies an atomic action, a fundamental building block that, when executed, returns a natural language description of its outcome. This granularity allows Agent-E to flexibly assemble these skills to tackle complex web automation workflows.\n\n![Agent-E AutoGen setup](docs/images/agent-e-autogen-setup.png?raw=true \"Agent-E AutoGen setup\")\n\nThe diagram above shows the configuration chosen on top of AutoGen. The skills can be partitioned differently, but this is the one that we chose for the time being. We chose to use skills that map to what humans learn about the web browser rather than allow the LLM to write code as it pleases. We see the use of configured skills to be safer and more predictable in its outcomes. Certainly it can click on the wrong things, but at least it is not going to execute malicious unknown code.\n\n### Agents\nAt the moment there are two agents, the User proxy (executes the skills), and Browser navigation. Browser navigation agent embodies all the skills for interacting with the web browser.\n\n### Skills Library\nAt the core of Agent-E's capabilities is the Skills Library, a repository of well-defined actions that the agent can perform; for now web actions. These skills are grouped into two main categories:\n\n- **Sensing Skills**: Skills like `get_dom_with_content_type` and `geturl` that help the agent understand the current state of the webpage or the browser.\n- **Action Skills**: Skills that allow the agent to interact with and manipulate the web environment, such as `click`, `enter text`, and `open url`.\n\nEach skill is created with the intention to be as conversational as possible, making the interactions with LLMs more intuitive and error-tolerant. For instance, rather than simply returning a boolean value, a skill might explain in natural language what happened during its execution, enabling the LLM to better understand the context and correct course if necessary.\n\nBelow are the skills we have implemented:\n\n| Sensing Skills | Action Skills |\n|----------------|---------------|\n| `geturl` - Fetches and returns the current url. | `click` - given a DOM query selector, this will click on it. |\n| `get_dom_with_content_type` - Retrieves the HTML DOM of the active page based on the specified content type. Content type can be:<br> - `text_only`: Extracts the inner text of the html DOM. Responds with text output.<br> - `input_fields`: Extracts the interactive elements in the DOM (button, input, textarea, etc.) and responds with a compact JSON object.<br> - `all_fields`: Extracts all the fields in the DOM and responds with a compact JSON object. | `enter_text_and_click` - Optimized method that combines enter text and click skills. The optimization here helps use cases such as enter text in a field and press the search button. Since the DOM would not have changed or changes should be immaterial to this action, identifying both selectors for an input field and an actionable button can happen based on the same DOM examination. |\n| `get_user_input` - Provides the orchestrator with a mechanism to receive user feedback to disambiguate or seek clarity on fulfilling their request. | `bulk_enter_text` - Optimized method that wraps enter_text method so that multiple text entries can be performed one shot. |\n|  | `enter_text` - Enters text in a field specified by the provided DOM query selector. |\n|  | `openurl` - Opens the given URL in current or new tab. |\n\n\n### DOM Distillation\n\nAgent-E's approach to managing the vast landscape of HTML DOM is methodical and, frankly, essential for efficiency. We've introduced DOM Distillation to pare down the DOM to just the elements pertinent to the user's task.\n\nIn practice, this means taking the expansive DOM and delivering a more digestible JSON snapshot. This isn't about just reducing size, it's about honing in on relevance, serving the LLMs only what's necessary to fulfill a request. So far we have three content types:\n\n- **Text only**: For when the mission is information retrieval, and the text is the target. No distractions.\n- **Input fields**: Zeroing in on elements that call for user interaction. It’s about streamlining actions.\n- **All content**: The full scope of distilled DOM, encompassing all elements when the task demands a comprehensive understanding.\n\nIt's a surgical procedure, carefully removing extraneous information while preserving the structure and content needed for the agent’s operation. Of course with any distillation there could be casualties, but the idea is to refine this over time to limit/eliminate them.\n\nSince we can't rely on all web page authors to use best practices, such as adding unique ids to each HTML element, we had to inject our own attribute (`mmid`) in every DOM element. We can then guide the LLM to rely on using `mmid` in the generated DOM queries.\n\nTo cutdown on some of the DOM noise, we use the DOM Accessibility Tree rather than the regular HTML DOM. The accessibility tree by nature is geared towards helping screen readers, which is closer to the mission of web automation than plain old HTML DOM.\n\nThe distillation process is a work in progress. We look to refine this process and condense the DOM further aiming to make interactions faster, cost-effective, and more accurate.\n\n## Testing and Benchmarking\n\nAgent-E builds on the work done by [Web Arena](https://github.com/web-arena-x/webarena) for testing and evaluation. The `test` directory contains a `tasks` subdirectory with JSON files that define test cases, which also serve as examples.\n\nAgent-E operates in a real-world web environment, which introduces variability in testing. As a result, not all tests may pass consistently due to changes in live websites. The goal is to ensure Agent-E works as expected across a wide range of tasks, with a focus on practical web automation.\n\n### Running Tests\n\nTo run the full test suite, use the following command:\n\n```bash\npython -m test.run_tests\n```\n\n### macOS Users\nIf you're running the tests on macOS and encounter `BlockingIOError`, run the tests with unbuffered output:\n```bash\npython -u -m test.run_tests\n```\n\n### Running Specific Tests\nIf you want to run specific tests, you can modify the minimum and maximum task indices. This will run a subset of the tasks defined in the test configuration file.\n\nExample:\n```bash\npython -m test.run_tests --min_task_index 0 --max_task_index 28 --test_results_id first_28_tests\n```\nThis command will run tests from index 0 to 27 and assign the results the identifier `first_28_tests`.\n\n### Parameters for run_tests\nHere are additional parameters that you can pass to customize the test execution:\n- `--min_task_index`: Minimum task index to start tests from (default: 0).\n- `--max_task_index`: Maximum task index to end tests with, non-inclusive.\n- `--test_results_id`: A unique identifier for the test results. If not provided, a timestamp is used.\n- `--test_config_file`: Path to the test configuration file. Default is `test/tasks/test.json`.\n- `--wait_time_non_headless`: The amount of time to wait between headless tests.\n- `--take_screenshots`: Takes screenshots after every operation performed. Example: `--take_screenshots` `true`. Default is `false`\n\n### Example Command\nHere’s an example of how to use the parameters (macOS Users add `-u` parameter to the command below):\n```bash\npython -m test.run_tests --min_task_index 0 --max_task_index 28 --test_results_id first_28_tests\n```\n\n\n## Contributing\n\nThank you for your interest in contributing to Agent-E! We welcome contributions from the community and appreciate your help in improving the project.\n\n### How to Contribute:\n\n1. **Fork the Repository**  \n   Start by forking the [Agent-E repository](https://github.com/EmergenceAI/Agent-E.git) to your GitHub account.\n\n2. **Create a New Branch**  \n   Create a new branch for your feature or bug fix:\n   ```bash\n   git checkout -b my-feature-branch\n   ```\n\n3. **Make Changes**\nImplement your changes in your new branch. Be sure to follow the project's coding style and best practices.\n\n4. **Run Tests**\nBefore submitting your pull request, ensure that all tests pass by running:\n```bash\npython -m test.run_tests\n```\n\n5. **Submit a Pull Request**\nOnce your changes are ready, push your branch to your GitHub fork and submit a pull request to the main repository. Please include a clear description of your changes and why they are necessary.\n\n### Contribution Guidelines:\n- Follow the [contributing guidelines](CONTRIBUTING.md) for more detailed information on contributing.\n- Be sure to write clear and concise commit messages.\n- When submitting a pull request, make sure to link any related issues and provide a detailed description of the changes.\n\n\n### Code of Conduct:\nPlease note that we have a [Code of Conduct](CODE_OF_CONDUCT.md) that all contributors are expected to follow. We are committed to providing a welcoming and inclusive environment for everyone.\n\n### Reporting Issues:\nIf you encounter a bug or have a feature request, please open an issue in the [GitHub issue tracker](https://github.com/EmergenceAI/Agent-E/issues). Be sure to provide detailed information so we can address the issue effectively.\n\n### Join the Discussion:\nWe encourage you to join our community on [Discord](https://discord.gg/wgNfmFuqJF) for discussions, questions, and updates on Agent-E.\n\n\n## Docs Generation\n\nAgent-E uses [Sphinx](https://www.sphinx-doc.org/en/master/) to generate its documentation. To contribute or generate documentation locally, follow these steps:\n\n### Prerequisites\n\nEnsure that you have installed the development dependencies before generating the docs. You can install them using the following command:\n\n```bash\nuv pip install -r pyproject.toml --extra dev\n```\n\n### Steps to Generate Documentation\n1. Navigate to the project root directory:\n```bash\ncd Agent-E\n```\n\n2. Create a `docs` directory if it doesn’t exist:\n```bash\nmkdir docs\ncd docs\n```\n\n3. Initialize Sphinx using the quickstart command:\n```bash\nsphinx-quickstart\n```\n\n4. Configure Sphinx by editing the `docs/conf.py` file. Add the following lines to include the project in the Sphinx path and enable extensions:\n```python\nimport os\nimport sys\nsys.path.insert(0, os.path.abspath('..'))\nextensions = ['sphinx.ext.autodoc', 'sphinx.ext.napoleon']\nhtml_theme = 'sphinx_rtd_theme'\n```\n\n5. Generate API Documentation:\nFrom the project root, run the following command to generate API documentation files:\n```bash\nsphinx-apidoc -o docs/source .\n```\n\n6. Build the Documentation:\nAfter generating the API documentation, go to the `docs` directory and build the HTML docs:\n```bash\nsphinx-build -b html . _build\n```\n\n### Viewing the Documentation\nOnce the documentation is built, open the generated HTML files in your browser by navigating to the `_build` directory and opening `index.html`.\n```bash\nopen _build/index.html\n```\n\nThis will display the generated documentation in your default web browser.\n\n## Join the Community\n\nWe encourage you to become part of the Agent-E community! Whether you're here to ask questions, share feedback, or contribute to the project, we welcome all participation.\n\n### Join the Conversation:\n\n- **Discord**: Connect with other users and developers in our [Discord community](https://discord.gg/wgNfmFuqJF). Feel free to ask questions, share your experiences, or discuss potential features with fellow users and contributors.\n\n### Stay Updated:\n\nStay informed about new features, updates, and announcements by following the project and engaging with the community.\n\n- **GitHub**: Keep an eye on the latest issues and pull requests, and contribute directly to the codebase on [GitHub](https://github.com/EmergenceAI/Agent-E).\n\nWe look forward to seeing you in the community!\n\n\n## Citation\n\nIf you use this work in your research or projects, please cite the following article:\n\n```\n@misc{abuelsaad2024-agente,\n      title={Agent-E: From Autonomous Web Navigation to Foundational Design Principles in Agentic Systems},\n      author={Tamer Abuelsaad and Deepak Akkil and Prasenjit Dey and Ashish Jagmohan and Aditya Vempaty and Ravi Kokku},\n      year={2024},\n      eprint={2407.13032},\n      archivePrefix={arXiv},\n      primaryClass={cs.AI},\n      url={https://arxiv.org/abs/2407.13032},\n}\n```\nYou can also view the paper on [arXiv](https://arxiv.org/abs/2407.13032).\n\n\n## TODO\n\nHere are some features and improvements planned for future releases of Agent-E:\n\n- **Robust Dropdown Handling**: Improve handling for dropdowns on sites like travel booking platforms (e.g., Booking.com, Expedia, Google Flights). Many of these menus have dynamic content or combine multiple aspects, such as selecting both departure and return dates within the same menu.\n- **Task Caching**: Implement caching for tasks that have been run before, allowing users to rerun tasks without requiring new LLM calls. This cache should be smart, selectively caching elements like DOM locators while excluding items such as information retrieval results to improve token efficiency.\n- **Open Source and Local LLM Compatibility**: Adapt Agent-E to work with open-source LLMs, ideally allowing it to run locally. This may involve simplifying prompts or refactoring skills to match the capabilities of these models.\n- **Multi-Tab and Bookmark Handling**: Add skills to support bookmarks and multi-tab usage. Currently, Agent-E can handle only one tab at a time, losing state if another tab is opened, requiring the user to restart.\n- **PDF Text Handling**: Enhance support for managing large amounts of text from PDFs that exceed LLM context windows, possibly by chunking the text with some overlap to retain context.\n- **Browser Nav Agent History Optimization**: Improve the Browser Navigation Agent by developing ways to trim browser history, aiming to reduce token usage and cognitive load on the LLM.\n- **Harvest User Preferences**: Integrate Long-Term Memory (LTM) support to automatically populate user preferences over time, with options for users to manually inject preferences. This may involve using a vector database like FAISS locally or an external hosted vector database.\n- **DOM Distillation Testing Harness**: Develop a testing harness for DOM distillation, allowing distillation improvements to be measured for accuracy and performance improvements.\n- **DOM Distillation Optimization**: Continue to make DOM distillation faster and more efficient.\n- **Shadow DOM Support**: Some sites use Shadow DOMs, support extracting its content from Accessibility Tree.\n- **Google Suite Compatibility**: Add support for Google Docs, Sheets, Slides, and Gmail, which often use canvas elements inaccessible via conventional DOM methods.\n- **Cross-Platform Installer**: Create an installer compatible with Windows, Mac, and ideally Ubuntu, aimed at non-technical users. This installer should allow for environment variable configuration within the app.\n- **Execution Process Video Recording**: Implement video recording to capture the execution process, as requested in issue [#106](https://github.com/EmergenceAI/Agent-E/issues/106).\n- **DOM Distillation Optimizations**: Replace deprecated `snapshot()` method for DOM distillation.\n- **Token Optimization**: Investigate ways to reduce the number of tokens used by the AutoGen-required prompts and annotations.\n- ~~**Action Verification**: Implement response for every skill that reflects DOM changes (using Mutation Observers), so the LLM can judge if the skill executed properly.~~\n- ~~**Execution Planner**: Develop a planner agent that can make the LLM decide on multiple steps ahead for faster execution.~~\n- ~~[Nested chat did the trick]**Group Chat**: Enable group chat features and move some skills to different agents.~~\n\n\n"
    },
    {
      "name": "victordibia/autogen-ui",
      "stars": 905,
      "img": "https://avatars.githubusercontent.com/u/1547007?s=40&v=4",
      "owner": "victordibia",
      "repo_name": "autogen-ui",
      "description": "Web UI for AutoGen (A Framework Multi-Agent LLM Applications)",
      "homepage": "",
      "language": "TypeScript",
      "created_at": "2023-10-05T23:28:44Z",
      "updated_at": "2025-04-23T03:33:16Z",
      "topics": [
        "agent-based-framework",
        "ai",
        "ai-agents",
        "autogen",
        "autogen-sample",
        "chatgpt",
        "deep-learning",
        "visualization"
      ],
      "readme": "# AutoGen UI\n\n![AutoGen UI Screenshot](docs/images/autogenuiscreen.png)\n\n> [!IMPORTANT]  \n> This repo has been updated to use the [AutoGen AgentChat](https://microsoft.github.io/autogen/dev/user-guide/agentchat-user-guide/quickstart.html) interface based on the new AutoGen 0.4x AgentChat api. Also, the API might change, so expect some breaking changes in the future.\n\nThe hello world for building a UI interface with AutoGen AgentChat API.\n\nExample UI to demonstrate how to build interfaces using the [AutoGen AgentChat](https://github.com/microsoft/autogen) API. The UI is built using Next.js and web apis built using FastApi.\n\n## What Does the App Do?\n\n![AutoGen UI Flow Diagram](docs/images/flowdiag.png)\n\n- [`autogenui.manager`](autogenui/manager.py) - provides a simple run method that takes a prompt and returns a response from a predefined [agent team](notebooks/default_team.json). Take a look at the [agent team](notebooks/default_team.json) json config file to see how the agents are configured. It gives a general idea on what types of agents are supported etc. Check out the [tutorial notebook](notebooks/tutorial.ipynb) for an example on how to use the provide class to load a team spec.\n\n- [`autogenui.web.app.py`](autogenui/web/app.py) - FastApi backend that serves a simple `/generate` endpoint that takes a prompt and returns a response from a predefined [agent team](notebooks/default_team.json).\n\n  - Creates a manager to run tasks\n  - Streams results of the task run to the client ui\n\n- [`frontend`](frontend) - Next.js frontend that provides a simple chat interface to interact with the backend.\n\n## What's Next?\n\nThis app is clearly just a starting point. Here are some ideas on how to extend it:\n\n- Extend the manager to support multiple team configurations from the UI\n- Storing and loading interaction history in a database.\n- Security - add authentication and authorization to the app\n\n> [!TIP] Note\n> [AutoGen Studio](https://github.com/microsoft/autogen/tree/main/python/packages/autogen-studio) is being rewritten on the AgentChat api to address most of the above points. Take a look at the implementation there for a more complete example.\n\n## Getting Started\n\nNote that you will have to setup your OPENAI_API_KEY to run the app.\n\n```bash\nexport OPENAI_API_KEY=<your key>\n```\n\nInstall dependencies. Python 3.9+ is required. You can install from pypi using pip.\n\n```bash\npip install autogenui\n```\n\nor to install from source\n\n```bash\ngit clone git@github.com:victordibia/autogen-ui.git\ncd autogenui\npip install -e .\n```\n\nRun ui server.\n\nSet env vars `OPENAI_API_KEY`\n\n```bash\nexport OPENAI_API_KEY=<your_key>\n```\n\n```bash\nautogenui # or with --port 8081\n```\n\nOpen http://localhost:8081 in your browser.\n\nTo modify the source files, make changes in the frontend source files and run `npm run build` to rebuild the frontend.\n\n## Development\n\nTo run the app in development mode, you will need to run the backend and frontend separately.\n\n## Backend - with hot-reload\n\n```bash\nautogenui --reload\n```\n\n> [!TIP] Tip\n> The UI loaded by this CLI in a pre-complied version by running the frontend build command show blow. That means if you make changes the frontend code or change the hostname or port the backend is running on the frontend updated frontend code needs to be rebuilt for it to load through this command.\n\n## Frontend\n\n```bash\ncd frontend\n```\n\nInstall dependencies\n\n```bash\nyarn install\n```\n\nRun in dev mode - with hot-reload\n\nSet `NEXT_PUBLIC_API_SERVER` on the command line.\n\n```bash\nexport NEXT_PUBLIC_API_SERVER=http://<your_backend_hostname>/api\n```\n\nOr create a `.env` file in the frontend folder with the following content.\n\n```bash\nNEXT_PUBLIC_API_SERVER=http://<your_backend_hostname>/api\n```\n\nwhere your_backend_hostname - is the hostname that autogenui is running on e.g. `localhost:8081`\n\n```bash\nyarn dev\n```\n\n(Re)build\n\nRemember to install dependencies and set `NEXT_PUBLIC_API_SERVER` before building.\n\n```bash\nyarn build\n```\n\n## Roadmap\n\nThere isnt really much of a roadmap for this project. It is meant as a simple example to get started with the AutoGen AgentChat API. For a more complete example, take a look at the [AutoGen Studio](https://github.com/microsoft/autogen/tree/main/python/packages/autogen-studio) project.\n\n## References\n\n- [AutoGen Studio](https://arxiv.org/abs/2308.08155).\n\n```\n@inproceedings{dibia2024autogen,\n  title={AutoGen Studio: A No-Code Developer Tool for Building and Debugging Multi-Agent Systems},\n  author={Dibia, Victor and Chen, Jingya and Bansal, Gagan and Syed, Suff and Fourney, Adam and Zhu, Erkang and Wang, Chi and Amershi, Saleema},\n  journal={EMNLP 2024},\n  year={2024}\n}\n```\n\n- [AutoGen](https://arxiv.org/abs/2408.15247).\n\n```\n@inproceedings{wu2023autogen,\n      title={AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation Framework},\n      author={Qingyun Wu and Gagan Bansal and Jieyu Zhang and Yiran Wu and Shaokun Zhang and Erkang Zhu and Beibin Li and Li Jiang and Xiaoyun Zhang and Chi Wang},\n      year={2023},\n      eprint={2308.08155},\n      archivePrefix={arXiv},\n      primaryClass={cs.AI}\n}\n```\n"
    },
    {
      "name": "UKGovernmentBEIS/inspect_ai",
      "stars": 901,
      "img": "https://avatars.githubusercontent.com/u/19221939?s=40&v=4",
      "owner": "UKGovernmentBEIS",
      "repo_name": "inspect_ai",
      "description": "Inspect: A framework for large language model evaluations",
      "homepage": "https://inspect.aisi.org.uk/",
      "language": "Python",
      "created_at": "2023-11-14T14:53:11Z",
      "updated_at": "2025-04-23T07:43:53Z",
      "topics": [],
      "readme": "[<img width=\"295\" src=\"https://inspect.aisi.org.uk/images/aisi-logo.svg\" />](https://aisi.gov.uk/)\n\nWelcome to Inspect, a framework for large language model evaluations created by the [UK AI Security Institute](https://aisi.gov.uk/).\n\nInspect provides many built-in components, including facilities for prompt engineering, tool usage, multi-turn dialog, and model graded evaluations. Extensions to Inspect (e.g. to support new elicitation and scoring techniques) can be provided by other Python packages.\n\nTo get started with Inspect, please see the documentation at <https://inspect.aisi.org.uk/>.\n\n***\n\nTo work on development of Inspect, clone the repository and install with the `-e` flag and `[dev]` optional dependencies:\n\n```bash\ngit clone https://github.com/UKGovernmentBEIS/inspect_ai.git\ncd inspect_ai\npip install -e \".[dev]\"\n```\n\nOptionally install pre-commit hooks via\n\n```bash\nmake hooks\n```\n\nRun linting, formatting, and tests via\n\n```bash\nmake check\nmake test\n```\n\nIf you use VS Code, you should be sure to have installed the recommended extensions (Python, Ruff, and MyPy). Note that you'll be prompted to install these when you open the project in VS Code.\n"
    },
    {
      "name": "OWASP/www-project-top-10-for-large-language-model-applications",
      "stars": 701,
      "img": "https://avatars.githubusercontent.com/u/155815?s=40&v=4",
      "owner": "OWASP",
      "repo_name": "www-project-top-10-for-large-language-model-applications",
      "description": "OWASP Foundation Web Respository",
      "homepage": null,
      "language": "TeX",
      "created_at": "2023-05-16T15:12:44Z",
      "updated_at": "2025-04-23T07:36:27Z",
      "topics": [],
      "readme": "[![pages-build-deployment](https://github.com/OWASP/www-project-top-10-for-large-language-model-applications/actions/workflows/pages/pages-build-deployment/badge.svg?branch=main)](https://github.com/OWASP/www-project-top-10-for-large-language-model-applications/actions/workflows/pages/pages-build-deployment)\n\n# www-project-top-10-for-large-language-model-applications\nOWASP Foundation Web Repository\n\n# OWASP Top 10 for Large Language Model Applications\n\n[![Current version in-flight](https://img.shields.io/badge/current_version-v2.0-purple)](https://www.linkedin.com/posts/wilsonsd_announcing-the-version-2-project-its-time-activity-7157734167244378113-s2v2?utm_source=share&utm_medium=member_ios)\n[![OWASP Lab Status project](https://img.shields.io/badge/owasp-labstatus-blue.svg)](https://owasp.org/projects/)\n[![License: CC BY-SA 4.0](https://img.shields.io/badge/License-CC%20BY--SA%204.0-lightgrey.svg)](https://creativecommons.org/licenses/by-sa/4.0/)\n[![llmtop10.com](https://img.shields.io/badge/officialsite-llmtop10.com-032CFA.svg)](https://llmtop10.com)\n\nWelcome to the official repository for the OWASP Top 10 for Large Language Model Applications!\n\n## Overview and Audience 🗣️\n\nThe OWASP Top 10 for Large Language Model Applications is a standard awareness document for developers and web application security. It represents a broad consensus about the most critical security risks to Large Language Model (LLM) applications. There are other ongoing frameworks both inside and outside of OWASP that are not to be confused with this project and is currently scoped towards only LLM Application Security.\n\nOur primary audience is developers, data scientists, and security experts tasked with designing and building applications and plugins leveraging LLM technologies. We aim to provide practical, actionable, and concise security guidance to help these professionals navigate the complex and evolving terrain of LLM application security.\n\n## Key Focus 📖\n\nThe primary aim of this project is to provide a comprehensible and adoptable guide to navigate the potential security risks in LLM applications. Our Top 10 list serves as a starting point for developers and security professionals who are new to this domain, and as a reference for those who are more experienced.\n\n## Mission Statement 🚀\n\nOur mission is to make application security visible, so that people and organizations can make informed decisions about application security risks related to LLMs. While our list shares DNA with vulnerability types found in other OWASP Top 10 lists, we do not simply reiterate these vulnerabilities. Instead, we delve into these vulnerabilities’ unique implications when encountered in applications utilizing LLMs.\n\nOur goal is to bridge the divide between general application security principles and the specific challenges posed by LLMs. The group’s goals include exploring how conventional vulnerabilities may pose different risks or be exploited in novel ways within LLMs and how developers must adapt traditional remediation strategies for applications utilizing LLMs.\n\n## Contribution 👋\n\nThe first version of this list was contributed by Steve Wilson of Contrast Security.  We encourage the community to contribute and help improve the project. If you have any suggestions, feedback or want to help improve the list, feel free to open an issue or send a pull request.\n\nWe have a working group channel on the [OWASP Slack](https://owasp.org/slack/invite), so please sign up and then join us on the #project-top10-llm channel.\n\nPlease hop over to [our wiki page](https://github.com/OWASP/www-project-top-10-for-large-language-model-applications/wiki) to collaborate on the project and stay up to date with the latest meetings and current roadmap.\n\n## License\n\nThis project is licensed under the terms of the [Creative Commons Attribution-ShareAlike 4.0 International License](https://creativecommons.org/licenses/by-sa/4.0/).\n\n## Support OWASP!\n\n<picture>\n  <source\n    media=\"(prefers-color-scheme: dark)\"\n    srcset=\"\n      https://api.star-history.com/svg?repos=OWASP/www-project-top-10-for-large-language-model-applications&type=Date&theme=dark\n    \"\n  />\n  <source\n    media=\"(prefers-color-scheme: light)\"\n    srcset=\"\n      https://api.star-history.com/svg?repos=OWASP/www-project-top-10-for-large-language-model-applications&type=Date\n    \"\n  />\n  <img\n    alt=\"Star History Chart\"\n    src=\"https://api.star-history.com/svg?repos=OWASP/www-project-top-10-for-large-language-model-applications&type=Date\"\n  />\n</picture>"
    },
    {
      "name": "kagent-dev/kagent",
      "stars": 588,
      "img": "https://avatars.githubusercontent.com/u/195942545?s=40&v=4",
      "owner": "kagent-dev",
      "repo_name": "kagent",
      "description": "Cloud Native Agentic AI | Discord: https://bit.ly/kagentdiscord",
      "homepage": "https://kagent.dev",
      "language": "Python",
      "created_at": "2025-01-21T17:03:23Z",
      "updated_at": "2025-04-23T08:36:35Z",
      "topics": [
        "agents",
        "ai",
        "cncf",
        "devops",
        "mcp"
      ],
      "readme": "<div align=\"center\">\n  <picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://raw.githubusercontent.com/kagent-dev/kagent/main/img/icon-dark.svg\" alt=\"kagent\" width=\"400\">\n    <source media=\"(prefers-color-scheme: light)\" srcset=\"https://raw.githubusercontent.com/kagent-dev/kagent/main/img/icon-light.svg\" alt=\"kagent\" width=\"400\">\n    <img alt=\"kagent\" src=\"https://raw.githubusercontent.com/kagent-dev/kagent/main/img/icon-light.svg\">\n  </picture>\n  <div>\n     <a href=\"https://discord.gg/Fu3k65f2k3\">\n      <img src=\"https://img.shields.io/discord/1346225185166065826?style=flat&label=Join%20Discord&color=6D28D9\" alt=\"Discord\">\n    </a>\n  </div>\n</div>\n\n---\n\n**kagent** is a Kubernetes native framework for building AI agents. Kubernetes is the most popular orchestration platform for running workloads, and **kagent** makes it easy to build, deploy and manage AI agents in kubernetes. The **kagent** framework is designed to be easy to understand and use, and to provide a flexible and powerful way to build and manage AI agents.\n\n<div align=\"center\">\n  <img src=\"img/hero.png\" alt=\"Autogen Framework\" width=\"500\">\n</div>\n\n\n## Get started\n\n- [Quick Start](https://kagent.dev/docs/getting-started/quickstart)\n- [Installation guide](https://kagent.dev/docs/introduction/installation)\n\n\n## Documentation\n\nThe kagent documentation is available at [kagent.dev/docs](https://kagent.dev/docs).\n\n## Core Concepts\n\n- **Agents**: Agents are the main building block of kagent. They are a system prompt, a set of tools, and a model configuration.\n- **Tools**: Tools are any external tool that can be used by an agent. They are defined as Kubernetes custom resources and can be used by multiple agents.\n\nAll of the above are defined as Kubernetes custom resources, which makes them easy to manage and modify.\n\n## Core Principles\n\n- **Kubernetes Native**: Kagent is designed to be easy to understand and use, and to provide a flexible and powerful way to build and manage AI agents.\n- **Extensible**: Kagent is designed to be extensible, so you can add your own agents and tools.\n- **Flexible**: Kagent is designed to be flexible, to suit any AI agent use case.\n- **Observable**: Kagent is designed to be observable, so you can monitor the agents and tools using all common monitoring frameworks.\n- **Declarative**: Kagent is designed to be declarative, so you can define the agents and tools in a yaml file.\n- **Testable**: Kagent is designed to be tested and debugged easily. This is especially important for AI agent applications.\n\n## Architecture\n\nThe kagent framework is designed to be easy to understand and use, and to provide a flexible and powerful way to build and manage AI agents.\n\n<div align=\"center\">\n  <img src=\"img/arch.png\" alt=\"Autogen Framework\" width=\"500\">\n</div>\n\nKagent has 4 core components:\n\n- **Controller**: The controller is a Kubernetes controller that watches the kagent custom resources and creates the necessary resources to run the agents.\n- **UI**: The UI is a web UI that allows you to manage the agents and tools.\n- **Engine**: The engine is a Python application that runs the agents and tools. The engine is built using [Autogen](https://github.com/microsoft/autogen).\n- **CLI**: The CLI is a command line tool that allows you to manage the agents and tools.\n\n\n## Roadmap\n\n`kagent` is currently in active development. The following is a list of features that are planned for the next few releases.\n\n- [ ] [Observability improvements:](https://github.com/kagent-dev/kagent/issues/130)\n  - [ ] More powerful Tracing capabilities\n  - [ ] Tighter oTEL integration\n  - [ ] Metrics\n- [ ] [Feedback/Testing:](https://github.com/kagent-dev/kagent/issues/131)\n  - [ ] Eval framework/integrations\n  - [ ] Debugging/Time travel\n  - [ ] Guided Learning\n- [ ] [Runtime/Engine improvements:](https://github.com/kagent-dev/kagent/issues/132)\n  - [ ] Multi-Agent support\n  - [ ] True Graph Execution\n  - [ ] Workflows\n  - [x] Multiple LLM Provider support\n- [ ] [Tools:](https://github.com/kagent-dev/kagent/issues/133)\n  - [x] Tool Discovery\n  - [x] Expose built-in tools as MCP server\n\n## Local development\n\nFor instructions on how to run everything locally, see the [DEVELOPMENT.md](DEVELOPMENT.md) file.\n\n## Contributing\n\nFor instructions on how to contribute to the kagent project, see the [CONTRIBUTION.md](CONTRIBUTION.md) file.\n"
    },
    {
      "name": "microsoft/AIOpsLab",
      "stars": 582,
      "img": "https://avatars.githubusercontent.com/u/6154722?s=40&v=4",
      "owner": "microsoft",
      "repo_name": "AIOpsLab",
      "description": "A holistic framework to enable the design, development, and evaluation of autonomous AIOps agents.",
      "homepage": "",
      "language": "Python",
      "created_at": "2024-10-11T00:44:00Z",
      "updated_at": "2025-04-22T00:04:49Z",
      "topics": [],
      "readme": "<div align=\"center\">\n\n<h1>AIOpsLab</h1>\n\n[🤖Overview](#🤖overview) | \n[🚀Quick Start](#🚀quickstart) | \n[📦Installation](#📦installation) | \n[⚙️Usage](#⚙️usage) | \n[📂Project Structure](#📂project-structure) |\n[📄How to Cite](#📄how-to-cite)\n\n[![ArXiv Link](https://img.shields.io/badge/arXiv-2501.06706-red?logo=arxiv)](https://arxiv.org/pdf/2501.06706)\n[![ArXiv Link](https://img.shields.io/badge/arXiv-2407.12165-red?logo=arxiv)](https://arxiv.org/pdf/2407.12165)\n</div>\n\n\n\n<h2 id=\"🤖overview\">🤖 Overview</h2>\n\n![alt text](./assets/images/aiopslab-arch-open-source.png)\n\n\nAIOpsLab is a holistic framework to enable the design, development, and evaluation of autonomous AIOps agents that, additionally, serve the purpose of building reproducible, standardized, interoperable and scalable benchmarks. AIOpsLab can deploy microservice cloud environments, inject faults, generate workloads, and export telemetry data, while orchestrating these components and providing interfaces for interacting with and evaluating agents. \n\nMoreover, AIOpsLab provides a built-in benchmark suite with a set of problems to evaluate AIOps agents in an interactive environment. This suite can be easily extended to meet user-specific needs. See the problem list [here](/aiopslab/orchestrator/problems/registry.py#L15).\n\n<h2 id=\"📦installation\">📦 Installation</h2>\n\n### Requirements\n- Python >= 3.11\n\nRecommended installation:\n```bash\nsudo apt install python3.11 python3.11-venv python3.11-dev python3-pip # poetry requires python >= 3.11\n```\n\nWe recommend [Poetry](https://python-poetry.org/docs/) for managing dependencies. You can also use a standard `pip install -e .` to install the dependencies.\n\n```bash\ngit clone --recurse-submodules <CLONE_PATH_TO_THE_REPO>\ncd AIOpsLab\npoetry env use python3.11\nexport PATH=\"$HOME/.local/bin:$PATH\" # export poetry to PATH if needed\npoetry install # -vvv for verbose output\npoetry self add poetry-plugin-shell # installs poetry shell plugin\npoetry shell\n```\n\n<h2 id=\"🚀quickstart\">🚀 Quick Start </h2>\n\n<!-- TODO: Add instructions for both local cluster and remote cluster -->\nChoose either a) or b) to set up your cluster and then proceed to the next steps.\n\n### a) Local simulated cluster\nAIOpsLab can be run on a local simulated cluster using [kind](https://kind.sigs.k8s.io/) on your local machine.\n\n```bash\n# For x86 machines\nkind create cluster --config kind/kind-config-x86.yaml\n\n# For ARM machines\nkind create cluster --config kind/kind-config-arm.yaml\n```\n\nIf you're running into issues, consider building a Docker image for your machine by following this [README](kind/README.md). Please also open an issue.\n\nAfter finishing cluster creation, proceed to the next \"Update `config.yml`\" step.\n\n### b) Remote cluster\nAIOpsLab supports any remote kubernetes cluster that your `kubectl` context is set to, whether it's a cluster from a cloud provider or one you build yourself. We have some Ansible playbooks we have to setup clusters on providers like [CloudLab](https://www.cloudlab.us/) and our own machines. Follow this [README](./scripts/ansible/README.md) to set up your own cluster, and then proceed to the next \"Update `config.yml`\" step.\n\n### Update `config.yml`\n```bash\ncd aiopslab\ncp config.yml.example config.yml\n```\nUpdate your `config.yml` so that `k8s_host` is the host name of the control plane node of your cluster. Update `k8s_user` to be your username on the control plane node. If you are using a kind cluster, your `k8s_host` should be `kind`. If you're running AIOpsLab on cluster, your `k8s_host` should be `localhost`.\n\n### Running agents\nHuman as the agent:\n\n```bash\npython3 cli.py\n(aiopslab) $ start misconfig_app_hotel_res-detection-1 # or choose any problem you want to solve\n# ... wait for the setup ...\n(aiopslab) $ submit(\"Yes\") # submit solution\n```\n\nRun GPT-4 baseline agent:\n\n```bash\nexport OPENAI_API_KEY=<YOUR_OPENAI_API_KEY>\npython3 clients/gpt.py # you can also change the problem to solve in the main() function\n```\n\nYou can check the running status of the cluster using [k9s](https://k9scli.io/) or other cluster monitoring tools conveniently.\n\n<h2 id=\"⚙️usage\">⚙️ Usage</h2>\n\nAIOpsLab can be used in the following ways:\n- [Onboard your agent to AIOpsLab](#how-to-onboard-your-agent-to-aiopslab)\n- [Add new applications to AIOpsLab](#how-to-add-new-applications-to-aiopslab)\n- [Add new problems to AIOpsLab](#how-to-add-new-problems-to-aiopslab)\n\n\n### How to onboard your agent to AIOpsLab?\n\nAIOpsLab makes it extremely easy to develop and evaluate your agents. You can onboard your agent to AIOpsLab in 3 simple steps:\n\n1. **Create your agent**: You are free to develop agents using any framework of your choice. The only requirements are:\n    - Wrap your agent in a Python class, say `Agent`\n    - Add an async method `get_action` to the class:\n\n        ```python\n        # given current state and returns the agent's action\n        async def get_action(self, state: str) -> str:\n            # <your agent's logic here>\n        ```\n\n2. **Register your agent with AIOpsLab**: You can now register the agent with AIOpsLab's orchestrator. The orchestrator will manage the interaction between your agent and the environment:\n\n    ```python\n    from aiopslab.orchestrator import Orchestrator\n\n    agent = Agent()             # create an instance of your agent\n    orch = Orchestrator()       # get AIOpsLab's orchestrator\n    orch.register_agent(agent)  # register your agent with AIOpsLab\n    ```\n\n3. **Evaluate your agent on a problem**:\n\n    1. **Initialize a problem**: AIOpsLab provides a list of problems that you can evaluate your agent on. Find the list of available problems [here](/aiopslab/orchestrator/problems/registry.py) or using `orch.probs.get_problem_ids()`. Now initialize a problem by its ID: \n\n        ```python\n        problem_desc, instructs, apis = orch.init_problem(\"k8s_target_port-misconfig-mitigation-1\")\n        ```\n    \n    2. **Set agent context**: Use the problem description, instructions, and APIs available to set context for your agent. (*This step depends on your agent's design and is left to the user*)\n\n\n    3. **Start the problem**: Start the problem by calling the `start_problem` method. You can specify the maximum number of steps too:\n\n        ```python\n        import asyncio\n        asyncio.run(orch.start_problem(max_steps=30))\n        ```\n\nThis process will create a [`Session`](/aiopslab/session.py) with the orchestrator, where the agent will solve the problem. The orchestrator will evaluate your agent's solution and provide results (stored under `data/results/`). You can use these to improve your agent.\n\n\n### How to add new applications to AIOpsLab?\n\nAIOpsLab provides a default [list of applications](/aiopslab/service/apps/) to evaluate agents for operations tasks. However, as a developer you can add new applications to AIOpsLab and design problems around them.\n\n> *Note*: for auto-deployment of some apps with K8S, we integrate Helm charts (you can also use `kubectl` to install as [HotelRes application](/aiopslab/service/apps/hotelres.py)). More on Helm [here](https://helm.sh).\n\nTo add a new application to AIOpsLab with Helm, you need to:\n\n1. **Add application metadata**\n    - Application metadata is a JSON object that describes the application.\n    - Include *any* field such as the app's name, desc, namespace, etc.\n    - We recommend also including a special `Helm Config` field, as follows:\n\n        ```json\n        \"Helm Config\": {\n            \"release_name\": \"<name for the Helm release to deploy>\",\n            \"chart_path\": \"<path to the Helm chart of the app>\",\n            \"namespace\": \"<K8S namespace where app should be deployed>\"\n        }\n        ```\n        > *Note*: The `Helm Config` is used by the orchestrator to auto-deploy your app when a problem associated with it is started.\n\n        > *Note*: The orchestrator will auto-provide *all other* fields as context to the agent for any problem associated with this app.\n\n    Create a JSON file with this metadata and save it in the [`metadata`](/aiopslab/service/metadata) directory. For example the `social-network` app: [social-network.json](/aiopslab/service/metadata/social-network.json)\n\n2. **Add application class**\n\n    Extend the base class in a new Python file in the [`apps`](/aiopslab/service/apps) directory:\n\n    ```python\n    from aiopslab.service.apps.base import Application\n\n    class MyApp(Application):\n        def __init__(self):\n            super().__init__(\"<path to app metadata JSON>\")\n    ```\n\n    The `Application` class provides a base implementation for the application. You can override methods as needed and add new ones to suit your application's requirements, but the base class should suffice for most applications.\n\n\n\n### How to add new problems to AIOpsLab?\n\nSimilar to applications, AIOpsLab provides a default [list of problems](/aiopslab/orchestrator/problems/registry.py) to evaluate agents. However, as a developer you can add new problems to AIOpsLab and design them around your applications.\n\nEach problem in AIOpsLab has 5 components:\n1. *Application*: The application on which the problem is based.\n2. *Task*: The AIOps task that the agent needs to perform.\n Currently we support: [Detection](/aiopslab/orchestrator/tasks/detection.py), [Localization](/aiopslab/orchestrator/tasks/localization.py), [Analysis](/aiopslab/orchestrator/tasks/analysis.py), and [Mitigation](/aiopslab/orchestrator/tasks/mitigation.py).\n3. *Fault*: The fault being introduced in the application.\n4. *Workload*: The workload that is generated for the application.\n5. *Evaluator*: The evaluator that checks the agent's performance.\n\nTo add a new problem to AIOpsLab, create a new Python file \nin the [`problems`](/aiopslab/orchestrator/problems) directory, as follows:\n\n1. **Setup**. Import your chosen application (say `MyApp`) and task (say `LocalizationTask`):\n\n    ```python\n    from aiopslab.service.apps.myapp import MyApp\n    from aiopslab.orchestrator.tasks.localization import LocalizationTask\n    ```\n\n2. **Define**. To define a problem, create a class that inherits from your chosen `Task`, and defines 3 methods: `start_workload`, `inject_fault`, and `eval`:\n\n    ```python\n    class MyProblem(LocalizationTask):\n        def __init__(self):\n            self.app = MyApp()\n        \n        def start_workload(self):\n            # <your workload logic here>\n        \n        def inject_fault(self)\n            # <your fault injection logic here>\n        \n        def eval(self, soln, trace, duration):\n            # <your evaluation logic here>\n    ```\n\n3. **Register**. Finally, add your problem to the orchestrator's registry [here](/aiopslab/orchestrator/problems/registry.py).\n\n\nSee a full example of a problem [here](/aiopslab/orchestrator/problems/k8s_target_port_misconfig/target_port.py). \n<details>\n  <summary>Click to show the description of the problem in detail</summary>\n\n- **`start_workload`**: Initiates the application's workload. Use your own generator or AIOpsLab's default, which is based on [wrk2](https://github.com/giltene/wrk2):\n\n    ```python\n    from aiopslab.generator.workload.wrk import Wrk\n\n    wrk = Wrk(rate=100, duration=10)\n    wrk.start_workload(payload=\"<wrk payload script>\", url=\"<app URL>\")\n    ```\n    > Relevant Code: [aiopslab/generators/workload/wrk.py](/aiopslab/generators/workload/wrk.py)\n\n- **`inject_fault`**: Introduces a fault into the application. Use your own injector or AIOpsLab's built-in one which you can also extend. E.g., a misconfig in the K8S layer:\n\n    ```python\n    from aiopslab.generators.fault.inject_virtual import *\n\n    inj = VirtualizationFaultInjector(testbed=\"<namespace>\")\n    inj.inject_fault(microservices=[\"<service-name>\"], fault_type=\"misconfig\")\n    ```\n\n    > Relevant Code: [aiopslab/generators/fault](/aiopslab/generators/fault)\n\n\n- **`eval`**: Evaluates the agent's solution using 3 params: (1) *soln*: agent's submitted solution if any, (2) *trace*: agent's action trace, and (3) *duration*: time taken by the agent.\n\n    Here, you can use built-in default evaluators for each task and/or add custom evaluations. The results are stored in `self.results`:\n    ```python\n    def eval(self, soln, trace, duration) -> dict:\n        super().eval(soln, trace, duration)     # default evaluation\n        self.add_result(\"myMetric\", my_metric(...))     # add custom metric\n        return self.results\n    ```\n\n    > *Note*: When an agent starts a problem, the orchestrator creates a [`Session`](/aiopslab/session.py) object that stores the agent's interaction. The `trace` parameter is this session's recorded trace.\n\n    > Relevant Code: [aiopslab/orchestrator/evaluators/](/aiopslab/orchestrator/evaluators/)\n\n</details>\n\n\n\n\n<h2 id=\"📂project-structure\">📂 Project Structure</h2>\n\n<summary><code>aiopslab</code></summary>\n<details>\n  <summary>Generators</summary>\n  <pre>\n  generators - the problem generators for aiopslab\n  ├── fault - the fault generator organized by fault injection level\n  │   ├── base.py\n  │   ├── inject_app.py\n  │  ...\n  │   └── inject_virtual.py\n  └── workload - the workload generator organized by workload type\n      └── wrk.py - wrk tool interface\n  </pre>\n</details>\n\n<details>\n  <summary>Orchestrator</summary>\n  <pre>\n  orchestrator\n  ├── orchestrator.py - the main orchestration engine\n  ├── parser.py - parser for agent responses\n  ├── evaluators - eval metrics in the system\n  │   ├── prompts.py - prompts for LLM-as-a-Judge\n  │   ├── qualitative.py - qualitative metrics\n  │   └── quantitative.py - quantitative metrics\n  ├── problems - problem definitions in aiopslab\n  │   ├── k8s_target_port_misconfig - e.g., A K8S TargetPort misconfig problem\n  │  ...\n  │   └── registry.py\n  ├── actions - actions that agents can perform organized by AIOps task type\n  │   ├── base.py\n  │   ├── detection.py\n  │   ├── localization.py\n  │   ├── analysis.py\n  │   └── mitigation.py\n  └── tasks - individual AIOps task definition that agents need to solve\n      ├── base.py\n      ├── detection.py\n      ├── localization.py\n      ├── analysis.py\n      └── mitigation.py\n  </pre>\n</details>\n\n<details>\n  <summary>Service</summary>\n  <pre>\n  service\n  ├── apps - interfaces/impl. of each app\n  ├── helm.py - helm interface to interact with the cluster\n  ├── kubectl.py - kubectl interface to interact with the cluster\n  ├── shell.py - shell interface to interact with the cluster\n  ├── metadata - metadata and configs for each apps\n  └── telemetry - observability tools besides observer, e.g., in-memory log telemetry for the agent\n  </pre>\n</details>\n\n<details>\n  <summary>Observer</summary>\n  <pre>\n  observer\n  ├── filebeat - Filebeat installation\n  ├── logstash - Logstash installation\n  ├── prometheus - Prometheus installation\n  ├── log_api.py - API to store the log data on disk\n  ├── metric_api.py - API to store the metrics data on disk\n  └── trace_api.py - API to store the traces data on disk\n  </pre>\n</details>\n\n<details>\n  <summary>Utils</summary>\n  <pre>\n  ├── config.yml - aiopslab configs\n  ├── config.py - config parser\n  ├── paths.py - paths and constants\n  ├── session.py - aiopslab session manager\n  └── utils\n      ├── actions.py - helpers for actions that agents can perform\n      ├── cache.py - cache manager\n      └── status.py - aiopslab status, error, and warnings\n  </pre>\n</details>\n\n<summary><code>cli.py</code>: A command line interface to interact with AIOpsLab, e.g., used by human operators.</summary>\n\n\n<h2 id=\"📄how-to-cite\">📄 How to Cite</h2>\n\n```bibtex\n@misc{chen2024aiopslab,\n  title = {AIOpsLab: A Holistic Framework to Evaluate AI Agents for Enabling Autonomous Clouds},\n  author = {Chen, Yinfang and Shetty, Manish and Somashekar, Gagan and Ma, Minghua and Simmhan, Yogesh and Mace, Jonathan and Bansal, Chetan and Wang, Rujia and Rajmohan, Saravan},\n  year = {2025},\n  url = {https://arxiv.org/abs/2501.06706} \n}\n@inproceedings{shetty2024building,\n  title = {Building AI Agents for Autonomous Clouds: Challenges and Design Principles},\n  author = {Shetty, Manish and Chen, Yinfang and Somashekar, Gagan and Ma, Minghua and Simmhan, Yogesh and Zhang, Xuchao and Mace, Jonathan and Vandevoorde, Dax and Las-Casas, Pedro and Gupta, Shachee Mishra and Nath, Suman and Bansal, Chetan and Rajmohan, Saravan},\n  year = {2024},\n  booktitle = {Proceedings of 15th ACM Symposium on Cloud Computing},\n}\n```\n\n## Code of Conduct\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/). For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n\n## License\n\nCopyright (c) Microsoft Corporation. All rights reserved.\n\nLicensed under the [MIT](LICENSE.txt) license.\n\n\n### Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow [Microsoft’s Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks). Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos is subject to those third-party’s policies.\n"
    },
    {
      "name": "microsoft/Trace",
      "stars": 551,
      "img": "https://avatars.githubusercontent.com/u/6154722?s=40&v=4",
      "owner": "microsoft",
      "repo_name": "Trace",
      "description": "End-to-end Generative Optimization for AI Agents",
      "homepage": "https://microsoft.github.io/Trace/",
      "language": "Python",
      "created_at": "2024-06-14T18:21:58Z",
      "updated_at": "2025-04-23T00:39:40Z",
      "topics": [
        "agentic-agi",
        "agentic-workflow",
        "agents",
        "ai",
        "autodiff",
        "compound-systems",
        "end-to-end",
        "generative-optimization",
        "large-language-models",
        "llm",
        "machine-learning",
        "optimization",
        "optimizer",
        "prompt-optimization",
        "python"
      ],
      "readme": "<p >\n    <img src=\"https://github.com/microsoft/Trace/blob/main/docs/images/Trace_Primary_C.png\" alt=\"drawing\" width=\"500\"/>\n</p>\n\n# End-to-end Generative Optimization for AI Agents\n\n![Static Badge](https://img.shields.io/badge/build-passing-brightgreen)\n![PyPI](https://img.shields.io/pypi/v/trace-opt)\n![PyPI - Python Version](https://img.shields.io/pypi/pyversions/trace-opt)\n![GitHub license](https://img.shields.io/badge/License-MIT-blue.svg)\n[![arXiv](https://img.shields.io/badge/arXiv-1234.56789-b31b1b.svg)](https://arxiv.org/abs/2406.16218)\n\nTrace is a new AutoDiff-like tool for training AI systems end-to-end with general feedback (like numerical rewards or\nlosses, natural language text, compiler errors, etc.). Trace generalizes the back-propagation algorithm by capturing and\npropagating an AI system's execution trace. Trace is implemented as a PyTorch-like Python library. Users write Python\ncode directly and can use Trace primitives to optimize certain parts, just like training neural networks!\n\n[Paper](https://arxiv.org/abs/2406.16218) | [Project website](https://microsoft.github.io/Trace/) | [Documentation](https://microsoft.github.io/Trace/intro.html) | [Blogpost](https://www.microsoft.com/en-us/research/blog/tracing-the-path-to-self-adapting-ai-agents/) | [Discord channel](https://discord.gg/4VeAvwFcWy)\n\n<p >\n    <img src=\"https://github.com/microsoft/Trace/blob/main/docs/images/platform2.png\" alt=\"drawing\" width=\"100%\"/>\n</p>\n\n## Setup\n\nSimply run\n\n    pip install trace-opt\n\nOr for development, clone the repo and run the following.\n\n    pip install -e .\n\nThe library requires Python >= 3.9. By default (starting with v0.1.3.5), we use [LiteLLM](https://github.com/BerriAI/litellm) as the backend of LLMs. For backward compatibility, we provide backend-support with [AutoGen](https://github.com/microsoft/autogen); when installing, users can add `[autogen]` tag to install a compatible AutoGen version (e.g., `pip install trace-opt[autogen]`). You may require [Git Large File Storage](https://git-lfs.com/) if\ngit is unable to clone the repository.\n\n**For questions or reporting bugs, please use Github Issues or post on our [Discord channel](https://discord.gg/4VeAvwFcWy). We actively check these channels.**\n\n\n## Updates\n- **2025.2.7** Trace was featured in the [G-Research NeurIPS highlight](https://www.gresearch.com/news/neurips-paper-reviews-2024-8/) by the Science Director Hugh Salimbeni.\n- **2024.12.10** Trace was demoed in person at NeurIPS 2024 Expo.\n- **2024.11.05** Ching-An Cheng gave a talk at UW Robotics Colloquium on Trace: [video](https://www.youtube.com/watch?v=T2g1Vo3u_9g).\n- **2024.10.21**    New [paper](https://arxiv.org/abs/2410.15625) by Nvidia, Stanford, Visa, & Intel applies Trace to\n  optimize for mapper code of parallel programming (for scientific computing and matrix multiplication). Trace (OptoPrime) learns code achieving 1.3X speed up under 10\n  minutes, compared to the code optimized by a system engineer expert.\n- **2024.9.30** Ching-An Cheng gave a talk to the AutoGen community: [link](https://twitter.com/qingyun_wu/status/1840093778595721727).\n- **2024.9.25** [Trace Paper](https://arxiv.org/abs/2406.16218) is accepted to NeurIPS 2024.\n- **2024.9.14** TextGrad is available as an optimizer in Trace.\n- **2024.8.18** Allen Nie gave a talk to [Pasteur Labs](https://pasteurlabs.ai/) & Institute for Simulation Intelligence.\n\nWe have a mailing list for announcements: [Signup](http://eepurl.com/iSscZ-/)\n\n## QuickStart\n\nTrace has two primitives: `node` and `bundle`. `node` is a primitive to define a node in the computation graph. `bundle`\nis a primitive to define a function that can be optimized.\n\n```python\nfrom opto.trace import node\n\nx = node(1, trainable=True)\ny = node(3)\nz = x / y\nz2 = x / 3  # the int 3 would be converted to a node automatically\n\nlist_of_nodes = [x, node(2), node(3)]\nnode_of_list = node([1, 2, 3])\n\nnode_of_list.append(3)\n\n# easy built-in computation graph visualization\nz.backward(\"maximize z\", visualize=True, print_limit=25)\n```\n\nOnce a node is declared, all the following operations on the node object will be automatically traced.\nWe built many magic functions to make a node object act like a normal Python object. By marking `trainable=True`, we\ntell our optimizer that this node's content\ncan be changed by the optimizer.\n\nFor functions, Trace uses decorators like @bundle to wrap over Python functions. A bundled function behaves like any\nother Python function.\n\n```python\nfrom opto.trace import node, bundle\n\n\n@bundle(trainable=True)\ndef strange_sort_list(lst):\n    '''\n    Given list of integers, return list in strange order.\n    Strange sorting, is when you start with the minimum value,\n    then maximum of the remaining integers, then minimum and so on.\n    '''\n    lst = sorted(lst)\n    return lst\n\n\ntest_input = [1, 2, 3, 4]\ntest_output = strange_sort_list(test_input)\nprint(test_output)\n```\n\nNow, after declaring what is trainable and what isn't, and use `node` and `bundle` to define the computation graph, we\ncan use the optimizer to optimize the computation graph.\n\n```python\nfrom opto.optimizers import OptoPrime\n\n\n# we first declare a feedback function\n# think of this as the reward function (or loss function)\ndef get_feedback(predict, target):\n    if predict == target:\n        return \"test case passed!\"\n    else:\n        return \"test case failed!\"\n\n\ntest_ground_truth = [1, 4, 2, 3]\ntest_input = [1, 2, 3, 4]\n\nepoch = 2\n\noptimizer = OptoPrime(strange_sort_list.parameters())\n\nfor i in range(epoch):\n    print(f\"Training Epoch {i}\")\n    test_output = strange_sort_list(test_input)\n    correctness = test_output.eq(test_ground_truth)\n    feedback = get_feedback(test_output, test_ground_truth)\n\n    if correctness:\n        break\n\n    optimizer.zero_feedback()\n    optimizer.backward(correctness, feedback)\n    optimizer.step()\n```\n\nThen, we can use the familiar PyTorch-like syntax to conduct the optimization.\n\nHere is another example of a simple sales agent:\n\n```python\nfrom opto import trace\n\n@trace.model\nclass Agent:\n\n    def __init__(self, system_prompt):\n        self.system_prompt = system_prompt\n        self.instruct1 = trace.node(\"Decide the language\", trainable=True)\n        self.instruct2 = trace.node(\"Extract name if it's there\", trainable=True)\n\n    def __call__(self, user_query):\n        response = trace.operators.call_llm(self.system_prompt,\n                                            self.instruct1, user_query)\n        en_or_es = self.decide_lang(response)\n\n        user_name = trace.operators.call_llm(self.system_prompt,\n                                             self.instruct2, user_query)\n        greeting = self.greet(en_or_es, user_name)\n\n        return greeting\n\n    @trace.bundle(trainable=True)\n    def decide_lang(self, response):\n        \"\"\"Map the language into a variable\"\"\"\n        return\n\n    @trace.bundle(trainable=True)\n    def greet(self, lang, user_name):\n        \"\"\"Produce a greeting based on the language\"\"\"\n        greeting = \"Hola\"\n        return f\"{greeting}, {user_name}!\"\n```\n\nImagine we have a feedback function (like a reward function) that tells us how well the agent is doing. We can then optimize this agent online:\n\n```python\nfrom opto.optimizers import OptoPrime\n\ndef feedback_fn(generated_response, gold_label='en'):\n    if  gold_label == 'en' and 'Hello' in generated_response:\n        return \"Correct\"\n    elif gold_label == 'es' and 'Hola' in generated_response:\n        return \"Correct\"\n    else:\n        return \"Incorrect\"\n\ndef train():\n    epoch = 3\n    agent = Agent(\"You are a sales assistant.\")\n    optimizer = OptoPrime(agent.parameters())\n\n    for i in range(epoch):\n        print(f\"Training Epoch {i}\")\n        try:\n            greeting = agent(\"Hola, soy Juan.\")\n            feedback = feedback_fn(greeting.data, 'es')\n        except trace.ExecutionError as e:\n            greeting = e.exception_node\n            feedback, terminal, reward = greeting.data, False, 0\n\n        optimizer.zero_feedback()\n        optimizer.backward(greeting, feedback)\n        optimizer.step(verbose=True)\n\n        if feedback == 'Correct':\n            break\n\n    return agent\n\nagent = train()\n```\n\nDefining and training an agent through Trace will give you more flexibility and control over what the agent learns.\n\n## Tutorials\n\n| **Level** | **Tutorial**                                                                              | **Run in Colab**                                                                                                                                                                                       | **Description**                                                                                                                                                                       |\n| --- |-------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Beginner | [Getting Started](https://microsoft.github.io/Trace/quickstart/quick_start.html)          | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/microsoft/Trace/blob/website/docs/quickstart/quick_start.ipynb)       | Introduces basic primitives like `node` and `bundle`. Showcases a code optimization pipeline.                                                                                         |\n| Beginner | [Adaptive AI Agent](https://microsoft.github.io/Trace/quickstart/quick_start_2.html)      | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/microsoft/Trace/blob/website/docs/quickstart/quick_start_2.ipynb)      | Introduce primitive `model` that allows anyone to build self-improving agents that react to environment feedback. Shows how an LLM agent learns to place a shot in a Battleship game.\n| Intermediate | [Multi-Agent Collaboration](https://microsoft.github.io/Trace/quickstart/virtualhome.html) | N/A                                                                                                                                                                                                    | Demonstrates how Trace can be used for multi-agent collaboration environment in Virtualhome.\n| Intermediate | [NLP Prompt Optimization](https://microsoft.github.io/Trace/examples/nlp/bigbench_hard.html) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/microsoft/Trace/blob/website/docs/examples/nlp/bigbench_hard.ipynb) | Shows how Trace can optimizes prompt and code together jointly for BigBench-Hard 23 tasks.\n| Advanced | [Robotic Arm Control](https://microsoft.github.io/Trace/examples/robotics/metaworld.html) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/microsoft/Trace/blob/website/docs/examples/robotics/metaworld.ipynb)                                     | Trace can optimize code to control a robotic arm after observing a full trajectory of interactions.                                                                                   |\n\n\n## Supported Optimizers\n\nCurrently, we support three optimizers:\n\n- OPRO: [Large Language Models as Optimizers](https://arxiv.org/abs/2309.03409)\n- TextGrad: [TextGrad: Automatic \"Differentiation\" via Text](https://arxiv.org/abs/2406.07496)\n- OptoPrime: [Our proposed algorithm](https://arxiv.org/abs/2406.16218) -- using the entire computational graph to perform parameter update. It is 2-3x\n  faster than TextGrad.\n\nUsing our framework, you can seamlessly switch between different optimizers:\n\n```python\noptimizer1 = OptoPrime(strange_sort_list.parameters())\noptimizer2 = OPRO(strange_sort_list.parameters())\noptimizer3 = TextGrad(strange_sort_list.parameters())\n```\n\nHere is a summary of the optimizers:\n\n|                                   | Computation Graph | Code as Functions | Library Support | Supported Optimizers      | Speed       | Large Graph |\n|-----------------------------------|-------------------|-------------------|------------------|---------------------------|-------------|-------------|\n| OPRO                              | ❌                 | ❌                 | ❌            | OPRO                      | ⚡️          | ✅      |\n| TextGrad                          | ✅                 | ❌                 | ✅            | TextGrad                  | 🐌          | ✅      |\n| Trace  | ✅                 | ✅                 | ✅            | OPRO, OptoPrime, TextGrad | ⚡  | ✅      |\n\nThe table evaluates the frameworks in the following aspects:\n\n- Computation Graph: Whether the optimizer leverages the computation graph of the workflow.\n- Code as Functions: Whether the framework allows users to write actual executable Python functions and not require\n  users to wrap them in strings.\n- Library Support: Whether the framework has a library to support the optimizer.\n- Speed: TextGrad is about 2-3x slower than OptoPrime (Trace). OPRO has no concept of computational graph, therefore is very fast.\n- Large Graph: OptoPrime (Trace) represents the entire computation graph in context, therefore, might have issue with graphs that have more than hundreds of operations. TextGrad does not have the context-length issue, however, might be very slow on large graphs.\n\nWe provide a comparison to validate our implementation of TextGrad in Trace:\n\n<p align=\"center\">\n    <img src=\"https://github.com/microsoft/Trace/blob/main/docs/images/compare_to_textgrad3.png?raw=True\" alt=\"drawing\" width=\"100%\"/>\n</p>\n\nTo produce this table, we ran the TextGrad pip-installed repo on 2024-10-30, and we also include the numbers reported in the TextGrad paper.\nThe LLM APIs are called around the same time to ensure a fair comparison. TextGrad paper's result was reported in 2024-06.\n\nYou can also easily implement your own optimizer that works directly with `TraceGraph` (more tutorials on how to work\nwith TraceGraph coming soon).\n\n## LLM API Setup\n\nCurrently we rely on [LiteLLM](https://github.com/BerriAI/litellm) or [AutoGen v0.2](https://github.com/microsoft/autogen/tree/0.2) for LLM caching and API-Key management.\n\nBy default, LiteLLM is used. To change the default backend, set the environment variable `TRACE_DEFAULT_LLM_BACKEND` on terminal\n```bash\nexport TRACE_DEFAULT_LLM_BACKEND=\"<your LLM backend here>\"  # 'LiteLLM' or 'AutoGen`\n```\nor in python before importing `opto`\n```python\nimport os\nos.environ[\"TRACE_DEFAULT_LLM_BACKEND\"] = \"<your LLM backend here>\"  # 'LiteLLM' or 'AutoGen`\nimport opto\n```\n\n\n\n### Using LiteLLM as Backend\n\nSet the keys as the environment variables, following the [documentation of LiteLLM](https://docs.litellm.ai/docs/providers). For example,\n\n```python\nimport os\nos.environ[\"OPENAI_API_KEY\"] = \"<your OpenAI API key here>\"\nos.environ[\"ANTHROPIC_API_KEY\"] = \"<your Anthropic API key here>\"\n```\nIn Trace, we add another environment variable `TRACE_LITELLM_MODEL` to set the default model name used by LiteLLM for convenience, e.g.,\n```bash\nexport TRACE_LITELLM_MODEL='gpt-4o'\n```\nwill set all LLM instances in Trace to use `gpt-4o` by default.\n\n\n### Using AutoGen as Backend\nFirst install Trace with autogen flag, `pip install trace-opt[autogen]`. AutoGen relies on `OAI_CONFIG_LIST`, which is a file you put in your working directory. It has the format of:\n\n```json lines\n[\n    {\n        \"model\": \"gpt-4\",\n        \"api_key\": \"<your OpenAI API key here>\"\n    },\n      {\n        \"model\": \"claude-sonnet-3.5-latest\",\n        \"api_key\": \"<your Anthropic API key here>\"\n    }\n]\n```\nYou can switch between different LLM models by changing the `model` field in this configuration file.\nNote AutoGen by default will use the first model available in this config file.\n\nYou can also set an `os.environ` variable `OAI_CONFIG_LIST` to point to the location of this file or directly set a JSON string as the value of this variable.\n\nFor convenience, we also provide a method that directly grabs the API key from the environment variable `OPENAI_API_KEY` or `ANTHROPIC_API_KEY`.\nHowever, doing so, we specify the model version you use, which is `gpt-4o` for OpenAI and `claude-sonnet-3.5-latest` for Anthropic.\n\n\n## Citation\n\nIf you use this code in your research please cite the following [publication](https://arxiv.org/abs/2406.16218):\n\n```\n@article{cheng2024trace,\n  title={Trace is the Next AutoDiff: Generative Optimization with Rich Feedback, Execution Traces, and LLMs},\n  author={Cheng, Ching-An and Nie, Allen and Swaminathan, Adith},\n  journal={arXiv preprint arXiv:2406.16218},\n  year={2024}\n}\n```\n\n## Papers / Projects that Use Trace\n\n[Improving Parallel Program Performance Through DSL-Driven Code Generation with LLM Optimizers](https://arxiv.org/pdf/2410.15625)\nWork from Stanford, NVIDIA, Intel, Visa Research.\n```\n@article{wei2024improving,\n  title={Improving Parallel Program Performance Through DSL-Driven Code Generation with LLM Optimizers},\n  author={Wei, Anjiang and Nie, Allen and Teixeira, Thiago SFX and Yadav, Rohan and Lee, Wonchan and Wang, Ke and Aiken, Alex},\n  journal={arXiv preprint arXiv:2410.15625},\n  year={2024}\n}\n```\n\n[The Importance of Directional Feedback for LLM-based Optimizers](https://arxiv.org/pdf/2405.16434)\nExplains the role of feedback in LLM-based optimizers. An early work that influenced Trace's clean separation between the platform, optimizer, and feedback.\n```\n@article{nie2024importance,\n  title={The Importance of Directional Feedback for LLM-based Optimizers},\n  author={Nie, Allen and Cheng, Ching-An and Kolobov, Andrey and Swaminathan, Adith},\n  journal={arXiv preprint arXiv:2405.16434},\n  year={2024}\n}\n```\n\n## Contributors Wall\n<a href=\"https://github.com/microsoft/Trace/graphs/contributors\">\n  <img src=\"https://contrib.rocks/image?repo=microsoft/Trace\" />\n</a>\n\n## Evaluation\n\nA previous version of Trace was tested with gpt-4-0125-preview on numerical optimization, simulated traffic control,\nbig-bench-hard, and llf-metaworld tasks, which demonstrated good optimization performance on multiple random seeds;\nplease see the paper for details.\n\n**Note**  For gpt-4o, please use the version **gpt-4o-2024-08-06** (onwards),\nwhich [fixes](https://platform.openai.com/docs/models/gpt-4o) the structured output issue of gpt-4o-2024-05-13.\nWhile gpt-4 works reliably most of the time, we've found gpt-4o-2024-05-13 often hallucinates even in very basic\noptimization problems and does not follow instructions. This might be due to the current implementation of optimizers\nrely on outputing in json format. Issues of gpt-4o with json have been reported in the communities (\nsee [example](https://community.openai.com/t/gpt-4o-doesnt-consistently-respect-json-schema-on-tool-use/751125)).\n\n## Disclaimers\n\n- Trace is an LLM-based optimization framework for research purpose only.\n- The current release is a beta version of the library. Features and more documentation will be added, and some\n  functionalities may be changed in the future.\n- System performance may vary by workflow, dataset, query, and response, and users are responsible for determining the\n  accuracy of generated content.\n- System outputs do not represent the opinions of Microsoft.\n- All decisions leveraging outputs of the system should be made with human oversight and not be based solely on system\n  outputs.\n- Use of the system must comply with all applicable laws, regulations, and policies, including those pertaining to\n  privacy and security.\n- The system should not be used in highly regulated domains where inaccurate outputs could suggest actions that lead to\n  injury or negatively impact an individual's legal, financial, or life opportunities.\n\n## Contributing\n\nThis project welcomes contributions and suggestions. Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft\ntrademarks or logos is subject to and must follow\n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft\nsponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n\n## Privacy\n\nSee [Microsoft Privacy Statement](https://privacy.microsoft.com/en-us/privacystatement).\n"
    },
    {
      "name": "microsoft/teams-ai",
      "stars": 524,
      "img": "https://avatars.githubusercontent.com/u/6154722?s=40&v=4",
      "owner": "microsoft",
      "repo_name": "teams-ai",
      "description": "SDK focused on building AI based applications and extensions for Microsoft Teams and other Bot Framework channels",
      "homepage": "",
      "language": "C#",
      "created_at": "2023-02-06T18:03:17Z",
      "updated_at": "2025-04-23T07:15:56Z",
      "topics": [],
      "readme": "# <img src=\"https://media.githubusercontent.com/media/microsoft/teams-ai/main/assets/icon.png\" width=\"100px\"> Teams AI Library\n\n[![Coverage Status](https://coveralls.io/repos/github/microsoft/teams-ai/badge.svg?branch=main)](https://coveralls.io/github/microsoft/teams-ai?branch=main)\n\n| Name       | Status                                                                                                                                                                                                                                                                  | Package                                                    |\n| ---------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------- |\n| Javascript | [![Javascript Build Status](https://dev.azure.com/DomoreexpGithub/Github_Pipelines/_apis/build/status%2Fteams-ai%2Fmicrosoft.teams-ai.javascript?branchName=main)](https://dev.azure.com/DomoreexpGithub/Github_Pipelines/_build/latest?definitionId=5&branchName=main) | [NPM](https://www.npmjs.com/package/@microsoft/teams-ai)   |\n| C#         | [![C# Build Status](https://dev.azure.com/DomoreexpGithub/Github_Pipelines/_apis/build/status%2Fteams-ai%2Fmicrosoft.teams-ai.dotnet?branchName=main)](https://dev.azure.com/DomoreexpGithub/Github_Pipelines/_build/latest?definitionId=4&branchName=main)             | [NuGet](https://www.nuget.org/packages/Microsoft.Teams.AI) |\n| Python     | [![Build Status](https://dev.azure.com/DomoreexpGithub/Github_Pipelines/_apis/build/status%2Fteams-ai%2Fmicrosoft.teams-ai.python?branchName=main)](https://dev.azure.com/DomoreexpGithub/Github_Pipelines/_build/latest?definitionId=10&branchName=main)               | [PyPI](https://pypi.org/project/teams-ai/)                 |\n\nWelcome to the Teams AI Library! This SDK is specifically designed to assist you in creating bots capable of interacting with Teams and Microsoft 365 applications. It is constructed using the [Bot Framework SDK](https://github.com/microsoft/botbuilder-js) as its foundation, simplifying the process of developing bots that interact with Teams' artificial intelligence capabilities.\n\n![Teams AI flow diagram](https://github.com/microsoft/teams-ai/assets/14900841/154353ff-bafe-4423-abcd-6dc5a8680fe9)\nThis is a diagram of the Teams-AI flow. Teams AI Library SDK hooks into the Teams SDK and Azure OpenAI SDK to provide a seamless experience for developers.\n\nThe SDK is currently available for JavaScript/TypeScript applications in the [`js`](./js) folder and via the [teams-ai package on NPM](https://www.npmjs.com/package/@microsoft/teams-ai). .NET support is available in the [`dotnet`](./dotnet) folder and via the [teams-ai package on NuGet](https://www.nuget.org/packages/Microsoft.Teams.AI). Python support is available in the [`python`](./python) folder and via the [teams-ai package on Pypi](https://pypi.org/project/teams-ai/)\n\n## Getting Started\n\n> ### 🖇️ Jump right in❗️ 📎\n>\n> If you want to jump immediately into AI, try out the [04.ai.a.teamsChefbot](./js/samples/04.ai-apps/a.teamsChefBot) sample. This sample is a simple bot that uses the OpenAI GPT model to build a Teams app. Just load it up in Visual Studio Code, rename the sample.env file to .env, add in your OpenAI key or Azure OpenAI key and endpoint in the .env file, and hit F5! 🎉\n\n### Start with our getting started guides\n\nThis SDK is now generally available. We welcome your feedback and contributions!\n\n**To get started, head over to the [Getting Started Guide](getting-started/README.md).**\n\n## Capabilities\n\n### Teams-centric component scaffolding\n\n> For examples of the below, browse through the [`js`](./js/samples/) sample folders, the [`dotnet`](./dotnet/samples) sample folders or the [`python`](./python/samples) sample folders.\n> Simple scaffolding for any conversational app component, including:\n\n- Chat bots\n- Message Extensions\n- Link unfurling\n- Adaptive Cards\n\n### Natural Language Modelling\n\nThe SDK is built to leverage OpenAI Large Language Models so you don't have to create your own. This saves you the complexity of processing natural language, and allows your users to talk to your app with their own words.\n\n### Prompt Engineering\n\nWith a simple text file written in human language, you can describe the functionality of your app to cue OpenAI to focus on the right user intentions and provide relevant responses.\n\n### Moderation\n\nA configurable API call to filter inappropriate content for input content, output content, or both.\n\nModerators are available whether you decide to you OpenAI or Azure OpenAI for your models.\n\nIf using OpenAI see:\n\n- [OpenAIModerator.ts](./js/packages/teams-ai/src/moderators/OpenAIModerator.ts)\n- [OpenAIModerator.cs](./dotnet/packages/Microsoft.TeamsAI/Microsoft.TeamsAI/AI/Moderator/OpenAIModerator.cs)\n- [openai_moderator.py](./python/packages/ai/teams/ai/moderators/openai_moderator.py).\n\nFor Azure's moderator, see:\n\n- [AzureContentSafetyModerator.ts](./js/packages/teams-ai/src/moderators/AzureContentSafetyModerator.ts)\n- [AzureContentSafetyModerator.cs](./dotnet/packages/Microsoft.TeamsAI/Microsoft.TeamsAI/AI/Moderator/AzureContentSafetyModerator.cs)\n- [azure_content_safety_moderator.py](./python/packages/ai/teams/ai/moderators/azure_content_safety_moderator.py)\n\n### Predictive Engine to Map Intents to Actions\n\nLeveraging provided prompts and topic filters, it's simple to create a predictive engine that detects user intents and map them to relevant app actions, where you can focus your business logic. These actions are even possible to chain together to make building complex workflows easy.\n\n### Conversational Session History\n\nThe state of your user's session is not lost, allowing conversations to flow freely and arrive quickly at right outcome.\n\n### Localization\n\nBecause OpenAI's models are trained on the open internet, they're tuned to any language, saving you the cost of localization.\n\n### LLM modularity\n\nWhile the SDK handles OpenAI's GPT models out of the box, you can choose to swap to the LLM of your choice without touching any of your conversational app code.\n\n## License\n\nThis SDK is licensed under the MIT License. This SDK includes tools to use APIs provided by third parties. These APIs are provided under their own separate terms.\n\n- OpenAI API. Use of the OpenAI API requires an API key, which can be obtained from OpenAI. By using this SDK, you agree to abide by the OpenAI API Terms of Use and Privacy Policy. You can find them at [OpenAI Terms of Use](https://openai.com/policies/terms-of-use)\n- Azure OpenAI Service. Use of the Azure OpenAI API requires an API key. By using this SDK, you agree to abide by the Azure OpenAI API terms. You can find them at [Azure OPENAI TOS](https://www.microsoft.com/licensing/terms/productoffering/MicrosoftAzure/MCA#ServiceSpecificTerms), and associated documentation at [Azure Cognitive Services](https://learn.microsoft.com/en-us/azure/cognitive-services/openai/).\n\n---\n\n## Contributing\n\nThis project welcomes contributions and suggestions. Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit [https://cla.opensource.microsoft.com](https://cla.opensource.microsoft.com).\n\nFor more details, see [./CONTRIBUTING.md](./CONTRIBUTING.md).\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft\ntrademarks or logos is subject to and must follow\n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos is subject to those third parties' policies.\n\n## Accessing daily builds\n\nPlease see [Daily Builds](https://github.com/microsoft/teams-ai/blob/main/docs/DAILYBUILDS.MD) for instructions on how to access daily builds.\n"
    },
    {
      "name": "test-zeus-ai/testzeus-hercules",
      "stars": 509,
      "img": "https://avatars.githubusercontent.com/u/178647201?s=40&v=4",
      "owner": "test-zeus-ai",
      "repo_name": "testzeus-hercules",
      "description": "Hercules is the world’s first open-source testing agent, enabling UI, API, Security, Accessibility, and Visual validations – all without code or maintenance. Automate testing effortlessly and let Hercules handle the heavy lifting! ⚡",
      "homepage": "https://testzeus.com/hercules",
      "language": "Python",
      "created_at": "2024-11-14T21:19:10Z",
      "updated_at": "2025-04-23T04:19:08Z",
      "topics": [
        "agent",
        "agentic-ai",
        "agents",
        "ai",
        "autogen",
        "automation",
        "browser",
        "end-to-end-testing",
        "hercules",
        "large-action-model",
        "playwright",
        "qa-automation",
        "rpa",
        "software-testing",
        "test-automation",
        "testing",
        "testing-tools",
        "testzeus"
      ],
      "readme": "# 💪 Hercules\n\n[![PyPI Total Downloads](https://static.pepy.tech/badge/testzeus-hercules)](https://pepy.tech/projects/testzeus-hercules)\n![Docker Pulls](https://img.shields.io/docker/pulls/testzeus/hercules)\n[![CI Test](https://github.com/test-zeus-ai/testzeus-hercules/actions/workflows/main-test.yml/badge.svg)](https://github.com/test-zeus-ai/testzeus-hercules/actions/workflows/main-test.yml)\n[![Slack](https://img.shields.io/badge/slack-TestZeus-brightgreen.svg?logo=slack)](https://join.slack.com/t/testzeuscommunityhq/shared_invite/zt-2v2br8wog-FAmo_76xRHx~k~1oNaGQ0Q)\n\nTesting modern web applications can be difficult, with frequent changes and complex features making it hard to keep up. That's where **Hercules** comes in. Hercules is the world's first open-source testing agent, built to handle the toughest testing tasks so you don't have to. It turns simple, easy-to-write Gherkin steps into fully automated **end to end** tests—no coding skills needed. Whether you're working with tricky platforms like Salesforce or running tests in your CI/CD pipeline, Hercules adapts to your needs and takes care of the details. With Hercules, testing becomes simple, reliable, and efficient, helping teams everywhere deliver better software. Here's a quick demo of lead creation using natural english language test (without any code):\n\n![HerculesUsage](statics/LeadcreationDemo.gif)\n\n\n\nAs you saw, using Hercules is as simple as feeding in your Gherkin features, and getting the results:\n![HerculesUsage](statics/assets/hercules.svg)\n\nAt [TestZeus](www.testzeus.com), we believe that **trustworthy and open-source code** is the backbone of innovation. That's why we've built Hercules to be transparent, reliable, and community-driven.\n\nOur mission? To **democratize and disrupt test automation**, making top-tier testing accessible to everyone, not just the elite few. No more gatekeeping—everyone deserves a hero on their testing team!\n\n### Video Tutorials: [@TestZeus](https://www.youtube.com/@TestZeus)\n---\n\n#### 🚀 **Getting Started with TestZeus Hercules**\n- **Introduction to TestZeus Hercules**  \n  Learn about the core features of TestZeus Hercules and how it can streamline end-to-end testing for your projects. \n   [![Autonomous execution with Hercules](statics/assets/introvideothumbnail2.png)](https://youtu.be/_m_NDjM6aZ0?si=ArtVKz8uSgGWTcAK)\n\n- **Installation and Setup Guide**  \n  Step-by-step instructions for installing and configuring TestZeus Hercules in your environment.  \n  _[Watch now](https://youtu.be/9D-SZGoDrfc?si=GL0IArWkB1ZgBdx5)_\n\n---\n\n#### 🧪 **Writing and Executing Test Cases**\n- **Creating BDD Test Cases**  \n  Learn how to write Behavior-Driven Development (BDD) test cases for Hercules and use dynamic testdata.  \n  _[Watch now](https://www.youtube.com/watch?v=yJD0cZ7Bx6Q&t)_\n\n---\n\n#### 🌐 **Integrations and Advanced Features**\n- **Testing Multilingual content**  \n  Learn how Hercules interacts with web browsers to perform Testing on Multilingual content via Multilingual testcase.  \n  _[Watch now](https://youtu.be/vI9iGSqKpGA?si=6NGAvKnwFboSyHT2)_\n---\n\n#### 🛠️ **Customization and Community Tools**\n- **Enhancing Hercules with Community-Driven Tools**  \n  Discover how to customize Hercules and incorporate additional tools provided by the community.  \n  _[Watch now](https://youtu.be/C8hUy5leRF0?si=yVMpZ7WFcI01BoI3)_\n---\n\n#### 🛠️ **API Testing**\n- **API testing all the way, new ways to do end to end**  \n  _[Watch now](https://youtu.be/qMt89DQH6LQ?si=lcSJbKOCaqqUGeQ8)_\n---\n\n#### 🛠️ **Security Testing**\n- **Security Testing done end to end**  \n  _[Watch now](https://youtu.be/o6MmfTakIh4?si=JL8iu4e3i85SWrxU)_\n\n#### 🛠️ **Visual validations and assertions**\n- **Using vision capabilities to check snapshots and components on the application**  \n  _[Watch now](https://youtu.be/EKzllLEguhw)_\n\n\n---\n\n## ⚙️ Installation and Usage\n\nHercules offers multiple ways to get started, catering to different user preferences and requirements.\nIf you are new to the Python ecosystem and don't know where to begin, dont worry and read the footnotes on understanding the basics.\n\n\nFor a quick taste of the solution, you can try the notebook here: \n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1YiZsXem1POTwkcr17QqflXnihhuSqwM2?usp=sharing)\n- **Note**: Colab might ask you to restart the session as python3.11 and some libs are installed during the installation of testzeus-hercules. Please restart the session if required and continue the execution. Also , we recommend one of the approaches below for getting the full flavor of the solution. \n\n### Approach 1: Using PyPI Package\n\n#### Installation\n\nInstall Hercules from PyPI:\n\n```bash\npip install testzeus-hercules\n```\n\nHercules uses Playwright to interact with web pages, so you need to install Playwright and its dependencies:\n\n```bash\nplaywright install --with-deps\n```\n\nFor detailed information about project structure and running tests, please refer to our [Run Guide](docs/run_guide.md).\n\n#### Basic Parameters\n\nOnce installed, you will need to provide some basic parameters to run Hercules:\n\n- `--input-file INPUT_FILE`: Path to the input Gherkin feature file to be tested.\n- `--output-path OUTPUT_PATH`: Path to the output directory. The path of JUnit XML result and HTML report for the test run.\n- `--test-data-path TEST_DATA_PATH`: Path to the test data directory. The path where Hercules expects test data to be present; all test data used in feature testing should be present here.\n- `--project-base PROJECT_BASE`: Path to the project base directory. This is an optional parameter; if you populate this, `--input-file`, `--output-path`, and `--test-data-path` are not required, and Hercules will assume all the three folders exist in the following format inside the project base:\n\n```\nPROJECT_BASE/\n├── gherkin_files/\n├── input/\n│   └── test.feature\n├── log_files/\n├── output/\n│   ├── test.feature_result.html\n│   └── test.feature_result.xml\n├── proofs/\n│   └── User_opens_Google_homepage/\n│       ├── network_logs.json\n│       ├── screenshots/\n│       └── videos/\n└── test_data/\n    └── test_data.txt\n```\n\n- `--llm-model LLM_MODEL`: Name of the LLM model to be used by the agent (recommended is `gpt-4o`, but it can take others).\n- `--llm-model-api-key LLM_MODEL_API_KEY`: API key for the LLM model, something like `sk-proj-k.......`.\n\n#### Environment Variables\n\nIn addition to command-line parameters, Hercules supports various environment variables for configuration:\n\n- `BROWSER_TYPE`: Type of browser to use (`chromium`, `firefox`, `webkit`). Default: `chromium`\n- `HEADLESS`: Run browser in headless mode (`true`, `false`). Default: `true`\n- `BROWSER_RESOLUTION`: Browser window resolution (format: `width,height`). Example: `1920,1080`\n- `BROWSER_COOKIES`: Set cookies for the browser context. Format: JSON array of cookie objects. Example: `[{\"name\": \"session\", \"value\": \"123456\", \"domain\": \"example.com\", \"path\": \"/\"}]`\n- `RECORD_VIDEO`: Record test execution videos (`true`, `false`). Default: `true`\n- `TAKE_SCREENSHOTS`: Take screenshots during test (`true`, `false`). Default: `true`\n\nFor a complete list of environment variables, see our [Environment Variables Guide](docs/environment_variables.md).\n\n#### Running Hercules\n\nAfter passing all the required parameters, the command to run Hercules should look like this:\n\n```bash\ntestzeus-hercules --input-file opt/input/test.feature --output-path opt/output --test-data-path opt/test_data --llm-model gpt-4o --llm-model-api-key sk-proj-k.......\n```\n\n\n## ⚙️ Running Hercules on a Windows Machine\n\nTo set up and run Hercules on a Windows machine:\n\n1. **Open PowerShell in Administrator Mode:**\n   - Click on the **Start Menu**, search for **PowerShell**, and right-click on **Windows PowerShell**.\n   - Select **Run as Administrator** to open PowerShell in administrator mode.\n\n2. **Navigate to the Helper Scripts Folder:**\n   - Use the `cd` command to navigate to the folder containing the `hercules_windows_setup.ps1` script. For example:\n     ```powershell\n     cd path\\to\\helper_scripts\n     ```\n\n3. **Run the Setup Script:**\n   - Execute the script to install and configure Hercules:\n     ```powershell\n     .\\hercules_windows_setup.ps1\n     ```\n\n4. **Follow On-Screen Instructions:**\n   - The script will guide you through installing Python, Playwright, FFmpeg, and other required dependencies.\n\n5. **Run Hercules:**\n   - Once the setup is complete, you can run Hercules from PowerShell or Command Prompt using the following command:\n     ```bash\n     testzeus-hercules --input-file opt/input/test.feature --output-path opt/output --test-data-path opt/test_data --llm-model gpt-4o --llm-model-api-key sk-proj-k.......\n     ```\n\n---\n\n\n#### Supported AI Models for TestZeus-Hercules\n- Anthropic: Compatible with Haiku 3.5 and above.\n- Groq: Supports any version with function calling and coding capabilities.\n- Mistral: Supports any version with function calling and coding capabilities. Mistral-large, Mistral-medium. Only heavey models.\n- OpenAI: Fully compatible with GPT-4o/o3-mini and above. Note: OpenAI GPT-4o-mini is only supported for sub-agents, for planner it is still recommended to use GPT-4o.\n- Ollama: Supported with medium models and function calling. Heavy models only 70b and above.\n- Gemini: [deprecated, because of flaky execution]. Refer: https://testzeuscommunityhq.slack.com/archives/C0828GV2HEC/p1740628636862819\n- Deepseek: only deepseek-chat v3 support.\n- Hosting: supported on AWS bedrock, GCP VertexAI, AzureAI. [tested models, OpenAI, Anthropic Sonet and Haiku, Llamma 60b above with function calling]\nNote: Kindly ensure that the model you are using can handle agentic activities like function calling. For example larger models like OpenAI GPT 4O, Llama >70B, Mistral large etc.\n\n#### Execution Flow\n\nUpon running the command:\n\n- Hercules will start and attempt to open a web browser (default is Chromium).\n- It will prepare a plan of execution based on the feature file steps provided.\n- The plan internally expands the brief steps mentioned in the feature file into a more elaborated version.\n- Hercules detects assertions in the feature file and plans the validation of expected results with the execution happening during the test run.\n- All the steps, once elaborated, are passed to different tools based on the type of execution requirement of the step. For example, if a step wants to click on a button and capture the feedback, it will be passed to the `click_using_selector` tool.\n\n#### Output and Logs\n\nOnce the execution is completed:\n\n- Logs explaining the sequence of events are generated.\n- The best place to start is the `output-path`, which will have the JUnit XML result file as well as an HTML report regarding the test case execution.\n- You can also find proofs of execution such as video recordings, screenshots per event, and network logs in the `proofs` folder.\n- To delve deeper and understand the chain of thoughts, refer to the `chat_messages.json` in the `log_files`. This will have exact steps that were planned by the agent.\n\n#### Sample Feature File\n\nHere's a sample feature file:\n\n```gherkin\nFeature: Account Creation in Salesforce\n\n Scenario: Successfully create a new account\n\n   Given I am on the Salesforce login page\n   When I enter my username \"user@example.com\" and password \"securePassword\"\n   And I click on the \"Log In\" button\n   And I navigate to the \"Accounts\" tab\n   And I click on the \"New\" button\n   And I fill in the \"Account Name\" field with \"Test Account\"\n   And I select the \"Account Type\" as \"Customer\"\n   And I fill in the \"Website\" field with \"www.testaccount.com\"\n   And I fill in the \"Phone\" field with \"123-456-7890\"\n   And I click on the \"Save\" button\n   Then I should see a confirmation message \"Account Test Account created successfully\"\n   And I should see \"Test Account\" listed in the account records\n```\n\n#### Sample Result Screenshot\n\n##### Sample HTML Result Screenshot\n![Sample HTML Result](statics/assets/testcase_html.png)\n##### Sample XML Result Screenshot\n![Sample XML Result](statics/assets/testcase_xml.png)\n\n---\n\n### Approach 2: Using Docker\n\nFor all the scale lovers, Hercules is also available as a Docker image.\n\n#### Pull the Docker Image\n\n```bash\ndocker pull testzeus/hercules:latest\n```\n\n#### Running Hercules in Docker\n\nRun the container using:\n\n```bash\ndocker run --env-file=.env \\\n  -v ./agents_llm_config.json:/testzeus-hercules/agents_llm_config.json \\\n  -v ./opt:/testzeus-hercules/opt \\\n  --rm -it testzeus/hercules:latest\n```\n\n- **Environment Variables**: All the required environment variables can be set by passing an `.env` file to the `docker run` command.\n- **LLM Configuration**: If you plan to have complete control over Hercules and which LLM to use beyond the ones provided by OpenAI, you can pass `agents_llm_config.json` as a mount to the container. This is for advanced use cases and is not required for beginners. Refer to sample files `.env-example` and `agents_llm_config-example.json` for details and reference.\n- **Mounting Directories**: Mount the `opt` folder to the Docker container so that all the inputs can be passed to Hercules running inside the container, and the output can be pulled out for further processing. The repository has a sample `opt` folder that can be mounted easily.\n- **Simplified Parameters**: In the Docker case, there is no need for using `--input-file`, `--output-path`, `--test-data-path`, or `--project-base` as they are already handled by mounting the `opt` folder in the `docker run` command.\n\n#### Browser Access in Docker\n\n- While running in Docker mode, understand that Hercules has access only to a headless web browser.\n- If you want Hercules to connect to a visible web browser, try the CDP URL option in the environment file. This option can help you connect Hercules running in your infrastructure to a remote browser like BrowserBase or your self-hosted grid.\n- Use `CDP_ENDPOINT_URL` to set the CDP URL of the Chrome instance that has to be connected to the agent.\n\n### Docker and Remote Browser Support\n\nWhen running Hercules in Docker, you can connect to remote browser instances using various platforms:\n\n1. **BrowserStack Integration**:\n```bash\nexport BROWSERSTACK_USERNAME=your_username\nexport BROWSERSTACK_ACCESS_KEY=your_access_key\nexport CDP_ENDPOINT_URL=$(python helper_scripts/browser_stack_generate_cdp_url.py)\n```\n\n2. **LambdaTest Integration**:\n```bash\nexport LAMBDATEST_USERNAME=your_username\nexport LAMBDATEST_ACCESS_KEY=your_access_key\nexport CDP_ENDPOINT_URL=$(python helper_scripts/lambda_test_generate_cdp_url.py)\n```\n\n3. **BrowserBase Integration**:\n```bash\nexport CDP_ENDPOINT_URL=wss://connect.browserbase.com?apiKey=your_api_key\n```\n\n4. **AnchorBrowser Integration**:\n```bash\nexport CDP_ENDPOINT_URL=wss://connect.anchorbrowser.io?apiKey=your_api_key\n```\n\nNote: Video recording is only supported on platforms that use connect_over_cdp (BrowserBase, AnchorBrowser). Platforms using the connect API (BrowserStack, LambdaTest) do not support video recording.\n\n#### Output and Logs\n\nAfter the command completion:\n\n- The container terminates, and output is written in the mounted `opt` folder, in the same way as described in the directory structure.\n- You will find the JUnit XML result file, HTML reports, proofs of execution, and logs in the respective folders.\n\n---\n\n### Approach 3: Building and running from Source Code\n\nFor the hardcore enthusiasts, you can use Hercules via the source code to get a complete experience of customization and extending Hercules with more tools.\n\n#### Prerequisites\n\n- Ensure you have **Python 3.11** installed on your system.\n\n#### Steps to Run from Source\n\n1. **Clone the Repository**\n\n   ```bash\n   git clone git@github.com:test-zeus-ai/testzeus-hercules.git\n   ```\n\n2. **Navigate to the Directory**\n\n   ```bash\n   cd testzeus-hercules\n   ```\n\n3. **Use Make Commands**\n\n   The repository provides handy `make` commands.\n\n   - Use `make help` to check out possible options.\n\n4. **Install Poetry**\n\n   ```bash\n   make setup-poetry\n   ```\n\n5. **Install Dependencies**\n\n   ```bash\n   make install\n   ```\n\n6. **Run Hercules**\n\n   ```bash\n   make run\n   ```\n\n   - This command reads the relevant feature files from the `opt` folder and executes them, putting the output in the same folder.\n   - The `opt` folder has the following format:\n\n     ```\n     opt/\n     ├── input/\n     │   └── test.feature\n     ├── output/\n     │   ├── test.feature_result.html\n     │   └── test.feature_result.xml\n     ├── log_files/\n     ├── proofs/\n     │   └── User_opens_Google_homepage/\n     │       ├── network_logs.json\n     │       ├── screenshots/\n     │       └── videos/\n     └── test_data/\n         └── test_data.txt\n     ```\n\n7. **Interactive Mode**\n\n   You can also run Hercules in interactive mode as an instruction execution agent, which is more useful for RPA and debugging test cases and Hercules's behavior on new environments while building new tooling and extending the agents.\n\n   ```bash\n   make run-interactive\n   ```\n\n   - This will trigger an input prompt where you can chat with Hercules, and it will perform actions based on your commands.\n\n### Approach 4: Setting Up via helper_script_custom.sh\n\nFor those who want a fully automated setup experience on Linux/macOS environments, we provide a helper_script.sh. This script installs Python 3.11 (if needed), creates a virtual environment, installs TestZeus Hercules, and sets up the base project directories in an opt folder.\n\n#### Prerequisites\n\n- Ensure you have **Python 3.11** installed on your system.\n\n#### Steps to Run from helper_script_custom.sh\n1. **Download or Create the Script**\n  You can copy the script below into a file named helper_script_custom.sh:\n  ```bash\n  #!/bin/bash\n  # set -ex\n\n  # curl -sS https://bootstrap.pypa.io/get-pip.py | python3.11\n\n  # Create a new Python virtual environment named 'test'\n  python3.11 -m venv test\n\n  # Activate the virtual environment\n  source test/bin/activate\n\n  # Upgrade the 'testzeus-hercules' package\n  pip install --upgrade testzeus-hercules\n  playwright install --with-deps\n\n  # create a new directory named 'opt'\n  mkdir -p opt/input opt/output opt/test_data\n\n  # download https://raw.githubusercontent.com/test-zeus-ai/testzeus-hercules/refs/heads/main/agents_llm_config-example.json\n  curl -sS https://raw.githubusercontent.com/test-zeus-ai/testzeus-hercules/main/agents_llm_config-example.json > agents_llm_config-example.json\n  mv agents_llm_config-example.json agents_llm_config.json\n\n  # prompt user that they need to edit the 'agents_llm_config.json' file, halt the script and open the file in an editor\n  echo \"Please edit the 'agents_llm_config.json' file to add your API key and other configurations.\"\n\n  # halt the script and mention the absolute path of the agents_llm_config.json file so that user can edit it in the editor\n  echo \"The 'agents_llm_config.json' file is located at $(pwd)/agents_llm_config.json\"\n  read -p \"Press Enter if file is updated\"\n\n  # download https://raw.githubusercontent.com/test-zeus-ai/testzeus-hercules/blob/main/.env-example\n  curl -sS https://raw.githubusercontent.com/test-zeus-ai/testzeus-hercules/main/.env-example > .env-example\n  mv .env-example .env\n\n  # prompt user that they need to edit the .env file, halt the script and open the file in an editor\n  echo \"The '.env' file is located at $(pwd)/.env\"\n  read -p \"Press Enter if file is updated\"\n\n  # create an input/test.feature file\n  # download https://raw.githubusercontent.com/test-zeus-ai/testzeus-hercules/refs/heads/main/opt/input/test.feature and save in opt/input/test.feature\n  curl -sS https://raw.githubusercontent.com/test-zeus-ai/testzeus-hercules/main/opt/input/test.feature > opt/input/test.feature\n\n  # download https://raw.githubusercontent.com/test-zeus-ai/testzeus-hercules/refs/heads/main/opt/test_data/test_data.json and save in opt/test_data/test_data.json\n  curl -sS https://raw.githubusercontent.com/test-zeus-ai/testzeus-hercules/main/opt/test_data/test_data.json > opt/test_data/test_data.json\n\n  # Run the 'testzeus-hercules' command with the specified parameters\n  testzeus-hercules --project-base=opt\n  ```\n2. **Make the Script Executable and Run**\n```bash\nchmod +x helper_script.sh\n./helper_script.sh\n```\n-\tThe script will:\n\t•\tCreate a Python 3.11 virtual environment named test.\n\t•\tInstall testzeus-hercules and Playwright dependencies.\n\t•\tCreate the opt folder structure (for input/output/test data).\n\t•\tDownload sample config files: agents_llm_config.json, .env, and example feature/test data files.\n\t•\tImportant: You will be prompted to edit both agents_llm_config.json and .env files. After you've added your API keys and other custom configurations, press Enter to continue.\n\n3. **Script Output**\n\t-\tAfter completion, the script automatically runs testzeus-hercules --project-base=opt.\n\t-\tYour logs and results will appear in opt/output, opt/log_files, and opt/proofs.\n---\n\n## 📝 Configuration Details\n\nFor a comprehensive guide to all environment variables and configuration options available in TestZeus Hercules, please refer to our [Environment Variables and Configuration Guide](docs/environment_variables.md). This document provides detailed information about core environment variables, LLM configuration, browser settings, testing configuration, device configuration, logging options, and more.\n\n### Disabling Telemetry\n\nTo disable telemetry, set the `TELEMETRY_ENABLED` environment variable to `0`:\n\n```bash\nexport TELEMETRY_ENABLED=0\n```\n\n### Auto Mode\n\nIf `AUTO_MODE` is set to `1`, Hercules will not request an email during the run:\n\n```bash\nexport AUTO_MODE=1\n```\n\n### Understanding the Environment File (`.env`)\n\nTo configure Hercules in detail:\n\n- Copy the base environment file:\n\n  ```bash\n  cp .env-example .env\n  ```\n\n- Hercules is capable of running in two configuration forms:\n\n  1. **Using single LLM for all work**\n\n     - For all the activities within the agent, initialize `LLM_MODEL_NAME` and `LLM_MODEL_API_KEY`.\n     - If using a non-OpenAI hosted solution but still OpenAI LLMs (something like OpenAI via Groq), then pass the `LLM_MODEL_BASE_URL` URL as well.\n\n  2. **Custom LLMs for different work or using hosted LLMs**\n\n     - If you plan to configure local LLMs or non-OpenAI LLMs, use the other parameters like `AGENTS_LLM_CONFIG_FILE` and `AGENTS_LLM_CONFIG_FILE_REF_KEY`.\n     - These are powerful options and can affect the quality of Hercules outputs.\n\n- Hercules considers a base folder that is by default `./opt` but can be changed by the environment variable `PROJECT_SOURCE_ROOT`.\n\n- **Connecting to an Existing Chrome Instance**\n\n  - This is extremely useful when you are running Hercules in Docker for scale.\n  - You can connect Hercules running in your infrastructure to a remote browser like BrowserBase or your self-hosted grid.\n  - Use `CDP_ENDPOINT_URL` to set the CDP URL of the Chrome instance that has to be connected to the agent.\n\n- **Controlling Other Behaviors**\n\n  You can control other behaviors of Hercules based on the following environment variables:\n\n  - `HEADLESS=true`\n  - `RECORD_VIDEO=false`\n  - `TAKE_SCREENSHOTS=false`\n  - `BROWSER_TYPE=chromium` (options: `firefox`, `chromium`)\n  - `CAPTURE_NETWORK=false`\n \nFor example: If you would like to run with a \"Headful\" browser, you can set the environment variable with ```export HEADLESS=false``` before triggering Hercules.\n\n- **How to Use Tracing in Playwright**\n\n  Tracing in Playwright allows you to analyze test executions and debug issues effectively. To enable tracing in your Playwright tests, follow these steps:\n\n  1. Ensure that tracing is enabled in the configuration.\n  2. Traces will be saved to the specified path: `{proof_path}/traces/trace.zip`.\n\n  #### Enabling Tracing\n\n  To enable tracing, set the following environment variable:\n\n  ```bash\n  export ENABLE_PLAYWRIGHT_TRACING=true\n  ```\n\n### Understanding `agents_llm_config-example.json`\n\n- It's a list of configurations of LLMs that you want to provide to the agent.\n\n- Example:\n\n  ```json\n  {\n    \"anthropic\": {\n\t\t\"planner_agent\": {\n\t\t\t\"model_name\": \"claude-3-5-haiku-latest\",\n\t\t\t\"model_api_key\": \"\",\n\t\t\t\"model_api_type\": \"anthropic\",\n            \"llm_config_params\": {\n                \"cache_seed\": null,\n                \"temperature\": 0.0,\n                \"seed\":12345\n            }\n\t\t},\n\t\t\"nav_agent\": {\n\t\t\t\"model_name\": \"claude-3-5-haiku-latest\",\n\t\t\t\"model_api_key\": \"\",\n\t\t\t\"model_api_type\": \"anthropic\",\n            \"llm_config_params\": {\n                \"cache_seed\": null,\n                \"temperature\": 0.0,\n                \"seed\":12345\n            }\n\t\t}\n    }\n  }\n  ```\n\n- The key is the name of the spec that is passed in `AGENTS_LLM_CONFIG_FILE_REF_KEY`, whereas the Hercules information is passed in sub-dicts `planner_agent` and `nav_agent`.\n\n- **Note**: This option should be ignored until you are sure what you are doing. Discuss with us while playing around with these options in our Slack communication. Join us at our [Slack](https://join.slack.com/t/testzeuscommunityhq/shared_invite/zt-2v2br8wog-FAmo_76xRHx~k~1oNaGQ0Q)\n\n---\n\n\n## ⚡️ Features\n\nHercules is production ready, and packs a punch with features:\n\n### Gherkin In, Results Out\n\nHercules makes testing as simple as Gherkin in, results out. Just feed your end-to-end tests in Gherkin format, and watch Hercules spring into action. It takes care of the heavy lifting by running your tests automatically and presenting results in a neat JUnit format. No manual steps, no fuss—just efficient, seamless testing.\n\n### Free and Open Source\n\nWith Hercules, you're harnessing the power of open source with zero licensing fees. Feel free to dive into the code, contribute, or customize it to your heart's content. Hercules is as free as it is mighty, giving you the flexibility and control you need.\n\n### Salesforce Ready\n\nBuilt to handle the most intricate UIs, Hercules conquers Salesforce and other complex platforms with ease. Whether it's complicated DOM or running your SOQL or Apex, Hercules is ready and configurable.\n\n### No Code Required\n\nSay goodbye to complex scripts and elusive locators. Hercules is here to make your life easier with its no-code approach, taking care of the automation of Gherkin features so you can focus on what matters most—building quality software.\n\n### Multilingual\n\nWith multilingual support right out of the box, Hercules is ready to work with teams across the globe. Built to bridge language gaps, it empowers diverse teams to collaborate effortlessly on a unified testing platform.\n\n### Precisely Accurate\n\nHercules records video of the execution, and captures network logs as well, so that you dont have to deal with \"It works on my computer\".\n\n\n### No Maintenance\n\nAutonomous and adaptive, Hercules takes care of itself with auto-healing capabilities. Forget about tedious maintenance—Hercules adjusts to changes and stays focused on achieving your testing goals.\n\n### UI Assertions\n\nGrounded in the powerful foundations of TestZeus, Hercules tackles UI assertions with unwavering focus, ensuring that no assertion goes unchecked and no bug goes unnoticed. It's thorough, it's sharp, and it's ready for action.\n\n### CI/CD Ready\n\nRun Hercules locally or integrate it seamlessly into your CI/CD pipeline. Docker-native and one-command ready, Hercules fits smoothly into your deployment workflows, keeping testing quick, consistent, and hassle-free.\n\nWith Hercules, testing is no longer just a step in the process—it's a powerful, streamlined experience that brings quality to the forefront.\n\n### Mobile Device Emulation\n\nHercules supports running the browser in \"mobile mode\" for a variety of device types. Playwright provides a large list of device descriptors here:\n[List of mobile devices supported](https://github.com/microsoft/playwright/blob/main/packages/playwright-core/src/server/deviceDescriptorsSource.json)\n\n#### Enabling Mobile Mode\nSet the RUN_DEVICE environment variable in your .env (or directly as an environment variable):\n```bash\nRUN_DEVICE=\"iPhone 15 Pro Max\"\n```\nWhen Hercules runs, it will now launch the browser with the corresponding viewport and user-agent for the chosen device, simulating a real mobile environment.\nHere's a quick demo:\n![MobileDemo](statics/mobiledemo.gif)\n\n#### Attaching Advanced Tools\nHercules can be extended with more powerful tools for advanced scenarios. Enable it by setting:\n```bash\nLOAD_EXTRA_TOOLS=true\n```\nWith LOAD_EXTRA_TOOLS enabled, Hercules looks for additional tool modules that can expand capabilities (e.g., geolocation handling).\n\nFor location-based tests, configure:\nsupported geo providers are maps.co and google\n```bash\nGEO_API_KEY=somekey\nGEO_PROVIDER=maps_co\n```\nThis allows Hercules to alter or simulate user location during test execution, broadening your test coverage for scenarios that rely on user geography.\n\nNote: If you are looking for native app test automation, we've got you covered, as we've built out early support for Appium powered native app automation [here](https://github.com/test-zeus-ai/testzeus-hercules/pull/41)\n\n---\n\n\n## 🦾 Architecture\n\n### Multi-Agentic Solution\n\nHercules leverages a multi-agent architecture based on the AutoGen framework. Building on the foundation provided by the AutoGen framework, Hercules's architecture leverages the interplay between tools and agents. Each tool embodies an atomic action, a fundamental building block that, when executed, returns a natural language description of its outcome. This granularity allows Hercules to flexibly assemble these tools to tackle complex web automation workflows.\n\n#### System View\n\n![Architecture Diagram](statics/assets/hercules-architecture.jpg)\n\nThe diagram above shows the configuration chosen on top of AutoGen architecture. The tools can be partitioned differently, but this is the one that we chose for the time being. We chose to use tools that map to what humans learn about the web browser rather than allow the LLM to write code as it pleases. We see the use of configured tools to be safer and more predictable in its outcomes. Certainly, it can click on the wrong things, but at least it is not going to execute malicious unknown code.\n\n\n#### Agents\n\nAt the moment, there are two agents:\n\n1. **Planner Agent**: Executes the planning and decomposition of tasks.\n2. **Browser Navigation Agent**: Embodies all the tools for interacting with the web browser.\n\n#### Tools Library\n\nAt the core of Hercules's capabilities is the Tools Library, a repository of well-defined actions that Hercules can perform; for now, web actions. These tools are grouped into two main categories:\n\n- **Sensing Tools**: Tools like `get_dom_with_content_type` and `geturl` that help Hercules understand the current state of the webpage or the browser.\n- **Action Tools**: Tools that allow Hercules to interact with and manipulate the web environment, such as `click`, `enter_text`, and `openurl`.\n\nEach tool is created with the intention to be as conversational as possible, making the interactions with LLMs more intuitive and error-tolerant. For instance, rather than simply returning a boolean value, a tool might explain in natural language what happened during its execution, enabling the LLM to better understand the context and correct course if necessary.\n\n##### Implemented Tools\n\n- **Sensing Tools**\n\n  - `geturl`: Fetches and returns the current URL.\n  - `get_dom_with_content_type`: Retrieves the HTML DOM of the active page based on the specified content type.\n    - `text_only`: Extracts the inner text of the HTML DOM. Responds with text output.\n    - `input_fields`: Extracts the interactive elements in the DOM (button, input, textarea, etc.) and responds with a compact JSON object.\n    - `all_fields`: Extracts all the fields in the DOM and responds with a compact JSON object.\n  - `get_user_input`: Provides the orchestrator with a mechanism to receive user feedback to disambiguate or seek clarity on fulfilling their request.\n\n- **Action Tools**\n\n  - `click`: Given a DOM query selector, this will click on it.\n  - `enter_text`: Enters text in a field specified by the provided DOM query selector.\n  - `enter_text_and_click`: Optimized method that combines `enter_text` and `click` tools.\n  - `bulk_enter_text`: Optimized method that wraps `enter_text` method so that multiple text entries can be performed in one shot.\n  - `openurl`: Opens the given URL in the current or new tab.\n\n#### DOM Distillation\n\nHercules's approach to managing the vast landscape of HTML DOM is methodical and essential for efficiency. We've introduced **DOM Distillation** to pare down the DOM to just the elements pertinent to the user's task.\n\nIn practice, this means taking the expansive DOM and delivering a more digestible JSON snapshot. This isn't about just reducing size; it's about honing in on relevance, serving the LLMs only what's necessary to fulfill a request. So far, we have three content types:\n\n1. **Text Only**: For when the mission is information retrieval, and the text is the target. No distractions.\n2. **Input Fields**: Zeroing in on elements that call for user interaction. It's about streamlining actions.\n3. **All Content**: The full scope of distilled DOM, encompassing all elements when the task demands a comprehensive understanding.\n\nIt's a surgical procedure, carefully removing extraneous information while preserving the structure and content needed for the agent's operation. Of course, with any distillation, there could be casualties, but the idea is to refine this over time to limit/eliminate them.\n\nSince we can't rely on all web page authors to use best practices, such as adding unique IDs to each HTML element, we had to inject our own attribute (`md`) in every DOM element. We can then guide the LLM to rely on using `md` in the generated DOM queries.\n\nTo cut down on some of the DOM noise, we use the **DOM Accessibility Tree** rather than the regular HTML DOM. The accessibility tree, by nature, is geared towards helping screen readers, which is closer to the mission of web automation than plain old HTML DOM.\n\nThe distillation process is a work in progress. We look to refine this process and condense the DOM further, aiming to make interactions faster, cost-effective, and more accurate.\n\n---\n### Non-Functional Capabilities\n\n#### Security Testing via Nuclei  \nHercules integrates with **Nuclei** to automate vulnerability scanning directly from Gherkin test cases, identifying issues like **misconfigurations, OWASP Top 10 vulnerabilities**, and API flaws. Security reports are generated alongside testing outputs for seamless CI/CD integration.\n\n#### Accessibility Testing  \nHercules supports **WCAG 2.0, 2.1, and 2.2** at **A, AA, and AAA levels**, enabling accessibility testing to ensure compliance with global standards. It identifies accessibility issues early, helping build inclusive and user-friendly applications.\n\n---\n\n## 🔬 Testing and Evaluation: QEvals\n\nWe wanted to ensure that Hercules stands up to the task of end-to-end testing with immense precision. So, we have run Hercules through a wide range of tests such as running APIs, interacting with complex UI scenarios, clicking through calendars, or iframes. A full list of evaluations can be found in the [tests folder](<Link to tests folder>).\n\n### Running Tests\n\nTo run the full test suite, use the following command:\n\n```bash\nmake test\n```\n\nTo run a specific test:\n\n```bash\nmake test-case\n```\n\nHercules builds on the work done by WebArena and Agent-E, and beyond that, to iron out the issues in the previous, we have written our own test cases catering to complex QA scenarios and have created tests in the `./tests` folder.\n\n---\n\n## 💡 Opinions\n\nWe believe that great quality comes from opinions about a product. So we have incorporated a few of our opinions into the product design. We welcome the community to question them, use them, or build on top of them. Here are some examples:\n\n1. **Gherkin is a Good Enough Format for Agents**: Gherkin provides a semi-structured format for the LLMs/AI Agents to follow test intent and user instructions. It provides the right amount of grammar (verbs like Given, When, Then) for humans to frame a scenario and agents to follow the instructions.\n\n2. **Tests Should Be Atomic in Nature**: Software tests should be atomic because it ensures that each test is **focused, independent, and reliable**. Atomic tests target one specific behavior or functionality, making it easier to pinpoint the root cause of failures without ambiguity.\n\n   **Here's a good example (Atomic Test)**:\n\n   ```gherkin\n   Feature: User Login\n\n   Scenario: Successful login with valid credentials\n\n     Given the user is on the login page\n     When the user enters valid credentials\n     And the user clicks the login button\n     Then the user should see the dashboard\n   ```\n\n   A non-atomic test confuses both the tester and the AI agent.\n\n3. **Open Core and Open Source**: Hercules is built on an **open-core model**, combining the spirit of open source with the support and expertise of a commercial company, **TestZeus**. By providing Hercules as open source (licensed under AGPL v3), TestZeus is committed to empowering the testing community with a robust, adaptable tool that's freely accessible and modifiable. Open source offers transparency, trust, and collaborative potential, allowing anyone to dive into the code, contribute, and shape the project's direction.\n\n4. **Telemetry** : All great products are built on good feedback. We have setup telemetry so that we can take feedback, without disturbing the user. Telemetry is enabled by default, but we also believe strongly in the values of \"Trust\" and \"Transparency\" so it can be turned off by the users.\n5. **Prompting Hercules** : A detailed guide to write tests for Hercules can be found on our blog [here](https://testzeus.com/blog/mastering-ai-driven-testing-writing-effective-tests-for-hercules) .\n\n---\n\n## 🪙 Token Usage\n\nHercules is an AI-native solution and relies on LLMs to perform reasoning and actions. Based on our experiments, we have found that a complex use case as below could cost up to **$0.20** using OpenAI's APIs gpt-4o, check the properties printed in testcase output to calculate for your testcase:\n\n```gherkin\nFeature: Account Creation in Salesforce\n\n Scenario: Successfully create a new account\n\n   Given I am on the Salesforce login page\n   When I enter my username \"user@example.com\" and password \"securePassword\"\n   And I click on the \"Log In\" button\n   And I navigate to the \"Accounts\" tab\n   And I click on the \"New\" button\n   And I fill in the \"Account Name\" field with \"Test Account\"\n   And I select the \"Account Type\" as \"Customer\"\n   And I fill in the \"Website\" field with \"www.testaccount.com\"\n   And I fill in the \"Phone\" field with \"123-456-7890\"\n   And I click on the \"Save\" button\n   Then I should see a confirmation message \"Account Test Account created successfully\"\n   And I should see \"Test Account\" listed in the account records\n```\n\n---\n\n## ⭐️ Difference from Other Tools\n\nHercules isn't just another testing tool—it's an **agent**. Powered by synthetic intelligence that can **think, reason, and react** based on requirements, Hercules goes beyond simple automation scripts. We bring an industry-first approach to open-source agents for **software testing**. This means faster, smarter, and more resilient testing cycles, especially for complex platforms.\n\nWith **industry-leading performance** and a fully open-source foundation, Hercules combines powerful capabilities with community-driven flexibility, making top-tier testing accessible and transformative for everyone.\n\n---\n\n## 🚀 High-Level Roadmap\n\n- **Enhanced LLM Support**: Integration with more LLMs and support for local LLM deployments.\n- **Advanced Tooling**: Addition of more tools to handle complex testing scenarios and environments.\n- **Improved DOM Distillation**: Refinements to the DOM distillation process for better efficiency and accuracy.\n- **Community Contributions**: Encourage and integrate community-driven features and tools.\n- **Extensive Documentation**: Expand documentation for better onboarding and usage.\n- **Bounty Program**: Launch a bounty program to incentivize contributions.\n\n---\n\n## 🤗 Contribution\n\nWe welcome contributions from the community!\n\n- **Read the [CONTRIBUTING.md](CONTRIBUTING.md) file** to get started.\n- **Bounty Program**: Stay tuned for upcoming opportunities! 😀\n\n### How to Contribute Back\n\n1. **Developing Tools**\n\n   - If you are developing tools for Hercules and want to contribute to the community, make sure you place the new tools in the `additional_tools` folder in your Pull Request.\n\n2. **Fixes and Enhancements**\n\n   - If you have a fix on sensing tools that are fundamental to the system or something in prompts or something in the DOM distillation code, then put the changes in the relevant file and share the Pull Request.\n\n### Extending and Attaching Tools\n\n1. **Creating a New Tool**\n\n   - You can start extending by adding tools to Hercules.\n   - Refer to `testzeus_hercules/core/tools/sql_calls.py` as an example of how to create a new tool.\n   - The key is the decorator `@tool` over the method that you want Hercules to execute.\n   - The tool decorator should have a very clear description and name so that Hercules knows how to use the tool.\n   - Also, in the method, you should be clear with annotations on what parameter is used for what purpose so that function calling in the LLM works best.\n\n2. **Adding the Tool**\n\n   - Once you have created the new tools files in some folder path, you can pass the folder path to Hercules in the environment variable so that Hercules can read the new tools during the boot time and make sure that they are available during the execution.\n   - Use `ADDITIONAL_TOOL_DIRS` to pass the path of the new tools folder where you have kept the new files.\n\n3. **Direct Addition (Not Recommended)**\n\n   - In case you opt for adding the tools directly, then just put your new tools in the `testzeus_hercules/core/tools` path of the cloned repository.\n\n   - **Note**: This way is not recommended. We prefer you try to use the `ADDITIONAL_TOOL_DIRS` approach.\n\n---\n\n## 🤙 Contact Us\n\nJoin us at our [Slack](https://join.slack.com/t/testzeuscommunityhq/shared_invite/zt-2v2br8wog-FAmo_76xRHx~k~1oNaGQ0Q) to connect with the community, ask questions, and contribute.\n\n---\n\n## ✍️ Examples\n\n- **Salesforce Examples**: [Link](tests/test_not_for_ci/ebikes/ebikes.feature)\n- **Wrangler Example**: [Link](tests/test_features/productSearch/productSearch.feature)\n\n---\n\n## 🙇 Credits\n\nHercules would not have been possible without the great work from the following sources:\n\n1. [Agent-E](https://arxiv.org/abs/2407.13032)\n2. [Q*](https://arxiv.org/abs/2312.10868)\n3. [Agent Q](https://arxiv.org/abs/2408.07199)\n4. [Autogen](https://arxiv.org/pdf/2308.08155)\n\nThe Hercules project is inferred and enhanced over the existing project of [Agent-E](https://github.com/EmergenceAI/Agent-E). We have improved lots of cases to make it capable of doing testing, especially in the area of complex DOM navigation and iframes. We have also added new tools and abilities (like Salesforce navigation) to Hercules so that it can perform better work over the base framework we had picked.\n\nHercules also picks some inspiration from the legacy TestZeus repo [here](https://www.testzeus.org).\n\n\n---\n\nWith Hercules, testing is no longer just a step in the process—it's a powerful, streamlined experience that brings quality to the forefront.\n\n\n## 🚀 Footnotes: On virtual environments and other important commands in Python\n\nIf you're coming from a **Java** or **JavaScript** background, working with Python might feel a bit different at first—but don't worry! Python's simplicity, combined with powerful tools like virtual environments, makes managing dependencies easy and efficient.\n\n### 🎓 **Understanding Virtual Environments**\n\nIn Java, you might use tools like Maven or Gradle to manage project dependencies, or in JavaScript, you'd use `npm` or `yarn`. In Python, **virtual environments** serve a similar purpose. They allow you to create isolated spaces for your project's dependencies, avoiding conflicts with other Python projects on your system.\n\nThink of it like a sandboxed environment where TestZeus Hercules and its dependencies live independently from other Python packages.\n\n\n### 🏗️ **Setup Instructions**\n\n#### ✅ **1. Prerequisites**\n\nFirst, ensure Python 3.11 or higher is installed. You can verify this by running:\n\n```bash\npython --version\n# or\npython3 --version\n```\n\nIf Python isn't installed, [download it here](https://www.python.org/downloads/) or if you are on Windows, just follow the instructions [here](https://github.com/test-zeus-ai/testzeus-hercules/#%EF%B8%8F-running-hercules-on-a-windows-machine).\n\n---\n\n#### 🏰 **2. Creating and Activating a Virtual Environment**\n\nTo set up a clean Python environment for your project:\n\n```bash\n# Create a virtual environment named 'venv'\npython -m venv venv\n\n# Activate the virtual environment\n# On Windows:\nvenv\\Scripts\\activate\n\n# On macOS/Linux:\nsource venv/bin/activate\n```\n\nYou'll notice your terminal prompt changes—this indicates the virtual environment is active.\n\nTo **deactivate** the virtual environment later, simply run:\n```bash\ndeactivate\n```\n\n---\n\n#### 📥 **3. Installing TestZeus Hercules**\n\nOnce your virtual environment is activated, install the latest version of TestZeus Hercules directly from PyPI:\n\n```bash\npip install testzeus-hercules\n```\n\n---\n\n#### 🔄 **4. Updating TestZeus Hercules**\n\nTo update to the latest version, use:\n\n```bash\npip install --upgrade testzeus-hercules\n```\n\n---\n\n#### ❌ **5. Uninstalling TestZeus Hercules**\n\nIf you need to remove the package:\n\n```bash\npip uninstall testzeus-hercules\n```\n\n---\n\n#### 🔍 **6. Checking Installed Packages**\n\nTo verify that TestZeus Hercules is installed and check its version:\n\n```bash\npip list\n```\n\n---\n\n### 🔗 **Additional Tips for Java/JS Developers**\n\n- **`pip`** is like `npm` or `mvn`—it's used for installing and managing Python packages.\n- Python's simplicity means fewer configuration files—most things can be done directly from the command line.\n\n---\n\n### 💡 **Next Steps**\n\nNow that TestZeus Hercules is installed and ready to go, dive into [our documentation](https://github.com/test-zeus-ai/testzeus-hercules/) to learn how to create and run your first test cases with ease!\n\n---\n\n## Star History\n\n[![Star History Chart](https://api.star-history.com/svg?repos=test-zeus-ai/testzeus-hercules&type=Date)](https://star-history.com/#test-zeus-ai/testzeus-hercules&Date)\n\n## ✅ Citation\nIf you use Hercules in your research or project, please cite:\n\n```\n@software{testzeus_hercules2024,\n  author = {Agnihotri, Shriyansh and Gupta, Robin},\n  title = {Hercules: World's first open source testing agent},\n  year = {2024},\n  publisher = {GitHub},\n  url = {https://github.com/test-zeus-ai/testzeus-hercules/}\n}\n```\n"
    },
    {
      "name": "i-am-bee/beeai-platform",
      "stars": 440,
      "img": "https://avatars.githubusercontent.com/u/178592583?s=40&v=4",
      "owner": "i-am-bee",
      "repo_name": "beeai-platform",
      "description": "Discover, run, and compose AI agents from any framework",
      "homepage": "https://beeai.dev",
      "language": "Python",
      "created_at": "2025-01-27T17:21:19Z",
      "updated_at": "2025-04-23T08:12:09Z",
      "topics": [
        "agents",
        "compose",
        "platform",
        "protocol"
      ],
      "readme": "<h1 align=\"center\">\n  <picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://raw.githubusercontent.com/i-am-bee/beeai/master/docs/logo/beeai_logo_white.svg\">\n    <source media=\"(prefers-color-scheme: light)\" srcset=\"https://raw.githubusercontent.com/i-am-bee/beeai/master/docs/logo/beeai_logo_black.svg\">\n    <img alt=\"BeeAI\" src=\"https://raw.githubusercontent.com/i-am-bee/beeai/master/docs/logo/beeai_logo_black.svg\" width=\"60\"><br><br>\n  </picture>\n  BeeAI\n</h1>\n\n<h4 align=\"center\">Discover, run, and compose AI agents from any framework</h4>\n\n<div align=\"center\">\n\n[![Apache 2.0](https://img.shields.io/badge/Apache%202.0-License-EA7826?style=plastic&logo=apache&logoColor=white)](https://github.com/i-am-bee/beeai-framework?tab=Apache-2.0-1-ov-file#readme)\n[![Follow on Bluesky](https://img.shields.io/badge/Follow%20on%20Bluesky-0285FF?style=plastic&logo=bluesky&logoColor=white)](https://bsky.app/profile/beeaiagents.bsky.social)\n[![Join our Discord](https://img.shields.io/badge/Join%20our%20Discord-7289DA?style=plastic&logo=discord&logoColor=white)](https://discord.com/invite/NradeA6ZNF)\n[![LF AI & Data](https://img.shields.io/badge/LF%20AI%20%26%20Data-0072C6?style=plastic&logo=linuxfoundation&logoColor=white)](https://lfaidata.foundation/projects/)\n\n</div>\n\n<p align=\"center\">\n    <a href=\"#key-features\">Key features</a> •\n    <a href=\"#quickstart\">Quickstart</a> •\n    <a href=\"#documentation\">Documentation</a> •\n    <a href=\"#agent-library\">Agent library</a>\n</p>\n\n<div align=\"center\">\n\nhttps://github.com/user-attachments/assets/dc6cc4d7-4577-44c9-acf7-ec0133268a2d\n\n</div>\n\nBeeAI is an open platform to help you discover, run, and compose AI agents from any framework and language. Whether building your agents or looking for powerful existing solutions, BeeAI makes it easy to find, connect, and orchestrate AI agents seamlessly.\n\n## Key features\n\n- 🌐 **Framework agnostic**: Integrate AI agents seamlessly, no matter the language or platform.\n- ⚙️ **Composition**: Build complex, multi-agent workflows from simple building blocks.\n- 🔍 **Discoverability**: Explore a [powerful agent catalog](https://beeai.dev/agents) with integrated search.\n- 🐝 **BeeAI ecosystem:** First-class support for [Python](https://github.com/i-am-bee/beeai-framework/tree/main/python) and [TypeScript](https://github.com/i-am-bee/beeai-framework/tree/main/typescript) agent developers via [BeeAI Framework](https://github.com/i-am-bee/beeai-framework).\n\n## Quickstart\n\n1. **Install** BeeAI using [Homebrew](https://brew.sh/) (or see the [installation guide](https://docs.beeai.dev/introduction/installation) for other methods):\n\n```sh\nbrew install i-am-bee/beeai/beeai\nbrew services start beeai\n```\n\n2. **Configure** LLM provider:\n\n```sh\nbeeai env setup\n```\n\n3. **Launch** the web interface:\n\n```sh\nbeeai ui\n```\n\n4. **Use** from the terminal:\n\n```sh\n# List commands\nbeeai --help\n\n# List all available agents\nbeeai list\n\n# Run the chat agent\nbeeai run chat\n\n# Compose agents\nbeeai compose sequential\n```\n\n## Documentation\n\nVisit [docs.beeai.dev](https://docs.beeai.dev) for full documentation.\n\n## Agent library\n\nVisit [beeai.dev/agents](https://beeai.dev/agents) for the list of reference agent implementations.\n\n## Community\n\nThe BeeAI community is active on [GitHub Discussions](https://github.com/i-am-bee/beeai/discussions) where you can ask questions, voice ideas, and share your projects.\n\nTo chat with other community members, you can join the BeeAI [Discord](https://discord.gg/AZFrp3UF5k) server.\n\nPlease note that our [Code of Conduct](./CODE_OF_CONDUCT.md) applies to all BeeAI community channels. We strongly encourage you to read and follow it.\n\n## Maintainers\n\nFor information about maintainers, see [MAINTAINERS.md](./MAINTAINERS.md).\n\n## Contributing\n\nContributions to BeeAI are always welcome and greatly appreciated. Before contributing, please review our [Contribution Guidelines](./CONTRIBUTING.md) to ensure a smooth experience.\n\nSpecial thanks to our contributors for helping us improve BeeAI.\n\n<a href=\"https://github.com/i-am-bee/beeai/graphs/contributors\">\n  <img alt=\"Contributors list\" src=\"https://contrib.rocks/image?repo=i-am-bee/beeai\" />\n</a>\n\n## Acknowledgements\n\nSpecial thanks to the following outstanding projects for their inspiration and influence:\n\n- [Model Context Protocol](https://github.com/modelcontextprotocol)\n- [Language Server Protocol](https://github.com/microsoft/language-server-protocol)\n- [JSON-RPC](https://www.jsonrpc.org/)\n- [Natural Language Interaction Protocol](https://github.com/nlip-project)\n\n---\n\nDeveloped by contributors to the BeeAI project, this initiative is part of the [Linux Foundation AI & Data program](https://lfaidata.foundation/projects/). Its development follows open, collaborative, and community-driven practices.\n"
    },
    {
      "name": "axzxs2001/Asp.NetCoreExperiment",
      "stars": 357,
      "img": "https://avatars.githubusercontent.com/u/522232?s=40&v=4",
      "owner": "axzxs2001",
      "repo_name": "Asp.NetCoreExperiment",
      "description": "原来所有项目都移动到**OleVersion**目录下进行保留。新的案例装以.net 5.0为主，一部分对以前案例进行升级，一部分将以前的工作经验总结出来，以供大家参考！",
      "homepage": "",
      "language": "C#",
      "created_at": "2017-03-05T04:42:04Z",
      "updated_at": "2025-04-21T07:33:31Z",
      "topics": [],
      "readme": "# Asp.NetCoreExperiment\n>_注意：为了提供更新，更精准的.net知识，特别对本项目文件进行了整理，原来所有项目都移动到**OleVersion**目录下进行保留。\n\n新的案例装以.net 6.0为主，一部分对以前案例进行升级，一部分将以前的工作经验总结出来，以供大家参考！\n\n[项目说明文档](https://github.com/axzxs2001/Asp.NetCoreExperiment/wiki)\n\n\n<a href=\"https://info.flagcounter.com/6o8P\"><img src=\"https://s11.flagcounter.com/count2/6o8P/bg_FFFFFF/txt_000000/border_CCCCCC/columns_2/maxflags_10/viewers_0/labels_0/pageviews_0/flags_0/percent_0/\" alt=\"Flag Counter\" border=\"0\"></a>\n"
    },
    {
      "name": "sambanova/ai-starter-kit",
      "stars": 220,
      "img": "https://avatars.githubusercontent.com/u/80118584?s=40&v=4",
      "owner": "sambanova",
      "repo_name": "ai-starter-kit",
      "description": null,
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2023-10-06T16:34:46Z",
      "updated_at": "2025-04-21T21:18:57Z",
      "topics": [],
      "readme": "<a href=\"https://sambanova.ai/\">\n<picture>\n  <source media=\"(prefers-color-scheme: dark)\" srcset=\"./images/SambaNova-light-logo-1.png\" height=\"60\">\n  <img alt=\"SambaNova logo\" src=\"./images/SambaNova-dark-logo-1.png\" height=\"60\">\n</picture>\n</a>\n\n# SambaNova AI Starter Kits\n\n# Overview\n\nSambaNova AI Starter Kits are a collection of open-source examples and guides designed to facilitate the deployment of AI-driven use cases for both developers and enterprises.\n\nTo run these examples, you can obtain a free API key using [SambaNova Cloud](https://cloud.sambanova.ai). Alternatively, if you are a current SambaNova customer, you can deploy your models using [SambaStudio](https://docs.sambanova.ai/sambastudio/latest/index.html). Most of the code examples are written in Python, although the concepts can be applied to any programming language.\n\nQuestions? Just <a href=\"https://community.sambanova.ai/latest\" target=\"_blank\">message us</a> on SambaNova Community <a href=\"https://community.sambanova.ai/latest\" target=\"_blank\"><img src=\"https://github.com/sambanova/ai-starter-kit/assets/150964187/aef53b52-1dc0-4cbf-a3be-55048675f583\" alt=\"Community\" width=\"22\"/></a> or <a href=\"https://github.com/sambanova/ai-starter-kit/issues/new/choose\" target=\"_blank\">create an issue</a> in GitHub. We're happy to help live!\n\n# Available AI Starter Kits\n\nThe table below lists the available kits, which are grouped into four categories: 1) Data Ingestion & Preparation, 2) Model Development & Optimization, 3) Intelligent Information Retrieval, and 4) Advanced AI Capabilities.\n\nFor functionalities related to third-party integrations, find a list in our [Integrations folder](https://github.com/sambanova/integrations).\n\n**Note**: For each kit, we specify whether it is compatible with SambaNova Cloud, SambaStudio, or both.\n\n<table style=\"width: 100%;\">\n<thead>\n<tr>\n<th width=\"20%\">Name</th>\n<th width=\"45%\">Kit Description</th>\n<th width=\"20%\">Compatible APIs</th>\n<th width=\"15%\">Category</th>\n  \n</tr>\n</thead>\n  \n<tbody>\n\n<tr>\n<td width=\"20%\"><a href=\"data_extraction/README.md\">Data Extraction</a></td>\n<td width=\"40%\">Series of notebooks that demonstrate methods for extracting text from documents in different input formats.</td>\n<td width=\"20%\"> SambaNova Cloud, SambaStudio</td>\n<td width=\"20%\"> Data Ingestion & Preparation </td>  \n</tr>\n\n<tr>\n<td width=\"20%\"><a href=\"e2e_fine_tuning/README.md\">End to End Fine-tuning</a></td>\n<td width=\"40%\"> Recipe to upload and train a Language Model (LLM) using your own data in SambaStudio platform. </td>\n<td width=\"20%\"> SambaStudio </td>\n<td width=\"20%\"> Data Ingestion & Preparation </td>\n</tr>\n\n<tr>\n<td width=\"20%\"><a href=\"fine_tuning_embeddings/README.md\"> Fine tuning embeddings</a></td>\n<td width=\"40%\">Example workflow for fine-tuning embeddings from unstructured data, leveraging Large Language Models (LLMs) and open-source embedding models to enhance NLP task performance.</td>\n<td width=\"20%\">SambaStudio</td>\n<td width=\"20%\"> Model Development & Optimization </td>  \n</tr>\n\n<tr>\n<td width=\"20%\"><a href=\"fine_tuning_sql/README.md\"> Fine tuning SQL</a></td>\n<td width=\"40%\">Example workflow for fine-tuning an SQL model for Question-Answering purposes, leveraging Large Language Models (LLMs) and open-source embedding models to enhance SQL generation task performance.</td>\n<td width=\"20%\">SambaStudio</td>\n<td width=\"20%\"> Model Development & Optimization </td>\n</tr>\n\n<tr>\n<td width=\"20%\"><a href=\"prompt_engineering/README.md\">Prompt Engineering</a></td>\n</td>\n<td width=\"40%\">Starting point demo for prompt engineering using SambaNova's API to experiment with different use case templates. Provides useful resources to improve prompt crafting, making it an ideal entry point for those new to this AISK.</td>\n<td width=\"20%\"> SambaNova Cloud, SambaStudio</td>\n<td width=\"20%\"> Model Development & Optimization </td> \n</tr>\n\n<tr>\n<td width=\"20%\"><a href=\"enterprise_knowledge_retriever/README.md\">Enterprise Knowledge Retrieval</a></td>\n<td width=\"40%\">Sample implementation of the semantic search workflow using the SambaNova platform to get answers to questions about your documents. Includes a runnable demo.</td>\n<td width=\"20%\"> SambaNova Cloud, SambaStudio</td>\n<td width=\"20%\"> Intelligent Information Retrieval </td>  \n</tr>\n\n<tr>\n<td width=\"20%\"><a href=\"image_search/README.md\">Image Search</a></td>\n<td width=\"40%\">This example workflow shows a simple approach to image search by image description or image similarity. All workflows are built using the SambaNova platform. </td>\n<td width=\"20%\"> SambaStudio </td>\n<td width=\"20%\"> Intelligent Information Retrieval </td>   \n</tr>\n\n<tr>\n<td width=\"20%\"><a href=\"multimodal_knowledge_retriever/README.md\">Multimodal Knowledge Retriever</a></td>\n<td width=\"40%\"> Sample implementation of the semantic search workflow leveraging the SambaNova platform to get answers using text, tables, and images to questions about your documents. Includes a runnable demo. </td>\n<td width=\"20%\"> SambaNova Cloud, SambaStudio</td>\n<td width=\"20%\"> Intelligent Information Retrieval </td>   \n</tr>\n\n<tr>\n<td width=\"20%\"><a href=\"post_call_analysis/README.md\">Post Call Analysis</a></td>\n<td width=\"40%\">Example workflow that shows a systematic approach to post-call analysis including Automatic Speech Recognition (ASR), diarization, large language model analysis, and retrieval augmented generation (RAG) workflows. All workflows are built using the SambaNova platform. </td>\n<td width=\"20%\">SambaNova Cloud, SambaStudio</td>\n<td width=\"20%\"> Intelligent Information Retrieval </td>   \n</tr>\n\n<tr>\n<td width=\"20%\"><a href=\"eval_jumpstart/README.md\">RAG Evaluation Kit</a></td>\n<td width=\"40%\">A tool for evaluating the performance of LLM APIs using the RAG Evaluation methodology.</td>\n<td width=\"20%\">SambaStudio</td>\n<td width=\"20%\"> Intelligent Information Retrieval </td>  \n</tr>\n\n<tr>\n<td width=\"20%\"><a href=\"search_assistant/README.md\">Search Assistant</a></td>\n<td width=\"40%\">Sample implementation of the semantic search workflow built using the SambaNova platform to get answers to your questions using search engine snippets, and website crawled information as the source. Includes a runnable demo.</td>\n<td width=\"20%\">SambaNova Cloud, SambaStudio</td>\n<td width=\"20%\"> Intelligent Information Retrieval </td>   \n</tr>\n\n<tr>\n<td width=\"20%\"><a href=\"web_crawled_data_retriever/README.md\">Web Crawled Data Retrieval</a></td>\n<td width=\"40%\">Sample implementation of a semantic search workflow built using the SambaNova platform to get answers to your questions using website crawled information as the source. Includes a runnable demo.</td>\n<td width=\"20%\">SambaNova Cloud, SambaStudio</td>\n<td width=\"20%\"> Intelligent Information Retrieval </td>   \n</tr>\n\n<tr>\n<td width=\"20%\"><a href=\"benchmarking/README.md\">Benchmarking</a></td>\n<td width=\"40%\">This kit evaluates the performance of multiple LLM models hosted in SambaStudio. It offers various performance metrics and configuration options. Users can also see these metrics within a chat interface.</td>\n<td width=\"20%\"> SambaNova Cloud, SambaStudio</td>\n<td width=\"20%\"> Advanced AI Capabilities </td>    \n</tr>\n\n<tr>\n<td width=\"20%\"><a href=\"https://github.com/sambanova/integrations/tree/main/continue\">Code Copilot</a></td>\n<td width=\"40%\">This example guide shows a simple integration with Continue VSCode and JetBrains extension using SambaNova platforms, to use Sambanova's hosted models as your custom coding assistant. </td>\n<td width=\"20%\"> SambaNova Cloud, SambaStudio</td>\n<td width=\"20%\"> Integrations </td>  \n</tr>\n\n<tr>\n<td width=\"20%\"><a href=\"bundle_jump_start/README.md\">Bundle jump start</a></td>\n<td width=\"40%\">This kit demonstrates how to call <a href=https://coe-1.Cloud.snova.ai/>SambaNova Bundle</a> models using the Langchain framework. The script offers different approaches for calling Bundle models, including using SambaStudio with a named expert, and using SambaStudio with routing.</td>\n<td width=\"20%\">SambaStudio</td>\n<td width=\"20%\"> Advanced AI Capabilities </td>  \n</tr>\n\n<tr>\n<td width=\"20%\"><a href=\"financial_assistant/README.md\"> Financial Assistant</a></td> \n<td width=\"40%\">This app demonstrates the capabilities of LLMs in extracting and analyzing financial data using function calling, web scraping, and RAG.</td>\n<td width=\"20%\"> SambaNova Cloud, SambaStudio</td>\n<td width=\"20%\"> Advanced AI Capabilities </td>\n</tr>\n\n<tr>\n<td width=\"20%\"><a href=\"function_calling/README.md\"> Function Calling</a></td>\n<td width=\"40%\">Example of tools calling implementation and a generic function calling module that can be used inside your application workflows.</td>\n<td width=\"20%\"> SambaNova Cloud, SambaStudio</td>\n<td width=\"20%\"> Advanced AI Capabilities </td>\n</tr>\n\n<tr>\n<td width=\"20%\"><a href=\"sambanova_scribe/README.md\"> SambaNova Scribe</a></td>\n<td width=\"40%\">Example implementation of a transcription and summarization workflow.</td>\n<td width=\"20%\"> SambaNova Cloud</td>\n<td width=\"20%\"> Advanced AI Capabilities </td>\n</tr>\n\n<tr>\n<td width=\"20%\"><a href=\"google_integration/README.md\"> SambaCloud - Google Integration</a></td>\n<td width=\"40%\">App Scripts intended for those with SambaCloud API keys to integrate LLMs into Google Workspaces.</td>\n<td width=\"20%\"> SambaNova Cloud</td>\n<td width=\"20%\"> Advanced AI Capabilities </td>\n</tr>\n\n</tbody>\n</table>\n\n# Getting Started \n\nGo to [SambaNova Cloud Quickstart Guide](./quickstart/README.md) If is your first time using the AI State Kits and you want to try out simple examples. Follow the next steps to read more detailed instructions or if you ar a SambaStudio user. \n\n## Getting a SambaNova API key and setting your generative models\n\nCurrently, there are two ways to obtain an API key from SambaNova. You can get a free API key using SambaNova Cloud. Alternatively, if you are a current SambaNova customer, you can deploy your models using SambaStudio. \n\n### Use SambaNova Cloud (Option 1)\n\nFor more information and to obtain your API key, visit the [SambaNova Cloud webpage](https://cloud.sambanova.ai).\n\nTo integrate SambaNova Cloud LLMs with this AI starter kit, update the API information by configuring the environment variables in the `ai-starter-kit/.env` file:\n\n- Create the `.env` file at `ai-starter-kit/.env` if the file does not exist.\n- Enter the SambaNova Cloud API key in the `.env` file, for example:\n  \n```bash\nSAMBANOVA_API_KEY = \"456789abcdef0123456789abcdef0123\"\n```\n\n### Use SambaStudio (Option 2)\n\nBegin by deploying your LLM of choice (e.g., Llama 3 8B) to an endpoint for inference in SambaStudio. Use either the GUI or CLI, as described in the [SambaStudio endpoint documentation](https://docs.sambanova.ai/sambastudio/latest/endpoints.html).\n\nTo integrate your LLM deployed on SambaStudio with this AI starter kit, update the API information by configuring the environment variables in the `ai-starter-kit/.env` file:\n\n- Create the `.env` file at `ai-starter-kit/.env` if the file does not exist.\n- Set your SambaStudio variables. For example, an endpoint with the URL\n\"https://api-stage.sambanova.net/api/predict/nlp/12345678-9abc-def0-1234-56789abcdef0/456789ab-cdef-0123-4567-89abcdef0123\"\nis entered in the `.env` file as:\n\n``` bash\nSAMBASTUDIO_URL=\"https://api-stage.sambanova.net/api/predict/nlp/12345678-9abc-def0-1234-56789abcdef0/456789ab-cdef-0123-4567-89abcdef0123\"\nSAMBASTUDIO_API_KEY=\"89abcdef-0123-4567-89ab-cdef01234567\"\n```\n\n## Setting your embedding models\n\nCurrently, you can set your embedding models on CPU or SambaStudio. Note that embedding models are not available yet through SambaNova Cloud, but they will be in future releases.\n\n### Use CPU embedding (Option 1)\n\nYou can run the Hugging Face embedding models locally on CPU. In this case, no information is needed in the `.env` file.\n\n### Use SambaStudio embedding (Option 2)\n\nAlternatively, you can use SambaStudio embedding model endpoints instead of the CPU-based HugginFace embeddings to increase inference speed. Please follow [this guide](https://docs.sambanova.ai/sambastudio/latest/e5-large.html#_deploy_an_e5_large_v2_endpoint) to deploy your SambaStudio embedding model.\n\nTo integrate your embedding model deployed on SambaStudio with this AI starter kit, update the API information by configuring the environment variables in the `ai-starter-kit/.env` file:\n\n- Create the `.env` file at `ai-starter-kit/.env` if the file does not exist.\n- Set your SambaStudio variables. For example, an endpoint with the URL\n`\"https://api-stage.sambanova.net/api/predict/generic/12345678-9abc-def0-1234-56789abcdef0/456789ab-cdef-0123-4567-89abcdef0123\"`\nis entered in the `.env` file as:\n\n``` bash\nSAMBASTUDIO_URL=\"https://api-stage.sambanova.net/api/predict/nlp/12345678-9abc-def0-1234-56789abcdef0/456789ab-cdef-0123-4567-89abcdef0123\"\nSAMBASTUDIO_API_KEY=\"89abcdef-0123-4567-89ab-cdef01234567\"\n```\n\n## Run the desired starter kit\n\nGo to the `README.md` of the starter kit you want to use and follow the instructions. See [Available AI Starter Kits](#available-ai-starter-kits).\n\n## Additional information\n\n<details>\n<summary>Use Sambanova's LLMs and Langchain chat model wrappers</summary>\n\n### LLM Wrappers\n\nSet your environment as shown in [integrate your model](#integrate-your-model-in-the-starter-kit).\n\n#### Using Sambastudio LLMs\n\n1. Import the **SambaStudio** langchain integration in your project and define your **SambaStudio* ChatModel:\n\n``` bash\npip install langchain-sambanova\n```\n\n- If using a Bundle endpoint:\n\n```python\nfrom langchain_sambanova import ChatSambaStudio\n\nload_dotenv('.env')\n\nllm = ChatSambaStudio(\n  max_tokens_to_generate = 512,\n  temperature = 0.0,\n  model = \"Meta-Llama-3-8B-Instruct\"\n)\n```\n\n- If using a single model endpoint\n\n```python\nfrom langchain_sambanova import ChatSambaStudio\n\nload_dotenv('.env')\n\nllm = ChatSambaStudio(\n  max_tokens_to_generate = 512,\n  temperature = 0.0\n)\n```\n\n2. Use the model\n\n```python\nllm.invoke(\"your prompt\")\n```\n\nSee [utils/usage.ipynb](./utils/usage.ipynb) for an example.\n\n### Using SambaNova Cloud LLMs\n\n1. Import our **SambaNovaCloud** langchain integration in your project and define your **SambaNovaCloud* ChatModel:\n\n\n``` bash\npip install langchain-sambanova\n```\n\n```python\nfrom langchain_sambanova import ChatSambaNovaCloud\n\nload_dotenv('.env')\n\nllm = ChatSambaNovaCloud(model='Meta-Llama-3.3-70B-Instruct')\n```\n\n2. Use the model\n\n```python\nllm.invoke(\"your prompt\")\n```\n\nSee [utils/usage.ipynb](./utils/usage.ipynb) for an example.\n\n### Embedding Wrapper\n\n1. Import the **SambaStudioEmbedding** langchain integration in your project and define your **SambaStudioEmbedding*  embedding:\n\n``` bash\npip install langchain-sambanova\n```\n\n- If using a Bundle endpoint\n\n```python\nfrom langchain_sambanova import SambaStudioEmbeddings\n\nload_dotenv('.env')\n\nembedding = SambaStudioEmbeddings(\n              batch_size=1,\n              model_kwargs = {\n                  \"select_expert\":\"e5-mistral-7b-instruct\"\n                  }\n              )\n```\n\n- If using a single embedding model endpoint\n\n```python\nfrom langchain_sambanova import SambaStudioEmbeddings\n\nload_dotenv('.env')\n\nembedding = SambaStudioEmbeddings(batch_size=32)\n```\n\n> Note that using different embedding models (cpu or sambastudio) may change the results, and change the way they are set and their parameters\n\n2. Use your embedding model in your langchain pipeline\n\nSee [utils/usage.ipynb](./utils/usage.ipynb) for an example.\n\n### Javascript Example\n\n1. Before running the code, ensure that you have Node.js installed on your system. You can download the latest version from the official Node.js [website](https://nodejs.org/en).\n\n2. Set Up the Environment. To set up the environment, run the following commands in your terminal:\n\n``` bash\nnpm init -y\n```\n\n``` bash\nnpm install @langchain/openai @langchain/core\n```\n\n> These commands will create a new package.json file and install the required dependencies.\n\n3. Create a new file named `app.js` and add the following code:\n\n```javascript\nimport { ChatOpenAI } from \"@langchain/openai\";\n\nconst SambaNovaCloudBaseURL = \"https://api.sambanova.ai/v1\";\nconst apiKey = \"your-api-key\";\n\nconst SambaNovaCloudChatModel = new ChatOpenAI({\n  temperature: 0.9,\n  model: \"Meta-Llama-3.3-70B-Instruct\",\n  configuration: {\n    baseURL: SambaNovaCloudBaseURL,\n    apiKey: apiKey,\n  },\n});\n\nconst response = await SambaNovaCloudChatModel.invoke(\"Hi there, tell me a joke!\");\nconsole.log(response.content);\n```\n\n4. To run the app, execute the following command in your terminal:\n\n``` bash\nnode app.js\n```\n\n---\n\n</details>\n\n<details>\n<summary>Setting up your virtual environment</summary><br/>\n\nThere are two approaches to setting up your virtual environment for the AI Starter Kits:\n\n1. **Individual Kit Setup (Traditional Method)**\n2. **Base Environment Setup**\n\n### 1. Individual Kit Setup\n\nEach starter kit (see table [above](#available-ai-starter-kits)) has its own `README.md` and `requirements.txt` file. You can set up a separate virtual environment for each kit by following the instructions in their respective directories. This method is suitable if you're only interested in running a single kit or prefer isolated environments for each project.\n\nTo use this method:\n1. Navigate to the specific kit's directory\n2. Create a virtual environment\n3. Install the requirements\n4. Follow the kit-specific instructions\n\n### 2. Base Environment Setup \n\nFor users who plan to work with multiple kits or prefer a unified development environment, we recommend setting up a base environment. This approach uses a Makefile to automate the setup of a consistent Python environment that works across all kits.\n\nBenefits of the base environment approach:\n- Consistent Python version across all kits\n- Centralized dependency management\n- Simplified setup process\n- Easier switching between different kits\n\n#### Prerequisites\n\n- **pyenv**: The Makefile will attempt to install pyenv if it's not already installed.\n- **Docker**: (Optional) If you want to use the Docker-based setup, ensure Docker is installed on your system.\n\n#### What the Base Setup Does\n\n1. Installs pyenv and Poetry if they are not already installed.\n2. Sets up a Python virtual environment using a specified Python version (default is 3.11.3).\n3. Installs all necessary dependencies for the base environment.\n4. Sets up the parsing service required by some kits.\n5. Installs system dependencies like Tesseract OCR and Poppler.\n6. Provides Docker-based setup options for consistent environments across different systems.\n\n#### Setting Up the Base Environment\n\n1. **Install and Set Up the Base Environment:**\n\n```bash\nmake all\n```\nThis command will set up the base ai-starter-kit environment, including installing all necessary tools and dependencies.\n\n2. **Activate the Base Environment:**\n\n```bash\nsource .venv/bin/activate\n```\n\n3. **Navigate to Your Chosen Starter Kit:**\n```bash\ncd path/to/starter_kit\n```\nWithin the starter kit there will be instructions on how to start the kit. You can skip the virtual environment creation \npart in the kits README.md as we've done it here.\n\n\n### Parsing Service Management\nFor certain kits, we utilise a standard parsing service. By Default it's started automatically with the base environment. To work with this service in isolation, following the steps in this section.\n\n- **Start Parsing Service:**\n```bash\nmake start-parsing-service\n```\n\n- **Stop Parsing Service:**\n```bash\nmake stop-parsing-service\n```\n\n- **Check Parsing Service Status:**\n```bash\nmake parsing-status\n```\n\n- **View Parsing Service Logs:**\n```bash\nmake parsing-log\n```\n\n### Docker-based Setup\n\nTo use the Docker-based setup:\n\n1. Ensure Docker is installed on your system.\n2. Build the Docker image:\n\n```bash\nmake docker-build\n```\n\n3. Run a specific kit in the Docker container:\n```bash\nmake docker-run-kit KIT=<kit_name>\n```\nReplace `<kit_name>` with the name of the starter kit you want to run (e.g., `function_calling`).\n\n4. To open a shell in the Docker container:\n```bash\nmake docker-shell\n```\n\n### Cleanup\n\nTo clean up all virtual environments created by the makefile and stop parsing services run the following command:\n```bash\nmake clean\n```\nThis command removes all virtual environments created with the makefile, stops the parsing service, and cleans up any temporary files.\n</details>\n\n<details>\n<summary>Troubleshooting</summary><br/>\n\nIf you encounter issues while setting up or running the AI Starter Kit, here are some common problems and their solutions:\n\n### Python version issues\n\nIf you're having problems with Python versions:\n\n1. Ensure you have pyenv installed: `make ensure-pyenv`\n2. Install the required Python versions: `make install-python-versions`\n3. If issues persist, check your system's Python installation and PATH settings.\n\n### Dependency conflicts\n\nIf you're experiencing dependency conflicts:\n\n1. Try cleaning your environment: `make clean`\n2. Update the lock file: `poetry lock --no-update`\n3. Reinstall dependencies: `make install`\n\n### pikepdf installation issues\n\nIf you encounter an error while installing `pikepdf`, such as:\n\n```\nERROR: Failed building wheel for pikepdf\nFailed to build pikepdf\n```\n\nThis is likely due to missing `qpdf` dependency. The Makefile should automatically install `qpdf` for you, but if you're still encountering issues:\n\n1. Ensure you have proper permissions to install system packages.\n2. If you're on macOS, you can manually install `qpdf` using Homebrew:\n   ```bash\n   brew install qpdf\n   ```\n3. On Linux, you can install it using your package manager, e.g., on Ubuntu:\n   ```\n   sudo apt-get update && sudo apt-get install -y qpdf\n   ```\n4. After installing `qpdf`, try running `make install` again.\n\nIf you continue to face issues, please ensure your system meets all the requirements for building `pikepdf` and consider checking the [pikepdf documentation](https://pikepdf.readthedocs.io/en/latest/installation.html) for more detailed installation instructions.\n\n### Parsing service issues\n\nIf the parsing service isn't starting or is behaving unexpectedly:\n\n1. Check its status: `make parsing-status`\n2. View its logs: `make parsing-log`\n3. Try stopping and restarting it: `make stop-parsing-service` followed by `make start-parsing-service`\n\n### System Dependencies Issues\n\nIf you encounter issues related to Tesseract OCR or Poppler:\n\n1. Ensure the Makefile has successfully installed these dependencies.\n2. On macOS, you can manually install them using Homebrew:\n ```bash\n   brew install tesseract poppler\n   ```\n3. On Linux (Ubuntu/Debian), you can install them manually:\n ```bash\n   sudo apt-get update && sudo apt-get install -y tesseract-ocr poppler-utils\n   ```\n4. On Windows, you may need to install these dependencies manually and ensure they are in your system PATH.\n\n### Docker-related Issues\n\nIf you're using the Docker-based setup and encounter issues:\n\n1. Ensure Docker is properly installed and running on your system.\n2. Try rebuilding the Docker image: `make docker-build`\n3. Check Docker logs for any error messages.\n4. Ensure your firewall or antivirus is not blocking Docker operations.\n\n### General troubleshooting steps\n\n1. Ensure all prerequisites (Python, pyenv, Poetry) are correctly installed.\n2. Try cleaning and rebuilding the environment: `make clean all`\n3. Check for any error messages in the console output and address them specifically.\n4. Ensure your `.env` file is correctly set up in the ai-starter-kit root with all necessary environment variables.\n\nIf you continue to experience issues, please [open an issue](https://github.com/sambanova/ai-starter-kit/issues/new) with details about your environment, the full error message, and steps to reproduce the problem.\n\n### Important Notes for Users\n\n- Ensure you have sufficient permissions to install software on your system.\n- The setup process may take several minutes, especially when installing Python versions or large dependencies.\n- If you encounter any issues during setup, check the error messages and ensure your system meets all prerequisites.\n- Always activate the base environment before navigating to and running a specific starter kit.\n- Some kits may require additional setup steps. Always refer to the specific README of the kit you're using.\n</details>\n\n### API Reference\n\n- Find more information about SambaNova Cloud [here](https://docs.sambanova.ai/cloud/docs/get-started/overview)\n\n- Find more information about SambaStudio [here](https://docs.sambanova.ai/sambastudio/latest/index.html)\n\n**Note:** These AI Starter Kit code samples are provided \"as-is,\" and are not production-ready or supported code. Bugfix/support will be on a best-effort basis only. Code may use third-party open-source software. You are responsible for performing due diligence per your organization policies for use in your applications.\n\n"
    },
    {
      "name": "microsoft/Multi-Agent-Custom-Automation-Engine-Solution-Accelerator",
      "stars": 180,
      "img": "https://avatars.githubusercontent.com/u/6154722?s=40&v=4",
      "owner": "microsoft",
      "repo_name": "Multi-Agent-Custom-Automation-Engine-Solution-Accelerator",
      "description": "The Multi-Agent Custom Automation Engine Solution Accelerator is an AI-driven orchestration system that manages a group of AI agents to accomplish tasks based on user input. Powered by AutoGen, Azure OpenAI, Cosmos, and infrastructure services, it provides a ready to go application to use as a reference, allowing you to hit the ground running.",
      "homepage": "",
      "language": "Python",
      "created_at": "2024-12-16T18:56:55Z",
      "updated_at": "2025-04-22T20:50:35Z",
      "topics": [],
      "readme": "# Multi-Agent-Custom-Automation-Engine – Solution Accelerator\n\nMENU: [**USER STORY**](#user-story) \\| [**QUICK DEPLOY**](#quick-deploy)  \\| [**SUPPORTING DOCUMENTATION**](#supporting-documentation) \\|\n\n<h2><img src=\"./documentation/images/readme/userStory.png\" width=\"64\">\n<br/>\nUser story\n</h2>\n\n### Overview\n\nProblem:\nAgentic AI systems are set to transform the way businesses operate, however it can be fairly complex to build an initial MVP to demonstrate this value. \n\nSolution:\nThe Multi-Agent-Custom Automation Engine Solution Accelerator provides a ready to go application to use as the base of the MVP, or as a reference, allowing you to hit the ground running.\n\n### Technology Note\nThis accelerator uses the AutoGen framework from Microsoft Research.  This is an open source project that is maintained by [Microsoft Research’s AI Frontiers Lab](https://www.microsoft.com/research/lab/ai-frontiers/). Please see this [blog post](https://devblogs.microsoft.com/autogen/microsofts-agentic-frameworks-autogen-and-semantic-kernel/) for the latest information on using the AutoGen framework in production solutions.\n\n### Use cases / scenarios\nThe multi-agent approach allows users to utilize multiple AI agents simultaneously for repeatable tasks, ensuring consistency and efficiency. \nThe agents collaborate with a manager on various assignments for onboarding a new employee, such as HR and tech support AI working together to set up software accounts, configure hardware, schedule onboarding meetings, register employees for benefits, and send welcome emails. Additionally, these agents can handle tasks like procurement and drafting press releases.\n\n### Business value\nMulti-agent systems represent the next wave of Generative AI use cases, offering entirely new opportunities to drive efficiencies in your business. The Multi-Agent-Custom-Automation-Engine Solution Accelerator demonstrates several key benefits:\n\n- **Allows people to focus on what matters:** by doing the heavy lifting involved with coordinating activities across an organization, peoples’ time is freed up to focus on their specializations.\n- **Enabling GenAI to scale:** by not needing to build one application after another, organizations are able to reduce the friction of adopting GenAI across their entire organization. One capability can unlock almost unlimited use cases.\n- **Applicable to most industries:** these are common challenges that most organizations face, across most industries.\n\nWhilst still an emerging area, investing in agentic use cases, digitization and developing tools will be key to ensuring you are able to leverage these new technologies and seize the GenAI moment.\n\n### Technical key features\n\nThis application is an AI-driven orchestration system that manages a group of AI agents to accomplish tasks based on user input. It uses a FastAPI backend to handle HTTP requests, processes them through various specialized agents, and stores stateful information using Azure Cosmos DB. The system is designed to:\n\n- Receive input tasks from users.\n- Generate a detailed plan to accomplish the task using a Planner agent.\n- Execute the plan by delegating steps to specialized agents (e.g., HR, Procurement, Marketing).\n- Incorporate human feedback into the workflow.\n- Maintain state across sessions with persistent storage.\n\nThis system is intended for developing and deploying custom AI solutions for specific customers. This code has not been tested as an end-to-end, reliable production application- it is a foundation to help accelerate building out multi-agent systems. You are encouraged to add your own data and functions to the agents, and then you must apply your own performance and safety evaluation testing frameworks to this system before deploying it.\n\n\\\n![image](./documentation/images/readme/macae-application.png)\n\n\n\n### Products used/licenses required\n\n-   Azure Container Application\n\n-   Azure OpenAI\n\n-   Azure Cosmos DB\n\n-   The user deploying the template must have permission to create\n    resources and resource groups.\n\n### Solution accelerator architecture\n![image](./documentation/images/readme/macae-architecture.png)\n\n\n\n\n### **How to install/deploy**\n\nThis guide provides step-by-step instructions for deploying your application using Azure Container Registry (ACR) and Azure Container Apps.\n\nThere are several ways to deploy the solution.  You can deploy to run in Azure in one click, or manually, or you can deploy locally.\n\n## Quick Deploy\n\n<h2><img src=\"./documentation/images/readme/oneClickDeploy.png\" width=\"64\"></h2>\n\n[![Deploy to Azure](https://aka.ms/deploytoazurebutton)](https://portal.azure.com/#create/Microsoft.Template/uri/https%3A%2F%2Fraw.githubusercontent.com%2Fmicrosoft%2FMulti-Agent-Custom-Automation-Engine-Solution-Accelerator%2Frefs%2Fheads%2Fmain%2Fdeploy%2Fmacae-continer-oc.json)\n\nWhen Deployment is complete, follow steps in [Set Up Authentication in Azure App Service](./documentation/azure_app_service_auth_setup.md) to add app authentication to your web app running on Azure App Service\n\n## Local Deployment\nTo run the solution site and API backend only locally for development and debugging purposes, See the [local deployment guide](./documentation/LocalDeployment.md).\n\n## Manual Azure Deployment\nManual Deployment differs from the ‘Quick Deploy’ option in that it will install an Azure Container Registry (ACR) service, and relies on the installer to build and push the necessary containers to this ACR.  This allows you to build and push your own code changes and provides a sample solution you can customize based on your requirements.\n\n### Prerequisites\n\n- Current Azure CLI installed\n  You can update to the latest version using ```az upgrade```\n- Azure account with appropriate permissions\n- Docker installed\n\n### Deploy the Azure Services\nAll of the necessary Azure services can be deployed using the /deploy/macae.bicep script.  This script will require the following parameters:\n\n```\naz login\naz account set --subscription <SUBSCRIPTION_ID>\naz group create --name <RG_NAME> --location <RG_LOCATION>\n```\nTo deploy the script you can use the Azure CLI.\n```\naz deployment group create \\\n  --resource-group <RG_NAME> \\\n  --template-file <BICEP_FILE> \\\n  --name <DEPLOYMENT_NAME>\n```\n\nNote: if you are using windows with PowerShell, the continuation character (currently ‘\\’) should change to the tick mark (‘`’).\n\nThe template will require you fill in locations for Cosmos and OpenAI services.  This is to avoid the possibility of regional quota errors for either of these resources.\n\n### Create the Containers\n#### Get admin credentials from ACR\n\nRetrieve the admin credentials for your Azure Container Registry (ACR):\n\n```sh\naz acr credential show \\\n--name <e.g. macaeacr2t62qyozi76bs> \\\n--resource-group <rg-name>\n```\n\n#### Login to ACR\n\nLogin to your Azure Container Registry:\n\n```sh\naz acr login --name <e.g. macaeacr2t62qyozi76bs>\n```\n\n#### Build and push the image\n\nBuild the frontend and backend Docker images and push them  to your Azure Container Registry. Run the following from the src/backend and the src/frontend directory contexts:\n\n```sh\naz acr build \\\n--registry <e.g. macaeacr2t62qyozi76bs> \\\n--resource-group <rg-name> \\\n--image <e.g. backendmacae:latest> .\n```\n\n### Add images to the Container APP and Web App services\n\nTo add your newly created backend image: \n- Navigate to the Container App Service in the Azure portal\n- Click on Application/Containers in the left pane\n- Click on the \"Edit and deploy\" button in the upper left of the containers pane\n- In the \"Create and deploy new revision\" page, click on your container image 'backend'.  This will give you the option of reconfiguring the container image, and also has an Environment variables tab\n- Change the properties page to \n  - point to your Azure Container registry with a private image type and your image name (e.g. backendmacae:latest)\n  - under \"Authentication type\" select \"Managed Identity\" and choose the 'mace-containerapp-pull'... identity setup in the bicep template\n- In the environment variables section add the following (each with a 'Manual entry' source):\n\n        name: 'COSMOSDB_ENDPOINT'\n        value: \\<Cosmos endpoint>\n\n        name: 'COSMOSDB_DATABASE'\n        value: 'autogen'\n\t    Note: To change the default, you will need to create the database in Cosmos\n\t\t\t  \n        name: 'COSMOSDB_CONTAINER'\n        value: 'memory'\n\n        name: 'AZURE_OPENAI_ENDPOINT'\n        value: <Azure OpenAI endpoint>\n\n        name: 'AZURE_OPENAI_DEPLOYMENT_NAME'\n        value: 'gpt-4o'\n\n        name: 'AZURE_OPENAI_API_VERSION'\n        value: '2024-08-01-preview'\n\t\tNote: Version should be updated based on latest available\n\n        name: 'FRONTEND_SITE_NAME'\n        value: 'https://<website Name>.azurewebsites.net'\n\n        name: 'APPLICATIONINSIGHTS_CONNECTION_STRING'\n        value: <Application Insights Connection String>\n\n- Click 'Save' and deploy your new revision\n\nTo add the new container to your website run the following:\n\n```\naz webapp config container set --resource-group <resource_group_name> \\\n--name <container_name> \\\n--container-image-name <e.g. macaeacr2t62qyozi76bs.azurecr.io/frontendmacae:latest>  \\\n--container-registry-url <e.g. https://macaeacr2t62qyozi76bs.azurecr.io>\n```\n\n\n### Add the Entra identity provider to the Azure Web App\nTo add the identity provider, please follow the steps outlined in [Set Up Authentication in Azure App Service](./documentation/azure_app_service_auth_setup.md)\n\n### Run locally and debug\n\nTo debug the solution, you can use the Cosmos and OpenAI services you have manually deployed.  To do this, you need to ensure that your Azure identity has the required permissions on the Cosmos and OpenAI services. \n\n- For OpenAI service, you can add yourself to the ‘Cognitive Services OpenAI User’ permission in the Access Control (IAM) pane of the Azure portal.  \n- Cosmos is a little more difficult as it requires permissions be added through script.  See these examples for more information: \n  - [Use data plane role-based access control - Azure Cosmos DB for NoSQL | Microsoft Learn](https://learn.microsoft.com/en-us/azure/cosmos-db/nosql/security/how-to-grant-data-plane-role-based-access?tabs=built-in-definition%2Cpython&pivots=azure-interface-cli) \n  - [az cosmosdb sql role assignment | Microsoft Learn](https://learn.microsoft.com/en-us/cli/azure/cosmosdb/sql/role/assignment?view=azure-cli-latest#az-cosmosdb-sql-role-assignment-create) \n\nAdd the appropriate endpoints from Cosmos and OpenAI services to your .env file.  \nNote that you can configure the name of the Cosmos database in the configuration.  This can be helpful if you wish to separate the data messages generated in local debugging from those associated with the cloud based solution.  If you choose to use a different database, you will need to create that database in the Cosmos instance as this is not done automatically.\n\nIf you are using VSCode, you can use the debug configuration shown in the [local deployment guide](./documentation/LocalDeployment.md).\n\n## Supporting documentation\n\n\n### \n\n### How to customize\n\nThis solution is designed to be easily customizable. You can modify the front end site, or even build your own front end and attach to the backend API. You can further customize the backend by adding your own agents with their own specific capabilities. Deeper technical information to aid in this customization can be found in this [document](./documentation/CustomizeSolution.md).\n\n### Additional resources\n\n- [Python FastAPI documentation](https://fastapi.tiangolo.com/learn/)\n- [AutoGen Framework Documentation](https://microsoft.github.io/autogen/dev/user-guide/core-user-guide/index.html)\n- [Azure Container App documentation](https://learn.microsoft.com/en-us/azure/azure-functions/functions-how-to-custom-container?tabs=core-tools%2Cacr%2Cazure-cli2%2Cazure-cli&pivots=container-apps)\n- [Azure OpenAI Service - Documentation, quickstarts, API reference - Azure AI services | Microsoft Learn](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/use-your-data)\n- [Azure Cosmos DB documentation](https://learn.microsoft.com/en-us/azure/cosmos-db/)\n  \n\n<h2><img src=\"./documentation/images/readme/customerTruth.png\" width=\"64\">\n</br>\nCustomer truth\n</h2>\nCustomer stories coming soon.\n\n<br/>\n<br/>\n<br/>\n\n---\n\n## Disclaimers\n\nTo the extent that the Software includes components or code used in or derived from Microsoft products or services, including without limitation Microsoft Azure Services (collectively, “Microsoft Products and Services”), you must also comply with the Product Terms applicable to such Microsoft Products and Services. You acknowledge and agree that the license governing the Software does not grant you a license or other right to use Microsoft Products and Services. Nothing in the license or this ReadMe file will serve to supersede, amend, terminate or modify any terms in the Product Terms for any Microsoft Products and Services. \n\nYou must also comply with all domestic and international export laws and regulations that apply to the Software, which include restrictions on destinations, end users, and end use. For further information on export restrictions, visit https://aka.ms/exporting. \n\nYou acknowledge that the Software and Microsoft Products and Services (1) are not designed, intended or made available as a medical device(s), and (2) are not designed or intended to be a substitute for professional medical advice, diagnosis, treatment, or judgment and should not be used to replace or as a substitute for professional medical advice, diagnosis, treatment, or judgment. Customer is solely responsible for displaying and/or obtaining appropriate consents, warnings, disclaimers, and acknowledgements to end users of Customer’s implementation of the Online Services. \n\nYou acknowledge the Software is not subject to SOC 1 and SOC 2 compliance audits. No Microsoft technology, nor any of its component technologies, including the Software, is intended or made available as a substitute for the professional advice, opinion, or judgement of a certified financial services professional. Do not use the Software to replace, substitute, or provide professional financial advice or judgment.  \n\nBY ACCESSING OR USING THE SOFTWARE, YOU ACKNOWLEDGE THAT THE SOFTWARE IS NOT DESIGNED OR INTENDED TO SUPPORT ANY USE IN WHICH A SERVICE INTERRUPTION, DEFECT, ERROR, OR OTHER FAILURE OF THE SOFTWARE COULD RESULT IN THE DEATH OR SERIOUS BODILY INJURY OF ANY PERSON OR IN PHYSICAL OR ENVIRONMENTAL DAMAGE (COLLECTIVELY, “HIGH-RISK USE”), AND THAT YOU WILL ENSURE THAT, IN THE EVENT OF ANY INTERRUPTION, DEFECT, ERROR, OR OTHER FAILURE OF THE SOFTWARE, THE SAFETY OF PEOPLE, PROPERTY, AND THE ENVIRONMENT ARE NOT REDUCED BELOW A LEVEL THAT IS REASONABLY, APPROPRIATE, AND LEGAL, WHETHER IN GENERAL OR IN A SPECIFIC INDUSTRY. BY ACCESSING THE SOFTWARE, YOU FURTHER ACKNOWLEDGE THAT YOUR HIGH-RISK USE OF THE SOFTWARE IS AT YOUR OWN RISK.  \n"
    },
    {
      "name": "Azure-Samples/dream-team",
      "stars": 178,
      "img": "https://avatars.githubusercontent.com/u/1844662?s=40&v=4",
      "owner": "Azure-Samples",
      "repo_name": "dream-team",
      "description": "This repo helps you to build a team of AI agents with Autogen",
      "homepage": "",
      "language": "TypeScript",
      "created_at": "2024-05-28T11:43:22Z",
      "updated_at": "2025-04-23T07:04:22Z",
      "topics": [
        "autogen",
        "azd",
        "azure",
        "azureopenai",
        "streamlit"
      ],
      "readme": "[![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat-square)](http://makeapullrequest.com)\n[![LinkedIn](https://img.shields.io/badge/LinkedIn-Connect-blue)](https://www.linkedin.com/in/yaniv-vaknin-7a8324178/)\n\n# Build your dream team with Autogen\n\nThis repository is a part of the Azure Samples collection and utilizes Microsoft Autogen 0.4 alongside Azure OpenAI. It seamlessly integrates with a React UI to create a comprehensive end-to-end multi-agent application. Designed for simplicity, this repository streamlines the process of building, testing, and deploying an advanced multi-agent framework. [Magentic One](https://www.microsoft.com/en-us/research/articles/magentic-one-a-generalist-multi-agent-system-for-solving-complex-tasks/)\n \n\n![Architecture](assets/architecture.png)\n\n\n:tada: February 25, 2025: We have a new React based UI with new business use cases\n\n:tada: January 11, 2025: The repo now support [Autogen 0.4.0 stable version](https://microsoft.github.io/autogen/stable/)\n\n:tada: December 3, 2024: The repo now support one click deployment with [Azure Developer CLI](https://learn.microsoft.com/en-us/azure/developer/azure-developer-cli/), if you would like to run it with the full process localy you can check [v0.21](https://github.com/yanivvak/dream-team/tree/v0.21)\n\n:tada: November 18, 2024: we are porting this repo to  [Autogen 0.4](https://microsoft.github.io/autogen/0.4.0.dev6/index.html), A new event driven, asynchronous architecture for AutoGen and [Magentic One](https://github.com/microsoft/autogen/tree/main/python/packages/autogen-magentic-one)\n\n\nhttps://github.com/user-attachments/assets/e3f1bbae-a93b-47d8-b661-b6a9507c243b\n\n\n# Key Features\n\nDream Team offers the following key features:\n\n- **Advanced multi agent framework**: this solution is based on the popular framework Autogen(35K stars) and Magentic One\n- **Friendly UI**: easy way to build and share data apps powered by React / Vite.js / Tailwind / Shadcn\n- **Single line deployment**: developer-friendly deployment that accelerates your path from a local development environment to Azure with single line of code - azd up.\n- **Secure code execution**:  Fast access to secure sandboxed with strong isolation environments that are ideal for running code or applications with Azure Container Apps dynamic sessions.\n- **Managed Identities**: Built in Azure Managed identities to eliminate the need for developers to manage these credentials\n- **Observability & Debugging**: Built-in features and tools for tracking, tracing, and debugging agent interactions and workflows, including PromptFlow tracing.\n\n\n# Prerequisites:\n\n1. Install [Azure Developer CLI](https://learn.microsoft.com/en-us/azure/developer/azure-developer-cli/install-azd?tabs=winget-windows%2Cbrew-mac%2Cscript-linux&pivots=os-windows).\n2. Ensure you have access to an Azure subscription\n3. Docker - Follow the [official Docker installation instructions](https://docs.docker.com/get-started/get-docker/) - make sure your docker is loged in (docker login -u \"username\" -p \"password\"\n )\n4. Python version >= 3.10, < 3.13\n5. Install [UV](https://github.com/astral-sh/uv?tab=readme-ov-file#installation) - optional for running locally\n\n\n# Step by step deployment\n   \n## 1. Clone the repository     \n```bash  \ngit clone https://github.com/Azure-Samples/dream-team  \n```\n## 2. Login to your Azure account\n```bash\nazd auth login\n```\n> You need to choose your preferred region (you can start with east us or sweden central or any other available region)\n\n## 3. Deploy Azure Resources and the app\n\n```bash\nazd up\n```\n\n## 4. [Optional] Ingest the demo documents into your AI Search\n\nIn case you want to use the demo data, you can run the ingestion script to populate your AI Search with the demo data. This step is optional and is only needed if you want to use the demo data.\n\n```bash\ncd backend\npython -m aisearch.py\n```\n\n> Notes:\n> 1. This step assumes you have already setup your infrastructure and your local `.env` file has been populated with the necessary values.\n> 2. Make sure your identity has appropriate acccess to AI Search (role `Search Index Data Contributor`) and to created storage (role `Storage Blob Data Contributor`), otherwise you will get an error when running the ingestion script.\n> 3. This creates four indexes: ag-demo-fsi-upsell, ag-demo-pred-maint, ag-demo-retail, ag-demo-safety\n\n# Notes \n- While using Web Surfer agent, you might want to change Content Safety on Azure OpenAI to accomodate your needs\n- currently it is \"bring your own AI Search\" (BYOS) - since its assuming you have your own search engine, we are working on a solution to make it easier for you\n   - you must add two ENV variables to backend service to connect to your search engine\n   - `AZURE_SEARCH_SERVICE_ENDPOINT` - your search engine endpoint\n   - `AZURE_SEARCH_ADMIN_KEY` - your search engine key (we are working to enable managed identity for this service)\n \n\n# Working locally  \n\nThere are two parts to this project: the backend and the frontend. The backend is written in Python, and the frontend is written in JavaScript using React.\n\n## Backend\n\n```bash  \ncd backend  \n```\nSet up a virtual environment (Preferred)\n```bash\nuv venv\n```\nOnce you’ve created a virtual environment, you may activate it.\n\nOn Windows, run:\n```bash\n.venv\\Scripts\\activate\n```\nOn Unix or MacOS, run:\n```bash\nsource .venv/bin/activate\n```\nTo deactivate :\n```bash\ndeactivate\n```\n> More information about virtual environments can be found [here](https://docs.python.org/3/tutorial/venv.html)\n\n### Install dependencies\n```bash\nuv sync\nplaywright install --with-deps chromium\n```\n\n> Important: Magentic-One code uses code execution, you need to have Docker installed to run the examples if you use local execution\n\n### Run\n```bash\nuvicorn main:app --reload\n```\n\n## Frontend (open a new terminal)\n```bash\ncd frontend\n```\n> Upadte the env variables in sample.env and rename to .env\n\n## Run\n```bash\nnpm run dev\n```\nIf your app is ready, you can browse to (typically) http://localhost:8501 to see the app in action.\n![Screenshot](./assets/application.png)\n\n# Learn\nCheck these resources:\n1. [Blogpost](https://techcommunity.microsoft.com/blog/Azure-AI-Services-blog/build-your-dream-team-with-autogen/4157961) - Build your dream team with Autogen\n2. [Webinar](https://youtu.be/wB9gD9FkgNA?si=WU3H0QL37RCiTGvl) - More agents is all you need\n"
    },
    {
      "name": "Azure-Samples/python-ai-agent-frameworks-demos",
      "stars": 125,
      "img": "https://avatars.githubusercontent.com/u/1844662?s=40&v=4",
      "owner": "Azure-Samples",
      "repo_name": "python-ai-agent-frameworks-demos",
      "description": "A repository of examples using Python AI Agent frameworks that work with GitHub Models and Azure OpenAI.",
      "homepage": null,
      "language": "Bicep",
      "created_at": "2025-04-08T14:15:26Z",
      "updated_at": "2025-04-23T05:53:31Z",
      "topics": [],
      "readme": "<!--\n---\nname: Python AI Agent Frameworks Demos\ndescription: Collection of Python examples for popular AI agent frameworks using GitHub Models or Azure OpenAI.\nlanguages:\n- python\nproducts:\n- azure-openai\n- azure\npage_type: sample\nurlFragment: python-ai-agent-frameworks-demos\n---\n-->\n# Python AI Agent Frameworks Demos\n\n[![Open in GitHub Codespaces](https://img.shields.io/static/v1?style=for-the-badge&label=GitHub+Codespaces&message=Open&color=brightgreen&logo=github)](https://codespaces.new/Azure-Samples/python-ai-agent-frameworks-demos)\n[![Open in Dev Containers](https://img.shields.io/static/v1?style=for-the-badge&label=Dev%20Containers&message=Open&color=blue&logo=visualstudiocode)](https://vscode.dev/redirect?url=vscode://ms-vscode-remote.remote-containers/cloneInVolume?url=https://github.com/Azure-Samples/python-ai-agent-frameworks-demos)\n\nThis repository provides examples of many popular Python AI agent frameworks using LLMs from [GitHub Models](https://github.com/marketplace/models). Those models are free to use for anyone with a GitHub account, up to a [daily rate limit](https://docs.github.com/github-models/prototyping-with-ai-models#rate-limits).\n\n* [Getting started](#getting-started)\n  * [GitHub Codespaces](#github-codespaces)\n  * [VS Code Dev Containers](#vs-code-dev-containers)\n  * [Local environment](#local-environment)\n* [Running the Python examples](#running-the-python-examples)\n* [Guidance](#guidance)\n  * [Costs](#costs)\n  * [Security guidelines](#security-guidelines)\n* [Resources](#resources)\n\n## Getting started\n\nYou have a few options for getting started with this repository.\nThe quickest way to get started is GitHub Codespaces, since it will setup everything for you, but you can also [set it up locally](#local-environment).\n\n### GitHub Codespaces\n\nYou can run this repository virtually by using GitHub Codespaces. The button will open a web-based VS Code instance in your browser:\n\n1. Open the repository (this may take several minutes):\n\n    [![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://codespaces.new/Azure-Samples/python-ai-agent-frameworks-demos)\n\n2. Open a terminal window\n3. Continue with the steps to run the examples\n\n### VS Code Dev Containers\n\nA related option is VS Code Dev Containers, which will open the project in your local VS Code using the [Dev Containers extension](https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-containers):\n\n1. Start Docker Desktop (install it if not already installed)\n2. Open the project:\n\n    [![Open in Dev Containers](https://img.shields.io/static/v1?style=for-the-badge&label=Dev%20Containers&message=Open&color=blue&logo=visualstudiocode)](https://vscode.dev/redirect?url=vscode://ms-vscode-remote.remote-containers/cloneInVolume?url=https://github.com/Azure-Samples/python-ai-agent-frameworks-demos)\n\n3. In the VS Code window that opens, once the project files show up (this may take several minutes), open a terminal window.\n4. Continue with the steps to run the examples\n\n### Local environment\n\n1. Make sure the following tools are installed:\n\n    * [Python 3.10+](https://www.python.org/downloads/)\n    * Git\n\n2. Clone the repository:\n\n    ```shell\n    git clone https://github.com/Azure-Samples/python-ai-agent-frameworks-demos\n    cd python-ai-agents-demos\n    ```\n\n3. Set up a virtual environment:\n\n    ```shell\n    python -m venv venv\n    source venv/bin/activate  # On Windows: venv\\Scripts\\activate\n    ```\n\n4. Install the requirements:\n\n    ```shell\n    pip install -r requirements.txt\n    ```\n\n## Running the Python examples\n\nYou can run the examples in this repository by executing the scripts in the `examples` directory. Each script demonstrates a different AI agent pattern or framework.\n\n| Example | Description |\n| ------- | ----------- |\n| autogen_basic.py | Uses AutoGen to build a single agent. |\n| autogen_tools.py | Uses AutoGen to build a single agent with tools. |\n| autogen_magenticone.py | Uses AutoGen with the MagenticOne orchestrator agent for travel planning. |\n| autogen_swarm.py | Uses AutoGen with the Swarm orchestrator agent for flight refunding requests. |\n| langgraph.py | Uses LangGraph to build an agent with a StateGraph to play songs. |\n| llamaindex.py | Uses LlamaIndex to build a ReAct agent for RAG on multiple indexes. |\n| openai_agents_basic.py | Uses the OpenAI Agents framework to build a single agent. |\n| openai_agents.py | Uses the OpenAI Agents framework to handoff between several agents with tools. |\n| openai_functioncalling.py | Uses OpenAI Function Calling to call functions based on LLM output. |\n| pydanticai.py | Uses PydanticAI to build a two-agent sequential workflow for flight planning. |\n| semantickernel.py | Uses Semantic Kernel to build a writer/editor two-agent workflow. |\n| smolagents_codeagent.py | Uses SmolAgents to build a question-answering agent that can search the web and run code. |\n\n## Configuring GitHub Models\n\nIf you open this repository in GitHub Codespaces, you can run the scripts for free using GitHub Models without any additional steps, as your `GITHUB_TOKEN` is already configured in the Codespaces environment.\n\nIf you want to run the scripts locally, you need to set up the `GITHUB_TOKEN` environment variable with a GitHub personal access token (PAT). You can create a PAT by following these steps:\n\n1. Go to your GitHub account settings.\n2. Click on \"Developer settings\" in the left sidebar.\n3. Click on \"Personal access tokens\" in the left sidebar.\n4. Click on \"Tokens (classic)\" or \"Fine-grained tokens\" depending on your preference.\n5. Click on \"Generate new token\".\n6. Give your token a name and select the scopes you want to grant. For this project, you don't need any specific scopes.\n7. Click on \"Generate token\".\n8. Copy the generated token.\n9. Set the `GITHUB_TOKEN` environment variable in your terminal or IDE:\n\n    ```shell\n    export GITHUB_TOKEN=your_personal_access_token\n    ```\n\n10. Optionally, you can use a model other than \"gpt-4o\" by setting the `GITHUB_MODEL` environment variable. Use a model that supports function calling, such as: `gpt-4o`, `gpt-4o-mini`, `o3-mini`, `AI21-Jamba-1.5-Large`, `AI21-Jamba-1.5-Mini`, `Codestral-2501`, `Cohere-command-r`, `Ministral-3B`, `Mistral-Large-2411`, `Mistral-Nemo`, `Mistral-small`\n\n## Provisioning Azure AI resources\n\nYou can run all examples in this repository using GitHub Models. If you want to run the examples using models from Azure OpenAI instead, you need to provision the Azure AI resources, which will incur costs.\n\nThis project includes infrastructure as code (IaC) to provision Azure OpenAI deployments of \"gpt-4o\" and \"text-embedding-3-large\". The IaC is defined in the `infra` directory and uses the Azure Developer CLI to provision the resources.\n\n1. Make sure the [Azure Developer CLI (azd)](https://aka.ms/install-azd) is installed.\n\n2. Login to Azure:\n\n    ```shell\n    azd auth login\n    ```\n\n    For GitHub Codespaces users, if the previous command fails, try:\n\n   ```shell\n    azd auth login --use-device-code\n    ```\n\n3. Provision the OpenAI account:\n\n    ```shell\n    azd provision\n    ```\n\n    It will prompt you to provide an `azd` environment name (like \"agents-demos\"), select a subscription from your Azure account, and select a location. Then it will provision the resources in your account.\n\n4. Once the resources are provisioned, you should now see a local `.env` file with all the environment variables needed to run the scripts.\n5. To delete the resources, run:\n\n    ```shell\n    azd down\n    ```\n\n## Resources\n\n* [AutoGen Documentation](https://microsoft.github.io/autogen/)\n* [LangGraph Documentation](https://langchain-ai.github.io/langgraph/tutorials/introduction/)\n* [LlamaIndex Documentation](https://docs.llamaindex.ai/en/latest/)\n* [OpenAI Agents Documentation](https://openai.github.io/openai-agents-python/)\n* [OpenAI Function Calling Documentation](https://platform.openai.com/docs/guides/function-calling?api-mode=chat)\n* [PydanticAI Documentation](https://ai.pydantic.dev/multi-agent-applications/)\n* [Semantic Kernel Documentation](https://learn.microsoft.com/semantic-kernel/overview/)\n* [SmolAgents Documentation](https://huggingface.co/docs/smolagents/index)\n"
    },
    {
      "name": "victordibia/multiagent-systems-with-autogen",
      "stars": 118,
      "img": "https://avatars.githubusercontent.com/u/1547007?s=40&v=4",
      "owner": "victordibia",
      "repo_name": "multiagent-systems-with-autogen",
      "description": "Building LLM-Enabled Multi Agent Applications with AutoGen",
      "homepage": "https://mng.bz/eVP9",
      "language": "Jupyter Notebook",
      "created_at": "2024-05-12T14:51:51Z",
      "updated_at": "2025-04-21T13:07:54Z",
      "topics": [
        "agents",
        "autogen",
        "generative-ai",
        "large-language-models",
        "multiagent-systems"
      ],
      "readme": "# Multi-Agent Systems with AutoGen\n\nThis repository contains code examples for building multi-agent applications (powered by generative AI models) based on the [AutoGen](https://github.com/microsoft/autogen) framework and is the official code repository for the book - [Multi-Agent Systems with AutoGen](https://mng.bz/eVP9), published by Manning Publications.\n\n[![Multi-Agent Systems with AutoGen Manning Early Access Program](/docs/images/bookcover.png)](https://mng.bz/eVP9)\n\nThe book is currently available for pre-order in the [Manning Early Access Program](https://mng.bz/eVP9) (only the first few chapters are available currently).\n\nPre-order the book now at https://mng.bz/eVP9.\n\n> [!TIP]\n> 🔥🔥 Use the code **mldibia** for a 50% discount, valid until **August 5th**.\n\nIn [Multi-Agent Systems with AutoGen](https://mng.bz/eVP9), you will learn about the core components of agents (Generative AI models, tools, memory, orchestration), how to implement them in python code using the AutoGen framework, how to evaluate, optimize and deploy them in your applications. Overall, the book will cover:\n\n- Core components for multi-agent systems and their implementation\n- UX design principles for multi-agent systems\n- Building agents to interact with various interface (web, mobile, desktop)\n- Evaluating your multi-agent system using benchmarks like GAIA, GPTQ, SWEBench and your own custom benchmarks\n- Performance optimization (e.g., agent-specific model tuning and parallel processing)\n- Use case deep dives like data analysis, customer service, and creativity workflows.\n\n### Useful Links\n\n- Link to official [source code on GiHub](https://github.com/victordibia/multiagent-systems-with-autogen)\n- Link to book on [Manning.com](https://mng.bz/eVP9)\n- Link to book website (interactive demos, about authors etc) - https://multiagentbook.com/\n\n> [!NOTE]\n> If you downloaded the code bundle from the Manning website, please consider visiting the official code repository on GitHub at https://github.com/victordibia/multiagent-systems-with-autogen for the latest updates.\n\nTo download a copy of this code repository, click on the [Download Zip](https://github.com/victordibia/multiagent-systems-with-autogen/archive/refs/heads/main.zip) button or run the following code from your terminal.\n\n```bash\ngit clone --depth 1 https https://github.com/victordibia/multiagent-systems-with-autogen.git\n```\n\n## Getting Jupyter Notebooks to work on your computer\n\nThis section explains how to install the pre-requisite libraries so that you can use the notebooks within this book. So that the libraries are safely installed for the context of this book, we use the python [virtual environment](https://docs.python.org/3/library/venv.html) concept.\n\n1. [Install](https://www.python.org/downloads/) Python on your computer. Recommended versions are 3.9 through 3.12\n2. Clone the repository: `git clone https://github.com/victordibia/multiagent-systems-with-autogen.git`\n3. Go into the directory: `cd multiagent-systems-with-autogen`\n4. Create a virtual environment: `python -m venv venv`\n5. Activate the virtual environment: `. ./venv/bin/activate`\n6. Install the required libraries into this environment: `pip install -r requirements.txt`\n7. Run Jupyter Lab: `jupyter lab`\n8. Within Jupyter Lab, change directories into the respective chapter and open the python notebooks.\n\n## Table of Contents\n\nThe code in this repo is organized into chapters (shown in the table). Each chapter contains code for the various concepts and tools discussed in the book.\n\n<!-- chapter, description, code links\n1. Understanding Multi-Agent Systems.  no code\n2. Building Your First Multi-Agent Application /ch02\n3. THE USER EXPERIENCE (UX) OF MULTI-AGENT SYSTEMS\n  -->\n\n| Chapter | Description                                      | Code                                                                                                                                            |\n| ------- | ------------------------------------------------ | ----------------------------------------------------------------------------------------------------------------------------------------------- |\n| 1       | Understanding Multi-Agent Systems                | no code                                                                                                                                         |\n| 2       | Building Your First Multi-Agent Application      | - [Your first multi-agent application](/ch02/ch2_first_application.ipynb) <br> - [Orchestrating groups of agents](/ch02/ch2_agent_groups.ipynb) |\n| 3       | The User Experience (UX) for Multi-Agent Systems | no code                                                                                                                                         |\n| 4       | Interface Agents                                 | - [Interface library (built from scratch)](/src/interface) <br> - [Implementing an interface agent notebook](/ch04/interface_agents.ipynb)      |\n\n## Questions and Feedback\n\nIf you have any questions or feedback about the book or the code in this repository, please feel free to open an [issue]().\n\nFor questions about the AutoGen framework, you can also visit the [AutoGen GitHub repository](https://github.com/microsoft/autogen) or the [AutoGen documentation](https://microsoft.github.io/autogen/).\n\n### Citation\n\nIf you find this book or code useful for your research, please consider citing it:\n\n```\n@book{multiagentsystems2024,\n  author       = {Dibia, Victor},\n  title        = {Multi-Agent Systems with AutoGen},\n  publisher    = {Manning},\n  year         = {2024},\n  isbn         = {9781633436145},\n  url          = {https://www.manning.com/books/multi-agent-systems-with-autogen},\n  github       = {https://github.com/victordibia/multiagent-systems-with-autogen}\n}\n```\n"
    },
    {
      "name": "LittleLittleCloud/StepWise",
      "stars": 113,
      "img": "https://avatars.githubusercontent.com/u/16876986?s=40&v=4",
      "owner": "LittleLittleCloud",
      "repo_name": "StepWise",
      "description": "A C# framework to build, visualize and execute event-driven workflow",
      "homepage": "https://littlelittlecloud.github.io/StepWise/",
      "language": "C#",
      "created_at": "2024-09-10T00:13:59Z",
      "updated_at": "2025-04-07T01:19:01Z",
      "topics": [
        "agentic",
        "dotnet",
        "llm",
        "nextjs",
        "shadcn-ui",
        "typescript",
        "workflow",
        "workflow-engine"
      ],
      "readme": "<a name=\"readme-top\"></a>\n\n\n<div align=\"center\">\n\n<img src=\"./asset/stepwise-logo.svg\" alt=\"StepWise Logo\" width=\"100\">\n\n# StepWise\n\n<div align=\"center\">\n <strong> <h3> A code-first, event-driven workflow framework for .NET </h3> </strong>\n</div>\n<p align=\"center\">\n\n[![License](https://img.shields.io/badge/license-MIT-green)](./LICENSE)\n[![NuGet Version](https://img.shields.io/nuget/v/LittleLittleCloud.StepWIse?label=NuGet&labelColor=grey&color=green)](https://www.nuget.org/packages/LittleLittleCloud.StepWise)\n[![Doc](https://img.shields.io/badge/Doc-Online-blue)](https://littlelittlecloud.github.io/StepWise/)\n[![build](https://github.com/LittleLittleCloud/StepWise/actions/workflows/dotnet-build.yml/badge.svg)](https://github.com/LittleLittleCloud/StepWise/actions/workflows/dotnet-build.yml)\n</p>\n\n<p align=\"center\">\n    <a href=\"https://www.reddit.com/r/StepWise_AI\">\n        <img src=\"https://img.shields.io/badge/StepWise-%23FF4500.svg?style=for-the-badge&logo=Reddit&logoColor=white\" />\n    </a>\n    <a href=\"https://www.linkedin.com/groups/14564165\">\n        <img src=\"https://img.shields.io/badge/LinkedIn-0077B5?style=for-the-badge&logo=linkedin&logoColor=white\" />\n    </a>\n    <a  href=\"https://twitter.com/stepwise_ai\">\n        <img src=\"https://img.shields.io/twitter/follow/StepWise?style=social\" height=\"28\" />\n    </a>\n</p>\n\n<p align=\"center\">\n  <a href=\"https://stepwisegallery20241128154731.azurewebsites.net/\">\n      <picture >\n        <source width=\"225\" media=\"(prefers-color-scheme: dark)\" srcset=\"./asset/try-live-demo.svg\"  >\n        <source width=\"225\" media=\"(prefers-color-scheme: light)\" srcset=\"./asset/try-live-demo.svg\"  >\n        <img alt=\"Try Live Demo\" src=\"./asset/try-live-demo.svg\" >\n      </picture>\n  </a>\n</p>\n\n</div>\n\n[Demo](https://github.com/user-attachments/assets/cca9d32d-1b59-455e-b19b-535943047ad0)\n\n## What is StepWise?\nStepWise is a .NET framework which assists you to code, visualize and execute event-base workflow. It is designed to help you build complex workflows in a simple and efficient way. StepWise comes with the following key features:\n- **Code-First**: Define workflows using C# code in your project.\n- **WebUI** Visualize and execute workflows from your favorite browser using StepWise WebUI.\n- **Event-Driven**: Execute steps in parallel and resolve dependencies automatically.\n- **AI-Powered**: Work with `Geeno`, a built-in AI assistant in StepWise WebUI to help you run and analyze workflows with ease.\n- **MCP Integration**: Create machine-callable workflows by turning StepWise workflows into MCP servers.\n- **Microsoft.Extensions.AI Support**: Seamlessly integrate StepWise Workflow with Microsoft.Extensions.AI `IChatClient` to utilize AI capabilities.\n\n## Table of Contents\n- [Quick Start](#quick-start)\n  - [Quick Start with Template](#quick-start-with-template)\n  - [Quick Start with StepWise WebUI](#quick-start-with-stepwise-webui)\n  - [Quick Start with Microsoft.Extensions.AI](#quick-start-with-microsoftextensionsai)\n  - [Quick Start with MCP](#quick-start-with-mcp)\n- [Examples](#examples)\n- [Dependency Management between Steps](#dependency-management-between-steps)\n\n\n## Quick Start\n\nHere's a simple example of how to define a workflow to prepare dinner. The workflow consists of several steps, such as chopping vegetables, boiling water, cooking pasta, and cooking sauce. The final step is to serve dinner, which depends on all the previous steps. When executed, the workflow will automatically resolve the dependencies between steps and execute them in the parallel if possible.\n\n```csharp\npublic class PrepareDinner\n{\n    [Step(description: \"\"\"\n        This example demonstrates how to use stepwise to create a simple dinner preparation workflow.\n        - source code: [PrepareDinner.cs](https://github.com/LittleLittleCloud/StepWise/blob/main/example/HelloWorld/PrepareDinner.cs)\n\n        It returns the current time when the workflow starts and then simulates the preparation of dinner.\n        \"\"\")]\n    public async Task<DateTime> Start()\n    {\n        return DateTime.Now;\n    }\n\n    [Step(description: \"boil water\")]\n    [DependOn(nameof(Start))]\n    public async Task<string> BoilWater()\n    {\n        await Task.Delay(2000);\n\n        return \"Boiled water in 2 seconds\";\n    }\n\n    [Step(description: \"cut vegetables\")]\n    [DependOn(nameof(Start))]\n    public async Task<string> CutVegetables()\n    {\n        await Task.Delay(3000);\n\n        return \"Cut vegetables in 3 seconds\";\n    }\n\n    [Step(description: \"cook vegetables\")]\n    [DependOn(nameof(CutVegetables))]\n    [DependOn(nameof(BoilWater))]\n    public async Task<string> CookVegetables(\n        [FromStep(nameof(CutVegetables))] string vegetables,\n        [FromStep(nameof(BoilWater))] string water)\n    {\n        await Task.Delay(4000);\n\n        return $\"Cooked vegetables in 4 seconds. {vegetables}, {water}\";\n    }\n\n    [Step(description: \"cook meat\")]\n    [DependOn(nameof(Start))]\n    public async Task<string> CookMeat()\n    {\n        await Task.Delay(5000);\n\n        return \"Cooked meat in 5 seconds\";\n    }\n\n    [Step(description: \"\"\"\n        Serve dinner.\n        This will call all the preparation dinner steps in parallel and return the time taken to prepare the dinner.\n        \"\"\")]\n    [DependOn(nameof(CookVegetables))]\n    [DependOn(nameof(CookMeat))]\n    public async Task<string> ServeDinner(\n        [FromStep(nameof(Start))] DateTime start,\n        [FromStep(nameof(CookVegetables))] string vegetables,\n        [FromStep(nameof(CookMeat))] string meat)\n    {\n        var time = DateTime.Now - start;\n        return $\"Dinner ready in {time.TotalSeconds} seconds\";\n    }\n}\n```\n\n### Quick Start with Template\nStepWise provides a list of templates to help you get started quickly. To install the template, run the following command:\n\n```bash\ndotnet new -i LittleLittleCloud.StepWise.Template\n```\n\nThis will install the following templates:\n- stepwise-console: A console application template with StepWise.\n\nTo create a new project using the template, run the following command. This will create a new console application project with StepWise WebUI configured.\n\n```bash\ndotnet new stepwise-console\n```\n\nAfter creating the project, you can run the project and visit `http://localhost:5123` to see the StepWise WebUI.\n\n### Quick Start with StepWise WebUI\n\nStepWise WebUI is a built-in WebUI for visualizing and executing workflows. To use StepWise WebUI, simply add the following code to your project:\n\n```csharp\n// program.cs\nvar host = Host.CreateDefaultBuilder()\n    //.UseEnvironment(\"Development\")\n    .ConfigureWebHostDefaults(webBuilder =>\n    {\n        webBuilder.UseUrls(\"http://localhost:5123\");\n    })\n    .UseStepWiseServer()\n    .Build();\n\nawait host.StartAsync();\n```\n\nThen, use `StepWiseClient` to add workflows to StepWise UI:\n\n```csharp\nvar stepWiseClient = host.Services.GetRequiredService<StepWiseClient>();\n\n// Add the workflow to the StepWise server\nstepWiseClient.AddWorkflow(prepareDinner);\n\n// Wait for the host to shutdown\nawait host.WaitForShutdownAsync();\n```\n\nNow, you can visit `http://localhost:5123` to see the StepWise UI and execute the workflow.\n\n### Quick Start with Microsoft.Extensions.AI\nPlease visit [M.E.AI Examples](example/MEAI/) to learn how to integrate StepWise with Microsoft.Extensions.AI `IChatClient`. This will allow you to use AI capabilities in your workflows, such as generating text or analyzing data using AI models.\n\n### Quick Start with MCP\nPlease visit [MCP Examples](example/MCP/README.md) to learn how to create a StepWise workflow and turn it into an MCP server.\n\n## Examples\nYou can find more examples in the [examples](https://github.com/LittleLittleCloud/StepWise/tree/main/example) directory.\n\n## Dependency Management between Steps\n### Step Dependency\nIn StepWise, you can define dependencies between steps using the `[DependsOn]` attribute. This ensures that a step is executed only after its dependencies have been satisfied.\n\n> [!Note]\n> Prevent circular dependencies between steps, otherwise, the workflow engine will remind you with an exception.\n\n### Variable Dependency\nVariable dependencies of a step means that the step requires certain variables to be available in the context before it can be executed. If all variable dependencies are met, the step can be executed in parallel with other steps that don't have dependencies on it. In StepWise, variable dependencies are the input parameters of a step.\n\n> [!Note]\n> `[FromStep]` attribute doesn't affect the step dependency. It is used to pass the output of one step as input to another step.\n\nStepWise automatically manages dependencies between Steps:\n- Use the `[DependsOn]` attribute to specify dependencies between Steps.\n- The StepwiseEngine resolves these dependencies and ensures Steps are executed in the correct order.\n\n## Parallel Execution\n\nStepWise supports parallel execution of steps that do not have step dependencies on each other. This can significantly improve the performance of your workflows by executing independent steps concurrently.\n\n## `StepWiseEngine`\n`StepWiseEngine` is the core component of StepWise that manages the execution of workflows. It uses a consumer-producer approach to execute steps in the correct order while handling dependencies between steps and parallel execution when possible. You can visit this [documentation](./article/DeepDiveToStepWiseEngine.md) to learn more about how the `StepWiseEngine` works.\n\n## Primitives\n\nStepWise is built around two main primitives:\n\n### 1. Step\n\nA Step is the smallest unit of work in StepWise. It represents a single task or operation within a workflow.\n\n- **Definition**: A Step is essentially a C# method decorated with the `[Step]` attribute.\n- **Properties**:\n  - Name: A unique identifier for the step.\n  - Input Parameters: The data required by the step to perform its task.\n  - Output: The result produced by the step (if any). **Must be a Task or Task<\\T>**.\n  - Dependencies: Other steps that must be executed before this step. This is specified using the `[DependsOn]` attribute.\n- **Usage**: \n  ```csharp\n  [Step]\n  [DependsOn(nameof(OtherStep))]\n  [DependsOn(nameof(AnotherStep))]\n  public Task<Data> GetData(int id)\n  {\n      // Implementation\n  }\n  ```\n\n### 2. Workflow\n\nA Workflow is a collection of Steps that together accomplish a larger task.\n\n- **Definition**: A Workflow is typically represented by a class containing multiple Step methods.\n- **Usage**: \n  ```csharp\n  public class DataProcessingWorkflow\n  {\n      [Step(Name = \"GetData\")]\n      public Task<Data> GetData(int id) { /* ... */ }\n\n      [Step(Name = \"ProcessData\")]\n      [DependsOn(nameof(GetData))]\n      public Task<Result> ProcessData([FromStep(\"GetData\")] Data data) { /* ... */ }\n\n      [Step(Name = \"SaveResult\")]\n        [DependsOn(nameof(ProcessData))]\n      public Task<string> SaveResult([FromStep(\"ProcessData\")] Result result) { /* ... */ }\n  }\n  ```\n\n## Contributing\n\nWe welcome contributions to StepWise! Please see our [Contributing Guide](https://github.com/LittleLittleCloud/StepWise/tree/main/CONTRIBUTING.md) for more details.\n\n## License\n\nStepWise is released under the MIT License. See the [LICENSE](https://github.com/LittleLittleCloud/StepWise/tree/main/LICENSE) file for details.\n\n## Support\n\nIf you encounter any issues or have questions, please file an issue on the GitHub issue tracker.\n"
    },
    {
      "name": "tylike/AI.Labs",
      "stars": 94,
      "img": "https://avatars.githubusercontent.com/u/920370?s=40&v=4",
      "owner": "tylike",
      "repo_name": "AI.Labs",
      "description": "openai chatgpt or local llm(llama.cpp gguf format)+TTS+STT+Word+Excel",
      "homepage": "",
      "language": "C#",
      "created_at": "2023-12-10T12:58:22Z",
      "updated_at": "2025-04-09T08:57:45Z",
      "topics": [
        "chatgpt",
        "excel",
        "llamacpp",
        "llm",
        "openai-api",
        "stt",
        "tts",
        "whispercpp",
        "word"
      ],
      "readme": "# AI.Labs (Cloud Ladder AI Integration Laboratory)\n\n[中文版](/Readme.cn.md)\n\nAI.Labs is an open-source project that aims to integrate the most advanced artificial intelligence technologies to provide a powerful AI platform. The core of this project lies in the integration of various AI services, including large language models, speech recognition, and speech synthesis, to support functionalities such as dialogue, voice interaction, and meeting transcription.  \nQQ Group: 306499618  \nThis project includes the following features:  \n[Bilibili Feature Demonstration Video 1](https://www.bilibili.com/video/BV1jC4y1Y7et/?share_source=copy_web&vd_source=11323d03e28fe3d5d656ff7d4c5662fb) Clicking the triple link will get you the installation package + model  \n[Bilibili Chat Video Demonstration 2](https://www.bilibili.com/video/BV1hb4y1579q/?share_source=copy_web&vd_source=11323d03e28fe3d5d656ff7d4c5662fb)  \n[How to Run Documentation](https://github.com/tylike/AI.Labs/blob/master/HowToRun.md)  \n![Functionality](./AI.Labs.Win/Images/ModuleMind.png)  \n\n[Project Repository:](https://github.com/tylike/ai.labs)  \n\n![Chat Interface](./AI.Labs.Win/Images/AI.Labs.Chat.png)  \n![Chat Settings](https://github.com/tylike/AI.Labs/blob/master/AI.Labs.Win/Images/ChatSettings.png)  \n![Audio Books](https://github.com/tylike/AI.Labs/blob/master/AI.Labs.Win/Images/AudioBook.png)  \n\nMain Features\n## 1. Large Language Model Dialogue System\n### 1.1 OpenAI API Interface:\nThe project integrates support for OpenAI's GPT-3.5 and GPT-4, enabling real-time intelligent dialogue through the OpenAI API.\nUtilizes the following project: https://github.com/betalgo/openai\n### 1.2 Local Language Model Server:\nDeploy large language models (LLM) locally using LMStudio or ChatGLM3-6B, interacting with a similar API to OpenAI, ensuring data privacy while improving response speed.\nNote: model and model services are not limited; any with OpenAI-style API support can be used.\n\nDownload Address: https://lmstudio.ai/  \nModel Download: https://huggingface.co/TheBloke (VPN required)  \nModel Download: https://hf-mirror.com/TheBloke (VPN not required)  \nModel Download: https://wisemodel.cn/models (VPN not required)  \nModel Download: https://gpt4all.io/ (VPN not required)  \n\n# 2. Speech Recognition STT/ASR\n## 2.1 Local Speech Recognition:\n### 2.1.1 Using whisper.cpp\nImplements local speech recognition, reducing dependence on external services and enhancing response speed.\n\n### 2.1.2 Azure Speech Recognition:\nAlso provides integration with Azure's speech recognition service to leverage its advanced cloud-based speech recognition technology.\n\n## 2.2 STT/ASR Application 1: Meeting Transcription Feature\nUsing the project's speech recognition capabilities, AI.Labs can carry out real-time recording of meeting content, making information archiving and retrieval more efficient.\nTechnology Stack\nThe project utilizes the DevExpress Application Framework, ensuring modernization and high customizability of the user interface.\n\n# 3. Speech-to-Text TTS\n## 3.1 EdgeTTS Voice Synthesis:\nUses EdgeTTS for local voice output, free of charge.\nUtilizes the python project edge-tts.\nRequires local installation of this python program.\nInstallation guide: https://github.com/rany2/edge-tts\n## 3.2 Azure TTS Voice Synthesis\nUsers can register their own accounts, with 5 hours per month free or 5 million characters.\nSupports Azure TTS service, allowing users to opt for cloud-based voice synthesis service.\nRegistration Address: https://portal.azure.com/  \n\n# 5. Integration of Translation and Chat\n\n# 6. DevExpress Application Framework (XAF)\nThe project uses XAF, website: http://www.devexpress.com\nPlease purchase the legitimate version of DevExpress for use in real-world environments.\nFor testing purposes, a one-month trial version can be downloaded.\n# 7. Environment\nAI.Labs is based on a combination of technologies including C# (.Net 6.0+)/SQLite database+XAF+OPENAI API+TTS+STT and more.\nThe dependency on python is minimized as much as possible. However, for instance, edge-tts still needs to be called upon.\n\nII. Questions and Doubts\n1. Data Privacy and Security: How is user data privacy and security ensured?\nIf you pay attention to this option, you can use LMStudio to build a private model locally (or within a LAN).\nSupports most models in gguf format or models from llama2.cpp.\nThis process is introduced in the Bilibili video. Note: The video is in Chinese.\nhttps://www.bilibili.com/video/BV1Fc411D7eK/  \n\n2. Multilingual Support: Does the project support multiple languages, and how are translations between different languages handled?\nXAF facilitates convenient expansion for multiple languages. Various versions will be rolled out progressively.\n"
    },
    {
      "name": "xuzeyu91/EasyAgent",
      "stars": 93,
      "img": "https://avatars.githubusercontent.com/u/26290929?s=40&v=4",
      "owner": "xuzeyu91",
      "repo_name": "EasyAgent",
      "description": "使用Blazor+AutoGen打造多角色的会话Agent，打造有趣的智能体。",
      "homepage": "",
      "language": "C#",
      "created_at": "2024-05-07T12:55:50Z",
      "updated_at": "2025-04-03T14:24:17Z",
      "topics": [
        "agent",
        "autogen",
        "blazor",
        "llm"
      ],
      "readme": "# EasyAgent\n使用AutoGen打造多角色的会话Agent，打造有趣的智能体。\n\n## 一个简单的测试项目\n通过.Net 集成AutoGen,可以在页面快速的配置不同角色的Agent进行群聊。示例如下：\n\n![demo](https://github.com/xuzeyu91/EasyAgent/blob/main/img/demo.png?raw=true)\n"
    },
    {
      "name": "Senparc/Senparc.AI",
      "stars": 85,
      "img": "https://avatars.githubusercontent.com/u/6435496?s=40&v=4",
      "owner": "Senparc",
      "repo_name": "Senparc.AI",
      "description": "Senparc 全家桶的 AI 扩展包",
      "homepage": null,
      "language": "HTML",
      "created_at": "2023-04-03T12:49:05Z",
      "updated_at": "2025-04-17T14:17:13Z",
      "topics": [],
      "readme": "# Senaprc.AI\r\nSenparc 全家桶的 AI 扩展包，目前主要集中于 LLM（大语言模型）的交互。\r\n\r\n\r\n## 项目介绍\r\n\r\n| 名称 | 说明 | Nuget |\r\n|--------|--------|--------|\r\n| Senparc.AI | 为所有标准接口和基础功能的基础模块 | [![Senparc.AI](https://img.shields.io/nuget/v/Senparc.AI.svg)](https://www.nuget.org/packages/Senparc.AI/) |\r\n| Senparc.AI.Kernel | 为基于 Senparc.AI 标准，使用 [SemanticKernel](https://github.com/microsoft/semantic-kernel) 实现的接口调用，可以实现即插即用。|  [![Senparc.AI.Kernel](https://img.shields.io/nuget/v/Senparc.AI.Kernel.svg)](https://www.nuget.org/packages/Senparc.AI.Kernel/) |\r\n| Senparc.AI.Agents | 为基于 Senparc.AI 标准，使用 [AutoGen](https://github.com/microsoft/autogen) 实现的 Agent 集成扩展模块。|  [![Senparc.AI.Agents](https://img.shields.io/nuget/v/Senparc.AI.Agents.svg)](https://www.nuget.org/packages/Senparc.AI.Agents/) |\r\n`Senparc.AI.PromptRange`<br>([独立项目](https://github.com/Senparc/Senparc.AI.PromptRange)) | 为基于 Senparc.AI 标准，为“PromptRange（提示词靶场）”生态提供底层标准支持的底层库。目前已经由 [Senparc.Xncf.PromptRange](https://github.com/NeuCharFramework/NcfPackageSources/tree/master/src/Extensions/Senparc.Xncf.PromptRange) 实现，可用于开发基于 PromptRange 的扩展应用，兼容 Web、桌面、手机等系统（支持 .NET 6.0 及以上框架）。[NeuCharFramework(NCF)](https://github.com/orgs/NeuCharFramework) 框架已经默认集成 [Senparc.Xncf.PromptRange](https://github.com/NeuCharFramework/NcfPackageSources/tree/master/src/Extensions/Senparc.Xncf.PromptRange)，可无需修改任何代码，直接使用。（[什么是 PromptRange？](https://github.com/Senparc/Senparc.AI.PromptRange/wiki/What's-PromptRange%3F)） |  |\r\n\r\n## 开发过程\r\n\r\n### 第一步：配置账号\r\n\r\n在 appsettings.json 中配置 OpenAI 或 Azure OpenAI 的接口信息，如：\r\n\r\n``` json\r\n//Senparc.AI 设置\r\n\"SenparcAiSetting\": {\r\n  \"IsDebug\": true,\r\n  \"AiPlatform\": \"NeuCharAI\", //注意修改为自己平台对应的枚举值\r\n  \"NeuCharAIKeys\": {\r\n    \"ApiKey\": \"<Your ApiKey>\", //在 https://www.neuchar.com/Developer/AiApp 申请\r\n    \"NeuCharEndpoint\": \"https://www.neuchar.com/<DeveloperId>\", //查看 ApiKey 时可看到 DeveloperId\r\n    \"ModelName\": {\r\n      \"Chat\": \"gpt-4o\",\r\n      \"Embedding\": \"text-embedding-ada-002\",\r\n      \"TextCompletion\": \"gpt-35-turbo-instruct\"\r\n    }\r\n  },\r\n  \"AzureOpenAIKeys\": {\r\n    \"ApiKey\": \"<Your AzureApiKey>\", \r\n    \"AzureEndpoint\": \"<Your AzureEndPoint>\", //https://xxxx.openai.azure.com/\r\n    \"AzureOpenAIApiVersion\": \"2022-12-01\", //调用限制请参考：https://learn.microsoft.com/en-us/azure/cognitive-services/openai/quotas-limits\r\n    \"ModelName\": {\r\n      \"Chat\": \"gpt-35-turbo\"\r\n    }\r\n  },\r\n  \"OpenAIKeys\": {\r\n    \"ApiKey\": \"<Your OpenAIKey>\",\r\n    \"OrganizationId\": \"<Your OpenAIOrgId>\",\r\n    \"OpenAIEndpoint\": null,\r\n    \"ModelName\": {\r\n      \"Chat\": \"gpt-35-turbo\"\r\n    }\r\n  },\r\n  \"HuggingFaceKeys\": {\r\n    \"Endpoint\": \"<Your EndPoint>\", //HuggingFace 的 Endpoint\r\n    \"ModelName\": {\r\n      \"TextCompletion\": \"chatglm2\"\r\n    }\r\n  },\r\n  \"Items\": {\r\n    // 更多自定义配置\r\n  }\r\n}\r\n```\r\n\r\n其中：`AiPlatform` 是平台类型，目前可选值为 `OpenAI`、`NeuCharAI`、`AzureOpenAI`、`HuggingFace` 和 `FastAPI`，分别对应 openai.com 官方接口（OpenAI）、https://www.neuchar.com 由 Senparc 提供的中转接口、基于微软 Azure 的 AzureOpenAI 接口（AOAI)、HuggingFace 接口和 FastAPI 接口，系统会根据配置自动实现切换，无需在逻辑代码中进行判断。\r\n\r\n仅当 `AiPlatform` 设置为 `OpenAI` 时，才需要设置 `OpenAIKeys` 及以下参数。\r\n\r\n仅当 `AiPlatform` 设置为 `NeuCharAI` 时，才需要设置 `NeuCharAIKeys` 及以下参数。\r\n\r\n仅当 `AiPlatform` 设置为 `AzureOpenAI` 时，才需要设置 `AzureOpenAIKeys` 及以下参数。\r\n\r\n其他平台以此类推。\r\n\r\n每一个平台类型配置下，都有一个 `ModelName` 节点，用于设置该平台下需要被支持的模型类型，例如需要将 Chat 接口设置为 GPT-4 模型，则设置：`\"Chat\": \"gpt-4\"`。\r\n\r\n> 提示：AzureOpenAI 调用限制请参考：https://learn.microsoft.com/en-us/azure/cognitive-services/openai/quotas-limits<br>\r\n> OpenAI 调用限制请参考 OpenAI 后台：https://platform.openai.com/docs/guides/rate-limits\r\n\r\n#### 进阶（一）：配置多模型环境\r\n\r\n#### 进阶（二）：动态配置模型参数\r\n\r\n### 第二步：开发\r\n\r\nSenparc.AI 使用了创新的对话式编程体验，您无需了解过多不同平台、SDK 的详细用法，只需要按照自己的想法进行定义和编程，最后接收结果，以目前最火的聊天场景（Chat）为例：\r\n\r\n```C#\r\n//获取 AI 模型配置（从 appsettings.json 自动读取）\r\nvar aiSetting = Senparc.AI.Config.SenparcAiSetting;\r\n\r\n// 创建 AI Handler 处理器（也可以通过工厂依赖注入）\r\nvar handler = new SemanticAiHandler(aiSetting);\r\n\r\n// 定义 AI 接口调用参数和 Token 限制等\r\nvar promptParameter = new PromptConfigParameter()\r\n{\r\n    MaxTokens = 2000,\r\n    Temperature = 0.7,\r\n    TopP = 0.5,\r\n};\r\n\r\n// 准备运行\r\nvar userId = \"JeffreySu\";//区分用户\r\nvar iWantToRun = \r\n     handler.IWantTo()\r\n            .ConfigModel(aiSetting.ModelName.Chat, userId, modelName)\r\n            .BuildKernel()\r\n            .RegisterSemanticFunction(\"ChatBot\", \"Chat\", promptParameter)\r\n            .iWantToRun;\r\n\r\n// 输入/提问，获取结果\r\nvar prompt = \"请问中国有多少人口？\";\r\nvar aiRequest = iWantToRun.CreateRequest(prompt, true, true);\r\nvar aiResult = await iWantToRun.RunAsync(aiRequest);\r\n//aiResult.Result 结果：中国的人口约为13.8亿。\r\n```\r\n\r\n\r\n<img width=\"623\" alt=\"image\" src=\"https://user-images.githubusercontent.com/2281927/230152103-3486fbfc-2426-407c-bcb6-74d4485eaf91.png\">\r\n\r\n## 示例\r\n\r\n所有快速参考示例位于 `/Samples/` 文件夹内\r\n\r\n文件夹 |  说明\r\n------|--------\r\nSamples/Senparc.AI.Samples.Consoles | 命令行\r\nSamples/Senparc.AI.Samples.Agents | Agent（智能体）测试，已集成 AutoGen\r\n\r\n## 命令示例行使用说明\r\n\r\n### 1. 打开解决方案\r\n\r\n打开解决方案 `Senparc.AI.sln`，设置 appsettings.json 中 ApiKey 等平台参数，启动项目 `Senparc.AI.Samples.Consoles`：\r\n\r\n<img width=\"801\" alt=\"image\" src=\"https://github.com/Senparc/Senparc.AI/assets/2281927/d38d15cd-6b88-488e-8b74-a26934dfb538\">\r\n\r\n### 2. 操作\r\n\r\n#### 2.1 对话\r\n\r\n输入 `1`，进入对话操作：\r\n\r\n<img width=\"738\" alt=\"image\" src=\"https://github.com/Senparc/Senparc.AI/assets/2281927/32933f21-c8db-4090-ad5a-a955039ee407\">\r\n\r\n#### 2.2 TextCompletion\r\n\r\n在主界面输入 `2` 进入 TextCompletion 操作：\r\n\r\n<img width=\"717\" alt=\"image\" src=\"https://github.com/Senparc/Senparc.AI/assets/2281927/b28e67de-5334-4703-8b74-3da7f9c328fd\">\r\n\r\n\r\n#### 2.3 Embedding\r\n\r\n在主界面输入 `3` 即可进入 Embedding 操作，Embedding 分为常规信息和引用信息两类，将在下一步中做选择：\r\n\r\n#### 2.3.1 常规 Embedding（Information）\r\n\r\n选择 `1`，进入到常规 Embedding 测试，输入信息由 3 个英文冒号分割，录入完成后输入 `n` 开始对话测试：\r\n\r\n<img width=\"708\" alt=\"image\" src=\"https://github.com/Senparc/Senparc.AI/assets/2281927/f22de4b1-1920-46fb-97fa-18265374d2ad\">\r\n\r\n\r\n#### 2.3.2 引用 Embedding（Reference）\r\n2.2.2 上一步选择 `2`，进入到引用 Embedding 测试，输入信息由 3 个英文冒号分割，录入完成后输入 `n` 开始对话测试：\r\n\r\n<img width=\"957\" alt=\"image\" src=\"https://github.com/Senparc/Senparc.AI/assets/2281927/effd6bab-ba23-40ec-a125-7270b4dbbd29\">\r\n\r\n\r\n#### 2.4 DallE 绘图操作\r\n\r\n初始界面中输入 `4`，进入 DallE 接口的绘图操作：\r\n\r\n<img width=\"941\" alt=\"image\" src=\"https://github.com/Senparc/Senparc.AI/assets/2281927/e353b059-3399-425a-80f3-ec0eed767b90\">\r\n\r\n结果将以 URL 的形式返回，此时出入 `s` ，可保存图片到本地：\r\n\r\n<img width=\"951\" alt=\"image\" src=\"https://github.com/Senparc/Senparc.AI/assets/2281927/61daa2e3-a781-40ea-b47b-c360a3cc4464\">\r\n\r\n> 注意：接口返回的 URL 是一个暂存地址，不可用于持久化的展示，需要及时保存，\r\n\r\n## 单元测试\r\n\r\n<img width=\"498\" alt=\"image\" src=\"https://github.com/Senparc/Senparc.AI/assets/2281927/6ec61a15-e1c7-423b-9886-0e401c070e35\">\r\n\r\n## TODO:\r\n1. [x] 实现更多模型和模式的匹配。\r\n1. [x] 实现全自动的工厂模块自动配置。\r\n1. [x] 集成到 [Senaprc.Weixin SDK](https://github.com/JeffreySu/WeiXinMPSDK)，0 逻辑代码实现 AI 能力接入（聊天场景为主）。\r\n1. [x] 集成到 [NeuCharFramework](https://github.com/NeuCharFramework/NCF)，0 逻辑代码实现 AI 能力接入（开发和云运营场景为主）。\r\n1. [ ] 完成更多默认模型适配（目前已经提供自定义扩展能力）\r\n1. [ ] 完善独立文档\r\n"
    },
    {
      "name": "Azure/gpt-rag-agentic",
      "stars": 79,
      "img": "https://avatars.githubusercontent.com/u/6844498?s=40&v=4",
      "owner": "Azure",
      "repo_name": "gpt-rag-agentic",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2024-09-13T02:10:37Z",
      "updated_at": "2025-04-14T02:10:17Z",
      "topics": [],
      "readme": "# Enterprise RAG Agentic Orchestrator\n\nPart of [GPT-RAG](https://aka.ms/gpt-rag)\n\n## Table of Contents\n\n1. [**Concepts**](#concepts)\n   - [1.1 How the Orchestrator Works](#how-the-orchestrator-works)\n   - [1.2 Agent Strategies](#selecting-an-agent-strategy)\n   - [1.3 NL2SQL and Chat with Fabric](docs/NL2SQL.md)   \n   - [1.4 Create your Own Agent Strategy](#how-to-add-and-configure-you-own-agent-strategies)\n2. [**Running the Orchestrator**](#running-the-orchestrator)\n   - [2.1 Environment Variables](#environment-variables)\n   - [2.2 Cloud Deployment](#cloud-deployment)\n   - [2.3 Running the Chat Client Locally](#running-the-chat-client-locally)\n   - [2.4 Running the Function Locally](docs/LOCAL_DEPLOYMENT.md)\n3. [**Evaluation**](#evaluation)\n4. [**Contributing**](#contributing)\n5. [**Trademarks**](#trademarks)\n\n---\n\n## Concepts\n\n### How the Orchestrator Works\n\nThe **GPT-RAG Agentic Orchestrator** is a powerful system that leverages AutoGen's AgetChat programming framework to facilitate collaboration among multiple specialized agents. This orchestrator is designed to handle complex tasks by coordinating the interactions of agents, each with a specific role, to produce coherent and accurate responses. For more details about AutoGen, refer to its [documentation](https://microsoft.github.io/autogen/stable/).\n\n#### Multi-Agent Group Chat    \n     \nOur orchestrator employs AutoGen's `Selector Group Chat` pattern to facilitate dynamic conversations involving multiple agents. The group chat coordinates agent interactions based on the selected strategy.\n\n![Selector Group Chat](media/selector-group-chat.svg)\n<BR>*Example of a Group Chat obtained from the AutoGen documentation.*\n\n> [!Note]\n> The agents shown here are examples. You can define your own agents tailored to your use cases.\n\n#### Agent Strategies\n\nThe orchestrator uses a factory pattern to create agents based on predefined strategies. The `AgentStrategyFactory` handles the creation of agents for each selected strategy, such as `classic_rag` or `nl2sql`. Each strategy defines a unique set of agents, their roles, and their interactions within the group chat.\n\nDifferent strategies support various queries and data interactions:\n\n- **Classic RAG Strategy (`classic_rag`)**: This strategy centers on retrieval-augmented generation, where agents collaborate to retrieve relevant information from a knowledge base and generate responses based on that information.\n- **NL2SQL Strategy (`nl2sql`)**: This strategy translates natural language queries into SQL statements, enabling users to interact with databases using everyday language.\n\n#### Elements of a Strategy:\n     \n- **The Agents Team**: Agents are instantiated with distinct roles and system messages. For example, in the `multimodal_rag` strategy, the agents include:    \n       \n   - **Assistant Agent**: Processes the user ask and invokes the necessary tools, such as `multimodal_vector_index_retrieve`, to gather relevant data. \n   - **Multimodal Message Creator**: Constructs a `MultiModalMessage` containing text and image data, ensuring the response is based on multimodal content. \n     \n- **Functions used by agents**: Functions (or tools) empowers agents it with specific capabilities such as data retrieval or executing complex queries. There's no need to register tools separately, simply inform the `AssistantAgent` which tool to execute.    \n     \n- **Agent Selection**: Transition rules are established to control the flow of conversations between agents. The **Selector Function** defines rules for selecting the next agent to engage in the conversation based on the current context and message flow.    \n\n#### Customization and Extensibility\n\nThe orchestrator is highly customizable, allowing developers to define custom strategies and agent behaviors. By subclassing `BaseAgentStrategy` and implementing the `create_agents` method, new strategies can be created to meet specific requirements. This extensibility ensures that the orchestrator can adapt to a wide range of operational scenarios.\n\n### Selecting an Agent Strategy\n\nThe **GPT-RAG Agentic Orchestrator** provides a range of agent strategies to handle different types of queries and data interactions. Selecting the appropriate strategy ensures that the orchestrator operates efficiently and meets the specific needs of your application. This section outlines how to select a strategy and provides detailed descriptions of the available strategies.\n\n### How to Select a Strategy\n\nThe orchestrator selects the agent strategy based on the `AUTOGEN_ORCHESTRATION_STRATEGY` environment variable. Be sure to set this variable to the name of the desired strategy. If you're running the chat client locally, set this variable in your shell environment. For deployments as a Function App, define it in the application settings.\n\n#### Available Strategies\n\nThe orchestrator supports the following strategies, each tailored to specific needs:\n\n- **Classical RAG**: The `classic_rag` strategy is the default mode of operation for the orchestrator. It is optimized for retrieving information from a predefined knowledge base indexed as an AI Search Index. This strategy leverages retrieval-augmented generation (RAG) techniques to fetch and synthesize information from existing documents or databases, ensuring accurate and relevant responses based on the available data.\n\n- **Multimodal RAG**: In the `multimodal_rag` strategy, user queries are searched in an index containing text content and image descriptions. The system combines text and images to generate a comprehensive response.\n\n- **NL2SQL**: The `nl2sql` strategy enables the orchestrator to convert natural language queries into SQL statements. This allows users to interact with relational databases using everyday language, simplifying data retrieval processes without the need to write complex SQL queries. Currently, this strategy is designed to execute queries on SQL databases in Azure.\n\n- **NL2SQL Fewshot**: The `nl2sql_fewshot` strategy enhances the standard `nl2sql` approach by utilizing AI-driven search to identify similar past queries. This few-shot learning technique improves the accuracy and relevance of the generated SQL statements by learning from a limited set of examples, thereby refining the query translation process.\n\n- **Chat with Fabric**: The `chat_with_fabric` strategy allows the orchestrator to interact with data stored in Microsoft Fabric, enabling users to query elements such as tables within both Lakehouse and semantic models. This strategy provides easy access to structured and semi-structured data in Fabric, allowing users to retrieve insights without deep knowledge of the underlying architecture.\n\n> [!TIP]\n> [Click here](docs/NL2SQL.md) for more information about **NL2SQL** and **Chat with Fabric** Strategies.\n\n### How to Add and Configure you Own Agent Strategies\n\nIf the available strategies don’t fully meet your requirements, you can extend the orchestrator by implementing custom strategies. This flexibility lets you adapt the orchestrator’s behavior to unique use cases and operational demands.\n\nDefine custom agent strategies by specifying distinctive agent behaviors. To create a custom strategy:\n\n1. **Subclass** `BaseAgentStrategy` and implement the `create_agents` method.\n2. **Register** the strategy in `AgentStrategyFactory` for environment variable selection.\n\n**Steps to Add a Custom Strategy:**\n\n1. **Create the Strategy Class:**  \n   Define a new strategy by inheriting from the base strategy class and specifying the required logic.\n   \n   ```python\n   from .strategies.base_strategy import BaseAgentStrategy\n\n   class CustomAgentStrategy(BaseAgentStrategy):\n       def execute(self, query):\n           # Implement custom strategy logic here\n           pass\n   ```\n\n2. **Update the AgentStrategyFactory:**  \n   Modify `AgentStrategyFactory` to recognize and instantiate your custom strategy.\n\n   ```python\n   from .strategies.custom_agent_strategy import CustomAgentStrategy\n\n   class AgentStrategyFactory:\n       @staticmethod\n       def get_strategy(strategy_type: Strategy):\n           # Existing strategy selections\n           if strategy_type == 'custom':\n               return CustomAgentStrategy()\n           # Other strategies...\n           else:\n               raise ValueError(f\"Unknown strategy type: {strategy_type}\")\n   ```\n\nEnsure the `AUTOGEN_ORCHESTRATION_STRATEGY` environment variable is correctly set to the desired strategy name, whether a predefined strategy or a custom one you’ve implemented.\n\n> [!NOTE]\n> The name `custom` is used here as an example. You should choose a name that best represents your specific case.\n\n---\n\n## Running the Orchestrator\n\n### Environment Variables\n\nTo successfully run the orchestrator Function App, you must configure the following environment variables. These variables should be defined locally using the `.env` file (following the structure from `.env.template`) and in cloud environments, such as Azure Function App, via the `app_settings.json` file in the Application Settings section.\n\n#### App Environment Variables\n\n| **Variable**                         | **Description** |\n|--------------------------------------|-----------------|\n| `APPLICATIONINSIGHTS_CONNECTION_STRING` | Connection string for Application Insights (telemetry and monitoring). |\n| `AUTOGEN_ORCHESTRATION_STRATEGY`       | The agent orchestration strategy (e.g., `multimodal_rag`). |\n| `AZURE_DB_ID`                        | Cosmos DB account ID (used to construct the Cosmos DB endpoint). |\n| `AZURE_DB_NAME`                      | Name of the Cosmos DB database. |\n| `AZURE_KEY_VAULT_NAME`               | Name of your Azure Key Vault (used for secrets retrieval). |\n| `AZURE_OPENAI_API_KEY`               | API key for Azure OpenAI access. |\n| `AZURE_OPENAI_API_VERSION`           | API version for Azure OpenAI (recommended: `2024-10-21`). |\n| `AZURE_OPENAI_CHATGPT_DEPLOYMENT`    | Deployment name of the ChatGPT model (e.g., `chat`). |\n| `AZURE_OPENAI_CHATGPT_MODEL`         | Name of the ChatGPT model (recommended: `gpt-4o`). |\n| `AZURE_OPENAI_EMBEDDING_DEPLOYMENT`  | Deployment name for the embedding model. |\n| `AZURE_OPENAI_EMBEDDING_MODEL`       | Name of the embedding model (recommended: `text-embedding-3-large`). |\n| `AZURE_OPENAI_MAX_TOKENS`            | Maximum number of tokens allowed in responses. |\n| `AZURE_OPENAI_RESOURCE`              | Azure OpenAI resource name (used to build the endpoint). |\n| `AZURE_SEARCH_API_VERSION`           | Azure Search API version (recommended: `2024-07-01`). |\n| `AZURE_SEARCH_APPROACH`              | Search approach to be used (recommended: `hybrid`). |\n| `AZURE_SEARCH_INDEX`                 | Name of the Azure Search index (e.g., `ragindex`). |\n| `AZURE_SEARCH_SEMANTIC_SEARCH_CONFIG`| Name of the semantic search configuration. |\n| `AZURE_SEARCH_SERVICE`               | Azure Search service name. |\n| `AZURE_SEARCH_TOP_K`                 | Maximum number of search results to return. |\n| `AZURE_SEARCH_USE_SEMANTIC`          | Enable (`true`) or disable (`false`) semantic search. |\n| `AZURE_STORAGE_ACCOUNT`              | Storage account name where documents are stored. |\n| `PYTHON_ENABLE_INIT_INDEXING`        | `1` | Enables initial Python indexing. |\n| `PYTHON_ISOLATE_WORKER_DEPENDENCIES` | `1` | Isolates Python worker dependencies. |\n\n#### Azure Function App Specific Settings\n\nThese variables must be configured directly in Function App Service as **Application Settings**, and do not need to be included in your local `.env` file. The recommended values are provided below.\n\n| **Variable**                         | **Recommended Value** | **Description** |\n|--------------------------------------|----------------------|-----------------|\n| `AzureWebJobsStorage__accountName`   | _your storage account name_ | Storage account used by Azure Functions. |\n| `AzureWebJobsStorage__credential`    | `managedidentity` | Credential type for storage access. |\n| `BUILD_FLAGS`                        | `UseExpressBuild` | Enables express build mode. |\n| `ENABLE_ORYX_BUILD`                  | `true` | Enables the Oryx build system. |\n| `SCM_DO_BUILD_DURING_DEPLOYMENT`     | `true` | Builds the app during deployment. |\n| `WEBSITE_HTTPLOGGING_RETENTION_DAYS` | `1` | Number of days to retain HTTP logs. |\n| `XDG_CACHE_HOME`                     | `/tmp/.cache` | Cache directory used during build/runtime. |\n| `FUNCTIONS_EXTENSION_VERSION`        | `~4` | Specifies the Azure Functions runtime version. |\n| `FUNCTIONS_WORKER_RUNTIME`           | `python` | Azure Functions worker runtime. |\n\n### Cloud Deployment\n\nThere are two main options for deploying the orchestrator to the cloud:\n\n#### **Option 1: Deploy using the Azure Developer CLI (Recommended)**\nIf you have previously provisioned GPT-RAG components using `azd provision`, this is the best approach, as it ensures deployment consistency with the existing infrastructure.\n\n```bash\nazd auth login\nazd env refresh\nazd deploy\n```\n\nThis method automatically detects your environment settings, applies necessary configurations, and deploys the orchestrator with minimal manual intervention.\n\nEnsure you have:\n- [Python 3.11](https://www.python.org/downloads/release/python-3118/)\n- [Azure Developer CLI](https://learn.microsoft.com/en-us/azure/developer/azure-developer-cli/install-azd) installed.\n\n#### **Option 2: Deploy using Azure Functions Core Tools**\nIf you have not used `azd provision` or are deploying in a standalone manner, you can publish the Function App directly using Azure Functions Core Tools.\n\n```bash\naz login\nfunc azure functionapp publish FUNCTION_APP_NAME --python\n```\n\nReplace `FUNCTION_APP_NAME` with the actual name of your Function App.\n\nAfter deployment, verify the function is available:\n```bash\nfunc azure functionapp list-functions FUNCTION_APP_NAME\n```\n\nEnsure you have:\n- [Azure Functions Core Tools](https://learn.microsoft.com/en-us/azure/azure-functions/functions-run-local?tabs=windows%2Cisolated-process%2Cnode-v4%2Cpython-v2%2Chttp-trigger%2Ccontainer-apps&pivots=programming-language-python#install-the-azure-functions-core-tools)\n- [Azure CLI](https://learn.microsoft.com/en-us/cli/azure/install-azure-cli) installed.\n\n### Running the Chat Client Locally\n\n1. Make sure your user has the roles needed to access CosmosDB and AI Search.\n\n### Bash\n```bash\n# Set variables for Cosmos DB role assignment\nresourceGroupName='your resource group name'  # Name of your resource group\ncosmosDbaccountName='CosmosDB Service name'   # Name of your CosmosDB account\nroleDefinitionId='00000000-0000-0000-0000-000000000002'  # Built-in CosmosDB role ID for Data Contributor\nprincipalId='Object id of your user in Microsoft Entra ID'  # Object ID of the user in Microsoft Entra ID\n\n# Assign CosmosDB Data Contributor role to the user\naz cosmosdb sql role assignment create --account-name $cosmosDbaccountName --resource-group $resourceGroupName --scope \"/\" --principal-id $principalId --role-definition-id $roleDefinitionId\n\n# Set variables for Azure OpenAI role assignment\nsubscriptionId='your subscription id'  # Subscription ID\nopenAIAccountName='Azure OpenAI service name'  # Name of the Azure OpenAI service\n\n# Assign Cognitive Services OpenAI User role\naz role assignment create --role \"Cognitive Services OpenAI User\" --assignee $principalId --scope /subscriptions/$subscriptionId/resourceGroups/$resourceGroupName/providers/Microsoft.CognitiveServices/accounts/$openAIAccountName\n\n# Set variables for Cognitive Search role assignment\nsearchServiceName='Azure Cognitive Search service name'  # Name of your Azure AI Search service\n\n# Assign Search Index Data Reader role\naz role assignment create --role \"Search Index Data Reader\" --assignee $principalId --scope /subscriptions/$subscriptionId/resourceGroups/$resourceGroupName/providers/Microsoft.Search/searchServices/$searchServiceName\n```\n\n### PowerShell\n```powershell\n# Set variables for Cosmos DB role assignment\n$resourceGroupName='your resource group name'  # Name of your resource group\n$cosmosDbaccountName='CosmosDB Service name'   # Name of your CosmosDB account\n$roleDefinitionId='00000000-0000-0000-0000-000000000002'  # Built-in CosmosDB role ID for Data Contributor\n$principalId='Object id of your user in Microsoft Entra ID'  # Object ID of the user in Microsoft Entra ID\n\n# Assign CosmosDB Data Contributor role to the user\naz cosmosdb sql role assignment create --account-name $cosmosDbaccountName --resource-group $resourceGroupName --scope \"/\" --principal-id $principalId --role-definition-id $roleDefinitionId\n\n# Set variables for Azure OpenAI role assignment\n$subscriptionId='your subscription id'  # Subscription ID\n$openAIAccountName='Azure OpenAI service name'  # Name of the Azure OpenAI service\n\n# Assign Cognitive Services OpenAI User role\naz role assignment create --role \"Cognitive Services OpenAI User\" --assignee $principalId --scope /subscriptions/$subscriptionId/resourceGroups/$resourceGroupName/providers/Microsoft.CognitiveServices/accounts/$openAIAccountName\n\n# Set variables for Cognitive Search role assignment\n$searchServiceName='Azure Cognitive Search service name'  # Name of your Azure AI Search service\n\n# Assign Search Index Data Reader role\naz role assignment create --role \"Search Index Data Reader\" --assignee $principalId --scope /subscriptions/$subscriptionId/resourceGroups/$resourceGroupName/providers/Microsoft.Search/searchServices/$searchServiceName\n``` \n2. Rename the `.env.template` file to `.env` and update the variables as needed.\n\n3. Run `./chat.sh` (for Bash) or `./chat.ps1` (for PowerShell) to start the client locally.\n\n![chat client](media/running_chat_client.png)\n\n### Running the Function Locally\n\nTo run the Azure Function locally, see [Testing the Solution Locally in VS Code](docs/LOCAL_DEPLOYMENT.md).\n\n## Evaluation\n\nAn evaluation program is provided for testing the orchestrator's performance. \n<BR>Refer to the [Evaluation Documentation](docs/EVALUATION.md) for details.\n\n## Contributing\n\nFor contribution guidelines, refer to [CONTRIBUTING.md](https://github.com/Azure/GPT-RAG/blob/main/CONTRIBUTING.md).\n\n## Trademarks\n\nThis project may contain trademarks. Follow [Microsoft's Trademark Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general) for proper use.\n"
    },
    {
      "name": "cyberark/agent-guard",
      "stars": 69,
      "img": "https://avatars.githubusercontent.com/u/30869256?s=40&v=4",
      "owner": "cyberark",
      "repo_name": "agent-guard",
      "description": "A toolset repository for AI agents",
      "homepage": "https://www.cyberark.com/",
      "language": "Python",
      "created_at": "2025-03-11T09:06:29Z",
      "updated_at": "2025-04-23T07:24:03Z",
      "topics": [],
      "readme": "<p style=\"text-align: center;\">\n    <img src=\"https://raw.githubusercontent.com/cyberark/agent-guard/refs/heads/main/resources/logo.png\" alt=\"agentwatch - AI Agent Observability Platform\" width=\"400\"/>\n    \n</p>\n<h3 style=\"font-family: 'Fira Mono', Monospace; text-align: center;\">Security Toolset for AI Agents</h3>\n\n<p style=\"text-align: center;\">\n    <a href=\"https://github.com/cyberark/agent-guard/commits/main\">\n        <img alt=\"GitHub last commit\" src=\"https://img.shields.io/github/last-commit/cyberark/agent-guard\">\n    </a>\n    <a href=\"https://github.com/cyberark/agent-guard\">\n        <img alt=\"GitHub code size\" src=\"https://img.shields.io/github/languages/code-size/cyberark/agent-guard\">\n    </a>\n    <a href=\"https://github.com/cyberark/agent-guard/blob/main/LICENSE\">\n        <img alt=\"GitHub License\" src=\"https://img.shields.io/github/license/cyberark/agent-guard\"/>\n    </a>\n    <a href=\"https://pypi.org/project/agent-guard-core\">\n        <img alt=\"PYPI version\" src=\"https://img.shields.io/pypi/v/agent-guard-core\" />\n    </a>   \n</p>\n\n\n## 🌟 Overview\n\nThis toolset is intended for AI agent builders to simplify your work and reduce the level of boilerplate code you need to write.\nThe toolset includes a [Python library](https://pypi.org/project/agent-guard-core/).\n\n\n## Key Features\n\n### ✨ Secured environment variables provisioning\n\nThis toolset can populate API keys and secrets as environment variables. The secrets are stored in your secret management of choice and are provisioned at runtime into your process memory.\nThe secrets can be populated and depopulated, for a specific context: Agent, Tool, HTTP call.\nCurrently [supported](https://github.com/cyberark/agent-guard/tree/main/agent_guard_core/credentials) secret providers:\n- CyberArk Conjur\n- AWS Secrets Manager\n- Local `.env` file (for development purposes)\n\nHowever, this functionality is extensible by implementing a [SecretsProvider](https://github.com/cyberark/agent-guard/tree/main/agent_guard_core/credentials) interface.\n\n#### Example\n\nFor full, runnable examples, please see the [examples](https://github.com/cyberark/agent-guard/tree/main/examples) directory.\n\n```python\n...\n\nfrom agent_guard_core.credentials.aws_secrets_manager_provider import AWSSecretsProvider\nfrom agent_guard_core.credentials.environment_manager import EnvironmentVariablesManager\n\n\n# Populate the environment variables from AWS Secrets Manager\n@EnvironmentVariablesManager.set_env_vars(AWSSecretsProvider())\nasync def main() -> None:\n    runtime = SingleThreadedAgentRuntime()\n    tools: List[Tool] = [\n        FunctionTool(get_stock_price, description='Get the stock price.')\n    ]\n    \n    await ToolAgent.register(runtime, 'tool_executor_agent',\n                             lambda: ToolAgent('tool executor agent', tools))\n\n    await ToolUseAgent.register(\n        runtime,\n        'tool_use_agent',\n        lambda: ToolUseAgent(\n            AzureOpenAIChatCompletionClient(\n                model='gpt-4o',\n                azure_endpoint=os.getenv('AZURE_OPENAI_ENDPOINT'),\n                azure_deployment='gpt-4o',\n                api_version='2024-02-01'),\n            [tool.schema for tool in tools], 'tool_executor_agent'),\n    )\n\n    ...\n```\n\n## ⚡ Getting Started\n\nPlease take a look at our [getting started](https://github.com/cyberark/agent-guard/blob/main/GETTING_STARTED.md) page and try it out!\n\n## 🤝 Contribution\n\nPlease make sure to read the [CONTRIBUTING.md](https://github.com/cyberark/agent-guard/blob/main/CONTRIBUTING.md) file if you want to contribute to this project.\n\n## 💁  Contact\n\nFeel free to contact us via GitHub issues or through LinkedIn: [Gil Adda](https://www.linkedin.com/in/gil-adda-6117b9/), [Rafi Schwarz](https://www.linkedin.com/in/rafi-schwarz/). \n"
    },
    {
      "name": "microsoft/dstoolkit-text2sql-and-imageprocessing",
      "stars": 67,
      "img": "https://avatars.githubusercontent.com/u/6154722?s=40&v=4",
      "owner": "microsoft",
      "repo_name": "dstoolkit-text2sql-and-imageprocessing",
      "description": "This repo accelerates development of RAG applications with rich data sources including SQL Warehouses and documents analysed with Azure Document Intelligence",
      "homepage": "https://www.ds-toolkit.com/",
      "language": "Python",
      "created_at": "2024-08-15T09:50:05Z",
      "updated_at": "2025-04-22T16:15:26Z",
      "topics": [
        "ai",
        "azure",
        "entity-relationship",
        "llm",
        "nl2sql",
        "query-builder",
        "rag",
        "search",
        "sql",
        "text2sql",
        "vector"
      ],
      "readme": "# Text2SQL and Image Processing in AI Search\r\n\r\nThis repo provides sample code for improving RAG applications with rich data sources including SQL Warehouses and documents analysed with Azure Document Intelligence.\r\n\r\nIt is intended that the plugins and skills provided in this repository, are adapted and added to your new or existing RAG application to improve the response quality.\r\n\r\n> [!IMPORTANT]\r\n>\r\n> - This repository now uses `uv` to manage dependencies and common utilities. See [uv](https://docs.astral.sh/uv/) for more details on how to get started.\r\n\r\n## Components\r\n\r\n- `./text_2_sql` contains an three Multi-Shot implementations for Text2SQL generation and querying which can be used to answer questions backed by a database as a knowledge base. A **prompt based** and **vector based** approach are shown, both of which exhibit great performance in answering sql queries. Additionally, a further iteration on the vector based approach is shown which uses a **query cache** to further speed up generation.  With these plugins, your RAG application can now access and pull data from any SQL table exposed to it to answer questions.\r\n- `./image_processing` contains code for linking **Azure Document Intelligence** with AI Search to process complex documents with charts and images, and uses **multi-modal models (gpt4o)** to interpret and understand these. With this custom skill, the RAG application can **draw insights from complex charts** and images during the vector search. This function app also contains a **Semantic Text Chunking** method that aims to intelligently group similar sentences, retaining figures and tables together, whilst separating out distinct sentences.\r\n- `./deploy_ai_search_indexes` provides an easy Python based utility for deploying an index, indexer and corresponding skillset for AI Search and for Text2SQL.\r\n\r\nThe above components have been successfully used on production RAG projects to increase the quality of responses.\r\n\r\n> [!WARNING]\r\n>\r\n> - The code provided in this repo is a accelerator of the implementation and should be review / adjusted before being used in production.\r\n\r\n## Contributing\r\n\r\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\r\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\r\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\r\n\r\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\r\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\r\nprovided by the bot. You will only need to do this once across all repos using our CLA.\r\n\r\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\r\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\r\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\r\n\r\n## Trademarks\r\n\r\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft\r\ntrademarks or logos is subject to and must follow\r\n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\r\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\r\nAny use of third-party trademarks or logos are subject to those third-party's policies.\r\n"
    },
    {
      "name": "wanaku-ai/wanaku",
      "stars": 62,
      "img": "https://avatars.githubusercontent.com/u/200620763?s=40&v=4",
      "owner": "wanaku-ai",
      "repo_name": "wanaku",
      "description": "Wanaku MCP Router",
      "homepage": "https://wanaku.ai",
      "language": "Java",
      "created_at": "2025-02-01T13:12:59Z",
      "updated_at": "2025-04-23T08:00:04Z",
      "topics": [
        "agentic",
        "agentic-ai",
        "agents",
        "artificial-intelligence",
        "mcp",
        "mcp-server"
      ],
      "readme": "# Wanaku - A MCP Router that connects everything\n\nThe Wanaku MCP Router is a router for AI-enabled applications powered by the [Model Context Protocol (MCP)](https://modelcontextprotocol.io/).\n\nThis protocol is an open protocol that standardizes how applications provide context to LLMs. \n\nThe project name comes from the origins of the word [Guanaco](https://en.wikipedia.org/wiki/Guanaco), a camelid native to\nSouth America.\n\n[![Getting Started With Wanaku](https://img.youtube.com/vi/-fuNAo2j4SA/0.jpg)](https://www.youtube.com/watch?v=-fuNAo2j4SA)\n\n\n## Usage Guide\n\nPlease follow the [usage guide](docs/usage.md) to learn how to use Wanaku.\n\n## Building Wanaku MCP Router\n\nIf you want to build to the project, then read [building](docs/building.md) guide.\n\n## Contributing \n\nIf you want to contribute to the project, then read the [contributing](docs/contributing.md) guide.\n"
    },
    {
      "name": "moxin-org/mofa",
      "stars": 57,
      "img": "https://avatars.githubusercontent.com/u/167464495?s=40&v=4",
      "owner": "moxin-org",
      "repo_name": "mofa",
      "description": "MoFA - Modular Framework for Agents. Modular, Compositional and Programmable.",
      "homepage": "",
      "language": "Python",
      "created_at": "2024-06-26T16:39:18Z",
      "updated_at": "2025-04-23T09:27:22Z",
      "topics": [],
      "readme": "# **MoFA**\n\n[English](README.md) | [简体中文](README_cn.md)\n\n## What\n\nMoFA: **M**odular **F**ramework for **A**gent\n\nMoFA is a software framework for building AI agents through a composition-based approach. Using MoFA, AI agents can be constructed via templates and combined in layers to form more powerful Super Agents.\n\n## WHY\n\nBuilding AI agents with MoFA offers:\n\n1. **Modularity**: Modular agent templates and agent services; simple configurations with straightforward interfaces between modules.\n\n2. **Clarity**:  A \"LEGO brick\"-style logic for assembling complex systems.\n\n3. **Composition**: Agentic Application gain greater capabilities and expand functionalities with put composible agents together.\n\n4. **Simplicity**: Constructing complex agents becomes a zero-code process.\n\n5. **High Performance**: Agents operate in the high-performance, low-latency distributed AI and robotics computation environment of DORA-RS, outperforming Python-based environments.\n\n6. **Diversity**: MoFA's agent composition combines capabilities organically, creating more powerful and comprehensive composite agents.\n\n7. Towards AIOS\n\n   : Designed with inspiration from Unix Philosophy and methodology:\n\n   - AIOS Core: MoFA provides services like task planning, memory, actions, and Retrieval-Augmented Generation (RAG).\n   - Utility and Applications: Common and foundational functionalities via agent templates.\n   - Shell: An environment for running agents and automating their processes.\n\n8. **Enabling Edge AI**: Together with the MoXin project for local model inference and the MoLy project for user interfaces, MoFA agents make AI applications more open and democratic.\n\n## Features\n\n#### Feature 1: Nesting Design Patterns of AI Agents\n\nAI agents are intelligent software applications. Similar to the design patterns in object-oriented programming, there are various design patterns for AI agents, include but not limited to  :\n\n- **LLM Inference**: Using large language models (LLMs) to inference is the most simplistic design pattern.\n- **Customized Prompt**: tailoring system prompts for  agents.\n- **Reflection Pattern**: Agents capable of self-review and improvement.\n- **Actor Pattern**: Agents with the ability to use external tools and resources, like generating code or searching the web.\n- **ReAct Pattern**: Combining reflection and tool usage to improve output quality.\n- **Multi-Agent Collaboration**: Agents taking on specialized roles and collaborating to complete complex tasks\n\nMore design patterns can be easily added into MoFA as technology advances.   Agent Application developers can create their own design patterns and can be reused by in the community. \n\n#### Feature 2: Agent Kernal Services\n\nSimilar to an operating system providing services to software, MoFA provides core services to agents, including memory, planning, knowledge base, RAG, and action capabilities.\n\nHowever, MoFA treat Kernel Services as Agents too, which makes them open to the the 3rd party developers. MoFA users can pick the kernal service agents that best fit their needs.  \n\n#### Feature 3: Composition\n\nComposition is the process of assembling elements into new entities without changing the original components. This modularity allows AI developers to build and recombine agents to create new functionalities.\n\n#### Feature 4: Dataflow-Driven Approach\n\nMoFA employs a dataflow-driven method instead of a workflow-driven one. By focusing on data dependencies rather than business rules, it simplifies and enhances modularity.\n\n## How\n\nMoFA currently supports agent development using the Dora-RS framework. For details, please refer to  the [python](python) directory's [README.md](python/README.md).\n\n\n\n## GOSIM China 2024 Super Agent Hackathon\n\nThe MoFA project is one of the agent programming frameworks for the GOSIM 2024 China Conference Super Agent Hackathon. \n\n- Documentation for participants is available here[>>>](Hackathon/documents/README.md).\n\n- Gosim China 2024 Super Agent Hackathon Website [>>>](https://gosim.gitcode.com/hackathon/)\n- Weave your agent with MoFA，An introductory presentation of MoFA @ GOSIM China 2024 [YouTube](https://www.youtube.com/watch?v=FhL3orAVO6U)\n"
    },
    {
      "name": "robertZaufall/mindmanager_ai",
      "stars": 52,
      "img": "https://avatars.githubusercontent.com/u/18199228?s=40&v=4",
      "owner": "robertZaufall",
      "repo_name": "mindmanager_ai",
      "description": "MindManager AI Integration on Windows and macOS",
      "homepage": "",
      "language": "Python",
      "created_at": "2023-10-22T01:20:04Z",
      "updated_at": "2025-04-17T23:47:54Z",
      "topics": [
        "applescript",
        "appscript",
        "azure-ai",
        "macos",
        "macros",
        "mermaid",
        "mindjet",
        "mindmanager",
        "mindmap",
        "ollama",
        "openai",
        "refinement",
        "vba",
        "wwb"
      ],
      "readme": "# MindManager AI Integration <br/> Windows + macOS\n\nThese automations and macros enhance mindmaps created by **MindManager** on macOS and Windows.\n\n## Windows  \n\nUsing MindManager Macro:  \n\n![Example](doc/anim_neu_1.gif)  \n\n## macOS  \n\nUsing Automator Workflow (Quick Action):  \n\n![Example](doc/anim_neu_2.gif)  \n\nUsing TKInter UI and \"Freetext\" option (run the `app_tkinter.py` file):  \n\n![Example](doc/ui_freetext.gif)  \n\nMore animated examples are in the `doc` folder.\n\n## Features\n\n### AI platforms and LLMs\n  - **Azure OpenAI** w/ `GPT-4.1`, `GPT-4o`, `o4-mini`, `o3-mini`, `o1`, `o1-mini` etc. (use your key or log in with `Azure EntraID`)  \n  - **OpenAI** w/ `GPT-4.1`, `GPT-4o`, `o4-mini`, `o3`, `o3-mini`, `o1`, `o1-mini` etc. (use your key)  \n  - **Anthropic** w/ `Claude 3.7` etc. models (use your key)  \n  - **xAI** w/ `grok-3`, `grok-2-vision` etc.  (use your key)  \n  - **Google Gemini** w/ `Gemini Pro 2.5`, `Gemini Flash 2.5`, `Gemma 3` etc. (use your key)  \n  - **Google Vertex AI** w/ `Gemini Pro 2.5`, `Gemini 2.5 Flash` etc. (use your access token / OAuth2)\n  - **DeepSeek** w/ `DeepSeek V3` (use your key)  \n  - **Alibaba Cloud** w/ `Qwen-Max`, `Qwen-Plus`, `Qwen-Turbo` (use your key)  \n  - **Mistral AI** w/ `Mistral-Large`, `Pixtral`, `Ministral` (use your key)\n  - **AWS Bedrock** w/ `Nova` native models or `Anthropic`, `Mistral` serverless models (use your key + secret)  \n  - **Azure AI Foundry** (platform)  w/ `LLaMA`, `Mistral` etc. (use your key)  \n  - **GitHub Models** (platform) w/ `GPT-4o`, `LLaMA` etc. (use your key)  \n  - **Perplexity** (platform) w/ `LLaMA 3.1-Sonar` etc. (use your key)  \n  - **Groq** (platform) w/ `Qwen`, `DeepSeek`, `LLaMA`, `Mixtral` etc. (use your key)  \n  - **Hugging Face** (platform)  w/ `LLaMA` and more models (use your token)  \n  - **Fireworks AI** (platform)  w/ `LLaMa`, `Qwen` (serverless) etc. (use your key)  \n  - **OpenRouter** (platform) w/ many/all models (use your key)\n  - **GPT4ALL** (local w/ SDK) w/ any `llama.cpp` model\n  - **Ollama** (local w/ API) w/ any `llama.cpp` or `MLX` model\n  - **LMStudio** (local w/ API) w/ any `llama.cpp` or `MLX` model\n  - **MLX** (local w/ API, Apple Silicon) w/ any `MLX` model\n\n### Image Generation Systems\n  - **Azure OpenAI** w/ `DALL-E 3` (use your key or log in with `Azure EntraID`) -> **best tested**\n  - **OpenAI** w/ `DALL-E 3` (use your key) -> **best results**\n  - **Stability AI** w/ `Stable Diffusion 3` `SD3.5` / `SD3` / `Ultra` / `Core` (use your key)  \n  - **Google Vertex AI** w/ `Imagen3` (use your access token / OAuth2, GCP approval required!)  \n  - **Ideogram AI** w/ `V1` / `V2` (use your key)  \n  - **Black Forrest Labs** w/ `Flux Pro 1.1 Ultra`, `Flux Pro 1.1`, `Flux.1 Pro`, `Flux.1 Dev`  (use your key)  \n  - **Recraft AI** w/ `RecraftV3`, `Recraft20B` (use your token)  \n  - **MLX** (local w/ SDK, Apple Silicon) w/ `Flux.1` models\n\n### Translation Services\n - **DeepL** (use your key)\n\n### Agentic Framework Examples  \n  - **Agno** (formerly \"Phidata\")  \n  - **Autogen** (Microsoft)  \n  - **CrewAI**  \n\n### Platform\n- Windows compatible (run macro/context menu or call the **Python** script directly)  \n- macOS compatible (run **Automator** workflow (Quick Action) or call the **Python** script directly)  \n- Only native API requests to AI systems - **no middleware needed**\n\n### Layout\n- Map format can be radial map or orgchart\n- Using map templates on macOS\n- Map styles on Windows are persistent, automatic collapsing of nodes\n\n## Implemented Business or Use Cases\n1. Refinement of the map or topic.  \n2. Refinement of the map or topic from a development perspective.  \n3. Create examples for one, more (selected) or all topics.  \n4. Clustering topics from scratch.  \n5. Clustering by one or more criterias eg. Organization/Process/Project/Expertise, Capex-Opex perspective.  \n6. Complex cases (multiple calls): eg. refinement + clustering + examples.  \n7. Image generation from topics\n8. Professional translation of all topics by DeepL\n9. Export `Mermaid` mindmap HTML document\n10. Export `Markmap` mindmap HTML document\n11. PDF to mindmap (multiple files / batch processing)\n12. Generate a working paper (argumentation) HTML document for a detailed mindmap\n13. Generate a glossary HTML document of all terms\n  \n![Refinemnt + Glossary](doc/glossary.gif)  \n\n## Other Use Cases (implemented or easy to add)\n- Export mindmap to any other text format\n- Change map layout by using a template (macOS)\n- Reorder topics by business value or importance\n- Misspelling or syntax correction\n- Create a map based on external text data\n\n## User Interface  \nThere's a new user interface using TKInter to execute most of the operations and more. The app stays always on top and has several UI tabs:  \n  \n### Action tab  \nChoose the desired model and action and click on execute:\n  \n<img src=\"doc/tab_action.png\" height=\"400\" >  \n  \n### Image tab  \nChose the desired model and the number of images to generate:\n\n<img src=\"doc/tab_image.png\" height=\"400\" >  \n  \n### Freetext tab  \nChoose the desired modell, fill in some text how to generate or modify the content and click on execute.  \nExamples:  \n- \"refine\"\n- \"translate all topics to German\"\n- \"add an emoji to every topic\" (does only work on MacOS)\n\n<img src=\"doc/tab_freetext.png\" height=\"400\" >  \n  \n### Translation tab  \nJust choose the destination language and click on execute:\n\n<img src=\"doc/tab_translation.png\" height=\"400\" >  \n  \n### Agent tab  \nChoose an already implemented agent and the desired models.  \nThe agents list is generated from scripts found in the `ai/agents` folder.  \nBefore execution the needed libraries have to be installed:  \n`pip install -r requirements_agents.txt`.  \nCrewAI does not install well on Windows ARM64 by the time of writing.  \nAgents do a lot of AI roundtrip calls, so the costs have to be monitored.\n  \n<img src=\"doc/tab_agent.png\" height=\"400\" >  \n  \n### Configuration tab  \nBy now only the resulting chart format is selectable (orgchart, radial map or automatic selection):  \n  \n<img src=\"doc/tab_configuration.png\" height=\"400\" >  \n  \n## Installation  \n### Windows  \nFirst install the Windows package manager `Chocolatey` from an administration shell or choose any other way following https://chocolatey.org/install.  \nThis is one line:  \n```\nSet-ExecutionPolicy Bypass -Scope Process -Force; [System.Net.ServicePointManager]::SecurityProtocol = [System.Net.ServicePointManager]::SecurityProtocol -bor 3072; iex ((New-Object System.Net.WebClient).DownloadString('https://community.chocolatey.org/install.ps1'))\n```  \nChange to folder `%localappdata%\\Mindjet\\MindManager\\23\\macros`:\n```\ncd %localappdata%\\Mindjet\\MindManager\\23\\macros\n```\nCopy all files from the GitHub repository to this location.  \nChange to `windows` folder:\n```\ncd windows\n```\nRun `install.bat` or  the following commands (requirements_auth.txt only if you want to use `Azure Entra ID` or `GCP OAuth2`):\n```\ncd ..\nchoco install python3\npip install -U -r .\\src\\requirements.txt\npip install -r .\\src\\requirements_auth.txt\npowershell -ExecutionPolicy Bypass -File .\\windows\\macro_registration.ps1\n```\nCheck in registry and MindManager, if the macros are available (right click on topic).  \nHint: The macro list is ordered according to the GUID-string, not the macro name.   \n\n![Registry](doc/windows_registry.png)  \n\nMacros can also be executed by the macro editor. The macros are similar but the action parameter.  \n\n![Automator](doc/windows_macroeditor.png)  \n\nYou can also check here if the path to the python file is correct.  \n\n### macOS  \nPython has to be installed first. Go to https://www.python.org/downloads/macos/ and download the desired installer.  \nInstall required python libraries (requirements_auth.txt only if you want to use `Azure Entra ID` or `GCP OAuth2`, requirements_mac_mlx.txt is only needed for local image generation using MLX):\n```\npip install -U -r requirements.txt\npip install -r requirements_mac_mlx.txt\npip install -r requirements_auth.txt\n```\nCreate the directory structure `~/git/mindmanager_ai` with Terminal:\n```\ncd ~/\nmkdir git\ncd git\nmkdir mindmanager_ai\ncd mindmanager_ai\n```\nCopy all repository files to this location as the Automator workflows contain this path.  \nAlternatively you can clone the repository in Terminal:  \n```\ncd ~/\nmkdir git\ncd git\ngit clone https://github.com/robertZaufall/mindmanager_ai.git\ncd mindmanager_ai\n```\nChange to folder `macos` and copy the Automator workflows to the `~/Library/Services` (hidden) folder:\n```\ncd macos/automator\nchmod +x ./copy_to_services.sh\n./copy_to_services.sh\n```\nIf you need elevated privileges for copying the files use this command:\n```\nsudo sh ./copy_to_services.sh\n```\nAll **Automator** workflow settings are similar but the action parameter:  \n\n<img src=\"doc/macos_automator.png\" width=\"600\" >  \n\nThe workflows are then available at the \"MindManager\" main menu -> Services  \n\n<img src=\"doc/macos_services.png\" width=\"400\" >  \n\n### VSCode\n\nI prefer to execute the python script directly from VSCode. Here you can easily adjust the settings, try different LLMs on the fly and even debug, if problems occur (external systems are sometimes not available).  \nThere are some actions already predefined for quick execution.  \n\n![VSCode](doc/vscode.png)  \n\n## How to use  \n### Configuration  \nThere are main configuration files, each for LLM, image generation and translation.\nOpen each config file and uncomment the AI model you want to use. In the `config` folder are several environment files for every supported AI model. For example, if you want to use OpenAI models, copy the file `config/openai.env.example` to `config/openai.env` and fill in your api key.  \nUse the apropriate LLM system for which you have an API key. These keys are available on the developer platforms of the AI vendors.  \nIf you want to run local models with `Ollama`, `GPT4All`, `LMStudio`, `MLX` you have to have either a newer Apple Mac model with M1-M4 processor or a desktop or notebook with NVidia graphic card with at least 8GB graphic ram.  \n\n### General\nYou can have more than one open document in MindManager. The document which should be processed must be the active document. For every processing a new document with the new topics will be created.  \n\nTo process the whole map, select the central topic (for right-clicking) or don't select any topic at all and call a macro manually (Windows), choose Automator Workflow from MindManager Menu -> Services or call the python script from VSCode or commandline `python3 process.py <action> <format>`. If it's not working try either `python3` or `python`.  \n\n### Map actions\nSelect the central topic or deselect all topics and call the automation.  \nYou can also select one or more topics and start the automation for just these topics, e.g. to generate examples for these topics, refine just these topics etc.  \n\n### Image generation\nJust select the topics for which you want to generate an image and choose the action \"Generate Image\" (macro on Windows or Automator Workflow on macOS) or call the Python script with parameter `image` or `image_n`.  \nAfter a while, the image will be opened and also stored in the MindManager-Library `Images`-folder.  \nUnfortunately, on macOS the image cannot automatically be inserted into the map or added to a topic due to insufficient library support.  \nOn Windows the image can be automatically set as the background image of the map.  \n\nThe results from the generation process are best with `FLUX.1`, good with `DALL-E 3` and `SD 3.5`. Prompt crafting/engineering is still in progress.  \nThe filename is enriched with the generation **seed** where this feature is supoported. This seed is useful if you want to generate similar images (e.g. with different prompt). `DALL-E 3` does not support a seed value anymore (by the time of writing).  \nThe prompt for image generation can optionally be optimized using a LLM call.\nImages can also be generated locally on macOS with Apple Silicon using the native `Apple MLX` framework.  \n\nRecently there are more image generation plattforms trending. `Black Forrest Labs`, `Ideogram AI` and `Recraft AI` image generation from mindmaps is already implemented and the results are amazing.  \n\n### Generation using text summarization (eg. PDF to mindmap)\nPut the files into the `input`-folder and use the action `pdf_mindmap`. The PDF files are first converted to markdown (MD) format. 'Reference' sections are removed as these contain no information but take a lot of tokens (e.g. arXiv papers). No OCR takes place by now. Tables are removed and the content will be highly sanitized by removing irrelevant characters, code blocks, href-links, whitespace etc.  \nThere is no local LLM model using Ollama working for me by now.  \n\nSome models (i.e. `Sonnet`) support native `PDF` processing, which is also implemented (action `pdfsimple_mindmap`).  \n\n### Text generation from mindmaps (working paper (argumentation), glossary)\nGeneration of larger text outputs needs a model with an higher max-token value like `GPT-4o`, `Gemini Flash`, `Sonnet`. Results are very good, most of the time. \n\n## LLM systems\n### Azure OpenAI / OpenAI\nThe solution is best tested with `Azure OpenAI`. Results are perfect for every use case. Cheapest ones are `gpt-4.1-mini` and `gpt-4.1-nano`  \nAzure EntraID authentication can be used in enterprise scenarios.  \n### Google Gemini / Vertex AI\n`Gemini Pro 2.5` results are best. `Gemini Flash 2.5` is also very good.  \nVertex AI needs an security token which you can generate using the cloud console.  \n### Anthropic Claude 3.5\nThe Anthropic `Claude 3.7 Sonnet` model ist very good. Anthropic `Claude 3.5 Haiku` is good and also very cheap.\n### xAI grok\nGrok is very good and is able to refine mindmaps for several levels. The models `grok-3-beta` and `grok-2-vision-1212` are very good. The vision model can be used for pdf ocr.  \n### Amazon Bedrock\nAmazon Bedrock has some native models i.e. `Nova` (best), and host also 3rd party models of `Anthropic Claude` and `Mistral`.\n### DeepSeek\nDeepSeek created an extraordinary open source model DeepSeek V3 which seems to be as good as GPT-4o. The reasoning model `r1` does not work by now.  \n### Alibaba Cloud\nAlibaba Cloud models cannot generate large amounts of tokens (`Qwen-Max`: 2000, `Qwen-Plus` + `Qwen-Turbo`: 1500) but the results are good. `Qwen-Turbo` is very fast. `Qwen 2.5` model is still not available outside China by now (2024-11-22).  \n### Mistral AI\nMistral AI is hosting their commercial flagship models `Mixtral-Large` and `Pixtral-Large`. `Mixtral-Large` is a 'best in class' model. The maximum numer of possible output tokens is a little bit unclear (max_tokens may meant to be the sum of input and output tokens).  \n### Groq (platform)\nGroq is sure the fastest LLM platform by now. `LLaMA3`, `Mixtral` and `Gemma2` are proven models. From time to time the supported models on the platform are changing.  \n### Perplexity (platform)\nPerplexity works perfect as an universal LLM platform. From time to time the supported models on the platform are changing.  \n### Hugging Face (platform)\nTo access better models a pro-subscription is needed. `LLaMA-3-8B` still can be used.  \n### Open Router (platform)  \nOn the Open Router platform there are a variety of models and systems available. Also fallback scenarios are supported. Furthermore you get access here to the newest OpenAI models like `o1-preview`.  \n### GPT4ALL, Ollama, LMStudio (hosted locally - no internet access needed)\nResults are dependent on the used model. `LLaMA3`, `Zephyr` and `Mixtral` are working well.  \n### MLX (hosted locally on Apple Silicon - no internet access needed)\nMLX results are dependent on the used model. `LLaMA3` works well.\n\n## Image generation systems  \n### Azure OpenAI / OpenAI - DALL-E 3  \nThe solution is best tested with `Azure OpenAI`. Results are very good. There is a problem with texts generated within images. Azure EntraID authentication can be used in enterprise scenarios.  \n### Stability AI - Stable Diffusion 3 + 3.5  \nImage generation with `SD3.5` and `SD3` is the most flexible, as you can use a seed value, negative prompt, etc. Prompt engineering is most important here, as the results are far from being perfect by now.  \n### Google Vertex AI - Imagen3  \nImage generation results are too simple by now as prompt engineering is also most important here. `Imagen3` has the highest image resolution (1:1 with 1536x1536). `Imagen3` is GA (globally available) but there is an approval process to get access to the API.  \n### Ideogram AI  \nImage generation is quite good using the `V_2` model. When activating API access, keep in mind that generating an API key immediately results in a $40 bill.  \n### Black Forrest Labs  \nImage generation is extraordinary. Flagship model is `Flux Pro 1.1 / Ultra`. As usual token have to be prepaid and you need accepted for accessing the platform.  \n### Recraft.AI\nImage genration is very good. There are many pre-defined styles which can be activated as needed. Available Models are `RecraftV3` and `Recraft20B`.  \n\n### MLX w/ Flux.1, SD3\nThis local image generation alternative is only available on macOS with Apple Silicon processors like M1 and higher. The results are above average using `Flux.1` model and under average using `SD3` mostly because the prompt is optimized for `Flux.1`.  \n\nThere is a new action defined (`image_n` eg. `image=10`) to generate a bunch of images in a row. A pre-executing step can be added to optimize the prompt using a LLM call. If there is only one topic selected there is a different prompt used as when more topics are selected. Only the first level of topics together with the central topic should be selected for better results.  \n\nWhen using this image generation way, the desired model and embeddings tokenizer will be downloaded automatically. The expected data amount to be downloaded is about 50GB using `Flux.1` and 6GB using `SD3`. If you are using `SD3` for the first time you have to login at **huggingface** with your token first as you have to agree to the terms of **Stability AI** and the usage of their model: `huggingface-cli login --token <xyz>`. Downloaded models are cached at `~/.cache/huggingface`.\n\n## Translation systems\n### DeepL\nTranslation works for these languages:\n```\n# supported languages as source\n# BG,CS,DA,DE,EL,EN,ES,ET,FI,FR,HU,ID,IT,JA,KO,LT,LV,NB,NL,PL,PT,RO,RU,SK,SL,SV,TR,UK,ZH\n\n# supported languages as target\n# BG,CS,DA,DE,EL,EN-GB,EN-US,ES,ET,FI,FR,HU,ID,IT,JA,KO,LT,LV,NB,NL,PL,PT-BR,PT-PT,RO,RU,SK,SL,SV,TR,UK,ZH\n```\nSource language will be detected automatically. `Formality` parameter is not supported for all languages, so it is disabled by now. `Context` parameter was not used as DeepL states it's deprecated.\nAPI requests point to the free tier. If you have a paid subscription change the URL in the `config.py`.\n\n## Agentic frameworks  \nThere are only some example implementations using agentic frameworks. Results are requested as markdown code from the agents and are transformed to Mermaid mindmap by another LLM call. This should be addressed soon.  \n\n### Agno (formerly Phidata)  \nhttps://github.com/agno-agi/agno  \nMighty but very performant and easy to use framework.  \n\n### Autogen (Microsoft)  \nhttps://github.com/microsoft/autogen  \nFramework is complex and not so easy to use. There is also a fork with a similar name `pyautogen`, renamed as AG2.  \n\n### CrewAI  \nhttps://github.com/crewAIInc/crewAI  \nMighty and complex framework. Installation on Windows 11 on ARM requires VS2022 build tools. Installation on MacOS seems to be more easy but still has a lot of dependencies.  \n\n## Prompt crafting  \nPrompt crafting is lightly implemented using the following strategy:  \n\n![Configuration](doc/prompt.png)  \n\n## Platform specific implementations\nThe `mindm` component is used on both platforms for MindManager automation.  \n[Link to GitHub Repository](https://github.com/robertzaufall/mindm/)  \n[Link to GitHub Pages](https://robertzaufall.github.io/mindm/)  \n\n### Windows  \nMindManager COM objects are addressed by using the PyWin32 library:  \n\n<img src=\"doc/windows_mindmanager.png\" height=\"210\" >\n\n### macOS  \nMindManager objects are addressed by using the AppScript library:  \n\n<img src=\"doc/macos_mindmanager.png\" height=\"200\" >\n\n## Implementation details  \nThe Mermaid mindmap syntax is used when talking to the OpenAI LLM as an intermediate \"language\". Log file contents for input, output, prompt can be used in other use cases eg. mindmap visualizations in GitHub markdown files.  \n\n![Log](doc/log.png)  \n\nLog files content:  \n\n<img src=\"doc/log_files.png\" width=\"500\" >\n\nExample using a Mermaid mindmap in a GitHub markdown file.  \nCode:\n```\n```mermaid\nmindmap\n  Creating an AI Startup\n    Market Research\n      Identify Target Audience\n      Analyze Competitors\n      Understand Market Trends\n      Assess Market Needs\n      Evaluate Market Size\n    Business Model\n      Define Value Proposition\n      Choose Revenue Streams\n      Plan Monetization Strategy\n      Identify Cost Structure\n      Determine Key Partnerships```  \n```\n\nGithub rendering of the map:\n\n```mermaid\nmindmap\n  Creating an AI Startup\n    Market Research\n      Identify Target Audience\n      Analyze Competitors\n      Understand Market Trends\n      Assess Market Needs\n      Evaluate Market Size\n    Business Model\n      Define Value Proposition\n      Choose Revenue Streams\n      Plan Monetization Strategy\n      Identify Cost Structure\n      Determine Key Partnerships \n```\n\nYou can also use the content inside the Mermaid online editor (https://mermaid.live/edit):  \n\n![Mermaid](doc/mermaid.png)  \n\n## Disclaimer\nThe API execution time depends heavily on the used LLM model or system and token count.  \n\nCurrently, this project is in the early development phase, and generated outputs may include errors. Automated testing has not yet been implemented.  \n\n\n"
    },
    {
      "name": "agiresearch/Cerebrum",
      "stars": 50,
      "img": "https://avatars.githubusercontent.com/u/130198651?s=40&v=4",
      "owner": "agiresearch",
      "repo_name": "Cerebrum",
      "description": "Cerebrum: Agent SDK for AIOS",
      "homepage": "https://aios.foundation",
      "language": "Python",
      "created_at": "2024-11-23T01:48:50Z",
      "updated_at": "2025-04-22T16:47:08Z",
      "topics": [],
      "readme": "# Cerebrum: Agent SDK for AIOS\n\n<a href='https://docs.aios.foundation/'><img src='https://img.shields.io/badge/Documentation-Cerebrum-green'></a>\n[![Code License](https://img.shields.io/badge/Code%20License-MIT-orange.svg)](https://github.com/agiresearch/AIOS/blob/main/LICENSE)\n<a href='https://discord.gg/B2HFxEgTJX'><img src='https://img.shields.io/badge/Community-Discord-8A2BE2'></a>\n\nAIOS is the AI Agent Operating System, which embeds large language model (LLM) into the operating system and facilitates the development and deployment of LLM-based AI Agents. AIOS is designed to address problems (e.g., scheduling, context switch, memory management, storage management, tool management, Agent SDK management, etc.) during the development and deployment of LLM-based agents, towards a better AIOS-Agent ecosystem for agent developers and agent users. AIOS includes the AIOS Kernel (the [AIOS](https://github.com/agiresearch/AIOS) repository) and the AIOS SDK (this [Cerebrum](https://github.com/agiresearch/Cerebrum) repository). AIOS supports both Web UI and Terminal UI.\n\n\n## 🏠 Cerebrum Architecture\n<p align=\"center\">\n<img src=\"docs/assets/details.png\">\n</p>\n\nThe AIOS-Agent SDK is designed for agent users and developers, enabling them to build and run agent applications by interacting with the [AIOS kernel](https://github.com/agiresearch/AIOS.git). \n\n## 📰 News\n- **[2024-11-26]** 🔥 Cerebrum is available for public release on PyPI!\n\n## Installation\n\n### Install From Source\n1. **Clone Repo**\n   ```bash\n   git clone https://github.com/agiresearch/Cerebrum.git\n\n   cd Cerebrum\n   ```\n\n2. **Create Virtual Environment**\n   ```bash\n   conda create -n cerebrum-env python=3.10\n   ```\n   or\n   ```bash\n   conda create -n cerebrum-env python=3.11\n   ```\n   or\n   ```bash\n   # Windows (cmd)\n   python -m venv cerebrum-env\n\n   # Linux/MacOS\n   python3 -m venv cerebrum-env\n   ```\n\n3. **Activate the environment**\n   ```bash\n   conda activate myenv\n   ```\n   or\n   ```bash\n   # Windows (cmd)\n   cd cerebrum-env\n   cd Scripts\n   activate.bat\n   cd ..\n   cd ..\n   \n\n   # Linux/MacOS\n   source cerebrum-env/bin/activate\n   ```\n\n4. **Install the package**  \n   Using uv (Recommended)\n   ```bash\n   pip install uv\n   uv pip install -e .\n   ```\n   or using pip\n   ```\n   pip install -e .\n   ```\n\n5. **Verify installation**\n   ```bash\n   python -c \"import cerebrum; from cerebrum.client import Cerebrum; print(Cerebrum)\"\n   ```\n\n## ✈️ Quickstart\n> [!TIP] \n>\n> Please see our [documentation](https://docs.aios.foundation/) for more information.\n\n### 1. Start the AIOS Kernel\n📝 See [here](https://docs.aios.foundation/getting-started/installation).\n\nBelow are some useful commands to use\n- [List agents from agenthub](./cerebrum/commands/list_agenthub_agents.py)\n    ```bash\n    list-agenthub-agents\n    ```\n- [List agents from local](./cerebrum/commands/list_local_agents.py)\n    ```\n    list-local-agents\n    ```\n- [Download agents](./cerebrum/commands/download_agent.py)\n    ```bash\n    download-agent \\\n        --agent_author <agent_author> \\\n        --agent_name <agent_name> \\\n        --agent_version <agent_version> \\\n        --agenthub_url <agenthub_url>\n    ```\n- [Upload agent](./cerebrum/commands/upload_agent.py)\n    ```bash\n    upload-agents \\\n        --agent_path <agent_path> \\\n        --agenthub_url <agenthub_url>\n    ```\n\n- [List tools from toolhub](./cerebrum/commands/list_toolhub_tools.py)\n    ```bash\n    list-toolhub-tools\n    ```\n- [List tools from local](./cerebrum/commands/list_local_tools.py)\n    ```bash\n    list-local-tools\n    ```\n- [Download tool](./cerebrum/commands/download_tool.py)\n    ```bash\n    download-tool \\\n        --tool_author <tool_author> \\\n        --tool_name <tool_name> \\\n        --tool_version <tool_version> \\\n        --toolhub_url <toolhub_url>\n    ```\n- [Upload tool](./cerebrum/commands/upload_tool.py)\n    ```bash\n    upload-tool \\\n        --tool_path <tool_path> \\\n        --toolhub_url <toolhub_url>\n    ```\n\n### 2. Run agents\n\nEither run agents that already exist in the local by passing the path to the agent directory\n\n```\nrun-agent \\\n    --mode local \\\n    --agent_path <agent_name_or_path> \\ # path to the agent directory\n    --task <task_input> \\\n    --agenthub_url <agenthub_url>\n```\n\nFor example, to run the test_agent in the local directory, you can run:\n\n```\nrun-agent \\\n    --mode local \\\n    --agent_path cerebrum/example/agents/test_agent \\\n    --task \"What is the capital of United States?\"\n```\n\nOr run agents that are uploaded to agenthub by passing the author and agent name\n\n```\nrun-agent \\\n    --mode remote \\\n    --agent_author <author> \\\n    --agent_name <agent_name> \\\n    --agent_version <agent_version> \\\n    --task <task_input> \\\n    --agenthub_url <agenthub_url>\n```\n\nFor example, to run the test_agent in the agenthub, you can run:\n\n```\nrun-agent \\\n    --mode remote \\\n    --agent_author example \\\n    --agent_name test_agent \\\n    --agent_version 0.0.3 \\\n    --task \"What is the capital of United States?\" \\\n    --agenthub_url https://app.aios.foundation\n```\n\n\n## 🚀 Develop and customize new agents\n\nThis guide will walk you through creating and publishing your own agents for AIOS. \n### Agent Structure\n\nFirst, let's look at how to organize your agent's files. Every agent needs three essential components:\n\n```\nauthor_name/\n└── agent_name/\n      │── entry.py        # Your agent's main logic\n      │── config.json     # Configuration and metadata\n      └── meta_requirements.txt  # Additional dependencies\n```\n\nFor example, if your name is 'demo_author' and you're building a demo_agent that searches and summarizes articles, your folder structure would look like this:\n\n```\ndemo_author/\n   └── demo_agent/\n         │── entry.py\n         │── config.json\n         └── meta_requirements.txt\n```\n\nNote: If your agent needs any libraries beyond AIOS's built-in ones, make sure to list them in meta_requirements.txt. Apart from the above three files, you can have any other files in your folder. \n\n### Configure the agent\n\n#### Set up Metadata\n\nYour agent needs a config.json file that describes its functionality. Here's what it should include:\n\n```json\n{\n   \"name\": \"demo_agent\",\n   \"description\": [\n      \"Demo agent that can help search AIOS-related papers\"\n   ],\n   \"tools\": [\n      \"demo_author/arxiv\"\n   ],\n   \"meta\": {\n      \"author\": \"demo_author\",\n      \"version\": \"0.0.1\",\n      \"license\": \"CC0\"\n   },\n   \"build\": {\n      \"entry\": \"agent.py\",\n      \"module\": \"DemoAgent\"\n   }\n}\n```\n\n### APIs to build your agents\n- [LLM APIs](./cerebrum/llm/apis.py)\n- [Memory APIs](./cerebrum/memory/apis.py)\n- [Storage APIs](./cerebrum/storage/apis.py)\n- [Tool APIs](./cerebrum/tool/apis.py)\n\n### Available tools\n\nThere are two ways to use tools in your agents:\n\n#### 1. Use tools from ToolHub\n\nYou can list all available tools in the ToolHub using the following command:\n\n```bash\nlist-toolhub-tools\n```\n\nThis will display all tools available in the remote ToolHub. \n\nTo load a tool from ToolHub in your code:\n\n```python\nfrom cerebrum.interface import AutoTool\ntool = AutoTool.from_preloaded(\"example/arxiv\", local=False)\n```\n\n#### 2. Use tools from local folders\n\nYou can also list tools available in your local environment using the following command:\n\n```bash\nlist-local-tools\n```\n\nTo load a local tool in your code:\n\n```python\nfrom cerebrum.tool import AutoTool\ntool = AutoTool.from_preloaded(\"google/google_search\", local=True)\n```\n\nIf you would like to create your new tools, refer to [How to develop new tools](#develop-and-publish-new-tools)\n\n### How to upload your agents to the agenthub\nRun the following command to upload your agents to the agenthub:\n\n```python\npython cerebrum/upload_agent.py \\\n    --agent_path <agent_path> \\ # agent path to the agent directory\n    --agenthub_url <agenthub_url> # the url of the agenthub, default is https://app.aios.foundation\n```\n\n## 🔧Develop and Customize New Tools\n### Tool Structure\nSimilar as developing new agents, developing tools also need to follow a simple directory structure:\n\n```\ndemo_author/\n└── demo_tool/\n    │── entry.py      # Contains your tool's main logic\n    └── config.json   # Tool configuration and metadata\n```\n\n> [!IMPORTANT]\n> To use the agents in your local device, you need to put the tool folder under the cerebrum/tool/core folder and register your tool in the cerebrum/tool/core/registry.py\n\n### Create Tool Class\nIn `entry.py`, you'll need to implement a tool class which is identified in the config.json with two essential methods:\n\n1. `get_tool_call_format`: Defines how LLMs should interact with your tool\n2. `run`: Contains your tool's main functionality\n\nHere's an example:\n\n```python\nclass Wikipedia:\n    def __init__(self):\n        super().__init__()\n        self.WIKIPEDIA_MAX_QUERY_LENGTH = 300\n        self.top_k_results = 3\n        self.lang = \"en\"\n        self.load_all_available_meta: bool = False\n        self.doc_content_chars_max: int = 4000\n        self.wiki_client = self.build_client()\n\n    def build_client(self):\n        try:\n            import wikipedia\n            wikipedia.set_lang(self.lang)\n\n        except ImportError:\n            raise ImportError(\n                \"Could not import wikipedia python package. \"\n                \"Please install it with `pip install wikipedia`.\"\n            )\n        return wikipedia\n\n    def run(self, params) -> str:\n        \"\"\"Run Wikipedia search and get page summaries.\"\"\"\n        query = params[\"query\"]\n        page_titles = self.wiki_client.search(query, results=self.top_k_results)\n        summaries = []\n        for page_title in page_titles[: self.top_k_results]:\n            if wiki_page := self._fetch_page(page_title):\n                if summary := self._formatted_page_summary(page_title, wiki_page):\n                    summaries.append(summary)\n        if not summaries:\n            return \"No good Wikipedia Search Result was found\"\n        return \"\\n\\n\".join(summaries)[: self.doc_content_chars_max]\n\n    @staticmethod\n    def _formatted_page_summary(page_title: str, wiki_page: Any) -> Optional[str]:\n        return f\"Page: {page_title}\\nSummary: {wiki_page.summary}\"\n\n    def get_tool_call_format(self):\n        tool_call_format = {\n\t\t\t\"type\": \"function\",\n\t\t\t\"function\": {\n\t\t\t\t\"name\": \"wikipedia\",\n\t\t\t\t\"description\": \"Provides relevant information about the destination\",\n\t\t\t\t\"parameters\": {\n\t\t\t\t\t\"type\": \"object\",\n\t\t\t\t\t\"properties\": {\n\t\t\t\t\t\t\"query\": {\n\t\t\t\t\t\t\t\"type\": \"string\",\n\t\t\t\t\t\t\t\"description\": \"Search query for Wikipedia\"\n\t\t\t\t\t\t}\n\t\t\t\t\t},\n\t\t\t\t\t\"required\": [\n\t\t\t\t\t\t\"query\"\n\t\t\t\t\t]\n\t\t\t\t}\n\t\t\t}\n\t\t}\n        return tool_call_format\n```\n\n\n### How to publish tools to the toolhub\nBefore publishing tools, you need to set up the configurations as the following: \n\n```json\n{\n    \"name\": \"wikipedia\",\n    \"description\": [\n        \"Search information in the wikipedia\"\n    ],\n    \"meta\": {\n        \"author\": \"example\",\n        \"version\": \"0.0.1\",\n        \"license\": \"CC0\"\n    },\n    \"build\": {\n        \"entry\": \"tool.py\",\n        \"module\": \"Wikipedia\"\n    }\n}\n```\n\nthen you can use the following command to upload tool\n\n```python\npython cerebrum/commands/upload_tool.py \\\n    --tool_path <tool_path> \\ # tool path to the tool directory\n    --toolhub_url <toolhub_url> # the url of the toolhub, default is https://app.aios.foundation\n```\n\n## Supported LLM Cores\n| Provider 🏢 | Model Name 🤖 | Open Source 🔓 | Model String ⌨️ | Backend ⚙️ | Required API Key |\n|:------------|:-------------|:---------------|:---------------|:---------------|:----------------|\n| Anthropic | Claude 3.5 Sonnet | ❌ | claude-3-5-sonnet-20241022 |anthropic | ANTHROPIC_API_KEY |\n| Anthropic | Claude 3.5 Haiku | ❌ | claude-3-5-haiku-20241022 |anthropic | ANTHROPIC_API_KEY |\n| Anthropic | Claude 3 Opus | ❌ | claude-3-opus-20240229 |anthropic | ANTHROPIC_API_KEY |\n| Anthropic | Claude 3 Sonnet | ❌ | claude-3-sonnet-20240229 |anthropic | ANTHROPIC_API_KEY |\n| Anthropic | Claude 3 Haiku | ❌ | claude-3-haiku-20240307 |anthropic | ANTHROPIC_API_KEY |\n| OpenAI | GPT-4 | ❌ | gpt-4 |openai| OPENAI_API_KEY |\n| OpenAI | GPT-4 Turbo | ❌ | gpt-4-turbo |openai| OPENAI_API_KEY |\n| OpenAI | GPT-4o | ❌ | gpt-4o |openai| OPENAI_API_KEY |\n| OpenAI | GPT-4o mini | ❌ | gpt-4o-mini |openai| OPENAI_API_KEY |\n| OpenAI | GPT-3.5 Turbo | ❌ | gpt-3.5-turbo |openai| OPENAI_API_KEY |\n| Google | Gemini 1.5 Flash | ❌ | gemini-1.5-flash |google| GEMINI_API_KEY |\n| Google | Gemini 1.5 Flash-8B | ❌ | gemini-1.5-flash-8b |google| GEMINI_API_KEY |\n| Google | Gemini 1.5 Pro | ❌ | gemini-1.5-pro |google| GEMINI_API_KEY |\n| Google | Gemini 1.0 Pro | ❌ | gemini-1.0-pro |google| GEMINI_API_KEY |\n| Groq | Llama 3.2 90B Vision | ✅ | llama-3.2-90b-vision-preview |groq| GROQ_API_KEY |\n| Groq | Llama 3.2 11B Vision | ✅ | llama-3.2-11b-vision-preview |groq| GROQ_API_KEY |\n| Groq | Llama 3.1 70B | ✅ | llama-3.1-70b-versatile |groq| GROQ_API_KEY |\n| Groq | Llama Guard 3 8B | ✅ | llama-guard-3-8b |groq| GROQ_API_KEY |\n| Groq | Llama 3 70B | ✅ | llama3-70b-8192 |groq| GROQ_API_KEY |\n| Groq | Llama 3 8B | ✅ | llama3-8b-8192 |groq| GROQ_API_KEY |\n| Groq | Mixtral 8x7B | ✅ | mixtral-8x7b-32768 |groq| GROQ_API_KEY |\n| Groq | Gemma 7B | ✅ | gemma-7b-it |groq| GROQ_API_KEY |\n| Groq | Gemma 2B | ✅ | gemma2-9b-it |groq| GROQ_API_KEY |\n| Groq | Llama3 Groq 70B | ✅ | llama3-groq-70b-8192-tool-use-preview |groq| GROQ_API_KEY |\n| Groq | Llama3 Groq 8B | ✅ | llama3-groq-8b-8192-tool-use-preview |groq| GROQ_API_KEY |\n| ollama | [All Models](https://ollama.com/search) | ✅ | model-name |ollama| - |\n| vLLM | [All Models](https://docs.vllm.ai/en/latest/) | ✅ | model-name |vllm| - |\n| HuggingFace | [All Models](https://huggingface.co/models/) | ✅ | model-name |huggingface| HF_HOME |\n\n\n## 🖋️ References\n```\n@article{mei2024aios,\n  title={AIOS: LLM Agent Operating System},\n  author={Mei, Kai and Li, Zelong and Xu, Shuyuan and Ye, Ruosong and Ge, Yingqiang and Zhang, Yongfeng}\n  journal={arXiv:2403.16971},\n  year={2024}\n}\n@article{ge2023llm,\n  title={LLM as OS, Agents as Apps: Envisioning AIOS, Agents and the AIOS-Agent Ecosystem},\n  author={Ge, Yingqiang and Ren, Yujie and Hua, Wenyue and Xu, Shuyuan and Tan, Juntao and Zhang, Yongfeng},\n  journal={arXiv:2312.03815},\n  year={2023}\n}\n```\n\n## 🚀 Contributions\nFor how to contribute, see [CONTRIBUTE](https://github.com/agiresearch/Cerebrum/blob/main/CONTRIBUTE.md). If you would like to contribute to the codebase, [issues](https://github.com/agiresearch/Cerebrum/issues) or [pull requests](https://github.com/agiresearch/Cerebrum/pulls) are always welcome!\n\n## 🌍 Cerebrum Contributors\n[![Cerebrum contributors](https://contrib.rocks/image?repo=agiresearch/Cerebrum&max=300)](https://github.com/agiresearch/Cerebrum/graphs/contributors)\n\n\n## 🤝 Discord Channel\nIf you would like to join the community, ask questions, chat with fellows, learn about or propose new features, and participate in future developments, join our [Discord Community](https://discord.gg/B2HFxEgTJX)!\n\n## 📪 Contact\n\nFor issues related to Cerebrum development, we encourage submitting [issues](https://github.com/agiresearch/Cerebrum/issues), [pull requests](https://github.com/agiresearch/Cerebrum/pulls), or initiating discussions in AIOS [Discord Channel](https://discord.gg/B2HFxEgTJX). For other issues please feel free to contact the AIOS Foundation ([contact@aios.foundation](mailto:contact@aios.foundation)).\n\n\n\n\n"
    },
    {
      "name": "gnosis/prediction-market-agent",
      "stars": 50,
      "img": "https://avatars.githubusercontent.com/u/24954468?s=40&v=4",
      "owner": "gnosis",
      "repo_name": "prediction-market-agent",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2024-01-08T08:45:53Z",
      "updated_at": "2025-04-23T08:37:10Z",
      "topics": [],
      "readme": "# Gnosis Agent\n\nA library for exploring the landscape of AI Agent frameworks, using the example application of a prediction market betting agent. The various agents interact with markets from [Manifold](https://manifold.markets/), [Presagio](https://presagio.pages.dev/) and [Polymarket](https://polymarket.com/).\n\nThese agents build on top of the prediction market APIs from https://github.com/gnosis/prediction-market-agent-tooling.\n\n## Setup\n\nInstall the project dependencies with `poetry`, using Python >=3.10:\n\n```bash\npython3.10 -m pip install poetry\npython3.10 -m poetry install\npython3.10 -m poetry shell\n```\n\nCreate a `.env` file in the root of the repo with the following variables:\n\n```bash\nMANIFOLD_API_KEY=...\nBET_FROM_PRIVATE_KEY=...\nOPENAI_API_KEY=...\n```\n\nDepending on the agent you want to run, you may require additional variables. See an exhaustive list in `.env.example`.\n\n## Interactive Streamlit Apps\n\n- An autonomous agent with function calling. Can be 'prodded' by the user to guide its strategy: `streamlit run prediction_market_agent/agents/microchain_agent/app.py` (Deployed [here](https://autonomous-trader-agent.ai.gnosisdev.com))\n- Pick a prediction market question, or create your own, and pick one or more agents to perform research and make a prediction: `streamlit run scripts/agent_app.py` (Deployed [here](https://pma-agent.ai.gnosisdev.com))\n\n## Dune Dashboard\n\nThe on-chain activity of the deployed agents from this repo can be tracked on a Dune dashboard [here](https://dune.com/gnosischain_team/omen-ai-agents).\n\n## Running\n\nExecute `prediction_market_agent/run_agent.py`, specifying the ID of the 'runnable agent', and the market type as arguments:\n\n```bash\n% python prediction_market_agent/run_agent.py --help\n\n Usage: run_agent.py [OPTIONS] AGENT:{coinflip|replicate_to_omen|think_thorough                                         \n                     ly|think_thoroughly_prophet|think_thoroughly_prophet_kelly                                         \n                     |knownoutcome|microchain|microchain_modifiable_system_prom                                         \n                     pt_0|microchain_modifiable_system_prompt_1|microchain_modi                                         \n                     fiable_system_prompt_2|microchain_modifiable_system_prompt                                         \n                     _3|microchain_with_goal_manager_agent_0|metaculus_bot_tour                                         \n                     nament_agent|prophet_gpt4o|prophet_gpt4|prophet_gpt4_final                                         \n                     |prophet_gpt4_kelly|olas_embedding_oa|social_media|omen_cl                                         \n                     eaner|ofv_challenger}                                                                              \n                     MARKET_TYPE:{omen|manifold|polymarket|metaculus}                                                   \n                                                                                                                        \n╭─ Arguments ──────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n│ *    agent            AGENT:{coinflip|replicate_to_omen|think_thorou  [default: None] [required]                     │\n│                       ghly|think_thoroughly_prophet|think_thoroughly                                                 │\n│                       _prophet_kelly|knownoutcome|microchain|microch                                                 │\n│                       ain_modifiable_system_prompt_0|microchain_modi                                                 │\n│                       fiable_system_prompt_1|microchain_modifiable_s                                                 │\n│                       ystem_prompt_2|microchain_modifiable_system_pr                                                 │\n│                       ompt_3|microchain_with_goal_manager_agent_0|me                                                 │\n│                       taculus_bot_tournament_agent|prophet_gpt4o|pro                                                 │\n│                       phet_gpt4|prophet_gpt4_final|prophet_gpt4_kell                                                 │\n│                       y|olas_embedding_oa|social_media|omen_cleaner|                                                 │\n│                       ofv_challenger}                                                                                │\n│ *    market_type      MARKET_TYPE:{omen|manifold|polymarket|metaculu  [default: None] [required]                     │\n│                       s}                                                                                             │\n╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n╭─ Options ────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n│ --install-completion          Install completion for the current shell.                                              │\n│ --show-completion             Show completion for the current shell, to copy it or customize the installation.       │\n│ --help                        Show this message and exit.                                                            │\n╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n```\n\n## Deploying\n\nThe easiest way to make your own agent that places a bet on a prediction market is to subclass the `DeployableTraderAgent`. See `DeployableCoinFlipAgent` for a minimal example.\n\nFrom there, you can add it to the `RUNNABLE_AGENTS` dict in `prediction_market_agent/run_agent.py`, and use that as the entrypoint for running the agent in your cloud deployment.\n\n## Contributing\n\nSee the [Issues](https://github.com/gnosis/prediction-market-agent/issues) for ideas of things that need fixing or implementing. The team is also receptive to new issues and PRs.\n\nA great self-contained first contribution would be to implement an agent using a framework in the ['Other frameworks to try'](https://github.com/gnosis/prediction-market-agent/issues/210) issue.\n"
    },
    {
      "name": "LittleLittleCloud/Agent-ChatRoom",
      "stars": 48,
      "img": "https://avatars.githubusercontent.com/u/16876986?s=40&v=4",
      "owner": "LittleLittleCloud",
      "repo_name": "Agent-ChatRoom",
      "description": "A multi-agent chatroom for AutoGen agents",
      "homepage": "https://littlelittlecloud.github.io/Agent-ChatRoom/",
      "language": "C#",
      "created_at": "2024-05-18T07:10:56Z",
      "updated_at": "2025-04-12T15:20:55Z",
      "topics": [
        "agent",
        "agent-platform",
        "agent-ui",
        "agentic-framework",
        "agentic-workflow",
        "autogen",
        "llm",
        "llm-agent",
        "llm-agents",
        "multi-agent-webui",
        "next",
        "react",
        "semantic-kernel",
        "typescript",
        "webui"
      ],
      "readme": "## Agent Chatroom\n#### An extensible multi-agent platform built on top of AutoGen.Net, shadcn and Orleans.\n\n[![NuGet version](https://badge.fury.io/nu/ChatRoom.SDK.svg)](https://badge.fury.io/nu/ChatRoom.Client) | [website](https://littlelittlecloud.github.io/Agent-ChatRoom/) | Join our [Discord](https://discord.gg/UYwcVfGaeq)\n\n![Agent Chatroom](assets/agent-chatroom-switch-theme.gif)\n\n## 🌟 Highlights\n- **Multi-Agent WebUI**: Chat with multiple agents simultaneously in a web browser.\n- **Code-First**: Use code-first approach to add agents to the chatroom, see [code-first examples](https://github.com/LittleLittleCloud/Agent-ChatRoom/tree/main/example) for more information.\n- **Server mode**: You can run `ChatRoom.Client` as a restful server. For more information, see [Server mode](#-server-mode).\n\n## Quick Start with code-first approach\nYou can create an agent by writing code and add it to chatroom. See [code-first examples](https://github.com/LittleLittleCloud/Agent-ChatRoom/tree/main/example) for how to do it.\n\n## Quick Start with template\nOther than the code-first approach, `Agent Chatroom` also provides a series of templates to help you get started quickly. Use the `list-templates` command to list all available templates and the `create` command to create a configuration from the selected template.\n\n### Step 1 - Install ChatRoom.Client from NuGet\n`Agent Chatroom` is published as a dotnet tool on nuget.org. You can install the latest `Agent Chatroom` client from nuget.org by running the following command, this will install the `ChatRoom.Client` globally:\n\n```bash\ndotnet tool install --global ChatRoom.Client\n```\n\n> [!TIP]\n> `create` command also generates a json schema file for this configuration to provide intellisense in your editor. Use it wisely can greatly save your effort on writing configuration.\n\n### Step 2 - Choose from one of the available templates to get started.\n```bash\nchatroom list-templates # list all available templates\n```\n\nThe command will list all available templates.\n\n![list-templates](assets/list-templates.png)\n### Step 3 - Create configuration from the selected template.\n\n```bash\n# Create \nchatroom create -t chatroom_openai -o chatroom_openai.json\n```\n\n![fill key](assets/fill-openai-key.png)\n\n### Step 4 - Start the chatroom client with the OpenAI agent.\n\nAfter filling in the OpenAI key in the configuration file, you can start the chatroom client with the following command.\n\n```bash\n# start chatroom server\nchatroom run -c chatroom_openai.json\n```\n\nYou will see the following output from chatroom client which indicates the web UI is available at `http://localhost:51237` and `https://localhost:51238`. You can navigate to the web UI in your browser and start chatting with the agents.\n\n```bash\nweb ui is available at: http://localhost:51237;https://localhost:51238\n```\n\n## 📦 Pre-configured Chatrooms\nWe provide the following configuration to help you get started quickly. More configurations will be added in the future.\n- [OpenAI-Chatroom](https://github.com/LittleLittleCloud/OpenAI-Chatroom): chat with OpenAI gpt.\n- [Powershell-Chatroom](https://github.com/LittleLittleCloud/Powershell-ChatRoom): chat with `ps-gpt` and `ps-runner` to help you write and run PowerShell scripts.\n\n## 🌐 Server mode\nTo start a restful backend from `ChatRoom.Client`, add a `server_config` section to the client configuration file.\n\n```json\n// file: chatroom-client-server.json\n\"server_config\": {\n    \"environment\": \"Development\", // one of Development, Staging, Production.\n    \"urls\": \"http://localhost:51234;https://localhost:51235\"\n}\n```\n\nThen, run the following command to start the server. And the server will start on the `http://localhost:51234` and `https://localhost:51235`.\n\n```bash\nchatroom -c chatroom-client-server.json\n```\n\n### Swagger UI for the server\nWhen the environment is set to `Development`, the server will automatically start the Swagger UI at `/swagger` and `/swagger/index.html`. You can also access the complete swagger schema at `/swagger/v1/swagger.json`, or visit [swagger-schema.json](schema/chatroom_client_swagger_schema.json) in this repository.\n"
    },
    {
      "name": "zhattention/ticktick-ai",
      "stars": 47,
      "img": "https://avatars.githubusercontent.com/u/7688914?s=40&v=4",
      "owner": "zhattention",
      "repo_name": "ticktick-ai",
      "description": "🎯 AI-powered voice assistant for TickTick, enabling natural language task management through speech. Built with OpenAI's speech recognition and TickTick's API integration, this assistant helps you manage your todos hands-free - create tasks, set reminders, and organize your schedule using just your voice.",
      "homepage": "",
      "language": "Python",
      "created_at": "2025-02-20T08:01:37Z",
      "updated_at": "2025-04-13T18:55:32Z",
      "topics": [],
      "readme": "# AI TickTick Project\n\nAn intelligent task management system that integrates TickTick with OpenAI's GPT for natural language task processing. Features voice input support and real-time AI processing.\n\n## Features\n\n- **Voice Task Creation**: Create tasks using voice input with real-time speech recognition\n- **Natural Language Processing**: Convert speech and text to structured tasks using GPT-4\n- **TickTick Integration**: Full integration with TickTick's task management features\n- **Real-time AI Processing**: Instant token calculation and processing with OpenAI's API\n- **Web Interface**: Modern, responsive web interface for task management\n\n## Project Structure\n\n```\n├── server.py           # Main server entry point\n├── server/            # Server functionality modules\n│   ├── __init__.py\n│   └── oai_realtime.py  # OpenAI real-time processing\n├── static/            # Web assets\n│   ├── asr.html       # Speech recognition interface\n│   └── index1.html    # Main web interface\n├── tools/             # Tool implementations\n│   └── ticktick.py    # TickTick task management tools\n├── clients/           # API clients\n│   └── ticktick/      # TickTick API client\n│       ├── client.py      # Main client implementation\n│       └── authenticate.py # Authentication handling\n└── tests/             # Test files\n    └── test_ticktick_full.py  # TickTick integration tests\n```\n\n## Prerequisites\n\n- Python 3.11+\n- TickTick Developer Account\n- OpenAI API Key\n\n## Installation\n\n1. Clone the repository:\n```bash\ngit clone <repository-url>\ncd ticktick-ai \n```\n\n2. Install dependencies:\n```bash\npip install -r requirements.txt\n```\n\n3. Create a `.env` file with your credentials:\n```env\nTICKTICK_CLIENT_ID=your_client_id\nTICKTICK_CLIENT_SECRET=your_client_secret\nOPENAI_API_KEY=your_openai_api_key\n```\n\n## Running the Server\n\n1. Start the server:\n```bash\npython server.py\n```\n\n2. Access the web interface:\n   - Main interface: `http://localhost:8000`\n   - Voice input interface: `http://localhost:8000/static/asr.html`\n\n3. Using the voice interface:\n   - Click the \"开始录音\" button to start recording\n   - Speak clearly into your microphone\n   - The transcribed text will appear in real-time\n   - Click the button again to stop recording\n\n4. Server Features:\n   - WebSocket-based real-time communication\n   - Automatic audio format conversion\n   - Integrated OpenAI token processing\n   - TickTick API integration\n\n## Testing\n\nRun the test suite:\n```bash\npython -m pytest tests/\n```\n\n## Features\n\n### TickTick Integration\n- Create, read, update, and delete tasks\n- Project management\n- Task prioritization\n- Due date and reminder settings\n\n### Natural Language Processing\n- Convert natural language to structured tasks\n- Intelligent task parsing\n- Context-aware task creation\n\n### Web Interface\n- **Voice Input**: Real-time speech-to-text conversion\n  - 16kHz sampling rate for optimal speech recognition\n  - WebSocket-based real-time communication\n  - Automatic reconnection handling\n- **Task Management**:\n  - Real-time task updates and status tracking\n  - Interactive task creation and editing\n  - Voice and text input support\n- **User Experience**:\n  - Modern, responsive design\n  - Visual feedback for recording status\n  - Error handling and recovery\n\n## API Documentation\n\n### TickTick Client\n```python\nfrom clients.ticktick import TickTickClient\n\nclient = TickTickClient(client_id, client_secret)\nclient.authenticate()\n```\n\n### Task Manager\n```python\nfrom tools.ticktick import TaskManager\n\nmanager = TaskManager(client_id, client_secret)\nmanager.create_task(\"Complete project documentation\", priority=3)\n```\n\n## Contributing\n\n1. Fork the repository\n2. Create your feature branch\n3. Commit your changes\n4. Push to the branch\n5. Create a new Pull Request\n\n## License\n\nThis project is licensed under the MIT License - see the LICENSE file for details.\n"
    },
    {
      "name": "agntcy/csit",
      "stars": 47,
      "img": "https://avatars.githubusercontent.com/u/197140426?s=40&v=4",
      "owner": "agntcy",
      "repo_name": "csit",
      "description": "Continuous System Integration Testing for Agntcy Projects",
      "homepage": "",
      "language": "Go",
      "created_at": "2025-02-06T08:40:31Z",
      "updated_at": "2025-04-22T08:41:28Z",
      "topics": [],
      "readme": "# CSIT - Continuous System Integration Testing\n\n- [CSIT - Continuous System Integration Testing](#csit---continuous-system-integration-testing)\n  - [Architecture](#architecture)\n- [Integration tests](#integration-tests)\n  - [Directory structure](#directory-structure)\n  - [Running tests](#running-tests)\n  - [Running tests using GitHub actions](#running-tests-using-github-actions)\n  - [How to extend tests with your own test](#how-to-extend-tests-with-your-own-test)\n- [Samples](#samples)\n  - [Running tests](#running-tests-1)\n- [Updating the `agntcy/dir` testdata](#updating-the-agntcydir-testdata)\n- [Copyright Notice](#copyright-notice)\n\n## Architecture\n\nAgncty CSIT system design needs to meet continuously expanding requirements of\nAgntcy projects including Agent Gateway Protocol, Agent Directory and many more.\n\nThe directory structure of the CSIT:\n\n```\ncsit\n└── integrations\n│   ├── Taskfile.yaml                   # Task definitions\n│   ├── docs                            # Documentations\n│   ├── environment\n│   │   └── kind                        # kind related manifests\n│   ├── agntcy-dir                      # Agent directory related tests, components, etc...\n│   │   ├── components                  # the compontents charts\n│   │   ├── examples                    # the examples that can be used for testing\n│   │   ├── manifests                   # requred manifests for tests\n│   │   └── tests                       # tests\n│   └── agntcy-agp                      # Agent Gateway related tests, components, etc...\n│       └── agentic-apps                # Agentic apps for gateway tests\n│           ├── autogen_agent\n│           └── langchain_agent\n│\n└── samples\n    ├── app1                            # Agentic application example\n    │   ├── model.json                  # Required model file\n    │   ├── build.config.yaml           # Required build configuration file\n    ├── app2                            # Another agentic application example\n    │   ├── model.json\n    │   ├── build.config.yaml\n```\n\n\n# Integration tests\n\n> Focuses on testing interactions between integrated components.\n\n## Directory structure\n\nInside csit integrations directory contains the tasks that creating the test\nenvironment, deploying the components that will be tested, and running the tests.\n\n```\nintegrations\n├── Taskfile.yaml                   # Task definitions\n├── docs                            # Documentations\n├── environment\n│   └── kind                        # kind related manifests\n├── agntcy-dir                      # Agent directory related tests, components, etc...\n│   ├── components                  # the compontents charts\n│   ├── examples                    # the examples that can be used for testing\n│   ├── manifests                   # requred manifests for tests\n│   └── tests                       # tests\n└── agntcy-agp                      # Agent Gateway related tests, components, etc...\n    └── agentic-apps                # Agentic apps for gateway tests\n        ├── autogen_agent\n        └── langchain_agent\n```\n\n## Running tests\n\nWe can launch tests using taskfile locally or in GitHub actions.\nRunning locally we need to create a test cluster and deploy the test env on\nit before running the tests.\nIt requires the following tools to be installed on local machine:\n  - [Taskfile](https://taskfile.dev/installation/)\n  - [Go](https://go.dev/doc/install)\n  - [Docker](https://docs.docker.com/get-started/get-docker/)\n  - [Kind](https://kind.sigs.k8s.io/docs/user/quick-start#installation)\n  - [Kubectl](https://kubernetes.io/docs/tasks/tools/#kubectl)\n  - [Helm](https://helm.sh/docs/intro/install/)\n\n```bash\ncd integrations\ntask kind:create\ntask directory:test-env:deploy\ntask directory:test\n```\n\nWe can focus on specified tests:\n```bash\ntask directory:test:compiler\n```\n\nAfter we finish the tests we can destroy the test cluster\n```bash\ntask kind:destroy\n```\n\n\n## Running tests using GitHub actions\n\nWe can run integration test using Github actions using `gh` command line tool or using the GitHub web UI\n\n```bash\ngh workflow run test-integrations -f testenv=kind\n```\n\nIf we want to run the tests on a specified branch\n\n```bash\ngh workflow run test-integrations --ref feat/integration/deploy-agent-directory -f testenv=kind\n```\n\n\n## How to extend tests with your own test\n\nContributing your own tests to our project is a great way to improve the robustness and coverage of our testing suite. Follow these steps to add your tests.\n\n1. Fork and Clone the Repository\n\nFork the repository to your GitHub account.\nClone your fork to your local machine.\n\n```bash\ngit clone https://github.com/your-username/repository.git\ncd repository\n```\n\n2. Create a New Branch\n\nCreate a new branch for your test additions to keep your changes organized and separate from the main codebase.\n\n\n```bash\ngit checkout -b add-new-test\n```\n\n3. Navigate to the Integrations Directory\n\nLocate the integrations directory where the test components are organized.\n\n```bash\ncd integrations\n```\n\n4. Add Your Test\n\nCreate a new sub-directory for your test if necessary, following the existing structure. For example, integrations/new-component.\nAdd all necessary test files, such as scripts, manifests, and configuration files.\n\n5. Update Taskfile\n\nModify the Taskfile.yaml to include tasks for deploying and running your new test.\n\n```yaml\ntasks:\n  test:env:new-component:deploy:\n    desc: Desription of deployig new component elements\n    cmds:\n      - # Command for deploying your components if needed\n\n  test:env:new-component:cleanup:\n    desc: Desription of cleaning up component elements\n    cmds:\n      - # Command for cleaning up your components if needed\n\n  test:new-component:\n    desc: Desription of the test\n    cmds:\n      - # Commands to set up and run your test\n```\n\n6. Test Locally\n\nBefore pushing your changes, test them locally to ensure everything works as expected.\n\n```bash\ntask kind:create\ntask new-componet:test-env:deploy\ntask new-component:test\ntask new-componet:test-env:cleanup\ntask kind:destroy\n```\n\n7. Document Your Test\n\nUpdate the documentation in the docs folder to include details about your new test. Explain the purpose of the test, any special setup instructions, and how it fits into the overall testing strategy.\n\n8. Commit and Push Your Changes\n\nCommit your changes with a descriptive message and push them to your fork.\n\n```bash\ngit add .\ngit commit -m \"feat: add new test for component X\"\ngit push origin add-new-test\n```\n\n9. Submit a Pull Request\n\nGo to the original repository on GitHub and submit a pull request from your branch.\nProvide a detailed description of what your test covers and any additional context needed for reviewers.\n\n# Samples\n\nThe directory sturcture of the samples applications:\n\n```\nsamples\n├── app1                            # Agentic application example\n│   ├── model.json                  # Required model file\n│   ├── build.config.yaml           # Required build configuration file\n├── app2                            # Another agentic application example\n│   ├── model.json\n│   ├── build.config.yaml\n```\n\nThe samples directory in the CSIT repository serves two primary purposes related to the testing of agentic applications:\n\n\n1. Compilation and Execution Verification: The agentic applications stored within the samples directory are subjected to sample tests. These tests are designed to run whenever changes are made to the agentic apps to ensure they compile correctly and are able to execute as expected.\n2. Base for Agent Directory Integration Test:\nThe agentic applications in the samples directory also serve as the foundation for the agent model build and push test. This specific test checks for the presence of two required files: model.json and build.config.yaml. If these files are present within an agentic application, the integration agent model build and push testa are triggered. This test is crucial for validating the construction and verification of the agent model, ensuring that all necessary components are correctly configured and operational.\n\n## Running tests\n\nWe can launch tests using taskfile locally or in GitHub actions.\nRunning locally we need some tools to build the sample applications and run the tests.\nIt requires the followings on local machine:\n  - [Taskfile](https://taskfile.dev/installation/)\n  - [Python 3.12.X](https://www.python.org/downloads/)\n  - [Poetry](https://python-poetry.org/docs/#installation)\n  - [Docker](https://docs.docker.com/get-started/get-docker/)\n  - [Kind](https://kind.sigs.k8s.io/docs/user/quick-start#installation)\n  - [Kubectl](https://kubernetes.io/docs/tasks/tools/#kubectl)\n\n```bash\ncd samples/[app-name]\ntask run:test\n```\n\n## Updating the agntcy/dir testdata\n\nIf we want to update the `integrations/agntcy-dir/examples/dir/e2e/testdata` directory we will need to add `agntcy/dir` as a remote and create a patch for it by diffing with the `agntcy/dir` repo\n\n```bash\n# add agntcy/dir as remote\ngit remote add -f dir https://github.com/agntcy/dir.git\n# fetch dir\ngit fetch dir\n# example of updating the integrations/agntcy-dir/examples/dir/e2e/testdata directory to the agntcy/dir main\ngit diff --binary HEAD:integrations/agntcy-dir/examples/dir/e2e/testdata dir/main:e2e/testdata | git apply --directory=integrations/agntcy-dir/examples/dir/e2e/testdata\n```\n\n## Copyright Notice\n\n[Copyright Notice and License](./LICENSE.md)\n\nDistributed under Apache 2.0 License. See LICENSE for more information.\nCopyright AGNTCY Contributors (https://github.com/agntcy)\n"
    },
    {
      "name": "Azure/ai-foundry-workshop",
      "stars": 45,
      "img": "https://avatars.githubusercontent.com/u/6844498?s=40&v=4",
      "owner": "Azure",
      "repo_name": "ai-foundry-workshop",
      "description": null,
      "homepage": "https://aifoundry.app",
      "language": "Jupyter Notebook",
      "created_at": "2025-01-08T01:47:59Z",
      "updated_at": "2025-04-19T00:43:12Z",
      "topics": [
        "agent-service",
        "agents",
        "ai-foundry",
        "azure",
        "azure-ai",
        "azure-ai-foundry",
        "azure-openai",
        "openai"
      ],
      "readme": "# Azure AI Foundry Workshop\n\n<div align=\"center\">\n\n[📦Prerequisites](#-prerequisites) | [🚀Quick Start](#-quick-start) | [🤖Overview](#-overview) | [📔Workshop Content](#-workshop-content) | [🧩Project Structure](#-project-structure) | [❓Support](#-support) | [🤝Contributing](#-contributing)\n\n</div>\n\n\n## 🤖 Overview\n\nA hands-on workshop that guides you through building intelligent apps and AI agents on top of Azure AI Foundry, with fun examples related to health and dietary advice. You will:\n- Learn Azure AI Foundry fundamentals\n- Set up authentication and project configuration\n- Deploy and test AI models\n- Build AI agents (health advisor examples)\n- Implement health calculations and dietary planning\n- Evaluate agent performance and monitor quality attributes\n- Deploy an end-to-end AI native sample app that incorporates all these capabilities and design patterns\n\n> **Duration**: 4-5 hours  \n> **Focus**: Hands-on exercises, interactive notebooks, practical examples, end-to-end project\n\n## 🎥 Workshop Overview Video\n\nWatch our overview video to get a comprehensive understanding of the workshop:\n[![Azure AI Foundry Workshop Overview](https://img.youtube.com/vi/0bGLgmZJ6DE/0.jpg)](https://youtu.be/0bGLgmZJ6DE)\n\n## 📸 Visuals\n\n### Sections Overview\n<img src=\"./3-ai-native-e2e-sample/assets/sections.png\" width=\"70%\" />\n*Figure 1: Overview of different sections in this workshop.*\n\n### End-to-End Sample UI\n<img src=\"./3-ai-native-e2e-sample/assets/ui.png\" width=\"70%\" />\n*Figure 2: User interface of the end-to-end AI native sample project.*\n\n---\n\n## 📦 Prerequisites    \n\nBefore starting the workshop, ensure you have:\n\n- [Python 3.10](https://www.python.org/downloads/) or higher installed\n- An active Azure subscription with access to [Azure AI Foundry](https://ai.azure.com)\n- [Azure CLI](https://learn.microsoft.com/en-us/cli/azure/install-azure-cli) installed\n- [Git](https://git-scm.com/downloads) installed\n- [VS Code](https://code.visualstudio.com/), [GitHub Codespaces](https://github.com/features/codespaces), or [Jupyter Notebook](https://jupyter.org/install) environment\n- Basic Python programming knowledge\n- Model deployment and [AI Search](https://learn.microsoft.com/en-us/azure/search/search-what-is-azure-search) connection configured in Azure AI Foundry\n\n---\n\n## 🚀 Quick Start\n\n1. **Clone the repo**:\n   ```bash\n   git clone https://github.com/Azure/ai-foundry-workshop.git\n   cd ai-foundry-workshop\n   ```\n\n2. **Install uv**:\n   ```bash\n   # Unix/Linux/macOS\n   curl -LsSf https://astral.sh/uv/install.sh | sh\n\n   # Windows (PowerShell)\n   (Invoke-WebRequest -Uri https://astral.sh/uv/install.ps1 -UseBasicParsing).Content | pwsh\n   ```\n\n3. **Create & activate a virtual environment**:\n   ```bash\n   uv venv\n   source .venv/bin/activate  # Windows: .venv\\Scripts\\activate\n   ```\n\n4. **Set up Azure AI Foundry**:\n\n   a. **Create Project and Deploy Resources**:\n      1. Go to [Azure AI Foundry](https://ai.azure.com)\n      2. Create a new AI Hub and Project using the AI Foundry Wizard\n      3. Deploy required models:\n         - GPT models(gpt-4o, gpt-4o-mini) for chat/completion (**set TPM to max** to avoid issues with Agents notebooks)\n         - Embedding model for vector search\n         - Ensure the model is deployed in `Global-Standard` or `DataZone-Standard`\n      4. Set up connections:\n         - Configure [Grounding with Bing](https://learn.microsoft.com/en-us/azure/ai-services/agents/how-to/tools/bing-grounding?view=azure-python-preview&tabs=python&pivots=overview) connection\n         - Configure Azure AI Search connection\n      5. Add your user account to the `Azure AI Developer` role from Azure AI Foundry Management Portal\n\n   b. **Configure Environment Variables**:\n      ```bash\n      cp .env.example .env\n      ```\n      Update `.env` with your Azure AI Foundry values:\n      - `PROJECT_CONNECTION_STRING`: Your project connection string from Azure ML workspace\n      - `MODEL_DEPLOYMENT_NAME`: Your model deployment name\n      - `EMBEDDING_MODEL_DEPLOYMENT_NAME`: Your embedding model deployment name\n      - `TENANT_ID`: Your tenant ID from Azure portal\n      - `BING_CONNECTION_NAME`: Your Bing search connection name\n      - `SERVERLESS_MODEL_NAME`: Your serverless model name\n\n      > **Note**: The model specified in `MODEL_DEPLOYMENT_NAME` must be supported by Azure AI Agents Service or Assistants API. See [supported models](https://learn.microsoft.com/en-us/azure/ai-services/agents/concepts/model-region-support?tabs=python#azure-openai-models) for details. For Grounding with Bing Search, you need to use `gpt-4o-mini` model.\n\n5. **Install dependencies**:\n   ```bash\n   # Install core Azure AI SDKs and Jupyter requirements\n   uv pip install azure-identity azure-ai-projects azure-ai-inference[opentelemetry] azure-search-documents azure-ai-evaluation azure-monitor-opentelemetry\n\n   # Install Jupyter requirements\n   uv pip install ipykernel jupyterlab notebook\n\n   # Register the kernel with Jupyter\n   python -m ipykernel install --user --name=.venv --display-name=\"Python (.venv)\"\n\n   # Install additional requirements (optional - for deploying repo or running mkdocs)\n   uv pip install -r requirements.txt\n   ```\n\n   > **Note**: If you encounter kernel errors in VS Code, try:\n   > 1. Select kernel: Click \"Select Kernel\" > \"Python Environments\" > \"Python (.venv)\"\n   > 2. If kernel is not listed, run `python -m ipykernel install --user --name=.venv` again, or use the \"Create New Kernel\" wizard in VS Code to create a new Python environment\n   > 3. Reload VS Code if needed\n\n6. **Choose your notebook environment**:\n\n   **Option A: VS Code**\n   - Install [VS Code Python extension](https://marketplace.visualstudio.com/items?itemName=ms-python.python)\n   - Install either:\n     - [Jupyter extension](https://marketplace.visualstudio.com/items?itemName=ms-toolsai.jupyter) for .ipynb files\n     - [Polyglot Notebooks extension](https://marketplace.visualstudio.com/items?itemName=ms-dotnettools.dotnet-interactive-vscode) for .dib files\n   - Open any notebook and select your Python kernel (.venv)\n\n   **Option B: GitHub Codespaces**\n   - Click \"Code\" > \"Create codespace\" on the repository\n   - Wait for environment setup\n   - Notebooks will be ready to run\n\n   **Option C: Jupyter Lab/Notebook**\n   ```bash\n   # Install Jupyter if you haven't already\n   uv pip install jupyterlab notebook\n\n   # Start Jupyter Lab (recommended)\n   jupyter lab\n\n   # Or start Jupyter Notebook\n   jupyter notebook\n   ```\n\n7. **Follow the Learning Path**:\n    1. **Introduction** (`1-introduction/`)\n       - `1-authentication.ipynb`: Set up your Azure credentials\n       - `2-environment_setup.ipynb`: Configure your environment\n       - `3-quick_start.ipynb`: Learn basic operations\n\n    2. **Main Workshop** (`2-notebooks/`)\n       - Chat Completion & RAG (`1-chat_completion/`)\n       - Agent Development (`2-agent_service/`)\n       - Quality Attributes (`3-quality_attributes/`)\n\n---\n\n## 📔 Workshop Learning Path\n\nFollow these notebooks in sequence to complete the workshop:\n\n### 1. Introduction (`1-introduction/`)\n| Notebook | Description |\n|----------|-------------|\n| [1. Authentication](1-introduction/1-authentication.ipynb) | Set up Azure credentials and access |\n| [2. Environment Setup](1-introduction/2-environment_setup.ipynb) | Configure your development environment |\n| [3. Quick Start](1-introduction/3-quick_start.ipynb) | Learn basic Azure AI Foundry operations |\n\n### 2. Main Workshop (`2-notebooks/`)\n| Topic | Notebooks |\n|-------|-----------|\n| **Chat Completion & RAG** | • [Chat Completion & RaG](2-notebooks/1-chat_completion/) |\n| **Agent Development** | • [Agent Development](2-notebooks/2-agent_service/) |\n| **Quality Attributes** | • [Observability & Evaluations](2-notebooks/3-quality_attributes/) |\n\n---\n\n## 🤝 Contributing\n\nWe welcome contributions! Please see our [Contributing Guide](CONTRIBUTING.md) for details on how to:\n- Submit bug reports and feature requests\n- Submit pull requests\n- Follow our coding standards\n- Participate in code reviews\n\n---\n\n## ❓ Support\n\nIf you need help or have questions:\n\n---\n\n\n\n\n"
    },
    {
      "name": "rvinothrajendran/MicrosoftSemanticKernelSamples",
      "stars": 40,
      "img": "https://avatars.githubusercontent.com/u/16264167?s=40&v=4",
      "owner": "rvinothrajendran",
      "repo_name": "MicrosoftSemanticKernelSamples",
      "description": null,
      "homepage": "",
      "language": "C#",
      "created_at": "2023-08-22T04:23:11Z",
      "updated_at": "2025-03-26T01:43:25Z",
      "topics": [
        "hacktoberfest",
        "hacktoberfest-accepted",
        "hacktoberfest-starter",
        "hacktoberfest2023"
      ],
      "readme": "# MicrosoftSemanticKernel-Samples\n\nThe repository named MicrosoftSemanticKernelSamples contains code samples featured in my video tutorials aimed at helping you understand Microsoft Semantic Kernel. \nTo acquire knowledge about Microsoft Semantic Kernel, please consider visiting [Microsoft Semantic Kernel](https://www.youtube.com/@vinothrajendran)\n.\n\n| Topic                 | Description                                                                                                                                                                                                                                 | Source Code                                                                                                                                                                                                                                                                                       | YouTube - Demo                                                                           |\n|-----------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------|\n| 01-HelloWorld         | In this example, we illustrate a Hello World application using the Semantic Kernel. We demonstrate how to construct a prompt, establish a connection to Azure OpenAI services, process the user's input, and finally, display the resulting output after execution. | [Source code](https://github.com/rvinothrajendran/MicrosoftSemanticKernelSamples/tree/main/SKSampleCSharp/01HelloWorld)                                                                                                                                                                           | [![HelloWorld Demo](https://img.youtube.com/vi/-lPI4DNKDWc/0.jpg)](https://www.youtube.com/watch?v=-lPI4DNKDWc&t=0s)                                  |\n| 02-Prompt plugins     | This example illustrates the utilization of a Plugin through Prompt functions in the Microsoft Semantic Kernel. The Prompt function is specified in both the config.json and skprompt.txt files.                                            | [Source code](https://github.com/rvinothrajendran/MicrosoftSemanticKernelSamples/tree/main/SKSampleCSharp/PromptFunction)                                                                                                                                                                        | [![Prompt Plugins Demo](https://img.youtube.com/vi/Hyfh1wf2QjM/0.jpg)](https://www.youtube.com/watch?v=Hyfh1wf2QjM)                                |\n| 03-YAML Config        | The example highlights a different approach to creating a prompt plugin within the Microsoft Semantic Kernel using YAML configuration. Unlike the conventional method that involves using two separate files, namely Config.json and SKPrompt.txt, to define the prompt template, this alternative approach condenses the process by consolidating the template creation into a single file. This simplification can enhance readability and manageability in certain scenarios. | [Source code](https://github.com/rvinothrajendran/MicrosoftSemanticKernelSamples/tree/main/SKSampleCSharp/PromptYAMLDemo)                                                                                                          | [![YAML Config Demo](https://img.youtube.com/vi/1mvb3wZg8JQ/0.jpg)](https://www.youtube.com/watch?v=1mvb3wZg8JQ)                                |\n| 04-KernelStreaming    | In this example, the \"InvokeStreamingAsync\" method is employed to establish a connection between Azure OpenAI and generate data. Utilizing this API enables real-time interaction with customers, facilitating ongoing communication.                           | [Source code](https://github.com/rvinothrajendran/MicrosoftSemanticKernelSamples/tree/main/SKSampleCSharp/KernelStreaming)                                                                                                 | [![KernelStreaming Demo](https://img.youtube.com/vi/pt8Rp4Tm2dM/0.jpg)](https://www.youtube.com/watch?v=pt8Rp4Tm2dM)                                |\n| 09-HandleBarsTemplate | Handlebars functions as a template engine, empowering the development of dynamic and reusable prompts for large language models (LLMs) within the Semantic Kernel. The Semantic Kernel serves as a platform specifically crafted for building generative AI applications. By harnessing the capabilities of Handlebars, users have the flexibility to utilize variables, expressions, helpers, and partials to customize prompts, thereby augmenting their adaptability and effectiveness. The C# SDK of the Semantic Kernel seamlessly integrates with Handlebars, and the HandlebarsPromptTemplateFactory class streamlines the creation and management of Handlebars templates. | [Source code](https://github.com/rvinothrajendran/MicrosoftSemanticKernelSamples/tree/main/SKSampleCSharp/HandleBarsPromptTemplate) | [![Watch the Demo](https://img.youtube.com/vi/iw4jlKdq4qA/0.jpg)](https://www.youtube.com/watch?v=iw4jlKdq4qA)                                            |\n"
    },
    {
      "name": "Cloud-Jas/DurableMultiAgents",
      "stars": 39,
      "img": "https://avatars.githubusercontent.com/u/96443739?s=40&v=4",
      "owner": "Cloud-Jas",
      "repo_name": "DurableMultiAgents",
      "description": "Durable Multi Agents is an attempt to make use of Azure Durable Functions with Semantic Kernel to build Multi-Agents workflows.",
      "homepage": null,
      "language": "JavaScript",
      "created_at": "2024-08-04T14:11:04Z",
      "updated_at": "2025-04-11T13:36:56Z",
      "topics": [],
      "readme": "<div id=\"top\"></div> <h1 align=\"center\"> <a href=\"https://travelagency.iamdivakarkumar.com/\" style=\"text-decoration: none;\" target=\"_blank\"> <img height=\"240\" src=\"./docs/images/logo.png\" alt=\"ContosoTravelAgency Logo\"/> <br/> ContosoTravelAgency </a> </h1> <p align=\"center\"> <b>Explore how ContosoTravelAgency leverage Azure Durable Functions and Semantic Kernel to build sophisticated Multi-Agent workflows.</b> </p> <br/>\n\n<p align=\"center\">\n  <a href=\"https://travelagency.iamdivakarkumar.com/\" target=\"_blank\" style=\"text-decoration: none;\">\n    <img src=\"https://img.shields.io/badge/Visit%20ContosoTravelAgency-blue?style=for-the-badge&logo=azure\" alt=\"Visit ContosoTravelAgency\"/>\n  </a>\n   <a href=\"https://contosotravelagency.iamdivakarkumar.com/\" target=\"_blank\" style=\"text-decoration: none;\">\n    <img src=\"https://img.shields.io/badge/More%20Info-blue?style=for-the-badge&logo=azure\" alt=\"More Info\"/>\n  </a>\n</p>\n\n![Banner Image](docs/images/banner.png)\n![Image description](docs/images/screenshot.png)\n\n# Architecture\n\n![Image description](docs/images/DurableMultiAgents.jpg)\n\nDurable Multi Agents is an attempt to make use of Azure Durable Functions with Semantic Kernel to build Multi-Agents workflows.\n\n<b style=\"color:orange\">\n\n```quote\nDisclaimer: Realtime Agent only works in local environment, will be updated soon to work in live URL\nUpdate config.json under .azure folder to point swedencentral for realtimeapilocation as it is not widely available\n\n```\n\n\n```json\n\"infra\": \n  {\n    \"parameters\":\n      {\n        \"openAILocation\": \"westus\",\n        \"realtimeOpenAILocation\": \"swedencentral\"\n      }\n  }\n```\n\n</b>\n\n# High level components\n\n![Image description](docs/images/azuredurablefunction.png)\n\n\n# Distributed Tracing\n\n## Jaeger tracing\n\n![Image description](docs/images/distributedtracing-1.png)\n\n![Image description](docs/images/distributedtracing-2.png)\n\n![Image description](docs/images/distributedtracing-3.png)\n\n![Image description](docs/images/distributedtracing-4.png)\n\n\n## Application Insights\n\n![Image description](docs/images/distributedtracing-5.png)\n\n# Highlights\n\n- Proposed solution have implemented different patterns with Multi-Agents , some of them are \n    - Fan-Out/ Fan-in\n    - Function Chaining\n- NL2SQL is built on top of Semantic layer with the help of Azure Cosmos DB\n- Vectorization is done on top of sematic layer to handle similarity based queries\n- High scalability of each agents\n- Managed Idenitty connectivity to Azure OpenAI and Azure Cosmos DB services\n- Provided seed data project for you to get started\n- Switch between Custom/SingleAI/AutoGen based Multi-Agents on the fly.\n- Monitoring agent's responses in the side pane of the application\n- Distributed tracing support \n\n## Prerequisites\n\n1. Install the [Azure Developer CLI](https://learn.microsoft.com/en-us/azure/developer/azure-developer-cli/install-azd?tabs=winget-windows%2Cbrew-mac%2Cscript-linux&pivots=os-windows)\n2. Install the [Azure Functions Core Tools](https://docs.microsoft.com/en-us/azure/azure-functions/functions-run-local?tabs=windows%2Ccsharp%2Cbash)\n3. Install Powershell (dotnet tool update --global PowerShell)\n\n\n# Deploy to Azure\n\nThe easiest way to deploy this app is using the [Azure Developer CLI](https://aka.ms/azd).\n\n## To provision and deploy:\n\n1) Open a new terminal and do the following from root folder:\n```bash\nazd up\n```\n2) **[Optional]** Once the deployment is complete, make sure to update the settings with postmark server token in the Azure Portal. This is to ensure the email notifications are sent properly.\n3) **[Optional]** If you want to switch to a different user, update the settings in the Azure Portal with the new userId.\n4) **[optional]** If you want to connect with Microsoft Data fabric , follow this [blog]() for instructions to setup. \n\n# Steps to run the project locally\n\n> **IMPORTANT**  \n> For existing users of ContosoTravelAgency:  \n> Make sure to add `HARD_RELOAD` in the `.env` file under the `.azure` folder to clear existing tables and rewrite with new data.\n\n\nAdd the `local.settings.json` in the Azure Durable Function project, with the below details\n\n```json\n{\n  \"IsEncrypted\": false,\n  \"Values\": {\n    \"AzureWebJobsStorage\": \"UseDevelopmentStorage=true\",\n    \"FUNCTIONS_WORKER_RUNTIME\": \"dotnet-isolated\",\n    \"APPINSIGHTS_INSTRUMENTATIONKEY\": \"Your_InstrumentationKey_Here\",\n    \"APPLICATIONINSIGHTS_CONNECTION_STRING\": \"Your_ConnectionString_Here\",\n    \"CosmosDBAccountEndpoint\": \"Your_CosmosDBAccountEndpoint_Here\",\n    \"SubscriptionId\": \"Your_SubscriptionId_Here\",\n    \"ResourceGroup\": \"Your_ResourceGroup_Here\",\n    \"DatabaseAccount\": \"Your_DatabaseAccount_Here\",\n    \"DatabaseId\": \"Your_DatabaseId_Here\",\n    \"TenantId\": \"Your_TenantId_Here\",\n    \"PostmarkServerToken\": \"Your_PostmarkServerToken_Here\",    \n    \"cosmosDB__clientSecret\": \"Your_ClientSecret_Here\",\n    \"cosmosDB__tenantId\": \"Your_TenantId_Here\",\n    \"cosmosDB__clientId\": \"Your_ClientId_Here\",\n    \"cosmosDB__accountEndpoint\": \"Your_CosmosDBAccountEndpoint_Here\",\n    \"OpenAIEndpoint\": \"Your_OpenAI_Endpoint\",\n    \"OpenAIChatCompletionDeploymentName\": \"Your_OpenAI_ChatCompletion_Deployment_Name\",\n    \"OpenAITextEmbeddingGenerationDeploymentName\": \"Your_OpenAI_TextEmbedding_Deployment_Name\",\n    \"RedisConnectionString\":\"Your_Redis_Conn_string\"\n  }\n}\n```\n\n- To get the `APPINSIGHTS_INSTRUMENTATIONKEY` and `APPLICATIONINSIGHTS_CONNECTION_STRING` you need to create an Application Insights in Azure and get the key from there\n- To get the `CosmosDBAccountEndpoint` you need to create an Azure Cosmos DB account and get the endpoint from there\n- To get the `SubscriptionId`, `ResourceGroup`, `DatabaseAccount`, `DatabaseId`, `TenantId`, you need to get the details from Azure Portal\n- To get the `PostmarkServerToken` and `FromMailAddress` you need to create an account in Postmark and get the token from there. FromMailAddress should be based on the domain you have registered in Postmark\n- To get the `cosmosDB__clientSecret`, `cosmosDB__tenantId`, `cosmosDB__clientId`, `cosmosDB__accountEndpoint` you need to create an Azure AD application and assign it with built-in Cosmos DB Data contributor role.\n- To get the `OpenAIEndpoint`, `OpenAIChatCompletionDeploymentName`, `OpenAITextEmbeddingGenerationDeploymentName` you need to create an Azure OpenAI service and create deployments for Chat Completion and Text Embedding Generation\n- Once you have all the details updated in the `local.settings.json` you can run the project\n\n# Steps to assign managed identity to Azure OpenAI Services\n\n- Go to Azure Portal\n- Search for Azure OpenAI Services\n- Select Access Control (IAM)\n- Click on Add Role Assignment\n- Select the role as Cognitive Services OpenAI Contributor\n- Select the managed identity you have created for the Azure Functions, if running locally make sure the logged in user has the required permissions\n\n# Steps to assign managed identity to Azure Cosmos DB\n\n- Go to Azure Portal\n- Open Azure Cloud Shell\n- Run the below command to assign Cosmos DB data Contributor role\n\t```bash\n\taz cosmosdb sql role assignment create --resource-group $resourceGroupName --account-name $cosmosName --role-definition-id \"00000000-0000-0000-0000-000000000002\" --principal-id \"<your-principal-id>\" --scope \"/\"\n\t```\n- Replace the `principal-id` with the managed identity id you have created for the Azure Functions or the Azure AD application's ObjectId if running locally.\n\n# Data Flow\n\n## Travel based query\n\n### Flight Agent flow\n\n![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/l923gxn4lhq5u3fsezg7.png)\n\n1. The user initiates a request to the ManagerAgent, stating, \"I'm planning for a trip from Chennai to Goa this month on the 10th for a 5-day stay.\"\n2. The ManagerAgent receives the request and decides to delegate the task to the travel orchestrator.\n3. The travel orchestrator activates and takes charge of the request.\n4. The travel orchestrator instructs the FlightAgent to perform flight searches.\n5. The FlightAgent sends a query to Azure Cosmos DB to retrieve flights based on the departure city (Chennai) and the departure date (10th of the month).\n6. Azure Cosmos DB processes the query and returns the relevant flight information to the FlightAgent.\n7. The FlightAgent sends another query to Azure Cosmos DB to retrieve flights based on the destination city (Goa) and the duration of the stay (5 days).\n8. Azure Cosmos DB processes the query and returns the relevant flight information to the FlightAgent.\n9. The FlightAgent consolidates the flight information and sends it back to the travel orchestrator.\n10. The travel orchestrator returns the final flight search results to the ManagerAgent.\n11. The ManagerAgent sends the final response back to the user.\n\n![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/4x2k8j0d37a1laagkz46.png)\n\n### Weather Agent flow\n\n![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/1zcypkt5vv3zzp7ppbxm.png)\n\n1. The user initiates a request to the ManagerAgent, asking, \"Can you let me know how is the weather?\"\n2. The ManagerAgent receives the request and decides to delegate the task to the travel orchestrator.\n3. The travel orchestrator activates and takes charge of the request.\n4. The travel orchestrator instructs the WeatherAgent to perform weather searches.\n5. The WeatherAgent sends a query to Azure Cosmos DB to retrieve weather conditions based on the departure city and the departure date.\n6. Azure Cosmos DB processes the query and returns the relevant weather information to the WeatherAgent.\n7. The WeatherAgent sends another query to Azure Cosmos DB to retrieve weather conditions based on the destination city and the return date.\n8. Azure Cosmos DB processes the query and returns the relevant weather information to the WeatherAgent.\n9. The WeatherAgent consolidates the weather information and sends it back to the travel orchestrator.\n10. The travel orchestrator returns the final weather search results to the ManagerAgent.\n11. The ManagerAgent sends the final response back to the user.\n\n\n![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/u21unebmccg8ww8c0ktc.png)\n\n\n### Booking flow\n\n![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/q4n93u2v2zqcbs2a9niw.png)\n![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/r2t3o53oc3greupyrv4b.png)\n\n## Similarity based query \n\n![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/5rbdwe8cwlt5b26ibkvy.png)\n\nUser query : \"Have I visited any beach destinations in the past?\"\n\n1. The user initiates a request to the ManagerAgent, asking, \"Have I visited any beach destinations in the past?\"\n2. The ManagerAgent receives the user’s request and decides to delegate the task to the default orchestrator.\n3. The ManagerAgent activates the Default Orchestrator to handle the request.\n4. The Default Orchestrator instructs the SemanticLayerAgent to perform a Natural Language to SQL (NL2SQL) conversion and fetch the relevant data.\n5. The SemanticLayerAgent processes the NL2SQL query conversion and sends a query to Azure Cosmos DB.\n6. Azure Cosmos DB processes the SQL query and returns the requested data to the SemanticLayerAgent.\n7. Simultaneously, the Default Orchestrator instructs the SemanticVectorAgent to perform a vector similarity search.\n8. The SemanticVectorAgent creates a vectorized query and sends it to Azure Cosmos DB for searching against embeddings.\n9. Azure Cosmos DB processes the vectorized query and returns the relevant data to the SemanticVectorAgent.\n10. The SemanticLayerAgent and SemanticVectorAgent both send their responses to the ConsolidatorAgent.\n11. The ConsolidatorAgent combines the responses from both agents to form a cohesive response.\n12. The ConsolidatorAgent returns the consolidated response to the ManagerAgent.\n13. The ManagerAgent sends the final response back to the user.\n\n![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/j2fpbhgwko9cn5vmz6o9.png)\n\n# Live URL\nhttps://travelagency.iamdivakarkumar.com/\n\n# Sponsor\n\nLeave a ⭐ if you like this project\n\n<a href=\"https://www.buymeacoffee.com/divakarkumar\" target=\"_blank\"><img src=\"https://cdn.buymeacoffee.com/buttons/v2/default-yellow.png\" alt=\"Buy Me A Coffee\" style=\"height: 40px !important;width: 145 !important;\" ></a>\n\n&copy; [Divakar Kumar](//github.com/Divakar-Kumar)\n\n# Contact\n\n[Website](//iamdivakarkumar.com) | [LinkedIn](https://www.linkedin.com/in/divakar-kumar/)\n"
    },
    {
      "name": "microsoft/agdebugger",
      "stars": 38,
      "img": "https://avatars.githubusercontent.com/u/6154722?s=40&v=4",
      "owner": "microsoft",
      "repo_name": "agdebugger",
      "description": null,
      "homepage": "",
      "language": "TypeScript",
      "created_at": "2025-01-29T23:30:02Z",
      "updated_at": "2025-04-21T14:59:34Z",
      "topics": [],
      "readme": "# AGDebugger\n\nAGDebugger is an interactive system to help you debug your agent teams. It offers interactions to:\n\n1. Send and step through agent messages\n2. Edit previously sent agent messages and revert to earlier points in a conversation\n3. Navigate agent conversations with an interactive visualization\n\n![screenshot of AGDebugger interface](.github/screenshots/agdebugger_sc.png)\n\n## Local Install\n\nYou can install AGDebugger locally by cloning the repo and installing the python package.\n\n```sh\n# Install & build frontend\ncd frontend\nnpm install\nnpm run build\n# Install & build agdebugger python package\ncd ..\npip install .\n```\n\n## Usage\n\nAGDebugger is built on top of [AutoGen](https://microsoft.github.io/autogen/stable/). To use AGDebugger, you provide a python file that exposes a function that creates an [AutoGen AgentChat](https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/index.html) team for debugging. You can then launch AgDebugger with this agent team.\n\nFor example, the script below creates a simple agent team with a single WebSurfer agent.\n\n```python\n# scenario.py\nfrom autogen_agentchat.teams import MagenticOneGroupChat\nfrom autogen_agentchat.ui import Console\nfrom autogen_ext.agents.web_surfer import MultimodalWebSurfer\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n\nasync def get_agent_team():\n    model_client = OpenAIChatCompletionClient(model=\"gpt-4o\")\n\n    surfer = MultimodalWebSurfer(\n        \"WebSurfer\",\n        model_client=model_client,\n    )\n    team = MagenticOneGroupChat([surfer], model_client=model_client)\n\n    return team\n```\n\nWe can then launch the interface with:\n\n```sh\n agdebugger scenario:get_agent_team\n```\n\nOnce in the interface, you can send a GroupChatStart message to the start the agent conversation and begin debugging!\n\n## Citation\n\nSee our [CHI 2025 paper](https://arxiv.org/abs/2503.02068) for more details on the design and evaluation of AGDebugger.\n\n```bibtex\n@inproceedings{epperson25agdebugger,\n    title={Interactive Debugging and Steering of Multi-Agent AI Systems},\n    author={Will Epperson and Gagan Bansal and Victor Dibia and Adam Fourney and Jack Gerrits and Erkang Zhu and Saleema Amershi},\n    year={2025},\n    publisher = {Association for Computing Machinery},\n    booktitle = {Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},\n    series = {CHI '25}\n}\n```\n"
    },
    {
      "name": "gagb/autogen-yt-agent",
      "stars": 28,
      "img": "https://avatars.githubusercontent.com/u/13227607?s=40&v=4",
      "owner": "gagb",
      "repo_name": "autogen-yt-agent",
      "description": "A YouTube agent built using AutoGen",
      "homepage": "",
      "language": "Python",
      "created_at": "2024-11-29T01:36:05Z",
      "updated_at": "2025-04-18T04:40:29Z",
      "topics": [
        "autogen",
        "autogen-extension",
        "autogen-sample"
      ],
      "readme": "# AutoGen YouTube Agent\n\n`autogen-yt-agent` is a Python package that provides an agent capable of downloading and processing YouTube videos. It includes functionalities such as extracting audio, transcribing audio with timestamps, and getting video length.\n\n## Installation\nTo install the package from the GitHub repository, use the following command:\n\n```bash\npip install git+https://github.com/gagb/autogen-yt-agent\n```\n\n## Usage\nSee [example.py](./example.py) for a simple example of how to use this agent. Note, this example requires\nthe OpenAI client, so please install the relevant extension.\n\n## License\nThis project is licensed under the MIT License.\n"
    },
    {
      "name": "skyflo-ai/skyflo",
      "stars": 27,
      "img": "https://avatars.githubusercontent.com/u/198904896?s=40&v=4",
      "owner": "skyflo-ai",
      "repo_name": "skyflo",
      "description": "AI Agent for Cloud Native.",
      "homepage": "https://skyflo.ai",
      "language": "Python",
      "created_at": "2025-02-28T13:49:42Z",
      "updated_at": "2025-04-21T14:47:09Z",
      "topics": [
        "ai-agent",
        "cloud-native",
        "devops"
      ],
      "readme": "# Skyflo.ai - AI Agent for Cloud Native\n\n<p align=\"center\">\n  <img src=\"./assets/readme.png\" alt=\"Skyflo.ai\" width=\"1000\"/>\n</p>\n\n<div align=\"center\">\n\n  [![Website](https://img.shields.io/badge/Website-Visit-blue.svg)](https://skyflo.ai)\n  [![Discord](https://img.shields.io/badge/Discord-Join-blue.svg)](https://discord.gg/kCFNavMund)\n  [![Twitter/X Follow](https://img.shields.io/twitter/follow/skyflo_ai?style=social)](https://x.com/skyflo_ai)\n  [![YouTube Channel](https://img.shields.io/badge/YouTube-Subscribe-red.svg)](https://www.youtube.com/@SkyfloAI)\n\n</div>\n\nSkyflo.ai is your AI-powered companion for cloud native operations, enabling seamless infrastructure management through natural language.\n\n## How to Install\n\n```bash\ncurl -sL https://raw.githubusercontent.com/skyflo-ai/skyflo/main/deployment/install.sh | OPENAI_API_KEY='your-openai-api-key' bash\n```\n\nFor more details, see the [Installation Guide](docs/install.md).\n\n## Architecture\n\nRead more about the architecture of Skyflo.ai in the [Architecture](docs/architecture.md) documentation.\n\n## Roadmap\n\nRead more about the [roadmap of Skyflo.ai here](https://skyflo.ai/roadmap).\n\n## Contributing\n\nWe welcome contributions! See our [Contributing Guide](CONTRIBUTING.md) for details on getting started.\n\n## Code of Conduct\n\nWe have a [Code of Conduct](code_of_conduct.md) that we ask all contributors to follow.\n\n## Community\n\n- [Discord](https://discord.gg/kCFNavMund)\n- [Twitter/X](https://x.com/skyflo_ai)\n- [YouTube](https://www.youtube.com/@SkyfloAI)\n- [GitHub Discussions](https://github.com/skyflo-ai/skyflo/discussions)\n"
    },
    {
      "name": "pablosalvador10/gbbai-agent-architecture-lab",
      "stars": 26,
      "img": "https://avatars.githubusercontent.com/u/31255154?s=40&v=4",
      "owner": "pablosalvador10",
      "repo_name": "gbbai-agent-architecture-lab",
      "description": "🚀  Embark on your agentic journey !",
      "homepage": "",
      "language": "Jupyter Notebook",
      "created_at": "2024-11-03T22:31:48Z",
      "updated_at": "2025-04-15T20:54:21Z",
      "topics": [],
      "readme": "<!-- markdownlint-disable MD033 -->\n\n# 🤖  AI Agentic Lab: Build Single and Multi-Agent Systems in Azure\n\nWelcome to the **Agentic System Design Lab**! This is your go-to space for exploring, designing, and implementing agent-based AI systems in Azure. Our mission is to help you understand and build single-agent systems, foundational orchestration frameworks, and advanced multi-agent strategies. We leverage the power of [**Azure Foundry (Azure AI Agents Service)**](https://azure.microsoft.com/en-us/products/ai-foundry/?msockid=0b24a995eaca6e7d3c1dbc1beb7e6fa8#Use-cases-and-Capabilities) and leading AI frameworks like [**Autogen**](https://microsoft.github.io/autogen/0.2/docs/Getting-Started/) and [**Semantic Kernel**](https://learn.microsoft.com/en-us/semantic-kernel/overview/) to provide you with the best resources and tools for building enterprise agentic designs patterns. Our focus is on hands-on learning—empowering you to learn the fundamentals trougth the labs and explore the art of the possible through real-world use cases.\n\n**Let’s build the future of AI, one agent at a time! 🚀**\n\n## What's new ✨\n\n➕ [**Building MaS with Azure AI Agents & Semantic Kernel Agent Framework (Experimental)**](labs/03-building-multi-agent-systems/sk-and-azure-ai-agents.ipynb) lab – Orchestrate single-agent  on Azure AI Agent Services using the experimental Semantic Kernel Agent Framework in Python.  \n➕ [**Building MaS with Azure AI Agents & Autogen v0.4 (New Autogen Architecture)**](labs/03-building-multi-agent-systems/autogen-and-azure-ai-agents.ipynb) lab – Orchestrate single-agent on Azure AI Agent Services with the event-driven Autogen v0.4 architecture.\n\n## Contents\n\n1. [🤖 Building Agentic Systems in Azure](#-building-agentic-systems-in-azure)\n1. [🧪 Labs](#-labs)\n1. [🚀 Uses Cases](#-getting-started)\n1. [📚 More Resources](#-other-resources)\n\n## 🤖 Building Agentic Systems in Azure\n\nIn today's fast-evolving Agentic AI landscape, staying ahead means embracing rapid experimentation. Our approach in ths repo is to keep it simple and to the point, starting with the development of robust, scalable **enterprise single agents** using the Azure AI Agent Service. These production-ready agents come equipped with integrated tools, persistent memory, traceability, and isolated execution—providing a solid foundation before scaling up.\n\nThen, of course, we'll tackle communication patterns between single agents. Just as clear conversation drives human collaboration, real-time event exchange between agents unlocks their full potential as a cohesive system. By leveraging frameworks like **AutoGen** and **Semantic Kernel**—or even crafting your own— you can establish an event-driven architecture that seamlessly ties everything together (single-agents) to build multi-agent systems.\n\n```text\nMulti-Agent Architecture = Σ (Production-Ready Single Agents [tools, memory, traceability, isolation]) + Preferred Framework (e.g., Semantic Kernel, AutoGen)\n```\n\n**Breaking It Down**\n\n- **Step 1:** Build robust, scalable single agents with the **Azure AI Agent Service**, managing them as micro-services.\n- **Step 2:** For complex tasks, deploy a fleet of specialized agents that communicate seamlessly via an event-driven framework of your choice.\n\n## 🧪 Labs\nReady to dive into developing agentic AI systems? Explore our labs to build, refine, and experiment with cutting-edge agent architectures on Azure.\n\n+ 🧪 **Building Single Agents with Azure AI Agent Service**:  \n   - [🧾 Notebook - Building Single Agents with Azure AI Agent Service](labs/01-azure-ai-agents/single-agent-with-azure-ai-agents.ipynb): Master the fundamentals of single-agent architectures and learn how to build managed agent as micro-services.\n- 🧪 **Deep Dive into Frameworks**: Discover Semantic Kernel and AutoGen\n   - [🧾 Notebook - Intro to Semantic Kernel](labs/02-frameworks/intro-to-semantic-kernel.ipynb)  \n   + [🧾 Notebook - Intro to Autogen](labs/02-frameworks/intro-to-autogen-v2.ipynb)\n   - [🧾 Notebook - Upgrade to new Autogen Architecture](labs/02-frameworks/upgrade-to-autogen-new-architecture.ipynb): In January 2025, AutoGen released its new generation v0.4—a significant evolution from version 0.2. For a smooth transition, review Notebook 03 before moving on to Notebook 03b to see why this new design is the event we needed.\n+ 🧪 **Building Multi-Agent Architectures**:\n   - [🧾 Notebook - Intro to Semantick Kernel Agentic Framework](labs/03-building-multi-agent-systems/sk-agent-framework.ipynb): *Caution: The SK Agentic framework is currently in experimental phase.*\n   + [🧾 Notebook - Building MaS with Azure AI Agents & Autogen v0.4 (New Autogen Architecture)](labs/03-building-multi-agent-systems/autogen-and-azure-ai-agents.ipynb)\n   - [🧾 Notebook - Building MaS with Azure AI Agents & Semantic Kernel Agent Framework (Experimental)](labs/03-building-multi-agent-systems/sk-and-azure-ai-agents.ipynb)\n\nFor more details, please visit the Labs [README](labs/README.md).\n\n## 🚀 Uses Cases\n\n**Coming very soon...Stay Tuned**\n\n## 📚 More Resources\n\n- **[Azure AI Foundry](https://azure.microsoft.com/en-us/products/ai-foundry/?msockid=0b24a995eaca6e7d3c1dbc1beb7e6fa8#Use-cases-and-Capabilities)**: Develop and deploy custom AI apps and APIs responsibly with a comprehensive platform.\n- **[Azure AI Agent Service](https://learn.microsoft.com/en-us/azure/ai-services/agents/overview)**: Learn about Azure AI Agent Service and its capabilities.\n- **[AutoGen Documentation](https://microsoft.github.io/autogen/0.2/docs/Getting-Started/)**: Comprehensive guides and API references for AutoGen.\n- **[Semantic Kernel Documentation](https://learn.microsoft.com/en-us/semantic-kernel/overview/)**: Detailed documentation on Semantic Kernel's features and capabilities.\n\n### Disclaimer\n\n> [!IMPORTANT]\n> This software is provided for demonstration purposes only. It is not intended to be relied upon for any purpose. The creators of this software make no representations or warranties of any kind, express or implied, about the completeness, accuracy, reliability, suitability or availability with respect to the software or the information, products, services, or related graphics contained in the software for any purpose. Any reliance you place on such information is therefore strictly at your own risk."
    },
    {
      "name": "Wannabeasmartguy/RAGENT",
      "stars": 25,
      "img": "https://avatars.githubusercontent.com/u/107250451?s=40&v=4",
      "owner": "Wannabeasmartguy",
      "repo_name": "RAGENT",
      "description": "Probably one of the lightest native RAG + Agent apps out there，experience the power of Agent-powered models and Agent-driven knowledge bases in one click, without complex configuration.",
      "homepage": "",
      "language": "Python",
      "created_at": "2024-04-21T06:42:24Z",
      "updated_at": "2025-04-18T18:33:26Z",
      "topics": [
        "agent",
        "autogen",
        "azure",
        "groq",
        "langchian",
        "llamafile",
        "llm",
        "local-development",
        "ollama",
        "openai",
        "rag"
      ],
      "readme": "# RAGENT\n\n**English** | [中文文档](./docs/README_zh.md) | [日本語](./docs/README_ja.md)\n\nProbably one of the lightest native RAG + Agent apps out there，experience the power of Agent-powered models and Agent-driven knowledge bases in one click, without complex configuration.\n\n![image](https://github.com/user-attachments/assets/bcc6395a-92ab-4ae6-8d36-a6831c240b16)\n\n## Features\n\nChat and Agent interactions:\n- [x] 💭 Simple, easy-to-use chat box interface.\n- [x] 🌏️ Language options (Simplified Chinese, English)\n- [x] 🔧 Inference support for multiple (local) model sources (Azure OpenAI, Groq, ollama, llamafile)\n- [x] Native Function Call (OpenAI, Azure OpenAI, OpenAI Like, Ollama)\n- [x] 🤖 Multiple Agent modes on-premises\n- [x] 🖥️ Local storage of dialog data and management\n  - [x] Multiple export formats(Markdown, HTML)\n  - [x] Multiple themes(HTML)\n\nKnowledgebase:\n- [x] **Native implementation** of Retrieval Augmentation Generation (RAG), lightweight and efficient\n- [x] Optional embedding models (Hugging Face/OpenAI)\n- [x] Easy-to-use knowledge base management\n- [x] Multiple search methods available: Hybrid search, reranking, and specified file retrieval\n\n> If you like this project, please star it, it's the biggest encouragement for me!\n\n## More details\n\n### General\n\n#### Export\n\nSupport export format, theme selection and export range control:\n\n<img src=\"https://github.com/user-attachments/assets/85756a3c-7ca2-4fcf-becc-682f22091c4e\" alt=\"Export settings and preview\" style=\"zoom:40%;\" />\n\nCurrently supported themes: \n\n| Default | Glassmorphism |\n| :-----: | :-----------: |\n| <img src=\"https://github.com/user-attachments/assets/6ac8132c-0821-4487-9a1a-a0297a35783a\" alt=\"default theme\" width=\"300\" /> | <img src=\"https://github.com/user-attachments/assets/87b07e86-dd98-4e66-a850-17b776fbeb1c\" alt=\"Glassmorphism theme\" width=\"300\" /> |\n\n### RAG Chat\n\n<img src=\"https://github.com/user-attachments/assets/bc574d1e-e614-4310-ad00-746c5646963a\" alt=\"image\" style=\"zoom:50%;\" />\n\nYou can set up the model (sidebar) and view detailed references:\n\n<img src=\"https://github.com/user-attachments/assets/a6ce3f0b-3c8f-4e3d-8d34-bceb834da81e\" alt=\"image\" style=\"zoom:50%;\" />\n\nConfigure RAG：\n\n<img src=\"https://github.com/user-attachments/assets/82480174-bac1-47d4-b5f4-9725774618f2\" alt=\"image\" style=\"zoom:50%;\" />\n\n### Function Call\n\n> Function calls are supported on `Chat` now, and `AgentChat` will be supported in the future.\n\nThe Function Calls on this page are native and work for all OpenAI Compatible models, but require the model itself to support Function calls.\n\nFunction Call can significantly enhance the capabilities of LLM, allowing it to complete tasks that it was previously unable to complete (such as mathematical calculations), as shown below:\n\n<img src=\"https://github.com/user-attachments/assets/fba30f4a-dbfc-47d0-9f1c-4443171fa018\" alt=\"image\" style=\"zoom:50%;\" />\n\nOr summarize the content of a webpage:\n\n<img src=\"https://github.com/user-attachments/assets/7da5ae4d-40d5-49b4-9e76-6ce2a39ac6d1\" alt=\"image\" style=\"zoom:50%;\" />\n\n> You can also customize the function you want to call, please refer to [toolkits.py](tools/toolkits.py) for writing rules.\n\n## Quick start\n\n### Git\n\n0. Use `git clone https://github.com/Wannabeasmartguy/RAGENT.git` to pull the code;\nThen open your runtime environment in **command prompt (CMD)** and use `pip install -r requirements.txt` to install the runtime dependencies.\n\n1. Configure the model dependencies: Modify the `.env_sample` file to `.env` and fill in the following:\n\n    - `LANGUAGE`: Support `English` and `简体中文`, if not set, default is `English`;\n    - `OPENAI_API_KEY` : If you are using an OpenAI model, fill in the api key here;\n    - `AZURE_OAI_KEY` : If you are using an Azure OpenAI model, fill in the api key here;\n    - `AZURE_OAI_ENDPOINT` : If you are using an OpenAI model, fill in the end_point here;\n    - `API_VERSION`: If you are using an Azure OpenAI model, fill in the api version here;\n    - `API_TYPE`: if you are using an Azure OpenAI model, fill in the api type here;\n    - `GROQ_API_KEY` : if you are using Groq as the model source, fill in the api key here;\n    - `COZE_ACCESS_TOKEN`: if you need to use the created Coze Bot, fill in the access token here;\n\n> If you are using Llamafile, please set the endpoint within the application after starting the Llamafile model.\n\n2. launch the application:\n\nRun: Run `streamlit run RAGENT.py` on the command line can start it.\n\nIf you want to use the AgentChat page, please use `python startup.py` to start the application rather than `streamlit run RAGENT.py`.\n\n## Contribution\n\nFor any issues encountered during use or new ideas, please submit issues and PRs!\n"
    },
    {
      "name": "EnkrateiaLucca/oreilly_live_training_autogen",
      "stars": 25,
      "img": "https://avatars.githubusercontent.com/u/32417736?s=40&v=4",
      "owner": "EnkrateiaLucca",
      "repo_name": "oreilly_live_training_autogen",
      "description": "Repo for my live-training about autogen",
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2023-10-30T13:59:53Z",
      "updated_at": "2025-01-15T20:09:32Z",
      "topics": [],
      "readme": "# O'Reilly Live Trainining - Getting Started with AutoGen\n\n## Setup\n\n**Conda**\n\n- Install [anaconda](https://www.anaconda.com/download)\n- This repo was tested on a Mac with python=3.11\n- Create an environment: `conda create -n oreilly-autogen python=3.11`\n- Activate your environment with: `conda activate oreilly-autogen`\n- Install requirements with: `pip install -r requirements.txt`\n- Setup your openai [API key](https://platform.openai.com/)\n\n**Pip**\n\n\n1. **Create a Virtual Environment:**\n    Navigate to your project directory. Make sure you hvae python3.11 installed! \n    If using Python 3's built-in `venv`:\n    ```bash\n    python -m venv oreilly_env\n    ```\n    If you're using `virtualenv`:\n    ```bash\n    virtualenv oreilly-autogen\n    ```\n\n2. **Activate the Virtual Environment:**\n    - **On Windows:**\n      ```bash\n      .\\oreilly-autogen\\Scripts\\activate\n      ```\n    - **On macOS and Linux:**\n      ```bash\n      source oreilly-autogen/bin/activate\n      ```\n\n3. **Install Dependencies from `requirements.txt`:**\n    ```bash\n    pip install python-dotenv\n    pip install -r requirements.txt\n    ```\n\n4. Setup your openai [API key](https://platform.openai.com/)\n\nRemember to deactivate the virtual environment once you're done by simply typing:\n```bash\ndeactivate\n```\n\n## Setup your .env file\n\n- Change the `.env.example` file to `.env` and add your OpenAI API key.\n\n## To use this Environment with Jupyter Notebooks:\n\n- ```pip install jupyter```\n- ```python3 -m ipykernel install --user --name=oreilly-autogen```\n\n## Notebooks\n\nHere are the notebooks available in the `notebooks/` folder:\n\n1. [Autogen Demo: Building Our First Agent](notebooks/1.0-ls-autogen-demo-building-our-first-agent.ipynb)\n\n   [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/EnkrateiaLucca/oreilly_live_training_autogen/blob/main/notebooks/1.0-ls-autogen-demo-building-our-first-agent.ipynb)\n\n2. [Autogen Demo: Student Assistant Expert](notebooks/2.0-ls-autogen-demo-student-assistant-expert.ipynb)\n\n   [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/EnkrateiaLucca/oreilly_live_training_autogen/blob/main/notebooks/2.0-ls-autogen-demo-student-assistant-expert.ipynb)\n\n3. [Autogen Demo: Retrieval Augmented Chat](notebooks/3.0-ls-autogen-demo-retrieval-augmented-chat.ipynb)\n\n   [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/EnkrateiaLucca/oreilly_live_training_autogen/blob/main/notebooks/3.0-ls-autogen-demo-retrieval-augmented-chat.ipynb)\n\n4. [Autogen Demo: Writer Commander Safeguard Multi-Agent Coding](notebooks/4.0-ls-autogen-demo-writer-commander-safeguard-multi-agent-coding.ipynb)\n\n   [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/EnkrateiaLucca/oreilly_live_training_autogen/blob/main/notebooks/4.0-ls-autogen-demo-writer-commander-safeguard-multi-agent-coding.ipynb)\n\n5. [Autogen Demo: Dynamic Group Chat](notebooks/5.0-ls-autogen-demo-dynamic-group-chat.ipynb)\n\n   [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/EnkrateiaLucca/oreilly_live_training_autogen/blob/main/notebooks/5.0-ls-autogen-demo-dynamic-group-chat.ipynb)\n\n6. [Autogen Demo: Agent Builder Recipe](notebooks/6.0-ls-autogen-demo-agent-builder-recipe.ipynb)\n\n   [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/EnkrateiaLucca/oreilly_live_training_autogen/blob/main/notebooks/6.0-ls-autogen-demo-agent-builder-recipe.ipynb)\n\n7. [Autogen Demo: Research Assistant](notebooks/7.0-ls-autogen-demo-research-assistant.ipynb)\n\n   [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/EnkrateiaLucca/oreilly_live_training_autogen/blob/main/notebooks/7.0-ls-autogen-demo-research-assistant.ipynb)\n\n8. [Autogen Demo: Personal Assistant](notebooks/8.0-ls-autogen-demo-personal-assistant.ipynb)\n\n   [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/EnkrateiaLucca/oreilly_live_training_autogen/blob/main/notebooks/8.0-ls-autogen-demo-personal-assistant.ipynb)\n\n9. [Building a Autogen Agent: Desktop Tasks](notebooks/building-a-autogen-agent-desktop-tasks.ipynb)\n\n   [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/EnkrateiaLucca/oreilly_live_training_autogen/blob/main/notebooks/building-a-autogen-agent-desktop-tasks.ipynb)\n\n10. [Demo: Autogen Function Calling](notebooks/demo-autogen-function-calling.ipynb)\n\n    [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/EnkrateiaLucca/oreilly_live_training_autogen/blob/main/notebooks/demo-autogen-function-calling.ipynb)\n"
    },
    {
      "name": "IBM/watsonx-developer-hub",
      "stars": 24,
      "img": "https://avatars.githubusercontent.com/u/1459110?s=40&v=4",
      "owner": "IBM",
      "repo_name": "watsonx-developer-hub",
      "description": "Examples and guides for building Gen AI applications on the watsonx platform.",
      "homepage": "https://www.ibm.com/watsonx/developer/",
      "language": "Python",
      "created_at": "2025-02-05T11:10:32Z",
      "updated_at": "2025-04-21T14:55:39Z",
      "topics": [
        "agents",
        "ai",
        "autogen",
        "beeai",
        "crewai",
        "genai",
        "langchain",
        "langgraph",
        "llama",
        "llamaindex",
        "llm"
      ],
      "readme": "# watsonx Developer Hub\n\nExamples and guides for building and deploying Gen AI applications using the watsonx APIs.\n\n- [Agents](/agents/README.md)\n\n## Getting started\n\nTo begin using the watsonx APIs, you will need 3 values:\n\n1. A project or space ID\n2. An endpoint for your region e.g. `https://us-south.ml.cloud.ibm.com`\n3. IBM Cloud API key\n\nYou can visit [Developer Access](https://dataplatform.cloud.ibm.com/developer-access?context=wx) to receive these values. Make sure to follow the link on [Developer Access](https://dataplatform.cloud.ibm.com/developer-access?context=wx) to get your IBM Cloud API key.\n\n## Support\n\nSee the [watsonx Developer Hub](https://ibm.com/watsonx/developer) for quickstarts and documentation. Please reach out to us on [Discord](https://ibm.biz/wx-discord) if you have any questions or want to share feedback. We'd love to hear from you!"
    },
    {
      "name": "Azure-Samples/azure-postgresql-openai-langchain-autogen-demo",
      "stars": 24,
      "img": "https://avatars.githubusercontent.com/u/1844662?s=40&v=4",
      "owner": "Azure-Samples",
      "repo_name": "azure-postgresql-openai-langchain-autogen-demo",
      "description": "Multi-Agent AI System with LangChain, AutoGen, Azure OpenAI GPT-4, and Azure PostgreSQL",
      "homepage": "",
      "language": "Jupyter Notebook",
      "created_at": "2024-11-20T21:47:46Z",
      "updated_at": "2025-04-22T17:04:55Z",
      "topics": [],
      "readme": "<div align=\"center\" style=\"background-color: #800080; padding: 20px; border-radius: 10px; color: white; font-size: 20px; font-weight: bold;\">\n  💜 IMPORTANT: This is the <u>latest and recommended</u> repository for using <i>PostgreSQL with AutoGen 0.4</i>.<br><br>\n  👉 <a href=\"https://github.com/Azure-Samples/MultiAgent_Azure_PostgreSQL_AutoGen0.4.git\" style=\"color: #FFD700; text-decoration: underline;\">Click here to explore the repo</a>\n</div>\n\n# **Multi-Agent AI System with LangChain, AutoGen, Azure OpenAI GPT-4, and Azure PostgreSQL**\n\n<div align=\"center\">\n  <img src=\"https://github.com/user-attachments/assets/26489916-3af8-4371-a035-9cbdb4db0c61\" alt=\"Architecture\">\n</div>\n\nThis repository demonstrates how to build a **multi-agent AI system** using:\n- **LangChain** for natural language to SQL translation.\n- **AutoGen** for coordinating AI agents in collaborative workflows.\n- **Azure OpenAI GPT-4** for intelligent language understanding and generation of SQL queries in PostgreSQL.\n- **Azure Database for PostgreSQL** for data storage and querying.\n\nThe application showcases a shipping company where agents manage shipments, customers and product informations. The main goal of this repository is to illustrate how easy it is to have agents not just reading data but also acting on them. It extends the \"Chat With Your Data\" to \"Chat and Act on Your Data\". ** We welcome contributions to help make those agents more reliable and under guardrails. Feel free to contribute to more agents as well! **\n\n## **Features**\n\n- 🌐 **Gradio UI**: User-friendly interface for natural language interactions.\n- 🤖 **AutoGen Multi-Agent System**: Agents collaborate to handle specific tasks:\n  - **SchemaAgent**: Manages database schema retrieval and sharing.\n  - **ShipmentAgent**: Handles shipment-related queries and updates. It can use the stored procedure *send_shipment* to create shipments.\n  - **CRMAgent**: Manages customer and product-related data. It can use the stored procedure *add_customer* to create new customers.\n- 🧠 **Azure OpenAI GPT-4**: Generates SQL queries and natural language responses.\n- 🛢️ **Azure PostgreSQL**: Stores shipment, customer, and product data.\n\n## **Getting Started**\n\n### **1. Prerequisites**\n\n- Python 3.9+ and <3.13\n- An Azure account with:\n  - **Azure OpenAI Service** (GPT-4 deployed). **Note: I am looking to see how we can use other GPT models but they keep adding ``` sql  which returns an error**\n  - **Azure Database for PostgreSQL** (configured with necessary tables).\n- Environment setup:\n  - `python-dotenv` for environment variables.\n  - PostgreSQL client library (`psycopg2` or similar).\n     \n---\n\n### **2. Setup Instructions**\n\n#### **Clone the Repository**\n\n```bash\ngit clone https://github.com/Azure-Samples/azure-postgresql-openai-langchain-autogen-demo.git\n```\n```bash\ncd azure-postgresql-openai-langchain-autogen-demo\n```\n\n#### **Create Tables, Add Data and Create Stored Procedures**\n- Use the file **SQL_Queries.sql** to set your database.\n\n#### **Configure .env File**\n\nCreate a **.env** file in the root directory to store sensitive credentials. Use the following template:\n\n```ini\n# Azure OpenAI\nAZURE_OPENAI_KEY=your_openai_api_key\nAZURE_OPENAI_ENDPOINT=https://your-openai-endpoint\nAZURE_OPENAI_DEPLOYMENT=gpt-4\n\n# PostgreSQL Database\nPOSTGRES_USER=your_username\nPOSTGRES_PASSWORD=your_password\nPOSTGRES_HOST=your_postgresql_host\nPOSTGRES_PORT=5432\nPOSTGRES_DB=your_database_name\n```\n\nReplace the placeholder values with your actual credentials. The Jupyter Notebook is configured with .env been located in the same root folder in my machine. \n\nIf you use Google Collab and you want to upload .env file, you will have to add the following code:\n\n```python\nfrom google.colab import files\nfiles.upload()  # Upload your .env file\n```\n\n#### Install libraries\n- I recommend using Jupyter Notebook in VS Code\n- You should create a virtual environment to avoid any library conflicts:\n  1. Go to your root folder where you have cloned the files\n  2. Create a virtual environment named `.venv` in your current directory:\n     ```python\n     python3 -m venv .venv\n     ```\n  3. Activate:\n\n     a. **On Windows:**\n        ```bash\n        .venv\\Scripts\\activate\n        ```\n\n     b. **On Mac/Linux:**\n        ```bash\n        source .venv/bin/activate\n        ```\n  4. Install the Requirement File:\n     ```python\n     pip install -r requirements.txt\n     ```\n\n#### **Usage - example of questions that you can ask:**\n\n##### **Chat with your Data Examples**:\n- Which products with names are currently tracking in transit?\n- Is Alice Johnson a Customer?\n\n##### **Multi-agents to help develop on the database example question**:\n- I would like to create a stored Procedure to delete customers. What would be the best way to do this?\n\n##### **Act on Your Data example**:\n- Can you add Marc with email address marcr@contoso.com, phone number +1 123 456 7890 and address in 1 Main Street, Redmond?\n- Can you add Marc with email address marcre@contoso.com, phone number +1 123 456 7890? **Note: the information is incomplete and the agents should not perfom an operation**\n- Can you create a new shipment of 1 Laptop and 1 Smartphone to Marc and ensure shipment is updated to Departed Origin from the location in New York and towards Los Angeles date is today?\n"
    },
    {
      "name": "aevatarAI/aevatar-station",
      "stars": 23,
      "img": "https://avatars.githubusercontent.com/u/190360083?s=40&v=4",
      "owner": "aevatarAI",
      "repo_name": "aevatar-station",
      "description": "Your all-in-one platform for creating, managing, and deploying AI agents.",
      "homepage": "",
      "language": "C#",
      "created_at": "2025-01-07T06:15:34Z",
      "updated_at": "2025-04-11T03:32:47Z",
      "topics": [],
      "readme": "# *Aevatar Station*\n\n*Your all-in-one platform for creating, managing, and deploying AI agents.*\n\n---\n## 🚀 **Introduction**\n\n**Aevatar Station** is a cutting-edge developer platform designed to simplify the creation, management, and deployment of intelligent AI agents. With a focus on flexibility, scalability, and ease of use, Aevatar Station empowers developers and organizations to harness the power of AI in a streamlined and efficient way.\n\n## Getting Started\n\n### Prerequisites\n\n- .NET 8.0 SDK\n- MongoDB\n- Elasticsearch\n- Redis\n\n## Configuration\n\n1. Update the `appsettings.json` file in the Silo project with your specific configurations (e.g., connection strings, Orleans clustering configurations).\n\n    ```json\n    {\n      \"ConnectionStrings\": {\n        \"Default\": \"mongodb://localhost:27017/Aevatar\"\n      },\n      \"Orleans\": {\n        \"ClusterId\": \"AevatarSiloCluster\",\n        \"ServiceId\": \"AevatarBasicService\",\n        \"AdvertisedIP\": \"127.0.0.1\",\n        \"GatewayPort\": 20001,\n        \"SiloPort\": 10001,\n        \"MongoDBClient\": \"mongodb://localhost:27017/?maxPoolSize=555\",\n        \"DataBase\": \"AevatarDb\",\n        \"DashboardUserName\": \"admin\",\n        \"DashboardPassword\": \"123456\",\n        \"DashboardCounterUpdateIntervalMs\": 1000,\n        \"DashboardPort\": 8080,\n        \"EventStoreConnection\": \"ConnectTo=tcp://localhost:1113; HeartBeatTimeout=500\",\n        \"ClusterDbConnection\": \"127.0.0.1:6379\",\n        \"ClusterDbNumber\": 0,\n        \"GrainStorageDbConnection\": \"127.0.0.1:6379\",\n        \"GrainStorageDbNumber\": 0\n      }\n    }\n    ```\n\n2. Update the `appsettings.json` file in the HttpApi.Host project with your specific configurations (e.g., connection strings, Orleans clustering configurations).\n\n    ```json\n    {\n      \"ConnectionStrings\": {\n        \"Default\": \"mongodb://localhost:27017/Aevatar\"\n      },\n      \"Orleans\": {\n        \"ClusterId\": \"AevatarSiloCluster\",\n        \"ServiceId\": \"AevatarBasicService\",\n        \"MongoDBClient\": \"mongodb://localhost:27017/?maxPoolSize=555\",\n        \"DataBase\": \"AevatarDb\"\n      }\n    }\n    ```\n\n### Running the Application\n\n1. Go to the `src` folder\n    ```shell\n    cd src\n    ```\n2. Run the `Aevatar.DbMigrator` project to create the initial database from `src`.\n    ```shell\n    cd Aevatar.DbMigrator\n    dotnet run\n    ```\n3. Run the `Aevatar.AuthServer` project to create the initial database from `src`.\n    ```shell\n    cd Aevatar.AuthServer\n    dotnet run\n    ```\n4. Run the `Aevatar.Silo` project to start the Orleans Silo from `src`.\n    ```shell\n    cd ../Aevatar.Silo\n    dotnet run\n    ```\n5. Run the `Aevatar.HttpApi.Host` project to start the API from `src`.\n    ```shell\n    cd ../Aevatar.HttpApi.Host\n    dotnet run\n    ```\n## Contributing\n\nIf you encounter a bug or have a feature request, please use the [Issue Tracker](https://github.com/AISmartProject/aevatar-station/issues/new). The project is also open to contributions, so feel free to fork the project and open pull requests.\n\n## License\n\nDistributed under the MIT License. See [License](LICENSE) for more information."
    },
    {
      "name": "denniszielke/agentic-playground",
      "stars": 23,
      "img": "https://avatars.githubusercontent.com/u/11569044?s=40&v=4",
      "owner": "denniszielke",
      "repo_name": "agentic-playground",
      "description": "This is an AI agent playground to demonstrate different agent orchestration patterns and capabilities",
      "homepage": "",
      "language": "Python",
      "created_at": "2025-03-07T19:17:02Z",
      "updated_at": "2025-04-04T10:24:40Z",
      "topics": [
        "autogen",
        "genai",
        "github",
        "langchain",
        "llama-index",
        "semantic-kernel"
      ],
      "readme": "# AI Agent playground\n\nThis project demonstrates agentic concepts, orchestration patterns and functional scenarios using GitHub models and various open souce frameworks like Llama Index, Semantic Kernel, LangChain and AutoGen.\n\nThe objective is to try out and learn about the following capabilities of intelligent models:\n- Leveraging Tools for model interaction with external systems\n- Using visual model to ingest and reason about images using a multimodal model\n- Giving up control over the conversation flow using a realtime voice model\n- Handing over structured data types like Knowledge Graphs and Onthologies to models \n\nThat gives you the chance to learn how to combine these capabilities by orchestrating them:\n- Implementing a ReAct pattern with a single agent to plan and execute iterative tasks\n- Forcing the model to use and think in Domain Specific Languages to interact with existing code\n- Combining different agent types to solve complex problems through defined interaction patterns\n- Tasking a reasoning model to solve complex problems on its own without orchestration\n\nSince there are multiple agentic frameworks and hosting runtimes these will let you see how different collaboration patterns can be implemented:\n- Planned agent interactions can be implemented with any agent framework\n- Graph based interactions between agents can be implemented with LangGraph\n- Dynamic agent interaction can be implemented with MagenticOne/AutoGen\n- Event driven agent interaction can be implemented with Llama Index or Semantic Kernel\n- Distributed agent platforms can be built with Llama deploy\n\n## What is an Agent?\n\n> ***agent***: \tperceives its environment, makes decisions, takes actions autonomously in order to achieve goals, and may improve its performance with learning or acquiring knowledge \n\n![What is an agent](/img/agents.png)\n\nA simple LLM-based chatbot primarily focuses on generating responses based on predefined patterns and language models, often requiring user input to continue the conversation. In contrast, autonomous agents are designed to perform tasks independently, making decisions and taking actions without constant user input, often leveraging advanced AI techniques to achieve their goals. \n\n![Spectrum of agentic behaviour](/img/spectrum.png)\n\n## Preparation\n\nThis project does not require azure resources and support GitHub AI models.\n\n1. Create a personal access token\n\nTo authenticate with the model you will need to generate a personal access token (PAT) in your GitHub settings or set up an Azure production key. [GitHub Free AI Token](https://github.com/settings/tokens)\n\nYou can now access AI inference with your GitHub PAT. [Learn more about limits based on your plan](https://github.com/marketplace/models/azure-openai/gpt-4o-mini/playground#:~:text=Learn%20more%20about%20limits%20based%20on%20your%20plan.). You do not need to give any permissions to the token. \n\nTo use the code snippets below, create an environment variable to set your token as the key for the client code.\n\nIf you're using bash:\n```\nexport GITHUB_TOKEN=\"<your-github-token-goes-here>\"\n```\n\nor rename the file `.env_template` to `.env` and put the value inside the `.env` file. Each python script will load the value from that value automatically.\n\n2. Install dependencies (should be already done when using a GitHub codespace)\n\n```\npython -m pip -r requirements.txt\n```\n\n## Workshop contents\n\nThe scope of this workshop covers the following scenarios and technology stacks:\n\n| Name | Description | Technology  |\n| :-- | :--| :-- |\n| [Hello World](./src/01-basic/README.md) | Hello World model | OpenAI |\n| [Multimodal models](./src/02-multimodal-models/README.md) | Multimodel Prompting | OpenAI, Vision model, Realtime model |\n| [Knowledge Graphs](./src/03-complex-data/README.md) | Knowledge graph generation | OpenAI, Structured output |\n| [Onthologies](./src/03-complex-data/README.md) | Onthologies | OpenAI, OWL |\n| [Custom DSL](./src/04-complex-problems/README.md) | Domain specific languages | OpenAI, Tools |\n| [Single Agent](./src/05-search-agent/README.md) | ReAct Agent | OpenAI, LangChain, Llama Index, Tools |\n| [Human in the loop](./src/06-human-in-the-loop/README.md) | Human in the loop agents | OpenAI, LangChain, LangGraph, Semantic Kernel Tools |\n| [Multi agent collaboration](./src/07-multi-agent-collaboration/README.mdy) | Multi turn agent collaboration| OpenAI, Semantic Kernel, LangGraph |\n| [Society of agents](./src/08-society-of-agents/README.md) | Dynamic planning | OpenAI, AutoGen, MagenticOne |\n| [Event Driven Agents](./src/09-eventdriven-agents/README.md) | Distributed agents agent platforms| OpenAI, Semantic Kernel, Llama deploy |\n"
    },
    {
      "name": "Azure/agent-innovator-lab",
      "stars": 23,
      "img": "https://avatars.githubusercontent.com/u/6844498?s=40&v=4",
      "owner": "Azure",
      "repo_name": "agent-innovator-lab",
      "description": "The Agent Innovator Lab offers a hands-on learning experience in AI agent development using Microsoft Azure’s core services. Participants explore topics like search optimization, agent design, and evaluation, with practical tools and RAG best practices for advancing AI system deployment.",
      "homepage": "",
      "language": "Jupyter Notebook",
      "created_at": "2025-01-02T08:11:52Z",
      "updated_at": "2025-04-23T01:11:58Z",
      "topics": [],
      "readme": "# Agent Innovator Lab\n\nThe Agent Innovator Lab is designed to provide a structured learning experience for AI agent development by leveraging Microsoft Azure's core services (Data & AI, App, and Infra). Each lab focuses on a specific topic, covering areas such as search algorithm optimization, agentic design patterns, and evaluation frameworks. Through this hands-on workshop, participants will gain practical experience in building, optimizing, and evaluating Azure-based AI agents, ultimately driving innovation and enhancing real-world AI system deployment.\nThis repository includes RAG best practices, along with tools and techniques for innovating current architecture. \n\n\nThis hands-on lab is suitable for the following purposes:\n\n1. 1-day workshop (4-7 hours depending on customer)\n2. Hackathon starter code\n3. Reference guide for Evaluation-driven Multi-Agent design patterns\n\n\n[**Hands-on guide**](https://azure.github.io/agent-innovator-lab/) | [**Requirements**](#requirements) | [**Get started**](#get-started) \n\n----------------------------------------------------------------------------------------\n\n## List of workshops\n\nProvided below is a list of currently published modules:\n\n| Title  | Description and Link  |\n|-------|-----|\n| Lab 0. Basic RAG | [Create RAG application with Azure AI Search](0_basic-rag)  |\n| Lab 0. Basic Agent | [Basic Concepts of Agent and Agent toolkits (AutoGen and LangGraph)](0_basic-agent) |\n| Lab 1. Agentic Design Pattern | [Practice representative patterns of Agentic RAG](1_agentic-design-ptn) |\n| Lab 2. Evaluation Design Pattern | [Practice the Evaluation-Driven RAG patterns](2_eval-design-ptn) |\n| Lab 3. Optimization Design Pattern | [(In development) Practice the Optimization Design patterns](3_optimization-design-ptn)   |\n| Lab Intermission. Agentic Workflow Design Lab | [Design Agentic Workflow before each hands-on session ](lab_intermission) |\n \n\n## Requirements\nBefore starting, you should meet the following requirements:\n\n- [Access to Azure OpenAI Service](https://go.microsoft.com/fwlink/?linkid=2222006)\n- [Azure AI Foundry getting started](https://int.ai.azure.com/explore/gettingstarted): Need to create a project\n- [Access to Azure AI Search](https://learn.microsoft.com/en-us/azure/search/search-what-is-azure-search)\n- [Access to Azure Bing Search](https://learn.microsoft.com/en-us/bing/search-apis/bing-web-search/create-bing-search-service-resource)\n\n- ***[Evaluation driven design pattern]*** Need to grant ***Storage Blob Data Contributor*** at the storage of AI Foundry role to user, group, service principle and managed Identity which you are trying to access the data executing evaluators in cloud.\n\n- ***[Evaluation driven design pattern - custom evaluator]*** Need to access ***Azure ML*** and ***Storage Account*** to upload your custom evaluators and data.\n\n- In order to run ***azure.ai.evaluation.evaluate***, ***Azure CLI*** installed and you are logged into your Azure account by running ***az login***.\n\n\n**Please do not forget to modify the `.env` file to match your account. Rename `sample.env` to `.env` or copy and use it**\n\n## Get started\n\n### If you are using your own local \n```shell\ngit clone https://github.com/Azure/agent-innovator-lab.git\ncd agent-innovator-lab \npip install -r requirements.txt\n```\n\n### If you are using Azure ML Compute Instance\n- First you need to create and set up a compute instance on AI Foundry, then access the compute instance on Azure ML \n```shell\ngit clone https://github.com/Azure/agent-innovator-lab.git\ncd agent-innovator-lab && conda activate azureml_py310_sdkv2\npip install -r requirements.txt\n```\n\n## 🔥 New Feature (10-Apr-2025)\n\n### Prompt Optimization using PromptWizard<br>\n- This hands-on demonstrates how to optimize prompts using PromptWizard. PromptWizard, released as open source and paper by Microsoft, is a prompt optimization tool for maximizing the performance of LLM. It is a prompt optimization framework that employs a self-evolving mechanism in which LLM generates, critiques, refines, and continuously improves prompts and examples through feedback and synthesis\n- <a href=\"https://github.com/Azure/agent-innovator-lab/tree/main/3_optimization-design-ptn/03_prompt-optimization\">Go to notebook</a>\n\n### Semantic Kernel hands-on lab<br>\n- This hands-on demonstrates how to use Semantic Kernel (SK) to build a simple agentic application.\n- The lab covers the following topics:\n    - Core concepts and architecture of Semantic Kernel (SK)\n- Agent implementation examples:\n    - Azure AI Agent, Azure Assistant Agent, Chat Completion Agent, Azure Responses Agent\n- Tool and connector integration:\n    - Azure AI Search, File Tool, Bing Search API via Bing Search Connector, Bing Grounding Tool, Azure Evaluation SDK \n- How to build a simple agentic application using SK, leveraging various agentic design patterns (e.g. Corrective / Adaptive RAG)\n- You can also leverage existing evaluation design patterns and optimization design patterns to evaluate and optimize your Semantic Kernel application.\n  <a href=\"https://github.com/Azure/agent-innovator-lab/blob/main/0_basic-agent/SK/1_basic-concept-with-sk.ipynb\">Go to notebook</a>\n\n\n## Trouble Shooting\n\n### EvaluationException: (UserError) Failed to connect to your Azure AI project. Please check if the project scope is configured correctly, and make sure you have the necessary access permissions. Status code: 401.\n- run `az login`. To set up your tenant scope, try the option `--scope https://graph.microsoft.com//.default`\n- For example, `az login --scope https://graph.microsoft.com//.default`\n\n### azure_ai_project evaluationexception: (internalerror) azure ml managed identity configuration not found in environment. invalid_scope\n- Check the managed identity role that your compute instance has. You can create a **compute instance on Azure AI Foundry (not on Azure ML)** to avoid the complex identity setup. \n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n"
    },
    {
      "name": "cognitivecomputations/agenticworker",
      "stars": 23,
      "img": "https://avatars.githubusercontent.com/u/155425657?s=40&v=4",
      "owner": "cognitivecomputations",
      "repo_name": "agenticworker",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2024-04-15T00:04:39Z",
      "updated_at": "2024-11-19T01:46:55Z",
      "topics": [],
      "readme": "# Agentic Worker\n\nGather data for agentic dataset, from various frameworks.\n- AutoGen\n- CrewAI\n- MemGPT\n- LangChain Agents\n- princeton-nlp/SWE-agent\n- Devika\n- Open-Devin\n- llama-agentic-system\n\n## Start the logging proxy\n\n`python proxy.py`\n\n## run a scenario\n\n`python worker.py autogen scenario1`\n"
    },
    {
      "name": "zhattention/autogen-toolsmith",
      "stars": 22,
      "img": "https://avatars.githubusercontent.com/u/7688914?s=40&v=4",
      "owner": "zhattention",
      "repo_name": "autogen-toolsmith",
      "description": "AutoGen Toolsmith simplifies the process of creating and managing tools that can be used by agents in the AutoGen framework.",
      "homepage": null,
      "language": "Python",
      "created_at": "2025-02-25T09:22:49Z",
      "updated_at": "2025-03-13T02:16:03Z",
      "topics": [],
      "readme": "# AutoGen Toolsmith\n\nA Python library for automatically generating, testing, and managing tools for AI agents.\n\n![License](https://img.shields.io/badge/license-MIT-blue)\n![Python](https://img.shields.io/badge/python-3.8%2B-blue)\n![AutoGen](https://img.shields.io/badge/AutoGen-0.4.0%2B-green)\n\n## 🚀 Overview\n\nAutoGen Toolsmith is a powerful framework that dramatically improves the efficiency of creating tools for AI agents. It serves as:\n\n1. **A Tool Creation Accelerator**: Generate fully functional tools from natural language descriptions in minutes\n2. **A Foundation for Agent Capabilities**: Build the essential building blocks that determine what agents can do\n3. **A Framework-Agnostic Solution**: Easily integrate with AutoGen, LangChain, CrewAI and other agent frameworks\n4. **An MCP Protocol Compatible System**: Support standardized communication in multi-agent systems\n5. **A Curated Tool Repository**: Access pre-built tools for common tasks like web search, web scraping, and document processing\n\n## 🔧 Installation\n\n```bash\npip install autogen-toolsmith\n```\n\n## 🏁 Quick Start\n\n```python\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\nfrom autogen_toolsmith import ToolGenerator\n\n# Initialize the model client\nmodel_client = OpenAIChatCompletionClient(\n    model=\"gpt-4\",\n    api_key=\"your_openai_api_key\"\n)\n\n# Initialize the tool generator\ngenerator = ToolGenerator(model_client=model_client)\n\n# Generate a new tool from a description\ntool_spec = \"\"\"\nCreate a tool that can fetch weather data for a given location.\nThe tool should take a city name and return current temperature and conditions.\n\"\"\"\n\n# Generate, test and register the tool\ntool_path = generator.create_tool(tool_spec)\n\n# The tool is now available in your tool registry\nfrom autogen_toolsmith.tools import get_tool\n\nweather_tool = get_tool(\"weather_fetcher\")\nresult = weather_tool.run(\"New York\")\nprint(result)\n\n# Or save the tool to your own project directory\nfrom pathlib import Path\nmy_project_dir = Path.cwd() / \"my_project\"\ncustom_tool_path = generator.create_tool(\n    specification=tool_spec,\n    output_dir=my_project_dir,\n    register=False  # Optional: don't register in global registry\n)\n```\n\n## ✨ Key Features\n\n### 🛠️ Efficient Tool Creation\n\n- Generate tools from natural language descriptions\n- Automatic test generation and validation\n- Built-in security checks\n- Comprehensive documentation generation\n- Save tools to your own project directory\n\n### 🔄 Tool Dependency Management\n\n- Create tools that leverage existing tools\n- Automatic dependency tracking and validation\n- Build complex tool chains with simple descriptions\n\n### 🔌 Framework Integration\n\n- Native support for AutoGen agents\n- Easy adapters for other frameworks\n- Consistent interfaces across all tools\n- Support for third-party models via OpenAI-compatible interface\n\n### 📚 Pre-built Tool Library\n\n- Web interaction tools (search, scraping)\n- Document processing tools\n- Data analysis utilities\n- API integrations\n- And many more!\n\n## 📖 Documentation\n\nFor complete documentation, visit our [documentation site](https://github.com/yourusername/autogen-toolsmith/docs).\n\n- [Getting Started Guide](docs/getting_started.md)\n- [Tool Creation Tutorial](docs/tool_creation.md)\n- [API Reference](docs/api_reference.md)\n- [Project Vision](docs/project_vision.md)\n- [Custom Output Directory](docs/custom_output.md)\n\n## 🤝 Contributing\n\nWe welcome contributions! See our [Contributing Guide](docs/contributing.md) for more information.\n\n## 📄 License\n\nMIT "
    },
    {
      "name": "filipw/strathweb-phi-engine",
      "stars": 20,
      "img": "https://avatars.githubusercontent.com/u/1710369?s=40&v=4",
      "owner": "filipw",
      "repo_name": "strathweb-phi-engine",
      "description": "A cross platform library with C#/Swift/Kotlin/Python bindings for running Phi inference",
      "homepage": "",
      "language": "Rust",
      "created_at": "2024-04-12T09:52:33Z",
      "updated_at": "2025-04-19T06:32:13Z",
      "topics": [
        "ai",
        "phi"
      ],
      "readme": "# Strathweb Phi Engine\n\nA cross-platform library for running Microsoft's [Phi-3](https://azure.microsoft.com/en-us/blog/introducing-phi-3-redefining-whats-possible-with-slms/) locally using [candle](https://github.com/huggingface/candle) in GGUF and safe tensors format.\n\n## Supported platforms\n\n|                        | **Swift**                                        | **.NET**                                                                                    | **Kotlin**                        | **Python**                                                  |\n|------------------------|--------------------------------------------------|---------------------------------------------------------------------------------------------|-----------------------------------|-------------------------------------------------------------|\n| **Package**            | Swift Package                                    | Nuget                                                                                       | N/A                               | N/A                                                         |\n| **Manual Integration** | Bindings + XCFramework Bindings + native library | Bindings + native library                                                                   | Bindings + native library         | Bindings + native library                                   |\n| **Platforms**          | macOS arm64<br/>iOS                                  | Windows x64<br/>Windows arm64 (not via Nuget)<br/>Linux x64<br/>Linux arm64 (not via Nuget)<br/>macOS arm64 | Windows x64<br/>Linux x64<br/>macOS arm64 | Windows x64<br/>Windows arm64<br/>Linux x64<br/>Linux arm64<br/>macOS arm64 |\n\n![](content/strathweb-phi-engine.excalidraw.png)\n\n## Building instructions\n\n### Swift\n\nBuild the Swift Package (arm64 Mac required).\n\n```shell\n./build-swift.sh\n```\n\nThis builds:\n - the Swift Package under `packages/swift/Strathweb.Phi.Engine`\n - XCFramework under `artifacts/swift/strathweb_phi_engine_framework.xcframework`\n\nNow open `samples/io/phi.engine.sample/phi.engine.sample.xcodeproj` and build the SwiftUI app (iOS), or go to `samples/swift` and run `./run.sh` (macOS) to launch the Swift console app.\n\n### C#\n\nInstall UniFFI C# bindings generator\n\n```shell\ncargo install uniffi-bindgen-cs --git https://github.com/NordSecurity/uniffi-bindgen-cs --tag v0.8.0+v0.25.0\n```\n\nBuild the Nuget package for your platform:\n\n```shell\n./build-dotnet.sh\n```\n\nor (on Windows)\n\n```cmd\nbuild-dotnet.bat\n ```\n\nor\n\n```shell\ncargo build --release --manifest-path strathweb-phi-engine/Cargo.toml\ndotnet build packages/csharp -c Release\ndotnet pack packages/csharp -c Release -o artifacts/csharp\n```\n\nNuget package will be in `artifacts/csharp/Strathweb.Phi.Engine.0.1.0.nupkg`.\n(Optional) Run the sample console app:\n\n```shell\ncd samples/csharp/console\ndotnet run -c Release\n```\n\n### Kotlin\n\nRun the sample console app:\n\n```shell\ncd samples/kotlin\n./run.sh\n```\n\n### Python\n\nRun the sample console app:\n\n```shell\ncd samples/python/console\n./run.sh # or run.bat on Windows\n```\n\nor use the Jupyter Notebooks\n\n```shell\ncd samples/python/jupyter\n./init.sh # or init.bat on Windows\n```\n\nNow open the Notebook and run the cells.\n\n## Compatibility notes\n\n### .NET\n\n✅ Tested on Windows arm64\n\n✅ Tested on Windows x64\n\n✅ Tested on Linux arm64\n\n✅ Tested on Linux x64\n\n✅ Tested on macOS arm64.\n\n### Swift\n\n✅ Tested on macOS arm64.\n\n✅ Tested on iPad Air M1 8GB RAM\n\n✅ Should work on 6GB RAM iPhones too\n\n❌ Will not work on 4GB RAM iPhones\n\nHowever, for 4GB RAM iPhones, it's possible to use the (very) low fidelity Q2_K quantized model. Such model is not included in the official Phi-3 release, but I tested [this one from HuggingFace](https://huggingface.co/SanctumAI/Phi-3-mini-4k-instruct-GGUF) on an iPhone 12 mini successfully.\n\n### Kotlin\n\n✅ Tested on macOS arm64.\n\n### Python\n\n✅ Tested on Windows arm64\n\n✅ Tested on macOS arm64.\n\n## GPU Support\n\nCurrently the library supports Metal on MacOS. On other platforms only CPU is supported.\n\n## AutoGen\n\nThe repository also contains a C# integration library for [AutoGen](https://github.com/microsoft/autogen/tree/dotnet/dotnet), called `Strathweb.Phi.Engine.AutoGen`. There is an example in the `samples/csharp/autogen` folder. It allows creating a local Phi-3 agent and integrating it into the other typical AutoGen workflows.\n\nThe NuGet package is built when running `./build-dotnet.sh` (`build-dotnet.bat` on Windows).\n\n## Blog posts\n\n📝 an initial [announcement post](https://strathweb.com/2024/07/announcing-strathweb-phi-engine-a-cross-platform-library-for-running-phi-3-anywhere/)\n\n📝 safe tensors [announcement post](https://www.strathweb.com/2024/11/strathweb-phi-engine-now-with-safe-tensors-support/)\n\n📝 [AutoGen library](https://www.strathweb.com/2024/09/using-local-phi-3-models-in-autogen-with-strathweb-phi-engine/)\n\n📝 [.NET examples and Microso.Extensions.AI support](https://www.strathweb.com/2024/12/running-phi-inference-in-net-applications-with-strathweb-phi-engine/)\n\n"
    },
    {
      "name": "amaziahyahalom/aider-model-train",
      "stars": 20,
      "img": "https://avatars.githubusercontent.com/u/192240462?s=40&v=4",
      "owner": "amaziahyahalom",
      "repo_name": "aider-model-train",
      "description": null,
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2025-01-08T15:41:59Z",
      "updated_at": "2025-04-23T05:30:54Z",
      "topics": [],
      "readme": "<a name=\"readme-top\"></a>\n\n<div align=\"center\">\n<img src=\"https://microsoft.github.io/autogen/0.2/img/ag.svg\" alt=\"AutoGen Logo\" width=\"100\">\n\n[![Twitter](https://img.shields.io/twitter/url/https/twitter.com/cloudposse.svg?style=social&label=Follow%20%40pyautogen)](https://twitter.com/pyautogen) [![LinkedIn](https://img.shields.io/badge/LinkedIn-Company?style=flat&logo=linkedin&logoColor=white)](https://www.linkedin.com/company/105812540) [![Discord](https://img.shields.io/badge/discord-chat-green?logo=discord)](https://aka.ms/autogen-discord) [![GitHub Discussions](https://img.shields.io/badge/Discussions-Q%26A-green?logo=github)](https://github.com/microsoft/autogen/discussions) [![0.2 Docs](https://img.shields.io/badge/Docs-0.2-blue)](https://microsoft.github.io/autogen/0.2/) [![0.4 Docs](https://img.shields.io/badge/Docs-0.4-blue)](https://microsoft.github.io/autogen/dev/)\n\n[![PyPi autogen-core](https://img.shields.io/badge/PyPi-autogen--core-blue?logo=pypi)](https://pypi.org/project/autogen-core/0.4.0.dev13/) [![PyPi autogen-agentchat](https://img.shields.io/badge/PyPi-autogen--agentchat-blue?logo=pypi)](https://pypi.org/project/autogen-agentchat/0.4.0.dev13/) [![PyPi autogen-ext](https://img.shields.io/badge/PyPi-autogen--ext-blue?logo=pypi)](https://pypi.org/project/autogen-ext/0.4.0.dev13/)\n</div>\n\n# AutoGen\n\n> [!IMPORTANT]\n>\n> - (12/19/24) Hello!\nThe majority of the AutoGen Team members will be resting and recharging with family and friends over the holiday period. Activity/responses on the project may be delayed during the period of Dec 20-Jan 06. We will be excited to engage with you in the new year!\n> - (12/11/24) We have created a new Discord server for the AutoGen community. Join us at [aka.ms/autogen-discord](https://aka.ms/autogen-discord).\n> - (11/14/24) ⚠️ In response to a number of asks to clarify and distinguish between official AutoGen and its forks that created confusion, we issued a [clarification statement](https://github.com/microsoft/autogen/discussions/4217).\n> - (10/13/24) Interested in the standard AutoGen as a prior user? Find it at the actively-maintained *AutoGen* [0.2 branch](https://github.com/microsoft/autogen/tree/0.2) and `autogen-agentchat~=0.2` PyPi package.\n> - (10/02/24) [AutoGen 0.4](https://microsoft.github.io/autogen/dev) is a from-the-ground-up rewrite of AutoGen. Learn more about the history, goals and future at [this blog post](https://microsoft.github.io/autogen/blog). We’re excited to work with the community to gather feedback, refine, and improve the project before we officially release 0.4. This is a big change, so AutoGen 0.2 is still available, maintained, and developed in the [0.2 branch](https://github.com/microsoft/autogen/tree/0.2).\n> - *[Join us for Community Office Hours](https://github.com/microsoft/autogen/discussions/4059)* We will host a weekly open discussion to answer questions, talk about Roadmap, etc.\n\nAutoGen is an open-source framework for building AI agent systems.\nIt simplifies the creation of event-driven, distributed, scalable, and resilient agentic applications.\nIt allows you to quickly build systems where AI agents collaborate and perform tasks autonomously\nor with human oversight.\n\n- [Key Features](#key-features)\n- [API Layering](#api-layering)\n- [Quickstart](#quickstart)\n- [Roadmap](#roadmap)\n- [FAQs](#faqs)\n\nAutoGen streamlines AI development and research, enabling the use of multiple large language models (LLMs), integrated tools, and advanced multi-agent design patterns. You can develop and test your agent systems locally, then deploy to a distributed cloud environment as your needs grow.\n\n## Key Features\n\nAutoGen offers the following key features:\n\n- **Asynchronous Messaging**: Agents communicate via asynchronous messages, supporting both event-driven and request/response interaction patterns.\n- **Full type support**: use types in all interfaces and enforced type check on build, with a focus on quality and cohesiveness\n- **Scalable & Distributed**: Design complex, distributed agent networks that can operate across organizational boundaries.\n- **Modular & Extensible**: Customize your system with pluggable components: custom agents, tools, memory, and models.\n- **Cross-Language Support**: Interoperate agents across different programming languages. Currently supports Python and .NET, with more languages coming soon.\n- **Observability & Debugging**: Built-in features and tools for tracking, tracing, and debugging agent interactions and workflows, including support for industry standard observability with OpenTelemetry\n\n<p align=\"right\" style=\"font-size: 14px; color: #555; margin-top: 20px;\">\n  <a href=\"#readme-top\" style=\"text-decoration: none; color: blue; font-weight: bold;\">\n    ↑ Back to Top ↑\n  </a>\n</p>\n\n# API Layering\n\nAutoGen has several packages and is built upon a layered architecture.\nCurrently, there are three main APIs your application can target:\n\n- [Core](https://microsoft.github.io/autogen/dev/user-guide/core-user-guide/index.html)\n- [AgentChat](https://microsoft.github.io/autogen/dev/user-guide/agentchat-user-guide/index.html)\n- [Extensions](https://microsoft.github.io/autogen/dev/user-guide/extensions-user-guide/index.html)\n\n## Core\n\n- [Installation](https://microsoft.github.io/autogen/dev/user-guide/core-user-guide/installation.html)\n- [Quickstart](https://microsoft.github.io/autogen/dev/user-guide/core-user-guide/quickstart.html)\n\nThe core API of AutoGen, `autogen-core`, is built following the\n[actor model](https://en.wikipedia.org/wiki/Actor_model).\nIt supports asynchronous message passing between agents and event-based workflows.\nAgents in the core layer handle and produce typed messages, using either direct messaging,\nwhich functions like RPC, or via broadcasting to topics, which is pub-sub.\nAgents can be distributed and implemented in different programming languages,\nwhile still communicating with one another.\n**Start here if you are building scalable, event-driven agentic systems.**\n\n## AgentChat\n\n- [Installation](https://microsoft.github.io/autogen/dev/user-guide/agentchat-user-guide/installation.html)\n- [Quickstart](https://microsoft.github.io/autogen/dev/user-guide/agentchat-user-guide/quickstart.html)\n\nThe AgentChat API, `autogen-agentchat`, is task driven and at a high level like AutoGen 0.2.\nIt allows you to define conversational agents, compose them into teams and then\nuse them to solve tasks.\nAgentChat itself is built on the core layer, but it abstracts away much of its\nlow-level system concepts.\nIf your workflows don't fit into the AgentChat API, target core instead.\n**Start here if you just want to focus on quickly getting started with multi-agents workflows.**\n\n## Extensions\n\nThe extension package `autogen-ext` contains implementations of the core interfaces using 3rd party systems,\nsuch as OpenAI model client and Azure code executors.\nBesides the built-in extensions, the package accommodates community-contributed\nextensions through namespace sub-packages.\nWe look forward to your contributions!\n\n<p align=\"right\" style=\"font-size: 14px; color: #555; margin-top: 20px;\">\n  <a href=\"#readme-top\" style=\"text-decoration: none; color: blue; font-weight: bold;\">\n    ↑ Back to Top ↑\n  </a>\n</p>\n\n## Quickstart\n\n### Python (AgentChat)\n\nFirst install the packages:\n\n```bash\npip install \"autogen-agentchat==0.4.0.dev13\" \"autogen-ext[openai]==0.4.0.dev13\"\n```\n\nThe following code uses OpenAI's GPT-4o model and you need to provide your\nAPI key to run.\nTo use Azure OpenAI models, follow the instruction\n[here](https://microsoft.github.io/autogen/dev/user-guide/core-user-guide/cookbook/azure-openai-with-aad-auth.html).\n\n```python\nimport asyncio\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_agentchat.ui import Console\nfrom autogen_agentchat.conditions import TextMentionTermination\nfrom autogen_agentchat.teams import RoundRobinGroupChat\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n\n# Define a tool\nasync def get_weather(city: str) -> str:\n    return f\"The weather in {city} is 73 degrees and Sunny.\"\n\nasync def main() -> None:\n    # Define an agent\n    weather_agent = AssistantAgent(\n        name=\"weather_agent\",\n        model_client=OpenAIChatCompletionClient(\n            model=\"gpt-4o-2024-08-06\",\n            # api_key=\"YOUR_API_KEY\",\n        ),\n        tools=[get_weather],\n    )\n\n    # Define termination condition\n    termination = TextMentionTermination(\"TERMINATE\")\n\n    # Define a team\n    agent_team = RoundRobinGroupChat([weather_agent], termination_condition=termination)\n\n    # Run the team and stream messages to the console\n    stream = agent_team.run_stream(task=\"What is the weather in New York?\")\n    await Console(stream)\n\nasyncio.run(main())\n```\n\n### C\\#\n\nThe .NET SDK does not yet support all of the interfaces that the python SDK offers but we are working on bringing them to parity.\nTo use the .NET SDK, you need to add a package reference to the src in your project.\nWe will release nuget packages soon and will update these instructions when that happens.\n\n```\ngit clone https://github.com/microsoft/autogen.git\ncd autogen\n# Switch to the branch that has this code\ngit switch staging-dev\n# Build the project\ncd dotnet && dotnet build AutoGen.sln\n# In your source code, add AutoGen to your project\ndotnet add <your.csproj> reference <path to your checkout of autogen>/dotnet/src/Microsoft.AutoGen/Core/Microsoft.AutoGen.Core.csproj\n```\n\nThen, define and run your first agent:\n\n```csharp\nusing Microsoft.AutoGen.Contracts;\nusing Microsoft.AutoGen.Core;\nusing Microsoft.Extensions.DependencyInjection;\nusing Microsoft.Extensions.Hosting;\n\n// send a message to the agent\nvar app = await App.PublishMessageAsync(\"HelloAgents\", new NewMessageReceived\n{\n    Message = \"World\"\n}, local: true);\n\nawait App.RuntimeApp!.WaitForShutdownAsync();\nawait app.WaitForShutdownAsync();\n\n[TopicSubscription(\"agents\")]\npublic class HelloAgent(\n    IAgentContext worker,\n    [FromKeyedServices(\"EventTypes\")] EventTypes typeRegistry) : ConsoleAgent(\n        worker,\n        typeRegistry),\n        ISayHello,\n        IHandle<NewMessageReceived>,\n        IHandle<ConversationClosed>\n{\n    public async Task Handle(NewMessageReceived item)\n    {\n        var response = await SayHello(item.Message).ConfigureAwait(false);\n        var evt = new Output\n        {\n            Message = response\n        }.ToCloudEvent(this.AgentId.Key);\n        await PublishEventAsync(evt).ConfigureAwait(false);\n        var goodbye = new ConversationClosed\n        {\n            UserId = this.AgentId.Key,\n            UserMessage = \"Goodbye\"\n        }.ToCloudEvent(this.AgentId.Key);\n        await PublishEventAsync(goodbye).ConfigureAwait(false);\n    }\n    public async Task Handle(ConversationClosed item)\n    {\n        var goodbye = $\"*********************  {item.UserId} said {item.UserMessage}  ************************\";\n        var evt = new Output\n        {\n            Message = goodbye\n        }.ToCloudEvent(this.AgentId.Key);\n        await PublishEventAsync(evt).ConfigureAwait(false);\n        await Task.Delay(60000);\n        await App.ShutdownAsync();\n    }\n    public async Task<string> SayHello(string ask)\n    {\n        var response = $\"\\n\\n\\n\\n***************Hello {ask}**********************\\n\\n\\n\\n\";\n        return response;\n    }\n}\npublic interface ISayHello\n{\n    public Task<string> SayHello(string ask);\n}\n```\n\n```bash\ndotnet run\n```\n\n<p align=\"right\" style=\"font-size: 14px; color: #555; margin-top: 20px;\">\n  <a href=\"#readme-top\" style=\"text-decoration: none; color: blue; font-weight: bold;\">\n    ↑ Back to Top ↑\n  </a>\n</p>\n\n## Roadmap\n\n- AutoGen 0.2 - This is the current stable release of AutoGen. We will continue to accept bug fixes and minor enhancements to this version.\n- AutoGen 0.4 - This is the first release of the new architecture. This release is still in *preview*. We will be focusing on the stability of the interfaces, documentation, tutorials, samples, and a collection of built-in agents which you can use. We are excited to work with our community to define the future of AutoGen. We are looking for feedback and contributions to help shape the future of this project. Here are some major planned items:\n  - More programming languages (e.g., TypeScript)\n  - More built-in agents and multi-agent workflows\n  - Deployment of distributed agents\n  - Re-implementation/migration of AutoGen Studio\n  - Integration with other agent frameworks and data sources\n  - Advanced RAG techniques and memory services\n\n<p align=\"right\" style=\"font-size: 14px; color: #555; margin-top: 20px;\">\n  <a href=\"#readme-top\" style=\"text-decoration: none; color: blue; font-weight: bold;\">\n    ↑ Back to Top ↑\n  </a>\n</p>\n\n## FAQs\n\n### What is AutoGen 0.4?\n\nAutoGen v0.4 is a rewrite of AutoGen from the ground up to create a more robust,\nscalable, easier to use, cross-language library for building AI Agents.\nSome key features include asynchronous messaging, support for scalable distributed agents,\nmodular extensible design (bring your own agents, implement behaviors however you like),\ncross-language support, improved observability, and full typing integration.\nIt is a breaking change.\n\n### Why these changes?\n\nWe listened to our AutoGen users, learned from what was working, and adapted to fix what wasn't.\nWe brought together wide-ranging teams working on many different types of AI Agents\nand collaborated to design an improved framework with a more flexible\nprogramming model and better scalability.\n\n### Is this project still maintained?\n\nWe want to reaffirm our commitment to supporting both the original version of AutoGen (0.2) and the redesign (0.4) . AutoGen 0.4 is still work-in-progress, and we shared the code now to build with the community. There are no plans to deprecate the original AutoGen anytime soon, and both versions will be actively maintained.\n\n### Who should use it 0.4?\n\nThis code is still experimental, so expect changes and bugs while we work towards a stable 0.4 release. We encourage early adopters to\ntry it out, give us feedback, and contribute.\nFor those looking for a stable version we recommend to continue using 0.2\n\n### I'm using AutoGen 0.2, should I upgrade?\n\nIf you consider yourself an early adopter, you are comfortable making some\nchanges to your code, and are willing to try it out, then yes.\n\n### How do I still use AutoGen 0.2?\n\nAutoGen 0.2 can be installed with:\n\n```sh\npip install autogen-agentchat~=0.2\n```\n\n### Will AutoGen Studio be supported in 0.4?\n\nYes, this is on the [roadmap](#roadmap).\nOur current plan is to enable an implementation of AutoGen Studio\non the AgentChat high level API which implements a set of agent functionalities\n(agents, teams, etc).\n\n### How do I migrate?\n\nFor users familiar with AutoGen, the AgentChat library in 0.4 provides similar concepts.\nWe are working on a migration guide.\n\n### Is 0.4 done?\n\nWe are still actively developing AutoGen 0.4. One exciting new feature is the emergence of new SDKs for .NET. The python SDKs are further ahead at this time but our goal is to achieve parity. We aim to add additional languages in future releases.\n\n### What is happening next? When will this release be ready?\n\nWe are still working on improving the documentation, samples, and enhancing the code. We are hoping to release before the end of the year when things are ready.\n\n### What is the history of this project?\n\nThe rearchitecture of the framework started with multiple Microsoft teams coming together\nto address the gaps and learnings from AutoGen 0.2 - merging ideas from several predecessor projects.\nThe team worked on this internally for some time to ensure alignment before moving work back to the open in October 2024.\n\n### What is the official channel for support?\n\nUse GitHub [Issues](https://github.com/microsoft/autogen/issues) for bug reports and feature requests.\nUse GitHub [Discussions](https://github.com/microsoft/autogen/discussions) for general questions and discussions.\n\n### Do you use Discord for communications?\n\nWe are unable to use the old Discord for project discussions, many of the maintainers no longer have viewing or posting rights there. Therefore, we request that all discussions take place on <https://github.com/microsoft/autogen/discussions/>  or the [new discord server](https://aka.ms/autogen-discord).\n\n### What about forks?\n\n<https://github.com/microsoft/autogen/> remains the only official repo for development and support of AutoGen.\nWe are aware that there are thousands of forks of AutoGen, including many for personal development and startups building with or on top of the library. We are not involved with any of these forks and are not aware of any plans related to them.\n\n### What is the status of the license and open source?\n\nOur project remains fully open-source and accessible to everyone. We understand that some forks use different licenses to align with different interests. We will continue to use the most permissive license (MIT) for the project.\n\n### Can you clarify the current state of the packages?\n\nCurrently, we are unable to make releases to the `pyautogen` package via Pypi due to a change to package ownership that was done without our involvement. Additionally, we are moving to using multiple packages to align with the new design. Please see details [here](https://microsoft.github.io/autogen/dev/packages/index.html).\n\n### Can I still be involved?\n\nWe are grateful to all the contributors to AutoGen 0.2 and we look forward to continuing to collaborate with everyone in the AutoGen community.\n\n<p align=\"right\" style=\"font-size: 14px; color: #555; margin-top: 20px;\">\n  <a href=\"#readme-top\" style=\"text-decoration: none; color: blue; font-weight: bold;\">\n    ↑ Back to Top ↑\n  </a>\n</p>\n\n## Legal Notices\n\nMicrosoft and any contributors grant you a license to the Microsoft documentation and other content\nin this repository under the [Creative Commons Attribution 4.0 International Public License](https://creativecommons.org/licenses/by/4.0/legalcode),\nsee the [LICENSE](LICENSE) file, and grant you a license to any code in the repository under the [MIT License](https://opensource.org/licenses/MIT), see the\n[LICENSE-CODE](LICENSE-CODE) file.\n\nMicrosoft, Windows, Microsoft Azure, and/or other Microsoft products and services referenced in the documentation\nmay be either trademarks or registered trademarks of Microsoft in the United States and/or other countries.\nThe licenses for this project do not grant you rights to use any Microsoft names, logos, or trademarks.\nMicrosoft's general trademark guidelines can be found at <http://go.microsoft.com/fwlink/?LinkID=254653>.\n\nPrivacy information can be found at <https://go.microsoft.com/fwlink/?LinkId=521839>\n\nMicrosoft and any contributors reserve all other rights, whether under their respective copyrights, patents,\nor trademarks, whether by implication, estoppel, or otherwise.\n\n<p align=\"right\" style=\"font-size: 14px; color: #555; margin-top: 20px;\">\n  <a href=\"#readme-top\" style=\"text-decoration: none; color: blue; font-weight: bold;\">\n    ↑ Back to Top ↑\n  </a>\n</p>\n"
    },
    {
      "name": "lordlinus/travel-chatbot",
      "stars": 20,
      "img": "https://avatars.githubusercontent.com/u/2416507?s=40&v=4",
      "owner": "lordlinus",
      "repo_name": "travel-chatbot",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2024-10-23T04:59:49Z",
      "updated_at": "2025-04-22T11:34:17Z",
      "topics": [],
      "readme": "# Travel Chatbot - Multi-Agent System\n\nAutogen core api based chatbot for planning trips, booking flights, hotels, car rentals, and providing destination information. The system routes user requests to specialized agents for a streamlined experience.\n\n## Key Features\n\nRouter Agent: Determines user intent and routes messages.\nUser Proxy Agent: Manages WebSocket connections between users and agents.\nSpecialized Agents: Handles flights, hotels, car rentals, activities, and destination details.\nGroup Chat Manager: Coordinates responses from multiple agents to provide a comprehensive travel plan.\n\n## Message Flow\n\n1. **User Interaction**: The user sends a message via the WebSocket connection managed by the UserProxyAgent.\n2. **Routing**: The UserProxyAgent forwards the message to the SemanticRouterAgent.\n3. **Intent Classification**: The SemanticRouterAgent analyzes the message to determine the user's intent.\n4. **Agent Selection**:\n   - If the intent maps to a single agent, the message is forwarded directly to that agent.\n   - If multiple agents are required, the message is sent to the GroupChatManager.\n5. **Task Handling**:\n   - **Single Agent**: The selected agent processes the request and sends a response back to the UserProxyAgent, which then sends it to the user.\n   - **Group Chat**: The GroupChatManager coordinates between multiple agents, collects their responses, and compiles a final plan. This plan is then sent back to the user via the UserProxyAgent.\n6. **Handoff**: If an agent cannot handle the request (e.g., the request is too complex), it sends a HandoffMessage back to the SemanticRouterAgent to re-route or escalate the request. #TODO\n\nHigh level overview of the message flow:\n\n![High level overview of the message flow](./images/messageflow_highlevel_overview.png)\n\n## Usage\n\n### Running the Application\n\n#### Clone the Repository:\n\n```bash\ngit clone https://github.com/yourusername/travel-chatbot.git\ncd travel-chatbot\n```\n\n#### Install Dependencies:\n\nEnsure you have Python 3.11 or higher installed. Install required packages using pip:\n\n```bash\npip install -r requirements.txt\n```\n\n#### Set Up Environment Variables:\n\nCreate a `.env` file in the `backend` directory with the necessary configuration. Refer to the Configuration section for details.\n\n#### Run the Application:\n\n```bash\nuvicorn backend.app:app --host 127.0.0.1 --port 8000\n```\n\n#### Access the Chatbot:\n\nConnect to the WebSocket endpoint at `ws://127.0.0.1:8000/chat` to start interacting with the chatbot.\n\n### Testing\n\nThe `tests` directory contains test cases to validate the functionality of the chatbot.\n\n#### Run Tests:\n\n```bash\npytest tests/\n```\n\n#### Test Chatbot Responses:\n\nThe `test_chatbot.py` script sends predefined questions to the chatbot and records responses for evaluation.\n\n## Dependencies\n\nThe project uses several external libraries and services:\n\n- **Azure OpenAI**: For language understanding and response generation.\n- **Autogen Core and Extensions**: For building agent-based systems.\n- **FastAPI**: For the web server and API endpoints.\n- **WebSockets**: For real-time communication with users.\n- **aiohttp**: For asynchronous HTTP requests.\n- **Pydantic**: For data validation and serialization.\n- **OpenTelemetry**: For tracing and logging.\n\n## Configuration\n\nThe application requires several environment variables to be set, which can be placed in a `.env` file in the `backend` directory:\n\n```ini\n# Azure OpenAI Configuration\nAZURE_OPENAI_DEPLOYMENT_NAME=your-deployment-name\nAZURE_OPENAI_API_VERSION=your-api-version\nAZURE_OPENAI_ENDPOINT=your-openai-endpoint\nAZURE_OPENAI_API_KEY=your-openai-api-key\n\n# Bing Custom Search\nBING_CUSTOM_CONFIG_ID=your-bing-config-id\nBING_CUSTOM_SEARCH_KEY=your-bing-search-key\n```\n\n## Directory Structure\n\n- `backend`: Contains the main application code.\n  - `agents`: Directory containing all agent implementations.\n  - `app.py`: Entry point for the FastAPI application.\n  - `config.py`: Configuration settings and environment variable handling.\n  - `data_types.py`: Definitions of custom data types and message formats.\n  - `utils.py`: Utility functions for initializing the agent runtime.\n- `tests`: Contains test scripts and fixtures for automated testing.\n\n## Monitoring and Logging\n\nThis project uses OpenTelemetry for tracing and logging. Traces are sent to local Aspire backend. [Instruction](https://microsoft.github.io/autogen/0.4.0.dev2/user-guide/core-user-guide/cookbook/instrumenting.html) on how to use OpenTelemetry with Autogen.\n\n## Monitoring and Logging\n\nThis project uses OpenTelemetry for tracing and logging. Traces are sent to a local Aspire backend. Follow the instructions at this [link](https://microsoft.github.io/autogen/0.4.0.dev2/user-guide/core-user-guide/cookbook/instrumenting.html) on how to use OpenTelemetry with Autogen.\n\n![Sample Trace](./images/sample_trace.png)\n\n## Frontend\n\nA simple frontend is provided in the `frontend` directory. It uses NextJs and WebSocket to communicate with the backend.\n![Webchat Interface](./images/sample_frontend.png)\n"
    },
    {
      "name": "yanxiaodi/MyCodeSamples",
      "stars": 19,
      "img": "https://avatars.githubusercontent.com/u/3395915?s=40&v=4",
      "owner": "yanxiaodi",
      "repo_name": "MyCodeSamples",
      "description": "Some demos in my blogs.",
      "homepage": null,
      "language": "C#",
      "created_at": "2021-01-11T02:10:01Z",
      "updated_at": "2025-04-11T10:24:29Z",
      "topics": [],
      "readme": "# MyCodeSamples\nSome demos in my blogs.\n\n* [4 ways to generate code in C# — Including Source Generators in .NET 5](https://levelup.gitconnected.com/four-ways-to-generate-code-in-c-including-source-generators-in-net-5-9e6817db425): [CodeGeneratorDemo](https://github.com/yanxiaodi/MyCodeSamples/tree/main/CodeGeneratorDemo)\n\n\n"
    },
    {
      "name": "LxYuan0420/nlp",
      "stars": 19,
      "img": "https://avatars.githubusercontent.com/u/25739658?s=40&v=4",
      "owner": "LxYuan0420",
      "repo_name": "nlp",
      "description": "A collection of NLP notebooks for quick demos and hands-on learning.",
      "homepage": "",
      "language": "Jupyter Notebook",
      "created_at": "2021-06-26T04:52:45Z",
      "updated_at": "2025-04-01T06:38:29Z",
      "topics": [],
      "readme": "### NLP Thingy\n\n\n#### [Tensorflow]\n- [IMDb Review Classification using CNN Tensorflow](notebooks/IMDb_Review_Classification_using_CNN_Tensorflow.ipynb)\n- [IMDB Review Classification using RNN Tensorflow](notebooks/IMDB_Review_Classification_using_RNN_Tensorflow.ipynb)\n- [Large Scale Multi Label Text Classification Tensorflow](notebooks/Large_scale_multi_label_text_classification_Tensorflow.ipynb)\n- [IMDB Review Classification using BERT Tensorflow](notebooks/IMDB_Review_Classification_using_BERT_Tensorflow.ipynb)\n- [IMDb Review Binary Classification using DistilBERT model HF/TF Trainer](notebooks/IMDb_Review_Classification_using_DistilBert_HF_Tensorflow_Trainer.ipynb)\n- [AG News Multiclass Classification using DistilBERT model HF/TF Native](notebooks/AG_News_Classification_using_DistilBERT_Tensorflow.ipynb)\n- [Conll2003 NER Classification using DistilBERT model HF/TF Script](notebooks/Conll2003_NER_Classification_using_DistilBERT_HF_Tensorflow_Script.ipynb)\n- [ConllPP NER Classification using DistilBERT model HF/TF Script](notebooks/ConllPP_NER_Classification_using_DistilBERT_HF_Tensorflow_Script.ipynb)\n- [Seq2Seq Addition using LSTM Tensorflow](notebooks/Seq2Seq_Addition_using_LSTM_Tensorflow_.ipynb)\n- [Text Extraction with BERT Tensorflow](notebooks/Text_Extraction_with_BERT.ipynb)\n- [Semantic Similarity using BERT HF Tensorflow](notebooks/Semantic_Similarity_using_BERT_HF_Tensorflow.ipynb)\n- [Text Generation with an RNN](notebooks/Text_Generation_with_an_RNN.ipynb)\n\n\n#### [Pytorch]\n- [IMDb Review Classification using CNN Pytorch](notebooks/IMDb_Review_Classification_using_CNN_Pytorch.ipynb)\n- [IMDb Review Binary Classification using DistilBERT model HF/PT Native](notebooks/IMDb_Review_Classification_using_DistilBert_Pytorch.ipynb)\n- [AG News Multiclass Classification using Torchtext Library Pytorch](notebooks/AG_News_Classification_using_torchtext_Library.ipynb)\n- [AG News Multiclass Classification using DistilBERT model HF/PT Native](notebooks/AG_News_Classification_using_DistilBERT_Pytorch.ipynb)\n- [Conll2003 NER Classification using DistilBERT model HF/PT Script](notebooks/Conll2003_NER_Classification_using_DistilBERT_HF_Pytorch_Script.ipynb)\n- [Conll2003 NER Classification using DistilBERT model HF/PT Trainer](notebooks/Conll2003_NER_Classification_using_DistilBERT_HF_Pytorch_Trainer.ipynb)\n- [ConllPP NER Classification using DistilBERT model HF/PT Script](notebooks/ConllPP_NER_Classification_using_DistilBERT_HF_Pytorch_Script.ipynb)\n- [Semantic Search using Multlingual Distilled Universal Sentence Encoder Pytorch](notebooks/Semantic_Search_using_Multlingual_Distilled_Universal_Sentence_Encoder.ipynb)\n- [Train Your Tokenizer from HF Tutorials HF/PT Native](notebooks/Train_your_tokenizer_from_HF_Tutorials.ipynb)\n- [Classifying Names with a Character level RNN Pytorch](notebooks/Classifying_Names_with_a_Character_level_RNN_Pytorch.ipynb)\n- [Generating Names with a Character level RNN Pytorch](notebooks/Generating_Names_With_A_Character_level_RNN.ipynb)\n- [Fine-tuning BERT for Multilabel Classification task using Pytorch Trainer](https://github.com/LxYuan0420/nlp/blob/main/notebooks/Fine_tuning_BERT_for_multi_label_text_classification_pytorch_trainer.ipynb)\n- [Inference with MarkupLM for QA on webpages](notebooks/Inference_with_MarkupLM_for_question_answering_on_web_pages.ipynb)\n- [Inference with GenerativeImage2Text GIT for image captioning and VQA](notebooks/Inference_with_GIT_for_image_video_captioning_and_image_video_QA.ipynb)\n- [Finetuning DistilGPT-2 Language Model on Wikitext Dataset](notebooks/Fine_tune_DistilGPT2_language_model_on_wikitext_dataset.ipynb)\n- [Finetuning DistilGPT-2 Language Model on Combined Wiki Dataset](notebooks/Finetune_DistilGPT2_language_model_combined_wikitext_dataset.ipynb)\n- [Distillation using Zero-shot pipeline with mDistilBERT Student Model](notebooks/Distilling_Zero_Shot_multilingual_distilbert_sentiments_student.ipynb)\n- [DistillBERT for Banking Intent Classification](notebooks/distillbert-intent-classification-banking.ipynb)\n- [Finetuning DistilGPT-2 Language Model on Combined Finance Dataset](notebooks/finetune_distilgpt2_language_model_on_finance_dataset.ipynb)\n- [Finetuning DistilBART Language Model on Combined Summarisation Dataset](notebooks/distilbart-finetune-summarisation.ipynb)\n- [SetFit Text Classification Example](notebooks/setfit-text-classification-example.ipynb)\n- [SpanMarker NER Classification Example](scripts/spanmarker_train.py)\n- [Reuters-21578 MultiLabel Classification using Transformer Trainer API](notebooks/transformer_reuters.ipynb)\n- [Topic Modelling with LLMs (OpenHermes_2_5_Mistral_7B_GGUF)](notebooks/Topic_Modeling_with_Quantized_LLMs_(OpenHermes_2_5_Mistral_7B_GGUF).ipynb)\n- [Inference Llama-2-7B-Chat-AWQ Notebook](notebooks/Inference_Llama_2_7B_Chat_using_AWQ.ipynb)\n- [4-bit Inferencing lxyuan/AeolusBlend-7B-slerp](notebooks/Inference_4bit_AeolusBlend.ipynb)\n- [From Data to Diagrams: Learn Knowledge Graph Basics with AeolusBlend LLM](notebooks/Inference_AeolusBlend_KnowledgeGraph.ipynb)\n- [2-bit Inferencing with AQLM](notebooks/Inference_with_AQLM_Transformers.ipynb)\n- [Finetuning Sentence Transformer Model from DistilBERT on NLI Triplet Dataset](notebooks/finetuning_distilbert_base_uncased_nli_triplet.ipynb)\n- [Finetuning Sentence Transformer Model from DistilBERT with multi dataset](notebooks/finetuning-distilbert-base-uncased-multi-dataset.ipynb)\n- [Finetuning Sentence Transformer Model using Matryoshka Loss](notebooks/Finetuning_Embedding_Model_Matryoshka_768_64_NLI.ipynb)\n\n#### [spaCy]\n- [Extract Subject-Verb-Object Relation using spaCy Dependency Matcher](notebooks/Extract_Subject_Verb_Object_Relation_using_spaCy_DependencyMatcher.ipynb)\n- [Toxic Comment Multilabel Classification using spaCy v3 TextCategorizer](notebooks/Toxic_Comment_Multilabel_Classification_using_spaCy_v3_TextCategorizer.ipynb)\n- [Drug Name Detection using spaCy v3 NER](notebooks/Drug_Name_Detection_spaCy_v3_NER.ipynb)\n- [Quickstart with zhort](notebooks/Quickstart_with_zshot.ipynb) \n- [spaCy Entity Linker](notebooks/Spacy_Entity_Linker_Demo.ipynb)\n- [Chinese Legal Text Classification with spaCy](notebooks/Chinese_Legal_NLP_Text_Classification_with_spaCy.ipynb)\n\n\n#### [DEMO]\n- [Chinese NER Model Demo with Gradio](notebooks/Chinese_NER_Model_Demo_with_Gradio.ipynb)\n- [Microsoft Phi1.5 Chatbot with Gradio](notebooks/gradio_microsoft_phi15.ipynb)\n\n#### [OTHERS]\n- [Quickstart with BERTopic](notebooks/Quickstart_BERTopic.ipynb)\n- [Reproducing_LUKE_experimental_results_Open_Entity](notebooks/Reproducing_LUKE_experimental_results_Open_Entity.ipynb)\n- [Detect bad labels with cleanlab and pytorch](notebooks/Image_Classification_with_PyTorch_and_Cleanlab.ipynb)\n- [Huggingface int8 demo](notebooks/HuggingFace_int8_demo.ipynb)\n- [DocsQA with LangChain and OpenAI](notebooks/DocQA_with_PDFs_with_LangChain_and_OpenAI.ipynb)\n- [Evaluator from Huggingface](notebooks/Evaluator_from_Huggingface.ipynb)\n- [Text Classification with Transformers and Datalab](notebooks/Text_Data_Cleaning_using_cleanlab_and_SentenceTransformer.ipynb)\n- [RAG with RAGatouille](notebooks/RAG_with_RAGatouille.ipynb)\n- [Structural Information Extraction with NuExtract-large](notebooks/Inference_with_NuExtract_large_for_Structural_Information_Extraction.ipynb)\n- [Experimenting Torchchat in Colab](notebooks/Experimenting_with_TorchChat_in_Colab_.ipynb)\n- [Unstructured.io Introductio Notebook](notebooks/Unstructured_Quick_Tour.ipynb)\n- [Text deduplication using model2vec](notebooks/Semantic_deduplication_using_model2vec.ipynb)\n- [AutoGen Weather Assistant Example](scripts/basic_weather_example.py)\n- [AutoGen Bank Assistant Example](scripts/bank_chatbot_example.py)\n- [AutoGen Debate Assistant Example](scripts/debate_example.py)\n- [AutoGen Funny Counting Assistant Example](scripts/funny_counting_example.py)\n- [Surya - Document OCR Toolkit](scripts/surya_gui.py) [SS1](docs/Screenshot-2025-03-10-at-12.42.16 PM.png) [SS2](docs/Screenshot-2025-03-10-at-12.42.55 PM.png) [SS3](docs/Screenshot-2025-03-10-at-12.44.18 PM.png) [SS4](docs/Screenshot-2025-03-10-at-12.45.32 PM.png) [SS5](docs/Screenshot-2025-03-10-at-12.48.45 PM.png)\n- [Routing Banking Example - Openai Agent SDK](scripts/routing_agent_banking_example.py)\n\n\n#### [Computer Vision]\n- [ViT X-ray Pneumonia Classification Notebook](notebooks/ViT-xray-classification.ipynb)\n\n\n#### [Speech]\n- [Inference with Text-to-Speech VIT model](notebooks/exp_text_to_speech_vits.ipynb)\n- [viXTTS_Customization_Introducing_Random_Pauses_and_Fading_Transitions.ipynb](notebooks/viXTTS_Customization_Introducing_Random_Pauses_and_Fading_Transitions.ipynb)\n"
    },
    {
      "name": "ababdotai/awesome-agent-quickstart",
      "stars": 18,
      "img": "https://avatars.githubusercontent.com/u/196296006?s=40&v=4",
      "owner": "ababdotai",
      "repo_name": "awesome-agent-quickstart",
      "description": "Ready-to-use helloworld examples and manually-verified quickstarts for LangGraph, AutoGen, Smolagents, OpenAI Agents, and more.",
      "homepage": "",
      "language": "Python",
      "created_at": "2025-02-04T11:42:14Z",
      "updated_at": "2025-04-11T03:19:47Z",
      "topics": [
        "agents",
        "ai",
        "autogen",
        "langchain",
        "langgraph",
        "llm",
        "smolagents"
      ],
      "readme": "# awesome-agent-quickstart\n\nYour fast lane to AI Agent development! \n\nThis repository helps you bypass setup complexities and dive straight into latest AI Agent frameworks. Go from zero to running your first agent in minutes, whether you're interested in LangGraph, AutoGen, Smolagents, or other popular frameworks.\n\n## 🎯 Features\n\n- ⚡ Zero configuration needed - get started in minutes\n- 🎓 Learn by example - all examples are runnable\n- 🔄 Model-agnostic - support any LLM\n- 🛠️ Centralized configuration - shared settings across framework\n\n## 🚀 Supported Frameworks\n\nReady-to-run templates for:\n\n### Flow control\n- LangChain\n- LangGraph (Python&JS)\n- Smolagents\n\n### Role-playing\n- LangGraph-Swarm\n- OpenAI Swarm\n- AutoGen\n- CrewAI\n- OpenAI Agents SDK\n\n### Tools\n- LangChain MCP Adapters\n- Browser-use\n\n\n## 📁 Project Structure\n\nEach directory is a self-contained example of an agent framework. For example:\n\n```\nawesome-agent-quickstart/\n├── langgraph/                # Framework name\n│   ├── config.py             # Common configurations (model params, API settings&checking)\n│   ├── helloworld.py         # Basic example: Simple conversational agent\n│   ├── requirements.txt      # Dependency management\n│   └── .env.example          # Environment variables template\n│   └── INTRODUCTION.md       # The framework's README, latest&raw\n│   └── README.md             # A step by step guide to use the framework\n```\n\n## 🐍 Setup\n\nSince some frameworks reply on the latest Python features, we recommend create a virtual environment:\n\n```\nconda create -n agents-quickstart python=3.13\nconda activate agents-quickstart\n```\n\n## 🤝 Contributing\n\nContributions for more agent framework examples are welcome! Please ensure:\n1. Create examples under the respective framework directory\n2. Use common configurations from `config.py`\n3. Provide clear documentation and comments\n4. Include `requirements.txt` and `.env.example`\n5. Follow project code style and best practices\n\n## 📝 Development Guidelines\n\n1. Code Structure\n   - Follow modular design\n   - Separate configuration from logic\n   - Include proper error handling\n   - Keep dependency files up-to-date\n\n2. Documentation\n   - Add docstrings for main functions and classes\n   - Include usage examples\n   - Explain key concepts and design decisions\n\n3. Security\n   - Use environment variables for sensitive data\n   - Include rate limiting considerations\n   - Add proper input validation\n\nSee [Contributing Guidelines](CONTRIBUTING.md) for more details.\n\n## 📃 License\n\nMIT License - see [LICENSE](LICENSE)\n\n## 🌟 Community\n\n- ⭐ Star us on GitHub\n- 🐛 [Report issues](https://github.com/ababaidotai/awesome-agent-quickstart/issues)\n- 📧 Contact: ababai.ai@outlook.com\n\n## 🙏 Acknowledgments\n\nMade with ❤️ by the AI community, for the AI community.\n\n- Thanks to all contributors!\n- Thanks to the framework development teams!\n- Thanks to the LLM community!"
    },
    {
      "name": "gxjorge/AI-Agents-Lab",
      "stars": 17,
      "img": "https://avatars.githubusercontent.com/u/3693420?s=40&v=4",
      "owner": "gxjorge",
      "repo_name": "AI-Agents-Lab",
      "description": "AI-Agents-Lab",
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2024-12-18T10:16:03Z",
      "updated_at": "2025-03-01T05:39:14Z",
      "topics": [],
      "readme": "# Agents in Action: Transforming through Automation and Intelligence\n\n## Table of Contents\n\n1. [📘 Introduction to Agentic Systems](#introduction-to-agentic-systems)\n2. [🤖 What is an Agent?](#what-is-an-agent)\n   - [Characteristics of Agents](#characteristics-of-agents)\n3. [🚀 Applications of Agentic Systems](#applications-of-agentic-systems)\n   - [Automotive & Transportation](#automotive--transportation)\n   - [Retail & Consumer Packaged Goods](#retail--consumer-packaged-goods)\n   - [Finance](#finance)\n   - [Manufacturing](#manufacturing)\n4. [☁️ Azure AI Agent Service](#azure-ai-agent-service)\n   - [Key Features of Azure AI Agent Service](#key-features-of-azure-ai-agent-service)\n   - [How Azure AI Agent Service Works](#how-azure-ai-agent-service-works)\n   - [Setup](#setup)\n   - [Getting started with the Azure Agent Service with the OpenAI SDK](#getting-started-with-the-azure-agent-service-with-the-openai-sdk)\n   - [Getting started with the Azure AI Foundry SDK](#getting-started-with-the-azure-ai-foundry-sdk)\n   - [Use other models with the Azure AI Agent Service](#use-other-models-with-the-azure-ai-agent-service)\n   - [Grounding with Bing](#grounding-with-bing)\n   - [Actions with Code Interpreter](#actions-with-code-interpreter)\n   - [OpenAPI Actions](#openapi-actions)\n   - [Actions with Azure functions](#actions-with-azure-functions)\n   - [Tracing and monitoring](#tracing-and-monitoring)\n   - [Multi-agent with SK/Autogen](#multi-agent-with-skautogen)\n5. [💼 Bonus exercise: Personal Financial Assistant](#bonus-exercise-personal-financial-assistant)\n6. [📊 Bonus exercise: Sales Analyst Agent](#bonus-exercise-sales-analyst-agent)\n\n## Introduction to Agentic Systems\n\nIn the rapidly evolving field of Artificial Intelligence, agentic systems represent a significant step forward in designing intelligent applications. These systems are composed of autonomous entities, known as *agents*, that can perceive their environment, make decisions, and act upon their environment to achieve specific goals.\n\n### What is an Agent?\n\nAn *agent* can be defined as a software entity designed to autonomously or semi-autonomously:\n\n- **Perceive**: Gather data from its environment through sensors or data inputs.\n- **Reason**: Process perceived information to make informed decisions based on its goals.\n- **Act**: Execute decisions by performing actions that influence their environment.\n- **Remember**: Store past experiences and information to inform future decisions, enabling learning and adaptation.\n\n#### Characteristics of Agents:\n\n1. **Autonomy**: Operates without direct human intervention.\n2. **Proactiveness**: Takes initiative to achieve goals.\n3. **Reactivity**: Responds to changes in its environment in real-time.\n4. **Event-Driven Behavior**: Reacts to specific events or changes in its environment, enabling real-time adaptability.\n5. **Social Ability**: Communicates with other agents or systems to collaborate.\n\n### Applications of Agentic Systems\n\nHere are some use cases for agentic Systems in different sectors:\n* Automotive & Transportation\n    * Autonomous Vehicles\n        * Self-Driving Capabilities: AI agents use a combination of sensors, cameras, and machine learning algorithms to perceive the environment, make decisions, and control the vehicle. This includes tasks like lane keeping, obstacle detection, and adaptive cruise control.\n        * Improved Navigation: AI agents can analyze real-time traffic data, road conditions, and map information to optimize routes, reduce travel time, and improve fuel efficiency.\n        * Enhanced Safety Features: AI agents can monitor driver behavior, detect potential hazards, and take preventive actions such as automatic braking, lane departure warnings, and collision avoidance systems.\n    * Predictive Maintenance\n        * Vehicle Health Monitoring: AI agents continuously collect data from various sensors in the vehicle to monitor the condition of critical components like the engine, brakes, and transmission.\n        * Predicting Maintenance Needs: By analyzing historical data and current performance metrics, AI agents can predict when a component is likely to fail and schedule maintenance before a breakdown occurs.\n        * Scheduling Services: AI agents can automatically book service appointments, order necessary parts, and notify the owner, ensuring timely maintenance and reducing downtime.\n    * Customer Experience\n        * Personalized Recommendations: AI agents can analyze user preferences and driving habits to recommend features, services, and even new vehicles that match the customer’s needs.\n        * Vehicle Configuration Assistance: AI agents can guide customers through the process of configuring their vehicle, suggesting options and packages based on their preferences and budget.\n        * Enhanced Customer Service: AI agents can provide 24/7 support through chatbots and virtual assistants, answering questions, scheduling test drives, and providing updates on service status.\n    * Transportation\n    * Fleet Management\n        * Route Optimization: AI agents can analyze traffic conditions, delivery schedules, and vehicle locations to determine the most efficient routes, reducing fuel consumption and travel time.\n        * Schedule Management: AI agents can manage driver schedules, ensuring compliance with regulations and optimizing shift patterns to maximize productivity.\n        * Vehicle Health Monitoring: Similar to predictive maintenance in automotive, AI agents can monitor the health of fleet vehicles, predict maintenance needs, and schedule services to minimize downtime.\n    * Traffic Management\n        * Traffic Pattern Analysis: AI agents can analyze real-time traffic data from various sources to identify congestion patterns and predict traffic flow.\n        * Signal Timing Optimization: By adjusting traffic signal timings based on current traffic conditions, AI agents can reduce congestion and improve traffic flow.\n        * Congestion Management: AI agents can implement dynamic traffic management strategies, such as rerouting traffic, adjusting speed limits, and providing real-time traffic updates to drivers.\n    * Logistics Optimization\n        * Inventory Management: AI agents can forecast demand, manage stock levels, and automate reordering processes, ensuring that inventory is always at optimal levels.\n        * Demand Forecasting: By analyzing historical sales data and market trends, AI agents can predict future demand, helping businesses plan their logistics operations more effectively.\n        * Optimized Logistics: AI agents can plan and coordinate the movement of goods, optimizing routes, schedules, and load distribution to ensure timely delivery and reduce cost\n\n* Retail & Consumer Packaged Goods\n    * Personalized Marketing\n        * Customer Data Analysis: AI agents can analyze vast amounts of customer data, including purchase history, browsing behavior, and demographic information, to understand individual preferences and behaviors.\n        * Targeted Campaigns: Using this data, AI agents can create highly targeted marketing campaigns that resonate with specific customer segments, increasing engagement and conversion rates.\n        * Product Recommendations: AI agents can recommend products to customers based on their past purchases and browsing history, enhancing the shopping experience and boosting sales.\n        * Dynamic Pricing: AI agents can optimize pricing strategies by analyzing market trends, competitor pricing, and customer demand, ensuring competitive pricing while maximizing profits.\n    * Inventory Management\n        * Demand Forecasting: AI agents can predict future demand by analyzing historical sales data, market trends, and seasonal patterns, helping retailers maintain optimal inventory levels.\n        * Stock Level Management: AI agents can monitor stock levels in real-time, ensuring that popular items are always available and reducing the risk of overstocking or stockouts.\n        * Automated Reordering: When stock levels fall below a certain threshold, AI agents can automatically place reorders, streamlining the supply chain and reducing manual intervention.\n        * Waste Reduction: By accurately predicting demand and managing stock levels, AI agents can help reduce waste, particularly for perishable goods.\n    * Customer Experience\n        * Virtual Assistants: AI-powered virtual assistants can assist customers with their queries, provide product information, and guide them through the shopping process, enhancing customer satisfaction.\n        * Personalized Recommendations: AI agents can offer personalized product recommendations based on individual customer preferences and past behavior, making the shopping experience more enjoyable and relevant.\n        * Efficient Customer Service: AI agents can handle routine customer service tasks, such as answering FAQs, processing returns, and tracking orders, freeing up human agents to handle more complex issues.\n    * Consumer Packaged Goods (CPG)\n    * Supply Chain Optimization\n        * Demand Forecasting: AI agents can analyze sales data, market trends, and external factors (like weather or economic conditions) to accurately forecast demand, ensuring that supply meets demand.\n        * Inventory Management: AI agents can manage inventory levels across the supply chain, ensuring that products are available where and when they are needed, reducing stockouts and excess inventory.\n        * Logistics Optimization: AI agents can optimize logistics by planning efficient routes, consolidating shipments, and managing transportation schedules, reducing costs and improving delivery times.\n    * Product Development\n        * Market Trend Analysis: AI agents can analyze market trends, consumer feedback, and social media data to identify emerging trends and consumer preferences, guiding product development.\n        * Consumer Feedback Analysis: By analyzing customer reviews, surveys, and feedback, AI agents can identify areas for improvement in existing products and suggest new product ideas.\n        * Rapid Prototyping: AI agents can assist in the rapid prototyping and testing of new products, using data-driven insights to refine and improve product designs before full-scale production.\n    * Quality Control\n        * Production Monitoring: AI agents can monitor production processes in real-time, detecting any deviations from quality standards and alerting operators to potential issues.\n        * Defect Detection: Using advanced image recognition and machine learning algorithms, AI agents can detect defects in products during the manufacturing process, ensuring high-quality output.\n        * Process Optimization: AI agents can analyze production data to identify inefficiencies and suggest improvements, optimizing processes and reducing waste.\n\n* Finance\n    * Fraud Detection\n        * Transaction Data Analysis: AI agents can process vast amounts of transactional data to identify patterns and detect anomalies in real time.\n        * Detect Unusual Patterns: By using advanced machine learning algorithms, AI agents can flag transactions that deviate from normal behavior, signaling potential fraud.\n        * Prevent Fraud: Proactive monitoring and analysis by AI agents help prevent fraudulent activities before they escalate, ensuring financial security for customers.\n        * Customer Security: AI agents enhance customer trust by safeguarding sensitive financial data and ensuring secure transactions.\n    * Risk Management\n        * Analyze Market Data: AI agents can analyze diverse datasets, including market trends, economic indicators, and geopolitical events, to assess risks.\n        * Predict Trends: By leveraging predictive analytics, AI agents can forecast market movements and potential risks, enabling proactive decision-making.\n        * Insights and Recommendations: AI agents provide actionable insights and tailored recommendations for minimizing financial risks and maximizing returns.\n    * Customer Service\n        * Virtual Assistants: AI-powered virtual assistants can handle customer inquiries, provide account information, and resolve common issues efficiently.\n        * Process Transactions: AI agents can automate routine transactions, such as fund transfers and bill payments, reducing wait times and improving convenience.\n        * Provide Financial Advice: AI agents can offer personalized financial advice based on a customer’s financial history, goals, and risk appetite.\n    * Algorithmic Trading\n        * Analyze Market Conditions: AI agents analyze real-time market data to identify trading opportunities and potential risks.\n        * Execute Trades: AI agents can execute trades automatically, ensuring timely and accurate execution in fast-moving markets.\n        * Optimize Trading Strategies: By analyzing historical data and market trends, AI agents can refine and optimize trading algorithms for better outcomes.\n        * Reduce Human Error: Automation by AI agents minimizes the risk of human error, ensuring precise and consistent trading operations.\n    * Regulatory Compliance\n        * Monitor Transactions and Communications: AI agents ensure compliance by continuously monitoring financial transactions and communications for irregularities.\n        * Flag Suspicious Activities: AI agents can identify and flag potential compliance breaches or suspicious activities for further investigation.\n        * Automatically Report to Regulatory Authorities: AI agents streamline compliance processes by generating and submitting regulatory reports automatically, reducing manual effort.\n\n* Manufacturing \n    * Predictive Maintenance\n        * Machine Sensor Data Analysis: AI agents analyze real-time sensor data from machinery to identify patterns and anomalies that may indicate potential issues.\n        * Failure Prediction: By leveraging advanced machine learning models, AI agents can predict equipment failures before they occur, minimizing downtime.\n        * Preemptive Maintenance Scheduling: AI agents enable manufacturers to schedule maintenance proactively, reducing the risk of unexpected breakdowns and increasing operational efficiency.\n    * Energy Management\n        * Monitor Energy Usage: AI agents continuously monitor energy consumption across production facilities to identify inefficiencies.\n        * Optimize Usage: AI agents provide actionable insights to optimize energy use, reducing costs while maintaining operational requirements.\n        * Adjust Energy-Intensive Processes: By analyzing energy-intensive operations, AI agents can recommend adjustments to improve energy efficiency and   * sustainability.\n    * Process Automation\n        * Material Handling: AI agents automate material handling processes, ensuring precise and efficient movement of goods within facilities.\n        * Assembly Line Operations: AI agents enhance assembly line efficiency by automating repetitive tasks and monitoring workflows for bottlenecks or issues.\n    * Supply Chain Optimization\n        * Demand Forecasting: AI agents predict demand by analyzing historical sales data, market trends, and external factors, ensuring an optimized supply chain.\n        * Logistics Optimization: AI agents plan efficient logistics routes and schedules, reducing transportation costs and delivery times.\n        * Inventory Optimization: By analyzing stock levels and demand patterns, AI agents ensure the right products are available at the right time, minimizing overstock and stockouts.\n    * Product Development\n        * Market Trend Analysis: AI agents analyze market trends and consumer preferences to guide the development of new products.\n        * Consumer Feedback Analysis: By processing customer reviews and feedback, AI agents identify areas for product improvement and innovation.\n        * Rapid Prototyping: AI agents assist in designing and testing prototypes quickly, using data-driven insights to refine product designs.\n    * Quality Control\n        * Production Monitoring: AI agents monitor production processes in real-time to ensure adherence to quality standards.\n        * Defect Detection: Using advanced image recognition, AI agents detect defects in products during manufacturing, improving overall quality.\n        * Process Optimization: AI agents analyze production data to identify inefficiencies, streamline workflows, and reduce waste.\n    \n## Azure AI Agent Service\n\nThe Azure AI Agent Service is a fully managed platform designed to enable developers to build, deploy, and manage intelligent agents efficiently. By leveraging Azure's ecosystem, developers can focus on the logic and behaviors of their agents without worrying about infrastructure and scalability challenges.\n\n### Key Features of Azure AI Agent Service\n\n1. **Agent Lifecycle Management**: Provides tools to simplify the development, deployment, and monitoring of agents across various environments.\n2. **Event-Driven Architecture**: Agents respond to triggers and events, enabling real-time decision-making and interactions.\n3. **Skill Integration and Orchestration**: Enables agents to connect and orchestrate multiple external tools, APIs, and services to extend their functionality.\n4. **State Management**: Supports memory and state retention to ensure agents can maintain context and continuity during interactions.\n5. **Multi-Modal Interactions**: Allows agents to interact through multiple channels, such as chat, voice, and APIs.\n6. **Security and Compliance**: Leverages Azure’s security features to ensure secure and compliant deployment.\n7. **Interoperability**: Seamlessly integrates with Azure OpenAI, Cognitive Services, and other Azure tools to create comprehensive solutions.\n8. **Low-Code/No-Code Options**: Empowers users with varying technical expertise to design and deploy agents using intuitive interfaces and pre-built templates.\n\n### How Azure AI Agent Service Works\n\nWithin Azure AI Foundry, an AI Agent acts as a \"smart\" microservice that can be used to answer questions (RAG), perform actions, or completely automate workflows. It achieves this by combining the power of generative AI models with tools that allow it to access and interact with real-world data sources.\n\nBecause Azure AI Agent Service uses the same wire protocol as Azure OpenAI Assistants, you can use either OpenAI SDKs or Azure AI Foundry SDKs to create and run an agent in just a few lines of code. \n\nLet's do a quick recap of the concepts we will need to understand to use the Azure AI Agent Service with the OpenAI SDK or the AI Foundry SDK:\n\n* **Agent / Assistant***: An AI entity that uses AI models in conjunction with tools. It can be instructed and controlled to perform various tasks.\n* **Tool**:\tTools help extend an agent’s ability to reliably and accurately respond during conversation. Such as connecting to user-defined knowledge bases to ground the model, or enabling web search to provide current information.\n* **Thread**: Represents a conversation session between an AI entity (or Agent) and another agent or user. It stores messages and handles truncation to fit content into a model’s context.\t\n* **Message**:  A message created by an agent or a user. Messages can include text, images, and other files. Messages are stored as a list on the Thread.\n* **Run**: Activation of an agent to begin running based on the contents of Thread. The agent uses its configuration and Thread’s Messages to perform tasks by calling models and tools. As part of a Run, the agent appends Messages to the Thread.\n* **Run Step**: A detailed list of steps the agent took as part of a Run. An agent can call tools or create Messages during its run. Examining Run Steps allows you to understand how the agent is getting to its results.\n\nAzure AI Agent Service provides a more streamlined and secure way to build and deploy AI agents. This includes:\n* Automatic tool calling – no need to parse a tool call, invoke the tool, and handle the response; all of this is now done server-side\n* Securely managed data – instead of managing your own conversation state, you can rely on threads to store all the information you need\n* Out-of-the-box tools – In addition to the file retrieval and code interpreter tools provided by Azure OpenAI Assistants, Azure AI Agent Service also comes with a set of tools that you can use to interact with your data sources, such as Bing, Azure AI Search, and Azure Functions.\n\n### Setup: \n\nThe machines that you will be using, have a predeployed version of the Standard agent setup resource architecture:\n![Standard Agent Setup Resource Architecture](./assets/img/standard-agent-setup-resources.png \"Standard Agent Setup Resource Architecture\")\nResources for the AI hub, AI project, key vault, storage account, AI Services, and AI Search are created for you. The AI Services, AI Search, key vault, and storage account are connected to your project and hub. A gpt-4o-mini model is deployed in eastus region using the Azure OpenAI endpoint for your resource.\n\nBefore creating and running any agents we need to you will need to create a connection string using information from the project that has been created for you. This string is in the format: \n```\n<HostName>;<AzureSubscriptionId>;<ResourceGroup>;<ProjectName>\n```\n You can also find your connection string in the overview for your project in the Azure AI Foundry portal, under Project details > Project connection string.\n![Connection String](./assets/img/portal-connection-string.png \"Connection String\")\n\nNow set this connection string as an environment variable named *PROJECT_CONNECTION_STRING*\n\n   - [🧾 Notebook Lab00 - Intro and Set Up of Azure AI Agents](lab00-intro+setup.ipynb)  \n\n### Getting started with the Azure Agent Service with the OpenAI SDK \nFinally we are able to start executing code.\n\nIn this first example we will be using the OpenAI SDK to create an Agent. \n\nFirst we will start by importing the necessary libraries:\n``` python\nimport os, time\nfrom azure.ai.projects import AIProjectClient\nfrom azure.identity import DefaultAzureCredential\nfrom openai import AzureOpenAI\n```\n\nNext we will :\n```python\nwith AIProjectClient.from_connection_string(\n    credential=DefaultAzureCredential(),\n    conn_str=os.environ[\"PROJECT_CONNECTION_STRING\"],\n) as project_client:\n\n    # Explicit type hinting for IntelliSense\n    client: AzureOpenAI = project_client.inference.get_azure_openai_client(\n        # The latest API version is 2024-10-01-preview\n        api_version = os.environ.get(\"AZURE_OPENAI_API_VERSION\"),\n    )\n\n    with client:\n        agent = client.beta.assistants.create(\n            model=\"gpt-4o-mini\", name=\"my-agent\", instructions=\"You are a helpful agent\"\n        )\n        print(f\"Created agent, agent ID: {agent.id}\")\n\n        thread = client.beta.threads.create()\n        print(f\"Created thread, thread ID: {thread.id}\")\n\n        message = client.beta.threads.messages.create(thread_id=thread.id, role=\"user\", content=\"Hello, tell me a joke\")\n        print(f\"Created message, message ID: {message.id}\")\n\n        run = client.beta.threads.runs.create(thread_id=thread.id, assistant_id=agent.id)\n\n        # Poll the run while run status is queued or in progress\n        while run.status in [\"queued\", \"in_progress\", \"requires_action\"]:\n            time.sleep(1)  # Wait for a second\n            run = client.beta.threads.runs.retrieve(thread_id=thread.id, run_id=run.id)\n            print(f\"Run status: {run.status}\")\n\n        client.beta.assistants.delete(agent.id)\n        print(\"Deleted agent\")\n\n        messages = client.beta.threads.messages.list(thread_id=thread.id)\n        print(f\"Messages: {messages}\")\n```\n\n   - [🧾 Notebook Lab01 - Basics of Azure AI Agents](lab01-basics.ipynb)  \n\n### Getting started with the Azure AI Foundry SDK\n\nNow that we have seen how to create an agent with the OpenAI SDK, let's try to do the same with the Azure AI Foundry SDK: \n```python\nimport os\nfrom azure.ai.projects import AIProjectClient\nfrom azure.ai.projects.models import CodeInterpreterTool\nfrom azure.identity import DefaultAzureCredential\nfrom typing import Any\nfrom pathlib import Path\n\nproject_client = AIProjectClient.from_connection_string(\n    credential=DefaultAzureCredential(), conn_str=os.environ[\"PROJECT_CONNECTION_STRING\"]\n)\n\nwith project_client:\n    # Create an instance of the CodeInterpreterTool\n    code_interpreter = CodeInterpreterTool()\n\n    # The CodeInterpreterTool needs to be included in creation of the agent\n    agent = project_client.agents.create_agent(\n        model=\"gpt-4o-mini\",\n        name=\"my-agent\",\n        instructions=\"You are helpful agent\",\n        tools=code_interpreter.definitions,\n        tool_resources=code_interpreter.resources,\n    )\n    print(f\"Created agent, agent ID: {agent.id}\")\n\n    # Create a thread\n    thread = project_client.agents.create_thread()\n    print(f\"Created thread, thread ID: {thread.id}\")\n\n    # Create a message\n    message = project_client.agents.create_message(\n        thread_id=thread.id,\n        role=\"user\",\n        content=\"Could you please create a bar chart for the operating profit using the following data and provide the file to me? Company A: $1.2 million, Company B: $2.5 million, Company C: $3.0 million, Company D: $1.8 million\",\n    )\n    print(f\"Created message, message ID: {message.id}\")\n\n    # Run the agent\n    run = project_client.agents.create_and_process_run(thread_id=thread.id, assistant_id=agent.id)\n    print(f\"Run finished with status: {run.status}\")\n\n    if run.status == \"failed\":\n        # Check if you got \"Rate limit is exceeded.\", then you want to get more quota\n        print(f\"Run failed: {run.last_error}\")\n\n    # Get messages from the thread\n    messages = project_client.agents.list_messages(thread_id=thread.id)\n    print(f\"Messages: {messages}\")\n\n    # Get the last message from the sender\n    last_msg = messages.get_last_text_message_by_sender(\"assistant\")\n    if last_msg:\n        print(f\"Last Message: {last_msg.text.value}\")\n\n    # Generate an image file for the bar chart\n    for image_content in messages.image_contents:\n        print(f\"Image File ID: {image_content.image_file.file_id}\")\n        file_name = f\"{image_content.image_file.file_id}_image_file.png\"\n        project_client.agents.save_file(file_id=image_content.image_file.file_id, file_name=file_name)\n        print(f\"Saved image file to: {Path.cwd() / file_name}\")\n\n    # Print the file path(s) from the messages\n    for file_path_annotation in messages.file_path_annotations:\n        print(f\"File Paths:\")\n        print(f\"Type: {file_path_annotation.type}\")\n        print(f\"Text: {file_path_annotation.text}\")\n        print(f\"File ID: {file_path_annotation.file_path.file_id}\")\n        print(f\"Start Index: {file_path_annotation.start_index}\")\n        print(f\"End Index: {file_path_annotation.end_index}\")\n        project_client.agents.save_file(file_id=file_path_annotation.file_path.file_id, file_name=Path(file_path_annotation.text).name)\n\n    # Delete the agent once done\n    project_client.agents.delete_agent(agent.id)\n    print(\"Deleted agent\")\n```\n\n### Use other models with the Azure AI Agent Service\n\nThe Azure AI Agent Service also supports the following models from the Azure AI Foundry model catalog:\n* Llama 3.1-70B-instruct\n* Mistral-large-2407\n* Cohere command R+\n\nNow let's create agents using Llama 3 in Azure AI Agents Service. To get started: \n1. Go to ai.azure.com and select **Model catalog** in the left navigation menu, and scroll down to **Meta-Llama-3-70B-Instruct**. You can also use  this link.  \n\n2. Select **Deploy**. \n\n3. In the Deployment options screen that appears, select Serverless API with Azure AI Content Safety. \n\n    ![An image of the llama model project selection  screen](./assets/img/llama-deployment.png)\n \n4. Select your project and click **Subscribe and deploy**. \n\n     ![An image of the llama model deployment screen](./assets/img/llama-deployment-2.png)\n\n5. Add the Serverless connection to your hub / project. The deployment name you choose will be the one you reference in your code.  \n\n6. When calling agent creation API, set the `models` parameter to your deployment name. For example:\n\n    \n    ```python\n    agent = project_client.agents.create_agent( model=\"llama-3\", name=\"my-agent\", instructions=\"You are a helpful agent\" ) \n    ```\n\nFeel free to test with other models\n\n   - [🧾 Notebook Lab02 - How to use other models in Azure AI Agent Service](lab02-models.ipynb)  \n\n\n### Grounding with Bing\n**Grounding with Bing Search** allows your Azure AI Agents to incorporate real-time public web data when generating responses. You need to create a Grounding with Bing Search resource, and then connect this resource to your Azure AI Agents. When a user sends a query, Azure AI Agents decide if Grounding with Bing Search should be leveraged or not. If so, it will leverage Bing to search over public web data and return relevant chunks. Lastly, Azure AI Agents will use returned chunks to generate a response.  \n\nYou can ask questions such as \"*what is the top news today*\" or \"*what is the recent update in the retail industry in the US?*\", which require real-time public data.\n\nDevelopers and end users don't have access to raw content returned from Grounding with Bing Search. The response, however, includes citations with links to the websites used to generate the response, and a link to the Bing query used for the search. These two *References* must be retained and displayed in the exact form provided by Microsoft, as per Grounding with Bing Search's [Use and Display Requirements](https://www.microsoft.com/en-us/bing/apis/grounding-legal#use-and-display-requirements). See the [how to display Grounding with Bing Search results](#how-to-display-grounding-with-bing-search-results) section for details.\n\n>[!IMPORTANT]\n> 1. Your usage of Grounding with Bing Search may incur costs. See the [pricing page](https://www.microsoft.com/bing/apis/grounding-pricing) for details.\n> 1. By creating and using a Grounding with Bing Search resource through code-first experience, such as Azure CLI, or deploying through deployment template, you agree to be bound by and comply with the terms available at https://www.microsoft.com/en-us/bing/apis/grounding-legal, which may be updated from time to time.\n\n\n#### **Setup:**  \n\n> [!NOTE]\n> 1. Grounding with Bing Search only works with the following Azure OpenAI models: `gpt-3.5-turbo-0125`, `gpt-4-0125-preview`, `gpt-4-turbo-2024-04-09`, `gpt-4o-0513`\n\n1. Register the Bing Search provider.\n\n   ```console\n       az provider register --namespace 'Microsoft.Bing'\n   ```\n\n1. Create a new Grounding with Bing Search resource in the [Azure portal](https://portal.azure.com/#create/Microsoft.BingGroundingSearch), and select the different fields in the creation form. Make sure you create this Grounding with Bing Search resource in the same resource group as your Azure AI Agent, AI Project, and other resources.\n\n1. After you have created a Grounding with Bing Search resource, you can find it in [Azure portal](https://portal.azure.com/#home). Navigate to the resource group you've created the resource in, search for the Grounding with Bing Search resource you have created.\n\n   ![alt text](assets/img/resource-azure-portal.png)\n\n1. Select the Grounding with Bing Search resource you have created and copy any of the API keys.\n\n    ![alt text](assets/img/key-resource-azure-portal.png)\n\n1. Go to the [Azure AI Foundry portal](https://ai.azure.com/) and select the AI Project (make sure it's in the same resource group of your Grounding with Bing Search resource). Click **management center**.\n\n    ![alt text](assets/img/project-settings-button.png)\n\n1. Select **+ new connection** in the settings page. \n\n    >[!NOTE]\n    > If you re-generate the API key at a later date, you need to update the connection with the new key.\n\n    ![alt text](assets/img/project-connections.png)\n\n1. Select **API key** in **other resource types**.\n\n    ![alt text](assets/img/api-key-connection.png)\n\n1. Enter the following information and then create a new connection to your Grounding with Bing Search resource.\n\n    - Endpoint: `https://api.bing.microsoft.com/`\n    - Key: `YOUR_API_KEY`\n    - Connection name: `YOUR_CONNECTION_NAME` (You will use this connection name in the sample code below.)\n    - Access: you can choose either *this project only* or *shared to all projects*. Just make sure in the sample code below, the project you entered connection string for has access to this connection.\n\nNow that we have configured everything, let's create a client object, which will contain the connection string for connecting to your AI project and other resources. To make the Grounding with Bing search tool available to your agent, use a connection to initialize the tool and attach it to the agent. You can find your connection in the **connected resources** section of your project in the Azure AI Foundry portal.\n\n\n```python\nimport os\nfrom azure.ai.projects import AIProjectClient\nfrom azure.identity import DefaultAzureCredential\nfrom azure.ai.projects.models import BingGroundingTool\n\n\nproject_client = AIProjectClient.from_connection_string(\n    credential=DefaultAzureCredential(),\n    conn_str=os.environ[\"PROJECT_CONNECTION_STRING\"],\n)\n\nbing_connection = project_client.connections.get(\n    connection_name=os.environ[\"BING_CONNECTION_NAME\"]\n)\nconn_id = bing_connection.id\n\nprint(conn_id)\n\n# Initialize agent bing tool and add the connection id\nbing = BingGroundingTool(connection_id=conn_id)\n\n# Create agent with the bing tool and process assistant run\nwith project_client:\n    agent = project_client.agents.create_agent(\n        model=\"gpt-4o\",\n        name=\"my-assistant\",\n        instructions=\"You are a helpful assistant\",\n        tools=bing.definitions,\n        headers={\"x-ms-enable-preview\": \"true\"}\n    )\n    print(f\"Created agent, ID: {agent.id}\")\n\n# Create thread for communication\nthread = project_client.agents.create_thread()\nprint(f\"Created thread, ID: {thread.id}\")\n\n# Create message to thread\nmessage = project_client.agents.create_message(\n    thread_id=thread.id,\n    role=\"user\",\n    content=\"What is the top news today\",\n)\nprint(f\"Created message, ID: {message.id}\")\n\n# Create and process agent run in thread with tools\nrun = project_client.agents.create_and_process_run(thread_id=thread.id, assistant_id=agent.id)\nprint(f\"Run finished with status: {run.status}\")\n\n# Retrieve run step details to get Bing Search query link\n# To render the webpage, we recommend you replace the endpoint of Bing search query URLs with `www.bing.com` and your Bing search query URL would look like \"https://www.bing.com/search?q={search query}\"\nrun_steps = project_client.agents.list_run_steps(run_id=run.id, thread_id=thread.id)\nrun_steps_data = run_steps['data']\nprint(f\"Last run step detail: {run_steps_data}\")\n\nif run.status == \"failed\":\n    print(f\"Run failed: {run.last_error}\")\n\n# Delete the assistant when done\nproject_client.agents.delete_agent(agent.id)\nprint(\"Deleted agent\")\n\n# Fetch and log all messages\nmessages = project_client.agents.list_messages(thread_id=thread.id)\nprint(f\"Messages: {messages}\")\n```\n> **Important Considerations:** \n>    * According to Grounding with Bing's [terms of use and use and display requirements](https://www.microsoft.com/en-us/bing/apis/grounding-legal#use-and-display-requirements), you need to display both website URLs and Bing search query URLs in your custom interface. You can find website URLs through `annotations` parameter in API response and Bing search query URLs through `runstep` details. To render the webpage, we recommend you replace the endpoint of Bing search query URLs with `www.bing.com` and your Bing search query URL would look like \"https://www.bing.com/search?q={search query}\"\n>    * Microsoft will use data you send to Grounding with Bing to improve Microsoft products and services. Where you send personal data to this service, you are responsible for obtaining sufficient consent from the data subjects. The Data Protection Terms in the Online Services Terms do not apply to Grounding with Bing. \n\n   - [🧾 Notebook Lab03 - How to add real-time knowledge to your Azure AI Agent via Bing (tool)](lab02-models.ipynb)  \n\n### Actions with Code Interpreter\n\nCode Interpreter allows the agents to write and run Python code in a sandboxed execution environment. With Code Interpreter enabled, your agent can run code iteratively to solve more challenging code, math, and data analysis problems. When your Agent writes code that fails to run, it can iterate on this code by modifying and running different code until the code execution succeeds.\n\n> [!IMPORTANT]\n> Code Interpreter has [additional charges](https://azure.microsoft.com/pricing/details/cognitive-services/openai-service/) beyond the token based fees for Azure OpenAI usage. If your Agent calls Code Interpreter simultaneously in two different threads, two code interpreter sessions are created. Each session is active by default for one hour.\n\nTo use code interpreter, first add the `import` statements shown in the example, and create a project client, which will contain a connection string to your AI project, and will be used to authenticate API calls. \n\nThe [models page](https://learn.microsoft.com/en-us/azure/ai-services/agents/quotas-limits) contains the most up-to-date information on regions/models where agents and code interpreter are supported.\n\nWe recommend using Agents with the latest models to take advantage of the new features, larger context windows, and more up-to-date training data.\n\n\n```python\nimport os\nfrom azure.ai.projects import AIProjectClient\nfrom azure.ai.projects.models import CodeInterpreterTool\nfrom azure.ai.projects.models import FilePurpose\nfrom azure.identity import DefaultAzureCredential\nfrom pathlib import Path\n\nproject_client = AIProjectClient.from_connection_string(\n    credential=DefaultAzureCredential(), conn_str=os.environ[\"PROJECT_CONNECTION_STRING\"]\n)\n\nfile = project_client.agents.upload_file_and_poll(\n    file_path=\"nifty_500_quarterly_results.csv\", purpose=FilePurpose.AGENTS\n)\nprint(f\"Uploaded file, file ID: {file.id}\")\n\ncode_interpreter = CodeInterpreterTool(file_ids=[file.id])\n\n# create agent with code interpreter tool and tools_resources\nagent = project_client.agents.create_agent(\n    model=\"gpt-4o-mini\",\n    name=\"my-agent\",\n    instructions=\"You are helpful agent\",\n    tools=code_interpreter.definitions,\n    tool_resources=code_interpreter.resources,\n)\n# create a thread\nthread = project_client.agents.create_thread()\nprint(f\"Created thread, thread ID: {thread.id}\")\n\n# create a message\nmessage = project_client.agents.create_message(\n    thread_id=thread.id,\n    role=\"user\",\n    content=\"Could you please create bar chart in the TRANSPORTATION sector for the operating profit from the uploaded csv file and provide file to me?\",\n)\nprint(f\"Created message, message ID: {message.id}\")\n\n# create and execute a run\nrun = project_client.agents.create_and_process_run(thread_id=thread.id, assistant_id=agent.id)\nprint(f\"Run finished with status: {run.status}\")\n\nif run.status == \"failed\":\n    # Check if you got \"Rate limit is exceeded.\", then you want to get more quota\n    print(f\"Run failed: {run.last_error}\")\n\n# delete the original file from the agent to free up space (note: this does not delete your version of the file)\nproject_client.agents.delete_file(file.id)\nprint(\"Deleted file\")\n\n# print the messages from the agent\nmessages = project_client.agents.list_messages(thread_id=thread.id)\nprint(f\"Messages: {messages}\")\n\n# get the most recent message from the assistant\nlast_msg = messages.get_last_text_message_by_sender(\"assistant\")\nif last_msg:\n    print(f\"Last Message: {last_msg.text.value}\")\n```\n\nFiles generated by Code Interpreter can be found in the Agent message responses. You can download image file generated by code interpreter, by iterating through the response's `image_contents` and calling `save_file()` with a name and the file ID.  \n\n```python\n# save the newly created file\nfor image_content in messages.image_contents:\n  print(f\"Image File ID: {image_content.image_file.file_id}\")\n  file_name = f\"{image_content.image_file.file_id}_image_file.png\"\n  project_client.agents.save_file(file_id=image_content.image_file.file_id, file_name=file_name)\n  print(f\"Saved image file to: {Path.cwd() / file_name}\") \n```\n\nIn the following exercise \n#### **Solution:** \n```python\n\n```\n\n### OpenAPI Actions\n\nYou can now connect your Azure AI Agent to an external API using an OpenAPI 3.0 specified tool, \nallowing for scalable interoperability with various applications. Enable your custom tools \nto authenticate access and connections with managed identities (Microsoft Entra ID) for \nadded security, making it ideal for integrating with existing infrastructure or web services.\n\nOpenAPI Specified tool improves your function calling experience by providing standardized, \nautomated, and scalable API integrations that enhance the capabilities and efficiency of your agent. \n[OpenAPI specifications](https://spec.openapis.org/oas/latest.html) provide a formal standard for \ndescribing HTTP APIs. This allows people to understand how an API works, how a sequence of APIs \nwork together, generate client code, create tests, apply design standards, and more. Currently, we support 3 authentication types with the OpenAPI 3.0 specified tools: `anonymous`, `API key`, `managed identity`.\n\n\n\n1. Check the OpenAPI spec for the following requirements:\n    1. `operationId` should only contain letters, `-` and `_`. You can modify it to meet the requirement. We recommend using descriptive name to help models efficiently decide which function to use.\n\n\n1. Verify that the OpenAPI spec supports API keys: it has `securitySchemes` section and has one scheme of type `apiKey\". An example would be:\n   ```json\n       \"securitySchemes\": {\n          \"apiKeyHeader\": {\n                    \"type\": \"apiKey\",\n                    \"name\": \"x-api-key\",\n                    \"in\": \"header\"\n                }\n        }\n   ```\n   If the security schemes include multiple schemes, we recommend keeping only one of them.\n\n1. Remove any parameter in the OpenAPI spec that needs API key, because API key will be stored and passed through a connection, as described here bellow.\n\n1. Create a `custom keys` connection to store your API key.\n\n    1. Go to the [Azure AI Foundry portal](https://ai.azure.com/) and select the AI Project. Click **connected resources**.\n    ![alt text](assets/img/project-settings-button.png)\n\n    1. Select **+ new connection** in the settings page. \n        >[!NOTE]\n        > If you re-generate the API key at a later date, you need to update the connection with the new key.\n        \n       ![alt text](assets/img/project-connections.png)\n\n   1. Select **custom keys** in **other resource types**.\n    \n        ![alt text](assets/img/api-key-connection.png)\n    \n   1. Enter the following information\n      - key: `name` of your security scheme. In this example, it should be `x-api-key`\n        ```json\n               \"securitySchemes\": {\n                  \"apiKeyHeader\": {\n                            \"type\": \"apiKey\",\n                            \"name\": \"x-api-key\",\n                            \"in\": \"header\"\n                        }\n                }\n        ```\n      - value: YOUR_API_KEY\n      - Connection name: YOUR_CONNECTION_NAME (You will use this connection name in the sample code below.)\n      - Access: you can choose either *this project only* or *shared to all projects*. Just make sure in the sample code below, the project you entered connection string for has access to this connection.\n\nLet's now try this out with the following code:\n```python\nimport os\nimport jsonref\nfrom azure.ai.projects import AIProjectClient\nfrom azure.identity import DefaultAzureCredential\nfrom azure.ai.projects.models import OpenApiTool, OpenApiAnonymousAuthDetails\n\nproject_client = AIProjectClient.from_connection_string(\n    credential=DefaultAzureCredential(),\n    conn_str=os.environ[\"PROJECT_CONNECTION_STRING\"],\n)\n\n# You might want to store the OpenAPI specification in another file and import the content to initialize the tool. Please note the sample code is using `anonymous` as authentication type.\nwith open('./weather_openapi.json', 'r') as f:\n    openapi_spec = jsonref.loads(f.read())\n\n# Create Auth object for the OpenApiTool (note that connection or managed identity auth setup requires additional setup in Azure)\nauth = OpenApiAnonymousAuthDetails()\n\n# Initialize agent OpenApi tool using the read in OpenAPI spec\nopenapi = OpenApiTool(name=\"get_weather\", spec=openapi_spec, description=\"Retrieve weather information for a location\", auth=auth)\n```\nIf you want to use connection, which stores API key, for authentication, replace the line with\n```python\nauth = OpenApiConnectionAuthDetails(security_scheme=OpenApiConnectionSecurityScheme(connection_id=\"your_connection_id\"))\n```\nYour connection ID looks like `/subscriptions/{subscription ID}/resourceGroups/{resource group name}/providers/Microsoft.MachineLearningServices/workspaces/{project name}/connections/{connection name}`.\n\nIf you want to use managed identity for authentication, replace the line with\n```python\nauth = OpenApiManagedAuthDetails(security_scheme=OpenApiManagedSecurityScheme(audience=\"https://your_identity_scope.com\"))\n```\nAn example of the audience would be ```https://cognitiveservices.azure.com/```.\n\nThen create the thread\n```python\n# Create agent with OpenApi tool and process assistant run\nwith project_client:\n    agent = project_client.agents.create_agent(\n        model=\"gpt-4o-mini\",\n        name=\"my-assistant\",\n        instructions=\"You are a helpful assistant\",\n        tools=openapi.definitions\n    )\n    print(f\"Created agent, ID: {agent.id}\")\n\n    # Create thread for communication\n    thread = project_client.agents.create_thread()\n    print(f\"Created thread, ID: {thread.id}\")\n\n# Create a run and observe that the model uses the OpenAPI Spec tool to provide a response to the user's question.\n    message = project_client.agents.create_message(\n        thread_id=thread.id,\n        role=\"user\",\n        content=\"What's the weather in Seattle?\",\n    )\n    print(f\"Created message, ID: {message.id}\")\n\n    # Create and process agent run in thread with tools\n    run = project_client.agents.create_and_process_run(thread_id=thread.id, assistant_id=agent.id)\n    print(f\"Run finished with status: {run.status}\")\n\n    if run.status == \"failed\":\n        print(f\"Run failed: {run.last_error}\")\n\n    # Delete the assistant when done\n    project_client.agents.delete_agent(agent.id)\n    print(\"Deleted agent\")\n\n    # Fetch and log all messages\n    messages = project_client.agents.list_messages(thread_id=thread.id)\n    print(f\"Messages: {messages}\")\n```\n\n### Actions with Azure functions\n\nThe Azure AI Agent Service integrates with Azure Functions, enabling you to create intelligent, event-driven applications with minimal overhead. This combination allows AI-driven workflows to leverage the scalability and flexibility of serverless computing, making it easier to build and deploy solutions that respond to real-time events or complex workflows.\n\nAzure Functions provide support for triggers and bindings, which simplify how your AI Agents interact with external systems and services. Triggers determine when a function executes—such as an HTTP request, message from a queue, or a file upload to Azure Blob Storage and allows agents to act dynamically based on incoming events.\n\nMeanwhile, bindings facilitate streamlined connections to input or output data sources, such as databases or APIs, without requiring extensive boilerplate code. For instance, you can configure a trigger to execute an Azure Function whenever a customer message is received in a chatbot and use output bindings to send a response via the Azure AI Agent.\n\n   - [🧾 Notebook Lab04 - How to make your Azure AI Agent act](lab04-actions.ipynb)  \n\n### Tracing and monitoring\n\n   - [🧾 Notebook Lab05 - Intro to Tracing and Monitoring in Azure AI foundry](lab05-monitoring.ipynb)  \n\n### Multi-agent with SK/Autogen\n\n   + [🧾 Notebook Lab06 - Building MaS with Azure AI Agents & [Autogen v0.4 (New Autogen Architecture) or SK Agentic Framework)]](lab06-multiagents.ipynb)\n\n   > If you need an in-depth hands-on learning lab about AutoGen and Semantic Kernel agentic frameworks, please visit this GitHub repo: https://github.com/pablosalvador10/gbbai-agent-architecture-lab \n\n#### **Formula (and Mental Model) for building MaS**\n\n```\nMulti-Agent Architecture = Σ (Production-Ready Single Agents (with tools, memory, traceability, and isolated execution)) + Preferred Framework (Semantic Kernel, AutoGen, etc.)\n```\n\nAlways start by breaking down the problem into its fundamental components. Identify the decoupled, modular functions that need to be developed, and then design and build the system from a high-level perspective down to the low-level details. In other words:\n\n**First, build single agents:** Ensure that individual agents are scalable and work reliably for the identified specialized task. Leverage Azure AI Agent Service.\n\n**Then, establish connections between agents, if needed:** Begin constructing inter-agent connections as add-on capabilities. This enables agents to collaborate on tasks—either by directly negotiating or by being indirectly guided toward a common goal. This approach will define your overall architecture and design pattern.\n\n#### **Why Start with Single Agents?**\n\nThe `Azure AI Agent Service` makes it straightforward to design agents that are robust, context-aware, and capable of achieving specific goals autonomously. Once these agents successfully handle their tasks as singletons, you can gradually expand them into more complex multi-agent architectures.\n\n**But how do we enable our agents to talk to each other?** In the human world, we might simply put people in the same room to converse naturally. Similarly, agents can be set up to interact and collaborate in virtual rooms, exchanging messages and forming the foundation for advanced, event-driven agent architectures. To achieve this, you can either build your code from scratch or leverage an existing framework. There are many options available, but at Microsoft, we are focusing on two open-source SDKs.\n\n#### **Enable Agent Communication with the Power Duo: Semantic Kernel or AutoGen**\n\n- **AutoGen:**  \nIdeal for creativity and experimentation, AutoGen serves as a state-of-the-art research SDK. It allows you to test new ideas, experiment with collaboration patterns, and push the boundaries of agent capabilities.\n\n- **Semantic Kernel:**  \nProvides an enterprise-grade orchestration framework that supports seamless, non-breaking changes once your ideas are validated. It's designed for production-scale reliability, enabling teams to move quickly without sacrificing stability.\n\n### Bonus exercise: Personal Financial Assistant\n\nIn this exercise the goal is to demonstrates a practical use case where an AI Agent will be leveraged to gain actionable insights and address key analytical questions related to managing an investment portfolio. For this use case we will ask you to combine Function Calling and Code Interpreter. These tools work together to retrieve stock prices and calculate portfolio metrics, replicating real-world workflows in investment management.\n\n#### **Key Steps:**\n1. Upload Investment Data: Import a CSV file containing the user’s investment portfolio into the OpenAI Project.\n1. Fetch Real-Time Stock Prices: Use the Yahoo! Finance API via Function Calling to retrieve up-to-date closing stock data.\n1. Perform Portfolio Analysis: Leverage Code Interpreter to compute key portfolio metrics and insights.\n1. Create Data Visualations: Leverage Code Interpreter to generate portfolio visualization, and leverage Python libraries to render image.\n\n#### **Hints:**\nHere bellow you can find a potential yahoo Finance API function:\n```py\nimport yfinance as yf\n\ndef fetch_stock_price(ticker_symbol: str) -> str:\n    \"\"\"\n    Fetch the latest stock price for a given ticker symbol.\n\n    Parameters:\n    - ticker_symbol (str): The ticker symbol of the stock to retrieve data for.\n\n    Returns:\n    - str: The closing price of the stock for the latest trading day, or an error message if data is unavailable.\n\n    Example:\n    >>> fetch_stock_price(\"AAPL\")\n    \"148.9\"\n    \"\"\"\n    \n    try:\n        # Fetch the stock's trading history for the last day\n        stock = yf.Ticker(ticker_symbol)\n        stock_data = stock.history(period=\"1d\")\n\n        # Check if the data is empty, indicating an invalid ticker or no data available\n        if stock_data.empty:\n            return f\"Error: No data found for ticker symbol: {ticker_symbol}\"\n\n        # Retrieve and return the latest closing price\n        latest_close_price = stock_data['Close'].iloc[-1]\n        return str(round(latest_close_price, 3))\n\n    except KeyError as e:\n        return f\"Error: Data missing for key: {e}. Verify the ticker symbol.\"\n\n    except Exception as e:\n        return f\"Error: Unexpected issue occurred - {type(e).__name__}: {e}\"\n    \nprint(\"Function defined successfully.\")\n```\nAnd its correspondant json to add to the tool list:\n```json\n{\n    \"type\": \"function\",\n    \"function\": {\n        \"name\": \"fetch_stock_price\",\n        \"description\": \"Retrieve the latest closing price of a stock using its ticker symbol.\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\"ticker_symbol\": {\"type\": \"string\", \"description\": \"The ticker symbol of the stock\"}},\n            \"required\": [\"ticker_symbol\"],\n        },\n    },\n}\n```\n\n### Bonus exercise: Sales Analyst Agent\n\nIn this bonus exercise you will have to use of Azure Agent Service to analyze sales data for AdventureWorks. The AI agent will need to performs key tasks, including retrieving files, conducting sales calculations, and generating actionable insights. The key objective here is to upload, and analyze sales data to generate actionable insights like regional revenue metrics."
    },
    {
      "name": "joslat/SemanticKernelAgenticChatWorkflowsExtension",
      "stars": 16,
      "img": "https://avatars.githubusercontent.com/u/9831011?s=40&v=4",
      "owner": "joslat",
      "repo_name": "SemanticKernelAgenticChatWorkflowsExtension",
      "description": "Playing around with extending Semantic Kernel and implementing several agentic chat workflow patterns",
      "homepage": null,
      "language": "C#",
      "created_at": "2024-09-15T15:13:30Z",
      "updated_at": "2025-04-14T17:56:59Z",
      "topics": [],
      "readme": "# SemanticKernelAgenticChatWorkflowsExtension\nPlaying around with extending Semantic Kernel and implementing several agentic chat workflow patterns.\n\nI wanted to implement some advanced patterns with Semantic Kernel and got stuck with some things I could not extend.\nEssentially some classes are sealed and do not have any interfaces to implement.\n\nI have made an issue and a PR to get this in a better, more extensible state.\n- Issue: https://github.com/microsoft/semantic-kernel/issues/8719\n- PR: https://github.com/microsoft/semantic-kernel/pull/8720\n\n## Semantic Kernel extensions\nI have implemented some interfaces so to extend Semantic Kernel and implement several agentic chat workflow patterns.\n- IAgentGroupChat & IAgentChat so I can create customized chat workflows.\n- Implemented a wrapper for the AgentGroupChat, AgentGroupChatEx so I can inject any kind of IAgentGroupChat and extend it\n- Created a AgentGroupChatExt which wraps the AgentGroupChat and implements IAgentGroupChat\n- Created the BaseAgentGroupChat which is a base class for other AgentGroupChat implementations\n- Implemented the SequAgentChat which is a sequential chat workflow.\n- Implemented the TwoAgentChat which is a chat between two agents managed through code (fast).\n- Implemented a SequeentialTwoAgentChat which is a sequential chat between several two agent workflows, through code (fast).\n- Tried to implement a Middleware pattern as it is in AutoGen.NET but did not fully work. Got stuck with some things I could not extend...\n\n## Application\nI have created a WPF application based on a sample from Marco Casalaina exhibited in the Cozy AI Kitchen series from John Maeda.\nSources:\nhttps://www.youtube.com/watch?v=7VCkdxKNBl4\nhttps://techcommunity.microsoft.com/t5/ai-ai-platform-blog/the-future-of-ai-exploring-multi-agent-ai-systems/ba-p/4226593\nhttps://github.com/mcasalaina/QuestionnaireMultiagent\n\nI have extended a bit to plug dynamically different chat workflows and to be able to change the chat workflow on the fly.\nAlso improved the UI to my liking and needs.\n\n## Idea\nI'd like to thank Chris Rickman, https://github.com/crickman, a Microsoft Principal Software Engineer working on the Semantic Kernel team, \nfor the great discussions and his suggestions on how to extend Semantic Kernel which leaded to this repository.\n\n\n## Current implementations (in UI, and workflow providers)\n- Code Crafter workflow (single  - agent) - to improve or program some code\n- Two Agent Chat Workflow - chat between two agents\n- - Sequential Agent Chat Workflow - sequential chat between agents\n- Sequential Two Agent Chat Workflow - sequential chat between a set of a number of two agent chats\n- TestWorkflow - a test workflow to test the chat workflows with a \"normal chat of X Agents\"\n- TestV2ChatWorkflow - IAgentGroupChat Chat group with termination function - just based on the AgentGroupChatExt wrapper"
    },
    {
      "name": "modular/max-recipes",
      "stars": 16,
      "img": "https://avatars.githubusercontent.com/u/150859076?s=40&v=4",
      "owner": "modular",
      "repo_name": "max-recipes",
      "description": "MAX Recipes",
      "homepage": "https://builds.modular.com/?category=recipes",
      "language": "Python",
      "created_at": "2025-01-30T18:28:32Z",
      "updated_at": "2025-04-15T19:06:26Z",
      "topics": [
        "genai",
        "max",
        "modular",
        "recipes"
      ],
      "readme": "# 🚀 MAX Recipes\n\n[![CI](https://github.com/modular/max-recipes/actions/workflows/ci.yml/badge.svg)](https://github.com/modular/max-recipes/actions/workflows/ci.yml)\n\nThis repository contains the accompanying code for [MAX Recipes](https://builds.modular.com/?category=recipes) featured on [MAX Builds](https://builds.modular.com/).\n\nEach recipe is self-contained and demonstrates how to build and deploy AI applications using [MAX](https://docs.modular.com/max/).\nThe recipes are designed to be practical, reusable examples that you can copy, customize, and deploy.\n\n## 🎯 Featured Recipes\n\n| Recipe | Description |\n|--------|-------------|\n| [📚 Auto Documentation Agent](./autodoc-repo-chat-agent/) | 🤖 AI-powered documentation generator with repository understanding agent |\n| [🛡️ Safe Code Executor Agent](./code-execution-sandbox-agent-with-e2b/) | 🔒 Secure code execution agent with sandboxed environment and safety checks |\n| [🌤️ AI Weather Assistant](./ai-weather-agent/) | 🎯 Intelligent weather agent with multi-stage LLM pipeline, semantic caching, and real-time updates |\n| [👁️ Multimodal RAG](./multimodal-rag-with-colpali-llamavision-reranker/) | 🔍 Multimodal RAG with Colpali embedding, Llama3.2-Vision, Qdrant, Reranker and MAX Serve |\n| [⚡ Custom ops AI Applications](./custom-ops-ai-applications/) | 🚀 Top-K token sampler and Flash Attention as fused custom op on GPU (Mojo 🔥 + Python 🐍) |\n| [💻 GPU Functions](./gpu-functions-mojo/) | 🔥 Writing thread-parallel GPU functions and dispatch using MAX Driver API. C -> Mojo patterns |\n| [🌐 Open WebUI and MAX Serve](./max-serve-open-webui/) | 🖥️ Seamlessly integrate Open WebUI with MAX Serve |\n\n## Getting started\n\n1. Install MAX following the [getting started](https://docs.modular.com/max/get-started)\n2. Browse the recipes in this repository\n3. Each recipe is self-sufficient - simply follow the `README.md` in its directory for specific setup and usage instructions\n\n## 🤝 Contributing\n\nWe welcome contributions!\n\n- 🐛 [Report a Bug](https://github.com/modular/max-recipes/issues/new?template=bug_report.yml)\n- 💡 [Request a Feature](https://github.com/modular/max-recipes/issues/new?template=feature_request.yml)\n- 🔧 [Submit a PR](https://github.com/modular/max-recipes/pulls)\n\n## 📚 Resources\n\n- [MAX Documentation](https://docs.modular.com/max/)\n- [MAX Builds](https://builds.modular.com/)\n- [Community Forum](https://forum.modular.com/)\n- [Discord Community](https://discord.gg/modular)\n\n## License\n\nSee the [LICENSE](LICENSE) file for details.\n\n---\n\n<p align=\"center\">\n  Made with ❤️ by the Modular team\n</p>\n"
    },
    {
      "name": "vballoli/autogen-openaiext-client",
      "stars": 16,
      "img": "https://avatars.githubusercontent.com/u/26684166?s=40&v=4",
      "owner": "vballoli",
      "repo_name": "autogen-openaiext-client",
      "description": null,
      "homepage": "",
      "language": "Python",
      "created_at": "2024-11-28T01:32:27Z",
      "updated_at": "2025-03-29T13:24:49Z",
      "topics": [
        "autogen",
        "autogen-extension"
      ],
      "readme": "# autogen-openaiext-client\n\nThis Autogen client is to help interface *quickly* with non-OpenAI LLMs through the OpenAI API.\n\nSee [here](https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.models.openai.html#autogen_ext.models.openai.OpenAIChatCompletionClient) for more information on using with custom LLMs.\n\n> This repository simply include clients you can use to initialize your LLMs easily - since the Autogen >v0.4 supports the non-OpenAI LLMs within the `autogen_ext` package itself with a really nice and clean changes from [jackgerrits](https://github.com/jackgerrits) [here](https://github.com/microsoft/autogen/pull/4856).\n\n\n=======\n# Install\n\n`pip install autogen-openaiext-client`\n\n# Usage\n\n```python\nfrom autogen_openaiext_client import GeminiChatCompletionClient\nimport asyncio\n\n# Initialize the client\nclient = GeminiChatCompletionClient(model=\"gemini-1.5-flash\", api_key=os.environ[\"GEMINI_API_KEY\"])\n\n# use the client like any other autogen client. For example:\nresult = asyncio.run(\n    client.create(\n        [UserMessage(content=\"What is the capital of France?\", source=\"user\")]\n    )\n)\nprint(result.content)\n# Paris\n```\n\nCurrently, `Gemini`, `TogetherAI` and `Groq` clients are supported through the `GeminiChatCompletionClient`, `TogetherAIChatCompletionClient` and `GroqChatCompletionClient` respectively.\n\n\n# Demo\n\n[![YouTube](http://i.ytimg.com/vi/HjAXCcJPa9I/hqdefault.jpg)](https://www.youtube.com/watch?v=HjAXCcJPa9I)\n\n[Magnetic-One](https://github.com/microsoft/autogen/tree/main/python/packages/autogen-magentic-one) example using Gemini client. \n\nInstall [Magentic-One](https://github.com/microsoft/autogen/tree/main/python/packages/autogen-magentic-one) and run `python examples/magentic_one_example.py --hil_mode --logs_dir ./logs` for a complete run.\n\n# Contributing\n\n1. Adding a new model to existing external providers\n   1. For example, adding a new model to `GeminiChatCompletionClient` includes modifying the `GeminiInfo` class in `info.py` and adding the new model to `_MODEL_CAPABILITIES` and `_MODEL_TOKEN_LIMITS` dictionaries.\n2. Adding a new external provider\n   1. Add a new client class in `client.py`, relevant `ProviderInfo` class in `info.py` and add it to `__init__.py` for easy import.\n\n# Disclaimer\n\nThis is a community project for Autogen. Feel free to contribute via issues and PRs and I will try my best to get to it every 3 days.\n\n"
    },
    {
      "name": "wronkiew/autogen-agent-server",
      "stars": 15,
      "img": "https://avatars.githubusercontent.com/u/6463853?s=40&v=4",
      "owner": "wronkiew",
      "repo_name": "autogen-agent-server",
      "description": "Lightweight server for developing conversational agents using Microsoft AutoGen 0.4",
      "homepage": "",
      "language": "Python",
      "created_at": "2025-03-13T04:36:56Z",
      "updated_at": "2025-04-23T06:07:14Z",
      "topics": [
        "ai-agents",
        "ai-agents-framework",
        "autogen-sample",
        "openai-api"
      ],
      "readme": "# AutoGen Agent Server\n\n## Overview\nAutoGen Agent Server is a lightweight and customizable server for developing conversational agents using Microsoft [AutoGen 0.4](https://github.com/microsoft/autogen). Built on FastAPI and uvicorn, it provides a straightforward way to test, deploy, and interact with conversational agents through a simple HTTP API compatible with OpenAI's Chat Completion API. Instead of building a custom UI, you can easily integrate frontends such as [Open WebUI](https://github.com/open-webui/open-webui) or any other OpenAI-compatible client. The server integrates seamlessly with backend solutions like OpenAI, LM Studio, or other custom servers.\n\n## Key Features:\n- **Plugin-Based Design:** Extend agent functionalities with a simple plugin system. Example plugins, including a password-game agent, are provided.\n- **OpenAI-Compatible API:** Implements an HTTP API compatible with OpenAI's Chat Completion API, enabling direct integration with existing clients.\n- **Flexible Configuration:** Configuration managed through Pydantic and environment variables, supporting easy customization and testing.\n- **Streaming and Standard Responses:** Supports both Server-Sent Events (SSE) for streaming and traditional request-response interactions.\n- **Stateless Architecture:** Each interaction is independent, simplifying deployment and scalability.\n- **Efficient Operation:** FastAPI and uvicorn ensure efficient performance and quick response handling.\n\n## 简介\nAutoGen Agent 服务器是一款基于 Microsoft [AutoGen 0.4](https://github.com/microsoft/autogen) 的轻量、可定制化对话代理服务器。项目使用 FastAPI 和 uvicorn 构建，提供了一个简单的平台，帮助开发者测试、部署和交互对话式代理程序。服务器实现了与 OpenAI Chat Completion API 兼容的 HTTP API，您无需创建额外的用户界面，即可与 [Open WebUI](https://github.com/open-webui/open-webui) 或其他兼容的前端直接集成。服务器同时支持与后端服务器（如 OpenAI、LM Studio）或自定义方案的便捷集成。\n\n## 主要功能：\n- **插件化设计：** 通过插件轻松扩展代理功能。项目包含示例插件，例如密码游戏代理，帮助您快速入门。\n- **兼容 OpenAI 的 API：** 提供与 OpenAI Chat Completion API 兼容的 HTTP 接口，方便直接集成现有客户端。\n- **灵活的配置：** 使用 Pydantic 和环境变量进行配置管理，易于测试和自定义部署。\n- **流式和标准响应：** 同时支持流式响应（Server-Sent Events，SSE）和传统的请求-响应交互。\n- **无状态架构：** 每个交互独立进行，简化了部署过程和扩展性。\n- **高效运行：** 利用 FastAPI 和 uvicorn 提供快速响应和高效性能。\n\n---\n\n## Example agents\n\n### Passthrough\nThis is an example passthrough agent. It connects `AssistantAgent` to the server.\n```python\nfrom registry import add_agent, get_default_model\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_core.models import ChatCompletionClient\nfrom autogen_core.model_context import ChatCompletionContext\n\nname = \"hello-world\"\n\n# Constructor for the 'passthrough' agent. Creates a new AssistantAgent that will handle a\n# single user message and connects it to the default backend LLM. The complete conversation\n# history is passed in and loaded by the agent.\ndef create_agent(user_message: str,\n                 context: ChatCompletionContext\n                 ) -> ChatCompletionClient:\n    system_message = ( \"You are a helpful assistant.\" )\n    model_client = get_default_model()\n    return AssistantAgent(name=name, model_client=model_client, model_client_stream=True,\n                          model_context=context, system_message=system_message)\n\n# Register this agent when the module is imported.\nadd_agent(name, create_agent)\n```\n\nLet's try something more interesting.\n\n### Password\n![](https://raw.githubusercontent.com/wronkiew/autogen-agent-server/refs/heads/main/.github/images/password_agent_demo.gif)\n\nThis demonstrates tool use by `AssistantAgent`.\n\n```python\nfrom registry import add_agent, get_default_model\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_core.models import ChatCompletionClient\nfrom autogen_core.model_context import ChatCompletionContext\nimport re\n\nname = \"password\"\n\n# This is a demo agent that plays a simple game. The agent does not know the secret word\n# but can retrieve it by relaying the password from the user.\n\ndef remove_punctuation(text):\n    return re.sub(r'[^\\w\\s]', '', text)\n\n# get_secret is a tool for the LLM to use.\nasync def get_secret(password: str) -> str:\n    \"\"\"If the password is correct, provide the secret word.\"\"\"\n    if remove_punctuation(password) == \"bapple\":\n        return \"The secret word is 'stawberry'\"\n    else:\n        raise Exception(\"Incorrect password\")\n\n# Constructor for the 'password' agent. Creates a new AssistantAgent that will handle a \n# single user message and connects it to the default backend LLM. The complete conversation\n# history is passed in and loaded by the agent.\n# user_message is not used, but it could be if a different agent is needed for certain \n# requests, like generating the conversation title in the web UI.\ndef create_agent(user_message: str,\n                 context: ChatCompletionContext\n                 ) -> ChatCompletionClient:\n    model_client = get_default_model()\n    # system_message is None to bypass the AssistantAgent default message.\n    return AssistantAgent(name=name, model_client=model_client, model_client_stream=True,\n                          model_context=context, tools=[get_secret], reflect_on_tool_use=True,\n                          system_message=None)\n\n# Register this agent when the module is imported.\nadd_agent(name, create_agent)\n```\n\n### Web surfer\nSee the source at [server_agents/web_surfer_agent.py](./server_agents/web_surfer_agent.py).\n\n![](https://raw.githubusercontent.com/wronkiew/autogen-agent-server/refs/heads/main/.github/images/web_surfer_agent_demo.gif)\n\n---\n\n## Installation\n\n1. **Clone the Repository**  \n   ```bash\n   git clone https://github.com/yourusername/autogen-agent-server.git\n   cd autogen-agent-server\n   ```\n\n2. **Ensure Python 3.10+ is Installed**  \n   This project requires Python 3.10 or higher.\n\n3. **Create and Activate a Virtual Environment (Recommended)**  \n   ```bash\n   python3.10 -m venv venv\n   # On Linux/macOS:\n   source venv/bin/activate\n   # On Windows:\n   venv\\Scripts\\activate\n   ```\n\n4. **Install Dependencies**  \n   ```bash\n   pip install -r requirements.txt\n   ```\n\n5. **Set Up Configuration**  \n   1. Copy the provided `.env.example` file to `.env`.\n      ```bash\n      cp .env.example .env\n      ```\n   2. Review and edit any relevant variables in your new `.env` file, such as:\n      - `SERVER_HOST` (default `0.0.0.0`)\n      - `SERVER_PORT` (default `11435`)\n      - `AGENT_DIR` (default `server_agents`)\n      - `OPENAI_API_KEY` must be set, either in the `.env` file, as an environment variable, or as a command-line option\n      - `DEFAULT_LLM` if using an OpenAI backend, or the local LLM configuration variables if using a local backend (e.g., `BACKEND_URL`, `DEFAULT_LLM`, `DEFAULT_LLM_FAMILY`, etc.)\n\n6. **(Optional) Install Playwright for Web Browsing Support**  \n   ```bash\n   playwright install\n   ```\n   This step is only needed if you plan to use agent plugins/tools that require browser automation.\n\n7. **Run the Server**  \n   ```bash\n   python agent_server.py\n   ```\n   The server will start based on the settings from your `.env` file (e.g., `http://127.0.0.1:11435` by default if no port is specified). You can now interact with the API or connect any OpenAI-compatible frontend.\n\n## Docker\n\nThis project includes a Dockerfile to containerize the application. Follow the instructions below to build and run the Docker container.\n\n### Building the Docker Image\n\nFrom the project root, build the Docker image with:\n\n```bash\ndocker build -t autogen-agent-server .\n```\n\n### Running the Docker Container\n\nMount a host directory into the container for your agents. For example, if your host agents directory is located at `/path/to/your/agents`, run:\n\n```bash\ndocker run -d --name agent-server -p 11435:11435 \\\n  -v /path/to/your/agents:/app/server_agents \\\n  autogen-agent-server\n```\n\n- **Volume Mount:**  \n  `-v /path/to/your/agents:/app/agents` mounts your host’s agents directory to `/app/server_agents` inside the container.\n\nThis setup allows you to manage agent files on your host system while the container uses them at runtime.\n\n### Passing Environment Variables from a `.env` File\n\nIf you have a `.env` file with configuration variables (e.g., `OPENAI_API_KEY`, `SERVER_HOST`, etc.), you can pass these to the container using the `--env-file` option. For example, if your `.env` file is located in the project root, run:\n\n```bash\ndocker run -d --name agent-server -p 11435:11435 \\\n  --env-file .env \\\n  -v /path/to/your/agents:/app/server_agents \\\n  autogen-agent-server\n```\n\n- **Environment Variables:**  \n  The `--env-file .env` flag loads all the environment variables defined in your `.env` file into the container."
    },
    {
      "name": "Azure-Samples/multi-agent-workshop",
      "stars": 15,
      "img": "https://avatars.githubusercontent.com/u/1844662?s=40&v=4",
      "owner": "Azure-Samples",
      "repo_name": "multi-agent-workshop",
      "description": "A short set of exercises that showcase the usage of autogen to create agents",
      "homepage": null,
      "language": "Python",
      "created_at": "2025-02-03T14:00:51Z",
      "updated_at": "2025-04-17T11:45:38Z",
      "topics": [],
      "readme": "\r\n# Multi-Agents with Autogen and Semantic Kernel\r\n\r\nThis project showcases autogen (0.4) by presenting a set of simple scripts that create different type of agents and interactions between them with the purpose of highlighting the main capabilities of the agentic framework. This repo is based on [https://github.com/krishsub/MultiagentHackathon](https://github.com/krishsub/MultiagentHackathon).\r\n\r\n## Getting Started\r\n\r\n### Prerequisites\r\n\r\n- Open AI service deployed with GPT-4o\r\n- Azure Container Apps Session Pool\r\n\r\n### Development environment\r\n\r\n- Dev container option: The project provides a devcontainer configuration that can be used with github codespaces or your own local dev container. So, if you opt for this, you need to have docker on your system.\r\n- Virtual Environment option: if you have python > 3.8 you could choose to create a venv and install all the requirements there.\r\n\r\n#### Leverage AZD to deploy the prerequisites\r\n\r\nYou can leverage the [Azure Developer CLI](https://learn.microsoft.com/azure/developer/azure-developer-cli/), `azd` for short, to deploy the prerequisites to a subscription. It'll create the resources and export some environment variables which you can use to run the exercises. To leverage `azd` you need to have it installed and configured. After that, all it takes is a simple `azd up` and the components will be installed.\r\n\r\nFor a detailed explanation on what it deploys, check out the [README.md in the infra directory](/infra/README.md).\r\n\r\nAfter the components have been deployed, you can navigate to [AI Foundry](https://ai.azure.com/) and obtain the Open AI Key and Endpoint. The Open AI endpoint, as well as the Azure Container Apps endpoint, will be stored in the `azd` environment variables. You can leverage those as well. The Open AI Key is not exposed in this manner for security considerations.\r\n\r\n### Quickstart\r\n\r\n#### Environment Variables\r\n\r\nFor this set of scripts, a gpt-4o model instance was used.\r\nCreate a `.env` file with your Azure OpenAI credentials in the `src` folder:\r\n\r\n```bash\r\nAZURE_OPENAI_API_KEY=your_api_key\r\nAZURE_OPENAI_ENDPOINT=your_endpoint\r\n```\r\n\r\n#### Installation\r\n\r\nChoose one of the following methods to run the different scripts. Make sure to `cd src`.\r\n\r\n#### Option 1: Using uv (Recommended)\r\n\r\n[uv](https://github.com/astral-sh/uv) is a fast Python package installer and runner. If you haven't installed it yet:\r\n\r\n```bash\r\ncurl -LsSf https://astral.sh/uv/install.sh | sh\r\n```\r\n\r\nThen run the script directly (this will automatically install dependencies):\r\n\r\n```bash\r\nuv run 0X_SCRIPT_NAME.py\r\n```\r\n\r\n#### Option 2: Using pip\r\n\r\n1. Install dependencies:\r\n\r\n   ```bash\r\n   pip install -r requirements.txt\r\n   ```\r\n\r\n2. Run the script:\r\n\r\n   ```bash\r\n   python 0X_SCRIPT_NAME.py\r\n   ```\r\n\r\n\r\n# Multi-Agent Hackathon Guide\r\n\r\n## Introduction\r\n\r\nWelcome to the MultiagentHackathon workshop! This project is designed to help you learn and practice implementing multi-agent systems using frameworks like AutoGen, Semantic Kernel, and more. The repository contains a series of progressive exercises that will guide you through building increasingly complex agent systems, from simple single-agent interactions to sophisticated multi-agent collaborative scenarios where code is executed in a dynamic Azure Container Apps Pool.\r\n\r\n## Setup\r\n\r\n### Prerequisites\r\n\r\n- [Python](https://www.python.org/) 3.9+ installed\r\n- An Azure OpenAI API key or OpenAI API key (For production ready deployments, you should refrain from using keys, and switch to [managed identities](https://learn.microsoft.com/entra/identity/managed-identities-azure-resources/overview))\r\n- [Visual Studio Code](https://code.visualstudio.com/)\r\n\r\n### Installation\r\n\r\n1. Open Visual Studio Code (Link in your desktop)\r\n2. Open a Powershell terminal using the upper menu `Terminal -> New Terminal` or by pressing CTRL+SHIFT+` (backtick). Make sure to select the Powershell terminal in the dropdown menu on the top right of the terminal window.\r\n3. Clone the repository:\r\n\r\n   ```Powershell\r\n   git clone https://github.com/Azure-Samples/multi-agent-workshop\r\n   cd multi-agent-workshop\r\n   ```\r\n\r\n4. Select open folder in your VS Code and open the `multi-agent-workshop` folder to see the code in your VS Code.\r\n5. (**Optional, and takes some time**) Open your code in a devcontainer, using the Dev Containers plugin of VS Code and the devcontainer provided in the repo. Once the plugin is installed, if you open the `.devcontainer/devcotainer.json` file, it should ask you to re-open your repo in a devcontainer.\r\n6. Install depedencies (might take a few minutes):\r\n\r\n    [uv](https://github.com/astral-sh/uv) is a fast Python package installer and runner. If you haven't installed it yet:\r\n\r\n    ```Powershell\r\n    powershell -ExecutionPolicy ByPass -c \"irm https://astral.sh/uv/install.ps1 | iex\"\r\n    $env:Path += \";C:\\Users\\Admin\\.local\\bin\"\r\n    uv sync\r\n    ```\r\n\r\n    **OR (without using uv)**\r\n\r\n    ```bash\r\n        python -m venv .venv\r\n    ```\r\n\r\n    ```powershell\r\n        # On Windows\r\n        .venv\\Scripts\\activate\r\n    ```\r\n\r\n    ```bash\r\n        # On macOS/Linux\r\n        source venv/bin/activate\r\n    ```\r\n\r\n    ```powershell\r\n      curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py\r\n      python get-pip.py\r\n      pip --version\r\n    ```\r\n\r\n    ```bash\r\n        pip install -r requirements.txt\r\n    ```\r\n\r\n### Infrastructure Setup\r\n\r\n#### Infrastructure as Code (Option 1 - Preferred)\r\n\r\nYou can leverage the [Azure Developer CLI](https://learn.microsoft.com/azure/developer/azure-developer-cli/), `azd` for short, to deploy the prerequisites to a subscription. It'll create the resources and export some environment variables which you can use to run the exercises. To leverage `azd` you need to have it installed and configured. After that, all it takes is a simple `azd up` and the components will be installed.\r\n\r\nTo install azd you can execute the following command in Powershell.\r\n\r\n```Powershell\r\npowershell -ex AllSigned -c \"Invoke-RestMethod 'https://aka.ms/install-azd.ps1' | Invoke-Expression\"\r\n```\r\n\r\nIf using Linux, you can run:\r\n\r\n```bash\r\ncurl -fsSL https://aka.ms/install-azd.sh | bash\r\n```\r\n\r\nFor a detailed explanation on what it deploys, check out the [README.md in the infra directory](/infra/README.md).\r\n\r\nAfter the components have been deployed, you can navigate to [AI Foundry](https://ai.azure.com/) and obtain the Open AI Key and Endpoint. The Open AI endpoint, as well as the Azure Container Apps endpoint will be stored in the azd environment variables. You can leverage those as well. The Open AI Key is not exposed in this manner for security considerations.\r\n\r\n**Restart your VS Code** before running the next commands in the root folder of the repo.\r\n\r\n```Powershell\r\nazd auth login\r\n```\r\n\r\nTo login follow the instructions and use the credentials for your Azure Subscription from the `Resources` tab\r\n\r\n```Powershell\r\nazd up  \r\n```\r\n\r\nAnd follow the instructions in the terminal to name your environment resources, make sure to select the correct subscription. We recommend choosing `Sweden Central` as Location.\r\n\r\n#### Azure Portal (Option 2)\r\n\r\n1. Navigate to the Azure Portal from the browser of your skillable lab `https://portal.azure.com/#home`\r\n2. Login using the Username and Password available in the Resources tab\r\n3. Open a new tab in your browser and go to `https://ai.azure.com` (you should be logged in already)\r\n4. Create a new project, it should also setup a hub for you.\r\n![Project Creation AI Foundry](images/foundry_project.jpeg)\r\n5. Deploy a GPT-4o model. You can follow the numbered steps in the screenshot below:\r\n![Create deployment](images/foundry_create_deployment.jpeg)\r\n6. Go back to the Azure Portal, and create a Container App Session Pool. The screenshots below can guide you in the process:\r\n![Search Dynamic Pools](images/search_dynamic_pools.jpeg)\r\n\r\n![Create Session Pool](images/create_session_pool.jpeg)\r\n\r\n![Parameters Session Pool](images/parameters_session_pool.jpeg)\r\n\r\n### Configuring your repository\r\n\r\n1. Navigate to the *Azure Portal -> Resource Groups -> YOUR_RESOURCE_GROUP -> openai-YOUR_OWN_SUFFIX -> Explore Azure AI Foundry portal*. Make a note of the endpoint and key, you need then for step 3.  \r\n\r\nYou can also just go to `ai.azure.com` and in the overview of your project you should be able to see your credentials, similar to the ones visible in the picture below. \r\n\r\nThe following screenshot show you how to get your credentials:\r\n   ![Azure Portal Credentials](images/get_ai_credentials.jpeg)\r\n\r\n2. Navigate to *Azure Portal -> Resource Groups -> YOUR_RESOURCE_GROUP -> aca_pool-YOUR_OWN_SUFFIX* and get the Pool Management Endpoint.\r\n   ![Azure Portal Credentials](images/aca_pool_credentials.jpeg)\r\n\r\n3. Set up environment variables:\r\n   - Create a `.env` file in your `exercises` directory.\r\n   - Add your API keys and endpoints (you could get this from the Azure Portal, make sure to have the values between quotes):\r\n\r\n     ``` commandline\r\n     AZURE_OPENAI_URL=your_azure_endpoint\r\n     AZURE_OPENAI_API_KEY=your_azure_api_key\r\n     ACA_POOL_MANAGEMENT_ENDPOINT=you_ACA_pool_endpoint\r\n     ```\r\n\r\n## Repository Structure\r\n\r\n- [`src/`](/src/): Contains working examples of each exercise. We recommend you to not copy-paste solutions, but only look at this folder when you are stuck with an exercise and need some inspiration.\r\n- [`exercises/`](/exercises/): Contains exercise templates for you to complete.\r\n\r\n## Getting Started\r\n\r\nStart with the first exercise and progress through them sequentially. Each exercise builds upon concepts introduced in the previous ones. Click Next when you are ready to continue.\r\n\r\n===\r\n## Introduction to Azure AI Agent Service \r\n\r\nAzure AI Agent Service is a fully managed service designed to empower developers to securely build, deploy, and scale high-quality, extensible AI agents without needing to manage the underlying compute and storage resources. This service simplifies the process of creating AI agents that can answer questions, perform actions, or automate workflows by combining generative AI models with tools that allow interaction with real-world data sources. \r\n\r\nAgent service offers several benefits:\r\n- Ease of Use: What originally took hundreds of lines of code to support client-side function calling can now be done in just a few lines of code.\r\n- Scalability: As a fully managed service, it allows you to focus on building workflows and agents without worrying about scaling, security, or infrastructure management.\r\n- Integration: It uses the same wire protocol as Azure OpenAI Assistants, enabling seamless integration with OpenAI SDKs or Azure AI Foundry SDKs.For example, to create an AI Agent with Azure AI Foundry SDK, you can simply define which model the AI uses, the instructions for how it should complete tasks, and the tools it can use to access and interact with other services. \r\n\r\n### Getting started with Azure AI Agent Service in the AI Foundry portal\r\n\r\n1. Navigate to the Agents Playground underneath \"Build and customize\" to select the gpt-4o model created through the portal earlier. \r\n\r\n![Agent Service Start](images/agent_service_1.png)\r\n\r\n2. After selecting next you will receive a confirmation message that your agent is created, and your agent will also automatically be given a unique ID. Select the agent you just created and then click \"Try in Playground\" to the left.\r\n![Agent Service Start in Playground](images/agent_service_2.png)\r\n\r\n3. Once in the playground view add the following description to the instructions for the agent (in the `Instructions` placeholder on the right side of the screen):\r\n\r\n```\r\nYou are a helpful assistant that can search files and answer questions based on the file content. You can utilize file search grounded on internal data to efficiently search through proprietary documents and provide accurate answers based on the content. This feature allows you to manage and index uploaded files for efficient keyword and semantic search, enhancing your ability to assist users with their queries.\r\n            \r\nYou have access to a code interpreter tool that allows you to analyze data, create visualizations, and perform calculations. Be thorough and precise in your answers. For CSV and structured data, provide meaningful insights and summaries when appropriate.\r\n\r\nUse the code interpreter when:\r\n   1. Analyzing numerical data or statistics in CSV or JSON files\r\n   2. Creating visualizations of data when it would help explain your answer\r\n   3. Performing calculations or transformations on data\r\n   4. Extracting specific information from complex structured data\r\n\r\n When using the code interpreter, follow these guidelines:\r\n   - Write clear, concise code with comments explaining your approach\r\n   - Use pandas for data analysis, matplotlib/seaborn for visualizations\r\n   - Use descriptive variable names and follow best practices\r\n   - Show intermediate steps for complex analyses\r\n   - Interpret results for the user in plain language after showing code output\r\n            \r\nWhen creating visualizations:\r\n   - Use clear titles, axis labels, and legends\r\n   - Choose appropriate chart types for the data\r\n   - Use matplotlib or seaborn for creating visualizations\r\n\r\n```\r\n\r\n4. Add a knowledge source for the agent and a code interpreter tool by clicking the + next to Knowledge & Actions. \r\n\r\n- When you upload data for file search in Azure AI Agent Service, a vector store is created to manage and search the uploaded files. This process involves several steps to ensure the data is properly indexed and searchable.\r\n- First, the uploaded files are automatically parsed and chunked into smaller segments. Each chunk is then embedded, meaning it is converted into a vector representation that captures the semantic meaning of the content. These vectors are stored in a vector database, which supports both keyword and semantic search capabilities\r\n- Once the vector store is created, it can be attached to both agents and threads, allowing for seamless integration and efficient file management across different AI applications\r\n- The code interpreter tool enables agents to write and run Python code in a sandboxed environment. This feature is useful for tasks like generating graphs, performing calculations, and analyzing datasets. Agents can iteratively modify and rerun code until execution succeeds.\r\n\r\n![Agent Service Add Data & Tool](images/agent_service_3.png)\r\n\r\nFor the Knowledge Source click Files, and then Select Local Files to add and upload the `multi-agent-workshop/data/Internal Policy Document for Contoso Tech Support Agents.pdf` \r\n\r\n![Agent Service Knowledge Source](images/agent_service_4.png)\r\n\r\nFor the Actions, select Code Interpreter and then to add a Code Interpreter Action select `multi-agent-workshop/data/Contoso_Tech_Product_Data.csv` from local files and upload it to the Code Interpreter tool\r\n![Agent Service Code Interpreter](images/agent_service_5.png)\r\n\r\nFor context on the two files we have just uploaded:\r\n- The pdf file is an Internal Policy Document for Contoso Tech Support Agents, which outlines the policies and procedures that support agents must follow when assisting customers. It covers key guidelines for handling returns, processing warranty claims, shipping options, order tracking, privacy policy, and customer support procedures. \r\n- The csv file provided contains sales data for various products sold by Contoso Tech. Each product is listed with its name, category, price, units sold, and the quarter in which the sales occurred. This data provides insights into the sales performance of different products over specific quarters. \r\n\r\n5. Chat with your agent \r\nNow you're free to ask your agent questions based on the data provided, here are some sample questions to try out and see how the agent responds. We can also ask for analysis based on the data provided. \r\n\r\nQuestions for the policy document:\r\n```\r\nWhat are the key guidelines for handling returns at Contoso Tech?\r\n\r\nWhich items are non-refundable according to Contoso Tech's policy?\r\n\r\nWhat are the contact details for Contoso Tech's support team?\r\n```\r\n\r\nPrompts for the laptop data:\r\n```\r\nCreate a bar chart highlighting the top 5 products with the highest units sold.\r\n\r\nGenerate a pie chart showing the units sold for each product category. Include the number of units per category as well as percentages.\r\n```\r\n\r\nIn this section we explored the Azure AI Agent Service, focusing on creating agents and adding tools like file search for searching through proprietary data and the code interpreter for data analysis. These features enable efficient data management, automated workflows, and insightful visualizations. The agent service playground provides a robust environment for experimenting with these tools and enhancing your projects. In the following section we explore how to get started with running and creating agents from a code-first point of view. \r\n\r\n===\r\n## Exercise 0: Call a model\r\n\r\n### Objective\r\n\r\nLearn how to call your LLM model without using agents.\r\n\r\n### Instructions\r\n\r\nRefer to [`exercises/00_call_models.py`](/exercises/00_call_models.py) for a complete example.\r\n\r\n```Powershell\r\n uv run .\\exercises\\00_call_models.py\r\n```\r\n\r\nOR (if you are not using uv)\r\n\r\n```Powershell\r\npython .\\exercises/00_call_models.py\r\n```\r\n\r\nThis is a very simple script that only calls the LLM deployed. It should serve as the starting point of the next exercises and should validate the connection to your LLM. If the code runs properly, you should see a joke created by the model in the terminal.\r\n\r\n===\r\n\r\n## Exercise 1: Single Agent\r\n\r\n### Objective\r\n\r\nLearn how to create and interact with a single AI agent.\r\n\r\n### Instructions\r\n\r\nRefer to [`exercises/01_single_agent.py`](/exercises/01_single_agent.py) for a complete example.\r\n\r\nThis is the foundation of agent-based systems. Understand how a basic agent system works before proceeding to more complex multi-agent scenarios.\r\n\r\n===\r\n\r\n## Exercise 2: Two Agents\r\n\r\n### Objective\r\n\r\nImplement a conversation between two agents (Chandler and Joey) who exchange jokes.\r\n\r\n### Instructions\r\n\r\n1. Open [`exercises/02_two_agents.py`](/exercises/02_two_agents.py)\r\n2. Complete the TODOs in the file:\r\n   - Create a `ChatCompletionClient` using the provided `llm_config`\r\n   - Create two `AssistantAgent` instances with appropriate system messages:\r\n     - Chandler should tell short story jokes related to friends\r\n     - Joey should respond to jokes with another joke\r\n     - Both should be able to end the conversation after 2 jokes by saying 'FINISH'\r\n   - Create a termination condition using `TextMentionTermination`\r\n   - Create a `RoundRobinGroupChat` team with both agents\r\n   - Run the conversation and print the results\r\n   - Reset the team and run another conversation as a stream\r\n3. Documentation is available here: [Autogen Docs](https://microsoft.github.io/autogen/stable/index.html)\r\n\r\n### Expected Outcome\r\n\r\nTwo agents exchanging jokes in a structured conversation that terminates after a set number of exchanges.\r\n\r\n===\r\n\r\n## Exercise 3: Two Agents Guessing Game\r\n\r\n### Objective\r\n\r\nCreate a number guessing game where two agents interact: one tries to guess a random number, and the other provides feedback.\r\n\r\n### Instructions\r\n\r\n1. Open [`exercises/03_two_agents_guessing_game.py`](/exercises/03_two_agents_guessing_game.py)\r\n2. Complete the TODOs in the file:\r\n   - Create an OpenAI model client using `ChatCompletionClient.load_component()`\r\n   - Create a guesser agent that tries to guess a number between 1-100\r\n   - Create a player agent that provides feedback on guesses (too high/too low)\r\n   - Set up a termination condition that ends the game when 'FINISH' is mentioned\r\n   - Create a team with the two agents using `RoundRobinGroupChat`\r\n   - Uncomment the code that runs the team chat to test your implementation\r\n3. Documentation is available here: [Autogen Docs](https://microsoft.github.io/autogen/stable/index.html)\r\n\r\n### Expected Outcome\r\n\r\nA functional guessing game where agents take turns until the correct number is guessed.\r\n\r\n===\r\n\r\n## Exercise 4: Generate and Run Code in Conversations\r\n\r\n### Objective\r\n\r\nBuild a system with two agents that can write and execute code collaboratively.\r\n\r\n### Instructions\r\n\r\n1. Open [`exercises/04_generate_and_run_code_in_conversations.py`](/exercises/04_generate_and_run_code_in_conversations.py)\r\n2. Complete the TODOs:\r\n   - Create a system message for the code writer agent with specific instructions\r\n   - Set up a local command line executor using `LocalCommandLineCodeExecutor`\r\n   - Create a code executor agent that uses this executor\r\n   - Create a code writer agent with the system message defined earlier\r\n   - Implement a termination condition for when \"FINISH\" is mentioned\r\n   - Create a team with round-robin chat including both agents\r\n3. Documentation is available here: [Autogen Docs](https://microsoft.github.io/autogen/stable/index.html)\r\n\r\n### Expected Outcome\r\n\r\nA system where one agent proposes Python code to calculate the 14th Fibonacci number, and another agent executes it. It is important to note that in this example, the `code_executor_agent` executes the code locally, can you think about reasons to avoid this?\r\n\r\n===\r\n\r\n## Exercise 5: Custom Agents Run Code\r\n\r\n### Objective\r\n\r\nImplement custom agents with code execution capabilities using the AutoGen Core framework. You can think of this code as an evolved or more production ready version of the previous exercise, where the agents interact for a longer time to solve a more complex problem. You will notice that the Assistant agent keeps track of the history of the conversation and keep working on the problem until the executor is able to provide the expected output.\r\n\r\n### Instructions\r\n\r\n1. Open [`exercises/05_custom_agents_run_code.py`](/exercises/05_custom_agents_run_code.py)\r\n2. Complete the TODOs:\r\n   - Initialize chat history with a SystemMessage in the Assistant class\r\n   - Implement message handling logic for the Assistant\r\n   - Create a function to extract code blocks from markdown text\r\n   - Implement the message handler for the Executor agent\r\n   - Complete the main function to set up and run the coding agents\r\n   - Call the `coding_agents` function with `asyncio.run()`\r\n3. Documentation is available here: [Autogen Docs](https://microsoft.github.io/autogen/stable/index.html)\r\n\r\n### Expected Outcome\r\n\r\nA system where an assistant generates code in markdown blocks, and an executor extracts and runs that code. After the code is executed succesfully, you should be able to see the output in the [`exercises/generated`](/exercises/generated/) folder.\r\n\r\n===\r\n\r\n## Exercise 6: Human in the Loop\r\n\r\n### Objective\r\n\r\nCreate a human-in-the-loop interaction between an assistant agent and a user proxy agent.\r\n\r\n### Instructions\r\n\r\n1. Open [`exercises/06_human_in_the_loop.py`](/exercises/06_human_in_the_loop.py)\r\n2. Complete the TODOs:\r\n   - Create a `ChatCompletionClient` using the provided LLM configuration\r\n   - Initialize an `AssistantAgent` with the created client\r\n   - Initialize a `UserProxyAgent` to get user input from the console\r\n   - Create a termination condition that ends when the user says \"APPROVE\"\r\n   - Create a `RoundRobinGroupChat` team with the assistant and user proxy agents\r\n   - Run the conversation and stream to the console\r\n3. Documentation is available here: [Autogen Docs](https://microsoft.github.io/autogen/stable/index.html)\r\n\r\n### Expected Outcome\r\n\r\nAn interactive session where a human can converse with an AI assistant until they approve the results.\r\n\r\n===\r\n\r\n## Exercise 7: Functions Invoked by Agents\r\n\r\n### Objective\r\n\r\nImplement a function that can be invoked by an agent and configure the agent to use it.\r\n\r\n### Instructions\r\n\r\n1. Open [`exercises/07_functions_invoked_by_agents.py`](/exercises/07_functions_invoked_by_agents.py)\r\n2. Complete the TODOs:\r\n   - Implement the calculator function to perform basic arithmetic operations\r\n   - Initialize the `AssistantAgent` with proper configuration:\r\n     - Give it an appropriate name\r\n     - Write a system message instructing it to use the calculator and get_time tool\r\n     - Set up the model client\r\n     - Add the calculator function to the tools list\r\n     - Configure whether the agent should reflect on tool use\r\n   - Process user input and get the assistant's response\r\n3. Documentation is available here: [Autogen Docs](https://microsoft.github.io/autogen/stable/index.html)\r\n\r\n### Expected Outcome\r\n\r\nAn agent that can perform calculations using a custom calculator function when prompted.\r\n\r\n===\r\n\r\n## Exercise 8: Generate Run Code in Remote Container on ACA\r\n\r\n### Objective\r\n\r\nLearn how to execute code in a remote Azure Container Apps environment for secure and isolated execution.\r\n\r\n### Troubleshooting\r\n\r\nThis is the first time that you will interact with the ACA Session Pool, thus, you might require to run the following commands beforehand (run only if you don't have Azure CLI installed):\r\n\r\nOpen a Windows Powershell terminal as Administrator and run the command below to install the azure cli:\r\n\r\n```Powershell\r\n$ProgressPreference = 'SilentlyContinue'; Invoke-WebRequest -Uri https://aka.ms/installazurecliwindows -OutFile .\\AzureCLI.msi; Start-Process msiexec.exe -Wait -ArgumentList '/I AzureCLI.msi /quiet'; Remove-Item .\\AzureCLI.msi\r\n```\r\nIn your VS Code, close the current terminal and open a new one to make sure that the PATH is updated, and then run: \r\n\r\n```Powershell\r\naz login \r\n## Select your subscription from the list and continue\r\n```\r\nIt might be the case that your uv is not on your PATH after killing your current VS Code terminal, if that is the case, run again:\r\n\r\n```Powershell\r\n$env:Path += \";C:\\Users\\Admin\\.local\\bin\"\r\n```\r\nIf you experiment issues when running the code in the container, due to lack of permissions, you might need to add the `Azure ContainerApps Session Executor` to your user in the ACA Dynamic Pool, use the user that you have in the `Resources` tab.\r\n\r\n![Add Role 1](images/add_role_1.jpeg)\r\n![Add Role 2](images/add_role_2.jpeg)\r\n![Add Role 3](images/add_role_3.jpeg)\r\n\r\n### Instructions\r\n\r\n1. Open [`exercises/08_generate_run_code_in_remote_container_on_aca_langchain.py`](/exercises/08_generate_run_code_in_remote_container_on_aca_langchain.py)\r\n2. Implement the `RemoteExecutor` class:\r\n   - Initialize parameters for connection to Azure Container Apps\r\n   - Implement the `execute_code_blocks` method to run code in a remote container\r\n3. Complete the `run_remote_coding_agents` function:\r\n   - Load environment variables\r\n   - Set up the agent runtime\r\n   - Initialize the model client and remote executor\r\n   - Register the assistant and executor agents\r\n   - Start the runtime and publish an initial message\r\n4. Documentation is available here: [Autogen Docs](https://microsoft.github.io/autogen/stable/index.html); [LangChain](https://api.python.langchain.com/en/latest/tools/langchain_azure_dynamic_sessions.tools.sessions.SessionsPythonREPLTool.html)\r\n\r\n### Expected Outcome\r\n\r\nA system that can generate and execute code in a secure, remote container environment.\r\n\r\n===\r\n\r\n## Exercise 9: Group Chat Coding Problem with Semantic Kernel\r\n\r\n### Objective\r\n\r\nSet up a group chat between two agents using Semantic Kernel to solve coding problems.\r\n\r\n### Instructions\r\n\r\n1. Open [`exercises/09_group_chat_coding_problem_sk.py`](/exercises/09_group_chat_coding_problem_sk.py)\r\n2. Implement the missing functions:\r\n   - `create_code_agent`: Create an agent specialized in executing Python code\r\n     - Configure the Python code interpreter tool\r\n     - Add the code interpreter plugin to the kernel\r\n     - Create a ChatCompletionAgent with appropriate description\r\n   - `create_chat_agent`: Create a chat agent that interacts with users and coordinates with the code agent\r\n   - Complete the `main` function to:\r\n     - Create the agents\r\n     - Set up an agent group chat\r\n     - Create a chat history with initial system message\r\n     - Add the user question and start the group chat\r\n     - Display messages from each agent\r\n3. Documentation is available here: [Autogen Docs](https://microsoft.github.io/autogen/stable/index.html); [Semantic Kernel](https://learn.microsoft.com/en-us/semantic-kernel/)\r\n\r\n### Expected Outcome\r\n\r\nA group chat where agents collaborate to solve coding problems, with one agent generating code and another executing it.\r\n\r\n===\r\n\r\n## Conclusion\r\n\r\nCongratulations on completing the MultiagentHackathon exercises! You've learned how to create and manage various types of agent systems, from simple single-agent interactions to complex multi-agent collaborations with code execution capabilities.\r\n\r\n### Next Steps\r\n\r\n1. Try modifying the agents' system prompts to see how it affects their behavior\r\n2. Experiment with different termination conditions\r\n3. Create your own multi-agent system for a specific use case\r\n4. Explore more advanced features like:\r\n   - Memory and state management\r\n   - Tool use and function calling\r\n   - Integration with external APIs\r\n\r\nWe welcome your contributions and feedback to improve this hackathon. Please submit issues or pull requests to the repository.\r\n\r\nHappy coding!\r\n"
    },
    {
      "name": "SongChiYoung/autogen-oaiapi",
      "stars": 14,
      "img": "https://avatars.githubusercontent.com/u/7540984?s=40&v=4",
      "owner": "SongChiYoung",
      "repo_name": "autogen-oaiapi",
      "description": "An OpenAI-compatible API server for AutoGen",
      "homepage": "",
      "language": "Python",
      "created_at": "2025-04-12T08:40:26Z",
      "updated_at": "2025-04-23T06:12:11Z",
      "topics": [
        "autogen",
        "autogen-extension"
      ],
      "readme": "# 🧠 autogen-oaiapi\n\nOpenAI-style Chat API server for [AutoGen](https://github.com/microsoft/autogen) teams.  \nDeploy your own `/v1/chat/completions` endpoint using any AutoGen-compatible team configuration.\n\n---\n\n## ✨ Features\n\n- ✅ **OpenAI-compatible** API interface\n- ✅ Plug in any AutoGen `GroupChat` or `SocietyOfMindAgent`\n- ✅ Session-aware execution (per session_id)\n- ✅ FastAPI-based server with `/v1/chat/completions` endpoint\n- ✅ `stream=True` response support (coming soon)\n\n---\n\n## 📦 Installation\n```shell\npip install autogen-oaiapi\n```\n\n---\n\n## How to use?\nUsing just `SIMPLE` api!\n\nexample\n```python\nclient = OpenAIChatCompletionClient(\n    model=\"claude-3-5-haiku-20241022\"\n)\nagent1 = AssistantAgent(name=\"writer\", model_client=client)\nagent2 = AssistantAgent(name=\"editor\", model_client=client)\nteam = RoundRobinGroupChat(\n    participants=[agent1, agent2],\n    termination_condition=TextMentionTermination(\"TERMINATE\")\n)\n\nserver = Server(team=team, source_select=\"writer\")\nserver.run(host=\"0.0.0.0\", port=8000)  # you could do not filled that args. default is that host=\"0.0.0.0\", port=8000\n```\n\nJust write AutoGen team, and... Run it!\n\nCURL call test!\nexample\n```bash\ncurl -X POST http://localhost:8000/v1/chat/completions \\\n -H \"Content-Type: application/json\" \\\n -d '{\n \"session_id\": \"test-session\",\n \"messages\": [ { \"role\": \"user\", \"content\": \"Please write 5 funny stories.\" } ]\n}'\n```\n\n---\n\n## Demo\n![Demo](https://github.com/SongChiYoung/autogen-oaiapi/blob/main/demo.gif?raw=true)\n"
    },
    {
      "name": "msalemor/llm-use-cases",
      "stars": 14,
      "img": "https://avatars.githubusercontent.com/u/44649358?s=40&v=4",
      "owner": "msalemor",
      "repo_name": "llm-use-cases",
      "description": null,
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2023-04-26T18:27:33Z",
      "updated_at": "2025-04-21T15:33:04Z",
      "topics": [],
      "readme": "# Foundational Model Use Cases\n\n## Foundational Models\n\n## Requirements\n\n- Python 3.7-3.10\n- Create `notebooks\\.env` file\n\n```\nOPENAI_URI=https://<NAME>.openai.azure.com/\nOPENAI_KEY=<API_KEY>\nOPENAI_VERSION=2023-07-01-preview\nOPENAI_GPT_DEPLOYMENT=<GPT_DEPLOYMENT_NAME>\nOPENAI_ADA_DEPLOYMENT=<ADA_DEPLOYMENT_NAME>\n```\n\n- Open a Notebook and click play on the cells\n\n## Common Use Cases\n\n- [Summarization & Risk Analysis](SUMMARIZATION.md)\n  - Get a contract, summarize it, and identify the risks.\n- [Recommendation system](RECOMMENDATION.md)\n  - Recommend from an up-to-date list of restaurants\n- [Language Translation](TRANSLATION.md)\n  - Translate technical text from a target to a source language\n- [Content generation and Personalization](GENERATION.md)\n  - Generate a sales car description\n- [Intent and entities - A LUIS-like scenario](INTENT.md)\n  - Find the user's intent and entities\n- [Sentiment Analysis - Zero Shot](SENTIMENT.md)\n  - Sentiment Analysis  \n- [Scoring - Zero Shot](SCORING.md)\n  - Scoring\n- [Classification](CLASSIFICATION.md)\n  - Classification\n- [RAG Pattern without a vector database](RAGNODB.md)\n  - Provide the container and answer the question based only on the provided content.\n"
    },
    {
      "name": "ZhaoYis/Berry.Spider",
      "stars": 13,
      "img": "https://avatars.githubusercontent.com/u/22954818?s=40&v=4",
      "owner": "ZhaoYis",
      "repo_name": "Berry.Spider",
      "description": "基于Selenium自动化框架实现的爬虫程序（目前主要有百度、头条、搜狗）",
      "homepage": "",
      "language": "C#",
      "created_at": "2022-02-28T15:20:17Z",
      "updated_at": "2025-04-09T15:17:21Z",
      "topics": [
        "abp-vnext",
        "chrome",
        "csharp",
        "selenium",
        "webdriver"
      ],
      "readme": "# Berry.Spider\n`Berry.Spider`是一个基于Abp vNext框架、Selenium自动化框架开发的一款的爬虫程序（目前主要有百度、头条、搜狗），项目目前还有很多不完善的地方，后续会持续地更新优化，欢迎PR🍬\n\n此项目主要使用到的框架以及相关开源项目：\n- [.NET 8](https://dotnet.microsoft.com/zh-cn/download/dotnet/8.0)\n- [Abp vNext框架](https://github.com/abpframework/abp)\n- [Selenium](https://www.selenium.dev)\n- [Chrome & WebDriver](https://chromedriver.storage.googleapis.com/index.html)\n- [AgileConfig](https://github.com/dotnetcore/AgileConfig)\n- [CAP](https://github.com/dotnetcore/cap)\n- [FreeRedis](https://github.com/2881099/FreeRedis)\n- [jieba.NET](https://github.com/anderscui/jieba.NET)\n- [ToolGood.Words](https://github.com/toolgood/ToolGood.Words)\n- [Refit](https://github.com/reactiveui/refit)\n- [Betalgo.OpenAI](https://github.com/betalgo/openai)\n- [RabbitMQ](https://www.rabbitmq.com)\n- [Useragents.me](https://www.useragents.me)\n- [NSSM](http://www.nssm.cc/download)\n- [SemanticKernel](https://github.com/microsoft/semantic-kernel)\n- [Ollama](https://ollama.com/library/qwen2)\n- [OllamaSharp](https://github.com/awaescher/OllamaSharp)\n- ...\n\n🈲*重要说明：此项目仅用于个人学习、演示使用。*\n\n## 本项目已得到[JetBrains](https://www.jetbrains.com/shop/eform/opensource)的支持！\n\n<img src=\"https://www.jetbrains.com/shop/static/images/jetbrains-logo-inv.svg\" height=\"100\">\n"
    },
    {
      "name": "akshata29/finagent",
      "stars": 13,
      "img": "https://avatars.githubusercontent.com/u/18509807?s=40&v=4",
      "owner": "akshata29",
      "repo_name": "finagent",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2025-01-19T18:31:24Z",
      "updated_at": "2025-04-23T09:26:26Z",
      "topics": [],
      "readme": "# Multi-Agent: Financial Market Agent – Solution Accelerator\n\n### Overview\n\nProblem:\nAgentic AI systems are set to transform the way businesses operate, however it can be fairly complex to build an initial MVP to demonstrate this value. \n\nSolution:\nThe Multi-Agent - Financial Market Agent Solution Accelerator provides a ready to go application to use as the base of the MVP, or as a reference, allowing you to hit the ground running.\n\n### Technology Note\nThis accelerator uses the AutoGen framework from Microsoft Research.  This is an open source project that is maintained by [Microsoft Research’s AI Frontiers Lab](https://www.microsoft.com/research/lab/ai-frontiers/). Please see this [blog post](https://devblogs.microsoft.com/autogen/microsofts-agentic-frameworks-autogen-and-semantic-kernel/) for the latest information on using the AutoGen framework in production solutions.\n\n### Use cases / scenarios\nThe multi-agent approach allows users to utilize multiple AI agents simultaneously for repeatable tasks, ensuring consistency and efficiency. \nThe agents collaborate with a manager on various assignments for onboarding a new employee , such as HR and tech support AI working together to set up software accounts, configure hardware, schedule onboarding meetings, register employees for benefits, and send welcome emails. Additionally, these agents can handle tasks like procurement and drafting press releases.\n\n### Business value\nMulti-agent systems represent the next wave of Generative AI use cases, offering entirely new opportunities to drive efficiencies in your business. The Multi-Agent -Custom Automation Engine Solution Accelerator demonstrates several key benefits:\n\n- **Allows people to focus on what matters:** by doing the heavy lifting involved with coordinating activities across an organization, peoples’ time is freed up to focus on their specializations.\n- **Enabling GenAI to scale:** by not needing to build one application after another, organizations are able to reduce the friction of adopting GenAI across their entire organization. One capability can unlock almost unlimited use cases.\n- **Applicable to most industries:** these are common challenges that most organizations face, across most industries.\n\nWhilst still an emerging area, investing in agentic use cases, digitatization and developing tools will be key to ensuring you are able to leverage these new technologies and seize the GenAI moment.\n\n### Technical key features\n\nThis application is an AI-driven orchestration system that manages a group of AI agents to accomplish tasks based on user input. It uses a FastAPI backend to handle HTTP requests, processes them through various specialized agents, and stores stateful information using Azure Cosmos DB. The system is designed to:\n\n- Receive input tasks from users.\n- Generate a detailed plan to accomplish the task using a Planner agent.\n- Execute the plan by delegating steps to specialized agents (e.g., HR, Procurement, Marketing).\n- Incorporate human feedback into the workflow.\n- Maintain state across sessions with persistent storage.\n\nThis system is intended for developing and deploying custom AI solutions for specific customers. This code has not been tested as an end-to-end, reliable production application- it is a foundation to help accelerate building out multi-agent systems. You are encouraged to add your own data and functions to the agents, and then you must apply your own performance and safety evaluation testing frameworks to this system before deploying it.\n\n### Technical Capabilities\n\nThe solution is a multi-agent architecture designed to perform comprehensive financial data analysis and reporting. The system includes the following specialized agents, each leveraging GPT-4o and the Autogen framework for advanced natural language processing and contextual understanding:\n\n1. Company Agent\n    - Retrieve comprehensive company profile information.\n    - Access and analyze historical and real-time stock data.\n    - Extract and evaluate the latest financial metrics.\n    - Aggregate and summarize the latest company-related news.\n    - Provide detailed analyst recommendations for investment decisions.\n2. Earnings Calls Agent\n    - Retrieve and process earnings call transcripts.\n    - Generate executive summaries of transcripts.\n    - Identify and categorize key insights:\n      - Positive Outlook: Management's optimistic commentary and projections.\n      - Negative Outlook: Concerns or risks highlighted in discussions.\n      - Future Growth Opportunities: Strategies and market opportunities discussed during calls.\n3. SEC Agent\n    - Analyze SEC annual reports and filings.\n    - Extract key company and business highlights.\n    - Perform detailed risk assessment based on disclosures.\n    - Analyze financial statements, including:\n        - Cash Flow Statements\n        - Income Statements\n    - Generate comprehensive equity research reports summarizing findings.\n4. Integration and Technical Features\n    - Framework: The agents are built on the Autogen framework, enabling dynamic task orchestration and collaboration among agents.\n    - Foundation Model: All agents utilize GPT-4o, ensuring advanced contextual reasoning and data synthesis.\n    - Data Sources: The system seamlessly integrates data from:\n      - Yahoo Finance for real-time market data and analytics.\n      - SEC API for regulatory filings and compliance data.\n      - Private Financial Data Providers for exclusive market insights.\n\n### Applications\nThis solution is a robust, scalable platform for delivering high-value financial intelligence, leveraging cutting-edge AI capabilities to enhance productivity and decision-making in financial analysis.\n\n- Investment Research: Comprehensive equity analysis and decision support for institutional and retail investors.\n- Corporate Strategy: Data-driven insights for management planning and market positioning.\n- Risk Assessment: Automated analysis of financial risks and opportunities, tailored to specific companies or industries.\n\nHome Page\n\\\n![image](./documentation/images/readme/macae-home.png)\n\nTask Plan and execution\n\\\n![image](./documentation/images/readme/macae-application.png)\n\nGenerated Research Report\n\\\n![image](./documentation/images/readme/macae-report.png)\n\n\n### Products used\n\n-   Azure Container Application\n-   Azure OpenAI\n-   Azure Cosmos DB\n-   The user deploying the template must have permission to create\n    resources and resource groups.\n\n### Solution accelerator architecture\n![image](./documentation/images/readme/macae-architecture.png)\n\n\n### **How to install/deploy**\n\nThis guide provides step-by-step instructions for deploying your application using Azure Container Registry (ACR) and Azure Container Apps.\n\nThere are several ways to deploy the solution.  You can deploy to run in Azure in one click, or manually, or you can deploy locally.\n\n## Quick Deploy\n\n<h2><img src=\"./documentation/images/readme/oneClickDeploy.png\" width=\"64\"></h2>\n\n[![Deploy to Azure](https://aka.ms/deploytoazurebutton)](https://portal.azure.com/#create/Microsoft.Template/uri/https%3A%2F%2Fraw.githubusercontent.com%akshata29%2Ffinagents%2Frefs%2Fheads%2Fmain%2Fdeploy%2Ffinagents-container-oc.json)\n\nWhen Deployment is complete, follow steps in [Set Up Authentication in Azure App Service](./documentation/azure_app_service_auth_setup.md) to add app authentication to your web app running on Azure App Service\n\n## Local Deployment\nTo run the solution site and API backend only locally for development and debugging purposes, See the [local deployment guide](./documentation/LocalDeployment.md).\n\n## Manual Azure Deployment\nManual Deployment differs from the ‘Quick Deploy’ option in that it will install an Azure Container Registry (ACR) service, and relies on the installer to build and push the necessary containers to this ACR.  This allows you to build and push your own code changes and provides a sample solution you can customize based on your requirements.\n\n### Prerequisites\n\n- Current Azure CLI installed\n  You can update to the latest version using ```az upgrade```\n- Azure account with appropriate permissions\n- Docker installed\n\n### Deploy the Azure Services\nAll of the necessary Azure services can be deployed using the /deploy/macae.bicep script.  This script will require the following parameters:\n\n```\naz login\naz account set --subscription <SUBSCRIPTION_ID>\naz group create --name <RG_NAME> --location <RG_LOCATION>\n```\nTo deploy the script you can use the Azure CLI.\n```\naz deployment group create \\\n  --resource-group <RG_NAME> \\\n  --template-file <BICEP_FILE> \\\n  --name <DEPLOYMENT_NAME>\n```\n\nNote: if you are using windows with PowerShell, the continuation character (currently ‘\\’) should change to the tick mark (‘`’).\n\nThe template will require you fill in locations for Cosmos and OpenAI services.  This is to avoid the possibility of regional quota errors for either of these resources.\n\n### Create the Containers\n#### Get admin credentials from ACR\n\nRetrieve the admin credentials for your Azure Container Registry (ACR):\n\n```sh\naz acr credential show \\\n--name <e.g. macaeacr2t62qyozi76bs> \\\n--resource-group <rg-name>\n```\n\n#### Login to ACR\n\nLogin to your Azure Container Registry:\n\n```sh\naz acr login --name <e.g. macaeacr2t62qyozi76bs>\n```\n\n#### Build and push the image\n\nBuild the frontend and backend Docker images and push them  to your Azure Container Registry. Run the following from the src/backend and the src/frontend directory contexts:\n\n```sh\naz acr build \\\n--registry <e.g. macaeacr2t62qyozi76bs> \\\n--resource-group <rg-name> \\\n--image <e.g. backendmacae:latest> .\n```\n\n### Add images to the Container APP and Web App services\n\nTo add your newly created backend image: \n- Navigate to the Container App Service in the Azure portal\n- Click on Application/Containers in the left pane\n- Click on the \"Edit and deploy\" button in the upper left of the containers pane\n- In the \"Create and deploy new revision\" page, click on your container image 'backend'.  This will give you the option of reconfiguring the container image, and also has an Environment variables tab\n- Change the properties page to \n  - point to your Azure Container registry with a private image type and your image name (e.g. backendmacae:latest)\n  - under \"Authentication type\" select \"Managed Identity\" and choose the 'mace-containerapp-pull'... identity setup in the bicep template\n- In the environment variables section add the following (each with a 'Manual entry' source):\n\n        name: 'COSMOSDB_ENDPOINT'\n        value: \\<Cosmos endpoint>\n\n        name: 'COSMOSDB_DATABASE'\n        value: 'autogen'\n\t    Note: To change the default, you will need to create the database in Cosmos\n\t\t\t  \n        name: 'COSMOSDB_CONTAINER'\n        value: 'memory'\n\n        name: 'AZURE_OPENAI_ENDPOINT'\n        value: <Azure OpenAI endpoint>\n\n        name: 'AZURE_OPENAI_DEPLOYMENT_NAME'\n        value: 'gpt-4o'\n\n        name: 'AZURE_OPENAI_API_VERSION'\n        value: '2024-08-01-preview'\n\t\tNote: Version should be updated based on latest available\n\n        name: 'FRONTEND_SITE_NAME'\n        value: 'https://<website Name>.azurewebsites.net'\n\n- Click 'Save' and deploy your new revision\n\nTo add the new container to your website run the following:\n\n```\naz webapp config container set --resource-group macae_full_deploy2_rg \\\n--name macae-frontend-2t62qyozi76bs \\\n--container-image-name macaeacr2t62qyozi76bs.azurecr.io/frontendmacae:latest  \\\n--container-registry-url https://macaeacr2t62qyozi76bs.azurecr.io\n```\n\n\n### Add the Entra identity provider to the Azure Web App\nTo add the identity provider, please follow the steps outlined in [Set Up Authentication in Azure App Service](./documentation/azure_app_service_auth_setup.md)\n\n### Run locally and debug\n\nTo debug the solution, you can use the Cosmos and OpenAI services you have manually deployed.  To do this, you need to ensure that your Azure identity has the required permissions on the Cosmos and OpenAI services. \n\n- For OpeAI service, you can add yourself to the ‘Cognitive Services OpenAI User’ permission in the Access Control (IAM) pane of the Azure portal.  \n- Cosmos is a little more difficult as it requires permissions be added through script.  See these examples for more information: \n  - [Use data plane role-based access control - Azure Cosmos DB for NoSQL | Microsoft Learn](https://learn.microsoft.com/en-us/azure/cosmos-db/nosql/security/how-to-grant-data-plane-role-based-access?tabs=built-in-definition%2Cpython&pivots=azure-interface-cli) \n  - [az cosmosdb sql role assignment | Microsoft Learn](https://learn.microsoft.com/en-us/cli/azure/cosmosdb/sql/role/assignment?view=azure-cli-latest#az-cosmosdb-sql-role-assignment-create) \n\nAdd the appropriate endpoints from Cosmos and OpenAI services to your .env file.  \nNote that you can configure the name of the Cosmos database in the configuration.  This can be helpful if you wish to separate the data messages generated in local debugging from those associated with the cloud based solution.  If you choose to use a different database, you will need to create that database in the Cosmos instance as this is not done automatically.\n\nIf you are using VSCode, you can use the debug configuration shown in the [local deployment guide](./documentation/LocalDeployment.md).\n\n## Supporting documentation\n\n\n### \n\n### How to customize\n\nThis solution is designed to be easily customizable. You can modify the front end site, or even build your own front end and attach to the backend API. You can further customize the backend by adding your own agents with their own specific capabilities. Deeper technical information to aid in this customization can be found in this [document](./documentation/CustomizeSolution.md).\n\n### Additional resources\n\n- [Python FastAPI documentation](https://fastapi.tiangolo.com/learn/)\n- [AutoGen Framework Documentation](https://microsoft.github.io/autogen/dev/user-guide/core-user-guide/index.html)\n- [Azure Container App documentation](https://learn.microsoft.com/en-us/azure/azure-functions/functions-how-to-custom-container?tabs=core-tools%2Cacr%2Cazure-cli2%2Cazure-cli&pivots=container-apps)\n- [Azure OpenAI Service - Documentation, quickstarts, API reference - Azure AI services | Microsoft Learn](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/use-your-data)\n- [Azure Cosmos DB documentation](https://learn.microsoft.com/en-us/azure/cosmos-db/)\n  \n\n<br/>\n\n### Notes\n\nAdapted from the repo at [Template](https://github.com/microsoft/Multi-Agent-Custom-Automation-Engine-Solution-Accelerator) and  [FinRobot](https://github.com/AI4Finance-Foundation/FinRobot)\n\n<br/>\n<br/>\n\n---\n\n## Disclaimers\n\nTo the extent that the Software includes components or code used in or derived from Microsoft products or services, including without limitation Microsoft Azure Services (collectively, “Microsoft Products and Services”), you must also comply with the Product Terms applicable to such Microsoft Products and Services. You acknowledge and agree that the license governing the Software does not grant you a license or other right to use Microsoft Products and Services. Nothing in the license or this ReadMe file will serve to supersede, amend, terminate or modify any terms in the Product Terms for any Microsoft Products and Services. \n\nYou must also comply with all domestic and international export laws and regulations that apply to the Software, which include restrictions on destinations, end users, and end use. For further information on export restrictions, visit https://aka.ms/exporting. \n\nYou acknowledge that the Software and Microsoft Products and Services (1) are not designed, intended or made available as a medical device(s), and (2) are not designed or intended to be a substitute for professional medical advice, diagnosis, treatment, or judgment and should not be used to replace or as a substitute for professional medical advice, diagnosis, treatment, or judgment. Customer is solely responsible for displaying and/or obtaining appropriate consents, warnings, disclaimers, and acknowledgements to end users of Customer’s implementation of the Online Services. \n\nYou acknowledge the Software is not subject to SOC 1 and SOC 2 compliance audits. No Microsoft technology, nor any of its component technologies, including the Software, is intended or made available as a substitute for the professional advice, opinion, or judgement of a certified financial services professional. Do not use the Software to replace, substitute, or provide professional financial advice or judgment.  \n\nBY ACCESSING OR USING THE SOFTWARE, YOU ACKNOWLEDGE THAT THE SOFTWARE IS NOT DESIGNED OR INTENDED TO SUPPORT ANY USE IN WHICH A SERVICE INTERRUPTION, DEFECT, ERROR, OR OTHER FAILURE OF THE SOFTWARE COULD RESULT IN THE DEATH OR SERIOUS BODILY INJURY OF ANY PERSON OR IN PHYSICAL OR ENVIRONMENTAL DAMAGE (COLLECTIVELY, “HIGH-RISK USE”), AND THAT YOU WILL ENSURE THAT, IN THE EVENT OF ANY INTERRUPTION, DEFECT, ERROR, OR OTHER FAILURE OF THE SOFTWARE, THE SAFETY OF PEOPLE, PROPERTY, AND THE ENVIRONMENT ARE NOT REDUCED BELOW A LEVEL THAT IS REASONABLY, APPROPRIATE, AND LEGAL, WHETHER IN GENERAL OR IN A SPECIFIC INDUSTRY. BY ACCESSING THE SOFTWARE, YOU FURTHER ACKNOWLEDGE THAT YOUR HIGH-RISK USE OF THE SOFTWARE IS AT YOUR OWN RISK.  \n"
    },
    {
      "name": "rh-aiservices-bu/agentic-workshop",
      "stars": 12,
      "img": "https://avatars.githubusercontent.com/u/73964756?s=40&v=4",
      "owner": "rh-aiservices-bu",
      "repo_name": "agentic-workshop",
      "description": "AI Agents Workshop with Red Hat AI",
      "homepage": "https://rh-aiservices-bu.github.io/agentic-workshop/",
      "language": "Jupyter Notebook",
      "created_at": "2024-10-21T10:15:57Z",
      "updated_at": "2025-03-20T13:19:32Z",
      "topics": [],
      "readme": "# agentic-workshop\nWorkshop for Agentic AI within Red Hat AI\n"
    },
    {
      "name": "LittleLittleCloud/AI-Agentic-Design-Patterns-with-AutoGen.Net",
      "stars": 11,
      "img": "https://avatars.githubusercontent.com/u/16876986?s=40&v=4",
      "owner": "LittleLittleCloud",
      "repo_name": "AI-Agentic-Design-Patterns-with-AutoGen.Net",
      "description": null,
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2024-07-19T18:06:22Z",
      "updated_at": "2025-03-15T09:53:43Z",
      "topics": [],
      "readme": "# AI-Agentic-Design-Patterns-with-AutoGen.Net\n\n`AutoGen.Net` implementation of [`AI-Agentic-Design-Patterns-with-AutoGen`](https://www.deeplearning.ai/short-courses/ai-agentic-design-patterns-with-autogen/)\n\n> [!Note]\n> Some examples are not exactly match what python AutoGen does. This is because some libraries are not available in .NET. For example, we use Tic Tac Toe game as a tool to demonstrate the concept of tool use where the python one use Chess game. This is because the `chess` library is not available in .NET. Another case is in Lesson 5, where python ones use financial analysis to demostrate code interpreter usage. In .Net AutoGen, however, I replace the financial analysis with math problem solving because of lacking library support in .Net eco-system.\n\n## Console Apps\n- [x] [Lesson 1: Multi-Agent Conversation and Stand-up Comedy](./L1_MultiAgent_Conversation_and_Standup_Comedy/)\n- [x] [Lesson 2: Sequential Chats and Customer Onboarding](./L2_Sequential_Chats_and_Customer_Onboarding/)\n- [x] [Lesson 3: Reflection_and_Blogpost_Writing](./L3_Reflection_and_Blogpost_Writing/)\n- [x] [Lesson 4: Tool_Use_and_Conversational_Tic_Tac_Toe](./L4_Tool_Use_and_Conversational_Tic_Tac_Toe/)\n- [x] [Lesson 5: Coding_and_Math_Problem_Solving](./L5_Coding_and_Math_Problem_Solving/)\n- [x] [Lesson 6: L6-Planning_and_Stock_Report_Generation](./L6-Planning_and_Stock_Report_Generation/)\n\n## Notebooks\n- [x] [Lesson 1: Multi-Agent Conversation and Stand-up Comedy](./notebook/L1_MultiAgent_Conversation_and_Standup_Comedy.ipynb)\n- [x] [Lesson 2: Sequential Chats and Customer Onboarding](./notebook/L2_Sequential_Chats_and_Customer_Onboarding.ipynb)\n- [x] [Lesson 3: Reflection_and_Blogpost_Writing](./notebook/L3_Reflection_and_Blogpost_Writing.ipynb)\n- [x] [Lesson 4: Tool_Use_and_Conversational_Tic_Tac_Toe](./notebook/L4_Tool_Use_and_Conversational_Tic_Tac_Toe.ipynb)\n- [x] [Lesson 5: Coding_and_Math_Problem_Solving](./notebook/L5_Coding_and_Math_Problem_Solving.ipynb)\n- [x] [Lesson 6: Planning_and_Stock_Report_Generation](./notebook/L6_Planning_and_Stock_Report_Generation.ipynb)\n\n## Difference between Console Apps and Notebooks examples\n- Notebook also contains the output result while Console app doesn't.\n- The way to define a tool call is different between Console app and notebook examples. Console app uses `AutoGen.SourceGenerator` to create function definition directly from the structural comment while Notebook examples create function definition using semantic-kernel plugin style. This is because source generator is not available in Notebook use case.\n- The code executor is different: In Console app examples, it needs to start a dotnet interactive instance to run C# code, which is not necessary in Notebook and we can simply execute the code using the running kernel.\n\n"
    },
    {
      "name": "treeleaves30760/verilog-data-generator",
      "stars": 11,
      "img": "https://avatars.githubusercontent.com/u/43369966?s=40&v=4",
      "owner": "treeleaves30760",
      "repo_name": "verilog-data-generator",
      "description": null,
      "homepage": null,
      "language": "Verilog",
      "created_at": "2025-02-10T17:18:58Z",
      "updated_at": "2025-03-04T15:20:59Z",
      "topics": [],
      "readme": "# verilog data generator\n\n## Introduction\n\n## Install\n\n```bash\nconda create -n verilog-data-generator python==3.11.10 -y\nconda activate verilog-data-generator\npip install -r requirements.txt\n```\n\n## Usage\n\n```bash\npython generate.py\n```\n"
    },
    {
      "name": "fbellame/agent-comparison",
      "stars": 11,
      "img": "https://avatars.githubusercontent.com/u/29183055?s=40&v=4",
      "owner": "fbellame",
      "repo_name": "agent-comparison",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2024-12-05T21:12:01Z",
      "updated_at": "2025-03-05T00:15:00Z",
      "topics": [],
      "readme": "# A short comparison of popular Agent Frameworks\n\n<br>\n\n[![Introduction to Agent Frameworks](images/mini-agent-comp.png)](https://www.youtube.com/@faridautomatic)\n\n## Understanding the rise of Agent Frameworks in 2025\n\nThe next wave of AI is agentic, a sweeping tide reshaping the shores of technology and human interaction. Like a powerful current, it pulls everything in its path, transforming how we make decisions, solve problems, and navigate our lives. Dive with me into these waters as we explore three powerful currents - CrewAI's orchestrated flow, AutoGen's interweaving streams, and LangGraph's precise tidal patterns - each carving unique channels in the ever-evolving seascape of artificial intelligence.\n\n\n## Why Agent Frameworks matter\n\n<br>As artificial intelligence continues to transform industries, **agent frameworks** are emerging as a game-changer in **2025**.\n\nKey advantages include:\n\n## Comparison: Agent Framework vs. Large Language Model (LLM)\n\n### **1. Large Language Model (LLM)**\n\n#### **Definition**  \nA **Large Language Model** (like ChatGPT) is a pre-trained neural network that processes and generates human-like text. It uses large datasets to understand language patterns and context.\n\n### **Core Characteristics**:\n- **Primary Function**: Processes and generates text based on input prompts.\n- **Stateless**: Each interaction is independent unless explicitly extended with tools like memory.\n- **General Purpose**: Handles a wide variety of tasks (e.g., answering questions, summarization, writing code) but relies on prompts for task-specific behavior.\n- **Does Not Manage Workflows**: Operates on single-turn inputs without built-in orchestration or automation of complex workflows.\n- **Example**: ChatGPT or GPT-4 by OpenAI.\n\n---\n\n### **2. Agent Framework**\n\n#### **Definition**  \nAn **Agent Framework** is a system or library designed to orchestrate **multiple agents**, including LLMs, APIs, or custom logic, for performing **complex, multi-step tasks** in an automated and collaborative manner.\n\n#### **Core Characteristics**:\n- **Workflow Management**: Automates workflows involving multiple steps, tools, or agents.\n- **Stateful**: Tracks the state of tasks across multiple interactions, maintaining context continuity.\n- **Integration**: Combines LLMs with external tools (e.g., databases, APIs, cloud services).\n- **Customizable**: Tailored to specific use cases like code automation, data analysis, or customer support.\n- **Collaborative Agents**: Enables communication and coordination between multiple agents (e.g., human-in-the-loop systems, multi-agent collaboration).\n- **Example**: LangGraph, AutoGen, LangChain.\n\n---\n\n### **Key Differences**\n\n![comparison LLM vs Agent](images/llm-agent-comparison.png)\n\n\n---\n\n### **Analogy**\n\nThink of an **LLM like ChatGPT** as a highly skilled worker that can handle individual tasks on demand but needs explicit instructions every time.  \nAn **Agent Framework**, on the other hand, is like a **project manager** that oversees multiple workers (LLMs, APIs, tools), ensuring they collaborate effectively to complete a complex project.\n\n---\n\n### **Use Cases**\n- **LLM**: Quick text generation, idea brainstorming, or Q&A sessions.\n- **Agent Framework**: Automating customer service processes, orchestrating data pipelines, creating autonomous coding assistants, or managing multi-agent systems.\n\n\n\n- **Business Operations**: [Reuters: Autonomous agents and profitability to dominate AI agenda in 2025, executives forecast](https://www.reuters.com/technology/artificial-intelligence/autonomous-agents-profitability-dominate-ai-agenda-2025-executives-forecast-2024-12-12/)\n- **Productivity Tools**: AI assistants capable of reasoning are expected to become mainstream, transforming daily workflows. [Financial Times](https://www.ft.com/content/30677465-33bb-4f74-a8e6-239980091f7a)\n\n## A comparative analysis of leading Frameworks\n\nThis article provides a short comparison of three prominent agent frameworks:\n - **Autogen**\n - **CrewAI**\n - **LangGraph**\n \nThese frameworks were chosen for their growing popularity on GitHub, open-source accessibility, and robust feature sets, which make them standout choices for developers and researchers alike. \n\nBy exploring their capabilities, this comparison aims to equip readers with the knowledge needed to navigate the agent framework landscape.\n\n\n## Comparison criteria\n\nWhen selecting agent frameworks, clear evaluation criteria become your compass for navigating the complex landscape of AI solutions. These criteria encompass core aspects: technical capability, scalability potential, and development experience. By carefully weighing how CrewAI, AutoGen, and LangGraph perform across these dimensions, teams can better align their choice with project requirements while ensuring sustainable development paths. This methodical assessment helps cut through the complexity, leading to more confident and successful implementations.\n\n\n### 1. Ease of Use\n- Refers to how intuitive and user-friendly the framework is for developers.\n- Includes aspects such as clear documentation, straightforward APIs, and community support.\n\n### 2. Performance\n- Focuses on the framework’s ability to handle tasks efficiently.\n- Includes metrics like execution speed, resource optimization, and scalability.\n\n### 3. Deployment Capabilities\n- Evaluates the framework’s flexibility to be deployed across various environments.\n- Includes support for platforms like cloud services and on-premises systems.\n\n### 4. User Interface\n- Assesses the tools available for managing, monitoring, and debugging agents.\n- Includes features like visual workflows, logging systems, and dashboards.\n\n\n\n## Use case\n\nTo ensure a fair evaluation, we will implement the same use case across all three frameworks. \n\nThis approach allows us to directly compare their capabilities, development patterns, and performance while solving identical requirements. By maintaining consistent testing conditions, we can effectively highlight each framework's strengths and limitations.\n\n[![Use case](images/cas-utilisation.png)](https://youtube.com/clip/UgkxEY6keXam1mYTpz1jvvUv_X6V5om6FL0s?si=y9Uus_NOomiX5rOb)\n\n#### Objective\nTo provide a **rating (1-10)** for a stock by combining:\n1. **Yahoo Finance data**: Quantitative financial metrics.\n2. **Warren Buffett's investment principles**: Qualitative evaluation for long-term value investing.\n\n\n#### Workflow\n1. **Data Collection**:\n   - Gather financial data from Yahoo Finance.\n   - Apply Buffett's principles to evaluate qualitative aspects.\n\n2. **Analysis**:\n   - Combine **quantitative metrics** (Yahoo Finance data) with **qualitative insights** (Buffett principles).\n   - Assign weights to each factor (e.g., 50% financial metrics, 50% Buffett criteria).\n\n3. **Scoring**:\n   - Develop a formula to compute a final score (1-10).\n     - **1**: Poor investment.\n     - **10**: Excellent investment opportunity.\n\n4. **Output**:\n   - Present the **rating (1-10)** as a simplified indicator for investment decisions into a markdown report.\n\n\n## Agent Frameworks overview\n\n### CrewAI\n\n[![Overview of QAI Framework](images/crewai-logo.png)](https://www.crewai.com/)\n\n[CrewAI](https://www.crewai.com/) is a popular open source Agent framework, with more than 23k ⭐ on [gitHub](https://github.com/crewAIInc/crewAI): \n\n\n- Based on **LangGraph**  \n- Released in **2024**  \n- Raised **$18M** in funding\n- Features **role-based** agents\n\n#### Key Features of CrewAI\n\n**1. Agent orchestration**\n- CrewAI facilitates seamless collaboration among multiple agents by assigning specific roles and objectives.\n- This structured approach ensures efficient and organized interactions between agents.\n\n**2. Integration with tools**\n- Agents can integrate and utilize external tools such as **Yahoo Finance** and **file readers**.\n- These integrations enable agents to perform more complex and context-aware tasks effectively.\n\n**3. Memory management**\n- Agents retain memory of their interactions, enhancing consistency and improving task execution over time.\n- Memory capabilities allow for continuity in multi-step processes or recurring tasks.\n\n**4. Task delegation**\n- Tasks can be delegated among agents, optimizing workload distribution.\n- This feature ensures faster and more efficient completion of intricate workflows.\n\n---\n\n**Use case diagram**\n\n[![CrewAI use case](images/crewai-usecase.png)](https://youtube.com/clip/UgkxA25HzKdng8tkAmTCltpyTg5l9Ij8_Mna?si=rio23AVh13ZlBKnZ)\n\n**Evaluation**\n\n- **Ease of Use**: ⭐⭐⭐⭐ It offers clear visibility of agent concepts and connections even for non-developers.\n- **Performance**: ⭐⭐ Lacks tools for specific tasks causing issues like hallucinations in LLMs.\n- **Deployment**: ⭐ Limited to non-secure environments as it doesn’t allow code execution in a secure Docker container.\n- **User Interface**: ⭐ No widely recognized graphical interface available, limiting design and prototyping capabilities.\n\n---\n\n### Autogen\n\n[![Overview of QAI Framework](images/autogen-logo.png)](https://microsoft.github.io/autogen/0.2/)\n\n[AutoGen](https://microsoft.github.io/autogen/0.2/) is a popular open source Agent framework backed by Microsoft, with more than 36k ⭐ on [gitHub](https://github.com/microsoft/autogen): \n\n\n- Based on the research of [Dr. Chi Wang](https://www.microsoft.com/en-us/research/people/chiw/) at Microsoft and [Dr. Qingyun Wu](https://qingyun-wu.github.io/) at Penn State University\n- Recently forked into two branches: **version 0.2** and **version 0.4**\n- Support **Python** and **.Net** languages\n\n### Key Features of AutoGen\n\n**1. Multi-Agent Collaboration**  \n- AutoGen enables the creation of agent networks where agents collaborate or operate independently.  \n- This design supports workflows ranging from fully autonomous systems to those requiring human oversight.\n\n**2. Code Automation**  \n- Agents can generate, execute, and debug code automatically, streamlining software development and data analysis tasks.  \n- This reduces human intervention and accelerates project timelines.\n\n**3. Integration with Tools and APIs**  \n- AutoGen supports integration with external tools and APIs, such as Azure services and databases.  \n- This capability expands agent functionality for diverse applications.\n\n**4. Human-in-the-Loop Problem Solving**  \n- Agents can request human input for critical decisions, ensuring thoughtful oversight when needed.  \n- This feature is ideal for balancing automation with human expertise.\n\n----\n\n**Use case diagram**\n\n[![AutoGen use case](images/autogen-usecase.png)](https://youtube.com/clip/UgkxSdZERvmjX7tLQ2-0rFOyV7cgOBaZgJQ8?si=c7OjzYkz7bRUMwrO)\n\n**Evaluation**\n- **Ease of Use**: ⭐⭐⭐ Although it remains a developer-focused tool, it uses simple design patterns for implementation. \n- **Performance**: ⭐⭐⭐⭐ Best among the three, as it automatically generates necessary code for various tasks and offers a secure execution environment. \n- **Deployment**: ⭐⭐⭐ Provides versatile deployment options including local, cloud, and Docker environments.\n- **User Interface**: ⭐⭐⭐ Autogen Studio is the available interface, which despite some bugs, is relatively helpful for workflow design.\n\n----\n\n### **LangGraph Framework**\n\n[![LangGraph logo](images/langgraph-logo.png)](https://www.langchain.com/langgraph)\n\n[LangGraph](https://www.langchain.com/langgraph) is an open-source orchestration framework designed for building stateful, multi-agent applications with Large Language Models (LLMs). It offers enhanced control over agent workflows, enabling the development of complex, reliable, and fault-tolerant AI systems. It has more than 7k ⭐ on [gitHub](https://github.com/langchain-ai/langgraph): \n\n- Based on the popular [LanChain LLM framework](https://www.langchain.com/) \n\n### Key Features of LangGraph\n\n**1. Stateful Orchestration** \n - Manages shared state across nodes, facilitating complex workflows with cycles and branches.\n\n**2. Persistence** \n- Automatically saves state after each step, allowing for error recovery, human-in-the-loop interactions, and workflow resumption.\n\n**3. Human-in-the-Loop Support** \n- Enables interruption of graph execution for human review, approval, or editing of the agent's planned actions.\n\n**4. Streaming Support** \n- Provides real-time output streaming from each node, enhancing interactive applications.\n\n**5. Integration with LangChain**\n- Seamlessly integrates with LangChain and LangSmith, though it can operate independently.\n\n---\n\n**Use case diagram**\n\n[![LangGraph use case](images/langgraph-usecase.png)](https://youtube.com/clip/UgkxrGPOZuE_L3hrmLhoqTtVewF5MKduMlM-?si=cJiIW39-vmG_nQVF)\n\n**Evaluation**\n- **Ease of Use**: ⭐ Rated the least user-friendly. However, after initial setup, it proves effective despite its complexity and developer-centric design.\n- **Performance**: ⭐⭐⭐ For providing numerous customization options through its low-level control.\n- **Deployment**: ⭐⭐⭐⭐ Extremely versatile with high scores for production capabilities; multiple deployment methods are available.\n- **User Interface**: ⭐⭐ Lacks a proper design interface but allows debugging and visualizing workflows.\n\n## Key differences\n\nThe analysis reveals several important differences between the agents:\n- **CrewAI** is good for non-developers but has performance limitations.\n- **Autogen** excels in generating code and ease of securing data execution.\n- **LangGraph** offers greater customization and control, making it ideal for developer-heavy teams.\n\n\n\n## Applications and Use Cases\n\nUnderstanding where each framework excels:\n- **CrewAI** is suitable for simple agent tasks with less critical performance.\n- **Autogen** is favored for rapid prototyping and deployment in various environments.\n- **LangGraph** fits projects that require detailed control and customization, appealing to experienced development teams.\n\n## Conclusion\n\nSelecting the right agent framework depends heavily on your specific needs and team capabilities. \n\nThe insights gathered from the video provide a great foundation for understanding the complexities involved in choosing the right agent technology.\n\n\nFor further insights, watch the full video: \n\n[![1- CrewAI evaluation](images/thumbnail-crewai.png)](https://youtu.be/1zRyGq08ZcM?si=Mm9VM6_IZKTpZ_1X)\n\n[![2 - LangGraph evaluation](images/langgraph-thumbnail.png)](https://youtu.be/42TUwgEg94Y?si=Ca9mLfKNWAFn0VCr)\n\n[![3 - AutoGen](images/ag-thumbnail.png)](https://youtu.be/iKM3isuTpiI?si=iHzvcHhLHzeEYEi0)\n\n[![4 - Final comparison](images/final-thumbnail.png)](https://youtu.be/ChTx278XJAM?si=lch2qA3y6xzEJYYu)\n\nAccess the full code on [GitHub](https://github.com/fbellame/agent-comparison/tree/master/stock-analysis)\n\n## More about me\n\nPassionate about Generative AI since the launch of ChatGPT in November 2022, I enthusiastically explore the capabilities of this transformative technology. I blog, speak at conferences, [co-organize a local Generative AI meetup in Montreal](https://www.meetup.com/generative-ai-montreal/), and run a small [YouTube channel](https://www.youtube.com/@faridautomatic) dedicated to these experiments. \n\nFeel free to connect with me on LinkedIn!\n\n\n[![Farid Bellameche](images/farid.png)](https://www.linkedin.com/in/farid-bellameche-6606332/)\n\n"
    },
    {
      "name": "dragosMC91/AutoGen-Experiments",
      "stars": 11,
      "img": "https://avatars.githubusercontent.com/u/11348201?s=40&v=4",
      "owner": "dragosMC91",
      "repo_name": "AutoGen-Experiments",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2023-11-05T20:41:08Z",
      "updated_at": "2025-03-30T08:22:28Z",
      "topics": [],
      "readme": "1. [Getting Started](#getting-started)\n2. [Prerequisites](#prerequisites)\n3. [Python Version](#python-version)\n4. [Environment Setup](#environment-setup)\n    1. [Using pyenv](#using-pyenv)\n    2. [Using conda](#using-conda)\n    3. [Install project dependencies](#install-project-dependencies)\n5. [Configuration](#configuration)\n    1. [Define the LLMs config](#1-define-the-llms-config)\n    2. [Configure the proxy server UI](#2-configure-the-proxy-server-ui)\n6. [Useful tools](#useful-tools)\n    1. [Lint code](#1-lint-code)\n    2. [Review code from file using gpt4](#2-review-code-from-file-using-gpt4)\n    3. [Start the LiteLLM proxy server](#3-start-the-litellm-proxy-server)\n    4. ~~[Start Autogen Studio GUI (terminal usage recommended instead of ui)](#4-start-autogen-studio-gui-terminal-usage-recommended-instead-of-ui)~~ (temporarily deprecated)\n    5. [Monitor costs and view usage](#5-monitor-costs-and-view-usage)\n7. [Running the Applications](#running-the-applications)\n    1. [Start conversation](#1-start-conversation)\n    2. [Ask Chef](#2-ask-ai-chef)\n9. [Setup local and third party LLMs](#setup-local-and-third-party-llms)\n\nThis project is a small wrapper over [AutoGen](https://github.com/microsoft/autogen). It integrates the [LiteLLM](https://github.com/BerriAI/litellm) proxy server to give you easy access to any LLM (proprietary or open source), without having to leave the comfort of your terminal.\n\nIt uses the [Rich](https://github.com/Textualize/rich) and [prompt_toolkit](https://github.com/prompt-toolkit/python-prompt-toolkit) libraries to colorize inputs/ outputs which makes reading large responses in a plain terminal a bit more digestible:\n\nInput:\n![Example Terminal Output](docs/llm_terminal_input.jpg)\nOutput\n![Example Terminal Output](docs/llm_terminal_response.jpg)\n\n## Getting Started\n\nThis section provides instructions on how to set up your development environment for working on this project.\n\n## Prerequisites\n\nBefore you begin, ensure you have either `pyenv` or `conda` installed on your system to manage Python versions and environments.\n\n## Python Version\n\nThis project is tested with Python `3.11.8`. It is recommended to use this specific version to avoid any compatibility issues.\n\n## Environment Setup\n\n### Using [uv](https://github.com/astral-sh/uv) (*recommended*)\n\n```sh\nuv python install 3.11.8\nuv venv --python 3.11.8\nsource .venv/bin/activate\n```\n\n### Using [pyenv](https://github.com/pyenv/pyenv)\n\nIf you are using `pyenv`, you can install Python `3.11.8` and set it as the local version for the project using the following commands:\n\n```sh\npyenv install 3.11.8\npyenv local 3.11.8\npyenv virtualenv myenv\n```\n\n### Using conda\n\nIf you prefer `conda`, create a new environment with Python `3.11.8` using the following command:\n\n```sh\nconda create --name myenv python=3.11.8\nconda activate myenv\n```\n\nReplace `myenv` with a name of your choice for the environment.\n\n### Install project dependencies\n\nOnce you have the correct version of Python set up, install the project dependencies.\nOnly add `uv` prefix if you are using this tool.\n\n```sh\nuv pip install -e .\n```\n\nIf you also plan to setup and use the the litellm proxy dashboard UI, which offers neat costs monitoring and usage statistics, install `autogen-experiments[proxy]` extra dependencies via:\n```sh\nuv pip install -e '.[proxy]'\n```\n\nDepending on your use cases, you might need other extra dependencies like `autogen-experiments[text-compression]` | `autogen-experiments[rag]` which you can install individually or just install everything via\n```sh\npip install -e '.[all]'\n```\n\n## Configuration\n\n### 1. Define the LLMs config\nCopy the `.env.secrets.example` file to create your own `.env.secrets` file and fill in your specific API keys for services you plan you use like OpenAI, MistralAI, Anthropic models or others.\n\n```sh\ncp .env.secrets.example .env.secrets\n```\n\nEdit the `.env.secrets` file with your preferred text editor and update the API keys as needed.\n\n### 2. Configure the proxy server UI\nFirst you need to setup a postgres DB to store usage information from litellm. The easiest way to do this is to create a free [Superbase](https://supabase.com/) postgres instance hosted with AWS.\n\nOnce the DB is ready to use, make sure you can connect o it\n\n```sh\npsql postgresql://<user>:<password>@<host>:<port>/<dbname>\n```\n\nIf connection is successful, just set the `DATABASE_URL` env var in your `.env.secrets` file and you're ready to go. The next time you will start the litellm server, it will automatically connect to the DB.\n\nWith these steps, you should be ready to work on the project and run the applications.\n\n## Useful tools\n### 1. Lint code\nRun the following command\n```\npython setup.py fix\n```\n### 2. Review code from file using gpt4\n```\npython setup.py review --file path/to/file\n```\n### 3. Start the LiteLLM proxy server\nOne of [LiteLLM](https://litellm.vercel.app/docs/)'s strong points is that it offers a unified interface for calling various LLMs among other cool features.\nIn this context we use to extend AutoGen's capabilities to use other AI models (which is currently not supported out of the box, but is a WIP).\n```\npython setup.py litellm\n```\n\n### 4. Start Autogen Studio GUI (terminal usage recommended instead of ui)\nAutogen also offers a [neat UI](https://github.com/microsoft/autogen/tree/main/samples/apps/autogen-studio).\n\nThe following command starts the AGS app and makes the ui available at http://localhost:8083.\nThe AGS sqlite DB together with other related artifacts are stored in `src/ui`.\n\n```\npython setup.py ui\n```\n\n### 5. Monitor costs and view usage\nBy default, OpenAI model requests are not routed through litellm. This is because, if you're only using this LLM provider, it does not make sense to add an extra layer because you can easily monitor costs in https://platform.openai.com/usage.\nHowever, if you use multiple LLM providers, it makes sense to have all data aggregated within a single dashboard -> `LiteLLM UI`.\n\nIf the LiteLLM proxy server is up and running, then all openai requests will automatically be routed through the proxy.\n\nSteps:\n1. Start the litellm server -> make sure to go over step [2. Configure the proxy server UI](#2-configure-the-proxy-server-ui).\n2. Inspect http://localhost:30000/ui\n![LLM Spend](docs/llm_costs_dashboard.jpg)\n\n## Running the Applications\n\nAfter setting up the environment and configuration, you can run the applications within the `src/applications/` directory.\n\n\nExecuting an application creates a new conversation session based on the flow defined in that app.\n\n### 1. Start conversation\n\nThere are 2 generic conversation apps:\n1. `start_conversation.py` starts a simple conversation a selected agent from a list of predefined specialized agents.\n2. `start_curated_conversation.py` adds a critic AI to the conversation which reviews the main agent's claims\n\nNew custom agents can be added in the agent definition file `agents/custom_agents.py`.\n\n\nYou can either tweak the `message` inside each application, or execute the script and input your prompt in the command line (recommended).\nPrompts have multiline support so `Enter` just inserts a new line. In order to submit the prompt press `(Meta|Esc)+Enter`.\n\n### 2. Ask AI chef\n```\npython chef.py\n```\n\nThe `chef.py` application demonstrates how to use agents to facilitate a conversation about cooking. It shows the setup of a group chat with multiple agents and the initiation of a chat with a user query.\n\n---\n\n## Setup local and third party LLMs\n\n1. Install the [ollama tool](https://github.com/jmorganca/ollama)\n2. Download any model you want for example for codellama run\n```\nollama run codellama\n```\n3. Update the `litellm_config.yml` file to contain the new local model references or references to any other API services like [Mistral AI](https://mistral.ai/).\n4. Start the LiteLLM proxy server \nSee the [3. Start the LiteLLM proxy server](#3-start-the-litellm-proxy-server) section for more information.\n\nOther popular models: https://huggingface.co/WizardLM"
    },
    {
      "name": "thirdweb-dev/ai",
      "stars": 10,
      "img": "https://avatars.githubusercontent.com/u/79496167?s=40&v=4",
      "owner": "thirdweb-dev",
      "repo_name": "ai",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2025-03-13T05:23:03Z",
      "updated_at": "2025-04-21T00:42:28Z",
      "topics": [],
      "readme": "# thirdweb AI\n\n_AI Agents with Onchain Intelligence_\n\n## 📖 Overview\n\nthirdweb AI is thirdweb's comprehensive toolkit for blockchain data analysis, wallet management, and AI agent interaction with blockchains. It simplifies complex blockchain operations into four core components: Insight for data analysis, Engine for wallet and contract operations, Storage for decentralized file management, and Nebula for natural language-powered blockchain interactions.\n\n## 🌐 Features\n\n### Insight\nComprehensive blockchain data intelligence:\n- **Chains**: Multi-chain support and network information\n- **Transactions**: Transaction analysis and monitoring\n- **Blocks**: Block data exploration and metrics\n- **Events**: Smart contract event tracking and filtering\n- **Prices**: Real-time token price feeds\n- **Tokens**: Detailed token information and analytics\n\n### Engine\nCore blockchain interaction capabilities:\n- **Wallet**: Secure wallet management and transaction signing\n- **Read**: Read operations for smart contracts and blockchain data\n- **Write**: Transaction creation and contract interaction\n\n### Storage\nDecentralized storage capabilities:\n- **Upload**: Upload files, directories, and JSON data to IPFS\n- **Fetch**: Retrieve content from IPFS using thirdweb gateway\n\n### Nebula\nAI agent blockchain interaction:\n- **Natural Language Agent Action**: Completing blockchain tasks through natural language instructions\n\n## 🚀 Quickstart\n\n### MCP Server\n\n#### Installation\n\n```bash\n### Run using uvx\nTHIRDWEB_SECRET_KEY=... \\\n    uvx thirdweb-mcp\n\n### Install and run using pipx (and run thirdweb-mcp)\npipx install thirdweb-mcp\n\nTHIRDWEB_SECRET_KEY=... \\\n    thirdweb-mcp\n```\n\nMore [information](python/thirdweb-mcp)\n\n### Python SDK\n\n#### Installation\n\n```bash\n# Install core package with all framework adapters\npip install \"thirdweb-ai[all]\"\n\n# Or install with specific framework adapters\npip install \"thirdweb-ai[openai]\"    # For OpenAI Agents\npip install \"thirdweb-ai[langchain]\" # For LangChain\npip install \"thirdweb-ai[agentkit]\" # For Coinbase Agentkit\npip install \"thirdweb-ai[goat]\" # For GOAT SDK\n# ... many more framework supported\n```\n\nSee the list of [supported framework and installation guides](python/thirdweb-ai#install-with-framework-specific-adapters)\n\n#### Basic Usage\n\n```python\nfrom thirdweb_ai import Engine, Insight, Nebula, Storage, Tool\n\n# Initialize services\ninsight = Insight(secret_key=...)\nnebula = Nebula(secret_key=...)\nengine = Engine(...)\nstorage = Storage(secret_key=...)\n\n# Example: Create tools for AI agents\n# Option 1: Use Nebula alone (recommended when you need a self-sufficient blockchain agent)\n# Nebula already uses most other services internally\ntools = [\n    *nebula.get_tools(),\n]\n\n# Option 2: Use individual services directly without Nebula\n# tools = [\n#     *insight.get_tools(),\n#     *engine.get_tools(),\n#     *storage.get_tools(),\n# ]\n\n# Example: Framework integration (LangChain)\nfrom thirdweb_ai.adapters.langchain import get_langchain_tools\nlangchain_tools = get_langchain_tools(tools)\nagent = create_tool_calling_agent(tools=langchain_tools, ...)\n\n# Example: Framework integration (OpenAI Agents)\nfrom thirdweb_ai.adapters.openai import get_openai_tools\nopenai_tools = get_openai_tools(tools)\nagent = Agent(name=\"thirdweb Assistant\", tools=tools)\n\n# see python/examples for other framework integration\n```\n\nMore [information](python/thirdweb-ai)\n\n### TypeScript SDK\n\nComing soon.\n\n## 📜 Documentation\n\nFor comprehensive documentation, please visit:\n\n- [thirdweb Documentation](https://portal.thirdweb.com/)\n\n## 🚨 Security and Bug Reports\n\nWe take security seriously. If you discover a security vulnerability within thirdweb AI, please email security@thirdweb.com rather than using the issue tracker.\n\nFor non-security-related bugs, please use the GitHub issue tracker.\n\n## ⚠️ Important Usage Notes\n\nWhen using Nebula, do not combine it with other tools (Insight, Engine, Storage) in the same agent implementation as Nebula already calls these tools in the background. Using them together can lead to compatibility issues and unexpected behavior.\n\n## 📦 Publishing Workflow\n\nTo publish a new version of thirdweb AI packages:\n\n1. Create a git tag for the new version: `git tag -a v0.X.Y -m \"Release v0.X.Y\"`\n2. Push the tag to GitHub: `git push origin v0.X.Y`\n3. Go to GitHub and create a release using this tag\n4. The CI/CD pipeline will automatically build and publish both packages to PyPI with matching version numbers\n\n## 📧 Contact\n\n- **Website**: [thirdweb.com](https://thirdweb.com)\n- **X**: [@thirdweb](https://x.com/thirdweb)\n- **Telegram**: [Join our community](https://t.me/officialthirdweb)\n- **Discord**: [Join our community](https://discord.gg/thirdweb)\n- **Email**: support@thirdweb.com\n\n## 📝 License\n\nthirdweb AI is licensed under the Apache-2.0 License. See the [LICENSE](./LICENSE) file for details.\n"
    },
    {
      "name": "dspencej/UnscriptedAdventures",
      "stars": 10,
      "img": "https://avatars.githubusercontent.com/u/110250829?s=40&v=4",
      "owner": "dspencej",
      "repo_name": "UnscriptedAdventures",
      "description": "UnscriptedAdventures is a text-based RPG game that combines the creative storytelling of classic role-playing games with the power of Large Language Models (LLMs). Players interact with a dynamic Game Master (GM) and NPCs, making choices that shape an ever-evolving adventure.",
      "homepage": "",
      "language": "Python",
      "created_at": "2024-09-07T14:42:50Z",
      "updated_at": "2025-03-24T13:05:08Z",
      "topics": [
        "5e",
        "ai",
        "collaborative",
        "dynamic-narrative",
        "fastapi",
        "immersion",
        "interactive-fiction",
        "llm",
        "rpg",
        "storytelling",
        "text-based-game",
        "worlds-most-popular-rpg"
      ],
      "readme": "\n# UnscriptedAdventures\n\nUnscriptedAdventures is a text-based RPG game that leverages Large Language Models (LLMs) to deliver a dynamic, immersive experience. Players engage with a virtual Game Master (GM) and Non-Player Characters (NPCs) through a chatbot interface, making choices that directly influence the unfolding narrative. The application is built using FastAPI, a modern, high-performance web framework for building APIs with Python.\n\n**Note:** UnscriptedAdventures is currently in Alpha development. Documentation updates are ongoing. The project remains actively developed, and contributions are welcome. If you wish to contribute, please create a Pull Request.\n\n![UnscriptedAdventures Preview](https://raw.githubusercontent.com/dspencej/UnscriptedAdventures/main/images/social_preview.png)\n\n## Features\n\n- **Dynamic Storytelling:** A virtual GM driven by an LLM agent guides the story, adapting to player choices and actions in real-time.\n- **Character Creation:** Customize your character with a variety of traits, professions, and backgrounds to suit your play style.\n- **Engaging Interactions:** Communicate with NPCs and make decisions that influence the game's evolving storyline and outcomes.\n- **Reward System:** An innovative system evaluates player engagement and adjusts the GM's strategy to keep the experience exciting.\n\n## Getting Started\n\nFor detailed instructions, see the [Getting Started Guide](https://github.com/dspencej/UnscriptedAdventures/wiki/Getting-Started-Guide) in the Wiki.\n\n### Installation\n\nNote: Requires Python >=3.12.0\n\n1. **Clone the Repository:**\n\n   ```bash\n   git clone https://github.com/dspencej/UnscriptedAdventures.git\n   cd UnscriptedAdventures\n   ```\n\n2. **Create a Virtual Environment:**\n\n   ```bash\n   python -m venv .venv\n   source .venv/bin/activate  # For Windows, use `.venv\\Scripts\\activate`\n   ```\n\n3. **Install the Required Packages:**\n\n   ```bash\n   pip install -r requirements.txt\n   ```\n\n### Running the Application\n\n1. **Start the FastAPI Application with Uvicorn:**\n\n   ```bash\n   uvicorn app:app --reload\n   ```\n\n   The `--reload` flag enables auto-reloading, allowing the server to restart upon code changes. This is useful during development.\n\n2. **Open Your Browser:**\n\n   Visit `http://127.0.0.1:8000` in your web browser to start playing.\n\n3. **Interactive API Documentation:**\n\n   FastAPI provides interactive API documentation:\n\n   - **Swagger UI:** Accessible at `http://127.0.0.1:8000/docs`\n   - **ReDoc:** Accessible at `http://127.0.0.1:8000/redoc`\n\n## Contributing\n\nFor guidelines on contributing, see the [How to Contribute](https://github.com/dspencej/UnscriptedAdventures/wiki/How-to-Contribute) section of the Wiki.\n\n1. **Fork the repository** and clone it locally.\n2. **Create a new branch** for your feature or bug fix.\n3. **Make your changes** and commit them with clear and concise messages.\n4. **Push your changes** to your forked repository.\n5. **Create a Pull Request** to the main branch of the original repository.\n\n## License\n\nThis project is licensed under the MIT License—see the [LICENSE](LICENSE) file for details.\n\n## Contact\n\nIf you have any questions, feedback, or suggestions, feel free to join the [discussion](https://github.com/dspencej/UnscriptedAdventures/discussions) or open an [issue](https://github.com/dspencej/UnscriptedAdventures/issues/new).\n\nFor bug reports, you can use the [Bug Report Template](https://github.com/dspencej/UnscriptedAdventures/issues/new?assignees=&labels=&template=bug_report.md&title=Bug%3A). To suggest a feature, use the [Feature Request Template](https://github.com/dspencej/UnscriptedAdventures/issues/new?assignees=&labels=&template=feature_request.md&title=Feature+Request%3A).\n\n---\n\nEnjoy your next story in the world of UnscriptedAdventures!\n\n![UnscriptedAdventures Preview](https://raw.githubusercontent.com/dspencej/UnscriptedAdventures/main/images/social_preview_2.png)\n"
    },
    {
      "name": "joslat/AgenticAIAutoGen",
      "stars": 9,
      "img": "https://avatars.githubusercontent.com/u/9831011?s=40&v=4",
      "owner": "joslat",
      "repo_name": "AgenticAIAutoGen",
      "description": "Demos for my Talk at .NET Day Switzerland",
      "homepage": null,
      "language": "C#",
      "created_at": "2024-08-26T15:07:19Z",
      "updated_at": "2025-01-31T13:44:36Z",
      "topics": [],
      "readme": "# AgenticAIAutoGen\nDemos and Slides for my AI Agents Talk at .NET Day Switzerland\n\n## Slides\nAgentic AI -Unleash Your AI Potential with AutoGen - final.pdf\nNote I have updated the perception on agents at the beginning of the talk and at the end (using Menti)\n\n## Code\n### A01_MultiAgent_Critic_ToM_Argument_CheatSheet.cs\nThe first demo showcases a writer-critic workflow applying ToM to create an Argumentary and polish it\nthrough subsequent reviews. \nUse it for discussing next year's .NET Day Switzerland!!! ;)\n(...and let me know if it helped! Hint: it should!)\n\n### A02_Reflection_with_Multi_Critic.cs\nThis second demo showcases the creation of what I'd like to call \"committee of experts\" and use \nin the context of a critic workflow, with a summarization and wrapped in a critic agent.\nAlso to highlight the value of the Groundedness check (I call it fact checker - FactChecker)\nand it ensures that any hallucinations or facts it believes to be true (Gpt-4o mini) - as it \nhas been trained up to Sept 2023 it knows of .NET Day Switzerland 2023 and earlier...\nSo you can see how it corrects itself and the substantial improvement this layer does. \n\nAgain, thanks so much to LittleLittleCloud, Xiaoyun Zhang, for his help and curiosity - was\ntalking to him as I would introduce him and his efforts on the .NET side of AutoGen... and \nhe was curious on the code and once I shared it, he provided a PR with improvements and we \nhad a call over it with suggestions and ideas - which was very motivating.\nThanks Xiaoyun! \n\nYou can check his valuable GitHub at https://github.com/LittleLittleCloud\n\n"
    },
    {
      "name": "tezansahu/ai-garage",
      "stars": 9,
      "img": "https://avatars.githubusercontent.com/u/31898274?s=40&v=4",
      "owner": "tezansahu",
      "repo_name": "ai-garage",
      "description": "Mini-Projects using Cutting-Edge AI Frameworks",
      "homepage": "",
      "language": "Python",
      "created_at": "2024-12-25T22:55:25Z",
      "updated_at": "2025-04-07T01:08:06Z",
      "topics": [
        "ai",
        "ai-agents",
        "artificial-intelligence",
        "autogen",
        "crewai",
        "hands-on",
        "mini-projects",
        "rag"
      ],
      "readme": "# AI Garage 🛠️\n\nWelcome to **AI Garage**, your one-stop repository for exploring the cutting edge of Artificial Intelligence! \n\nDive into projects showcasing the cutting-edge applications of:\n- Large Language Models (LLMs)\n- Multimodal LLMs\n- Retrieval-Augmented Generation (RAG)\n- Agentic Frameworks\n- & more...\n\nThis space is designed to combine deep learning with hands-on experimentation, helping you stay ahead in the rapidly evolving AI landscape.\n\n\nWhether you're just beginning your AI journey, honing your skills as a practitioner, or pushing boundaries as a researcher, this repository has resources to fuel your growth and creativity.\n\n## Projects\n\n- [AI Powered Diagram Generator](./diagram-generator/)\n- [Multi-Agent System for Software System Design (Powered by AutoGen)](./autogen-system-design-agent/)\n- [Agentic Workflow for Job Description-Based Resume Optimization (Powered by CrewAI)](./job-description-based-resume-optimizer/)\n- [Finance AI Agent using Model Context Protocol (MCP)](./mcp-agent/)\n- [Swarm Multiagent Debate using Autogen & Chainlit](./swarm-multiagent-debate-autogen-chainlit/)\n- [Data Analyst Agent (using AutoGen)](./data-analyst-agent-autogen/) - [⭐ Try it out LIVE ⭐](https://data-analyst-agent.streamlit.app/)\n\n> **More Projects Coming Soon**\n>\n> _Stay tuned for more exciting projects that will be added to this repository. Each project will come with detailed documentation to help you get started quickly._\n\n## Contributing\n\nContributions are welcome! If you have an interesting AI project or improvement, feel free to open a pull request or create an issue.\n\n## License\n\nThis repository is licensed under the GNU GPLv3 License. See the LICENSE file for more details.\n\n---\n\nHappy experimenting and building with AI!"
    },
    {
      "name": "up1/workshop-ai-with-technical-team",
      "stars": 9,
      "img": "https://avatars.githubusercontent.com/u/25113?s=40&v=4",
      "owner": "up1",
      "repo_name": "workshop-ai-with-technical-team",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2023-12-11T17:18:57Z",
      "updated_at": "2025-04-22T08:16:56Z",
      "topics": [],
      "readme": "## Generative AI for Softwre Development\n* Design\n* Develop\n* Test\n* Deploy\n\n## Coding and workshop\n* [Working with OpenAI, Anthropic, Local LLM](https://github.com/up1/workshop-ai-with-technical-team/tree/main/workshop/prompt)\n* Demo application\n  * [Chatbot](https://github.com/up1/workshop-ai-with-technical-team/tree/main/workshop/demo-chatbot) \n  * [Text to diagram](https://github.com/up1/workshop-ai-with-technical-team/wiki/Text-to-diagram) \n  * [Text to SQL](https://github.com/up1/workshop-ai-with-technical-team/tree/main/workshop/demo-sql)\n* [RAG workshop](https://github.com/up1/workshop-ai-with-technical-team/tree/main/workshop/rag)\n  * Vector database\n    * FAISS\n    * MongoDB Atlas\n    * ChromaDB\n    * [Redis](https://github.com/up1/course-redis/tree/main/workshop/demo-redis/demo-vector)\n* [Working Nodejs](https://github.com/up1/demo-openai-prompt-nodejs)\n* Working with Langchain library\n  * [CSV Agent](https://github.com/up1/workshop-ai-with-technical-team/tree/main/workshop/demo-agent/data-analysis)\n  * [Agent AI with AutoGen](https://github.com/up1/workshop-ai-with-technical-team/tree/main/workshop/agent-ai/demo-autogen)\n\n  \n"
    },
    {
      "name": "LittleLittleCloud/PowershellGPT",
      "stars": 8,
      "img": "https://avatars.githubusercontent.com/u/16876986?s=40&v=4",
      "owner": "LittleLittleCloud",
      "repo_name": "PowershellGPT",
      "description": "A multi-agent workflow that resolve tasks using powershell",
      "homepage": "",
      "language": "C#",
      "created_at": "2024-04-25T07:13:02Z",
      "updated_at": "2025-03-14T14:30:38Z",
      "topics": [
        "autogen",
        "autogen-net",
        "gpt",
        "llm",
        "llm-agent",
        "multi-agent-systems",
        "powershell"
      ],
      "readme": "## PS.GPT\n\nA dotnet tool which uses multi-agent workflow to resolve tasks using powershell scripts.\n\nTo those who struggles with lengthy powershell command.\n\n![NuGet Version](https://img.shields.io/nuget/v/PS.GPT)\n\n\n### Get start\n> [!Note]\n> Before running the tool, you need to set up the following environment variables:\n> - `OPENAI_API_KEY`: The key of the OpenAI service.\n> - `OPENAI_MODEL_ID`: The model to use, if not set, it will use the `gpt-3.5-turbo-0125`.\n> \n> If you want to use Azure OpenAI service, you need to set up the following environment variables:\n> - `AZURE_OPENAI_ENDPOINT`: The endpoint of the Azure OpenAI service.\n> - `AZURE_OPENAI_KEY`: The key of the Azure OpenAI service.\n> - `AZURE_OPENAI_DEPLOY_NAME`: The model of the Azure OpenAI service.\n\n#### Install the tool\n```bash\ndotnet tool install -g PS.GPT\n```\n#### Run the tool using ps-gpt command\n```bash\nps-gpt\n```\n#### Ask your question\n```bash\nlist all files assending by size\n```\n\n### Examples\nBelow are some examples of the tasks that can be resolved using this workflow.\n\n#### listing all files and its size, and sort by size in descending order.\n![Example](asset/output.gif)\n\n### Workflow overview\n![Workflow](asset/image.png)\n\n### Agents overview\n- User: accepts user input and send it to manager\n- Manager: Create task based on received user question. If the question is non-related to powershell or require more information, it will be sent to customer service for further assistance. Otherwise, it will be sent to enginner for resolution.\n- Engineer: Resolve the task using powershell script and send script to powershell agent for execution.\n- Powershell: Execute the script. If succeed, the result will be sent directly back to user. Otherwise, it will be sent to engineer for fixing.\n- Customer service: Asking user for more information or send the question back to user if it is non-related to powershell.\n\n### Extending the workflow\nThe workflow can be easily extended to support the following scenarios:\n- approve script before execution: Asking user for approval before executing the script.\n- support more bash languages: Adding more engineers!\n"
    },
    {
      "name": "GridfireAI/reddit-mcp",
      "stars": 8,
      "img": "https://avatars.githubusercontent.com/u/204763572?s=40&v=4",
      "owner": "GridfireAI",
      "repo_name": "reddit-mcp",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2025-03-24T17:45:32Z",
      "updated_at": "2025-04-11T12:34:24Z",
      "topics": [],
      "readme": "# Reddit MCP\n\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n\nA plug-and-play [MCP](https://modelcontextprotocol.io) server to browse, search, and read Reddit.\n\n## Demo\nHere's a short video showing how to use this in Claude Desktop:\n\nhttps://github.com/user-attachments/assets/a2e9f2dd-a9ac-453f-acd9-1791380ebdad\n\n## Features\n\n- Detailed parameter validation with [pydantic](https://docs.pydantic.dev)\n- Uses the reliable [PRAW](https://praw.readthedocs.io/) library under the hood\n- Built-in rate limiting protection thanks to PRAW\n\n## Caveats\n- Only supports read features for now. If you want to use write features, upvote the [issue](https://github.com/GridfireAI/reddit-mcp/issues/1) or [send a PR](CONTRIBUTING.md)! 🙌\n- Tools use tokens. To use this with Claude, you may need to be a Pro user to use many tool calls. Free tier users should be fine with lighter tool usage. Your token usage is your responsibility.\n\n## Installation\n\n### Prerequisite: Reddit API credentials\n\nCreate a [developer app](https://www.reddit.com/prefs/apps) in your Reddit account if you don't already have one. This will give you a `client_id` and `client_secret` to use in the following steps. If you already have these, you can skip this step.\n\n### Claude Desktop\n\nTo install into Claude Desktop:\n\n- Follow the instructions [here](https://modelcontextprotocol.io/quickstart/user) until the section \"Open up the configuration file in any text editor.\"\n- Add the following to the file depending on your preferred installation method:\n\n### Using [uvx](https://docs.astral.sh/uv/guides/tools/) (recommended)\n\n```json\n\"mcpServers\": {\n  \"reddit\": {\n    \"command\": \"uvx\",\n    \"args\": [\"reddit-mcp\"],\n    \"env\": {\n      \"REDDIT_CLIENT_ID\": \"<client_id>\",\n      \"REDDIT_CLIENT_SECRET\": \"<client_secret>\"\n    }\n  }\n}\n```\n\n### Using PIP\n\nFirst install the package:\n\n```bash\npip install reddit-mcp\n```\n\nThen add the following to the configuration file:\n\n```json\n\"mcpServers\": {\n  \"reddit\": {\n    \"command\": \"python\",\n    \"args\": [\"-m\", \"reddit_mcp\"],\n    \"env\": {\n      \"REDDIT_CLIENT_ID\": \"<client_id>\",\n      \"REDDIT_CLIENT_SECRET\": \"<client_secret>\"\n    }\n  }\n}\n```\n\n### Others\n\nYou can use this server with any [MCP client](https://modelcontextprotocol.io/docs/clients), including agent frameworks (LangChain, LlamaIndex, AutoGen, etc). For an example AutoGen integration, check out the [example](examples/autogen).\n\n## Tools\n\nThe tools the server will expose are:\n\n| Name                         | Description                              |\n| ---------------------------- | ---------------------------------------- |\n| `get_comment`                | Access a comment                         |\n| `get_comments_by_submission` | Access comments of a submission          |\n| `get_submission`             | Access a submission                      |\n| `get_subreddit`              | Access a subreddit by name               |\n| `search_posts`               | Search posts in a subreddit              |\n| `search_subreddits`          | Search subreddits by name or description |\n\n## Contributing\n\nContributions are welcome! See [CONTRIBUTING.md](CONTRIBUTING.md) for more information.\n\n## Acknowledgments\n\n- [PRAW](https://praw.readthedocs.io/) for an amazingly reliable library 💙\n"
    },
    {
      "name": "DonnC/pywce",
      "stars": 8,
      "img": "https://avatars.githubusercontent.com/u/47761288?s=40&v=4",
      "owner": "DonnC",
      "repo_name": "pywce",
      "description": "A template-driven WhatsApp ChatBot development framework",
      "homepage": "https://docs.page/donnc/wce",
      "language": "Python",
      "created_at": "2025-01-05T15:45:55Z",
      "updated_at": "2025-03-16T15:17:06Z",
      "topics": [
        "chatbot",
        "framework",
        "hooks",
        "pywce",
        "template",
        "whatsapp"
      ],
      "readme": "# WhatsApp ChatBot Engine\n\nA framework for creating WhatsApp chatbots of any scale using a template-driven approach - \nallowing you to define conversation flows and business logic in a clean and modular way. \n\n> [!NOTE]\n> Template engine and WhatsApp client library are decoupled - allowing you to use them independently or together. \n\n\n## Features\n\n- **Template-Driven Design**: Use YAML templates for conversational flows.\n- **Hooks for Business Logic**: Attach Python functions to process messages or actions.\n- Focus on your conversation flow and business logic.\n- Easy-to-use API for WhatsApp Cloud.\n- Supports dynamic messages with placeholders.\n- Built-in support for WhatsApp Webhooks.\n- Starter templates\n- Live Support / Human interaction portal template\n\n## Installation\n```bash\n\npip install pywce\n```\n\n\n---\n\n## Why pywce\nMost WhatsApp chatbot tutorials or libraries just scraps the surface, only sending a few message or handling simple logic or are client libraries only.\n\nThis library gives you a full-blown framework for chatbots of any scale allowing you access to full package of whatsapp client library and chatbot development framework.\n\n---\n\n## Setup\n### WhatsApp\nFollow the complete step by step WhatsApp Cloud API guide below. \n\n[![WhatsApp Cloud API Complete Setup Guide](https://img.youtube.com/vi/Y8kihPdCI_U/0.jpg)](https://www.youtube.com/watch?v=Y8kihPdCI_U)\n\nImportant settings needed for this framework\n1. Phone number ID (be it test number or live number)\n2. Access Token (Temporary or permanent)\n3. Webhook callback verification token of your choice\n\nCreate a `.env `with the below settings in your project or test folder (be it `example` or `portal` folders)\n\n```\nACCESS_TOKEN        = <your-whatsapp-access-token>\nPHONE_NUMBER_ID     = <your-number-phone-id>\nWEBHOOK_HUB_TOKEN   = <your-webhook-verification-token>\n\n# path to your templates & triggers folders\nTEMPLATES_DIR       = portal/chatbot/templates\nTRIGGERS_DIR        = portal/chatbot/triggers\n\n# your templates initial or start stage\nSTART_STAGE         = START-MENU\n```\n\n### Engine\nYou can either use `.env` or add your credentials directly to the WhatsAppConfig class\n```python\nimport os\nfrom dotenv import load_dotenv\nfrom pywce import client, Engine, EngineConfig\n\nload_dotenv()\n\nwhatsapp_config = client.WhatsAppConfig(\n    token=os.getenv(\"ACCESS_TOKEN\"),\n    phone_number_id=os.getenv(\"PHONE_NUMBER_ID\"),\n    hub_verification_token=os.getenv(\"WEBHOOK_HUB_TOKEN\")\n)\n\nwhatsapp = client.WhatsApp(whatsapp_config=whatsapp_config)\n\nengine_config = EngineConfig(\n    whatsapp=whatsapp,\n    templates_dir=os.getenv(\"TEMPLATES_DIR\"),\n    trigger_dir=os.getenv(\"TRIGGERS_DIR\"),\n    start_template_stage=os.getenv(\"START_STAGE\")\n)\n\nengine_instance = Engine(config=engine_config)\n```\n\n## Example ChatBot\nHere's a simple example template to get you started:\n\n> [!NOTE]\n> _Checkout complete working examples in the [example folder](https://github.com/DonnC/pywce/blob/master/example)_\n\n\n1. Define YAML template (Conversation Flow💬):\n\n```yaml\n# path/to/templates\n\"START-MENU\":\n  type: button\n  template: \"example.hooks.name_template.username\"\n  message:\n    title: Welcome\n    body: \"Hi {{ name }}, I'm your assistant, click below to start!\"\n    footer: pywce\n    buttons:\n      - Start\n  routes:\n    \"start\": \"NEXT-STEP\"\n\n\"NEXT-STEP\":\n  type: text\n  message: Great, lets get you started quickly. What is your age?\n  routes:\n    \"re://d{1,}\": \"NEXT-STEP-FURTHER\"\n```\n\n2. Write your hook (Supercharge⚡):\n```python\n# example/hooks/name.py\nfrom pywce import hook, HookArg, TemplateDynamicBody\n\n@hook\ndef username(arg: HookArg) -> HookArg:\n    # set render payload data to match the required template dynamic var\n    \n    # greet user by their whatsapp name 😎\n    arg.template_body = TemplateDynamicBody(\n        render_template_payload={\"name\": arg.user.name}\n    )\n\n    return arg\n```\n\n3. Engine client:\n\nUse `fastapi` or `flask` or any python library to create endpoint to receive whatsapp webhooks\n\n```python\n# ~ fastapi snippet ~\n\nasync def webhook_event(payload: Dict, headers: Dict) -> None:\n    \"\"\"\n    Process webhook event in the background using pywce engine.\n    \"\"\"\n    await engine_instance.process_webhook(payload, headers)\n\n@app.post(\"/chatbot/webhook\")\nasync def process_webhook(request: Request, background_tasks: BackgroundTasks):\n    \"\"\"\n    Handle incoming webhook events from WhatsApp \n    and process them in the background.\n    \"\"\"\n    payload = await request.json()\n    headers = dict(request.headers)\n\n    # handle event in the background\n    background_tasks.add_task(webhook_event, payload, headers)\n\n    # Immediately respond to WhatsApp with acknowledgment\n    return Response(content=\"ACK\", status_code=200)\n```\n\n### Run ChatBot\nIf you run your project or the example projects successfully, your webhook url will be available on `localhost:port/chatbot/webhook`.\n\n_You can use `ngrok` or any service to tunnel your local service_\n\nYou can then configure the endpoint in Webhook section on  Meta developer portal.\n\n## Live Support \nEngine comes with a default basic live support /  human interaction portal out-of-the-box powered by [Reflex](https://reflex.dev/)\n\nCheck out [Live Support Portal](https://github.com/DonnC/pywce/tree/feat/live-support/portal)\n\n## WhatsApp Client Library\n> [!NOTE]\n> _You can use pywce as a standalone whatsapp client library. See [Example](https://github.com/DonnC/pywce/blob/master/example/chatbot)_\n\nPyWCE provides a simple, Pythonic interface to interact with the WhatsApp Cloud API:\n\n- **Send messages** (text, media, templates, interactive)\n- **Receive and process webhooks**\n- **Media management** (upload and download)\n- **Out of the box utilities** using the `WhatsApp.Utils` class.\n\nExample usage:\n\n```python\nfrom pywce import client\n\nconfig = client.WhatsAppConfig(\n    token=\"your_access_token\",\n    phone_number_id=\"your_phone_number_id\",\n    hub_verification_token=\"your_webhook_hub_verification_token\"\n)\n\nwhatsapp = client.WhatsApp(whatsapp_config=config)\n\n# Sending a text message\nresponse = whatsapp.send_message(\n    recipient_id=\"recipient_number\",\n    message=\"Hello from PyWCE!\"\n)\n\n# verify if request was successful, using utils\nis_sent = whatsapp.util.was_request_successful(\n    recipient_id=\"recipient_number\",\n    response_data=response\n)\n\nif is_sent:\n    message_id = whatsapp.util.get_response_message_id(response)\n    print(\"Request successful with msg id: \", message_id)\n```\n\n\n## Documentation\n\nVisit the [official documentation](https://docs.page/donnc/wce) for a detailed guide.\n\n## Changelog\n\nVisit the [changelog list](https://github.com/DonnC/pywce/blob/master/CHANGELOG.md)  for a full list of changes.\n\n## Contributing\n\nWe welcome contributions! Please check out the [Contributing Guide](https://github.com/DonnC/pywce/blob/master/CONTRIBUTING.md) for details.\n\n## License\n\nThis project is licensed under the MIT License. See the [LICENSE](https://github.com/DonnC/pywce/blob/master/LICENCE) file for details.\n"
    },
    {
      "name": "neondatabase-labs/multi-agent-ai-azure-neon-openai",
      "stars": 8,
      "img": "https://avatars.githubusercontent.com/u/183852044?s=40&v=4",
      "owner": "neondatabase-labs",
      "repo_name": "multi-agent-ai-azure-neon-openai",
      "description": "Multi-Agent AI Example with LangChain, AutoGen, Azure OpenAI, and Neon Serverless Postgres",
      "homepage": "",
      "language": "Python",
      "created_at": "2025-02-10T14:59:48Z",
      "updated_at": "2025-02-27T03:13:30Z",
      "topics": [
        "ai-agents",
        "autogen",
        "azure",
        "langchain-python",
        "neondb",
        "openai",
        "postgres"
      ],
      "readme": "# Multi-Agent AI Example with LangChain, AutoGen, Azure OpenAI, and Neon Serverless Postgres\n\nThis repository demonstrates how to build a **multi-agent AI solution** using: \n- **LangChain** for natural language to SQL translation.\n- **[AutoGen](https://github.com/microsoft/autogen)** for coordinating AI agents in collaborative workflows.\n- **Azure OpenAI GPT-4o** for intelligent language understanding and generation of SQL queries in Neon PostgreSQL.\n- **Neon Serverless Postgres** for scalable database branching and dynamic data management.\n\nThe application showcases a shipping company where AI agents manage shipments, customers, and product information. The main goal is to demonstrate how AI agents can not only **query data** but also **act on it**, extending the \"Chat With Your Data\" concept to \"Chat and Act on Your Data.\"\n\n![Multi-Agent AI Example with LangChain, AutoGen, Azure OpenAI, and Neon Serverless Postgres](/assets/multi-ai-agents.png)\n\n---\n\n## Features\n\n- 🌐 **Gradio UI**: User-friendly interface for natural language interactions.\n- 🤖 **AutoGen Multi-Agent**: AI agents collaborate for specific tasks:\n  - **SchemaAgent**: Manages database schema retrieval and sharing.\n  - **ShipmentAgent**: Handles shipment-related queries and updates. Uses the *send_shipment* stored procedure.\n  - **CRMAgent**: Manages customer and product-related data. Uses the *add_customer* stored procedure.\n\n---\n\n## Getting Started\n\n### 1. Prerequisites\n\n- Python 3.9+ (Recommended: 3.10 or 3.11)\n- A **Neon** account ([Sign up for Neon](https://neon.tech))\n- **[Azure OpenAI API Endpoint](https://learn.microsoft.com/en-us/azure/ai-services/openai/quickstart?tabs=command-line%2Ckeyless%2Ctypescript-keyless%2Cpython-new&pivots=programming-language-python#retrieve-key-and-endpoint), Model Deployment Name and API Key** (For GPT-4)\n\n---\n\n### **2. Setup Instructions**\n\n#### **Clone the Repository**\n```bash\ngit clone https://github.com/your-username/multi-agent-ai-azure-neon-openai.git\ncd multi-agent-ai-azure-neon-openai\n```\n\n#### Create and Configure a Neon Serverless Postgres Database\n\nYou can manually create a **Neon project** in your [Neon Console](https://console.neon.tech/) or via **[Neon API](https://neon.tech/docs/reference/api-reference)**.\n\n**Initialize Schema & Data:**\nRun the SQL queries using [Neon Console SQL Editor](https://console.neon.tech/) to create databases, tables, insert sample data, and create stored procedures:\n\nFirst run SQL queries in `sql/schema_crm.sql` and then `sql/schema_shipment.sql` files.\n\n- 1️⃣ **CRM Database:** Stores **customer and product** information.\n- 2️⃣ **Shipment Database:** Stores **shipment tracking and logistics** data.\n\n---\n\n#### Configure .env File\n\nCreate a **.env** file in the root directory with the following contents:\n```ini\n# OpenAI API\nAZURE_OPENAI_KEY=your_openai_api_key\nAZURE_OPENAI_ENDPOINT=https://your-openai-endpoint\nAZURE_OPENAI_DEPLOYMENT=gpt-4o\n\n# Neon Database URIs\nNEON_CRM_DB_URI=\nNEON_SHIPMENT_DB_URI=\n\n```\n\n🚨 **Important:** Ensure you update these values with your **actual credentials.**\n\n---\n\n### **3. Virtual Environment & Dependencies**\n\n#### **Create a Virtual Environment**\n```bash\npython3 -m venv .venv\n```\n\n#### **Activate the Virtual Environment**\n- **Windows:**\n  ```bash\n  .venv\\Scripts\\activate\n  ```\n- **Mac/Linux:**\n  ```bash\n  source .venv/bin/activate\n  ```\n\n#### **Install Required Libraries**\n```bash\npip install -r requirements.txt\n```\n\n---\n\n## **Usage Examples**\n\n### **🔍 Chat With Your Data**\n✅ **Example Questions:**\n- \"Which products are currently in transit?\"\n- \"Is Alice Johnson a customer?\"\n\n### **💡 Database Development Assistance**\n✅ **Example Task:**\n- \"I need to create a stored procedure to delete customers. What would be the best way to do this?\"\n\n### **🚀 Acting on Data**\n✅ **Example Actions:**\n- \"Can you add Marc with email `marc@contoso.com`, phone `+1 123 456 7890`, and address `1 Main Street, Redmond`?\"\n- \"Can you create a new shipment of **1 Laptop and 1 Smartphone** for Marc and update its status to **Departed Origin** from New York to Los Angeles?\"\n\n---\n\n## **🔧 Running the Gradio UI**\n\n```bash\npython -m app.app\n```\n\nThis will launch a **Gradio UI** where you can interact with the multi-agent AI chat.\n\n---\n\n"
    },
    {
      "name": "detoxio-ai/ai-red-teaming-training",
      "stars": 8,
      "img": "https://avatars.githubusercontent.com/u/165997948?s=40&v=4",
      "owner": "detoxio-ai",
      "repo_name": "ai-red-teaming-training",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2024-12-28T10:23:54Z",
      "updated_at": "2025-04-19T06:54:55Z",
      "topics": [],
      "readme": "# **AI Red Teaming**\n\nThis repository contains a structured and detailed guide on **AI Red Teaming**, exploring its history, risks, techniques, and architectural principles. The book provides both theoretical and practical insights into the red teaming of AI systems, covering foundational knowledge and hands-on demonstrations.\n\n<img src=\"https://github.com/user-attachments/assets/c19c4858-5e81-44cc-923f-45ca8062f8c7\" alt=\"AI Red Teaming Book Cover\" width=\"1000\" />\n\n---\n\n## **Introduction**\n\nAI Red Teaming is a structured process designed to evaluate the security, robustness, and ethical alignment of artificial intelligence systems. This book is an effort to guide practitioners, researchers, and enthusiasts in understanding and implementing AI red teaming strategies effectively.\n\nThrough this repository, you will:\n- Learn the history and evolution of AI risks and failures.\n- Understand the taxonomy of AI attacks and adversarial techniques.\n- Dive into the inner workings of large language models (LLMs), including transformers, tokenization, and hyperparameters.\n- Gain hands-on experience with tools and techniques such as adversarial testing and jailbreaking.\n\n---\n\n## **Structure**\n\nThe content is divided into chapters across four main sections: **AI Red Teaming**, **LLM Architecture**, **Prompt Injections**, and **LLM Training**. Below is the structure with direct links to the files.\n\n### **1. Introduction to AI Attacks**\n- [1.1 History of AI Risks](content/1_intro_ai_red_teaming/1_1_history_of_ai_risks.md)\n- [1.2 AI Risks](content/1_intro_ai_red_teaming/1_2_ai_risks.md)\n- [1.3 What is AI Red Teaming?](content/1_intro_ai_red_teaming/1_3_ai_red_teaming.md)\n- [1.4 AI Attacks Taxonomy (Part 1)](content/1_intro_ai_red_teaming/1_4_ai_attacks_taxonomy_part_1.md)\n- [1.5 AI Attacks Taxonomy (Part 2)](content/1_intro_ai_red_teaming/1_5_ai_attacks_taxonomy_part_2.md)\n- [1.6 Jailbreaking Demo](content/1_intro_ai_red_teaming/1_6_jailbreaking_demo.md)\n\n### **2. LLM Architecture for Security Professionals**\n- [2.1 History of LLMs](content/2_llm_architecture/2_1_history_of_llms.md)\n- [2.2 Tokenization](content/2_llm_architecture/2_2_tokenization.md)\n- [2.3 Self-Attention](content/2_llm_architecture/2_3_self_attention.md)\n- [2.4 Transformer Architecture](content/2_llm_architecture/2_4_transformer.md)\n- [2.5 Hyperparameters](content/2_llm_architecture/2_5_hyperparameters.md)\n- [2.6 LLM Comparisons](content/2_llm_architecture/2_6_llm_comparisons.md)\n\n### **3. Prompt Injections**\n- [3.1 Intro to Prompt Injections](content/3_prompt_injections/3_1_intro_prompt_injections.md)\n- [3.1.1 Real-life Prompt Injection Examples](content/3_prompt_injections/3_1_1_realife_prompt_injection_examples.md)\n- [3.2 Anatomy of a Prompt](content/3_prompt_injections/3_2_anatomy_of_a_prompt.md)\n- [3.3 Craft a Prompt Injection Prompt](content/3_prompt_injections/3_3_craft_a_prompt_injection_prompt.md)\n- [3.4 Basics of Prompt Injection Techniques](content/3_prompt_injections/3_4_basics_of_prompt_injection_techniques.md)\n- [3.5 Intermediate Prompt Injection Techniques](content/3_prompt_injections/3_5_intermediate_prompt_injection_techniques.md)\n- [3.7 Try Prompt Injections on Medusa Pokebot](content/3_prompt_injections/3_7_try_prompt_injections_on_medusa_pokebot.md)\n- [3.8 Advanced Prompt Injections](content/3_prompt_injections/3_8_advance_prompt_injections.md)\n\n### **4. LLM Training**\n- [4.1 Inference Advanced Parameters](content/4_llm_training/4_1_inference_advance_params.md)\n- [4.2 Foundation Model Training](content/4_llm_training/4_2_foundation_model_training.md)\n- [4.3 Fine-Tuning LLMs](content/4_llm_training/4_3_finetuing_llms.md)\n- [4.4 Retrieval-Augmented Generation (RAG)](content/4_llm_training/4_4_rag.md)\n\n---\n\n## **Getting Started**\n\nTo begin exploring the content:\n1. Clone this repository:\n   ```bash\n   git clone https://github.com/your-repository-name.git\n   ```\n2. Navigate through the chapters using the links provided in the **Structure** section.\n\n---\n\n## **How to Contribute**\n\nContributions are highly encouraged! Here's how you can get involved:\n\n### 1. **Submit a Ticket**\n   - Found a bug, typo, or inconsistency?  \n     You can contribute by creating a ticket or an issue.  \n     To do so:\n     1. Go to the **Issues** tab in this repository.\n     2. Click on **New Issue**.\n     3. Fill in the issue template with relevant details.\n     4. Submit your ticket to notify maintainers of the problem or suggestion.\n\n   This is an excellent way to contribute even if you're not comfortable with coding or writing directly in the repository.\n\n### 2. **Fork the Repository**\n   - Click the **Fork** button at the top right of this repository to create your own copy.\n\n### 3. **Clone Your Fork**\n   - Clone your forked repository to your local machine:\n     ```bash\n     git clone https://github.com/your-username/ai-red-teaming.git\n     ```\n\n### 4. **Create a Branch**\n   - Create a new branch to work on a specific feature or fix:\n     ```bash\n     git checkout -b feature-or-fix-name\n     ```\n\n### 5. **Make Changes**\n   - Add your contributions or edits to the appropriate file(s).\n\n### 6. **Test Your Changes**\n   - Ensure that your changes are accurate and do not introduce issues.\n\n### 7. **Commit Changes**\n   - Commit your changes with a descriptive message:\n     ```bash\n     git add .\n     git commit -m \"Add detailed description of your changes\"\n     ```\n\n### 8. **Push to Your Fork**\n   - Push your changes to your forked repository:\n     ```bash\n     git push origin feature-or-fix-name\n     ```\n\n### 9. **Create a Pull Request**\n   - Go to the original repository and create a Pull Request (PR) from your forked branch. Describe your changes and why they should be merged.\n\n---\n\n## **License**\n\nThis project is licensed under the MIT License. See the [LICENSE](LICENSE) file for details.\n"
    },
    {
      "name": "Shailender-Youtube/Stock-Analysis-AutoGen-Multi-Agent",
      "stars": 8,
      "img": "https://avatars.githubusercontent.com/u/174622601?s=40&v=4",
      "owner": "Shailender-Youtube",
      "repo_name": "Stock-Analysis-AutoGen-Multi-Agent",
      "description": "Multi Agent Stock Analysis App using AutoGen 0.4 and Azure AI Agent Service",
      "homepage": null,
      "language": "Python",
      "created_at": "2025-01-29T11:23:22Z",
      "updated_at": "2025-04-21T15:15:23Z",
      "topics": [],
      "readme": "# Stock-Analysis-AutoGen-Multi-Agent\nMulti Agent Stock Analysis App using AutoGen 0.4 and Azure AI Agent Service\n\n# Watch the Video for Step by Step Configuration\n\n[![Video Title](https://img.youtube.com/vi/eUZOyMQSU_Q/0.jpg)](https://www.youtube.com/watch?v=eUZOyMQSU_Q)\n"
    },
    {
      "name": "MSFT-Innovation-Hub-India/Multi-Agent-autogen-Report-Gen",
      "stars": 8,
      "img": "https://avatars.githubusercontent.com/u/63641889?s=40&v=4",
      "owner": "MSFT-Innovation-Hub-India",
      "repo_name": "Multi-Agent-autogen-Report-Gen",
      "description": "This project leverages autogen multi agent framework along with Azure OpenAI Assistants API to automate data analysis and report generation. It uses multiple agents to process data, generate insights, and compile a final report document",
      "homepage": null,
      "language": "Python",
      "created_at": "2025-01-28T16:48:42Z",
      "updated_at": "2025-04-11T08:38:01Z",
      "topics": [],
      "readme": "# Analytics and Reporting using autogen\n\n## Overview\nThis project leverages autogen multi agent framework along with Azure OpenAI Assistants API to automate data analysis and report generation. It uses multiple agents to process data, generate insights, and compile a final report document. \nI have discussed the design, approach in this post [here](https://techcommunity.microsoft.com/blog/azure-ai-services-blog/autogen--openai-assistants-api-powered-collaborative-report-generation/4371174)\n\n\n## Key Components\n- **app.py**: Main application file that orchestrates the agents and handles the workflow.\n- **config.py**: Configuration file for setting up API keys and other constants.\n- **data_files/**: Directory containing the input data files.\n- **report_template.txt**: Template for the final report document.\n- **requirements.txt**: List of dependencies required for the project.\n\n## Azure Resources\n\nThis solution uses the Azure OpenAI Assistants API in tandem with autogen Assistants.\n\n1. Create the Coding Assistant\n\n- The system prompt for this Assistant to use can be taken from app.py\n- Enable the code interpreter\n- Upload the (Contoso Products.csv)Product Master and (Contoso_Product_Sales_Data_5000_Records.csv)Product Sales Transaction     files to this Assistants API. Note down the Coding Agents Assistants API ID. This has to be set in the .env file of the application. \n\nSee screen shot below:\n\n![alt text](./images/image.png)\n\n- Note down the File IDs of the 2 files you uploaded above. See screen shot below:\n\n![alt text](./images/image1.png)\n\n2. Create the Data Analyst Assistant\n\n- Paste the system prompt for this assistant from app.py.\n- Enable the code interpreter\n- Note down the Assistants API ID. See screen shot below:\n\n![alt text](./images/image2.png)\n\n## Setup\n1. Clone the repository.\n2. Create a virtual environment and activate it:\n    ```sh\n    python -m venv venv\n    source venv/bin/activate  # On Windows use `venv\\Scripts\\activate`\n    ```\n3. Install the required dependencies:\n    ```sh\n    pip install -r requirements.txt\n    ```\n4. Set up your environment variables in a `.env` file:\n    ```\n    az_openai_endpoint=\"oai-abcd12345-eastus2\"\n    az_open_ai_key=\"<>\"\n    az_open_ai_model=\"gpt-4o-2024-05-13\"\n    az_deployment_name=\"gpt-4o\"\n    az_openai_api_version=\"2024-08-01-preview\"\n    az_openai_coder_assistant_id=\"asst_lefWVQYL02ZIjy7ZXHgAVQjk\"\n    az_openai_analyst_assistant_id=\"asst_9zxvPj2SqqTWTj5bpGLTpJOn\"\n    az_data_file_products_master_file_id=\"assistant-WPw9Mzf1q2EfLts21NLh45ld\"\n    az_data_file_products_sales_file_id=\"assistant-cEZqBjjPyduNl5ttXfOOPHB2\"\n    ```\n\n## Usage\n1. Run the main application:\n    ```sh\n    python app.py\n    ```\n\nSee the debug log from a sample run of the app, stored [here](./output/debug-run-log.txt)\n\n\n2. The application will process the data files and generate a report document.\n\nA sample Word document generated from the app run is stored in the folder [here](report_0706556a-d8dc-4bf3-aa9c-dfe42c89ead8.docx)\n\n## Functions\n- **read_file()**: Reads the content of the report template.\n- **get_report_file_name()**: Generates a unique file name for the report document that should be created.\n- **download_document()**: Downloads the generated report document from Azure OpenAI Assistants API.\n\n## Agents\n- **PlanningAgent**: Orchestrates the task, breaks it down into subtasks, and delegates them to other agents.\n- **CodingAgent**: Generates data tables from the input files.\n- **DataAnalystAgent**: Creates visualizations, insights, and compiles the final report document.\n\n## Contributing\nFeel free to open issues or submit pull requests for any improvements or bug fixes.\n\n## License\nThis project is licensed under the MIT License.\n"
    },
    {
      "name": "hyssh/azure-openai-quickstart",
      "stars": 8,
      "img": "https://avatars.githubusercontent.com/u/42417311?s=40&v=4",
      "owner": "hyssh",
      "repo_name": "azure-openai-quickstart",
      "description": "Natual Language to SQL Query",
      "homepage": "",
      "language": "Jupyter Notebook",
      "created_at": "2023-03-22T19:46:31Z",
      "updated_at": "2025-04-08T16:01:00Z",
      "topics": [],
      "readme": "# Natual Language to SQL Query\n\nAzure OpenAI can help you to find business insights using natual language.\n![](./images/1.sample102.png)\n\n## Requirements\n\nAzure subscription\n\n- Azure OpenAI account\n  - Make sure you have `gpt-35-turbo` [deployed](https://learn.microsoft.com/en-us/azure/cognitive-services/openai/how-to/create-resource?pivots=cli)\n- Azure SQL Database\n  - [WWI sample database](https://learn.microsoft.com/en-us/sql/samples/wide-world-importers-what-is?view=sql-server-ver16)\n\nWindows PC/Mac\n- Python\n  - (*Optional*) Conda\n  - [Download Microsoft ODBC Driver 18 for SQL Server (x64)](https://learn.microsoft.com/en-us/sql/connect/odbc/download-odbc-driver-for-sql-server?redirectedfrom=MSDN&view=sql-server-ver16)\n- Git\n- VSCode\n\n## Setup Environment\n\n0. [Download Microsoft ODBC Driver 18 for SQL Server (x64)](https://learn.microsoft.com/en-us/sql/connect/odbc/download-odbc-driver-for-sql-server?redirectedfrom=MSDN&view=sql-server-ver16)\n\n1. Clone repo to your local PC\n\n2. Create `.env` in `src` folder\n\n```\nOPENAI_API_BASE=https://#######.openai.azure.com/\nOPENAI_API_KEY=\nOPENAI_API_VERSION=2023-03-15-preview\nSQL_SERVER=\nSQL_DATABASE=\nSQL_USERNAME=\nSQL_PASSWORD=\n```\n\n3. Open terminal and install packages `pip install -r ./requirements.txt`\n\n4. Run command `streamlit run ./src/sample101.py` or run command `streamlit run ./src/sample102.py`\n\n\n## Prompt Engineering\n\nIt's important to understand that the prompt used for interacting with GPT is crucial in getting accurate results. \n\nTo generate a SQL query that aligns with the user's needs, providing relevant background information within the text prompt is essential.\n\nUser can provide information about database table and field names.\n\nAlso adding appropreate guides to prevent getting privacy infromation.\n\nFollowing is an example of World Wide Importer database\n\n\n```text\nIt has ten tables\nSalesLT.Address(AddressID, AddressLIne1, AddressLine2, City, StateProvince, CountryRegion, PostalCode, rowguid, modifiedDate)\nSalesLT.Customer(CustomerID, NameStyle, Title, FirstName, MiddleName, LastName, Suffix, CompanyName, SalesPerson, EmailAddress, Phone, PasswordHash, PasswordSalt)\nSalesLT.CustomerAddress(CustomerID, AddressID, AddressType)\nSalesLT.Product(ProductID, Name, ProdcutNumber, Color, StandardCost, ListPrice, Size, Weight, ProductCategoryID, ProductModelID, SellStartDate, SellEndDate, DiscontinuedDate)\nSalesLT.ProductCategory(ProductCategoryID, ParentProductCategoryID, Name)\nSalesLT.ProdcutDescription(ProductDescriptionID, Description)\nSalesLT.ProductModel(ProductModelID, Name, CatalogDesciortion)\nSalesLT.ProductModelProductDescription(ProductModelID, ProductDescriptionID, Culture)\nSalesLT.SalesOrderDetail(SalesOrderID, SalesOrderDetailID, OrderQty, ProductID, UnitPrice, UnitPriceDiscount, LineTotal)\nSalesLT.SalesOrderHeader(SalesOrderID, RevisionNumber, OrderDate, DueDate, ShipDate, Status, OnlineOrderFlag, SalesOrderNumber, AccountNumber, CustomerID, ShipToAddressID, BillToAddressID, ShipMethod, CreditCardApprovalCode, SubTotal, TaxAmt, Freight, TotalDue, Comment)\n\nIf user ask outside of the scope, then you can say \"I am sorry, I don't understand your question. Can you please rephrase your question?\"\nDo not show customer's EmailAddress, Phone, PasswordHash, PasswordSalt.\nIt is important to now show customer's personal information.\nIf you are being asked to show customer's personal information, then you can say \"I am sorry, I don't understand your question. Can you please rephrase your question?\"\n\nWrite a SQL query to get the following information:\n```\n\nAnd here is a sample for system message\n\n```text\nYou are a data analyst working for a company that provides data analysis services to clients. \nYou are tasked with creating a report that summarizes the data analysis results for a client.\n```\n\nThe sample app will combine the messages with a user's request which will be look like following\n\n```json\n[\n  {\n    \"role\": \"system\",\n    \"content\": \"\\nYou are a data analyst working for a company that provides data analysis services to clients. \\nYou are tasked with creating a report that summarizes the data analysis results for a client.\\n\\nIt has ten tables\\nSalesLT.Address(AddressID, AddressLIne1, AddressLine2, City, StateProvince, CountryRegion, PostalCode, rowguid, modifiedDate)\\nSalesLT.Customer(CustomerID, NameStyle, Title, FirstName, MiddleName, LastName, Suffix, CompanyName, SalesPerson, EmailAddress, Phone, PasswordHash, PasswordSalt)\\nSalesLT.CustomerAddress(CustomerID, AddressID, AddressType)\\nSalesLT.Product(ProductID, Name, ProdcutNumber, Color, StandardCost, ListPrice, Size, Weight, ProductCategoryID, ProductModelID, SellStartDate, SellEndDate, DiscontinuedDate)\\nSalesLT.ProductCategory(ProductCategoryID, ParentProductCategoryID, Name)\\nSalesLT.ProdcutDescription(ProductDescriptionID, Description)\\nSalesLT.ProductModel(ProductModelID, Name, CatalogDesciortion)\\nSalesLT.ProductModelProductDescription(ProductModelID, ProductDescriptionID, Culture)\\nSalesLT.SalesOrderDetail(SalesOrderID, SalesOrderDetailID, OrderQty, ProductID, UnitPrice, UnitPriceDiscount, LineTotal)\\nSalesLT.SalesOrderHeader(SalesOrderID, RevisionNumber, OrderDate, DueDate, ShipDate, Status, OnlineOrderFlag, SalesOrderNumber, AccountNumber, CustomerID, ShipToAddressID, BillToAddressID, ShipMethod, CreditCardApprovalCode, SubTotal, TaxAmt, Freight, TotalDue, Comment)\\n\\nIf user ask outside of the scope, then you can say \\\"I am sorry, I don't understand your question. Can you please rephrase your question?\\\"\\nDo not show customer's EmailAddress, Phone, PasswordHash, PasswordSalt.\\nIt is important not to show customers' personal information.\\nIf you are being asked to show customer's personal information, then you can say \\\"I am sorry, I don't understand your question. Can you please rephrase your question?\\\"\\n\\nWrite a SQL query to get the following information:\\n\"\n  },\n  {\n    \"role\": \"user\",\n    \"content\": \"get 10 customer who purchased our products the most with the amount of the purchases\"\n  }\n]\n```\n"
    },
    {
      "name": "tsinggggg/autogen-watsonx-client",
      "stars": 8,
      "img": "https://avatars.githubusercontent.com/u/20978879?s=40&v=4",
      "owner": "tsinggggg",
      "repo_name": "autogen-watsonx-client",
      "description": "This is an autogen>=0.4 extension for watsonx client integration.",
      "homepage": "",
      "language": "Python",
      "created_at": "2024-11-10T21:51:26Z",
      "updated_at": "2025-04-11T21:56:03Z",
      "topics": [
        "agentic",
        "agentic-ai",
        "autogen",
        "autogen-extension",
        "ibm",
        "watsonx",
        "watsonx-ai"
      ],
      "readme": "# autogen-watsonx-client\nThis is an autogen>=0.4 extension for watsonx client integration.\n\n\n## Disclaimer\n- This is a community extension for the [Autogen](https://github.com/microsoft/autogen) project, specifically for the new [Autogen >= 0.4](https://microsoft.github.io/autogen/0.2/blog/2024/10/02/new-autogen-architecture-preview) architecture. The goal is to support IBM Watsonx.ai hosted LLMs in the Autogen framework.\n- This project is still in a very early stage under development, please create issues in this github repo for bug reports.\n- This project is a personal endeavor and is not affiliated with, endorsed by, or connected to any organization/employer in any way. The views, ideas, and opinions expressed in this project are solely my own and do not reflect those of others.\n\n\n## Announcements\n- 2025-02-13: `create_stream` is now implemented!\n- 2025-01-13: the version requirement on Autogen has been updated to `>=0.4.1`, the integration examples have been updated accordingly, see [here](doc/README.md).\n\n## Usage\n\n### Prerequisites\n- create a python environment with version `3.10` or above\n- `pip install --upgrade autogen-watsonx-client`\n- `pip install --upgrade autogen-agentchat>=0.4.1`\n- access to a watsonx.ai instance, setting up environment variables `WATSONX_API_KEY`, one of `WATSONX_SPACE_ID` or `WATSONX_PROJECT_ID`, optionally `WATSONX_URL`\n\n\n### code snippets\n\nImporting dependencies:\n\n```python\nimport os\n\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_agentchat.ui import Console\nfrom autogen_agentchat.conditions import TextMentionTermination\nfrom autogen_agentchat.teams import RoundRobinGroupChat\nfrom autogen_watsonx_client.config import WatsonxClientConfiguration\nfrom autogen_watsonx_client.client import WatsonXChatCompletionClient\n```\n\nCreate a watsonx client\n\n```python\nwx_config = WatsonxClientConfiguration(\n    model_id=\"meta-llama/llama-3-2-90b-vision-instruct\",  # pick a model you have access to on wx.ai here\n    api_key=os.environ.get(\"WATSONX_API_KEY\"),\n    url=os.environ.get(\"WATSONX_URL\"),\n    space_id=os.environ.get(\"WATSONX_SPACE_ID\"),\n    project_id=os.environ.get(\"WATSONX_PROJECT_ID\"),\n)\n\nwx_client = WatsonXChatCompletionClient(**wx_config)\n```\n\nDefine an agent using the watsonx client and register a dummy tool for querying weather\n\n```python\n# Define a tool\nasync def get_weather(city: str) -> str:\n    return f\"The weather in {city} is 73 degrees and Sunny.\"\n\n\nasync def main() -> None:\n    # Define an agent\n    weather_agent = AssistantAgent(\n        name=\"weather_agent\",\n        model_client=wx_client,\n        tools=[get_weather],\n    )\n\n    # Define termination condition\n    termination = TextMentionTermination(\"TERMINATE\")\n\n    # Define a team\n    agent_team = RoundRobinGroupChat([weather_agent], termination_condition=termination)\n\n    # Run the team and stream messages to the console\n    stream = agent_team.run_stream(task=\"What is the weather in New York?\")\n    await Console(stream)\n\n\n# NOTE: if running this inside a Python script you'll need to use asyncio.run(main()).\nawait main()\n```\n\nRefer to [here](doc/README.md) for more detailed examples."
    },
    {
      "name": "foundation-models/MediNoteAI",
      "stars": 7,
      "img": "https://avatars.githubusercontent.com/u/1612939?s=40&v=4",
      "owner": "foundation-models",
      "repo_name": "MediNoteAI",
      "description": "MediNote AI transforms medical note-taking with AI, streamlining documentation for efficient, accurate patient records, and freeing up time for direct patient care.",
      "homepage": null,
      "language": "Python",
      "created_at": "2023-12-03T08:01:20Z",
      "updated_at": "2025-04-20T10:48:27Z",
      "topics": [],
      "readme": "# MediNoteAI\nMediNote AI transforms medical note-taking with AI, streamlining documentation for efficient, accurate patient records, and freeing up time for direct patient care.\n\nIf it's the first time run ```make init``` otherwise run ```make update``` to update the latest version of codes\n"
    },
    {
      "name": "dasiths/llm-plan-and-execute-knowledge-provider-mesh",
      "stars": 7,
      "img": "https://avatars.githubusercontent.com/u/16128850?s=40&v=4",
      "owner": "dasiths",
      "repo_name": "llm-plan-and-execute-knowledge-provider-mesh",
      "description": "Extending the capabilities of LLMs using Planning agents and using \"knowledge providers\"",
      "homepage": "https://dasith.me",
      "language": "Python",
      "created_at": "2023-09-07T16:01:54Z",
      "updated_at": "2025-03-26T13:06:05Z",
      "topics": [
        "agent",
        "agents",
        "api",
        "autogen",
        "azure-openai",
        "gpt4",
        "llm",
        "llms",
        "openai",
        "openai-functions",
        "plan-and-execute",
        "rest"
      ],
      "readme": "# Agentic Workflows With Planners\n\n## Introduction\n\nThis repository demonstrates four different approaches to implementing agentic workflows that connect to external knowledge sources:\n\n1. **LangChain Plan and Execute**: A single-agent workflow using LangChain's [Plan and Execute](https://github.com/langchain-ai/langchain/blob/4852ab8d0a756700c2b2645baa53498ddde04040/cookbook/plan_and_execute_agent.ipynb) agent to connect to external knowledge providers via a REST API.\n\n2. **AutoGen Selector Group Chat**: A multi-agent workflow using AutoGen's [SelectorGroupChat](https://microsoft.github.io/autogen/stable//user-guide/agentchat-user-guide/selector-group-chat.html) to orchestrate specialized agents that interact with external systems. This example showcases integration with Model Context Protocol (MCP) servers for file system operations and Jira issue management.\n\n3. **Dapr Multi-Agent Virtual Actors**: An event-driven multi-agent workflow using Dapr's virtual actor model in [Dapr Agents](https://github.com/dapr/dapr-agents) to implement autonomous agents as microservices that collaborate through pub/sub messaging. This example demonstrates how to build resilient, distributed agent systems with state persistence and message-based coordination. It makes use of the [LLM based coordinator](https://github.com/dapr/dapr-agents/blob/main/docs/concepts/agents.md#llm-based-workflow) to plan.\n\n4. **Dapr Multi-Agent Workflow**: An alternative implementation of event-driven multi-agent workflow using [Dapr Workflows](https://docs.dapr.io/developing-applications/building-blocks/workflow/workflow-overview/) in Dapr Agents, which provides a simpler programming model while still leveraging pub/sub messaging for agent communication.\n\nAll examples show how to leverage Large Language Models (LLMs) to plan and reason over external data through API integrations, allowing agents to retrieve and process information from various sources.\n\nA \"knowledge provider\" is a wrapper around an existing API or data source that exposes that to the Agent in a curated fashion.\n\n## Repository Structure\n\n- `/langchain_plan_and_execute_example/`: Single-agent example using LangChain's Plan and Execute pattern\n- `/autogen_selector_group_chat_example/`: Multi-agent example using AutoGen's group chat with MCP server integration\n- `/dapr_multi_agent_actor_example/`: Event-driven multi-agent workflow using Dapr's virtual actor model and pub/sub messaging\n- `/dapr_multi_agent_workflow_example/`: Event-driven multi-agent workflow using Dapr workflows with pub/sub messaging\n"
    },
    {
      "name": "jennifermarsman/MultiagentResearch",
      "stars": 7,
      "img": "https://avatars.githubusercontent.com/u/10062762?s=40&v=4",
      "owner": "jennifermarsman",
      "repo_name": "MultiagentResearch",
      "description": "An exploration of using multiple agents collaborating to perform research",
      "homepage": null,
      "language": "Python",
      "created_at": "2025-02-10T16:38:16Z",
      "updated_at": "2025-04-22T13:31:45Z",
      "topics": [],
      "readme": "# Multiagent Research\nAn exploration of using multiple agents collaborating to perform research\n\n## Scenarios\nThis repo contains two different scenarios, both implemented using multiple agents collaborating using AutoGen 0.4.  \n+ **journalism_research.py** - run this script for a group of agents to conduct online research to craft a news article\n+ **shopping.py** - run this script for a group of agents to conduct online research to compare products and make a recommendation\n\n## Setup\nYou will first need to create an [Azure OpenAI resource](https://portal.azure.com/#create/Microsoft.CognitiveServicesOpenAI) with a GPT-4o model deployment, and update the .env file with the endpoint and key (and the deployment name if you change from the default).  \n\nTo use the Bing Search API, you will also need to create a [Bing resource](https://portal.azure.com/#create/Microsoft.BingSearch) and update the .env file with its key.  \n\nFinally, use the following commands in a python environment (such as an Anaconda prompt window) to set up your environment. This creates and activates an environment and installs the required packages. For subsequent runs after the initial install, you will only need to activate the environment and then run the python script.\n\n### First run\n```\nconda create --name research -y\nconda activate research\n\npip install -r requirements.txt\npython journalism_research.py\n```\n\n### Subsequent runs\n```\nconda activate research\npython journalism_research.py\n```\n"
    },
    {
      "name": "KnoWhiz/DeepTutor",
      "stars": 7,
      "img": "https://avatars.githubusercontent.com/u/112668778?s=40&v=4",
      "owner": "KnoWhiz",
      "repo_name": "DeepTutor",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2024-10-20T02:36:30Z",
      "updated_at": "2025-04-19T22:42:38Z",
      "topics": [],
      "readme": "# DeepTutor\n\n## Webapp Demo\n\nOur production webapp is online! Try it out here:\n\nhttps://deeptutor.knowhiz.us/\n\nAnd also please feel free to try out our demo webapp hosted on Streamlit Cloud and feel free to leave any comments!\n\nhttps://deeptutor.streamlit.app/\n\n## Installation\n\nCreate environment and install required packages\n\n```bash\nconda create --name deeptutor python=3.12\nconda activate deeptutor\npip install -r requirements.txt\n# Install AgentChat and OpenAI client from Extensions\npip install -U \"autogen-agentchat\" \"autogen-ext[openai]\"\n# Install AutoGen Studio for no-code GUI\npip install -U \"autogenstudio\"\n```\n\n## Setup ```.env``` file\n\nIf use OpenAI API\n\n```bash\nAZURE_OPENAI_API_KEY=\"xxx\"\nAZURE_OPENAI_ENDPOINT=\"xxx\"\nSAMBANOVA_API_KEY=\"xxx\"\nSAMBANOVA_API_ENDPOINT=\"xxx\"\nGRAPHRAG_API_KEY=\"xxx\"\nGRAPHRAG_LLM_MODEL=\"xxx\"\nGRAPHRAG_API_BASE=\"xxx\"\nGRAPHRAG_API_VERSION=\"xxx\"\nUSER_POOL_ID=\"xxx\"\nCLIENT_ID=\"xxx\"\nAZURE_STORAGE_CONNECTION_STRING=\"xxx\"\nAZURE_STORAGE_CONTAINER_NAME=\"xxx\"\nWEBHOOK_URL=\"xxx\"\nENVIRONMENT=\"local\"   # \"local\" or \"staging\" or \"production\"\nMARKER_API_KEY=\"xxx\"\nMARKER_API_ENDPOINT=\"xxx\"\n```\n\n## Run Native\n\nRun the streamlit app via\n\n```bash\npython -m streamlit run tutor.py\n```\n\n## Commone errors\n\n1. According to [PyMuPDF Documentation](https://pymupdf.readthedocs.io/en/latest/installation.html#option-2-install-from-binaries) you need to download a wheel file that is specific to your platform (e.g windows, mac, linux). The wheel files can be found on [PyMuPDF files](https://pypi.org/project/PyMuPDF/#files).\n\n2. Make sure to check the correct version of your python running on your system ```python -V```\n\n3. Once downloaded place it at the root directory of your project.\n\n4. Then run ```pip install PyMuPDF-<...>.whl``` replace ```PyMuPDF-<...>.whl``` with the name of the wheel file you have downloaded in (1) above.\n\n5. Now import fitz should be available in your module.\n"
    },
    {
      "name": "guangying94/magentic-one-autogen-sample",
      "stars": 7,
      "img": "https://avatars.githubusercontent.com/u/12764889?s=40&v=4",
      "owner": "guangying94",
      "repo_name": "magentic-one-autogen-sample",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2025-01-12T15:49:37Z",
      "updated_at": "2025-04-09T17:30:31Z",
      "topics": [],
      "readme": "# Magentic-One, Sample Implementation using Streamlit\nThis is a repo to demostrate an implementation of Magentic-One, using Streamlit and Microsoft Autogen 0.4. The deployment is done via GitOps (Flux and Kustomize) with Kubernetes, and the python apps can run locally as well without deploying to Kubernetes.\n\nAt the same time, this repo is also a demo of how to deploy apps to Azure Arc enabled Kubernetes clusters, which can be on-prem or in other clouds.\n\n## Folder Structure\n- `magentic-one-streamlit/`: Contains the Streamlit app\n- `base`: Contains the yaml to deploy the app to Kubernetes\n- `overlays`: Contains the configuration for different environments (cloud, on-prem, etc)\n\n## How to run locally\nRefers to the README in `magentic-one-streamlit/` for instructions on how to run the app locally.\n\n## How to deploy to Kubernetes\nThis implementation is done in Azure Kubernetes Services, as well as Azure Arc enabled Kubernetes clusters. The deployment here assumes you have a Kubernetes cluster ready. The deployment leverages Azure Key Vault as secret provider for the Kubernetes cluster.\n\n### Step 0: Prerequisites\nHave a Kubernetes cluster ready, either Azure Kubernetes or Azure Arc enabled Kubernetes. Refers to the following for setting up the cluster:\n- [Azure Kubernetes Services](https://learn.microsoft.com/en-us/azure/aks/learn/quick-kubernetes-deploy-portal?tabs=azure-cli)\n- [Azure Arc enabled Kubernetes](https://learn.microsoft.com/en-us/azure/azure-arc/kubernetes/quickstart-connect-cluster?tabs=azure-cli)\n\nNext, set up Azure Key Vault and store the secrets required for the deployment. Refers to the following for setting up Azure Key Vault:\n- [Azure Key Vault](https://learn.microsoft.com/en-us/azure/key-vault/general/quick-create-portal)\n\nLastly, setup a container registry to store the container images. Refers to the following for setting up Azure Container Registry:\n- [Azure Container Registry](https://learn.microsoft.com/en-us/azure/container-registry/container-registry-get-started-portal?tabs=azure-cli)\n\nFollows the instructions in the README in `magentic-one-streamlit/` to build the container image.\n\n### Step 1: Create service principal for Kubernetes\nCreate a service principal for the Kubernetes cluster. Refers to the following for creating a service principal:\n- [Create service principal](https://learn.microsoft.com/en-us/entra/identity-platform/howto-create-service-principal-portal)\n\nThis service principal will be used to perform the following actions:\n- Pull the container image from the container registry\n- Access secret from Azure Key Vault\n\n### Step 2: Enable Azure Key Vault Provider for Kubernetes\nAzure Key Vault Provider for Kubernetes allows you to use Azure Key Vault as a secret provider for Kubernetes. Refers to the following for enabling Azure Key Vault Provider for Kubernetes:\n- [Azure Key Vault Provider for Azure Kubernetes Service](https://learn.microsoft.com/en-us/azure/aks/csi-secrets-store-driver)\n- [Azure Key Vault Provider for Azure Arc enabled Kubernetes](https://learn.microsoft.com/en-us/azure/azure-arc/kubernetes/tutorial-akv-secrets-provider)\n\nOnce the Azure Key Vault Provider is enabled, don't forget to grant the service principal access to the Key Vault. Then, create necessary secrets in Azure Key Vault as per the README in `magentic-one-streamlit/`.\n\nFor demo purposes, the apps will deploy in a new namespace within the Kubernetes cluster. Run the following command to create the namespace:\n```bash\nkubectl create namespace arc-app-demo\n```\n\nTo enable the authentication via service principal, you need to create a secret in the Kubernetes cluster. Run the following command to create the secret:\n```bash\nkubectl create secret generic secrets-store-creds --from-literal clientid=\"xxxxxxxx-xxx-xxxx-xxxx-xxxxxxxxxxxx\" --from-literal clientsecret=\"xxxxxxxxxxxxxx\" -n arc-app-demo\n\nkubectl label secret secrets-store-creds secrets-store.csi.k8s.io/used=true -n arc-app-demo\n```\nLastly, deploy the Kubernetes sercet provider class. Refers to `csi-secret/akv-sample.yaml` for the sample configuration.\n```bash\nkubectl deploy -f csi-secret/akv-sample.yaml\n```\n\n### Step 3: Deploy the app to Kubernetes\nThere are two ways to deploy the app to Kubernetes:\n1. Using kubectl\n2. Using Flux and Kustomize - GitOps\n\nThe deployment consist of a single replica of the app, and a service - Load Balancer. The deployment is done in the `base` folder, and the configuration for different environments is done in the `overlays` folder.\n\n#### Using kubectl\nRun the following command to deploy the app to Kubernetes:\n```bash\nkubectl apply -k overlays/cloud # For cloud environment\nkubectl apply -k overlays/arc # For azure arc enabled kubernetes\n```\n\n#### Using GitOps (Flux and Kustomize) in Kubernetes\nRefer to the following guide to setup GitOps in Azure Kubernetes Services or Azure Arc enabled Kubernetes:\n- [GitOps in Azure Kubernetes Services](https://learn.microsoft.com/en-us/azure/azure-arc/kubernetes/tutorial-use-gitops-flux2?tabs=azure-portal)\n\n### Step 4: Access the app\nOnce the deployment is successful, you can access the app via the Load Balancer IP. You will have a public IP for Azure Kubernetes Services, and a private IP (local IP) for Azure Arc enabled Kubernetes, hence access the app accordingly.\n\n## Demo screenshot\n#### Download the full video demo here: [Magentic-One Demo Video](https://share.marcustgy.dev/u/e17E57.mov)\n\n![Magentic-One Streamlit](./assets/sample-app.png)\n"
    },
    {
      "name": "mgfrantz/CTME-llm-lecture-resources",
      "stars": 7,
      "img": "https://avatars.githubusercontent.com/u/18216423?s=40&v=4",
      "owner": "mgfrantz",
      "repo_name": "CTME-llm-lecture-resources",
      "description": "Resources for CalTech CTME lectures",
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2024-07-23T21:48:53Z",
      "updated_at": "2025-01-12T20:48:41Z",
      "topics": [],
      "readme": "# CTME-llm-lecture-resources\nResources for CalTech CTME lectures\n"
    },
    {
      "name": "waldiez/jupyter",
      "stars": 7,
      "img": "https://avatars.githubusercontent.com/u/178387055?s=40&v=4",
      "owner": "waldiez",
      "repo_name": "jupyter",
      "description": "A Waldiez JupyterLab extension.",
      "homepage": "",
      "language": "TypeScript",
      "created_at": "2024-08-22T02:58:52Z",
      "updated_at": "2025-04-20T20:13:03Z",
      "topics": [],
      "readme": "# @waldiez/jupyter\n\n![CI Build](https://github.com/waldiez/jupyter/actions/workflows/main.yaml/badge.svg) [![Coverage Status](https://coveralls.io/repos/github/waldiez/jupyter/badge.svg)](https://coveralls.io/github/waldiez/jupyter) [![PyPI version](https://badge.fury.io/py/waldiez-jupyter.svg)](https://badge.fury.io/py/waldiez-jupyter)\n\nA Waldiez JupyterLab extension.\n\n![Overview](https://raw.githubusercontent.com/waldiez/jupyter/refs/heads/main/overview.webp)\n\nThis extension is composed of a Python package named `waldiez-jupyter`\nfor the server extension and a NPM package named `@waldiez/jupyter`\nfor the frontend extension.\n\n## Quick Start\n\nUsing docker:\n\n```shell\nCONTAINER_COMMAND=docker # or podman\n$CONTAINER_COMMAND run \\\n  --rm \\\n  -it \\\n  -p 10000:8888 \\\n  -v ${PWD}/notebooks:/home/user/notebooks \\\n  ghcr.io/waldiez/jupyter:latest\n\n# with selinux and/or podman, you might get permission errors, so you can try:\n$CONTAINER_COMMAND run \\\n  --rm \\\n  -it \\\n  -p 10000:8888 \\\n  -v ${PWD}/notebooks:/home/user/notebooks \\\n  --userns=keep-id \\\n  --security-opt label=disable ghcr.io/waldiez/jupyter:latest\n```\n\nThen open your browser at `http://localhost:10000` and you should see the JupyterLab interface.\n\nOptional environment variables for the container:\n\n```shell\n# no password or token by default\nJUPYTER_PASSWORD=\nJUPYTER_TOKEN=\n```\n\n## Requirements\n\n- JupyterLab >= 4.0.0\n\n## Install\n\nTo install the extension, execute:\n\n```shell\n## if not already, install jupyter:\n# pip install jupyter\npip install waldiez-jupyter\n## you can now start jupyter lab:\n# jupyter lab\n```\n\n## Uninstall\n\nTo remove the extension, execute:\n\n```shell\npip uninstall waldiez_jupyter\n```\n\n## Troubleshoot\n\nIf you are seeing the frontend extension, but it is not working, check\nthat the server extension is enabled:\n\n```shell\njupyter server extension list\n```\n\nIf the server extension is installed and enabled, but you are not seeing\nthe frontend extension, check the frontend extension is installed:\n\n```shell\njupyter labextension list\n```\n\n## Contributors ✨\n\nThanks goes to these wonderful people ([emoji key](https://allcontributors.org/docs/en/emoji-key)):\n\n<!-- ALL-CONTRIBUTORS-LIST:START - Do not remove or modify this section -->\n<!-- prettier-ignore-start -->\n<!-- markdownlint-disable -->\n<table>\n  <tbody>\n    <tr>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://scholar.google.com/citations?user=JmW9DwkAAAAJ\"><img src=\"https://avatars.githubusercontent.com/u/29335277?v=4?s=100\" width=\"100px;\" alt=\"Panagiotis Kasnesis\"/><br /><sub><b>Panagiotis Kasnesis</b></sub></a><br /><a href=\"#projectManagement-ounospanas\" title=\"Project Management\">📆</a> <a href=\"#research-ounospanas\" title=\"Research\">🔬</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/lazToum\"><img src=\"https://avatars.githubusercontent.com/u/4764837?v=4?s=100\" width=\"100px;\" alt=\"Lazaros Toumanidis\"/><br /><sub><b>Lazaros Toumanidis</b></sub></a><br /><a href=\"https://github.com/waldiez/jupyter/commits?author=lazToum\" title=\"Code\">💻</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://humancentered.gr/\"><img src=\"https://avatars.githubusercontent.com/u/3456066?v=4?s=100\" width=\"100px;\" alt=\"Stella Ioannidou\"/><br /><sub><b>Stella Ioannidou</b></sub></a><br /><a href=\"#promotion-siioannidou\" title=\"Promotion\">📣</a> <a href=\"#design-siioannidou\" title=\"Design\">🎨</a></td>\n  </tbody>\n  <tfoot>\n    <tr>\n      <td align=\"center\" size=\"13px\" colspan=\"7\">\n        <img src=\"https://raw.githubusercontent.com/all-contributors/all-contributors-cli/1b8533af435da9854653492b1327a23a4dbd0a10/assets/logo-small.svg\">\n          <a href=\"https://all-contributors.js.org/docs/en/bot/usage\">Add your contributions</a>\n        </img>\n      </td>\n    </tr>\n  </tfoot>\n</table>\n\n<!-- markdownlint-restore -->\n<!-- prettier-ignore-end -->\n\n<!-- ALL-CONTRIBUTORS-LIST:END -->\n\nThis project follows the [all-contributors](https://github.com/all-contributors/all-contributors) specification. Contributions of any kind welcome!\n\n## License\n\nThis project is licensed under the [Apache License, Version 2.0 (Apache-2.0)](https://github.com/waldiez/jupyter/blob/main/LICENSE).\n"
    },
    {
      "name": "questcollector/autogen-kubernetes",
      "stars": 7,
      "img": "https://avatars.githubusercontent.com/u/28866236?s=40&v=4",
      "owner": "questcollector",
      "repo_name": "autogen-kubernetes",
      "description": "support kubernetes feature for autogen(https://github.com/microsoft/autogen)",
      "homepage": "",
      "language": "Python",
      "created_at": "2024-10-15T08:14:32Z",
      "updated_at": "2024-12-18T12:38:48Z",
      "topics": [
        "autogen",
        "autogen-extension"
      ],
      "readme": "# autogen-kubernetes\n\nThis is autogen(>=0.4) kubernetes extension which provides code executor on kuberentes pod\n\nWe plan to add \"autogen\" features needs Kubernetes features.\n\n## Usage\n\n### PodCommandLineCodeExecutor\n\nLike DockerCommandLineCodeExecutor, this code executor runs codes on a container in a kubernetes pod\n\nUnlike DockerCommandLineCodeExecutor, PodCommandLineCodeExecutor is not support container restart feature.\n\n```python\nfrom autogen_core import CancellationToken\nfrom autogen_core.code_executor import CodeBlock\nfrom autogen_kubernetes.code_executors import PodCommandLineCodeExecutor\n\nasync with PodCommandLineCodeExecutor() as executor:\n    code_result = await executor.execute_code_blocks(\n        code_blocks=[\n            CodeBlock(language=\"python\", code=\"print('Hello, World!')\"),\n        ],\n        cancellation_token=CancellationToken(),\n    )\n    print(code_result)\n```\n```\nCommandLineCodeResult(exit_code=0, output='Hello, World!\\n', code_file='/workspace/tmp_code_07da107bb575cc4e02b0e1d6d99cc204.py')\n```\n\nin default options, pod will be created like\n\n```python\n{\n  \"kind\": \"Pod\",\n  \"apiVersion\": \"v1\",\n  \"metadata\": {\n    # name is like autogen-code-exec-{uuid4}\n    \"name\": \"autogen-code-exec-a2826c87-9b8d-46ec-be36-5fffc5d8f899\",\n    # created on default namespace\n    \"namespace\": \"default\",\n    ...\n  },\n  \"spec\": {\n    \"containers\": [\n      {\n        # container named autogen-executor\n        \"name\": \"autogen-executor\",\n        # default image python:3-slim\n        \"image\": \"python:3-slim\",\n        # the container is kept alive by running keep-alive loop: /bin/sh -c while true;do sleep 5; done\n        \"command\": [\n          \"/bin/sh\"\n        ],\n        \"args\": [\n          \"-c\",\n          \"while true;do sleep 5; done\"\n        ],\n        ...\n    ],\n    ...\n  }\n}\n```\n\nThere are arguments that change pod specification of PodCommandLineCodeExecutor.\n\nThere are several arguments to change pod settings\n\n|argument|datatype|describe|\n|--|--|--|\n|image|str|Container image|\n|pod_name|str|Pod name|\n|namespace|str|Pod namespace|\n|workspace_path|str, Path|Path in container where the LLM generated code script will be saved|\n|volume|dict, str, Path, V1Volume, None|Volume to be mounted on pods|\n|pod_spec|dict, str, Path, V1Pod, None|Pod specification for command line code executor|\n\nOther parameters image, pod_name, namespace and volume are ignored when pod_spec parameter is provided.\n\n#### volume parameter usage\n\n\"volume\" parameter is to mount pre-existing volume on code executor container\n\nThe volume will be mounted in the container's workspace path provided as the executor's parameter \"workspace_path\".\n\n\"volume\" parameter can be provided like:\n\n- kubernetes.client.models.V1Volume\n\n```python\nfrom autogen_kubernetes.code_executors import PodCommandLineCodeExecutor\nfrom kubernetes.client.models import V1Volume\n\nvolume = V1Volume(\n    name=\"test-volume\",\n    empty_dir=V1EmptyDirVolumeSource(medium=\"Memory\", size_limit=\"5Mi\"),\n)\n\nexecutor = PodCommandLineCodeExecutor(volume=volume)\n...\n\n```\n\n- dictionary, must conform kubernetes volume specification.\n  reference: [\n    https://github.com/kubernetes-client/python/blob/master/kubernetes/docs/V1Volume.md, \n    https://kubernetes.io/docs/concepts/storage/volumes/\n  ]\n\n```python\nfrom autogen_kubernetes.code_executors import PodCommandLineCodeExecutor\n\nvolume = {\"name\": \"test-volume\", \"emptyDir\": {\"medium\": \"Memory\", \"sizeLimit\": \"5Mi\"}}\n\nexecutor = PodCommandLineCodeExecutor(volume=volume)\n...\n```\n\n- string which is expresses yaml or json format\n\n```python\nfrom autogen_kubernetes.code_executors import PodCommandLineCodeExecutor\n\nvolume = \"\"\"\nname: \"test-volume\"\nemptyDir:\n  medium: \"Memory\"\n  sizeLimit: \"5Mi\"\n\"\"\"\n\nexecutor = PodCommandLineCodeExecutor(volume=volume)\n...\n```\n\n- file path of yaml or json format\n```python\nfrom autogen_kubernetes.code_executors import PodCommandLineCodeExecutor\n\nvolume = \"./test/test-volume.yaml\"\n\nexecutor = PodCommandLineCodeExecutor(volume=volume)\n...\n```\n\n#### pod_spec parameter usage\n\nTo create more complex pod specification of PodCommandLineCodeExecutor, use pod_spec parameter.\n\nLike volume parameter, pod_spec parameter can be provided like:\n\n- kubernetes.client.models.V1Volume\n\n```python\nfrom autogen_kubernetes.code_executors import PodCommandLineCodeExecutor\n\npod_spec = V1Pod(\n    metadata=V1ObjectMeta(name=\"test-pod\", namespace=\"default\"),\n    spec=V1PodSpec(\n        restart_policy=\"Never\",\n        containers=[\n            V1Container(\n                args=[\n                    \"-c\",\n                    \"echo 'test container'; while true;do sleep 5; done\",\n                ],\n                command=[\"/bin/sh\"],\n                name=\"autogen-executor\", # container named \"autogen-executor\" must be included\n                image=\"python:3-slim\",\n            )\n        ],\n    ),\n)\n\nexecutor = PodCommandLineCodeExecutor(pod_spec=pod_spec)\n...\n```\n\n- dictionary, must conform kubernetes pod specification.\n  reference: [\n    https://github.com/kubernetes-client/python/blob/master/kubernetes/docs/V1Pod.md, \n    https://kubernetes.io/docs/concepts/workloads/pods/\n  ]\n\n```python\nfrom autogen_kubernetes.code_executors import PodCommandLineCodeExecutor\n\npod = {\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\"test\": \"true\"},\n        \"name\": \"test-pod\",\n        \"namespace\": \"default\",\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"python:3-slim\",\n                \"name\": \"autogen-executor\", # container named \"autogen-executor\" must be included\n                \"args\": [\n                    \"/bin/sh\",\n                    \"-c\",\n                    \"echo 'test container'; while true;do sleep 5; done\",\n                ],\n            },\n        ],\n    },\n}\n\nexecutor = PodCommandLineCodeExecutor(pod_spec=pod_spec)\n...\n```\n\n- string which is expresses yaml or json format\n\n```python\nfrom autogen_kubernetes.code_executors import PodCommandLineCodeExecutor\n\npod = (\n    \"apiVersion: v1\\n\"\n    \"kind: Pod\\n\"\n    \"metadata:\\n\"\n    \"  annotations:\\n\"\n    \"    test: 'true'\\n\"\n    \"  name: test-pod\\n\"\n    \"  namespace: default\\n\"\n    \"spec:\\n\"\n    \"  containers:\\n\"\n    \"  - args:\\n\"\n    \"    - sh\\n\"\n    \"    - -c\\n\"\n    \"    - while true;do sleep 5; done\\n\"\n    \"    image: python:3-slim\\n\"\n    \"    name: autogen-executor\\n\"\n)\n\nexecutor = PodCommandLineCodeExecutor(pod_spec=pod_spec)\n...\n```\n\n- file path of yaml or json format\n\n```python\nfrom autogen_kubernetes.code_executors import PodCommandLineCodeExecutor\n\npod = \"./test/test-pod.yaml\"\n\nexecutor = PodCommandLineCodeExecutor(pod_spec=pod_spec)\n...\n```\n\n#### Kubeconfig usage\n\nBy default, PodCommandLineCodeExecutor uses default kubeconfig file to communicate with kubernetes API server.\n\nDefault kubeconfig file path is provded by Environment variable \"KUBECONFIG\"\n\nTo use other kubeconfig file, provide it's path on \"KUBECONFIG\" environment variable.\n\nhttps://github.com/kubernetes-client/python/blob/master/kubernetes/base/config/kube_config.py#L48\n```python\n...\nKUBE_CONFIG_DEFAULT_LOCATION = os.environ.get('KUBECONFIG', '~/.kube/config')\n...\n```\n\nIf the PodCommandLineExecutor is initialized on kubernetes object, incluster config(serviceAccount tokens) is used.\n\nTo use incluster config, make sure to have sufficient permissions.\n\nbelow is minimum permission:\n\n|resource|verb|\n|--|--|\n|pods|get, create, delete|\n|pods/status|get|\n|pods/exec|create|\n|pods/log|get|\n\n\nFor example, create serviceAccount and bind role with sufficient permissions.\n\ncreatae ServiceAccount\n```sh\nkubectl create serviceaccount autogen-executor-sa\n```\n\ncreate clusterRole/Role(namespaced role) with sufficient permissions\n```sh\nkubectl create clusterrole autogen-executor-role \\\n  --resource=pods --verb=get,create,delete \\\n  --resource=pods/exec --verb=create \\\n  --resource=pods/status,pods/log --verb=get\n```\n\nbind clusterRole/role with ServiceAccount\n```sh\nkubectl create rolebinding autogen-executor-rolebinding \\\n  --clusterrole autogen-executor-role --serviceaccount default:autogen-executor-sa\n```\n\nThen, PodCommandLineCodeExecutor will work alright where the pod uses the serviceAccount created before.\n\ncreate pod uses serviceAccount\n```sh\nkubectl run autogen-executor --image python:3 \\\n  --overrides='{\"spec\": {\"serviceAccount\": \"autogen-executor-sa\"}}' \\\n  -- sh -c 'pip install autogen-kubernetes && sleep infinity'\n```\n\nexecute the pod\n```sh\nkubectl exec autogen-executor -it -- python\n```\n\nexecute PodCommandLineCodeExecutor\n```python\nfrom autogen_core import CancellationToken\nfrom autogen_core.code_executor import CodeBlock\nfrom autogen_kubernetes.code_executors import PodCommandLineCodeExecutor\n\nasync with PodCommandLineCodeExecutor() as executor:\n    code_result = await executor.execute_code_blocks(\n        code_blocks=[\n            CodeBlock(language=\"python\", code=\"print('Hello, World!')\"),\n        ],\n        cancellation_token=CancellationToken(),\n    )\n    print(code_result)\n```\n```\nCommandLineCodeResult(exit_code=0, output='Hello, World!\\n', code_file='/workspace/tmp_code_07da107bb575cc4e02b0e1d6d99cc204.py')\n```\n\n#### Function module usage\n\nTo make code executor pod to have pre-installed packages, provide \"functions\" parameter.\n\n```python\nimport pandas as pd\nfrom autogen_core import CancellationToken\nfrom autogen_core.code_executor import CodeBlock\nfrom autogen_kubernetes.code_executors import PodCommandLineCodeExecutor, Alias, with_requirements\n\n@with_requirements(python_packages=[\"pandas\"], global_imports=[Alias(name=\"pandas\", alias=\"pd\")])\ndef load_data() -> pd.DataFrame:\n    \"\"\"\n    Load pandas sample dataframe\n\n    Returns:\n        pd.DataFrame: sample Dataframe with columns name(str), age(int)\n    \"\"\"\n    data = {\n        \"name\": [\"Sam\", \"Brown\"],\n        \"age\": [37, 57],\n    }\n    return pd.DataFrame(data)\n\nasync with PodCommandLineCodeExecutor(functions=[load_data]) as executor:\n    code = f\"from {executor._functions_module} import load_data\\nprint(load_data())\"\n    code_result = await executor.execute_code_blocks(\n        code_blocks=[\n            CodeBlock(language=\"python\", code=f\"from \"),\n        ],\n        cancellation_token=CancellationToken(),\n    )\n    print(code_result)\n\n```\n\n```\nCommandLineCodeResult(exit_code=0, output='    name  age\\n0    Sam   37\\n1  Brown   57\\n', code_file='/workspace/tmp_code_bd92ac3930fbd4f6f627885646227d5ff54753166555b98206cc01fdc023a7ef.py')\n```\n\nAnother example with FunctionWithRequirements and ImportFromModule.\n\n```python\nimport inspect\nfrom autogen_core import CancellationToken\nfrom autogen_core.code_executor import CodeBlock\nfrom autogen_kubernetes.code_executors import PodCommandLineCodeExecutor, ImportFromModule, FunctionWithRequirements\n\n## this section will be written on function module file(workspace_path/functions.py)\nfrom kubernetes.client import CoreV1Api\nfrom kubernetes.config import load_config\n\ndef kubernetes_enabled() -> bool:\n    try:\n        load_config()  # type: ignore\n        api_client = CoreV1Api()\n        api_client.list_namespace()\n        return True\n    except Exception:\n        return False\n\n##\n\ntest_function = FunctionWithRequirements.from_str(\n    inspect.getsource(kubernetes_enabled),\n    [\"kubernetes\"],\n    [\n        ImportFromModule(module=\"kubernetes.client\", imports=(\"CoreV1Api\",)),\n        ImportFromModule(module=\"kubernetes.config\", imports=(\"load_config\",)),\n    ],\n)\nasync with PodCommandLineCodeExecutor(functions=[test_function]) as executor:\n    code = f\"from {executor._functions_module} import kubernetes_enabled\\nprint(kubernetes_enabled())\"\n    code_result = await executor.execute_code_blocks(\n        code_blocks=[\n            CodeBlock(language=\"python\", code=code),\n        ],\n        cancellation_token=CancellationToken(),\n    )\n    print(code_result)\n```\n```\nCommandLineCodeResult(exit_code=0, output='kube_config_path not provided and default location (~/.kube/config) does not exist. Using inCluster Config. This might not work.\\nTrue\\n', code_file='/workspace/tmp_code_c61a3c1e421357bd54041ad195e242d8205b86a3a4a0778b8e2684bc373aac22.py')\n```\n\n\n## Contribute\n\nThis project's structure conforms microsoft/autogen python package\n\n```\n├── LICENSE\n└── python\n    ├── run_task_in_pkgs_if_exist.py\n    ├── pyproject.toml\n    ├── shared_tasks.toml\n    ├── uv.lock\n    └── packages\n        └── autogen-kubernetes\n            ├── tests\n            │   ├── test_utils.py\n            │   ├── conftest.py\n            │   ├── test_kubernetees_code_executor.py\n            │   ├── test-pod.yaml\n            │   └── test-volume.yaml\n            ├── LICENSE-CODE\n            ├── pyproject.toml\n            ├── README.md\n            └── src\n                └── autogen_kubernetes\n                    ├── py.typed\n                    └── code_executors\n                        ├── _utils.py\n                        └── _kubernetes_code_executor.py\n```\n\n### Install uv\n\nInstall uv according to your environment.\n\nhttps://docs.astral.sh/uv/getting-started/installation/\n\n### Sync project\n\n```sh\ncd autogen-kubernetes/python\nuv venv --python python >=3.10\nsource .venv/bin/activate\nuv sync --locked --all-extras\n```\n\n### Common tasks\n\n- Format: `poe format`\n- Lint: `poe lint`\n- Test: `poe test`\n- Mypy: `poe mypy`\n- Check all: `poe check`\n\n## Licensing\n\nThis project is licensed under the MIT License.\n\n### Code Modification\n\nThis project includes code from the microsoft/autogen project (licensed under the MIT License), \n\nwith modifications made by kiyoung you(questcollector), See the [LICENSE-CODE](python/packages/autogen-kubernetes/LICENSE-CODE) file for details\n\n\n### Third-Party Dependencies\n\nThis project uses the following third-party dependencies:\n\n1. **kubernetes**\n    License: Apache License, Version 2.0\n    Source: https://github.com/kubernetes-client/python\n2. **httpx**\n    License: BSD 3-Clause \"New\" or \"Revised\"\n    Source: https://github.com/encode/httpx\n3. **websockets**\n    License: BSD 3-Clause \"New\" or \"Revised\"\n    Source: https://github.com/python-websockets/websockets\n4. **PyYAML**\n    License: MIT License\n    Source: https://github.com/yaml/pyyaml\n\nFor details, see the LICENCE-THIRD-PARTY file."
    },
    {
      "name": "LittleLittleCloud/WebSurferAgent",
      "stars": 6,
      "img": "https://avatars.githubusercontent.com/u/16876986?s=40&v=4",
      "owner": "LittleLittleCloud",
      "repo_name": "WebSurferAgent",
      "description": "A web surfer agent demo using playwrite.net and autogen.net",
      "homepage": "",
      "language": "C#",
      "created_at": "2024-08-28T16:44:05Z",
      "updated_at": "2025-02-28T21:10:39Z",
      "topics": [
        "llm",
        "playwrite",
        "web"
      ],
      "readme": "# Web Surfer Agent\n\nAutomate web surfing using GPT-4o and playwright.Net\n\n> [!Note]\n> This project is modified from the [WebSurferAgent](https://github.com/microsoft/autogen/blob/main/autogen/agentchat/contrib/web_surfer.py).\n> With a few modifications to better follow the dotnet style.\n> The original project is written in python and uses playwright-python.\n> The tag_utils.js is copied from https://github.com/reworkd/tarsier, with little modification to return the list of interactive elements.\n\n![chat-ui](./assets/chat-ui.png)\n\n## Getting Started\n\n### Pre-requisites\n- `dotnet 8.*`\n\n### Installation\n- Clone the repository\n- restore dotnet tool\n```bash\ndotnet tool restore\n```\n- build the project\n```bash\ndotnet build\n```\n- install required browser drivers using playwright.ps1\n```bash\ndotnet pwsh bin/Debug/net8.0/playwright.ps1 install\n```\n- run the project\n```bash\ndotnet run\n```\n\n## Workflow\n![Web Surfer Agent](./assets/workflow.png)\n"
    },
    {
      "name": "kimtth/azure-openai-llm-cookbook",
      "stars": 6,
      "img": "https://avatars.githubusercontent.com/u/13846660?s=40&v=4",
      "owner": "kimtth",
      "repo_name": "azure-openai-llm-cookbook",
      "description": "🫧 A one-stop hub, like a sample library 🪂 Azure OpenAI 100+ Sample Code 🧪 Organized by topic for quick reference. (Updated regularly)",
      "homepage": "",
      "language": "Jupyter Notebook",
      "created_at": "2025-03-05T08:29:56Z",
      "updated_at": "2025-04-23T09:21:27Z",
      "topics": [
        "agent",
        "azure",
        "azure-openai",
        "chatgpt",
        "cookbook",
        "hub",
        "langgraph",
        "library",
        "llama-index",
        "llm",
        "openai",
        "rag"
      ],
      "readme": "# Azure OpenAI LLM Cookbook\n\n![Static Badge](https://img.shields.io/badge/llm-azure_openai-blue?style=flat-square) ![GitHub Created At](https://img.shields.io/github/created-at/kimtth/azure-openai-samples?style=flat-square)\n\n## 📌 Quick Reference: Curated Sample Collection\n\n`A one-stop hub, like a sample library.` This repository is organized by topic to help reduce the time spent searching for and reviewing sample code. It offers a curated collection of minimal implementations and sample code from various sources.\n\n> [!IMPORTANT]\n> 🔹For more details and the latest code updates, please refer to the original link provided in the `README.app.md` file within each directory.  \n> 🔹Disclaimer: Some examples are created for OpenAI-based APIs. \n\n💡[How to switch between OpenAI and Azure OpenAI endpoints with Python](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/switching-endpoint)\n\n- Programming Languages\n    - Python:🐍 \n    - Jupyter Notebook:📔\n    - JavaScript/TypeScript:🟦\n    - Extra:🔴 \n- Status & Action\n    - Created:✨ (A unique example found only in this repository)\n    - Modified:🎡 (An example that has been modified from a referenced source)\n    - Copied:🧲 (When created or modified emojis are not following) \n    - See the details at the URL:🔗\n- Microsoft libraries or products:🪟\n\n⭐ If you find this repository useful, please consider giving it a star!\n\n## 📖 Repository structure\n\n## 📁 agent\n- a2a_semantic_kernel🐍✨🔗🪟: Agent2Agent (A2A) Protocol Implementation with Semantic Kernel\n- a2a_server_client🐍: Agent2Agent (A2A) Protocol - official implementation of Server/Client\n- agent_multi-agent_pattern📔🪟: Agent multi-agent pattern\n- agent_planning_pattern📔🪟: Agent planning pattern\n- agent_react_pattern📔: Agent react pattern\n- agent_reflection_pattern📔: Agent reflection pattern with LangGraph\n- agent_reflection_pattern📔: Agent reflection pattern\n- agent_tool_use_pattern📔🪟: Agent tool use pattern\n- arxiv_agent🐍✨🎡: ArXiv agent\n- chess_agent🐍: Chess agent\n- multi_agentic_system_simulator🐍✨🔗: A Multi-Agentic System Simulator. Visualize Agent interactions.\n- role_playing📔: Role-playing\n- web_scrap_agent🐍✨🎡: Web scraping agent\n- x-ref: [📁industry](#-industry) \n\n## 📁 azure\n- azure_ai_foundry_sft_finetuning📔🪟: Supervised Fine-tuning\n- azure_ai_foundry_workshop📔🪟: Azure AI Foundry Workshop\n- azure_ai_search📔🪟: Chunking, Document Processing, Evaluation\n- azure_bot📔🪟: Bot Service API\n- azure_cosmos_db📔🪟: Cosmos DB as a Vector Database\n- azure_cosmos_db_enn🐍✨🪟: Cosmos DB Exact Nearest Neighbor (ENN) Vector Search for Precise Retrieval\n- azure_devops_(project_status_report)🐍✨🪟: Azure DevOps – Project Status Report\n- azure_document_intelligence🐍🪟: Azure Document Intelligence\n- azure_evaluation_sdk🐍🪟: Azure Evaluation SDK\n- azure_machine_learning📔🪟: Azure Machine Learning\n- azure_postgres_db📔🪟: pgvector for Vector Database\n- azure_sql_db📔🪟: Azure SQL as a Vector Database\n- copilot_studio🔗🪟: A low-code platform for bots and agents (formerly Power Virtual Agents)\n- m365_agents_sdk🟦🪟: Rebranding of Azure Bot Framework\n- sentinel_openai🔗🪟: Sentinel – Security Information and Event Management (SIEM)\n- sharepoint_azure_function📔🪟: SharePoint Integration with Azure Functions\n- teams_ai_sdk🔗🪟: Teams AI SDK\n\n## 📁 cookbook\n\n- anthropic: [Anthropic Cookbook](https://github.com/anthropics/anthropic-cookbook)\n- gemini: [Gemini API Cookbook](https://github.com/google-gemini/cookbook)\n- openai: [OpenAI Cookbook](https://github.com/openai/openai-cookbook)\n\n## 📁 data\n- azure_oai_usage_stats_(power_bi)🔴🪟: Azure OpenAI usage stats using Power BI\n- azure_ocr_scan_doc_to_table🐍✨🪟: Azure Document Intelligence – Extract tables from document images and convert them to Excel\n- chain-of-thought🐍🔴: Chain-of-thought reasoning prompt\n- fabric_cosmosdb_chat_analytics📔🔴✨(visual)🪟: [Fabric](https://learn.microsoft.com/en-us/fabric/): Data processing, ingestion, transformation, and reporting on a single platform\n- firecrawl_(crawling)🐍: Firecrawl – Web crawling and scraping\n- ms_graph_api📔🪟: Microsoft Graph API\n- presidio_(redaction)📔🪟: Presidio – Data redaction and anonymization\n- prompt_buddy_(power_app)🔴🪟: Prompt sharing application built on Power App\n- prompt_leaked🔴: Prompt leakage detection and analysis\n- sammo_(prompt_opt)📔🪟: A Structure-Aware Approach to Efficient Compile-Time Prompt Optimization\n- semantic_chunking_(rag)📔: Semantic chunking for Retrieval-Augmented Generation (RAG)\n\n## 📁 dev\n- code_editor_(vscode)🐍✨🔗🪟: Visual Studio Code extension development\n- diagram_to_infra_template_(bicep)🐍✨🪟: Bicep – Infrastructure as Code (IaC) language\n- e2e_testing_agent📔🪟: End-to-end testing with Playwright automation framework\n- git_repo_with_chat🐍✨: Chat with Github repository\n- gui_automation🔗🪟: Omni Parser – Screen parsing tool / Windows Agent Arena (WAA)\n- llm_router🐍✨🎡: LLM request routing and orchestration\n- mcp_(model_context_protocol)🐍✨🔗: Model Context Protocol\n- mcp_(sse)🐍✨🔗: Remote MCP (Model Context Protocol) calls  \n- mcp_to_openai_func_call🐍✨: MCP Tool Spec to OpenAI Function Call Converter\n- memory_for_llm🐍🔗: Memory management techniques for LLMs – [K-LaMP](https://arxiv.org/pdf/2311.06318.pdf)🪟\n- memory_graphiti🐍✨: Graph and neo4j based Memory \n- mini-copilot🐍✨🔗: DSL approach to calling the M365 API\n- mixture_of_agents🐍✨🎡: Multi-agent system for collecting responses from multiple LLMs\n- open_telemetry🐍✨: OpenTelemetry – Tracing LLM requests and logging\n\n## 📁 eval\n- evaluation_llm_as_judge📔: Using LLMs for automated evaluation and scoring\n- guardrails📔: Guardrails for AI safety and compliance\n- pyrit_(safety_eval)📔🪟: Python Risk Identification Tool\n\n## 📁 framework\n- agno_(framework)🐍: Agno – A simple, intuitive agent framework\n- autogen_(framework)🐍🪟: AutoGen – A Framework for LLM Agent\n- crewai_(framework)🐍: CrewAI – Agent collaboration framework\n- dspy_(framework)🐍📔: DSPy – Declarative Language Model Calls into Self-Improving Pipelines\n- guidance_(framework)📔🪟: Guidance – Prompt programming framework\n- haystack_(framework)🐍📔: Haystack – NLP framework for RAG and search\n- langchain_(framework)📔: LangChain – Framework for LLM applications\n- llamaindex_(framework)📔: LlamaIndex – Data framework for LLM retrieval/agent\n- magentic-one_(agent)🐍🪟: Magentic-One – Multi-agent system for solving open-ended web and file-based tasks\n- mem0_(framework)🐍📔: Mem0 – LLM Memory\n- omniparser_(gui)📔🪟: OmniParser – GUI automation and parsing tool\n- prompt_flow_(framework)📔🪟: Prompt Flow – LLM Workflow\n- prompty_(framework)🔗🔴🪟: Prompty – Prompt management\n- pydantic_ai_(framework)🐍: Pydantic AI – Pydantic agent framework\n- semantic_kernel_(framework)🐍🪟: Semantic Kernel – Microsoft LLM orchestration framework\n- smolagent_(framework)🐍: SmolAgent – Hugging Face Lightweight AI agent framework\n- tiny_troupe_(framework)📔🪟: Tiny Troupe – Multi agent persona simulation\n- x-ref: [📁microsoft-frameworks-and-libraries](#-microsoft-frameworks-and-libraries): \n\n## 📁 industry\n- auto_insurance_claims📔: Automation for auto insurance claims processing\n- career_assistant_agent📔: Career guidance and job recommendation agent\n- contract_review📔: Legal contract analysis and review\n- customer_support_agent📔: Customer support automation\n- damage_insurance_claims📔: Automated claims processing for damage insurance\n- invoice_sku_product_catalog_matching📔: Invoice and SKU reconciliation for accounting\n- invoice_payments📔: Automation for invoice payments\n- invoice_standardization📔: Standardizing invoice units for consistency\n- music_compositor_agent📔: Music composition assistant\n- news_summarization_agent📔: Automated summarization of news articles\n- nyc_taxi_pickup_(ui)🐍: NYC taxi pickup analysis and UI visualization\n- patient_case_summary📔: Summaries for patient medical cases\n- project_management📔: a tools for project tracking and task management\n- stock_analysis🐍✨🔗: AutoGen demo for analyzing stock investments\n- travel_planning_agent📔: Travel itinerary planner\n- youtube_summarize🐍✨: Summarizing YouTube videos using AI\n\n## 📁 llm\n- finetuning_grpo📔: Group Relative Policy Optimization (GRPO) for LLM fine-tuning\n- knowledge_distillation📔: Compressing LLM knowledge into smaller models\n- llama_finetuning_with_lora📔: LoRA – Low-Rank Adaptation of Large Language Models\n- nanoGPT🐍: Lightweight GPT implementation\n- nanoMoE🐍: Lightweight Mixture of Experts (MoE) implementation\n\n## 📁 llmops\n- azure_prompt_flow🔗🪟: Azure AI Foundry - Prompt flow: E2E development tools for creating LLM flows and evaluation\n- mlflow📔: OSS platform managing ML workflows\n\n## 📁 multimodal\n- image_gen📔: Image creation\n- image_gen_dalle📔: Image creation with segmentaion\n- openai-agents-sdk-voice-pipeline📔✨: OpenAI Agents SDK for voice processing\n- openai-chat-vision📔: Multimodal chat with vision capabilities\n- phi-series-cookbook_(slm)🔗🪟: Phi series models cookbook (small language models)\n- video_understanding📔: Video content analysis and understanding\n- vision_rag📔: Combining visual data with retrieval-augmented generation (RAG)\n- visualize_embedding📔: Tools for embedding visualization and analysis\n- voice_audio🟦: RTClient sample for using the Realtime API in voice applications\n\n## 📁 nlp\n- multilingual_translation_(co-op-translator)🐍🪟: a library for multilingual translation\n- search_the_internet_and_summarize📔: Internet search and summarization\n- sentiment_analysis_for_customer_feedback📔: Sentiment analysis for customer feedback\n- translate_manga_into_english🐍✨: Manga translation into English\n- txt2sql🐍: Converting natural language queries into SQL\n\n## 📁 rag\n- adaptive-rag📔: Adaptive retrieval-augmented generation (RAG)\n- agentic_rag📔: Agent-based RAG system\n- contextual_retrieval_(rag)📔: Context-aware retrieval for RAG\n- corrective_rag📔: Improving retrieval results with corrective techniques\n- fusion_retrieval_reranking_(rag)📔: Fusion-based retrieval and reranking for RAG\n- graphrag📔🪟: Graph-based retrieval-augmented generation\n- hyde_(rag)📔: Hypothetical Document Embeddings for better retrieval\n- query_rewriting_(rag)📔: Enhancing RAG by rewriting queries for better retrieval\n- raptor_(rag)📔: Recursive Abstractive Processing for Tree-Organized Retrieval\n- self_rag📔: Self-improving retrieval-augmented generation\n\n## 📁 research\n- analysis_of_twitter_the-algorithm_source_code📔: Analyzing [Twitter’s open-source ranking algorithm ](https://github.com/twitter/the-algorithm)\n- deep_research_langchain🐍📔: AI-driven deep research and analysis tools using LangChain\n- deep_research_smolagents🐍📔: AI-driven deep research and analysis tools using smolagents\n- openai_code_interpreter🐍📔: OpenAI’s code interpreter for data analysis\n- r&d-agent🐍🪟: Research and development AI agent\n\n## 🛠️ Comparing Local with Remote Repository\n\nYou can use the `git_cmp.py` script (and related files) to compare your local project directories with their corresponding remote GitHub repositories. \n\n### Typical Workflow\n\n1. **Index all projects and their GitHub URLs:**\n    ```bash\n    python git_cmp.py --index --root <root_dir> --csv git_cmp_index.csv\n    ```\n    This creates a CSV file listing all projects and their remote URLs.\n\n2. **Compare local and remote repositories:**\n    ```bash\n    python git_cmp.py --compare --root <root_dir> --csv git_cmp_index.csv --report git_cmp_report.txt --update_csv git_cmp_needs_update.csv\n    ```\n    This generates a report and a CSV of projects needing updates. It also copies changed files into `.cache/` for review.\n\n3. **Update local files from cache (optional, use with care):**\n    ```bash\n    python git_cmp.py --manipulate --root <root_dir> --update_csv git_cmp_needs_update.csv\n    ```\n    This copies files from `.cache/` back into your project directories, optionally deleting files if flagged.\n\n### Options\n\n- `--delay_sec <seconds>`: Add a delay between GitHub API calls to avoid rate limits.\n- `--index`: Index projects and write a CSV.\n- `--compare`: Compare projects and write a report.\n- `--manipulate`: Update local files from the cache based on the update CSV.\n\n### Example\n\n```bash\npython git_cmp.py --index --root . --csv git_cmp_index.csv\npython git_cmp.py --compare --root . --csv git_cmp_index.csv --report git_cmp_report.txt --update_csv git_cmp_needs_update.csv\npython git_cmp.py --manipulate --root . --update_csv git_cmp_needs_update.csv\n```\n\n> **Note:**  \n> - Create `.env` file. Set the `GITHUB_TOKEN`. `e.g.,GITHUB_TOKEN=<your_key>`\n> - Review `.cache/` and the generated report before running `--manipulate`.\n> - See comments and docstrings in `git_cmp.py` for more details.\n\n## 📚 References & Sources\n\n- [OpenAI Cookbook](https://github.com/openai/openai-cookbook)\n- [LangChain Cookbook](https://github.com/langchain-ai/langchain/tree/master/cookbook)\n- [LlamaCloud Demo](https://github.com/run-llama/llamacloud-demo)\n- [Chainlit Cookbook](https://github.com/Chainlit/cookbook)\n- [Microsoft AI Agents for Beginners](https://github.com/microsoft/ai-agents-for-beginners)\n- [GenAI Agents by NirDiamant](https://github.com/NirDiamant/GenAI_Agents)\n- [RAG Techniques by NirDiamant](https://github.com/NirDiamant/RAG_Techniques)\n- [Gemini API Cookbook](https://github.com/google-gemini/cookbook)\n- [Anthropic Cookbook](https://github.com/anthropics/anthropic-cookbook)\n- [Awesome LLM Apps](https://github.com/Shubhamsaboo/awesome-llm-apps)\n- [AI Engineering Hub](https://github.com/patchy631/ai-engineering-hub)\n\n## 💻 Microsoft Frameworks and Libraries\n\n1. [Semantic Kernel](https://devblogs.microsoft.com/semantic-kernel/) (Feb 2023): An open-source SDK for integrating AI services like OpenAI, Azure OpenAI, and Hugging Face with conventional programming languages such as C# and Python. It's an LLM orchestrator, similar to LangChain. / [git](https://github.com/microsoft/semantic-kernel) ![GitHub Repo stars](https://img.shields.io/github/stars/microsoft/semantic-kernel?style=flat-square&label=%20&color=gray&cacheSeconds=36000)\n1. [Azure ML Prompt Flow](https://learn.microsoft.com/en-us/azure/machine-learning/prompt-flow/overview-what-is-prompt-flow) (Jun 2023): A visual designer for prompt crafting using Jinja as a prompt template language. / [ref](https://techcommunity.microsoft.com/t5/ai-machine-learning-blog/harness-the-power-of-large-language-models-with-azure-machine/ba-p/3828459) / [git](https://github.com/microsoft/promptflow)\n ![GitHub Repo stars](https://img.shields.io/github/stars/microsoft/promptflow?style=flat-square&label=%20&color=gray&cacheSeconds=36000)\n1. [SAMMO](https://github.com/microsoft/sammo) (Apr 2024): A general-purpose framework for prompt optimization. / [ref](https://www.microsoft.com/en-us/research/blog/sammo-a-general-purpose-framework-for-prompt-optimization/)\n ![GitHub Repo stars](https://img.shields.io/github/stars/microsoft/sammo?style=flat-square&label=%20&color=gray&cacheSeconds=36000)\n1. [guidance](https://github.com/microsoft/guidance) (Nov 2022): A domain-specific language (DSL) for controlling large language models, focusing on model interaction and implementing the \"Chain of Thought\" technique.\n ![GitHub Repo stars](https://img.shields.io/github/stars/microsoft/guidance?style=flat-square&label=%20&color=gray&cacheSeconds=36000)\n1. [Autogen](https://github.com/microsoft/autogen) (Mar 2023): A customizable and conversable agent framework. / [ref](https://www.microsoft.com/en-us/research/blog/autogen-enabling-next-generation-large-language-model-applications/) / [Autogen Studio](https://www.microsoft.com/en-us/research/blog/introducing-autogen-studio-a-low-code-interface-for-building-multi-agent-workflows/) (June 2024)\n ![GitHub Repo stars](https://img.shields.io/github/stars/microsoft/autogen?style=flat-square&label=%20&color=gray&cacheSeconds=36000)\n1. [UFO](https://github.com/microsoft/UFO) (Mar 2024): A UI-focused agent for Windows OS interaction.\n ![GitHub Repo stars](https://img.shields.io/github/stars/microsoft/UFO?style=flat-square&label=%20&color=gray&cacheSeconds=36000)\n1. [Prompty](https://github.com/microsoft/prompty) (Apr 2024): A template language for integrating prompts with LLMs and frameworks, enhancing prompt management and evaluation.\n ![GitHub Repo stars](https://img.shields.io/github/stars/microsoft/prompty?style=flat-square&label=%20&color=gray&cacheSeconds=36000)\n1. [OmniParser](https://github.com/microsoft/OmniParser) (Sep 2024): A simple screen parsing tool towards pure vision based GUI agent.\n ![GitHub Repo stars](https://img.shields.io/github/stars/microsoft/OmniParser?style=flat-square&label=%20&color=gray&cacheSeconds=36000)\n1. [TinyTroupe](https://github.com/microsoft/TinyTroupe): LLM-powered multiagent persona simulation for imagination enhancement and business insights. [Mar 2024] ![GitHub Repo stars](https://img.shields.io/github/stars/microsoft/TinyTroupe?style=flat-square&label=%20&color=gray&cacheSeconds=36000)\n1. [RD-Agent](https://github.com/microsoft/RD-Agent): open source R&D automation tool [ref](https://rdagent.azurewebsites.net/) [Apr 2024]\n ![GitHub Repo stars](https://img.shields.io/github/stars/microsoft/RD-Agent?style=flat-square&label=%20&color=gray&cacheSeconds=36000)\n1. [Magentic-One](https://aka.ms/magentic-one): Built on AutoGen. A Generalist Multi-Agent System for Solving Complex Tasks [Nov 2024]\n1. [PyRIT](https://github.com/Azure/PyRIT) (Dec 2023): Python Risk Identification Tool for generative AI, focusing on LLM robustness against issues like hallucination, bias, and harassment.\n ![GitHub Repo stars](https://img.shields.io/github/stars/Azure/PyRIT?style=flat-square&label=%20&color=gray&cacheSeconds=36000)\n1. [Presidio](https://github.com/microsoft/presidio): Presidio (Origin from Latin praesidium ‘protection, garrison’). Context aware, pluggable and customizable data protection and de-identification SDK for text and images. [Oct 2019]\n1. [Microsoft Fabric](https://learn.microsoft.com/en-us/fabric/): Fabric integrates technologies like Azure Data Factory, Azure Synapse Analytics, and Power BI into a single unified product [May 2023]\n\n## ➡️ Convert ipynb to Python\n\n- To convert a Jupyter notebook (.ipynb) into a runnable Python scrip\n\n```bash\npip install nbformat nbconvert\n```\n\n```python\nimport nbformat\nfrom nbconvert import PythonExporter\n\n# Load the notebook\nnotebook_filename = 'your_notebook.ipynb'\nwith open(notebook_filename, 'r', encoding='utf-8') as notebook_file:\nnotebook_content = nbformat.read(notebook_file, as_version=4)\n\n# Convert the notebook to a Python script\npython_exporter = PythonExporter()\npython_code, _ = python_exporter.from_notebook_node(notebook_content)\n\n# Save the converted Python code to a .py file\npython_filename = notebook_filename.replace('.ipynb', '.py')\nwith open(python_filename, 'w', encoding='utf-8') as python_file:\npython_file.write(python_code)\n\nprint(f\"Notebook converted to Python script: {python_filename}\")\n```\n\n## **Contributor** 👀\n\n<a href=\"https://github.com/kimtth/azure-openai-samples/graphs/contributors\">\n<img src=\"https://contrib.rocks/image?repo=kimtth/azure-openai-samples\" />\n</a>\n\nⓒ `https://github.com/kimtth` all rights reserved."
    },
    {
      "name": "gswithjeff/autogen-multi-agent-workflow",
      "stars": 6,
      "img": "https://avatars.githubusercontent.com/u/135394694?s=40&v=4",
      "owner": "gswithjeff",
      "repo_name": "autogen-multi-agent-workflow",
      "description": "AI-powered YouTube Shorts creation with AutoGen 0.4. Agents handle scripting, voiceovers, images, and video assembly—all automated!",
      "homepage": "https://www.gettingstarted.ai/autogen-multi-agent-workflow-tutorial/",
      "language": "Python",
      "created_at": "2025-03-02T14:41:20Z",
      "updated_at": "2025-04-09T05:32:56Z",
      "topics": [
        "agentic-ai",
        "ai",
        "autogen",
        "autogen-sample"
      ],
      "readme": "# 🚀 AutoGen Multi-Agent Workflow Tutorial  \n\nThis repository contains a beginner-friendly **AutoGen 0.4** tutorial demonstrating how to create an AI-powered **multi-agent system** with **external API integrations** (text-to-speech, image generation) and Ollama integration.\n\n🎥 **[Watch the YouTube video here](https://youtu.be/0PFexhfA4Pk)**  \n📖 **[Read the blog post here](https://www.gettingstarted.ai/autogen-multi-agent-workflow-tutorial)**  \n\n## 🛠️ Features  \n- 🤖 **Multi-Agent System:** Script Writer, Voice Actor, Graphic Designer, and Director agents working together.  \n- 🎙️ **Text-to-Speech:** Converts AI-generated text into voiceovers using **ElevenLabs API**.  \n- 🖼️ **Image Generation:** Creates AI-generated visuals using **Stability AI**.  \n- 🏡 **Local LLM Support:** Optional integration with **Ollama** for running AI models offline.  \n- 🎮 **Interactive Console:** Type prompts and watch agents generate content dynamically.  \n\n---\n\n## 📂 Folder Structure  \n```plaintext\nautogen-multi-agent-workflow/\n│── tools.py               # Utility functions (text-to-speech, image generation)\n│── main.py                # Entry point for running the workflow\n│── .env                   # API keys (not included, create your own)\n│── .gitignore             # \n│── requirements.txt       # Dependencies for the project\n│── README.md              # Documentation\n```\n\n---\n\n## 🚀 Quick Start  \n\n### 1️⃣ Clone the Repository  \n```bash\ngit clone https://github.com/gswithjeff/autogen-multi-agent-workflow.git\ncd autogen-multi-agent-workflow\n```\n\n### 2️⃣ Create & Activate a Virtual Environment  \n#### **For macOS/Linux:**  \n```bash\npython -m venv venv\nsource venv/bin/activate\n```\n#### **For Windows:**  \n```bash\npython -m venv venv\nvenv\\Scripts\\activate\n```\n\n### 3️⃣ Install Dependencies  \n```bash\npip install -r requirements.txt\n```\n\n### 4️⃣ Set Up API Keys  \nCreate a `.env` file and add your API keys:  \n```plaintext\nOPENAI_API_KEY=your-openai-api-key\nELEVENLABS_API_KEY=your-elevenlabs-api-key\nSTABILITY_API_KEY=your-stability-ai-api-key\n```\n\n### 5️⃣ Create Required Accounts  \nBefore running the workflow, you'll need to create accounts for the following services:  \n\n- **ElevenLabs (Text to Speech)**: [Sign up here](https://try.elevenlabs.io/)\n- **Stability AI (Image Generation)**: [Sign up here](https://platform.stability.ai/)  \n\nThese accounts provide API access for text-to-speech and image generation, which are required for the agents to function.\n\n### 6️⃣ Run the Workflow  \n```bash\npython main.py\n```\nYou’ll be prompted to enter a task, and the agents will collaborate to generate a script, voiceover, and images dynamically.\n\n---\n\n## 🛠️ How It Works  \n1️⃣ **Script Writer Agent** generates structured captions.  \n2️⃣ **Voice Actor Agent** converts text to speech.  \n3️⃣ **Graphic Designer Agent** creates images based on captions.  \n4️⃣ **Director Agent** orchestrates the final output.  \n\n---\n\n## 🎯 Example Usage  \n\n### **User Prompt:**  \n```plaintext\nCreate a short AI-generated video about space exploration.\n```\n\n### **Generated JSON Response:**  \n```json\n{\n    \"topic\": \"Space Exploration\",\n    \"takeaway\": \"The future of space travel is closer than we think!\",\n    \"captions\": [\n        \"What lies beyond our galaxy?\",\n        \"Humans are reaching new frontiers.\",\n        \"AI is shaping space exploration.\",\n        \"New planets are waiting to be discovered.\",\n        \"The universe is limitless!\"\n    ]\n}\n```\n\n✅ **Voiceovers are generated**  \n✅ **Images are created**  \n✅ **Final video assembly is handled**  \n\n---\n\n## 🔧 Customization  \n- **Use a different LLM:** Swap OpenAI for **Ollama** to run locally. (See blog for more)\n- **Modify agent behaviors:** Edit the `system_message` for each agent.  \n- **Integrate new APIs:** Extend `tools.py` for additional functionality.  \n\n---\n\n## 🤝 Contributing  \nPull requests are welcome! If you find issues or want to improve the workflow, feel free to open an issue.  \n\n---\n\n## 📜 License  \nThis project is licensed under the **MIT License**.  \nYou are free to **use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the software**,  \nas long as the original copyright notice and permission notice appear in all copies.  \n\nFor full details, see the [LICENSE](LICENSE) file.\n\n---\n\n## 🌟 Support & Feedback  \nIf you find this project helpful, **⭐️ star the repo** and share your thoughts!  "
    },
    {
      "name": "ziaukhan/shopping-agent",
      "stars": 6,
      "img": "https://avatars.githubusercontent.com/u/545458?s=40&v=4",
      "owner": "ziaukhan",
      "repo_name": "shopping-agent",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2025-02-23T18:58:07Z",
      "updated_at": "2025-03-09T17:57:10Z",
      "topics": [],
      "readme": "# Shopping Agent\n\nWe can build a shopping agent using AutoGen 0.4.7+, CrewAI, and LangGraph Functional API.\n\nIn this repo we are building it using AutoGen. We can learn AutoGen from here:\n\nhttps://github.com/panaversity/learn-agentic-ai/tree/main/15_autogen_microsoft\n\n\nYou can also learn CrewAI from here:\n\nhttps://github.com/panaversity/learn-agentic-ai/tree/main/06b_crew_ai\n\n\nYou can also learn LangGraph Functional API from here:\n\nhttps://github.com/panaversity/learn-agentic-ai/tree/main/12a_langgraph_functional_api"
    },
    {
      "name": "CantCode023/evoke",
      "stars": 6,
      "img": "https://avatars.githubusercontent.com/u/95113225?s=40&v=4",
      "owner": "CantCode023",
      "repo_name": "evoke",
      "description": "Evoking excitement.",
      "homepage": "",
      "language": "Python",
      "created_at": "2025-02-08T20:11:58Z",
      "updated_at": "2025-02-26T10:19:57Z",
      "topics": [
        "agent",
        "ai",
        "autogen",
        "projects",
        "python",
        "recommendation-system"
      ],
      "readme": "![Header](./assets/header.png)\n\n# About\nEvoke is a simple Multi-AI Agent to suggest project ideas based on user's prompts.\nEvoke first generates a search query based on the user's prompts, Then, it uses the search query to query the relevant repositories and recommends the repositories that best fit the user's prompts.\nEvoke aims to help users find a project idea to relieve the coding itch, therefore, **evoking excitement**.\n\n# 💡 Key Features\n- 🤖 Query generation using Gemini 2.0 Flash Experimental\n- 🔎 Searches repositories based on user's prompts\n- 💡 Recommends repositories that best fit the user's prompts\n- 📈 Give detailed reports of a specified repository\n\n# Table of Contents\n- [About](#about)\n- [💡 Key Features](#-key-features)\n- [🚀 Getting Started](#-getting-started)\n  - [📄 Cloning the repository](#-cloning-the-repository)\n  - [📦 Installation](#-installation)\n  - [🔑 Environment variables](#-environment-variables)\n  - [🚀 Usage](#-usage)\n  - [📝 License](#-license)\n  - [🖊️ TODO-List](#️-todo-list)\n  - [📷 Video walkthrough](#-video-walkthrough)\n- [Made with ❤️ by @CantCode023](#made-with-❤️-by-cancode023)\n\n---\n\n# 🚀 Getting Started\n\n## 📄 Cloning the repository\n\n```bash\ngit clone https://github.com/CantCode023/evoke.git\n```\n\n## 📦 Installation\n\n```bash\npip install -r requirements.txt\n```\n\n## 🔑 Environment variables\n\nYou can get your API key from [OpenRouter](https://openrouter.ai/settings/keys).\nCreate a `.env` file in the project directory and add the following variables:\n\n```bash\nOPENROUTER_API_KEY=\"YOUR_API_KEY\"\n```\n\n## 🚀 Usage\n\n```bash\npython main.py\n```\n\n## 📝 License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n\n## 🖊️ TODO-List\n- [x] ~🤖 Fine-tune **query generation** to avoid hallucinations.~\n- [x] ~🤖 Fine-tune **recommend agent** to prioritize project repositories than package repositories.~\n- [x] ~⌨️ Add loop-support to allow agent to change suggestions.~\n- [x] ~📈 Add CLI implementation using Rich.~\n- [x] ~🐑 Use OpenRouter instead of Google AI Studio to avoid rate limits.~\n- [x] ~⚠️ Add support to respect rate limits.~\n- [x] ~✨ Refactor and optimize codebase.~\n- [ ] 💻 Use Streamlit for better readability\n- [ ] 🤖 Add **project generation** to generate unique projects based on saved repositories.\n\n## 📷 Video walkthrough\n[![Watch the video walkthrough](https://img.youtube.com/vi/5pRpD6zeyUQ/maxresdefault.jpg)](https://youtu.be/5pRpD6zeyUQ)\n\n---\n\nMade with ❤️ by [**@CantCode023**](https://github.com/CantCode023)"
    },
    {
      "name": "krishsub/MultiagentHackathon",
      "stars": 6,
      "img": "https://avatars.githubusercontent.com/u/17887541?s=40&v=4",
      "owner": "krishsub",
      "repo_name": "MultiagentHackathon",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2024-11-28T18:12:37Z",
      "updated_at": "2025-04-11T22:10:30Z",
      "topics": [],
      "readme": "# Getting Started\n\nTo get a copy of the code, you can either fork the repository, download it as a ZIP file, or clone it using Git.\n\n## Fork the Repository\n\n1. Go to the repository on GitHub. [https://github.com/krishsub/MultiagentHackathon](https://github.com/krishsub/MultiagentHackathon)\n2. Click the \"Fork\" button at the top right of the page.\n3. This will create a copy of the repository in your GitHub account.\n\n## Download as ZIP\n\n1. Go to the repository on GitHub.\n2. Click the \"Code\" button.\n3. Select \"Download ZIP\".\n4. Extract the downloaded ZIP file to your desired location.\n\n## Clone with Git\n\n1. Open your terminal.\n2. Choose an appropriate top-level directory (e.g. C:\\Users\\\\<you\\>\\mycode)\n3. Create this top-level directory (`code` in above example) and navigate to it\nfrom the command line\n4. Run the following command:\n\n    ```sh\n    git clone <repository-url>\n    ```\n\nReplace `<repository-url>` with the URL you copied (either this repo or yours \nif you forked it).\n\nThat's it! You now have a copy of the code.\n\n## Important:\n* Right-click the <code>MultiagentHackthon</code> folder and open in VS Code\n* Within VS Code, open the folder in devcontainer when prompted\n* This will take many minutes while the docker image and dependencies are\ndownloaded (one time)\n* Once the devcontainer opens, ***don't*** create a Python virtual \nenvironment/Conda if prompted, just close that window (we are already in a\nvirtual environment with devcontainers)\n"
    },
    {
      "name": "denniszielke/discover-ai",
      "stars": 6,
      "img": "https://avatars.githubusercontent.com/u/11569044?s=40&v=4",
      "owner": "denniszielke",
      "repo_name": "discover-ai",
      "description": "Demos for the Discovery Hour GenerativeAI on Azure",
      "homepage": "",
      "language": "Jupyter Notebook",
      "created_at": "2024-06-24T07:17:57Z",
      "updated_at": "2025-01-30T13:35:48Z",
      "topics": [
        "azure",
        "demos",
        "generative-ai",
        "openai"
      ],
      "readme": "# Discover Generative AI Apps on Azure\nThis repository contains demos and templates for building intelligent Apps using OpenAI on Azure.\n\nRegions that this deployment can be executed:\n- uksouth\n- swedencentral\n- canadaeast\n- australiaeast\n\n\n## Inventory\n\nThe following scenarios are implemented in this repository:\n\n| Name | Description | Technology  |\n| :-- | :--| :-- |\n| [Prompt Engineering](./src/01-prompting/basic.ipynb)| Basic prompt engineering examples | Azure Prompt Flow  |\n| [Embeddings](./src/02-embeddings/similarity.ipynb)| Learning about embedding models for text and images | Azure OpenAI, Azure Computer Vision  |\n| [Vector Search](./src/03-rag/rag-ai-search.ipynb)| Learning about low level retrieval augmented generation | Azure OpenAI, Azure AI Search  |\n| [Orchestrated Vector Search](./src/03-rag/app.py)| Learning about retrieval augmented generation with Orchestration | Azure OpenAI, Azure AI Search, LangChain, Streamlit |\n| [Persisting Chat Memory](./src/04-orchestration-lc/chat-conversation.ipynb)| Learning about persisting chat messages | Azure OpenAI, Azure AI Search, LangChain, Azure CosmosDB  |\n| [Tracing Language Models](./src/04-orchestration-lc/tracing.ipynb)| Learning about tracing language model invocations | Azure OpenAI, LangChain, Azure PromptFlow  |\n| [Tracing Chat application](./src/04-orchestration-lc/app.py)| Learning about tracing a running application | Azure OpenAI, LangChain, Azure PromptFlow, Streamlit, Azure AI Search  |\n| [Function Calling](./src/05-tools/tools-openai.ipynb)| Learning about using tools from a model | Azure OpenAI, Azure PromptFlow |\n| [Create picture generation](./src/06-agents-creative-ag/autogen-gtp4o.ipynb)| Learning about different models to collaborate for better picutres | Azure OpenAI, Autogen |\n| [Orchestrating tools](./src/05-tools/app.py)| Learning about orchestrating tools from an application | Azure OpenAI, Azure PromptFlow, LangChain |\n| [Orchestrating tools](./src/05-agents-coding-lg/app.py)| Learning about models generating better code | Azure OpenAI, Azure PromptFlow, LangChain |\n| [Orchestrating multiple tools](./src/05-multi-tools-tag-lc/app.py)| Learning about multiple orchestrating tools from an application | Azure OpenAI, Azure PromptFlow, LangChain |\n| [Agentic RAG](./src/06-agents-lg/app.py)| Learning about using agentic retrieval augmented generation | Azure OpenAI, Azure PromptFlow, LangChain, Langgraph |\n| [Async Agents Tools](./src/06-agents-tools-li/app.py)| Learning about using agentic async tools | Azure OpenAI, Azure PromptFlow, Lama Index |\n| [Distributed Async Agents Tools](./src/06-agents-kafka-li/app.py)| Learning about using agentic tools in a distributed system | Azure OpenAI, Azure PromptFlow, Lama Index, Kafka |\n\n## Quickstart & Infrastructure setup\n\nThe following lines of code will connect your Codespace az cli and azd cli to the right Azure subscription:\n\n```\naz login\n\nazd auth login\n\n```\n\nNow deploy the infrastructure components with azure cli\n\n```\nazd up\n```\n\nGet the values for some env variables\n```\nazd env get-values | grep AZURE_ENV_NAME\nsource <(azd env get-values)\n```\n\nLast but not least: deploy a dummy container in Azure Container Apps. \n```\nbash ./azd-hooks/deploy.sh 03-rag $AZURE_ENV_NAME\n\n```\n\n## Start locally\n\n```\npython -m streamlit run app.py --server.port=8000\n```\n\n## Deploy resources for Chapter 03\n\nRun the following script\n\n```\nazd env get-values | grep AZURE_ENV_NAME\nsource <(azd env get-values | grep AZURE_ENV_NAME)\nbash ./azd-hooks/deploy.sh 06-agents-reports-lg $AZURE_ENV_NAME\n```\n\nAll the other chapters work the same.\n\n### Configure prompt flow tracing\n\nhttps://microsoft.github.io/promptflow/reference/pf-command-reference.html\n"
    },
    {
      "name": "okareo-ai/okareo-python-sdk",
      "stars": 6,
      "img": "https://avatars.githubusercontent.com/u/139079097?s=40&v=4",
      "owner": "okareo-ai",
      "repo_name": "okareo-python-sdk",
      "description": "Python library for interacting with Okareo Cloud APIs",
      "homepage": "https://okareo.com/",
      "language": "Python",
      "created_at": "2023-10-03T20:46:48Z",
      "updated_at": "2025-03-27T18:29:22Z",
      "topics": [],
      "readme": "# Okareo Python SDK\n\n[![PyPI](https://img.shields.io/pypi/v/okareo?style=flat-square)](https://pypi.python.org/pypi/okareo/)\n[![PyPI - Python Version](https://img.shields.io/pypi/pyversions/okareo?style=flat-square)](https://pypi.python.org/pypi/okareo/)\n[![PyPI - License](https://img.shields.io/pypi/l/okareo?style=flat-square)](https://pypi.python.org/pypi/okareo/)\n\n---\n\n**PyPI**: [https://pypi.org/project/okareo/](https://pypi.org/project/okareo/)\n\n---\n\nPython library for interacting with Okareo Cloud APIs\n\n## Documentation\n[Getting Started, Guides, and API docs](https://docs.okareo.com/)\n\n## Installation\n\n1. Install the package\n    ```sh\n    pip install okareo\n    ```\n2. Get your API token from [https://app.okareo.com/](https://app.okareo.com/)\n   (Note: You will need to register first.)\n\n3. Go directly to the **\"2. Create your API Token\"** on the landing page in above app.\n\n4. Set the environment variable `OKAREO_API_KEY` to your generated API token.\n\n## Get Started Example Notebooks\n\nPlease see and run this notebook:<br>\nhttps://github.com/okareo-ai/okareo-python-sdk/blob/main/examples/classification_eval.ipynb\n\nSee additional examples under:<br>\nhttps://github.com/okareo-ai/okareo-python-sdk/tree/main/examples\n\n## Using Okareo LangChain Callbacks Handler\n\nWe provide a LangChain callback handler that lets you easily integrate your current workflows with the Okareo platform.\n\nIf don't have LangChain dependencies installed in your environment, you can install the base ones (that will help you run the examples) with:\n```sh\npip install okareo[langchain]\n```\n\nIntegrating callbacks into your chain is as easy as importing the SDK in your module add adding the following\n```\nfrom okareo.callbacks import CallbackHandler\n...\nhandler = CallbackHandler(mut_name=\"my-model\", context_token=\"context-token\")\nllm = OpenAI(temperature=0.3, callbacks=[handler])\n\n```\nDuring the LangChain LLM runs we will collect input and output information so you can analyze it further with the Okareo toolkit.\n\nYou can also see an usage example in [./examples/langchain_callback_example.py](./examples/langchain_callback_example.py)\n\n\n---\n\nAll rights reserved for Okareo Inc\n"
    },
    {
      "name": "rominirani/autogen-experiments",
      "stars": 6,
      "img": "https://avatars.githubusercontent.com/u/1614870?s=40&v=4",
      "owner": "rominirani",
      "repo_name": "autogen-experiments",
      "description": "Experiments with Autogen Framework (https://github.com/microsoft/autogen) ",
      "homepage": null,
      "language": "Python",
      "created_at": "2024-09-30T04:48:03Z",
      "updated_at": "2025-04-01T10:38:49Z",
      "topics": [],
      "readme": "Experiments demonstrating [Autogen](https://github.com/microsoft/autogen) framework. \n\n> AutoGen is an open-source programming framework for building AI agents and facilitating cooperation among multiple agents to solve tasks. AutoGen aims to provide an easy-to-use and flexible framework for accelerating development and research on agentic AI, like PyTorch for Deep Learning.\n> It offers features such as agents that can converse with other agents, LLM and tool use support, autonomous and human-in-the-loop workflows, and multi-agent conversation patterns.\n\n![Autogen Framework](https://microsoft.github.io/autogen/0.2/assets/images/autogen_agentchat-250ca64b77b87e70d34766a080bf6ba8.png)\n\n| Folder Name    | Application  | Blog Post  |\n| -------------- | ------------ | ---------- |\n| [`app1`](app1) | Multi-turn conversation between 2 agents, CFP Writer and CFP Reviewer that converse with each other to improve a CFP. Their conversation lasts the number of `max_turns` mentioned while setting up i.e. initiating the chat.| [Multi-Agent Tutorial Series - Part 1](https://medium.com/google-cloud/multi-agent-interactions-using-autogen-with-gemini-a416008e5df6) |\n| [`app2`](app2) | Multi-turn conversation between 2 agents with an explicit termination done by the framework if the CFP Reviewer mentions \"looks good\". It does not depend on the `max_turns` parameter while initiating the chat. | [Multi-Agent Tutorial Series - Part 2](https://medium.com/google-cloud/multi-agent-interactions-with-autogen-and-gemini-part-2-terminating-conversations-883788137162) |\n| [`app3`](app3) | Conversation between a Human Agent and an Autonomous Agent. The Human asks the Autonomous Agent to do a task and then reviews it. The Human can then either terminate the conversation or ask the Autonomous Agent to do the task again.| [Multi-Agent Tutorial Series - Part 3](https://medium.com/google-cloud/tutorial-multi-agent-interactions-with-autogen-and-gemini-part-3-introducing-manual-human-8674fe02b7d9) |\n| [`app4`](app4) | This is the same example as `app3` , except that instead of invoking the Gemini models online, we use Ollama locally with the Google Gemma model as the LLM.| [Multi-Agent Tutorial Series - Part 4](https://medium.com/google-cloud/tutorial-multi-agent-interactions-with-autogen-and-gemini-part-4-using-local-llms-c6b2faa6a435) |\n| [`app5`](app5) | This application shows integration of [AgentOps.ai](https://www.agentops.ai/), that provides observability (monitoring, metrics, session replays, more) about your agents.| [Multi-Agent Tutorial Series - Part 5](https://medium.com/google-cloud/tutorial-multi-agent-interactions-with-autogen-and-gemini-part-5-agentops-a70912486c13) |\n| [`app6`](app6) | This application shows additional features of the framework, especially how to tap into conversation history, summarize conversations and get costs for a conversation between agents.| [Multi-Agent Tutorial Series - Part 6](https://medium.com/google-cloud/tutorial-multi-agent-interactions-with-autogen-and-gemini-part-6-history-summarization-and-3212431ff13d) |\n| [`app7`](app7) | This application shows one of the conversation design patterns i.e. Sequential Chat, a sequence of chats between two agents, chained together by a carryover mechanism, which brings the summary of the previous chat to the context of the next chat.| [Multi-Agent Tutorial Series - Part 7](https://medium.com/google-cloud/tutorial-multi-agent-interactions-with-autogen-and-gemini-part-7-sequential-chat-885fe1cc55d0) |\n| [`app8`](app8) | This application the Group Chat pattern, where we configure a bunch of agents, blog writer, hashtag generator and then allow a UserProxy Agent to co-ordinate the tasks by picking the right agent for it. | [Multi-Agent Tutorial Series - Part 8](https://medium.com/google-cloud/tutorial-multi-agent-interactions-with-autogen-and-gemini-part-8-group-chat-511440860129) |\n"
    },
    {
      "name": "LittleLittleCloud/code-interpreter-workflow",
      "stars": 5,
      "img": "https://avatars.githubusercontent.com/u/16876986?s=40&v=4",
      "owner": "LittleLittleCloud",
      "repo_name": "code-interpreter-workflow",
      "description": null,
      "homepage": null,
      "language": "C#",
      "created_at": "2024-08-02T00:31:52Z",
      "updated_at": "2025-04-03T10:52:54Z",
      "topics": [],
      "readme": "## Human-in-the-loop Code Interpreter workflow\n\n![demo](asset/screenshot.png)\n\n## Get start\n### Pre-requisite\n- dotnet 8.0\n- python with jupyter and ipykernel setup\n    - to install jupyter, run `pip install jupyter`\n    - to install ipykernel, run `pip install ipykernel`\n    - to setup ipykernel, run `python -m ipykernel install --user --name=python3`\n    - to confirm the setup, run `jupyter kernelspec list` and you should see `python3` in the list\n- env:OPENAI_API_KEY\n- env:BING_API_KEY\n\n### Run the workflow\n\nThe recommended way to run the workflow is to use the [Agent Chatroom](https://github.com/LittleLittleCloud/Agent-ChatRoom), which provides a chatroom UI to interact with the agents in the workflow.\n\nTo run the chatroom, cd to the cloned repo and run the following command:\n\n```bash\ndotnet run\n```\n\nA chatroom UI will be started at http://localhost:50001, and you can interact with the agents in `dotnet-interactive-chatroom` channel.\n\n## Workflow State Transition Diagram\n```mermaid\nstateDiagram\n[*] --> create_task@user\ncreate_task@user--> write_code@coder\ncreate_task@user  --> not_code_task@assistant\nwrite_code@coder --> review_code@user\n review_code@user --> run_code@runner\nreview_code@user --> fix_comment@coder\nfix_comment@coder --> review_code@user\nrun_code@runner --> succeed@assistant\nrun_code@runner --> fail@assistant\nrun_code@runner --> fix_code@coder\nfix_code@coder --> review_code@user\nfail@assistant --> [*]\nsucceed@assistant --> [*]\nnot_code_task@assistant --> [*]\n```\n"
    },
    {
      "name": "LittleLittleCloud/AIAgentTeam",
      "stars": 5,
      "img": "https://avatars.githubusercontent.com/u/16876986?s=40&v=4",
      "owner": "LittleLittleCloud",
      "repo_name": "AIAgentTeam",
      "description": "A demo to create AI agent team using AutoGen.Net",
      "homepage": null,
      "language": "C#",
      "created_at": "2024-04-10T23:13:33Z",
      "updated_at": "2024-10-02T09:22:00Z",
      "topics": [],
      "readme": "# AI Agent team\n\nThis demo shows how to use AutoGen.Net to create a virtual AI agent team that can answer questions related to Tesla.\n\nThe AI agent team is composed of 3 agents:\n- `User`: This agent represents the user and raises questions to the CEO.\n- `CEO`: This agent represents the CEO of Tesla and answers questions related to the company. When the question is related to marketing, the CEO forwards the question to the CMO.\n- `CMO`: This agent represents the CMO of Tesla and answers questions related to marketing. It has access to bing search to find the answer to the question.\n\n## Workflow\n![AI Agent team workflow](./asset/AI-Agent-Team.png)\n\n## How to run the demo\nFirstly, set up `OPENAI_API_KEY` and `BING_API_KEY` to your environment variables.\n> [!NOTE]\n> To get `BING_API_KEY`, you need to create a Bing Search resource in Azure and get the key from the resource. For information on how to create a Bing Search resource, see [Create a Bing Search resource](https://www.microsoft.com/en-us/bing/apis/bing-web-search-api).\n```\nexport OPENAI_API_KEY=<your_openai_api_key>\nexport BING_API_KEY=<your_bing_api_key>\n```\n\nThen, clone the repository and run the following commands:\n```\ndotnet run\n```\n\n## Example output\nIn the example below, two questions are raised from user\n- What's the sales data for Q1 2024 semester?\n- Can you explain the huge gap between produced number and delivered number in Q1 2024?\n\nIn the chat session, the CEO forwards the first question to CMO because it is related to marketing, who then uses Bing search to find the answer. The CEO answers the second question because it is related to the company-wide operation.\n\n![AI Agent team chat session](./asset/team-chat.gif)\n\n## Further reading\n- [AutoGen.Net](https://microsoft.github.io/autogen-for-net/)"
    },
    {
      "name": "NVISOsecurity/sans-webinar-robocop",
      "stars": 5,
      "img": "https://avatars.githubusercontent.com/u/5310436?s=40&v=4",
      "owner": "NVISOsecurity",
      "repo_name": "sans-webinar-robocop",
      "description": "This repository contains the demo code for the webcast organized by SANS titled \"From Playbooks to Robocop: The Evolution of SOC Automation\".",
      "homepage": "https://www.sans.org/webcasts/from-playbooks-robocop-evolution-soc-automation/",
      "language": "Python",
      "created_at": "2025-03-26T12:52:35Z",
      "updated_at": "2025-04-23T06:33:17Z",
      "topics": [
        "agent",
        "ai",
        "autogen",
        "sans",
        "webcast"
      ],
      "readme": "# Robocop: The Evolution of SOC Automation\n\nThis repository contains the demo code build on top of [Autogen](https://microsoft.github.io/autogen/stable/) for the webcast organized by SANS titled [**\"From Playbooks to Robocop: The Evolution of SOC Automation\"**](https://www.sans.org/webcasts/from-playbooks-robocop-evolution-soc-automation/).\n\n![SANS Webcast](images/SANSWebinar.jpg \"Title\")\n\n## Overview\n\nThe landscape of Security Operations is changing rapidly, and automation is leading the charge. In the second episode of the “Purple Team Power Hour”, we’ll explore how security teams are moving beyond static, pre-built playbooks toward dynamic, AI-driven solutions that can adapt to evolving threats in real time. \n\n\n## Prerequisites\n\n- [Azure OpenAI Service](https://learn.microsoft.com/en-us/azure/ai-services/openai/overview)\n\n## Getting Started\n\n1. Clone the repository:\n    ```bash\n    git clone git@github.com:NVISOsecurity/sans-webinar-robocop.git\n    cd sans-webinar-robocop\n    ```\n\n2. Install dependencies:\n    ```bash\n    pip install -r requirements.txt\n    ```\n\n3. Copy .env file:\n    ```bash\n    cp .env_example .env\n    ```\n\n4. Set values in `.env` file\n\n## Usage\n\nRun the demo scripts to explore the automation examples:\n```bash\npython 01_agent.py\n```\n\n## License\n\nThis project is licensed under the [MIT License](LICENSE).\n"
    },
    {
      "name": "jplck/from-single-to-multi-agent",
      "stars": 5,
      "img": "https://avatars.githubusercontent.com/u/8427174?s=40&v=4",
      "owner": "jplck",
      "repo_name": "from-single-to-multi-agent",
      "description": null,
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2025-03-18T07:42:27Z",
      "updated_at": "2025-04-20T11:44:15Z",
      "topics": [],
      "readme": "# From Single to Multi-Agent AI Systems\n\nThis repository contains a comprehensive collection of labs and resources for learning about AI agent development, from single-agent implementations to complex multi-agent systems. It explores various frameworks including Semantic Kernel, AutoGen, and more, demonstrating practical implementations of modern AI agent architectures.\n\n## Overview\n\nThe repository is structured to provide a progressive learning journey through the world of AI agents:\n\n- **Foundation concepts**: Learn about basic agent patterns like ReAct\n- **Advanced frameworks**: Explore Semantic Kernel, process frameworks, and AutoGen\n- **Multi-agent orchestration**: Discover techniques for coordinating multiple specialized agents\n\n## Lab Structure\n\nThe labs are organized in increasing complexity, starting from basic concepts and advancing to sophisticated multi-agent systems:\n\n| Lab | Description | Link |\n|-----|-------------|------|\n| Azure OpenAI Basics | Introduction to working with Azure OpenAI | [01-basics/01_azureopenai.ipynb](labs/01-basics/01_azureopenai.ipynb) |\n| Semantic Kernel - Orchestration | Learn to orchestrate AI capabilities with Semantic Kernel | [02-semantic-kernel/01_orchestrator.ipynb](labs/02-semantic-kernel/01_orchestrator.ipynb) |\n| Semantic Kernel - Functions | Working with functions in Semantic Kernel | [02-semantic-kernel/02_functions.ipynb](labs/02-semantic-kernel/02_functions.ipynb) |\n| Semantic Kernel - Multi-modal | Handling multi-modal content with Semantic Kernel | [02-semantic-kernel/03_multi-modal.ipynb](labs/02-semantic-kernel/03_multi-modal.ipynb) |\n| ReAct | Implementing AI planning capabilities | [03-planning/01_single_agent.ipynb](labs/03-planning/01_single_agent.ipynb) |\n| Reasoning | Implementing AI reasoning | [03-planning/02_reasoning_agent.ipynb](labs/03-planning/02_reasoning_agent.ipynb) |\n| Agent Framework - Basic Agents | Building individual agents with Semantic Kernel | [04-agent-framework/01_agents.ipynb](labs/04-agent-framework/01_agents.ipynb) |\n| Agent Framework - Group Chat | Implementing collaborative agent conversations | [04-agent-framework/02_agents-group-chat.ipynb](labs/04-agent-framework/02_agents-group-chat.ipynb) |\n| Process Framework - Basics | Understanding the Semantic Kernel Process Framework | [05-process-framework/01_process.ipynb](labs/05-process-framework/01_process.ipynb) |\n| Process Framework - Advanced | Building complex workflows with the Process Framework | [05-process-framework/02_process.ipynb](labs/05-process-framework/02_process.ipynb) |\n| AutoGen - Simple Group | Creating basic agent groups with AutoGen | [06-autogen/01_autogen-simple-group.ipynb](labs/06-autogen/01_autogen-simple-group.ipynb) |\n| AutoGen - Group Chat | Building multi-agent conversations with AutoGen | [06-autogen/02_autogen-group-chat.ipynb](labs/06-autogen/02_autogen-group-chat.ipynb) |\n| AutoGen - Reasoning | Implementing advanced reasoning with AutoGen | [06-autogen/03_autogen-reasoning.ipynb](labs/06-autogen/03_autogen-reasoning.ipynb) |\n| Single React Agent | Implementation of a basic ReAct pattern agent | [Single React Agent](labs/single_react_agent) |\n\n## Key Concepts Covered\n\n### Single-Agent Example\n\n- **ReAct (Reasoning + Acting)** - Combining reasoning with tool use for more effective agents\n- Function/tool calling capabilities\n- State management within agents\n\n### Semantic Kernel Framework\n\n- Creating kernel functions and tools\n- Building specialized agents with specific roles\n- Orchestrating multi-agent interactions\n- Process framework for structured AI workflows\n\n### Process Framework\n\n- Event-driven design for AI workflows\n- State management across process steps\n- Conditional branching and error handling\n- Complex workflow orchestration\n\n### Multi-Agent Architectures\n\n- Specialized agent roles and responsibilities\n- Agent coordination and communication\n- Multi-agent problem solving through collaboration\n- Group chat implementations\n\n### AutoGen Framework\n\n- Building agent groups\n- Implementing conversational agents\n- Advanced reasoning capabilities\n\n## Getting Started\n\n1. Clone this repository\n2. Install the required packages:\n   ```\n   pip install -r requirements.txt\n   ```\n3. (Optional) Configure your Azure OpenAI credentials in a `.env` file\n4. Start exploring the labs sequentially, beginning with the single agent implementation\n\n## Azure Integration (Optional)\n\nThis repository includes infrastructure templates for deploying solutions to Azure:\n- Azure OpenAI services configuration\n- Container Apps environments\n- Storage and hosting services"
    },
    {
      "name": "lukaskellerstein/ai",
      "stars": 5,
      "img": "https://avatars.githubusercontent.com/u/12882519?s=40&v=4",
      "owner": "lukaskellerstein",
      "repo_name": "ai",
      "description": "Artificial Intelligence projects",
      "homepage": null,
      "language": "Python",
      "created_at": "2023-02-19T12:49:14Z",
      "updated_at": "2025-04-15T10:16:51Z",
      "topics": [],
      "readme": "![ml-dl](./assets/ml-vs-dl.jpg)\n\n**Artificial intelligence (AI)** refers to the ability of machines to perform tasks that would normally require human intelligence, such as reasoning, perception, learning, and decision making. AI is a broad field that encompasses many different subfields, including machine learning and deep learning.\n\n**Machine learning** is a subset of AI that involves training machines to learn from data without being explicitly programmed. In machine learning, algorithms are developed that can learn from data and improve their performance over time. This process involves feeding large amounts of data into a machine learning model, which then makes predictions or classifications based on that data. Examples of machine learning applications include image recognition, natural language processing, and recommendation systems.\n\n**Deep learning** is a type of machine learning that involves the use of artificial neural networks, which are modeled after the structure and function of the human brain. Deep learning algorithms can automatically extract features from data, allowing them to learn from complex and large datasets. Examples of deep learning applications include speech recognition, object detection, and autonomous driving. Deep learning has shown great potential for solving complex problems in various fields, from healthcare to finance to robotics.\n\n**Reinforcement learning** is another important subset of machine learning that focuses on training agents to make decisions by interacting with an environment. In reinforcement learning, an agent learns through trial and error, receiving rewards or penalties based on its actions. The goal is to maximize cumulative rewards over time by learning an optimal strategy, or policy, for decision-making.\n\n## Notes\n\nRelease disk space for WSL2: https://stephenreescarter.net/how-to-shrink-a-wsl2-virtual-disk/\n\n## Tutorial\n\n1. Pytorch\n   - ANN\n   - CNN\n   - RNN\n   - Transformer\n2. OpenAI\n   - Prompt\n   - Prompt engineering\n   - Chat\n3. Hugging Face\n   - Explore\n     - text2text-generation\n     - text-generation\n     - text-img\n     - text-speech\n     - ...etc\n   - Models = Pre-trained Transformers\n     - Run (32bit, 16bit, 8bit, 4bit) (CPU vs. GPU) (Accelerate)\n     - Train / Fine-tune\n   - Datasets\n   - Multimodal models\n4. Langchain\n\n## Finish\n\n- Fine-tuning multimodal models\n\n- Autogen\n\n  - Agent with RAG (ideally ChromaDB) = Memory ?? https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/memory.html\n\n  - Team - SelectorGroupChat with Human-in-the-loop - https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/selector-group-chat.html\n\n  - Team - Magentic-One - https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/magentic-one.html\n\n- Semantic-kernel\n\n  - Agent with RAG (ideally ChromaDB)\n  - Orchestration of Agents = Teams ??\n\n- Reinforcement learning\n\n  - Gymansium environments\n  - Stable-baseline 3\n"
    },
    {
      "name": "shubhamchau222/AIAgentsjourney",
      "stars": 5,
      "img": "https://avatars.githubusercontent.com/u/61900242?s=40&v=4",
      "owner": "shubhamchau222",
      "repo_name": "AIAgentsjourney",
      "description": null,
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2025-01-30T18:32:14Z",
      "updated_at": "2025-04-03T13:43:33Z",
      "topics": [],
      "readme": "# AIAgentAllRepos\nContains all AI Agent Frameworks with handson\n\nclaude mcp server: https://github.com/shubhamchau222/AIAgentsjourney/tree/main/MCP\n"
    },
    {
      "name": "Shailender-Youtube/Autogen-0.4-Mini-Project",
      "stars": 5,
      "img": "https://avatars.githubusercontent.com/u/174622601?s=40&v=4",
      "owner": "Shailender-Youtube",
      "repo_name": "Autogen-0.4-Mini-Project",
      "description": "Autogen 0.4 Mini Project (Multi-Agent)",
      "homepage": null,
      "language": "Python",
      "created_at": "2025-01-25T12:03:31Z",
      "updated_at": "2025-04-15T19:48:38Z",
      "topics": [],
      "readme": "# Autogen-0.4-Mini-Project\nAutogen 0.4 Mini Project (Multi-Agent)\n\n# Watch the Video for Step by Step Configuration\n\n[![Video Title](https://img.youtube.com/vi/V3pYAAWKET0/0.jpg)](https://www.youtube.com/watch?v=V3pYAAWKET0)\n"
    },
    {
      "name": "filipw/strathweb-samples-azureopenai",
      "stars": 4,
      "img": "https://avatars.githubusercontent.com/u/1710369?s=40&v=4",
      "owner": "filipw",
      "repo_name": "strathweb-samples-azureopenai",
      "description": "Various Azure Open AI Samples using the .NET SDK",
      "homepage": "",
      "language": "C#",
      "created_at": "2023-04-20T21:47:45Z",
      "updated_at": "2025-03-27T09:02:59Z",
      "topics": [
        "azure-openai"
      ],
      "readme": "# Azure OpenAI Samples\n\nVarious Azure Open AI Samples using the .NET SDK\n\n## GPT-based Classification\n\n → classification of natural language data\\\n 📝 [Blog post: Building GPT powered applications with Azure OpenAI Service](https://www.strathweb.com/2023/04/building-gpt-powered-applications-with-azure-openai-service/) \\\n 💻 [Code](src/Strathweb.Samples.AzureOpenAI.GptClassification/)\n\n## Embeddings\n\n → similarity scores using embeddings \\\n 📝 [Blog post: Using embeddings model with Azure OpenAI Service](https://www.strathweb.com/2023/09/using-embeddings-model-with-azure-openai/)\\\n 💻 [Code](src/Strathweb.Samples.AzureOpenAI.Embeddings/)\n\n## Bring Your Own Data\n\n → retrieval augmented generation pattern using integration into Azure AI Search\\\n 📝 [Blog post part 1: Setting up Azure AI Search](https://www.strathweb.com/2023/11/using-your-own-data-with-gpt-models-in-azure-openai-part-1/)\\\n 📝 [Blog post part 2: Calling Azure OpenAI Service via REST API](https://www.strathweb.com/2023/11/using-your-own-data-with-gpt-models-in-azure-openai-part-2/)\\\n 📝 [Blog post part 3: Calling Azure OpenAI Service via .NET SDK](https://www.strathweb.com/2023/12/using-your-own-data-with-gpt-models-in-azure-openai-part-3/)\\\n 📝 [Blog post part 4: Adding vector search](https://www.strathweb.com/2024/02/using-your-own-data-with-gpt-models-in-azure-openai-part-4/)\\\n 💻 [Code](src/Strathweb.Samples.AzureOpenAI.BringYourOwnData/)\n\n## GPT Text Generation + Speech Synthesis\n\n → Azure OpenAI summarization with Azure AI Speech speech synthesis\\\n 📝 [Blog post: Combining Azure OpenAI with Azure AI Speech](https://www.strathweb.com/2024/03/combining-azure-openai-with-azure-ai-speech/)\\\n 💻 [Code](src/Strathweb.Samples.AzureOpenAI.SpeechSynthesis/)\n\n## Tool Calling\n\n→ using tool (function) calling to orchestrate application workflows\\\n📝 [Blog post: Tool Calling with Azure OpenAI - Part 1: The Basics](https://www.strathweb.com/2024/04/function-calling-with-azure-openai-part-1/)\\\n📝 [Blog post: Tool Calling with Azure OpenAI - Part 2: Using the tools directly via the SDK](https://www.strathweb.com/2024/04/function-calling-with-azure-openai-part-2/)\n💻 [Code](src/Strathweb.Samples.AzureOpenAI.ToolCalling/)\n\n## AutoGen Tool Calling\n\n→ tool (function) calling with the [AutoGen](https://github.com/microsoft/autogen) framework\\\n📝 Blog post - WIP\\\n💻 [Code](src/Strathweb.Samples.AzureOpenAI.AutoGenFunctionCalling/)\n\n## Assistant Tool Calling\n\n→ tool (function) calling with the [Assistants API](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/assistants)\\\n📝 Blog post - WIP\\\n💻 [Code](src/Strathweb.Samples.AzureOpenAI.AssistantToolCalling/)\n"
    },
    {
      "name": "Squidgy-AI/BoilerPlateV1",
      "stars": 4,
      "img": "https://avatars.githubusercontent.com/u/188782334?s=40&v=4",
      "owner": "Squidgy-AI",
      "repo_name": "BoilerPlateV1",
      "description": "Barebone Code structure",
      "homepage": "https://squidgy.vercel.app",
      "language": "Python",
      "created_at": "2024-11-17T16:57:28Z",
      "updated_at": "2025-04-21T20:33:25Z",
      "topics": [],
      "readme": "# Squidgy Project Deployment Guide\n\nThis guide provides instructions for deploying the Squidgy application, which consists of a frontend and two backend options (local testing and hosting).\n\n## Project Structure\n\n```\nBoilerPlateV1\\\n├── Code\\\n│   ├── squidgy-backend\\          # Backend for local testing\n│   ├── squidgy-backend_host\\     # Backend for deployment\n│   └── squidgy-frontend\\         # Frontend (common for both backends)\n```\n\n## Deployment Methods\n\nThere are two ways to deploy this application:\n\n1. **Manual Deployment**: Follow the step-by-step instructions below\n2. **Docker Deployment**: Use the Docker configuration provided in the \"Docker Deployment Instructions\" document\n\n## Docker Deployment\n\nFor Docker-based deployment, follow these steps:\n\n1. Create a Dockerfile in the backend_host directory:\n```dockerfile\n# Choose our version of Python\nFROM python:3.12\n\n# Set up a working directory\nWORKDIR /code\n\n# Copy just the requirements into the working directory so it gets cached by itself\nCOPY ./requirements.txt /code/requirements.txt\n\n# Install the dependencies from the requirements file\nRUN pip install --no-cache-dir --upgrade -r /code/requirements.txt\n\n# Copy the vector_store.py file to the root code directory\nCOPY ./app/vector_store.py /code/vector_store.py\nCOPY ./app/roles_config.py /code/roles_config.py\n\n# Copy the entire app directory with all files\nCOPY ./app /code/app\n\n# Copy GHL directory \nCOPY ./GHL /code/GHL\n\n# Copy conversation_templates.xlsx to the code directory\nCOPY ./app/conversation_templates.xlsx /code/conversation_templates.xlsx\n\nCOPY ./.env /code/.env\n\n# Tell uvicorn to start spin up our code, which will be running inside the container now\nCMD [\"uvicorn\", \"app.main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"80\"]\n```\n\n2. Create a Dockerfile in the frontend directory:\n```dockerfile\n# Use Node.js as the base image\nFROM node:18-alpine\n\n# Set working directory\nWORKDIR /app\n\n# Copy package.json and yarn.lock\nCOPY package.json yarn.lock ./\n\n# Install dependencies\nRUN yarn install --frozen-lockfile\n\n# Copy the rest of the code\nCOPY . .\n\n# Build the application and ignore ESLint errors\n# RUN yarn build --no-lint\n\n# Expose the port the app will run on\nEXPOSE 3000\n\n# Command to run the app\nCMD [\"yarn\", \"dev\", \"--hostname\", \"0.0.0.0\"]\n```\n\n3. Create a docker-compose.yml file in the root directory:\n```yaml\nversion: '3'\nservices:\n  backend:\n    build:\n      context: ./Code/squidgy-backend_host\n    ports:\n      - \"80:80\"\n    environment:\n      - OPENAI_API_KEY=${OPENAI_API_KEY}\n  \n  frontend:\n    build:\n      context: ./Code/squidgy-frontend\n    depends_on:\n      - backend\n    ports:\n      - \"3000:3000\"\n    environment:\n      - NEXT_PUBLIC_API_BASE=backend:80\n```\n\n4. Run the deployment:\n```bash\ndocker-compose up -d\n```\n\n**Note**: Make sure all environment variables required by both services are properly configured. The backend exposes port 80 in this configuration, so the frontend's API base should point to `backend:80` instead of `127.0.0.1:8080`.\n\n## Deployment Order\n\n**Important**: Always deploy the backend first, followed by the frontend. This ensures proper WebSocket connection establishment.\n\n## Backend Deployment (squidgy-backend_host)\n\n1. Navigate to the backend_host directory:\n   ```\n   cd Code\\squidgy-backend_host\n   ```\n\n2. Install required dependencies:\n   ```\n   pip install -r requirements.txt\n   ```\n\n3. Set up environment variables:\n   - Check the `.env` file in the backend_host folder\n   - Contact Soma for required environment variables if needed\n\n4. **Critical Step**: Update all occurrences of `127.0.0.1:8080` in the code to your actual deployment endpoint URL\n   - This includes file paths, configuration files, and connection strings\n   - This step ensures the backend can be properly accessed by the frontend\n\n5. Start the backend server:\n   ```\n   python main.py\n   ```\n\n6. Note the URL where your backend is running, as you'll need it for frontend configuration\n\n## Frontend Deployment (squidgy-frontend)\n\n1. Navigate to the frontend directory:\n   ```\n   cd Code\\squidgy-frontend\n   ```\n\n2. Install required dependencies:\n   ```\n   yarn install\n   ```\n\n3. Configure the API endpoint:\n   - Open the frontend configuration or `.env` file\n   - Update the API base URL to point to your deployed backend:\n     ```\n     # Located in .env or similar configuration file\n     NEXT_PUBLIC_API_BASE=your-backend-endpoint-url\n     ```\n   - This replaces the default `127.0.0.1:8080` with your actual backend URL\n\n4. Run the frontend application:\n   ```\n   yarn dev\n   ```\n   \n   The frontend will be accessible at `http://127.0.0.1:3000`\n\n\n## WebSocket Implementation Notes\n\n- This application uses WebSockets for real-time communication\n- Ensure that any firewalls or network configurations allow WebSocket connections\n- The backend must be running and accessible before the frontend is started\n- Test the WebSocket connection after deployment to ensure proper functionality\n\n## Troubleshooting\n\nIf you encounter connection issues:\n\n1. Verify the backend is running and accessible\n2. Check that all `127.0.0.1:8080` references were updated to the correct deployment URL\n3. Confirm environment variables are properly set in both backend and frontend\n4. Ensure WebSocket ports are not blocked by firewalls or network policies\n\n## Contact Information\n\nFor additional deployment assistance or to obtain required environment variables, please contact Soma."
    },
    {
      "name": "minyang-chen/seat-booking-agent",
      "stars": 4,
      "img": "https://avatars.githubusercontent.com/u/3905747?s=40&v=4",
      "owner": "minyang-chen",
      "repo_name": "seat-booking-agent",
      "description": "Experimental Seat Booking Agent implementation using OpenAI Agents SDK and Autogen memory and MCP tools support",
      "homepage": "",
      "language": "Python",
      "created_at": "2025-03-23T14:49:01Z",
      "updated_at": "2025-04-23T01:03:03Z",
      "topics": [],
      "readme": "# Experimental Seat Booking Agent\nExperimental seat booking agent implementation combines OpenAI Agents SDK, Autogen extensions, and MCP tools support.\n\n![seat-booking-workflow-graph](agentic-system-diagram.png?raw=true \"seat-booking-agent-workflow\")\n\n### Use Case\nThe self-service airline seat reservation kiosk, also known as a flight seat booking service agent or passenger service agent, is designed to provide comprehensive customer support and assistance. This kiosk ensures a smooth and positive travel experience by handling various tasks, including reservations, through its call center interface.\n\n### Key Features: \n- Triage agent answer user general inquiry with tools \n- Triage agent should route customer question to specialized agents\n- Triage agent retrieve user preferences on seating, airlines and food   \n- Triage agent use agents as tools on question routing  \n- Triage agent use MCP server as tools to perform web fetch content and read files system   \n- QA agent has knownledge to handling questions about Plane, Flight and Onboarding \n- Seat booking agent specifilized on plane seat reservation tasks\n- Web Chat interface for user self-service\n\n### Lab Technical Stack\n- Linux \n- OpenAI Agents SDK\n- Autogen 0.4 + extensions (MCP Tools,OpenAI, Ollama)\n- Chainlit (UI)\n- Ollama (local Qwen2,5)\n\n### Environment \n```\npython -m venv .venv\nsource .venv/bin/activate\npip install -r requirements.txt\n```\n### Running\n```\nchainlit run app.py\n```\n\n### Todos\n- save seat history at the end of the booking\n- improve FAQ agent to use RAG on actual documents\n- improve memory storage\n- add MCP tools to invoke actual booking service api \n- more to came...\n\n## Contributing\n\n1. Fork the repository\n2. Create a feature branch\n3. Commit your changes\n4. Push to the branch\n5. Create a Pull Request\n\n**Important**: Please keep your pull requests small and focused.  This will make it easier to review and merge.\n\n## Feature Requests\nIf you have a feature request, please open an [issue](https://github.com/minyang-chen/seat-booking-agent/issues) and make sure it is tagged with `enhancement`.\n\n## License\nThis project is licensed under the MIT License - see the LICENSE file for details.\n\n"
    },
    {
      "name": "knifeyspoony/cogentic",
      "stars": 4,
      "img": "https://avatars.githubusercontent.com/u/7167021?s=40&v=4",
      "owner": "knifeyspoony",
      "repo_name": "cogentic",
      "description": "Hypothesis-based question answering using autogen",
      "homepage": null,
      "language": "Python",
      "created_at": "2025-03-03T16:22:00Z",
      "updated_at": "2025-03-14T03:40:58Z",
      "topics": [],
      "readme": "# Cogentic\n\nCogentic is an AutoGen group chat, based on the [Magentic One](https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/magentic-one.html) orchestration pattern. It differs from the original Magentic One in that it explicitly forces hypothesis creation and testing to answer a question.\n\n## Purpose\n\nManages a group chat between AI agents using a scientific hypothesis-testing approach.\n\n## Key Components\n\n- Uses pydantic models for data validation and serialization\n- Fact management: Tracks facts and evidence during the conversation\n- Plan management: Maintains hypotheses and tests, and re-plans whenever a stall occurs or a test is completed\n\n## Installation\n\n```bash\n$> pip install cogentic\n```\n\n## Usage\n\nTo create a model client for cogentic, you'll need to be familiar with autogen.\n\nSee [AutoGen AgentChat Quickstart](https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/quickstart.html) for details on how to set up your environment and create a model client.\n\nYou can then follow the [sample](./examples/sample.py) to get started with a CogenticGroupChat.\n\n## Development Installation\n\n```bash\n$> git clone https://github.com/knifeyspoony/cogentic.git\n$> cd cogentic\n$> uv sync\n```\n\n\n"
    },
    {
      "name": "lukewu8023/agent-core",
      "stars": 4,
      "img": "https://avatars.githubusercontent.com/u/132513928?s=40&v=4",
      "owner": "lukewu8023",
      "repo_name": "agent-core",
      "description": "Core framework for LLM agent development",
      "homepage": "",
      "language": "Python",
      "created_at": "2024-12-29T18:20:12Z",
      "updated_at": "2025-04-23T00:24:24Z",
      "topics": [],
      "readme": "# [agent-core] Core framework for LLM agent development"
    },
    {
      "name": "aniket-work/Lets-Build-Enterprise-Cybersecurity-Risk-Assessment-Using-AI-Agents",
      "stars": 4,
      "img": "https://avatars.githubusercontent.com/u/59799105?s=40&v=4",
      "owner": "aniket-work",
      "repo_name": "Lets-Build-Enterprise-Cybersecurity-Risk-Assessment-Using-AI-Agents",
      "description": "Lets Build Enterprise Cybersecurity Risk Assessment Using AI Agents",
      "homepage": null,
      "language": "Python",
      "created_at": "2025-02-20T21:33:40Z",
      "updated_at": "2025-04-15T19:55:27Z",
      "topics": [],
      "readme": "# Let's Build Enterprise Cybersecurity Risk Assessment Using AI Agents \n\nCollaborative AI: How Multiple Agents Create Better Security Assessments\n\n## TL;DR\nI built an app that uses multiple AI agents (Security Architect, Risk Analyst, and Compliance Officer) to automatically review security proposals from different perspectives. The agents discuss the proposal together and generate a comprehensive security report. The code shows how to orchestrate multiple agents, manage their conversation, and present their findings through a clean web interface.\n\n## Introduction:\nEver been stuck waiting for the security team to review your project? Or maybe you’re on that security team, drowning in review requests? I built a system that uses AI agents to speed up cybersecurity risk assessments. The agents work together like a real security team — each with their own expertise and perspective — to provide comprehensive reviews in minutes instead of days.\n\n## What’s This Article About?\nThis article walks through building an AI-powered security review system. I’ve created a Streamlit application where users can submit security proposals for analysis. Behind the scenes, a team of specialized AI agents works together to assess the proposal:\n\n - A Security Architect examines technical vulnerabilities and suggests controls\n - A Risk Analyst evaluates business impacts and quantifies potential losses\n - A Compliance Officer checks for regulatory adherence to standards like GDPR and HIPAA\n\nThese agents have a structured conversation, challenge each other’s perspectives, and ultimately produce a comprehensive security recommendation. The application then formats this into a downloadable report that summarizes their findings.\n\nFull Article : [https://medium.com/@learn-simplified/lets-build-enterprise-cybersecurity-risk-assessment-using-ai-agents-6fd2e8be9056\n\n\n## Tech Stack\n\n| **Category**   | **Technology**                | **Purpose**                          |\n|----------------|-------------------------------|--------------------------------------|\n| Framework      | Streamlit                     | Web interface and UI components      |\n| AI             | AutoGen                       | Multi-agent conversation framework   |\n| AI             | OpenAI GPT-4                  | Large language model for agents      |\n| Configuration  | YAML                          | Agent and application configuration  |\n| Configuration  | JSON                          | LLM settings and feature flags       |\n| Environment    | python-dotenv                | Environment variable management      |\n| Utilities      | asyncio                       | Asynchronous processing              |\n| Reporting      | Markdown                      | Report generation format              |\n| Logging        | Python logging                | Application logging                  |\n\n\n\n## Why Read It?\nAI is transforming how businesses handle cybersecurity. According to Gartner, by 2026, organizations using AI in security will respond to incidents 80% faster than those that don’t. This article shows how even fictional companies like our “Enterprise Cyber AI Council” can implement AI agents to:\n\n - Scale security expertise across the organization\n - Standardize risk assessment processes\n - Dramatically reduce review turnaround times\n - Ensure consistent consideration of technical, business, and compliance perspectives\n\nThe approach demonstrated here can be adapted to your own organization’s security frameworks and risk appetite. By building this system, you’ll learn practical techniques for orchestrating AI agents that can be applied to many business processes beyond security.\n\n## Architecture\n\n![Design Diagram](design_docs/design.png)\n\n\n# Tutorial: Let's Build Enterprise Cybersecurity Risk Assessment Using AI Agents\n\n## Prerequisites\n- Python installed on your system.\n- A basic understanding of virtual environments and command-line tools.\n\n## Steps\n\n1. **Virtual Environment Setup:**\n   - Create a dedicated virtual environment for our project:\n   \n     ```bash\n     python -m venv Lets-Build-Enterprise-Cybersecurity-Risk-Assessment-Using-AI-Agents\n     ```\n   - Activate the environment:\n   \n     - Windows:\n       ```bash\n          Lets-Build-Enterprise-Cybersecurity-Risk-Assessment-Using-AI-Agents\\Scripts\\activate        \n       ```\n     - Unix/macOS:\n       ```bash\n       source Lets-Build-Enterprise-Cybersecurity-Risk-Assessment-Using-AI-Agents/bin/activate\n       ```\n   \n\n# Installation and Setup Guide\n\n**Install Project Dependencies:**\n\nFollow these steps to set up and run the  \"Let's Build Enterprise Cybersecurity Risk Assessment Using AI Agents\"\n\n1. Navigate to your project directory:\n   ```\n   cd path/to/your/project\n   ```\n   This ensures you're in the correct location for the subsequent steps.\n\n2. Install the required dependencies:\n   ```\n   pip install -r requirements.txt   \n   ```\n   This command installs all the necessary Python packages listed in the requirements.txt file.\n\n\nRun - Hands-On Guide: Let's Build Enterprise Cybersecurity Risk Assessment Using AI Agents\n\n  \n   ```\n   streamlit run app.py\n   ```\n   \n\n\n"
    },
    {
      "name": "michalmar/dream-team-v2",
      "stars": 4,
      "img": "https://avatars.githubusercontent.com/u/16072588?s=40&v=4",
      "owner": "michalmar",
      "repo_name": "dream-team-v2",
      "description": null,
      "homepage": null,
      "language": "TypeScript",
      "created_at": "2025-02-20T15:28:22Z",
      "updated_at": "2025-03-26T02:38:57Z",
      "topics": [],
      "readme": "[![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat-square)](http://makeapullrequest.com)\n[![LinkedIn](https://img.shields.io/badge/LinkedIn-Connect-blue)](https://www.linkedin.com/in/yaniv-vaknin-7a8324178/)\n\n# Build your dream team with Autogen\n\nThis repository leverages Microsoft Autogen 0.4, Azure OpenAI and integrates it with Streamlit, to build an end to end multi agents application, this repo makes it easy to build test and deploy an advanced multi agent framework, based on [Magentic One](https://www.microsoft.com/en-us/research/articles/magentic-one-a-generalist-multi-agent-system-for-solving-complex-tasks/)\n \n\n![Architecture](assets/architecture.png)\n\n:tada: January 11, 2025: The repo now support [Autogen 0.4.0 stable version](https://microsoft.github.io/autogen/stable/)\n\n:tada: December 3, 2024: The repo now support one click deployment with [Azure Developer CLI](https://learn.microsoft.com/en-us/azure/developer/azure-developer-cli/), if you would like to run it with the full process localy you can check [v0.21](https://github.com/yanivvak/dream-team/tree/v0.21)\n\n:tada: November 18, 2024: we are porting this repo to  [Autogen 0.4](https://microsoft.github.io/autogen/0.4.0.dev6/index.html), A new event driven, asynchronous architecture for AutoGen and [Magentic One](https://github.com/microsoft/autogen/tree/main/python/packages/autogen-magentic-one)\n\n# Key Features\n\nDream Team offers the following key features:\n\n- **Advanced multi agent framework**: this solution is based on the popular framework Autogen(35K stars) and Magentic One\n- **Friendly UI**: easy way to build and share data apps powered by React / Vite.js / Tailwind / Shadcn\n- **Single line deployment**: developer-friendly deployment that accelerates your path from a local development environment to Azure with single line of code - azd up.\n- **Secure code execution**:  Fast access to secure sandboxed with strong isolation environments that are ideal for running code or applications with Azure Container Apps dynamic sessions.\n- **Managed Identities**: Built in Azure Managed identities to eliminate the need for developers to manage these credentials\n- **Observability & Debugging**: Built-in features and tools for tracking, tracing, and debugging agent interactions and workflows, including PromptFlow tracing.\n\n\n# Prerequisites:\n\n1. Install [Azure Developer CLI](https://learn.microsoft.com/en-us/azure/developer/azure-developer-cli/install-azd?tabs=winget-windows%2Cbrew-mac%2Cscript-linux&pivots=os-windows).\n2. Ensure you have access to an Azure subscription\n3. Docker - Follow the [official Docker installation instructions](https://docs.docker.com/get-started/get-docker/) - make sure your docker is loged in (docker login -u \"username\" -p \"password\"\n )\n4. Python version >= 3.10, < 3.13\n5. Install [UV](https://github.com/astral-sh/uv?tab=readme-ov-file#installation) - optional for running locally\n\n\n# Step by step deployment\n   \n## 1. Clone the repository     \n```bash  \ngit clone https://github.com/Azure-Samples/dream-team  \n```\n## 2. Login to your Azure account\n```bash\nazd auth login\n```\n> You need to choose your preferred region (you can start with east us or sweden central or any other available region)\n## 3. Deploy Azure Resources and the app\n```bash\nazd up\n```\n\n# Notes \n\n- while using Web Surfer agent, you might want to change Content Safety on Azure OpenAI to accomodate your needs\n- currently it is \"bring your own AI Search\" (BYOS) - since its assuming you have your own search engine, we are working on a solution to make it easier for you\n   - you must add two ENV variables to backend service to connect to your search engine\n   - `AZURE_SEARCH_SERVICE_ENDPOINT` - your search engine endpoint\n   - `AZURE_SEARCH_ADMIN_KEY` - your search engine key (we are working to enable managed identity for this service)\n \n\n# Working locally  \n\nThere are two parts to this project: the backend and the frontend. The backend is written in Python, and the frontend is written in JavaScript using React.\n\n## Backend\n\n```bash  \ncd backend  \n```\nSet up a virtual environment (Preferred)\n```bash\nuv venv\n```\nOnce you’ve created a virtual environment, you may activate it.\n\nOn Windows, run:\n```bash\n.venv\\Scripts\\activate\n```\nOn Unix or MacOS, run:\n```bash\nsource .venv/bin/activate\n```\nTo deactivate :\n```bash\ndeactivate\n```\n> More information about virtual environments can be found [here](https://docs.python.org/3/tutorial/venv.html)\n\n### Install dependencies\n```bash\nuv sync\nplaywright install --with-deps chromium\n```\n\n### Update configuration\n\n   - If you used AZD to deploy the resources just run the code below\n   ```bash\n   azd env get-values > .env\n   ```` \n   - Alternatively, copy `.env.sample` (under backend) into `.env`\n\n> Important: Magentic-One code uses code execution, you need to have Docker installed to run the examples if you use local execution\n\n### Run\n```bash\nuvicorn main:app --reload\n```\n\n## Frontend\n```bash\ncd frontend\n```\n### Install dependencies\n```bash\nnpm install\n```\n\n> Note: In case you don't have necessary dependencies, you can run: \n```bash\nnpm install -D @types/react @types/react-dom @types/node typescript @vitejs/plugin-react vite\n```\n\n### Update configuration\n\nChange `env.local`\n```bash\nVITE_BASE_URL=http://localhost:8000\nVITE_ACTIVATON_CODE=\nVITE_ALLWAYS_LOGGED_IN=true\n```\n\n## Run\n```bash\nnpm run dev\n```\nIf your app is ready, you can browse to (typically) http://localhost:8501 to see the app in action.\n![Screenshot](./assets/application.png)\n\n# Learn\nCheck these resources:\n1. [Blogpost](https://techcommunity.microsoft.com/blog/Azure-AI-Services-blog/build-your-dream-team-with-autogen/4157961) - Build your dream team with Autogen\n2. [Webinar](https://youtu.be/wB9gD9FkgNA?si=WU3H0QL37RCiTGvl) - More agents is all you need\n"
    },
    {
      "name": "santicalavia/TabooGPT",
      "stars": 4,
      "img": "https://avatars.githubusercontent.com/u/82812961?s=40&v=4",
      "owner": "santicalavia",
      "repo_name": "TabooGPT",
      "description": "Taboo game  made with AutoGen using OpenAI LLM agents.",
      "homepage": null,
      "language": "Python",
      "created_at": "2025-01-26T18:20:37Z",
      "updated_at": "2025-02-11T09:39:06Z",
      "topics": [],
      "readme": "# TabooGPT\nTabooGPT is a Taboo command-line game built with AutoGen using OpenAI LLM agents.\n\nThe goal of the game is to guess the target word based on hints provided by your partner. When giving hints, you must avoid using words from the forbidden words list (including the target word itself, of course!), as well as words that are contained within the forbidden words or are too similar.\n\nIn this version of the game, you'll team up with an LLM partner to compete against a rival team of LLMs.\n\n### Turns\nThe game features three types of turns:\n\n**Player guesses:** You must guess the word based on the hints provided by your LLM partner.\n\n**Player gives hints:** You provide hints, and your LLM partner must guess the word. Be careful not to use any forbidden words!\n\n**CPU turn:** A rival team of LLMs will take their turn.\nYou'll score more points if your team guesses the word with fewer hints.\n\n### Agents\nEach of the described turns works thanks to a series of agents, each fulfilling a specific function:\n\n**HintGenerator:**\nThis agent generates hints for the game using an LLM, avoiding forbidden words or their derivatives. It ensures that previously given hints are not repeated and generates concise clues.\n\n**HintEvaluator:**\nThis agent evaluates the given hint to determine if any forbidden words are used. It uses the evaluate_hint function to validate the hint. This functions is implemented using Spacy to filter main words, combined with Levehnstein distance to evaluate similarity of words, and also checks if a word is contained in the hint.\n\n**GuessGenerator:**\nThis agent listens to the hints provided by another player and generates a guess for the target word using an LLM. It ensures that no previous guesses are repeated and that the response is a single word.\n\n**GuessEvaluator:**\nThis agent checks whether a guess is correct. The guess must exactly match or mean the same as the target word. It can only respond with \"ACIERTO\" (correct) or \"NOK\" (incorrect).\n\n**Player:**\nThis agent represents the human player in the game. It acts as an intermediary between the user and the system, allowing the player to interact with the AI agents.\n\n\n\n\n\n## Installation\nFirst, clone the repository and go to the repository folder:\n\n```bash\ngit clone https://github.com/santicalavia/TabooGPT.git\ncd TabooGPT\n```\n\nThis project was tested using Python 3.10. Create an environment with this Python version and install requirements.txt\nIf you are using Anaconda/Miniconda, you can run this command:\n```bash\n  conda create -n taboogpt python=3.10\n  activate taboogpt\n```\nInstall the requirements:\n```bash\n  pip install -r requirements.txt\n```\n\nThen, download Spanish language model for Spacy:\n\n```bash\n  python -m spacy download es_core_news_sm \n```\n\nFinally, create a .env file with the following keys : \n\n```bash\n  'OPENAI_API_KEY' = 'put-your-api-key-here'\n  'AUTOGEN_USE_DOCKER' = 'False'\n```\nAnd put the .env file in TabooGPT folder.\n\n## Usage\nTo play the game, execute main.py from your new python environment:\n```bash\npython main.py\n```\nYou can use the following arguments to customize the configuration. However, the default settings are sufficient for a standard game.\n```bash\n  -h, --help            show this help message and exit\n  -p CARDS_PATH, --cards_path CARDS_PATH\n                        Path for cards.csv file\n  -m MODEL, --model MODEL\n                        OpenAI model to use\n  -r ROUNDS, --rounds ROUNDS\n                        Number of rounds (each round consists of 4 turns: 2 by the player and 2 by the CPU)\n  -c CARDS_PER_TURN, --cards_per_turn CARDS_PER_TURN\n                        Number of cards per turn \n```\n\nThe different turns will proceed consecutively. \nIn the user's interactions, you can write some special words to perform some actions:\n- To skip to the next card, write \"PASO\". \n- To exit the game, type \"SALIR\".\n\nEnjoy!"
    },
    {
      "name": "guyq1997/Jupyter-Assistant",
      "stars": 4,
      "img": "https://avatars.githubusercontent.com/u/111009031?s=40&v=4",
      "owner": "guyq1997",
      "repo_name": "Jupyter-Assistant",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2025-01-23T19:40:13Z",
      "updated_at": "2025-04-11T16:58:12Z",
      "topics": [],
      "readme": "﻿# Jupyter-Assistant\r\nA web editor inspired by Cursor that uses large language models (LLM) to enhance Jupyter Notebook assignments.\r\n\r\n## Demo Video\r\n\r\n[![Jupyter-Assistant Demo](https://img.youtube.com/vi/WVLLtWDX8tE/0.jpg)](https://www.youtube.com/watch?v=WVLLtWDX8tE)\r\n\r\nClick the image above to watch a demonstration of Jupyter-Assistant in action.\r\n\r\n## Challenges with Editing Jupyter Notebooks\r\nDeveloping projects like web applications requires managing numerous scripts, typically using IDEs for efficiency. While Cursor leverages LLMs for code generation, it struggles with Jupyter Notebooks because they are long JSON files, making AI-driven editing error-prone and inefficient.\r\n\r\nIn educational and research settings, step-by-step code comprehension and real-time feedback are crucial, making Jupyter Notebooks the preferred tool. To overcome Cursor's limitations, Jupyter-Assistant is designed to facilitate easy cell-by-cell modifications using LLMs, allowing for seamless content generation and copy-pasting without manual repetition.\r\n\r\n## Jupyter Editor Introduction\r\n\r\n- **Text Editor Interface**:\r\n  - Similar to editors like Atom\r\n  - Edits local Jupyter Notebook content without executing code\r\n  - Code execution available via Jupyter Lab\r\n\r\n- **Cell Selection and Context**:\r\n  - Directly select Jupyter Notebook cells as context for the model\r\n  - Input requirements for the large language model to generate content\r\n\r\n- **User-Friendly Modifications**:\r\n  - Apply changes with a single click\r\n  - Facilitates prompt-oriented programming, akin to Cursor's chat mode\r\n\r\n- **Enhanced Capabilities**:\r\n  - Integrates an agent instead of a standard LLM\r\n  - Utilizes tools for:\r\n    - Searching notebook content\r\n    - Editing cells\r\n    - Performing web searches and web scraping\r\n\r\n- **Intelligent Task Management**:\r\n  - Features a scheduling mechanism that operates in a loop\r\n  - Handles user requests by using appropriate tools\r\n  - Progresses step-by-step, continuing based on current context until the task is completed or cannot proceed\r\n  \r\n## Docker Deployment\r\n\r\n### Prerequisites\r\n- Docker installed on your system\r\n- Git to clone the repository\r\n\r\n### Quick Start\r\n1. Clone the repository:\r\n   ```bash\r\n   git clone https://github.com/guyq1997/Jupyter-Assistant.git\r\n   cd Jupyter-Assistant\r\n   ```\r\n\r\n2. Build the Docker image:\r\n   ```bash\r\n   docker build -t notebook-assistant .\r\n   ```\r\n\r\n3. Run the container:\r\n   ```bash\r\n   docker run -p 8765:8765 notebook-assistant --OPENAI_API_KEY=<your-api-key>\r\n   ```\r\n\r\n4. Access the application:\r\n   - Open your web browser and navigate to `http://localhost:8765`\r\n   - The WebSocket endpoint will be available at `ws://localhost:8765/ws`\r\n\r\n## License\r\n\r\nThis project is licensed under the MIT License - see the LICENSE file for details.\r\n\r\n"
    },
    {
      "name": "kingjulio8238/Oceans7",
      "stars": 4,
      "img": "https://avatars.githubusercontent.com/u/120517860?s=40&v=4",
      "owner": "kingjulio8238",
      "repo_name": "Oceans7",
      "description": "Use many small models to jailbreak a SOTA model ",
      "homepage": null,
      "language": "Python",
      "created_at": "2024-11-02T19:07:17Z",
      "updated_at": "2025-01-12T18:29:45Z",
      "topics": [],
      "readme": "# Oceans7\nA group of agents powered by small models that collaborate to pursuade a large SOTA model. \n\n<p align=\"center\">\n  <img alt=\"memary_logo\" src=\"https://github.com/kingjulio8238/Oceans7/blob/main/small%20llamas%20herd.png?raw=true\">\n</p>\n\n"
    },
    {
      "name": "tituslhy/literate-octo-tribble",
      "stars": 4,
      "img": "https://avatars.githubusercontent.com/u/7207877?s=40&v=4",
      "owner": "tituslhy",
      "repo_name": "literate-octo-tribble",
      "description": "Basic LLM recipes for a happy life",
      "homepage": "",
      "language": "Jupyter Notebook",
      "created_at": "2024-09-15T00:19:05Z",
      "updated_at": "2025-01-29T14:20:29Z",
      "topics": [],
      "readme": "# Welcome to an exploration of the LLM Multiverse\nThis little GitHub repository aims to provide very basic cookbook recipes for LLMs. \n\n<p align=\"center\">\n    <img src=\"./images/llama_crew.jpg\">\n</p>\n\nThe aim is to explore the 'multiverse' of LLM land where there are many different tools that handle similar tasks (perfect competitors), each with its own pros and cons. Feel free to request for an article!\n\n## Roadmap\n1. ~Basic Langchain Recipes for LLMs (up to creating a single agent)~ - Done!\n2. ~Basic Haystack Recipe for LLMs (up to creating a single agent)~ - Done!\n3. ~Basic LlamaIndex Recipe for LLMs (up to creating a single agent)~ - Done!\n4. ~Basic Langgraph Multi Agent Recipes (building a personal assistant)~ - Done!\n5. ~Basic Autogen Multi Agent Recipes (building a stock assistant)~ - Done!\n6. ~Basic LlamaIndex Multi Agent Recipes~ - Done!\n7. ~Basic crewAI Mult Agent Recipes~ - Done!\n\n## Related articles\n1. [A gentle introduction to the LLM Multiverse (Part 1): Langchain](https://medium.com/@tituslhy/a-gentle-introduction-to-the-llm-multiverse-part-1-langchain-023a899d294e)\n2. [A gentle introduction to the LLM Multiverse (Part 2): Haystack](https://medium.com/mitb-for-all/a-gentle-introduction-to-the-llm-multiverse-part-2-haystack-c6af2548df04)\n3. [A gentle introduction to the LLM Multiverse (Part 3): LlamaIndex](https://medium.com/mitb-for-all/a-gentle-introduction-to-the-llm-multiverse-part-3-llamaindex-798344050c49)\n4. [A gentle introduction to the LLM Multi-Agents Multiverse (Part 1): Langgraph](https://medium.com/@tituslhy/a-gentle-introduction-to-the-llm-multi-agents-multiverse-part-1-langgraph-2ac56f1b5b3c)\n5. [A gentle introduction to the LLM Multi-Agent Multiverse (Part 2): Autogen](https://medium.com/mitb-for-all/a-gentle-introduction-to-the-llm-multi-agent-multiverse-part-2-autogen-5401a0075d75)\n6. [A gentle introduction to the LLM Multi-Agent Multiverse (Part 3a): LlamaIndex Workflows](https://medium.com/mitb-for-all/a-gentle-introduction-to-the-llm-multi-agent-multiverse-part-3a-llamaindex-workflows-c0f614c15b88)\n7. [A gentle introduction to the LLM Multi-Agent Multiverse (Part 3b): Deploying LlamaIndex Workflows](https://medium.com/@tituslhy/a-gentle-introduction-to-the-llm-multi-agent-multiverse-part-3b-deploying-llamaindex-workflows-df18381d36b9)\n8. [A gentle introduction to the LLM Multi-Agent Multiverse (Part 4): CrewAI](https://medium.com/mitb-for-all/a-gentle-introduction-to-the-llm-multi-agent-multiverse-part-4-crewai-147ada6db54c)\n\n\n## Repository Layout\nNote that each folder has its own requirements.txt! This is in the event of version conflicts - for example LlamaIndex and Langchain have moved to pydantic v2 but not all the LLM libraries have done the same!\n```\n.\n├── Autogen\n│   ├── notebooks\n│       ├── stock_analysis.ipynb          <- Code book for autogen stock analyst app\n│       ├── blogs                         <- Python scripts generated by Autogen's command line executor\n│       ├── report.md                     <- Final report generated\n│   ├── requirements.txt                  <- Requirements.txt for Autogen code\n│   ├── AutogenStudio                     <- Snapshots of AutogenStudio executions\n├── CrewAI\n│   ├── notebooks\n│       ├── basics.ipynb                  <- Code book for CrewAI Employment Hero Multi Agent System\n│   ├── pyproject.toml                    <- Requirements for CrewAI code\n│   ├── poetry.lock                       <- Poetry lock file for Autogen code\n│   ├── Config                            <- Yaml file configurations of tasks and agents\n├── Haystack                                \n│   ├── notebooks\n│       ├── the_basics.ipynb              <- Basic recipes for Haystack (up to RAG)\n│   ├── requirements.txt                  <- Requirements.txt for Haystack recipe\n├── Langchain\n│   ├── notebooks\n│       ├── the_basics.ipynb              <- Basic recipes for Langchain (up to single agents)\n│       ├── langgraph.ipynb               <- Code book for Langgraph personal assistant app\n│       ├── langgraph_studio              <- Files for loading into the Langgraph Studio software\n│   ├── requirements.txt                  <- Requirements.txt for LangChain and Langgraph codes\n├── LlamaIndex\n│   ├── App      \n│       ├── backend                       <- Llama Deploy backend\n│       ├── frontend                      <- Chainlit frontend for self discovery app\n│   ├── notebooks\n│       ├── 1. the_basics.ipynb           <- Basic recipes for LlamaIndex (up to single agents)\n│       ├── 2. workflow.ipynb             <- Basic recipes for LlamaIndex Workflows\n│       ├── 3. deployment.ipynb           <- Basic recipes for LlamaIndex Workflow HITL and deployment\n│   ├── requirements.txt                  <- Requirements.txt for LlamaIndex recipes\n├── data                                  <- Folder containing data sets for recipes\n│       ├── paul_graham                   \n│           |── paul_graham_essay.txt     <- txt file for RAG\n```"
    },
    {
      "name": "realtmxi/mag-review",
      "stars": 3,
      "img": "https://avatars.githubusercontent.com/u/42812404?s=40&v=4",
      "owner": "realtmxi",
      "repo_name": "mag-review",
      "description": "Multi Agent Acedemic Reviewer",
      "homepage": null,
      "language": "Python",
      "created_at": "2025-02-27T14:54:36Z",
      "updated_at": "2025-04-22T23:59:19Z",
      "topics": [],
      "readme": "#  Multi-Agent Research Assistant\n\nA modular research assistant built with **AutoGen AgentChat**, **Chainlit UI**, and **GitHub-hosted LLMs** on Azure Inference.\n\nIt simulates a full AI research pipeline:\n- 📚 **Literature Agent** – searches papers from arXiv & web\n- 🧾 **Paper Review Agent** – summarizes, visualizes, and enhances PDFs\n- ❓ **Q&A Agent** – answers follow-up questions from reviewed content\n\n---\n\n## 🚀 Features\n\n- 🔍 Dynamic academic search with arXiv + DuckDuckGo\n- 📄 Multi-mode PDF review: \"rapid\", \"academic\", \"visual\", \"enhanced\"\n- 🧠 Q&A powered by stored context and external search\n- 🗂 Modular agent architecture with AutoGen + Chainlit\n- 🧩 Pluggable LLMs: `gpt-4o`, `LLaMA`, `Mistral`, or custom deployments\n\n---\n\n## 🛠️ Tech Stack\n\n| Layer | Tool |\n|-------|------|\n| Agents | `autogen-agentchat` |\n| Tools  | `autogen-core`, `FunctionTool` |\n| LLMs   | GitHub-hosted models on Azure Inference |\n| Frontend | `Chainlit` for conversational UI |\n| PDF & Web | `PyMuPDF`, `duckduckgo-search`, `matplotlib` |\n\n---\n\n## 📂 Project Structure\n\n```\nproject_root/\n├── agents/\n│   ├── literature_agent.py\n│   ├── paper_review_agent.py\n│   └── qa_agent.py\n├── tools/\n│   ├── review_tools.py\n│   └── qa_tools.py\n├── orchestrator/\n│   └── multi_agent_router.py\n├── .env\n├── app.py (Chainlit entry)\n├── requirements.txt\n├── environment.yml\n└── README.md\n```\n\n---\n\n## ⚙️ Setup\n\n### 📦 Option 1: Conda Environment (Recommended)\n\n```bash\nconda env create -f environment.yml\nconda activate agent\n```\n\n### 💡 Option 2: Pip Virtual Environment\n\n```bash\npython -m venv venv\nsource venv/bin/activate  # Windows: .\\venv\\Scripts\\activate\npip install -r requirements.txt\n```\n\n---\n\n## 🔐 Configure Environment\nCreate a `.env` file:\n```env\nGITHUB_TOKEN=ghp_XXXXXXXXXXXXXXXXXXXXX\n```\n\n---\n\n##  Run the Agent System\n\n```bash\nchainlit run app.py\n```\nThen open [http://localhost:8000](http://localhost:8000)\n\n---\n\n## Example Queries\n\n- \"Search for top papers on temporal graph neural networks.\"\n- \"Review this PDF in enhanced mode.\"( to be implemented)\n- \"Give me a visual summary of this paper.\"\n- \"What is a temporal point process?\"\n\n---\n\n## Limitations\n\n- Azure Inference requires correct PAT and model permissions\n- Some models do not support auto tool calling (manual fix required)\n- Large PDFs are chunked to avoid context overflows\n\n---\n\n## Credits\n\n- Microsoft AutoGen\n- GitHub Models on Azure Inference\n- Chainlit.io\n\n---\n\n## 📜 License\n\nMIT License. Use freely, modify creatively, contribute collaboratively.\n\n---\n\n## 🤖 Future Work\n\n- [ ] Add memory and persistent context\n- [ ] Integrate PubMed, Semantic Scholar APIs\n- [ ] Stream output for long responses\n- [ ] Full PDF upload pipeline via Chainlit"
    },
    {
      "name": "Azure-Samples/AzureAIFoundryWorkshop",
      "stars": 3,
      "img": "https://avatars.githubusercontent.com/u/1844662?s=40&v=4",
      "owner": "Azure-Samples",
      "repo_name": "AzureAIFoundryWorkshop",
      "description": "Configuring Azure OpenAI and leveraging AI Foundry basic features (Korean)",
      "homepage": "",
      "language": "Jupyter Notebook",
      "created_at": "2025-04-05T12:29:07Z",
      "updated_at": "2025-04-10T06:07:09Z",
      "topics": [
        "aifoundry",
        "azure",
        "workshop"
      ],
      "readme": "# Azure AI Foundry Workshop\n\n<div align=\"center\">\n\n[📦Prerequisites](#-prerequisites) | [🚀Quick Start](#-quick-start) | [🤖Overview](#-overview) | [📔Workshop Content](#-workshop-content) | [🧩Project Structure](#-project-structure) | [❓Support](#-support) | [🤝Contributing](#-contributing)\n\n</div>\n\n\n## 🤖 Overview\n\n건강 및 식단 조언과 관련된 재미있는 예제를 통해 Azure AI Foundry를 기반으로 지능형 앱 및 AI 에이전트를 빌드하는 방법을 안내하는 실습 워크샵입니다. 다음의 내용을 포함합니다.\n- Azure AI Foundry 기본 사항 알아보기\n- 인증 및 프로젝트 구성 설정\n- AI 모델 배포 및 테스트\n- AI 에이전트 빌드(건강 어드바이저 예제)\n- 건강 계산 및 식단 계획 구현\n- 에이전트 성능 평가 및 품질 속성 모니터링\n- 모든 기능과 디자인 패턴이 통합된 엔드투엔드 AI 네이티브 샘플 앱 배포\n\n> **소요 예상 시간**: 4-5 시간  \n> **Focus**: 실습, 대화형 노트북, 실용적인 예제, 엔드투엔드 프로젝트\n\n## 🎥 Workshop Overview Video\n\n워크숍에 대한 포괄적인 이해를 돕기 위해 개요 동영상을 시청하세요:\n\n[![Azure AI Foundry Workshop Overview](https://img.youtube.com/vi/0bGLgmZJ6DE/0.jpg)](https://youtu.be/0bGLgmZJ6DE)\n\n## 📸 Visuals\n\n### Sections Overview\n<img src=\"./3-ai-native-e2e-sample/assets/sections.png\" width=\"70%\" />\n*Figure 1: Overview of different sections in this workshop.*\n\n### End-to-End Sample UI\n<img src=\"./3-ai-native-e2e-sample/assets/ui.png\" width=\"70%\" />\n*Figure 2: User interface of the end-to-end AI native sample project.*\n\n---\n\n## 📦 사전 준비 사항    \n\n워크샵을 시작하기 전에 다음이 설치되어 있는지 확인하세요:\n\n- [Python 3.10](https://www.python.org/downloads/) 이상 설치\n- [Azure AI Foundry](https://ai.azure.com) 에 대한 액세스 권한이 있는 활성 Azure 구독\n- [Azure CLI](https://learn.microsoft.com/en-us/cli/azure/install-azure-cli) 설치\n- [Git](https://git-scm.com/downloads) 설치\n- [VS Code](https://code.visualstudio.com/), [GitHub Codespaces](https://github.com/features/codespaces), 또는 [Jupyter Notebook](https://jupyter.org/install) 환경\n- 기본 Python 프로그래밍 지식\n- Azure AI Foundry에서 모델 배포 및 [AI Search](https://learn.microsoft.com/en-us/azure/search/search-what-is-azure-search) 연결 구성\n\n---\n\n## 🚀 빠른 시작\n\n1. **리포지토리 복제**:\n   ```bash\n   git clone https://github.com/Azure/ai-foundry-workshop.git\n   cd ai-foundry-workshop\n   ```\n\n2. **uv 설치**:\n   ```bash\n   # Unix/Linux/macOS\n   curl -LsSf https://astral.sh/uv/install.sh | sh\n\n   # Windows (PowerShell)\n   (Invoke-WebRequest -Uri https://astral.sh/uv/install.ps1 -UseBasicParsing).Content | pwsh\n   ```\n\n3. **가상 환경 생성 및 활성화**:\n   ```bash\n   uv venv\n   source .venv/bin/activate  # Windows: .venv\\Scripts\\activate\n   ```\n\n4. **Azure AI Foundry 설정**:\n\n   a. **프로젝트 생성 및 리소스 배포**:\n      1. [Azure AI Foundry](https://ai.azure.com)로 이동\n      2. AI 파운드리 마법사를 사용하여 새 AI 허브 및 프로젝트 만들기\n      3. 필요한 모델 배포:\n         - Chat/Completion을 위한 GPT 모델(gpt-4o, gpt-4o-mini) (Agent 노트북에서 발생할 수 있는 이슈를 방지하기 위해 **TPM을 최대**로 설정)\n         - 벡터 검색을 위한 임베딩 모델\n         - 모델이 `Global-Standard` 또는 `DataZone-Standard` 에 배포되었는지 확인하세요.\n      4. 연결을 설정합니다:\n         - [Grounding with Bing](https://learn.microsoft.com/en-us/azure/ai-services/agents/how-to/tools/bing-grounding?view=azure-python-preview&tabs=python&pivots=overview) 연결 설정\n         - Azure AI 검색 연결 구성\n      5. Azure AI 파운드리 관리 포털에서 `Azure AI Developer` 역할에 사용자 계정을 추가합니다.\n\n   b. **환경 변수 구성**:\n      ```bash\n      cp .env.example .env\n      ```\n      Azure AI Foundry 값으로 `.env`을 업데이트합니다:\n      - `PROJECT_CONNECTION_STRING`: Azure ML 워크스페이스의 프로젝트 연결 문자열\n      - `MODEL_DEPLOYMENT_NAME`: 모델 배포 이름\n      - `EMBEDDING_MODEL_DEPLOYMENT_NAME`: 임베딩 모델 배포 이름\n      - `TENANT_ID`: Azure 포털에서 확인한 테넌트 ID\n      - `BING_CONNECTION_NAME`: Bing 검색 연결 이름\n      - `SERVERLESS_MODEL_NAME`: 서버리스 모델 이름\n\n      > **Note**: `MODEL_DEPLOYMENT_NAME`에 지정된 모델은 Azure AI 에이전트 서비스 또는 어시스턴트 API에서 지원해야 합니다. 자세한 내용은 [지원되는 모델](https://learn.microsoft.com/en-us/azure/ai-services/agents/concepts/model-region-support?tabs=python#azure-openai-models)을 참조하세요. Bing Search를 사용한 그라운딩의 경우 `gpt-4o-mini` 모델을 사용해야 합니다.\n\n5. **종속 요소 설치**:\n   ```bash\n   # Core Azure AI SDK 및 Jupyter 필요 사항 설치\n   uv pip install azure-identity azure-ai-projects azure-ai-inference[opentelemetry] azure-search-documents azure-ai-evaluation azure-monitor-opentelemetry\n\n   # Jupyter 필요사항 설치\n   uv pip install ipykernel jupyterlab notebook\n\n   # Jupyter에 커널 등록하기\n   python -m ipykernel install --user --name=.venv --display-name=\"Python (.venv)\"\n\n   # 추가 필요사항 설치 (optional - for deploying repo or running mkdocs)\n   uv pip install -r requirements.txt\n   ```\n\n   > **Note**: VS Code에서 커널 오류가 발생하는 경우, 다음을 시도해보자:\n   > 1. 커널을 선택합니다: \"Select Kernel\" > \"Python Environments\" > \"Python (.venv)\" 을 클릭합니다.\n   > 2. 커널이 목록에 없으면, `python -m ipykernel install --user --name=.venv` 을 다시 실행하거나, VS Code의 “새 커널 만들기” 마법사를 사용하여 새 Python 환경을 생성합니다.\n   > 3. 필요한 경우 VS Code를 다시 로드합니다.\n\n6. **노트북 환경 선택**:\n\n   **옵션 A: VS Code**\n   - [VS Code Python extension](https://marketplace.visualstudio.com/items?itemName=ms-python.python) 설치\n   - 둘 중 하나를 설치합니다.:\n     - .ipynb 파일용 [Jupyter extension](https://marketplace.visualstudio.com/items?itemName=ms-toolsai.jupyter) \n     - .dib 파일용 [Polyglot Notebooks extension](https://marketplace.visualstudio.com/items?itemName=ms-dotnettools.dotnet-interactive-vscode) \n   - 아무 notebook이나 열고 Python 커널(.venv)을 선택합니다.\n\n   **옵션 B: GitHub Codespaces**\n   - 리포지토리에서 \"Code\" > \"Create codespace\" 을 클릭합니다.\n   - 환경이 설정될 때까지 기다립니다.\n   - Notebooks를 실행할 준비가 됩니다.\n\n   **옵션 C: Jupyter Lab/Notebook**\n   ```bash\n   # Install Jupyter if you haven't already\n   uv pip install jupyterlab notebook\n\n   # Start Jupyter Lab (recommended)\n   jupyter lab\n\n   # Or start Jupyter Notebook\n   jupyter notebook\n   ```\n\n7. **다음 과정을 차례대로 수행하세요**:\n    1. **시작하기** (`1-introduction/`)\n       - `1-authentication.ipynb`: SAzure 자격증명 설정\n       - `2-environment_setup.ipynb`: 환경 구성\n       - `3-quick_start.ipynb`: 기본 작업 알아보기\n         \n    2. **메인 워크샵** (`2-notebooks/`)\n       - 채팅 완료 및 RAG (`1-chat_completion/`)\n       - 에이전트 개발 (`2-agent_service/`)\n       - Quality Attributes (`3-quality_attributes/`)\n\n---\n\n## 📔 워크샵 학습 경로\n\n워크샵을 완료하려면 이 노트북을 순서대로 따라하세요:\n\n### 1. 시작하기 (`1-introduction/`)\n| Notebook | Description |\n|----------|-------------|\n| [1. Authentication](1-introduction/1-authentication.ipynb) | Azure 자격 증명 및 액세스 설정 |\n| [2. Environment Setup](1-introduction/2-environment_setup.ipynb) | 개발 환경 구성 |\n| [3. Quick Start](1-introduction/3-quick_start.ipynb) | 기본 Azure AI Foundry 작업 알아보기 |\n\n### 2. 메인 워크샵 (`2-notebooks/`)\n| Topic | Notebooks |\n|-------|-----------|\n| **채팅 완료 및 RAG** | • [Chat Completion & RaG](2-notebooks/1-chat_completion/) |\n| **에이전트 개발** | • [Agent Development](2-notebooks/2-agent_service/) |\n| **Quality Attributes** | • [Observability & Evaluations](2-notebooks/3-quality_attributes/) |\n\n---\n\n## 🤝 Contributing\n\nWe welcome contributions! Please see our [Contributing Guide](CONTRIBUTING.md) for details on how to:\n- Submit bug reports and feature requests\n- Submit pull requests\n- Follow our coding standards\n- Participate in code reviews\n\n---\n\n## ❓ Support\n\nIf you need help or have questions:\n\n---\n\n\n\n\n"
    },
    {
      "name": "epoikos-project/simulation",
      "stars": 3,
      "img": "https://avatars.githubusercontent.com/u/200770314?s=40&v=4",
      "owner": "epoikos-project",
      "repo_name": "simulation",
      "description": null,
      "homepage": "https://epoikos-project.github.io/docs/",
      "language": "Python",
      "created_at": "2025-03-28T20:52:01Z",
      "updated_at": "2025-04-22T23:04:45Z",
      "topics": [
        "agents",
        "ai",
        "llms",
        "simulation"
      ],
      "readme": "# EpOikoS Simulation\n\nThis project runs a [FastAPI](https://fastapi.tiangolo.com) Webserver as well as the [Faststream Plugin](https://faststream.airt.ai/latest/getting-started/integrations/fastapi/) to integrate with [NATS](https://nats.io), a highly performant messaging system.\n\n## Installation\n\n### Clone the GitHub Repo\n\nClone the repository to your local machine using the following command:\n```shell\ngit clone https://github.com/epoikos-project/simulation epoikos-simulation\ncd epoikos-simulation\n```\n\n### Prerequisites\n\nMake sure to have NATS installed. You can follow the [installation guide](https://docs.nats.io/running-a-nats-service/introduction/installation). Run the NATS server with the config located in `config/nats-server.conf` to enable the jetstream storage:\n```shell\nnats-server -c config/nats-server.conf\n```\n### Python Setup\n\nInstall Packages using [uv](https://docs.astral.sh/uv/getting-started/installation/).\n```shell\nuv sync\n```\n\nEnable precommit hooks:\n```shell\nuv run pre-commit\n```\n\nThis will ensure that you only push formatted code to github. _Be aware that commits may fail at the first attempt_.\n\n### Environment Variables\nCopy the `.env.example` file to `.env` and update the values as needed.\n\n```shell\ncp .env.example .env\n```\n\n> [!IMPORTANT] \n> Make sure to update the envs to your local setup! Particularly the OPENAI_API_KEY.\n\n\n```conf\n# Replace in .env\nOPENAI_APIKEY=<your-key>\n```\n\n## Running the Application\nStart the application using the following command:\n```shell\nuv run fastapi dev\n```\nAlternatively, if the above does not work run it using uvicorn:\n```shell\nuv run uvicorn main:app --port 8000 --reload\n```\n\nOnce the application is running, you can access the API documentation is available at [`http://localhost:8000/docs`](http://localhost:8000/docs).\n\n## File Structure\n\n```\n.\n├── README.md\n├── clients\n│   ├── milvus.py\n│   ├── nats.py\n│   └── tinydb.py\n├── config\n│   ├── base.py\n│   ├── milvus.py\n│   ├── nats-server.conf\n│   ├── nats.py\n│   └── tinydb.py\n├── data\n│   ├── jetstream\n│   ├── milvus\n│   └── tinydb\n├── main.py\n├── messages\n│   ├── agent\n│   ├── world\n│   └── message_base.py\n├── models\n│   ├── agent.py\n│   └── simulation.py\n├── routers\n│   ├── agent.py\n│   ├── debug.py\n│   ├── simulation.py\n│   └── world.py\n├── subscribers\n│   └── world.py\n└── uv.lock\n```\n\n- `clients/`: Contains the clients for the different services used in the application; in our case, there are three clients. For more details, see the [Clients README](clients/README.md).\n  - `milvus.py`: Contains the Milvus client.\n  - `nats.py`: Contains the NATS client.\n  - `tinydb.py`: Contains the TinyDB client.\n- `config/`: Contains the configuration files. It uses [pydantic-settings](https://docs.pydantic.dev/latest/concepts/pydantic_settings/) to manage the settings. \n- `data/`: The data directory will only be created once the application is running. It contains the data storage for the application.\n- `messages/`: This folder is used for an aligned message protocol to be used on the EventStream. It uses [pydantic](https://docs.pydantic.dev/latest/) to define the messages. You can read more about messages in the [Messages README](messages/README.md).\n- `models/`: Contains the logic of our entities in the application. Mainly\n  - `agent.py`: Contains the Agent model.\n  - `simulation.py`: Contains the Simulation model.\n- `routers/`: Contains the FastAPI routers.\n- `subscribers/`: Contains the NATS subscribers.\n- `main.py`: The main entrypoint for the FastAPI application."
    },
    {
      "name": "mdsiprojects/agenticai",
      "stars": 3,
      "img": "https://avatars.githubusercontent.com/u/73212683?s=40&v=4",
      "owner": "mdsiprojects",
      "repo_name": "agenticai",
      "description": null,
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2025-03-10T07:33:37Z",
      "updated_at": "2025-04-16T06:32:38Z",
      "topics": [],
      "readme": "# AI Agents\n\nAndrew Ng on Agentic (June 2024):\n[![Watch the video](https://img.youtube.com/vi/q1XFm21I-VQ/maxresdefault.jpg)](https://youtu.be/q1XFm21I-VQ)\n\n### [Watch this video on YouTube](https://youtu.be/q1XFm21I-VQ)\n\n### Presentation file: [AI Agents](/UTSMDSI-AI%20Agents-07.04.2025.pdf)\n\nRetrieval Augmented Generation: build your own RAG with Streamlit and LLMs: https://github.com/mdsiprojects/llm\n\nAgentic Design Patterns:\n- Intro: https://www.deeplearning.ai/the-batch/how-agents-can-improve-llm-performance/\n- Reflection: https://www.deeplearning.ai/the-batch/agentic-design-patterns-part-2-reflection/\n- Tool Use: https://www.deeplearning.ai/the-batch/agentic-design-patterns-part-3-tool-use/\n- Planning: https://www.deeplearning.ai/the-batch/agentic-design-patterns-part-4-planning/\n- Multi Agent Collaboration: https://www.deeplearning.ai/the-batch/agentic-design-patterns-part-5-multi-agent-collaboration/\n\n\n"
    },
    {
      "name": "DoctorKhan/quantum-resonance-ledger",
      "stars": 3,
      "img": "https://avatars.githubusercontent.com/u/7505793?s=40&v=4",
      "owner": "DoctorKhan",
      "repo_name": "quantum-resonance-ledger",
      "description": "A physics-inspired framework for highly scalable and adaptable distributed systems.",
      "homepage": null,
      "language": "Go",
      "created_at": "2025-03-28T01:52:52Z",
      "updated_at": "2025-04-22T06:00:31Z",
      "topics": [],
      "readme": "Read the [white paper](docs/whitepaper.md).\n\n# Quantum Resonance Ledger (QRL)\n\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n<!-- Optional: Add build status, coverage badges etc. here -->\n\n**A conceptual framework and simulation for a physics-inspired distributed system designed for extreme scalability and adaptability.**\n\nThis repository contains the research implementation and simulation related to the **Quantum Resonance Ledger (QRL)**, a novel framework detailed in the [technical yellowpaper](docs/yellowpaper.md). QRL evolves beyond traditional blockchain limitations by employing principles from statistical mechanics, wave mechanics, and field theory.\n\n## Core QRL Concepts & Features\n\nQRL introduces several key innovations:\n\n*   **Probabilistic Quantity Conservation:** Relaxes strict transaction ordering, enabling parallel processing for enhanced scalability, while ensuring probabilistic conservation of token quantities.\n*   **Laplacian ($\\nabla^2$) & D’Alembertian ($\\square$) Correction:** Utilizes physics-inspired field-theoretic operators ($\\nabla^2$ for smoothing, $\\square$ for spacetime propagation effects) to dynamically enforce and correct quantity imbalances across the network, maintaining ledger integrity probabilistically.\n*   **Bounded Parameter Management:** Models key network parameters (e.g., block size, fees) using probabilistic \"wavefunction\" envelopes ($\\psi$), allowing for dynamic adaptation within defined bounds.\n*   **Hamiltonian Optimization:** Employs a Hamiltonian ($H$) cost function to represent the network's state \"cost,\" driving dynamic parameter adjustments towards optimal configurations balancing performance, security, and stability.\n*   **Quantum-Inspired Uncertainty Relations:** Formalizes inherent trade-offs between network properties (e.g., scalability vs. reliability), guiding balanced optimization.\n*   **Cryptographic Uniqueness Tokens (CUTs):** Implements classically secure tokens guaranteeing uniqueness, providing the cryptographic foundation needed to trust probabilistic quantity conservation and prevent double-spending.\n*   **Path Integral / Probabilistic Consensus:** Explores consensus mechanisms that statistically favor optimal chain histories (\"paths\" with lower \"action\"), enabling faster probabilistic finality.\n\n**Disclaimer:** This repository contains a research implementation and simulation environment for exploring QRL concepts. It is **not** a production-ready blockchain.\n\n## Project Structure\n\n```shell\nqrl/\n├── python/                # Python-related code\n│   ├── src/                # Python source code\n│   │   └── quantum_blockchain.py\n│   ├── test/               # Python test files\n│   │   └── test_quantum_blockchain_pytest.py\n│   └── requirements.txt    # Python dependencies\n├── go/                    # Go-related code\n│   ├── cmd/                # Go command-line applications (simulations, examples)\n│   │   ├── simulation/     # Main simulation application in Go\n│   │   └── ...             # Other potential Go applications or examples\n│   ├── internal/           # Go internal packages (used by cmd/, not for external use)\n│   ├── pkg/                # Go packages intended for potential reuse (e.g., simulation core)\n│   ├── go.mod              # Go module definition file\n│   └── go.sum              # Go dependencies file\n├── docs/                   # Documentation files (Markdown, including Whitepaper)\n│   └── qrl_whitepaper.md   # The main QRL Whitepaper\n│   └── images/             # Images used in documentation\n├── scripts/                # Utility scripts (testing, building, etc.)\n├── LICENSE                 # License file\n├── README.md               # This file\n└── run                     # Script to run tests\n```\n\n*   **Go (`go/`):** The primary language for the simulation environment and command-line tools, leveraging Go's strengths in concurrency and systems programming.\n*   **Python (`python/`):** Used for prototyping core algorithms, data analysis, machine learning experiments related to parameter optimization, or parts of the framework logic less dependent on high concurrency. *[Note: Adjust this description if the Python role is different]*.\n\n## Core Concepts Implemented (Simulation)\n\nThe current Go simulation (`go/cmd/simulation/`) demonstrates key aspects of QRL, including:\n\n*   **Network Modeling:** Representation of nodes in a network with configurable latency and fee structures (using `pkg/simulation` or similar).\n*   **Dynamic Parameter Management:** Implementation of parameters with probabilistic bounds and updates driven by Hamiltonian gradients.\n*   **Laplacian Smoothing:** Application of the discrete graph Laplacian ($\\nabla^2$) to ensure parameter coherence across nodes.\n*   **Hamiltonian Cost Function:** A configurable cost function ($H$) representing network objectives used for parameter optimization.\n*   **Adaptive Weight Tuning:** Simple feedback mechanism to adjust Hamiltonian weights based on simulated performance.\n*   **Probabilistic Quantity Imbalance (Conceptual):** A simplified model demonstrating how quantity imbalances can be tracked and corrected using Laplacian smoothing.\n*   **(Future):** Integration of D’Alembertian correction ($\\square$), more sophisticated probabilistic consensus, full CUT implementation, and transaction commutator effects are part of the ongoing research and development outlined in the whitepaper.\n\n## Getting Started\n\nSee [Installation Instructions](docs/installation.md) for details on how to install, set up, and run the QRL node.\n\n*(Add prerequisites and build/run instructions here)*\n\n**Prerequisites:**\n\n*   Go (version 1.18 or higher recommended)\n*   Python (version 3.8 or higher) - *[Specify if required for core functionality or only analysis]*\n*   [Any other dependencies, e.g., specific libraries]\n\n**Installation:**\n\n```bash\n# Clone the repository\ngit clone https://github.com/[your-username]/quantum-resonance-ledger.git qrl\ncd qrl\n\n# Install Go dependencies (if using Go modules)\ncd go\ngo mod download\ncd ..\n\n# Install Python dependencies (if applicable)\npip install -r python/requirements.txt\n```\n\n**Running the Simulation:**\n\nTo run the main simulation example (adjust path if needed):\n\n```bash\ngo run go/cmd/simulation/main.go\n```\n\n*(Add options or configuration details if applicable, e.g., simulation duration, network size)*\n\n## Running Tests\n\nThis project utilizes Go's testing framework and Python's pytest framework.\n\n**Go Tests:**\n\n```bash\n# Run all Go tests in the project\ngo test ./...\n\n# Run tests for a specific Go package (e.g., simulation core)\ngo test ./go/pkg/simulation\n```\n\n**Python Tests:**\n\n```bash\n# Run Python tests\n./run test\n```\n\n## Development Philosophy (Includes TDD)\n\nThis project aims for high code quality and reliability. While striving for rapid prototyping of complex ideas, Test-Driven Development (TDD) principles (Red-Green-Refactor) are encouraged, particularly for core simulation components, to ensure correctness and maintainability. Comprehensive unit and integration tests are valued.\n\n## Next Steps / Roadmap\n\nFuture development aligns with the phases outlined in the whitepaper, focusing on:\n\n1.  **Enhanced Physics Modeling:** Deeper integration of Path Integral concepts, D’Alembertian dynamics ($\\square$), and potentially transaction commutators.\n2.  **Blockchain Primitives:** Full implementation of CUTs, block structure, and robust probabilistic consensus mechanisms.\n3.  **Advanced Features:** Cross-chain bridging (\"entanglement\"), visualization tools, performance benchmarking, and rigorous parameter tuning.\n4.  **Privacy Enhancements:** Exploring ZKPs, HE, or SMPC to protect node state privacy.\n\n## Contributing\n\nWe welcome contributions! Please read our [CONTRIBUTING.md](CONTRIBUTING.md) guidelines for details on how to submit issues, feature requests, and pull requests.\n\n## License\n\nThis project is licensed under the MIT License - see the `LICENSE` file for details.\n"
    },
    {
      "name": "yusufk/jarvis-azure",
      "stars": 3,
      "img": "https://avatars.githubusercontent.com/u/404139?s=40&v=4",
      "owner": "yusufk",
      "repo_name": "jarvis-azure",
      "description": "Demo bot application to test-drive Azure OpenAI services",
      "homepage": "",
      "language": "Python",
      "created_at": "2023-01-21T13:12:14Z",
      "updated_at": "2025-04-12T17:50:08Z",
      "topics": [
        "azure",
        "azurecontainerservice",
        "openai",
        "telegram-bot",
        "telegram-bot-api"
      ],
      "readme": "Jarvis POC/Demo Telegram bot\n===========================\nThis Telegram bot is my personal AI Assistant, primarily built to explore LLM's and how they can be used to enable AI assistants.\n\nThere are multiple branches with different experiments, using various LLM models. The most interesting are the ones where I've attempted to give the AI assistant the ability to have an internal conversation and develop it's own train of thought. My personal view is that this is an important component of conciousness. \n\nIt is based on the [python-telegram-bot](http://python-telegram-bot.org/) library and is currently deployed on Azure using Azure Container App.\n\nInstallation\n------------\nTo install the bot, you need to have Python 3.11 or higher installed on your system.\nThen, you can install the required dependencies using poetry:\n    `poetry install`\n\nUsage\n-----\nTo run the bot, you need to create a Telegram bot using the [BotFather](https://t.me/botfather).\nStore environment variables in `.env` in the root of the project directory.\n"
    },
    {
      "name": "sambanova/integrations",
      "stars": 3,
      "img": "https://avatars.githubusercontent.com/u/80118584?s=40&v=4",
      "owner": "sambanova",
      "repo_name": "integrations",
      "description": null,
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2025-02-21T23:09:58Z",
      "updated_at": "2025-04-16T00:37:08Z",
      "topics": [],
      "readme": "<a href=\"https://sambanova.ai/\">\n<picture>\n <source media=\"(prefers-color-scheme: dark)\" srcset=\"./images/SambaNova-light-logo-1.png\" height=\"60\">\n  <img alt=\"SambaNova logo\" src=\"./images/SambaNova-dark-logo-1.png\" height=\"60\">\n</picture>\n</a>\n\n# SambaNova Cloud Integrations\n\nWelcome to the SambaNova Cloud API ecosystem, where everyday our fast inference models are expanding alongside the best tools in the developer community. Explore the partners available below and get started building\\! \n\n### Need Assistance? \n\nIf you have any suggestions of integrations or questions, please post on our [Community Page](https://community.sambanova.ai/) so we can follow up. \n\n## Integrations\n\n| Company | Type | Description | Access |\n| :---- | :---- | :---- | :---- |\n| **Agno** | Agentic-first library | Agno is a lightweight framework for building multi-modal AI agents | [Documentation](https://docs.agno.com/models/sambanova)  |\n| **AutoGen** | Agentic-first library | AutoGen is an open-source tool that defines agents, integrates LLMs, and handles task termination.  | [Demo code](./autogen/) |\n| **Browser Use** | Assistant tool | Browser Use is an open-source project enabling AI agents to control web browsers, facilitating tasks like automated web navigation and data extraction. | [Documentation](https://docs.browser-use.com/quickstart) |\n| **Camel** | Agentic-first library | Camel AI is an open-source framework for intelligent agents, and supports building, customizing, and deploying multi-agent systems.   | [Demo code](./camel/) |\n| **Cline** | Assistant tool | Cline is a coding assistant tool that streamlines workflows, offers account management features, and optimizes provider routing for developers. | [Documentation](https://docs.cline.bot/getting-started/getting-started-new-coders) |\n| **Continue** | Assistant tool | Continue is an open-source coding assistant platform to modify and optimize coding within IDE. | [Documentation](./continue) |\n| **CrewAI** | Agentic-first library | CrewAI is an open-source framework for making automated workflows with agents. | [Demo code](./crewai_integration/) |\n| **Gradio** | Assistant tool | Gradio is an open-source Python package that allows users to create interactive web apps for machine learning models, APIs, and Python functions.  | [Demo code](https://github.com/gradio-app/sambanova-gradio) |\n| **Haystack** | LLM framework | Haystack is an open-source end-to-end framework that enables modular development of production-ready LLM applications.  | [Demo code](./haystack/) |\n| **Hugging Face**  | LLM framework | Hugging Face is a platform for building, training, and deploying open-source models.  | [Documentation](https://huggingface.co/docs/huggingface_hub/main/en/package_reference/inference_client) |\n| **Instructor** | LLM framework | Instructor enhances LLM interactions by enabling structured data extraction, multi-language compatibility, and parallel tool calling. | [Demo code](./instructor/) |\n| **LangChain** | LLM framework | Langchain implements a standard interface for LLMs to simplify development, productization, and deployment. | [Documentation](https://python.langchain.com/docs/integrations/providers/sambanova/) |\n| **Langflow** | Low code framework | Langflow is a visual framework for building multi-agent and RAG applications. | [Documentation](https://docs.langflow.org/components-models#sambanova) |\n| **LlamaIndex** | LLM framework | LlamaIndex is an orchestration framework to rapidly deploy LLM applications. | [Demo code](./llamaindex) |\n| **Llama Stack** | LLM framework | Llama Stack provides modular APIs and tools to efficiently build and deploy AI applications, enabling tasks like inference, safety moderation, memory management, and autonomous agent creation using the Llama model family. | [Documentation](https://llama-stack.readthedocs.io/en/latest/distributions/self_hosted_distro/sambanova.html) |\n| **LiteLLM** | LLM framework | LiteLLM is an open-source Python library that provides a unified interface for accessing LLMs, translating inputs and mapping exceptions. | [Documentation](https://docs.litellm.ai/docs/providers/sambanova) |\n| **Milvus** | Vector DB | Milvus is an open-source vector database from Milvus and can easily enable RAG applications. | [Demo code](./milvus) |\n| **Oumi** | LLM framework | Oumi is an open-source platform that streamlines the entire lifecycle of foundation models from data preparation and training to evaluation and deployment. | [Documentation](https://oumi.ai/docs/en/latest/api/oumi.inference.html#oumi.inference.SambanovaInferenceEngine) |\n| **Semantic Kernel** | Agentic-first library | Semantic Kernel is an open-source development tool to build agents and integrate them with the latest AI models into your codebase.  | [Demo code](./semantic_kernel) |\n| **Vercel** | LLM framework | Vercel is a platform for deploying and hosting web applications, which developers can use to easily manage their websites and serverless functions. | [Documentation](https://sdk.vercel.ai/providers/community-providers/sambanova) |\n\n\n## AI Starter kits\n\nSambaNova AI Starter Kits are a collection of open-source examples and guides designed to facilitate the deployment of AI-driven use cases for both developers and enterprises. check the available kits [here](https://github.com/sambanova/ai-starter-kit)\n\n## API Documentation\n\n- Find more information about SambaNova Cloud [here](https://docs.sambanova.ai/cloud/docs/get-started/overview)\n\n**Note:** These Integrations code samples are provided \"as-is,\" and are not production-ready or supported code. Bugfix/support will be on a best-effort basis only. Code may use third-party open-source software. You are responsible for performing due diligence per your organization policies for use in your applications.\n\n"
    },
    {
      "name": "Insight-Services-APAC/Insight_Ingenious",
      "stars": 3,
      "img": "https://avatars.githubusercontent.com/u/75322600?s=40&v=4",
      "owner": "Insight-Services-APAC",
      "repo_name": "Insight_Ingenious",
      "description": "Gen AI Accelerator ",
      "homepage": "https://insight-services-apac.github.io/Insight_Ingenious/",
      "language": "Python",
      "created_at": "2024-10-14T03:31:01Z",
      "updated_at": "2025-03-30T21:36:53Z",
      "topics": [],
      "readme": "# Insight Ingenious\n\nInsight Ingenious is a flexible, extensible AI framework for building, managing, and deploying intelligent agent-based conversations. It provides a robust infrastructure for creating multi-agent conversations with LLM-based agents, enabling complex workflows and sophisticated AI interactions.\n\n## Overview\n\nInsight Ingenious serves as a comprehensive platform for developing AI solutions with the following key capabilities:\n\n- **Multi-agent orchestration**: Enables the coordination of multiple specialized AI agents in a collaborative conversation flow\n- **Flexible deployment**: Supports both web API and command-line interfaces\n- **Database integration**: Connects with various databases (SQLite, Cosmos DB) for conversation history and data persistence\n- **Template-based prompting**: Uses Jinja templates to create consistent prompting patterns\n- **Extensibility**: Allows custom conversation flows through a plugin architecture\n- **Visualization**: Includes tools for data visualization and analysis\n\nThe framework is designed to facilitate complex interactions between AI components, making it suitable for applications in education, analytics, knowledge management, and more.\n\n## Key Components\n\n### Core Framework\n\n- **`ingenious/main.py`**: The main entry point for the FastAPI application, responsible for setting up API routes and middleware\n- **`ingenious/cli.py`**: Command-line interface for various operations including launching the REST API server\n- **`ingenious/dependencies.py`**: Dependency injection system for service registration and configuration\n\n### Configuration\n\n- **`ingenious/config/`**: Configuration management including profile handling and environment-based settings\n- **`ingenious/models/config.py`**: Configuration models (Pydantic) for type-safe configuration\n\n### Data Layer\n\n- **`ingenious/db/`**: Database abstractions with implementations for SQLite and Cosmos DB\n- **`ingenious/files/`**: Storage abstractions for file operations with local and Azure implementations\n\n### Chat Services\n\n- **`ingenious/services/chat_service.py`**: Core service for managing chat interactions\n- **`ingenious/services/chat_services/multi_agent/`**: Multi-agent conversation management\n- **`ingenious/services/chat_services/multi_agent/conversation_flows/`**: Specialized conversation flow implementations\n\n### Agent Patterns\n\n- **`ingenious/services/chat_services/multi_agent/conversation_patterns/`**: Reusable conversation patterns for different use cases\n- **`ingenious/services/chat_services/multi_agent/agents/`**: Agent definitions and implementations\n\n### API\n\n- **`ingenious/api/routes/`**: API route definitions for the REST interface\n- **`ingenious/models/chat.py`**: Data models for chat requests and responses\n\n### Utilities\n\n- **`ingenious/utils/`**: Various utilities including namespace handling, token counting, and templating\n- **`ingenious/templates/`**: Template files for agent prompts and responses\n\n## Available Conversation Flows\n\nThe framework comes with several pre-built conversation flows:\n\n- **Classification Agent**: For classifying content in a conversational context\n- **Knowledge Base Agent**: For retrieving information from knowledge bases\n- **SQL Manipulation Agent**: For generating and executing SQL queries\n- **Web Critic Agent**: For analyzing and critiquing web content\n- **Pandas Agent**: For data analysis and visualization using pandas\n\n## Extension Points\n\nInsight Ingenious is designed to be extended through:\n\n1. **Custom conversation flows**: Add new flows in `ingenious_extensions/services/chat_services/multi_agent/conversation_flows/`\n2. **Custom agent implementations**: Define new agents in `ingenious_extensions/services/chat_services/multi_agent/agents/`\n3. **Custom API routes**: Add new API endpoints in `ingenious_extensions/api/routes/`\n4. **Custom prompt templates**: Create new templates in `ingenious_extensions/templates/prompts/`\n\n## Setup Development Environment\n\nTo set up the development environment, follow these steps:\n\n1. **Deactivate and Remove Existing Virtual Environment (if applicable)**:\n\n   ```bash\n   deactivate\n   rm -rf .venv\n   ```\n\n2. **Create and Activate a New Virtual Environment**:\n\n   ```bash\n   python3 -m venv .venv\n   source .venv/bin/activate\n   ```\n\n3. **Install the Base Ingenious Package**:\n   Run the following command to install the `ingenious` package without dependencies:\n\n   ```bash\n   pip install git+https://github.com/Insight-Services-APAC/Insight_Ingenious.git#egg=ingenious --force-reinstall\n   ```\n\n   This installs the base framework which is extended by custom extensions such as models, services, and templates.\n\n4. **Create a `.gitignore` File**:\n   Generate a `.gitignore` file to exclude unnecessary files and directories from version control:\n\n   ```bash\n   echo \"\n   .DS_Store\n   /.venv\n   /.chainlit\n   /.idea\n   /.cache\n   /env_mkdocs/\n   /tmp/context.md\n   /tmp/*.db\n   /dist/\n   /functional_test_outputs/\n   __pycache__\" > .gitignore\n   ```\n\n5. **Create Profile and Configure Environment Variables**:\n   Set up the `APPSETTING_INGENIOUS_CONFIG` and `APPSETTING_INGENIOUS_PROFILE` environment variables.\n\n6. **Add/Create Template Folders**:\n\n   ```bash\n   ingen_cli initialize_new_project\n   ```\n\n   Check the `ingenious_extensions` and `tmp` folder in your project root directory. Ensure it contains the following structure:\n\n   ```\n   tmp/\n   ├── context.md\n   ingenious_extensions/\n   ├── local_files/\n   ├── models/\n   ├── services/\n   ├── templates/\n   └── tests/\n   ```\n\n7. **Run Tests**:\n   Execute the test batch using the following command:\n\n   ```bash\n   ingen_cli run-test-batch\n   ```\n\n8. **AI Test Harness**:\n\n   ```bash\n   python ingenious_extensions/tests/run_flask_app.py\n   ```\n\n9. **CLI Test Harness**:\n   ```bash\n   python ingenious_extensions/tests/run_ingen_cli.py\n   ```\n\n## Getting Started (For New Projects)\n\nTo set up a new project:\n\n1. Run `ingen_cli initialize_new_project` to create the necessary folder structure\n2. Configure your application in `config.yml` and profiles in `~/.ingenious/profiles.yml`\n3. Develop your extensions in the `ingenious_extensions` directory\n4. Run your application with `ingen_cli run-rest-api-server`\n\nThe framework includes templates and examples to help you get started quickly with your own AI applications.\n"
    },
    {
      "name": "Pykoncio/MentorAI",
      "stars": 3,
      "img": "https://avatars.githubusercontent.com/u/83032163?s=40&v=4",
      "owner": "Pykoncio",
      "repo_name": "MentorAI",
      "description": null,
      "homepage": "",
      "language": "Jupyter Notebook",
      "created_at": "2025-02-17T07:11:43Z",
      "updated_at": "2025-03-13T12:25:03Z",
      "topics": [],
      "readme": "# 📚 **MentorAI**\n\n<img src=\"https://drive.google.com/uc?id=1mzgfusMIpaVrHRGtVSgGEr7EyiA_T6gs\"/>\n\nMentorAI is a virtual tutoring system that utilizes intelligent agents to provide tutoring in various academic subjects using artificial intelligence techniques.\n\n# 👥 Contributors\nJesus Ruiz Toledo – [Github](https://github.com/jesusruiztoledo) – Email: jesusruiz2829@gmail.com – Contribution: 33.3%\n\nHugo Peralta Muñoz - [Github](https://github.com/Pykoncio) – Email: hugoperalta2003@gmail.com – Contribution: 33.3%\n\nBorja Bravo Casermeiro – [Github](https://github.com/boorjabraavo21) – Email: borjabravo07@gmail.com – Contribution: 33.3%\n\n## 📌 Objectives\n- Develop AI agents capable of tutoring in various academic subjects.\n- Implement Natural Language Processing (NLP) for interactive learning.\n- Enhance scalability and performance through containerized deployment with Docker.\n- Provide real-time news updates and external API integrations to keep content relevant.\n\nMentorAI is a virtual tutoring system that utilizes intelligent agents to provide tutoring in a variety of academic subjects using advanced artificial intelligence techniques.\n\n## 📑 Table of Contents\n- [Installation Guide with Docker](#️-installation-guide-with-docker)\n- [Installation Guide without Docker (Not Recommended)](#installation-guide-without-docker-not-recommended)\n- [Project Structure](#-project-structure)\n- [File Descriptions](#-file-descriptions)\n- [Presentation Resources](#-presentation-resources)\n- [Bibliography](#-bibliography)\n\n## 🛠️ **Installation Guide with Docker**\n\nThis is the easiest way to install and run MentorAI using Docker and Docker Compose.\n\n### **Requirements**\n\n- Docker\n- Docker Compose\n\n### **Installation**\n\n1. Create an `.env` file in the root of the project with the following variables:\n\n```sh\nOPENAI_API_KEY=your_openai_key\nNEWS_API_KEY=your_newsapi_key\nMYSQL_HOST=your_mysql_host\nMYSQL_USER=your_mysql_user\nMYSQL_PASSWORD=your_mysql_password\nMYSQL_ROOT_PASSWORD=your_mysql_root_password\nMYSQL_DATABASE=your_mysql_database\n``` \n\n2. Start the containers by running the following command from the project's root directory:\n\n`docker-compose -f app/docker/docker-compose.yml up --build`\n\nThis will start the FastAPI server on port 8000 and the Streamlit application on port 8501, as well as the MySQL server on the port 3307.\n\nOnce the containers are running, access the Streamlit web interface at: `http://localhost:8501`\n\n## **Installation Guide without Docker (Not Recommended)**\n\n### **Requirements:**\n\n- Python 3.11\n- A running database with the characteristics defined below.\n- All project dependencies installed.\n\n**Create a virtual environment**\n1. Create the virtual environment:\n`python -m venv venv`\n\n2. Activate the virtual environment:\n    * On Windows: \n        ```terminal\n        venv/Scripts/activate\n        ```\n\n    * On macOS/Linux:\n        ```terminal\n        source venv/bin/activate\n        ```\n\n3. Install the project dependencies:\n\n    `pip install -r requirements.txt`\n\n### **Environment Variables**\nCreate an `.env` file in the root of the project with the following variables:\n\n```sh\nOPENAI_API_KEY=your_openai_key\nNEWS_API_KEY=your_newsapi_key\nMYSQL_HOST=your_mysql_host\nMYSQL_USER=your_mysql_user\nMYSQL_PASSWORD=your_mysql_password\nMYSQL_ROOT_PASSWORD=your_mysql_root_password\nMYSQL_DATABASE=your_mysql_database\n``` \n\n> [!NOTE]  \n> When running without Docker, ensure that you have a database set up with the above characteristics.\n\n#### Running the Project\nTo start the FastAPI server:\n\n`uvicorn app.main:app --reload`\n\nTo start the Streamlit application:\n\n`streamlit run app/streamlit/streamlit_app.py`\n\nWhen deployment is complete, open your web browser and navigate to: `http://localhost:8501` to access the MentorAI interface.\n\n## 📂 **Project Structure**\n```markdown\n.env  \n.gitignore  \napp/  \n├── __init__.py  \n├── agents/  \n│   ├── __init__.py  \n│   ├── biology_teacher.py  \n│   ├── chemistry_teacher.py  \n│   ├── economy_teacher.py  \n│   ├── history_teacher.py  \n│   ├── languaje_teacher.py  \n│   ├── math_teacher.py  \n│   ├── news_agent.py  \n│   ├── physics_teacher.py  \n│   ├── planner.py  \n│   └── programming_teacher.py  \n├── core/  \n│   ├── __init__.py  \n│   └── config.py  \n├── docker/  \n│   ├── docker-compose.yml  \n│   ├── Dockerfile.fastapi  \n│   └── Dockerfile.streamlit  \n├── main.py  \n├── models/  \n│   ├── __init__.py  \n│   └── filtering_model/  \n│       ├── filter_model_badwords.ipynb  \n│       └── toxic_classifier.joblib  \n├── schemas/  \n│   ├── __init__.py  \n│   └── chat.py  \n├── services/  \n│   ├── __init__.py  \n│   ├── filtering_service.py  \n│   └── openai_service.py  \n└── streamlit/  \n    └── streamlit_app.py\ndocs/  \n├── logo_mentorai.png  \n└── Presentacion_MentorAI.pdf  \noutput/  \n└── messages_output.csv  \nREADME.md  \nrequirements.txt  \nsrc/\n└── templates/\n```\n\n\n## 📄 **File Descriptions**\n`main.py`\nThis is the main file that configures and runs the FastAPI server. It contains the endpoints configuration and application initialization.\n\n`agents`\nThis directory contains the tutoring agents for different subjects. Each agent is responsible for providing answers and tutoring in its specific subject area:\n\n* `biology_teacher.py`: Biology tutoring agent.\n* `chemistry_teacher.py`: Chemistry tutoring agent.\n* `economy_teacher.py`: Economy tutoring agent.\n* `history_teacher.py`: History tutoring agent.\n* `language_teacher.py`: Language tutoring agent.\n* `math_teacher.py`: Mathematics tutoring agent.\n* `news_agent.py`: News agent providing updated information.\n* `physics_teacher.py`: Physics tutoring agent.\n* `planner.py`: Planning agent that helps organize study sessions.\n* `programming_teacher.py`: Programming tutoring agent.\n\n`config.py` \n\nThis file uses pydantic to configure the project. It defines the global configurations and the environment variables required for the project to work.\n\n`models`\n\nThis directory contains the model used in the project for filtering inappropriate language. The model validates all data within the application before it's processed by the APIs.\n\n`schemas`\n\nDefines data schemas using pydantic. These schemas are used to validate and structure the incoming requests and outgoing responses from the endpoints:\n\n* `chat.py:` Schema for chat requests and responses.\n\n`services`\n\nThis directory contains the services used by the agents, including the OpenAI service and the filtering service. These services encapsulate business logic and interactions with external APIs:\n\n* `filtering_service.py`: Service for filtering inappropriate language.\n* `openai_service.py`: Service for interacting with the OpenAI API.\n\n`streamlit_app.py`\n\nThe Streamlit application provides a graphical interface for interacting with the tutoring system. It includes a chat where users can input questions and displays the conversation history during the session.\n\n## 🎬 Presentation Resources\n- [Project Presentation Video](https://www.youtube.com/watch?v=5s27SuEfBg0)\n- [Project Canvas](https://www.canva.com/design/DAGgZwpR4OY/wBnTFRACWRTl7wDuxULeVw/edit?utm_content=DAGgZwpR4OY&utm_campaign=designshare&utm_medium=link2&utm_source=sharebutton)\n- [Presentation PDF](/docs/Presentacion_MentorAI.pdf)\n\n## 📚 Bibliography\n* [Toxic Comment Classification Challenge, Dataset from Kaggle](https://www.kaggle.com/competitions/jigsaw-toxic-comment-classification-challenge/data)\n\n* [Streamlit Documentation](https://docs.streamlit.io/)\n\n* [FastAPI Documentation](https://fastapi.tiangolo.com/)\n\n* [Docker Documentation](https://docs.docker.com/)\n\n* [Mini-course on SQLAlchemy](https://www.youtube.com/watch?v=XSAjQDM8ZS4)\n\n* [OpenAI Api Reference](https://platform.openai.com/docs/api-reference/introduction)\n\n* [Advances in Intelligent Tutoring Systems](https://link.springer.com/book/10.1007/978-3-642-14363-2) - Nkambou, R., Mizoguchi, R., & Bourdeau, J. (2010). Springer Berlin Heidelberg.\n"
    },
    {
      "name": "MichalRogowski/CVAgent",
      "stars": 3,
      "img": "https://avatars.githubusercontent.com/u/47182107?s=40&v=4",
      "owner": "MichalRogowski",
      "repo_name": "CVAgent",
      "description": "CVAgent is an AI-driven tool that generates, critiques, and renders resumes tailored to specific job ads. Powered by Autogen and RenderCV, it refines user inputs and job requirements into a polished CV, saving time and ensuring alignment. Start building your next resume with ease.",
      "homepage": "https://www.beyondsenior.engineering",
      "language": "Typst",
      "created_at": "2025-01-26T17:51:27Z",
      "updated_at": "2025-02-10T08:30:37Z",
      "topics": [
        "ai",
        "aiagent",
        "python",
        "resume",
        "resume-builder",
        "resume-creator",
        "resume-template"
      ],
      "readme": "\n# CVAgent\n![Social-preview](./Social-preview.png)\nPowered by **Autogen** and the **RenderCV** library, it refines user inputs and job requirements into a polished CV, saving time and ensuring alignment. Start building your next resume with ease.\n---\n\n## Table of Contents\n1. [Prerequisites](#prerequisites)\n2. [Installation](#installation)\n3. [Configuration](#configuration)\n4. [Usage](#usage)\n5. [Output](#output)\n6. [Contributing](#contributing)\n7. [License](#license)\n\n---\n\n## Prerequisites\n- [Python 3.13+](https://www.python.org/downloads/)\n- [Poetry](https://python-poetry.org/docs/)\n\n---\n\n## Installation\n1. **Clone the repository**:\n   ```bash\n   git clone https://github.com/yourusername/CVAgent.git\n   cd CVAgent\n   ```\n\n2. **Install dependencies via Poetry**:\n   ```bash\n   poetry install\n   ```\n\n3. **Activate the virtual environment** (optional, but recommended):\n   ```bash\n   poetry shell\n   ```\n\n---\n\n## Configuration\n1. **Create a `.env` file** in the root folder to store your API key:\n   ```ini\n   OPENAI_API_KEY=your_api_key\n   ```\n   Replace `your_api_key` with your actual OpenAI API key.\n\n---\n\n## Usage\n1. **Update the following files** according to your details:\n   - `advert.txt` — The job advertisement you want to apply for.\n   - `CV-experience.txt` — Your professional experience, including positions and key highlights.\n   - `CV-personal.txt` — Your personal summary, education, contact details, and social platforms.\n   - `CV-skills.txt` — Your skills, including both soft and technical skills.\n\n2. **Run the main script**:\n   ```bash\n   poetry run python main.py\n   ```\n   If you have activated the virtual environment via `poetry shell`, you can also run:\n   ```bash\n   python main.py\n   ```\n3. **Cost Optimization**:\n    The script is set up to use `4o-mini` for all agents, but it often gets stuck when generating YAML because the model is not sophisticated enough or the prompt is not good enough.\nIf this happens, simply change the `yaml_writer` agent to use the pre-configured `openai_model_4o_client`.\n---\n\n## Architecture Diagram\n![Architecture Diagram](./CVAgent-architecture.png)\n\n## Output\n- By default, the generated PDF file is saved in the `render_output` folder (if AI won't change it).\n![Example_pdf_output](./rendercv_output/John_Doe_CV_1.png)\n---\n\n## Contributing\nFeel free to open issues or submit pull requests if you have any improvements or suggestions.\n---\n\n## License\nThis project is licensed under the [MIT License](LICENSE).\n"
    },
    {
      "name": "alejbormeg/AutoGen_Swarm",
      "stars": 3,
      "img": "https://avatars.githubusercontent.com/u/61965913?s=40&v=4",
      "owner": "alejbormeg",
      "repo_name": "AutoGen_Swarm",
      "description": "Experimento de swarm de agentes de Autogen.",
      "homepage": null,
      "language": "Python",
      "created_at": "2025-01-22T21:34:24Z",
      "updated_at": "2025-01-23T14:41:35Z",
      "topics": [],
      "readme": "🤖 Multi-Agent Financial & News Research Bot\n\nWelcome to the Multi-Agent Financial & News Research Bot repository! This project demonstrates a coordinated multi-agent approach to researching stock market data and news articles, then compiling a final report — all through a conversation-based interface.\n\nBelow you’ll find:\n\t1.\tOverview of the use case\n\t2.\tKey features\n\t3.\tInstallation & setup\n\t4.\tHow to run\n\t5.\tContributing\n\t6.\tLicense\n\n🔎 Use Case Overview\n\nIn many research or financial analysis scenarios, you have multiple specialized tasks:\n\t1.\tFetching financial data (e.g., stock price, volume, etc.)\n\t2.\tGathering relevant news about a company\n\t3.\tSynthesizing all findings into a cohesive report\n\nThis repo implements an autonomous team of agents — a Planner, a Financial Analyst, a News Analyst, and a Writer — each handling a specific role. They communicate via a conversation-driven workflow, coordinating tasks to produce a final Markdown report for users.\n\t•\tPlanner: Decides what to do and delegates tasks.\n\t•\tFinancial Analyst: Fetches stock data and writes partial reports.\n\t•\tNews Analyst: Fetches related news articles and writes partial reports.\n\t•\tWriter: Compiles a final Markdown report from the gathered data.\n\nAll of this happens interactively using OpenAI’s GPT-based agents and the News/Alpha Vantage APIs!\n\n✨ Key Features\n\t•\tAgent Swarm: Multiple agents, each with a specialized role, communicate via handoffs.\n\t•\tReal-Time Data: Fetches live stock data from Alpha Vantage and recent articles from the News API.\n\t•\tInteractive Console Flow: Runs in a conversational style, prompting the user only when needed.\n\t•\tReport Generation: Automatically writes Markdown reports with timestamps for traceability (saved in a reports directory).\n\n⚙️ Installation & Setup\n\t1.\tClone the repository:\n\ngit clone https://github.com/your-username/your-repo.git\ncd your-repo\n\n\n\t2.\tCreate and activate an isolated environment (recommended). You can use either:\n\t•\tPython venv:\n\npython3 -m venv venv\nsource venv/bin/activate\n\n\n\t•\tConda:\n\nconda create -n multiagent-env python=3.9\nconda activate multiagent-env\n\n\n\t3.\tInstall dependencies:\n\npip install -r requirements.txt\n\n\n\t4.\tSet up environment variables:\nCreate a .env file in the same directory with the following variables:\n\nALPHA_VANTAGE_API_KEY=YOUR_ALPHA_VANTAGE_API_KEY\nNEWS_API_KEY=YOUR_NEWS_API_KEY\nOPENAI_API_KEY=YOUR_OPENAI_API_KEY\n\nMake sure you sign up and obtain the keys for:\n\t•\tAlpha Vantage for stock data\n\t•\tNewsAPI for news articles\n\t•\tOpenAI for GPT-based chat models\n\n🚀 How to Run\n\t1.\tEnsure your .env file is properly configured with valid API keys.\n\t2.\tRun the script:\n\npython stock_web_search.py\n\n\n\t3.\tEnter an initial task when prompted (e.g., “Research Tesla stock”).\n\t4.\tFollow the console prompts. The agents will coordinate, fetch data, and hand off control if they need additional user input.\n\nWhen the process finishes, you should see a final Markdown report saved under reports/ for each agent’s results.\n\n🤝 Contributing\n\nContributions are welcome! If you have ideas to improve agent logic, add more tools, or enhance the conversation flow, feel free to:\n\t1.\tFork this repository\n\t2.\tCreate a new branch (git checkout -b feature-your-feature)\n\t3.\tCommit your changes (git commit -m 'Add your feature')\n\t4.\tPush to the branch (git push origin feature-your-feature)\n\t5.\tOpen a Pull Request\n\n📝 License\n\nThis project is licensed under the MIT License. Feel free to modify and reuse for your own projects.\n\nEnjoy exploring autonomous multi-agent financial research!\nIf you have any questions, create an issue or submit a pull request.\n\nHappy coding! 🎉\n"
    },
    {
      "name": "denniszielke/ai-agents-workshop",
      "stars": 3,
      "img": "https://avatars.githubusercontent.com/u/11569044?s=40&v=4",
      "owner": "denniszielke",
      "repo_name": "ai-agents-workshop",
      "description": "AI Agents hands on workshop",
      "homepage": "",
      "language": "Python",
      "created_at": "2024-11-22T19:37:10Z",
      "updated_at": "2025-02-10T17:39:34Z",
      "topics": [
        "agents",
        "ai",
        "azure",
        "langchain",
        "langgraph",
        "llamaindex",
        "semantickernel"
      ],
      "readme": "# AI Agent workshop\n\nRegions that this deployment can be executed:\n- northeurope (azure location)\n- swedencentral (aiResourceLocation)\n\n## Quickstart & Infrastructure setup\n\nThe following lines of code will connect your Codespace az cli and azd cli to the right Azure subscription:\n\n```\n# log in with the provided credentials - OPEN A PRIVATE BROWSER SESSION\naz login --use-device-code\n\n# if you need to log into a specific tenant - use the --tenant 00000000-0000-0000-0000-000000000000 parameter\naz login --use-device-code --tenant 00000000-0000-0000-0000-000000000000 \n\n# \"log into azure dev cli - only once\" - OPEN A PRIVATE BROWSER SESSION\nazd auth login --use-device-code\n\n# press enter open up https://microsoft.com/devicelogin and enter the code\n\n```\n\nNow deploy the infrastructure components with azure cli\n\n```\nazd up\n```\n\nGet the values for some env variables\n```\nazd env get-values | grep AZURE_ENV_NAME\nsource <(azd env get-values | grep AZURE_ENV_NAME)\n```\n\ndeploy the project lc-react-tools in Azure Container Apps. \n```\nbash ./azd-hooks/deploy.sh lc-react-tools $AZURE_ENV_NAME\n```\n\n\n# Workshop agenda\n\nThe scope of this workshop covers the following scenarios and technology stacks:\n\n| Name | Description | Technology  |\n| :-- | :--| :-- |\n| [af-simple](./src/af-simple/code-run.py) | Single Agent | AgentService, RAG |\n| [af-autogen](./src/af-autogen/simple.py) | Single Agent | AgentService, Autogen |\n| [lc-react-tools](./src/lc-react-tools/Readme.md) | Single Agent | Streamlit, Azure OpenAI, Langchain |\n| [lg-agents-01-coding](./src/lg-agents-01-coding/Readme.md) | Multi-agent code reviews | LangGraph, Azure OpenAI, Otel |\n| [lg-agents-02-shop](./src/lg-agents-02-shop/Readme.md) | Human in the loop | LangGraph, Qdrant, Azure OpenAI |\n| [li-workflows-01-simple](./src/li-workflows-01-simple/Readme.md) | Simple event driven workflow | Llama agents, Azure OpenAI |\n| [li-workflows-02-events](./src/li-workflows-02-events/Readme.md) | Event driven agent collaboration | Llama agents, Azure OpenAI |\n| [sk-agents-01-collaboration](./src/sk-agents-01-collaboration/Readme.md) | Simple mult agent discussion | Semantic kernel, Azure OpenAI |\n| [sk-agents-02-tools](./src/sk-agents-02-tools/Readme.md) | Using tools from agents | Semantic kernel, Azure OpenAI |\n| [sk-agents-03-creative](./src/sk-agents-03-creative/Readme.md) | Multi-turn multi agent discussion | Semantic kernel, Azure OpenAI |\n| [sk-agents-04-process](./src/sk-agents-04-process/Readme.md) | Event driven flow | Semantic kernel, Azure OpenAI |\n"
    },
    {
      "name": "huqianghui/azure-ai-agent-workshop",
      "stars": 3,
      "img": "https://avatars.githubusercontent.com/u/7360524?s=40&v=4",
      "owner": "huqianghui",
      "repo_name": "azure-ai-agent-workshop",
      "description": "azure-ai-agent-workshop",
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2025-01-20T00:52:42Z",
      "updated_at": "2025-04-22T09:30:31Z",
      "topics": [],
      "readme": "# azure ai agent servie & Autogen workshop\n\n## azure-ai-agent-service\n\n1. function call\n[1_function_call](./azure-ai-agent-service-sampleCode/notebooks/1_function_call.ipynb)\n\n2. code interpreter \n[2_Code_Interpreter](./azure-ai-agent-service-sampleCode/notebooks/2_Code_Interpreter.ipynb)\n\n3. grounding with documents\n[3_Grounding_with_Documents](./azure-ai-agent-service-sampleCode/notebooks/3_Grounding_with_Documents.ipynb)\n\n4. grounding with bing search\n[4_Grounding_with_Bing_Search](./azure-ai-agent-service-sampleCode/notebooks/4_Grounding_with_Bing_Search.ipynb)\n\n## AutoGen workshop \n\nAutoGen Version 0.4.x is a rewrite of AutoGen from the ground up to create a more robust, extensible, and easy-to-use cross-language library for building agents.\n\nThe application interface of AutoGen v0.4.x adopts a hierarchical architecture design, and there are multiple sets of software interfaces to meet the needs of different scenarios\n\n1. Core: event-driven interfaces, core interfaces, and autogen-core interfaces\n\nAn event-driven programming framework for building scalable, multi-agent AI systems.\n\n2. AgentChat:task-driven application interface, autogen-agentchat\n\nA programming framework for building sessional single-agent and multi-agent applications, built \non top of the Core core layer, abstracting many of the underlying system concepts.\n\n3. Extensions: third-party system interfaces, extension packages, autogen-ext\n\nImplementations of Core and AgentChat components that interact with external services or other libraries. You can find and use community extensions or create your own\n\n### 1） basic demo \n\nRelevant demo code is in the autogen-v0.4.x-sampleCode folder\n\n1. basic\n\n2. team run\n\n3. human feedback\n\n4. human feedback2\n\n### 2） MagenticOne-demo\n\nMagentic-One is a console-based, multi-agent assistant for performing web and file-based tasks. Built on AgentChat.it works based on a multi-Agent architecture, where the chief Orchestrator Agent is responsible for high-level planning, guiding other Agents, and tracking task progress.\nThe Orchestrator first needs to create a plan to handle the task, collecting necessary facts and informed guesses in the task ledger.At each step of the plan, the Orchestrator creates a progress ledger, reflecting on the task progress and checking if the task is completed.\nIf the task is not completed, it assigns a subtask to other Agents of Magentic-One to complete. Once the assigned Agent completes the subtask, the Orchestrator updates the progress ledger and continues in this manner until the task is completed.\nIf the Orchestrator finds the progress insufficient, it can update the task ledger and create a new plan. As shown in the figure above, the Orchestrator's work is divided into an outer loop of updating the task ledger and an inner loop of updating the progress ledger.\n![architecture-overview](./autogen-v0.4.x-sampleCode/MagenticOne-demo/01.png)\n\n2. Relevant demo code is in the MagenticOne-demo folder\n\n3. step:\n    pip install -U magentic-one-cli\n    input message：\"我今天的心情非常不错\"  Q&A\n\n### 3） AutoGen Studio\n\nThe application that allows you to prototype and manage agents without writing code. Built on AgentChat\n\nsteps:  \n    1) pip install -U autogenstudio \n    \n    2) command: autogenstudio ui --port 8081\n        \n        Support modifying related parameters to customize the application as follows:\n        --host Specify the host address, default is localhost\n        --appdir Specify the folder to store application files (such as databases and generated user files). Default is the .autogenstudio folder in the user's home directory\n        --port Specify the port number. Default is 8080\n        --upgrade-database Upgrade the database schema. Default is False\n        --reload Enable automatic server reload on code changes. Default is False\n        --database-uri Specify the database URI\n\n    3)  use the server: http://localhost:8081/\n\n\n### AI agent servie & AutoGen\n\n1) one autogen agent delegate the ai agent service's agent.\n[AzureAIAgentWithAutoGen-AsTools](./azure-ai-agent-service-with-autogen/1-AzureAIAgentWithAutoGen-AsTools.ipynb)\n\n2) delegated agents work with autogen agent.\n[2-AzureAIAgents_Intergration_With_AutoGen_Agents](./azure-ai-agent-service-with-autogen/2-AzureAIAgents_Intergration_With_AutoGen_Agents.ipynb)\n\n\n\n\n\n"
    },
    {
      "name": "strand1/ComfyUI-Autogen",
      "stars": 3,
      "img": "https://avatars.githubusercontent.com/u/1580273?s=40&v=4",
      "owner": "strand1",
      "repo_name": "ComfyUI-Autogen",
      "description": "Collection of useful nodes for using Autogen with ComfyUI",
      "homepage": null,
      "language": "Python",
      "created_at": "2025-01-16T11:30:24Z",
      "updated_at": "2025-03-27T06:46:53Z",
      "topics": [],
      "readme": "# ComfyUI-Autogen\nA collection of nodes for using Autogen with ComfyUI \n\n[[AutoGen Stable](https://microsoft.github.io/autogen/stable/)]: assistant agents, group chats, code executor, etc.\n\n## Features\n\n1. **Autogen Model**  \n   - Initializes an OpenAI-like model via `autogen_ext.models`.  \n   - Lets you configure model name, base URL, API key, and optional capabilities like vision, function calling, and JSON output.\n\n2. **Autogen Assistant Agent**  \n   - Wraps an `AssistantAgent` from `autogen_agentchat.agents`.  \n   - Lets you specify tools (by scanning a `tools` folder), temperature, max tokens, etc.  \n   - Great for orchestrating more conversational or tool-using LLM tasks within ComfyUI.\n\n3. **Autogen Code Executor**  \n   - A node wrapping the `CodeExecutorAgent` from `autogen_agentchat.agents` with a local command-line executor.  \n   - Enables code execution tasks, specifying a working directory, timeouts, etc.\n\n4. **Autogen Group Chat**  \n   - Leverages `RoundRobinGroupChat` to let multiple agents interact in a conversation.  \n   - Terminates the chat based on a max number of messages or a specific terminate string.\n\n## Installation\n### Copy the folder\nPlace the ComfyUI-Autogen folder into your `ComfyUI/custom_nodes/` directory.\n\n### Install dependencies\nActivate the same Python environment (or virtual environment) that ComfyUI uses, then run:\n\n```bash\npip install -r requirements.txt\n```\n\n### Restart ComfyUI\nThe nodes should appear under the “Autogen” category in your ComfyUI interface.\n\n### Requirements:\n```text\nautogen-core\nautogen-ext\nautogen-agentchat\n```\n## Example \nThis example uses the Company Research sample from Autogen [[here](https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/examples/company-research.html)]\n![Company Research Workflow](workflows/company_report_screenshot.png)\n*note the `analyze_stock` tool uses the `yfinance` library to retrieve stock information. It will need to be installed for this tool to work.\n\n## Todo\n- vision models need integration\n\n## Contributing\nFeel free to open issues or pull requests if you run into bugs, have ideas, or want to contribute enhancements. Feedback is welcome to help improve these nodes and their capabilities.\n"
    },
    {
      "name": "whiteduck-training/wd-ai-hackathon",
      "stars": 3,
      "img": "https://avatars.githubusercontent.com/u/185513507?s=40&v=4",
      "owner": "whiteduck-training",
      "repo_name": "wd-ai-hackathon",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2024-11-13T16:52:57Z",
      "updated_at": "2024-12-19T09:52:09Z",
      "topics": [],
      "readme": "# wd-ai-hackathon\n\n## Run locally\n\nTo run the notebooks locally, you need to have a docker runtime installed on your machine. You can install docker from [here](https://docs.docker.com/get-docker/).\n\nAfter installing, you can run this repository as a devcontainer by following the steps below:\n\n1. Clone this repository to your local machine.\n2. Open the repository in Visual Studio Code.\n3. Open the command palette by pressing `Ctrl+Shift+P` or `Cmd+Shift+P` and search for `Remote-Containers: Reopen in Container`.\n\nFinally, you need to rename the .env_template file to .env and replace the values with the following\n\n```sh\nAZURE_OPENAI_CHAT_DEPLOYMENT_NAME=\"gpt-4o-mini\"\nAZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME=\"text-embedding-3-large\"\nAZURE_OPENAI_ENDPOINT=\"https://models.inference.ai.azure.com\"\nAZURE_OPENAI_API_KEY=\"<YOUR API KEY>\"\n```\n\nYou can get the API key by creating a GitHub Personal Access Token. See [here](https://github.com/settings/tokens).\n\n\n## Installation\n\nWhen running as devcontainer or codespace everything should be set up already.\n\nWhen running locally `uv`needs to be installed:\n\n```sh\ncurl --proto '=https' --tlsv1.2 -LsSf https://github.com/astral-sh/uv/releases/download/0.5.4/uv-installer.sh | sh\n\n```\n\nafter installation run\n\n```sh\nuv sync\n```\n\nAlso following vscode extensions are needed:\n\n```sh\n\"ms-python.python\"\n\"ms-toolsai.jupyter\"\n\"ms-toolsai.jupyter-renderers\"\n\"ms-toolsai.jupyter-keymap\"\n\"charliermarsh.ruff\"\n\"GitHub.copilo\"\n\"GitHub.copilo-chat\"\n```\n\n## Sanity check\n\nOpen `notebooks/00_hello_world.ipynb` and select `.venv`as kernel/virtual environment.\n\nExecute the notebook to make sure it runs without error.\n\n## AI Hackathon: Building Intelligent Applications with Semantic Kernel\n\nThis repository contains a series of Jupyter notebooks that guide you through building AI-powered applications using Semantic Kernel. The content progresses from basic concepts to advanced implementations, with each module containing both theoretical foundations and practical exercises.\n\n### Prerequisites\n\n- Python 3.8 or later\n- Basic understanding of Python programming\n- OpenAI API key or Azure OpenAI access\n- Semantic Kernel library installed\n\n### Notebooks Overview\n\n#### Module 1: Introduction to Semantic Kernel\n\n- **01_a_introduction_to_semantic_kernel.ipynb**: Theory module covering:\n  - What is Semantic Kernel\n  - Setting up the Kernel\n  - Plugins (Native and Semantic)\n  - Memory capabilities\n  - Practical examples including an Emoji Translator\n- **01_b_Introduction_to_semantic_kernel.ipynb**: Practical exercises for implementing small applications using core concepts:\n  - Fusion-Chef: Recipe combination system\n  - QnA Bot: Memory-based question answering\n  - Roleplay Gamemaster: Interactive game system\n  - Shell Meister: Natural language to shell command translator\n\n#### Module 2: Planning and Execution\n\n- **02_a_a_bad_plan_is_better_than_no_plan.ipynb**: Theory module exploring:\n  - AI Planning concepts\n  - Types of Planners (Sequential and Function Calling Stepwise)\n  - Memory integration with planners\n  - Web research capabilities\n- **02_b_a_bad_plan_is_better_than_no_plan.ipynb**: Practical exercises including:\n  - Code Documentation Assistant\n  - Personal Finance Advisor\n  - AI Dungeon Master\n  - Research Paper Assistant\n\n#### Module 3: AI Agents\n\n- **03_a_agents.ipynb**: Theory module covering:\n  - Introduction to SK Agents\n  - Creating basic agents\n  - Agent capabilities and plugins\n  - Multi-agent interactions\n- **03_b_agents.ipynb**: Practical exercises for building:\n  - Code Review Team\n  - Game Master System\n  - Competitive Debate System\n  - Technical Support System\n  - Research Paper Collaboration System\n- **03_c_battle_of_the_agents.ipynb**: Advanced concepts on:\n  - Agent simulation and testing\n  - Conversation modeling\n  - Performance evaluation\n\n#### Module 4: Process Management\n\n- **04_a_its_a_process.ipynb**: Theory module on:\n  - Building an AI Travel Agent\n  - Process Steps and Event Handling\n  - State Management\n  - AI Integration\n- **04_b_its_a_process.ipynb**: Practical exercises for implementing:\n  - Smart Home Automation System\n  - Restaurant Kitchen Process System\n  - Document Approval Workflow\n  - Data ETL Pipeline System\n  - Project Management System\n\n#### Module 5: Capstone Project\n\n- **05_a_putting_it_all_together.ipynb**: Understanding Knowledge Graphs\n  - Theory and foundations\n  - Graph components and relationships\n  - Visualization techniques\n  - Best practices\n- **05_b_putting_it_all_together.ipynb**: Building an AI Knowledge System\n  - Core Knowledge Graph System\n  - Knowledge Extraction Agents\n  - Query Agent System\n  - Process Integration\n- **05_c_putting_it_all_together.ipynb**: Extended Practical Applications\n  - Research Paper Assistant\n  - Technical Documentation Assistant\n  - Learning Path Generator\n  - Troubleshooting Assistant\n  - Content Recommendation System\n\n### Learning Path\n\nThe notebooks are designed to be completed in order, as each module builds upon concepts introduced in previous modules. The progression goes from basic Semantic Kernel concepts to advanced multi-agent systems and process management, culminating in a comprehensive capstone project.\n\nEach module contains:\n\n- Theoretical foundations\n- Code examples\n- Practical exercises\n- Best practices and tips\n- Real-world applications\n"
    },
    {
      "name": "rejuve-bio/AI-Assistant",
      "stars": 3,
      "img": "https://avatars.githubusercontent.com/u/164211405?s=40&v=4",
      "owner": "rejuve-bio",
      "repo_name": "AI-Assistant",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2024-09-12T07:17:26Z",
      "updated_at": "2025-04-21T14:05:30Z",
      "topics": [],
      "readme": "# AI Assistant Backend API\n\nThis is the backend API for the RejuveBio Platform AI Assistant.\n\n## Prerequisites\n\nBefore you begin, ensure you have the following installed:\n\n* **Python 3.8+**\n* **Poetry** (for managing dependencies)\n\n## Installation\n\n### 1. Clone the repository\nFirst, clone the repository and navigate to the project folder:\n\n```bash\ngit clone https://github.com/rejuve-bio/AI-Assistant.git\ncd AI-Assistant\n```\n\n### 2. Install dependencies using Poetry\nInstall the required dependencies for the project:\n\n```bash\npoetry install\n```\n\n### 3. Activate the virtual environment\nActivate the Poetry-managed virtual environment:\n\n```bash\npoetry shell\n```\n\n## 4. Configuration\nThe application uses environment variables to set up its parameters.\n\n**Environment Variables**\nThe `.env` file contains sensitive information like API keys, credentials, and configuration overrides. The `.env.example` file is provided as a template. You can copy it to a `.env` file and fill in your actual values.\n\n```bash\ncp .env.example .env\n```\n\nEnsure that the environment variables are set correctly in `.env` before running the application:\n\n* **LLM Model Configuration:**\n  * `BASIC_LLM_PROVIDER`: Choose the provider for lighter tasks (openai or gemini).\n  * `BASIC_LLM_VERSION`: Version for the basic model (gpt-3.5-turbo, gemini-lite, etc.).\n  * `ADVANCED_LLM_PROVIDER`: Choose the provider for advanced tasks (openai or gemini).\n  * `ADVANCED_LLM_VERSION`: Version for the advanced model (gpt-4o, gemini-pro, etc.).\n* **API Keys:**\n  * `OPENAI_API_KEY`: Your OpenAI API key.\n  * `GEMINI_API_KEY`: Your Gemini API key.\n* **Neo4j Configuration:**\n  * `NEO4J_URI`, `NEO4J_USERNAME`, `NEO4J_PASSWORD`: Connection details for the Neo4j database.\n* **Annotation Service Configuration:**\n  * `ANNOTATION_AUTH_TOKEN`: Authentication token for the annotation service.\n  * `ANNOTATION_SERVICE_URL`: The URL for the annotation service, which processes queries.\n* **Flask Configuration:**\n  * `FLASK_PORT`: Port for the Flask server (default: 5002).\n* **Qdrant configuration:**\n  * `QDRANT_CLIENT`: Port for qdrant client(http://localhost:6333)\n\n## Usage\n\nOnce your environment is configured, you can run the Flask server and use the AI Assistant API.\n\nmake sure you set up qdrant local client :\n```bash\ndocker run -d \\\n    -p 6333:6333 \\\n    -v qdrant_data:/qdrant/storage qdrant/qdrant\n```\n\n### Authentication\nFirst, generate and copy your authentication token:\n```bash\npython helper/access_token_generator.py\n```\nUse this token in your API requests:\n- For Postman: Add header `Authorization: Bearer your_token_here`\n- For cURL: Add `-H \"Authorization: Bearer your_token_here\"`\n\n### 1. Start the Flask Server\nRun the Flask server with the following command:\n\n```bash\npython run.py\n```\nThis will start the server at http://localhost:5002.\n\n### 2. Send a POST request to the `/query` endpoint\nYou can send a POST request to the `/query` endpoint to interact with the AI Assistant.\n\n**Example using curl:**\n\n```bash\ncurl -X POST http://localhost:5002/query \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"query\": \"What enhancers are involved in the formation of the protein p78504?\"}'\n```\n\n**Request Body:**\n\n```json\n{\n  \"query\": \"Your natural language query here\"\n}\n```\n\n**Response:**\n\nA JSON object containing the processed results from the AI assistant, based on the model's analysis.\n\n## Acknowledgments\n\n* OpenAI for providing the GPT models.\n* Google for the Gemini models.\n* Neo4j for the graph database technology.\n* Flask for the lightweight web framework.\n\n\n## Using Docker to run the application\n\n## Installation\n\n### 1. Clone the repository\nFirst, clone the repository and navigate to the project folder:\n\n```bash\ngit clone [https://github.com/rejuve-bio/AI-Assistant.git](https://github.com/rejuve-bio/AI-Assistant.git)\ncd ai-assistant\n```\n\n### 2. Setting up .env files\n\nEnsure that the environment variables are set correctly in `.env` before running the application:\n\n* **LLM Model Configuration:**\n  * `BASIC_LLM_PROVIDER`: Choose the provider for lighter tasks (openai or gemini).\n  * `BASIC_LLM_VERSION`: Version for the basic model (gpt-3.5-turbo, gemini-lite, etc.).\n  * `ADVANCED_LLM_PROVIDER`: Choose the provider for advanced tasks (openai or gemini).\n  * `ADVANCED_LLM_VERSION`: Version for the advanced model (gpt-4o, gemini-pro, etc.).\n* **API Keys:**\n  * `OPENAI_API_KEY`: Your OpenAI API key.\n  * `GEMINI_API_KEY`: Your Gemini API key.\n* **Neo4j Configuration:**\n  * `NEO4J_URI`, `NEO4J_USERNAME`, `NEO4J_PASSWORD`: Connection details for the Neo4j database.\n* **Annotation Service Configuration:**\n  * `ANNOTATION_AUTH_TOKEN`: Authentication token for the annotation service.\n  * `ANNOTATION_SERVICE_URL`: The URL for the annotation service, which processes queries.\n\n## Usage\n\nOnce your environment is configured, you can run the app and use the AI Assistant API.\n\n### 1. Start the application:\n\n```bash\ndocker-compose up --build\n```\n\n**Example using curl:**\n```bash\ncurl -X POST http://localhost:5002/query \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"query\": \"your query here\"}'\n```\n\n### 2. To stop the services, use:\n  ```bash\n  docker-compose down\n  ```"
    },
    {
      "name": "Azure/auto-kyc",
      "stars": 3,
      "img": "https://avatars.githubusercontent.com/u/6844498?s=40&v=4",
      "owner": "Azure",
      "repo_name": "auto-kyc",
      "description": "AI-Powered KYC Verification with Azure Face API and OpenAI",
      "homepage": null,
      "language": "Python",
      "created_at": "2024-10-30T14:29:01Z",
      "updated_at": "2025-02-02T09:40:39Z",
      "topics": [],
      "readme": "# Auto-KYC Application\n\nThe Auto-KYC Application offers an advanced, AI-powered identity verification solution that streamlines the compliance process for financial institutions and businesses. By leveraging Azure's Face API for facial recognition and Azure OpenAI for intelligent data extraction and semantic comparison, our platform ensures accurate and secure verification of ID documents against customer records. This solution not only reduces manual verification time but also enhances fraud detection and compliance accuracy, making it ideal for companies seeking to optimize their onboarding and regulatory processes with cutting-edge AI technology. \nFace Liveness detection is used to determine if a face in an input video stream is real or fake. It's an important building block in a biometric authentication system to prevent imposters from gaining access to the system using a photograph, video, mask, or other means to impersonate another person. The goal of liveness detection is to ensure that the system is interacting with a physically present, live person at the time of authentication. \n\n\n## Table of Contents\n\n- [Features](#features)\n- [Architecture Overview](#architecture-overview)\n- [Installation](#installation)\n- [Usage](#usage)\n- [Folder Structure](#folder-structure)\n- [Backend Services](#backend-services)\n- [API Endpoints](#api-endpoints)\n- [Contributing](#contributing)\n- [License](#license)\n\n## Features\n\n- **Document Upload and Analysis**: Supports uploading documents (e.g., passport, driver's license) for analysis using Azure services.\n- **Automated Data Extraction**: Uses Azure OpenAI models to automatically extract relevant fields from documents.\n- **Semantic Field Comparison**: Compares extracted data with the corresponding records in the database using natural language processing techniques.\n- **Face Recognition and Matching**: Leverages Azure Face API to detect and compare faces from the uploaded document with the stored customer records.\n- **Log Viewer and Status Indicators**: Provides a visual interface to review data comparison results, including matching status and discrepancies.\n- **Face Liveness Check**: Determines if a face in an input video stream is real or fake\n\n## Architecture Overview\n\nThe KYC application is divided into two main components:\n\n1. **Frontend (React Application)**:\n   - A React-based user interface for uploading documents, viewing and editing customer data, and visualizing document analysis results.\n   - Responsive layout with a fixed-width sidebar (340px) for navigation and status indicators.\n   - Uses Material-UI for styling and layout components.\n\n2. **Backend (Python FastAPI Server)**:\n   - **Document Processing**: Handles file uploads and processes documents using Python libraries like `pdf2image` and `PIL` for image handling.\n   - **Data Extraction and Verification**: Integrates with Azure OpenAI models to extract structured information from uploaded documents.\n   - **Face Recognition**: Uses Azure Face API for detecting and comparing faces in documents.\n   - **Semantic Comparison**: Leverages Azure OpenAI to perform semantic matching of extracted fields with the stored data, going beyond simple string matching.\n   - **Storage and Database**: Utilizes Azure Blob Storage for storing document images and Azure Cosmos DB for storing customer data.\n\n## Installation\n\n### Prerequisites\n\n- npm package manager installed on your machine for the frontend\n- Python 3.10+ (for the backend)\n- Azure account with Face API and OpenAI services enabled\n- Azure Blob Storage and Cosmos DB set up\n\n\n\n### Setup Instructions\n\n1. **Clone the repository**:\n   ```bash\n   git clone https://github.com/Azure/auto-kyc.git\n   cd auto-kyc\n   ```\n\n2. **Backend Setup**:\n   - Install Python dependencies:\n     ```bash\n     conda create -n auto-kyc python=3.10\n     conda activate auto-kyc\n     pip install -r requirements.txt\n     ```\n   - Set up environment variables in a `.env` file:\n     ```\n     FACE_API_ENDPOINT=<your-face-api-endpoint>\n     FACE_API_KEY=<your-face-api-key>\n     AZURE_OPENAI_ENDPOINT=<your-openai-endpoint>\n     AZURE_OPENAI_KEY=<your-openai-key>\n     COSMOS_DB_URI=<your-cosmos-db-uri>\n     COSMOS_DB_KEY=<your-cosmos-db-key>\n     ```\n   - Run the FastAPI server:\n     ```bash\n     uvicorn api:app --port 80 --reload\n     ```\n\n3. **Frontend Setup**:\n   - Navigate to the frontend directory:\n     ```bash\n     cd ../ui/react-js\n     ```\n   - Install frontend dependencies:\n     ```bash\n     npm install\n     ```\n\n\n### Face API Access and Installation\n\nPlease follow the instructions in the [link here](https://github.com/Azure-Samples/azure-ai-vision-sdk/blob/main/GET_FACE_ARTIFACTS_ACCESS.md), to apply for access. \n\nAfter getting access, and creating the Face API resource on Azure, you will need to:\n1. Generate an access token\n1. Create an .npmrc file with the access token, please follow the instructions [here](https://github.com/Azure-Samples/azure-ai-vision-sdk/blob/main/samples/web/SetupEnvironment.md).\n1. Download the node module:\n    ```bash\n    npm install azure-ai-vision-face-ui@latest\n    ```\n1. After installing the `azure-ai-vision-face-ui` node package, navigate to the installed files under `ui > react-js > node_modules > azure-ai-vision-face-ui` and copy the whole folder and the contents of the `facelivenessdetector-assets` directory as is into the `ui > react-js > public` directory.\n\nThe `ui > react-js > public` directory should eventually look like this:\n\n```\npublic\n├── assets\n├── facelivenessdetector-assets\n├── sample-documents\n└── index.html\n```\n\n### Running the Solution\n\nAfter following the above instructions for installation, you can run the React App using the following commands. Please make sure that the FastAPI server has already started:\n\n   - Navigate to the frontend directory:\n     ```bash\n     cd ui/react-js\n     ```\n   - Optionally start the React development server (for development purposes only):\n     ```bash\n     npm start\n     ```\n   - The application will be available at `http://localhost:3000`.\n\n   - Build the React app which is served by the FastAPI server on port 80:\n     ```bash\n     npm run build\n     ```\n\n\n\n## Folder Structure\n\nThe project is organized as follows:\n\n```\nauto-kyc/\n├── code/                 # Python FastAPI backend\n│   ├── utils/               # Helper modules for storage, document processing, etc.\n│   ├── data_models/         # Data models for ID document processing\n│   └── env_vars.py          # Environment variable configuration\n├── frontend/                # React frontend application\n│   ├── public/              # Static assets\n│   ├── src/                 # Source code for the React application\n│   └── package.json         # Frontend dependencies\n└── api.py                   # API Server\n└── README.md                # Project documentation\n```\n\n## Backend Services\n\n### 1. **Face Recognition (Azure Face API)**\n\n- The backend uses Azure's Face API to detect faces in uploaded documents.\n- It draws rectangles around detected faces and performs face-to-face verification with the stored customer image.\n- The service supports quality checks to ensure the highest-quality face is used for verification.\n\n### 2. **Data Extraction (Azure OpenAI)**\n\n- Azure OpenAI models are used to extract key fields from documents, such as name, date of birth, ID number, address, etc.\n- The system is designed to work with various types of identification documents, including passports, driver's licenses, and national ID cards.\n\n### 3. **Semantic Field Comparison**\n\n- Once the data is extracted, it is semantically compared with the customer's existing database record using NLP techniques.\n- The backend can handle complex comparisons, such as matching name variations, address similarities, and date formats.\n\n### 4. **Data Storage and Retrieval**\n\n- The application uses Azure Blob Storage to securely store document images and processed files.\n- Azure Cosmos DB is employed to store and manage customer records, with the backend providing interfaces for reading and updating records.\n\n\n## Contributing\n\nContributions are welcome! Please follow the steps below:\n\n1. Fork the repository.\n2. Create a new branch (`git checkout -b feature-branch`).\n3. Make your changes.\n4. Commit and push your changes (`git push origin feature-branch`).\n5. Open a pull request.\n\n## License\n\nThis project is licensed under the MIT License. See the [LICENSE](LICENSE) file for details.\n"
    },
    {
      "name": "alphavector/all",
      "stars": 3,
      "img": "https://avatars.githubusercontent.com/u/11805788?s=40&v=4",
      "owner": "alphavector",
      "repo_name": "all",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2024-10-06T12:01:51Z",
      "updated_at": "2025-01-26T12:22:19Z",
      "topics": [],
      "readme": "python -m generator -w 100 -l 1000 -r requirements.txt\n\n200 - https://editor.mergely.com/vCYSrPu2\n500 - https://editor.mergely.com/Fuh9Gsyw\n"
    },
    {
      "name": "SongChiYoung/autogen-contextplus",
      "stars": 3,
      "img": "https://avatars.githubusercontent.com/u/7540984?s=40&v=4",
      "owner": "SongChiYoung",
      "repo_name": "autogen-contextplus",
      "description": "Autogen ContextPlus, User defined AutoGen model_context",
      "homepage": "",
      "language": "Python",
      "created_at": "2025-04-19T06:30:02Z",
      "updated_at": "2025-04-23T07:20:31Z",
      "topics": [
        "autogen",
        "autogen-extension"
      ],
      "readme": "# 🧠 Autogen ContextPlus\n\n> Modular, customizable, and serializable context engine for [AutoGen](https://github.com/microsoft/autogen) — enabling structured message summarization, filtering, and rewriting logic with full compatibility.\n\n---\n\n## ✨ What is ContextPlus?\n\n**`autogen-contextplus`** provides a general-purpose context modifier system for AutoGen’s `model_context` layer. It supports:\n\n- ✅ Condition-triggered message summarization\n- ✅ Agent- or function-based message rewriting\n- ✅ Component-based serialization / deserialization\n- ✅ Full support for user-defined logic via AutoGen `FunctionTool` or custom agents\n\n---\n\n## 🔧 Installation\n\n```bash\npip install autogen-contextplus\n```\n\n---\n\nFor development and type checking:\n```bash\npip install -e \".[dev]\"\n```\n\n---\n\n## Example\n\n```python\nimport asyncio\nfrom pprint import pprint\nfrom typing import List\nfrom autogen_core.models import UserMessage, AssistantMessage\nfrom autogen_ext.models.replay import ReplayChatCompletionClient\nfrom autogen_ext.models.anthropic import AnthropicChatCompletionClient\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_core import CancellationToken \nfrom autogen_core.model_context import BufferedChatCompletionContext\n\nfrom autogen_contextplus.conditions import (\n    MaxMessageCondition\n)\nfrom autogen_contextplus.base.types import (\n    ModifierFunction,\n)\nfrom autogen_contextplus import (\n    ContextPlusChatCompletionContext\n)\nfrom autogen_core.models import LLMMessage\n\n\ndef buffered_summary(\n    messages: List[LLMMessage],\n    non_summarized_messages: List[LLMMessage],\n) -> List[LLMMessage]:\n    \"\"\"Summarize the last `buffer_count` messages.\"\"\"\n    if len(messages) > 3:\n        return messages[-3:]\n    return messages\n\n\nasync def main():\n    client = ReplayChatCompletionClient(\n        chat_completions=[\n            \"paris\",\n            \"seoul\",\n            \"paris\",\n            \"seoul\",\n        ]\n    )\n    \n    context = ContextPlusChatCompletionContext(\n        modifier_func = buffered_summary,\n        modifier_condition = MaxMessageCondition(max_messages=2)\n    )\n    agent = AssistantAgent(\n        \"helper\",\n        model_client=client,\n        system_message=\"You are a helpful agent\",\n        model_context=context\n    )\n    \n    await agent.run(task=\"What is the capital of France?\")\n    res = await context.get_messages()\n    print(f\"[RESULTS] res:\")\n    pprint(res)\n    print(f\"[RESULTS] len_context : {len(res)}, context_type: {type(context)}\")\n\n    await agent.run(task=\"What is the capital of Korea?\")\n    res = await context.get_messages()\n    print(f\"[RESULTS] res:\")\n    pprint(res)\n    print(f\"[RESULTS] len_context : {len(res)}, context_type: {type(context)}\")\n\n    print(\"==========================\")\n\n    cancellation_token = CancellationToken() \n    await agent.on_reset(cancellation_token=cancellation_token)\n    test = agent.dump_component()\n    agent = AssistantAgent.load_component(test)\n    context = agent.model_context\n\n    await agent.run(task=\"What is the capital of France?\")\n    res = await context.get_messages()\n    print(f\"[RESULTS] res:\")\n    pprint(res)\n    print(f\"[RESULTS] len_context : {len(res)}, context_type: {type(context)}\")\n    await agent.run(task=\"What is the capital of Korea?\")\n    res = await context.get_messages()\n    print(f\"[RESULTS] res:\")\n    pprint(res)\n    print(f\"[RESULTS] len_context : {len(res)}, context_type: {type(context)}\")\n    \nif __name__ == \"__main__\":\n    asyncio.run(main())\n```"
    },
    {
      "name": "hackertron/LucidLoanMachine",
      "stars": 3,
      "img": "https://avatars.githubusercontent.com/u/7667514?s=40&v=4",
      "owner": "hackertron",
      "repo_name": "LucidLoanMachine",
      "description": "Lucid Loand Machine(LLM) : LLM and zk powered loan app",
      "homepage": null,
      "language": "Python",
      "created_at": "2024-06-18T05:10:26Z",
      "updated_at": "2025-03-05T11:29:00Z",
      "topics": [],
      "readme": "# LucidLoanMachine\nLucid Loand Machine(LLM) : LLM and zk powered loan app\n\n## tools\n1. Autogen\n2. tlsnNotary\n3. zkemail\n\n## quick demo/try\n1. install requirements.txt\n2. run python main.py\n\n## architecture\nneed to complete\n\n\n### notes\nuse raw email and make zkemail skill\nwhich is basically RSA but formatted for email check DKIM\ncheck if pdf if signed or not\n"
    },
    {
      "name": "unibaseio/aip-agent",
      "stars": 3,
      "img": "https://avatars.githubusercontent.com/u/149583566?s=40&v=4",
      "owner": "unibaseio",
      "repo_name": "aip-agent",
      "description": "An Agent Interoperability Protocol (AIP) implementation which allows tools and agents worldwide to achieve seamless interconnection through Unibase. ",
      "homepage": "",
      "language": "Python",
      "created_at": "2025-02-11T06:36:23Z",
      "updated_at": "2025-04-23T08:40:07Z",
      "topics": [],
      "readme": "# AIP Agent\n\n## Introduction\n\nThe Unibase Agent System is an innovative framework that stands at the intersection of distributed systems, blockchain technology, and artificial intelligence. It is designed to facilitate a structured and secure method of interaction among autonomous agents. The system's core is defined by three principal features: an agent interaction protocol, blockchain-based authorization, and direct invocation by Large Language Models (LLMs).\n\n### 1. Agent Interoperability Protocol (AIP)\n\nThe Agent Interoperabilit Protocol (AIP) is the backbone of the system, enabling seamless communication and collaboration between agents. It standardizes the way agents exchange information and perform tasks, ensuring consistency and predictability in their interactions.\n\n- **Agent Hub**: Serves as the central hub for agent coordination and control.\n\n  - **Permission Management**: Utilizes blockchain to authenticate and authorize agent access, ensuring secure interactions.\n  - **Configuration Management**: Manages agent configurations for efficient operation and query handling.\n  - **Memory Management**: Provides a global memory space for storing agent configurations, dialogue history, and prompt information.\n  - **Data Storage**: Facilitates decentralized data backup to protect against data loss.\n\n- **Protocol Support and Extension**: The AIP extends the foundational MCP (Model Context Protocol) to enable seamless integration with local/remote MCP tools, ensuring that agents and tools from anywhere can communicate and interoperate effectively.\n  - **SSE (Server-Sent Events) Protocol**: Facilitates remote connectivity with MCP tools over networks, enabling real-time event streaming and authorization.\n  - **gRPC Integration**: To address scenarios where remote tools/agents cannot establish direct connections, gRPC-based communication is introduced. This provides high-performance, bidirectional streaming and structured data exchange, ensuring reliable remote interactions.\n\n### 2. Blockchain-Based Authorization\n\nThe system leverages blockchain technology to implement a robust and transparent authorization mechanism.\n\n- **Agent Identity Management**: Agents are registered on the blockchain with a unique UUID, linking them to specific addresses and granting them a verifiable identity.\n  - **Chain-based Contract**: Records agent account information, token details, and permission settings, including read/write access and timeouts.\n  - **Permission Verification**: Ensures that only authorized agents can read from or write to the shared memory, as dictated by blockchain records.\n\n### 3. Direct Invocation by LLMs\n\nThe system is architected to be directly callable by LLMs, enhancing the capabilities of these models and enabling them to perform complex tasks through agent interactions.\n\n- **Agent Tools Integration**: Agents are designed to be accessible as tools for LLMs, allowing these models to leverage the agents' functionalities for a wide range of applications.\n  - **Tool Management**: Supports three types of tool communication protocols:\n    - `mcp`: Legacy MCP tool, access it locally\n    - `aip-grpc`: Internal network communication, accessible by agents from anywhere\n    - `aip-sse`: Direct access requiring the tool's machine to be directly accessible\n  - **Tool Registration**: Tools can be registered with the system and exposed through either gRPC or SSE endpoints\n  - **Tool Discovery**: Agents can dynamically discover and load available tools\n  - **Tool Authorization**: Secure tool access through blockchain-based authentication\n  - **Tool Loading**: Supports on-the-fly loading of functionalities during dialogues, allowing adaptive tool integration without interrupting workflows.\n  - **Tool Execution**: Asynchronous tool execution with parameter validation and result handling\n\n## Membase Hub\n\n- Link Hub: 13.212.116.103:8081\n  - agent connects to link hub at startup\n  - exchange messages via link hub\n- Memory Hub: https://testnet.hub.membase.io/\n  - conversation is auto saved in memory hub\n  - preload conversation from memory hub\n\n## Usage\n\n### Installation\n\n```shell\npip install git+https://github.com/unibaseio/aip-agent.git\n# or clone into local\ngit clone https://github.com/unibaseio/aip-agent.git\ncd aip-agent\n# install dependencies\nuv venv\nuv sync --dev --all-extras\n```\n\n### Requirements\n\n- MEMBASE_ID must be unique for each instance\n- MEMBASE_ACCOUNT must have balance in BNB testnet\n- Environment variables must be set:\n  - `MEMBASE_ID`: Unique identifier for the instance\n  - `MEMBASE_ACCOUNT`: Account with BNB testnet balance\n  - `MEMBASE_SECRET_KEY`: Secret key of account for authentication\n\n### demo video\n\n[![aip demo](./img/fdemo.png)](https://youtu.be/CJpuOGE2qqw)\n\n[![aip with mcp tool](./img/fmcp.png)](https://youtu.be/QNlPzXHuVpA)\n\n### Tool Server Setup\n\nsee [aip tool introduction](./examples/aip_tools/README.md)\n\n### Agent Setup and Usage\n\nAgents can interact with tools and other agents through a unified interface:\n\n```shell\nexport MEMBASE_ID=\"<membase uuid>\"\nexport MEMBASE_ACCOUNT=\"<membase account>\"\nexport MEMBASE_SECRET_KEY=\"<membase secret key>\"\ncd examples/aip_agents\nuv run grpc_full_agent_gradio.py\n```\n\n#### Agent-Tool Interaction\n\n1. **Tool Discovery and Connection**\n\n![Tool discovery and connection](./img/tool_load.png)\n_Search and connect to tool servers through LLM chat interface_\n\n2. **Tool Usage**\n\n![Tool usage example](./img/tool_use.png)\n_Interact with tools through LLM chat interface_\n\n#### Agent-Agent Interaction\n\n- start another agent, communicate with above agent\n\n![Agent interaction](./img/agent_call.png)\n_Demonstration of inter-agent communication_\n\n### Interactive Chess Game\n\nA chess game implementation with two AIP agents playing against each other.\n\nSee [aip chess game introduction](./examples/aip_chess_game/README.md)\n\n+ demo video\n\n[![aip chess game](./img/fchess.png)](https://youtu.be/COvK9Y9d8SE)\n\n### Trade Agent\n\nAn automated trading agent system based on the AIP framework, supporting token trading on the BSC testnet/mainnet. \n\nSee [aip trader agents introduction](./examples/aip_trader_agents/README.md)\n\n+ demo video\n\n[![aip automated trader agents](./img/ftrader.png)](https://youtu.be/QcRkOOZ1vZM)\n\n### Personal Agent\n\nA personal intelligent agent system that generates personalized agents based on Twitter/X users' historical records.\n\nSee [aip personal agents introduction](./examples/aip_personal_agents/README.md) \n\n+ demo video\n\n[![aip personal agents](./img/fpersonal.png)](https://youtu.be/jXorx1qrGF4)\n\n### Python Code Examples\n\n#### Full Agent Example\n\nThe `FullAgentWrapper` is designed to create a complete agent with LLM capabilities and memory management. Here's how to use it:\n\n```python\nfrom aip_agent.agents.full_agent import FullAgentWrapper\nfrom aip_agent.agents.custom_agent import CallbackAgent\nimport os\n\nasync def main():\n    # Initialize the full agent\n    full_agent = FullAgentWrapper(\n        agent_cls=CallbackAgent,  # Your custom agent implementation\n        name=os.getenv(\"MEMBASE_ID\"),  # Unique identifier\n        description=\"You are an assistant\",  # Agent description\n        host_address=\"13.212.116.103:8081\"  # GRPC server address\n    )\n\n    # Initialize the agent (this will:\n    # 1. Register membase_id on blockchain\n    # 2. Register in membase hub\n    # 3. Connect to membase memory hub)\n    await full_agent.initialize()\n\n    # Process user queries\n    response = await full_agent.process_query(\"Hello, how can you help me?\")\n    print(response)\n\n    # Stop the agent when needed\n    await full_agent.stop()\n\nif __name__ == \"__main__\":\n    import asyncio\n    asyncio.run(main())\n```\n\n#### Custom Agent Implementation\n\nYou can create custom agents by extending the base agent classes:\n\n```python\nfrom aip_agent.agents.full_agent import FullAgentWrapper\nfrom aip_agent.agents.custom_agent import CallbackAgent\n\nclass MyCustomAgent(CallbackAgent):\n    async def handle_message(self, message):\n        # Custom message handling logic\n        pass\n\nasync def main():\n    agent = FullAgentWrapper(\n        agent_cls=MyCustomAgent,\n        name=os.getenv(\"MEMBASE_ID\"),\n        description=\"Custom agent description\",\n        host_address=\"13.212.116.103:8081\"\n    )\n    await agent.initialize()\n    # Your agent logic here\n    await agent.stop()\n```\n\n### Key Features\n\n#### FullAgentWrapper Features\n\n1. **LLM Integration**\n\n   - Built-in LLM support for natural language processing\n   - Customizable response generation\n   - Context-aware conversations\n   - Direct LLM invocation capabilities\n\n2. **Tool Management**\n\n   - Dynamic tool loading and management\n   - Tool discovery and registration\n   - Tool execution with parameter validation\n   - Tool result processing\n\n3. **Memory Management**\n\n   - Persistent memory storage\n   - Conversation history tracking\n   - Context preservation\n   - Memory-based learning\n\n4. **Membase Hub Integration**\n\n   - Seamless connection to membase hub\n   - Remote service discovery\n   - Distributed communication and coordination\n\n5. **Message Handling & Security**\n\n   - Asynchronous message processing\n   - Message validation and verification\n   - Secure message routing\n   - Custom message handlers\n\n6. **Blockchain Integration**\n   - Automatic blockchain registration\n   - Identity management\n   - Permission-based access control\n   - Secure authorization\n\n#### ToolAgentWrapper Features\n\n1. **Tool Management**\n\n   - Tool registration and exposure\n   - Tool capability advertisement\n   - Parameter validation\n   - Execution monitoring\n\n2. **Membase Hub Integration**\n\n   - Service discovery\n   - Tool capability exposure\n   - Remote accessibility and coordination\n\n3. **Message Handling & Security**\n\n   - Message validation\n   - Secure message processing\n   - Request verification\n   - Response formatting\n\n4. **Blockchain Integration**\n   - Blockchain identity registration\n   - Permission management\n   - Access control\n   - Authorization verification\n\n## Development\n\n### Project Structure\n\n## Architecture Overview\n\n### System Components\n\n1. **Agent Core**\n   - LLM Integration Layer\n   - Memory Management System\n   - Tool Management System\n   - Blockchain Integration Layer\n\n2. **Network Layer**\n   - gRPC Communication\n   - SSE Event Streaming\n   - WebSocket Support\n   - REST API Endpoints\n\n3. **Security Layer**\n   - Blockchain Authentication\n   - Permission Management\n   - Data Encryption\n   - Access Control\n\n### Data Flow\n\n1. **Agent Initialization**\n   - Register on blockchain\n   - Connect to Membase Hub\n   - Load configuration\n   - Initialize memory\n\n2. **Tool Integration**\n   - Discover available tools\n   - Register tool capabilities\n   - Establish secure connections\n   - Validate permissions\n\n3. **Message Processing**\n   - Receive user input\n   - Process through LLM\n   - Execute tool actions\n   - Update memory\n   - Return response\n\n## Advanced Features\n\n### Memory Management\n\n- **Persistent Storage**\n  - Conversation history\n  - Tool configurations\n  - Agent states\n  - User preferences\n\n- **Memory Operations**\n  - Read/Write access\n  - Memory versioning\n  - Data synchronization\n  - Conflict resolution\n\n### Tool System\n\n- **Tool Types**\n  - Local tools\n  - Remote tools\n  - Hybrid tools\n  - Composite tools\n\n- **Tool Lifecycle**\n  - Registration\n  - Discovery\n  - Loading\n  - Execution\n  - Cleanup\n\n### Blockchain Integration\n\n- **Smart Contracts**\n  - Agent registration\n  - Permission management\n  - Token operations\n  - Event logging\n\n- **Security Features**\n  - Identity verification\n  - Access control\n  - Transaction signing\n  - Audit trails\n\n## Best Practices\n\n### Development Guidelines\n\n1. **Agent Development**\n   - Follow modular design\n   - Implement error handling\n   - Use type hints\n   - Write unit tests\n   - Document code\n\n2. **Tool Development**\n   - Define clear interfaces\n   - Handle edge cases\n   - Implement timeouts\n   - Validate inputs\n   - Log operations\n\n3. **Security Practices**\n   - Use environment variables\n   - Implement rate limiting\n   - Validate permissions\n   - Encrypt sensitive data\n   - Monitor access\n\n### Performance Optimization\n\n1. **Memory Management**\n   - Implement caching\n   - Use efficient data structures\n   - Clean up unused resources\n   - Monitor memory usage\n\n2. **Network Optimization**\n   - Use connection pooling\n   - Implement retry logic\n   - Optimize payload size\n   - Handle timeouts\n\n3. **Tool Execution**\n   - Parallel processing\n   - Resource allocation\n   - Load balancing\n   - Error recovery\n\n## Troubleshooting\n\n### Common Issues\n\n1. **Connection Problems**\n   - Check network settings\n   - Verify hub addresses\n   - Validate credentials\n   - Test connectivity\n\n2. **Tool Integration**\n   - Verify tool registration\n   - Check permissions\n   - Validate configurations\n   - Monitor logs\n\n3. **Memory Issues**\n   - Check storage limits\n   - Verify access rights\n   - Monitor performance\n   - Clean up old data\n\n\n4. **Lifecycle Exit**\n   - Lifecycle task encountered an error: unhandled errors in a TaskGroup\n   - Use python instead of uv \n\n### Debugging Tools\n\n- **Logging System**\n  - Error tracking\n  - Performance metrics\n  - User actions\n  - System events\n\n- **Monitoring Tools**\n  - Resource usage\n  - Network traffic\n  - Tool performance\n  - Memory status\n\n## Contributing\n\nWe welcome contributions to the AIP Agent project. Here's how you can help:\n\n1. **Code Contributions**\n   - Fork the repository\n   - Create feature branch\n   - Write tests\n   - Submit pull request\n\n2. **Documentation**\n   - Update README\n   - Write tutorials\n   - Add examples\n   - Improve comments\n\n3. **Testing**\n   - Report bugs\n   - Write test cases\n   - Test new features\n   - Performance testing\n\n## License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n\n## Contact\n\nFor questions, suggestions, or support:\n- GitHub Issues: [https://github.com/unibaseio/aip-agent/issues](https://github.com/unibaseio/aip-agent/issues)\n- Email: support@unibase.io\n- Discord: [Join our community](https://discord.gg/unibase)\n"
    },
    {
      "name": "playbooks-ai/playbooks",
      "stars": 3,
      "img": "https://avatars.githubusercontent.com/u/182034399?s=40&v=4",
      "owner": "playbooks-ai",
      "repo_name": "playbooks",
      "description": "Playbooks AI is a framework for creating AI agents using human-readable and LLM-executed playbooks.",
      "homepage": "https://runplaybooks.ai",
      "language": "Python",
      "created_at": "2024-09-18T21:56:08Z",
      "updated_at": "2025-04-03T03:57:28Z",
      "topics": [
        "agents",
        "ai",
        "framework",
        "llm",
        "llm-os",
        "low-code-framework",
        "multi-agent-systems",
        "playbook",
        "pseudocode"
      ],
      "readme": "<div align=\"center\">\n  <h1 align=\"center\">Playbooks AI</h1>\n  <h2 align=\"center\">Finally, Natural Language Programming is here!</h2>\n</div>\n\n<div align=\"center\">\n   <a href=\"https://pypi.org/project/playbooks/\">\n      <img src=\"https://img.shields.io/pypi/v/playbooks?logo=pypi&style=plastic&color=blue\" alt=\"PyPI Version\"/></a>\n   <a href=\"https://www.python.org/\">\n      <img src=\"https://img.shields.io/badge/Python-3.10-blue?style=plastic&logo=python\" alt=\"Python Version\"></a>\n   <a href=\"https://github.com/playbooks-ai/playbooks/blob/master/LICENSE\">\n      <img src=\"https://img.shields.io/github/license/playbooks-ai/playbooks?logo=github&style=plastic&color=green\" alt=\"GitHub License\"></a>   \n   <a href=\"https://github.com/playbooks-ai/playbooks/tree/master/docs\">\n      <img src=\"https://img.shields.io/badge/Docs-GitHub-blue?logo=github&style=plastic&color=red\" alt=\"Documentation\"></a>\n   <br>\n   <a href=\"https://github.com/playbooks-ai/playbooks/actions/workflows/test.yml\">\n      <img src=\"https://github.com/playbooks-ai/playbooks/actions/workflows/test.yml/badge.svg\", alt=\"Test\"></a>\n   <a href=\"https://github.com/playbooks-ai/playbooks/actions/workflows/lint.yml\">\n      <img src=\"https://github.com/playbooks-ai/playbooks/actions/workflows/lint.yml/badge.svg\", alt=\"Lint\"></a>\n   <a href=\"https://runplaybooks.ai/\">\n      <img src=\"https://img.shields.io/badge/Homepage-runplaybooks.ai-green?style=plastic&logo=google-chrome\" alt=\"Homepage\"></a>\n</div>\n\nPlaybooks AI™ lets you program AI agents using plain English instead of code. Our patent-pending engine turns human-readable instructions into executable AI behavior — no coding required.\n\n**Status**: Playbooks AI is still in early development. We're working hard and would love your feedback and contributions.\n\n## Table of Contents\n\n- [Show me!](#show-me)\n- [What is Natural Language Programming?](#what-is-natural-language-programming)\n- [Features](#features)\n- [How it works](#how-it-works)\n- [Quick start](#quick-start)\n- [Who Should Use Playbooks?](#who-should-use-playbooks)\n- [Roadmap](#roadmap)\n- [Contributing](#contributing)\n- [License](#license)\n- [Contributors](#contributors)\n\n## Show me!\n\nHere's the simplest Natural Language Program you can write — a Hello World agent:\n\n```playbooks\n# Hello World!\n\n## Say Hello\n\n### Trigger\nAt the beginning\n\n### Steps\n- Greet the user with a friendly \"Hello, World!\" message.\n```\n\nThat's it! Just plain English that both humans and AI can understand. Here \"Hello World!\" is name of the AI agent, while \"Say Hello\" is a playbook. A playbook is executed as soon as its trigger condition is satisfied.\n\nNow, let's look at something more powerful. Here's a web search agent that:\n\n- Uses search engines when needed\n- Performs deep research to gather information\n- Filters out inappropriate topics\n\nSee [Playbooks AI implementation](src/playbooks/examples/playbooks/web_search_chat.md) of a Web Search Chat agent — about **50 lines of English instructions**. \n\nNow compare that with an equivalent [LangGraph implementation](src/playbooks/examples/langgraph/web_search_chat.py) — about **200 lines of complex code** that's harder to understand and modify.\n\n🔗 Ready to write your first Natural Language Program? [Get started here](#quick-start).\n\n## What is Natural Language Programming?\n\nNatural Language Programming lets you create AI applications by writing instructions in plain English. Think of it as pseudocode that actually runs.\n\nBuilding AI agents today forces you to choose between three frustrating options:\n\n1. **Writing complex code** → Requires technical expertise\n2. **Using no-code UI builders** → Gets messy for complex workflows\n3. **Direct prompting** → Results in unpredictable behavior\n\nWith Playbooks AI, you simply write clear instructions in a playbook format that:\n- Business people can read and modify\n- AI can execute reliably\n- Handles complex logic, tool usage, and multi-agent collaboration\n\n### **Natural Language Programming vs. Traditional Approaches**\n\n| Feature                 | **Playbooks AI** 🏆 | **Code-Based Frameworks** | **UI-Based Agent builders** | **Direct Prompting** |\n|-------------------------|------------------|-------------------------------|-------------------------------|-------------------------------|\n| **Ease of Use**         | ✅ Write in plain English | ❌ Requires Python expertise | ✅ No-code UI, but gets messy | ✅ Just type a prompt |\n| **Behavior Control** | ✅ Easily modify agent behavior | ❌ Requires coding to change | ❌ Hard to translate requirements into UI | ❌ Unpredictable results |\n| **Workflow Complexity** | ✅ Handles simple & complex logic | ✅ Handles complex logic, but requires coding | ❌ Hard to scale beyond simple workflows | ❌ No structured execution |\n| **External API Calls**  | ✅ Simple tool calling | ✅ Explicit tool calling | ✅ Often requires prebuilt integrations | ❌ Manual copy-pasting, no automation |\n| **Scalability** | ✅ Designed for 100s-1000s of playbooks | ✅ No limit, but code complexity grows | ❌ UI becomes unmanageable at scale | ❌ Cannot scale beyond one-off conversations |\n| **Business User Friendly**     | ✅ Yes | ❌ No, requires coding | ❌ No, complex workflow graphs | ❌ No, requires prompt engineering |\n\n## Features\n\n### Write programs in plain English\n- Define AI agent behavior using natural language instead of code\n- Let non-technical team members understand and modify agent behavior\n- Talk with a copilot to improve your natural language programs\n\n### Powerful execution engine\n- Playbooks AI faithfully follows your instructions\n- Build complex behavior using hundreds or thousands of playbooks\n- Easily create multi-agent systems\n- Call external tools with simple language\n- Dynamic triggering to handle special cases and validations\n- Respond to external events\n\n### Build any AI application\n\n- Create a wide range of applications:\n  - Intelligent chatbots\n  - Customer support agents\n  - Virtual assistants\n  - Team automation tools\n  - Workflow automation\n\nWhat will you build with Playbooks AI?\n\n## Quick start\n\n2 easy ways to try Playbooks AI:\n\n1. Visit [runplaybooks.ai](https://runplaybooks.ai) and try out the demo playground, OR\n\n2. On command line\n\n```bash\npip install playbooks\npoetry run python src/playbooks/applications/agent_chat.py src/playbooks/examples/playbooks/chat.md --stream\n```\n\n## Who Should Use Playbooks?\n\nNatural Language Programming with Playbooks AI is perfect for:\n\n✅ **Developers & Engineers** – Create AI agents without writing complex state machines\n\n✅ **Business Teams** – Modify AI behavior without coding or technical expertise\n\n✅ **Product Managers** – Quickly prototype and iterate on AI features\n\n✅ **AI Researchers** – Experiment with multi-agent systems more efficiently\n\n✅ **Automation Specialists** – Build intelligent workflows with API integrations\n\n## Roadmap\n\nWe're just getting started! Here's what's coming next:\n\n- Playbooks Observer for observability and debugging\n- Online planning by generating playbooks\n- Process multiple trigger matches simultaneously\n- Playbooks Hub for community sharing\n- VSCode extension for debugging\n- Copilot for conversational playbook creation\n- Multi-agent communication\n- Inference speed optimizations\n- Tool sandboxing\n- PlaybooksLM fine-tuned model\n- Playbooks Platform with enterprise features\n\n## Contributing\n\nWelcome to the Playbooks community! We're excited to have you contribute. \n\nIf you want to help, checkout some of the issues marked as `good-first-issue` or `help-wanted` found [here](https://github.com/playbooks-ai/playbooks/labels/good%20first%20issue). They could be anything from code improvements, a guest blog post, or a new cookbook.\n\n### Development Environment Setup\n\n1. **Clone the Repository**\n   ```bash\n   git clone https://github.com/playbooks-ai/playbooks.git\n   cd playbooks\n   ```\n\n2. **Environment Variables**\n   Set up environment variables for the playbooks package (`.env`):\n   ```bash\n   cp .env.example .env\n   ```\n\n   Edit `.env` to configure LLM and API settings.\n\n3. **playbooks Python package Setup**\n   ```bash\n   # Create and activate a virtual environment (recommended)\n   \n   python -m venv venv # or conda create -n venv python, or pyenv virtualenv venv\n\n   source venv/bin/activate  # On Windows: venv\\Scripts\\activate\n   \n   # Install playbooks Python package in development mode\n   pip install poetry\n   poetry install\n   ```\n   \n### Testing\n\nWe use pytest for testing. Here's how to run the tests:\n\n1. **Run playbooks Python Package Tests**\n   ```bash\n   pytest\n   ```\n\n### Getting Help\n\n- Join our [Discord community](https://discord.com/channels/1320659147133423667/1320659147133423670)\n- Check existing issues and discussions\n- Reach out to maintainers\n\nWe appreciate your contributions to making Playbooks better! If you have any questions, don't hesitate to ask.\n\n## License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n\n## Contributors\n\n<!-- ALL-CONTRIBUTORS-LIST:START - Do not remove or modify this section -->\n<!-- prettier-ignore-start -->\n<!-- markdownlint-disable -->\n\n<!-- markdownlint-restore -->\n<!-- prettier-ignore-end -->\n\n<!-- ALL-CONTRIBUTORS-LIST:END -->\n\n<a href=\"https://github.com/playbooks-ai/playbooks/graphs/contributors\">\n  <img src=\"https://contrib.rocks/image?repo=playbooks-ai/playbooks\" />\n</a>\n\nThis project is maintained by [Playbooks AI](https://runplaybooks.ai)."
    },
    {
      "name": "richard-gyiko/json-schema-to-pydantic",
      "stars": 3,
      "img": "https://avatars.githubusercontent.com/u/36221820?s=40&v=4",
      "owner": "richard-gyiko",
      "repo_name": "json-schema-to-pydantic",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2024-12-20T07:49:42Z",
      "updated_at": "2025-04-07T18:13:36Z",
      "topics": [],
      "readme": "# JSON Schema to Pydantic\n\nA Python library for automatically generating Pydantic v2 models from JSON Schema definitions.\n\n![PyPI - Version](https://img.shields.io/pypi/v/json-schema-to-pydantic)\n[![codecov](https://codecov.io/github/richard-gyiko/json-schema-to-pydantic/graph/badge.svg?token=YA2Y769H1K)](https://codecov.io/github/richard-gyiko/json-schema-to-pydantic)\n\n## Features\n\n- Converts JSON Schema to Pydantic v2 models\n- Supports complex schema features including:\n  - References ($ref) with circular reference detection\n  - Combiners (allOf, anyOf, oneOf) with proper type discrimination\n  - Type constraints and validations\n  - Array and object validations\n  - Format validations (email, uri, uuid, date-time)\n- Full type hinting support\n- Clean, simple API\n\n## Installation\n\n```bash\npip install json-schema-to-pydantic\n```\n\n## Development Setup\n\n1. Clone the repository\n2. Install development dependencies:\n```bash\n# Using uv (recommended)\nuv pip install -e \".[dev]\"\n\n# Or using pip\npip install -e \".[dev]\"\n```\n\n3. Run tests:\n```bash\npytest\n```\n\n## Quick Start\n\n```python\nfrom json_schema_to_pydantic import create_model\n\n# Define your JSON Schema\nschema = {\n    \"title\": \"User\",\n    \"type\": \"object\",\n    \"properties\": {\n        \"name\": {\"type\": \"string\"},\n        \"email\": {\"type\": \"string\", \"format\": \"email\"},\n        \"age\": {\"type\": \"integer\", \"minimum\": 0}\n    },\n    \"required\": [\"name\", \"email\"]\n}\n\n# Generate your Pydantic model\nUserModel = create_model(schema)\n\n# Use the model\nuser = UserModel(\n    name=\"John Doe\",\n    email=\"john@example.com\",\n    age=30\n)\n\n# Example with relaxed array item validation\nRelaxedModel = create_model(\n    {\"type\": \"object\", \"properties\": {\"tags\": {\"type\": \"array\"}}},\n    allow_undefined_array_items=True\n)\nrelaxed_instance = RelaxedModel(tags=[1, \"two\", True])\n```\n\n## Advanced Usage\n\nFor more complex scenarios, you can use the `PydanticModelBuilder` directly:\n\n```python\nfrom json_schema_to_pydantic import PydanticModelBuilder\n\nbuilder = PydanticModelBuilder()\nmodel = builder.create_pydantic_model(schema, root_schema)\n```\n\n## Error Handling\n\nThe library provides specific exceptions for different error cases:\n\n```python\nfrom json_schema_to_pydantic import (\n    SchemaError,     # Base class for all schema errors\n    TypeError,       # Invalid or unsupported type\n    CombinerError,   # Error in schema combiners\n    ReferenceError,  # Error in schema references\n)\n\ntry:\n    model = create_model(schema)\nexcept TypeError as e:\n    print(f\"Invalid type in schema: {e}\")\nexcept ReferenceError as e:\n    print(f\"Invalid reference: {e}\")\n```\n\n## Documentation\n\nSee [docs/features.md](docs/features.md) for detailed documentation of supported JSON Schema features.\n\n## Contributing\n\n1. Fork the repository\n2. Create a new branch for your feature\n3. Make your changes\n4. Run tests and ensure they pass\n5. Submit a pull request\n\n## License\n\nThis project is licensed under the terms of the license included in the repository.\n"
    },
    {
      "name": "MaxAkbar/Local.NET.LLM",
      "stars": 3,
      "img": "https://avatars.githubusercontent.com/u/13856299?s=40&v=4",
      "owner": "MaxAkbar",
      "repo_name": "Local.NET.LLM",
      "description": null,
      "homepage": null,
      "language": "C#",
      "created_at": "2024-12-26T08:05:18Z",
      "updated_at": "2025-04-07T07:03:13Z",
      "topics": [],
      "readme": ""
    },
    {
      "name": "aevatarAI/AISmart",
      "stars": 3,
      "img": "https://avatars.githubusercontent.com/u/190360083?s=40&v=4",
      "owner": "aevatarAI",
      "repo_name": "AISmart",
      "description": "AI Agent Framework based on ABP + Orleans and others",
      "homepage": "",
      "language": "JavaScript",
      "created_at": "2024-12-02T11:19:05Z",
      "updated_at": "2025-01-08T06:26:34Z",
      "topics": [],
      "readme": "# AI Agent Framework based on ABP + Orleans and others\n\nAI Smart project for ABP Framework and Orleans backend.\n\n- [About The Project](#about-the-project)\n- [Getting Started](#getting-started)\n    - [Prerequisites](#prerequisites)\n    - [Running the Application](#running-the-application)\n- [Examples](#examples)\n- [Contributing](#contributing)\n- [License](#license)\n\n## About The Project\n\nAI Smart project for ABP 8 Framework and Orleans 7 backend.\n## Getting Started\n\n### Prerequisites\n\n- .NET 8.0 SDK\n- MongoDB\n- Redis\n\n## Configuration\n\n1. Update the `appsettings.json` file in the Silo project with your specific configurations (e.g., connection strings, Orleans clustering configurations).\n\n    ```json\n    {\n      \"ConnectionStrings\": {\n        \"Default\": \"mongodb://localhost:27017/AISmart\"\n      },\n      \"Orleans\": {\n        \"ClusterId\": \"AISmartSiloCluster\",\n        \"ServiceId\": \"AISmartBasicService\",\n        \"AdvertisedIP\": \"127.0.0.1\",\n        \"GatewayPort\": 20001,\n        \"SiloPort\": 10001,\n        \"MongoDBClient\": \"mongodb://localhost:27017/?maxPoolSize=555\",\n        \"DataBase\": \"AISmartDb\",\n        \"DashboardUserName\": \"admin\",\n        \"DashboardPassword\": \"123456\",\n        \"DashboardCounterUpdateIntervalMs\": 1000,\n        \"DashboardPort\": 8080,\n        \"EventStoreConnection\": \"ConnectTo=tcp://localhost:1113; HeartBeatTimeout=500\",\n        \"ClusterDbConnection\": \"127.0.0.1:6379\",\n        \"ClusterDbNumber\": 0,\n        \"GrainStorageDbConnection\": \"127.0.0.1:6379\",\n        \"GrainStorageDbNumber\": 0\n      }\n    }\n    ```\n\n2. Update the `appsettings.json` file in the HttpApi.Host project with your specific configurations (e.g., connection strings, Orleans clustering configurations).\n\n    ```json\n    {\n      \"ConnectionStrings\": {\n        \"Default\": \"mongodb://localhost:27017/AISmart\"\n      },\n      \"Orleans\": {\n        \"ClusterId\": \"AISmartSiloCluster\",\n        \"ServiceId\": \"AISmartBasicService\",\n        \"MongoDBClient\": \"mongodb://localhost:27017/?maxPoolSize=555\",\n        \"DataBase\": \"AISmartDb\"\n      }\n    }\n    ```\n\n### Running the Application\n\n1. Go to the `src` folder\n    ```shell\n    cd src\n    ```\n2. Run the `<Your Project Name>.DbMigrator` project to create the initial database from `src`.\n    ```shell\n    cd <Your Project Name>.DbMigrator\n    dotnet run\n    ```\n3. Run the `<Your Project Name>.Silo` project to start the Orleans Silo from `src`.\n    ```shell\n    cd ../<Your Project Name>.Silo\n    dotnet run\n    ```\n4. Run the `<Your Project Name>.HttpApi.Host` project to start the API from `src`.\n    ```shell\n    cd ../<Your Project Name>.HttpApi.Host\n    dotnet run\n    ```\n\n## Examples\n\nYou can find examples from the `Books` or `Authors` folders in respective projects.\n\n## Contributing\n\nIf you encounter a bug or have a feature request, please use the [Issue Tracker](https://github.com/AElfProject/aelf-dapp-factory/issues/new). The project is also open to contributions, so feel free to fork the project and open pull requests.\n\n## License\n\nDistributed under the Apache License. See [License](LICENSE) for more information.\nDistributed under the MIT License. See [License](LICENSE) for more information.\n"
    },
    {
      "name": "devcodemonkey/de.devcodemonkey.AIChecker",
      "stars": 3,
      "img": "https://avatars.githubusercontent.com/u/77974465?s=40&v=4",
      "owner": "devcodemonkey",
      "repo_name": "de.devcodemonkey.AIChecker",
      "description": "The program is designed to compare various Large Language Models (LLMs) based on their performance, accuracy, and resource efficiency. This project is maintained and developed by David Höll",
      "homepage": "https://devcodemonkey.de/AiChecker/",
      "language": "C#",
      "created_at": "2024-09-14T00:40:58Z",
      "updated_at": "2025-01-20T20:23:25Z",
      "topics": [],
      "readme": "# Project Description\n\nThis project is currently in a **beta program**.\n\nThe beta releases are deployed to: [https://devcodemonkey.de/AiChecker/](https://devcodemonkey.de/AiChecker/)\n\nThe program is designed to compare various Large Language Models (LLMs) based on their performance, accuracy, and resource efficiency. The primary goal is to evaluate LLMs across multiple dimensions, including inference speed, model size, and fine-tuning capabilities. The program can connect to:\n\n- LM Studio API\n- OpenAI API\n\n# Third-Party Licenses\n\nThis software uses third-party libraries, which can be looked up under the following link: [Third-Party Licenses](https://github.com/devcodemonkey/de.devcodemonkey.AIChecker/blob/main/THIRD_PARTY_LICENSES.md)\n\n## Commands\n\nTo get help, simply run `AIChecker.exe` followed by the desired command.\n\n### Supported Commands\n\n1. **help**\n\n   - **Description**: Display more information on a specific command.\n   - **Command**: `help <command>`\n\n2. **info**\n\n   - **Description**: Shows information about the developer of the AiChecker.\n   - **Command**: `info`\n\n3. **version**\n\n   - **Description**: Display version information.\n   - **Command**: `version`\n\n4. **recreateDatabase**\n\n   - **Description**: Recreates the database.\n   - **Command**: `recreateDatabase [-f | --force]`\n     - `-f, --force`: Force recreation of the database.\n\n5. **viewResultSets**\n\n   - **Description**: View all result sets.\n   - **Command**: `viewResultSets`\n\n6. **viewResults**\n\n   - **Description**: Displays the results of a specific result set with optional formatting.\n   - **Command**: `viewResults -r <ResultSet> [-f | --formatTable]`\n     - `-r, --resultSet`: Specifies the result set to view.\n     - `-f, --formatTable`: Format the table for better viewing.\n\n7. **viewAverage**\n\n   - **Description**: Displays the average time of API requests for a specific result set.\n   - **Command**: `viewAverage -r <ResultSet>`\n     - `-r, --resultSet`: Specifies the result set for which the average time is displayed.\n\n8. **importQuestions**\n\n   - **Description**: Imports questions and answers into the database from a file.\n   - **Command**: `importQuestions -p <Path> -c <Category>`\n     - `-p, --path`: Path to the file containing questions and answers.\n     - `-c, --category`: Category of the questions and answers.\n\n9. **importQuestionsFromResults**\n\n   - **Description**: Imports questions from results into the database.\n   - **Command**: `importQuestionsFromResults -r <ResultSet> -c <Category>`\n     - `-r, --resultSet`: Name of the result set.\n     - `-c, --category`: Category of the questions.\n\n10. **deleteAllQuestions**\n\n    - **Description**: Deletes all questions and answers from the database.\n    - **Command**: `deleteAllQuestions`\n\n11. **deleteResultSet**\n\n    - **Description**: Deletes a specific result set from the database.\n    - **Command**: `deleteResultSet -r <ResultSet>`\n    - `-r, --resultSet`: Name of the result set.\n\n12. **createMoreQuestions**\n\n    - **Description**: Creates more questions based on a system prompt and saves them under a specified result set.\n    - **Command**: `createMoreQuestions -r <ResultSet> -s <SystemPrompt> -m <Model> -c <Category> [-t <MaxTokens>] [-p <Temperature>]`\n    - `-r, --resultSet`: Name of the result set.\n    - `-s, --systemPrompt`: System prompt or file path (path:<file-path>).\n    - `-m, --model`: Name of the model.\n    - `-c, --category`: Category for the questions.\n    - `-t, --maxTokens`: Maximum number of tokens (optional).\n    - `-p, --temperature`: Temperature value (optional).\n\n13. **sendToLMS**\n\n    - **Description**: Sends an API request to LmStudio and saves the result to the database.\n    - **Command**: `sendToLms -m <Message> -r <ResultSet> -s <SystemPrompt> [-t <MaxTokens>] [-p <Temperature>] [-c <RequestCount>] [-u] [-i <SaveInterval>] [-w] [--environmentTokenName <Name>] [--source <URL>] [--model <Model>]`\n    - `-m, --message`: User message.\n    - `-r, --resultSet`: Name of the result set.\n    - `-s, --systemPrompt`: System prompt (default: \"You are a helpful assistant\").\n    - `-t, --maxTokens`: Maximum number of tokens (optional).\n    - `-p, --temperature`: Temperature value (optional).\n    - `-c, --requestCount`: Number of requests to send (optional).\n    - `-u, --saveProcessUsage`: Save process usage statistics.\n    - `-i, --saveInterval`: Interval in seconds to save process usage data.\n    - `-w, --writeOutput`: Write the process output to the console.\n    - `--environmentTokenName`: Environment token name for the API.\n    - `--source`: Source URL for the API request.\n    - `--model`: Model name for the API request.\n\n14. **rankPrompt**\n\n    - **Description**: Tests prompts and creates a ranking for them.\n    - **Command**: `rankPrompt -r <ResultSet> -m <Models> -p <PromptRequirements> [-t <MaxTokens>]`\n    - `-r, --resultSet`: Name of the result set.\n    - `-m, --models`: Comma-separated list of models to test.\n    - `-p, --promptRequirements`: Requirements for the prompts.\n    - `-t, --maxTokens`: Maximum number of tokens (optional).\n\n15. **exportPromptRank**\n\n    - **Description**: Exports the ranking of the prompts.\n    - **Command**: `exportPromptRank -r <ResultSet> [-t <FileType>] [-o]`\n    - `-r, --resultSet`: Name of the result set.\n    - `-t, --fileType`: File type for export (default: PDF; options: PDF, HTML, Docx, Markdown).\n    - `-o, --notOpenFolder`: Do not open the folder after export.\n\n16. **database**\n\n    - **Description**: Manage the database (start, stop, backup, restore, recreate).\n    - **Command**: `database [-s | --stop] [-r | --start] [-b | --backup] [-o | --restore] [--branch <Branch>] [--recreateDatabase] [-f | --force]`\n    - `-s, --stop`: Stop the database.\n    - `-r, --start`: Start the database.\n    - `-b, --backup`: Backup the database.\n    - `-o, --restore`: Restore the database.\n    - `--branch`: Branch to restore the database from.\n    - `--recreateDatabase`: Recreate the database.\n    - `-f, --force`: Force the operation.\n\n17. **model**\n\n    - **Description**: Manage models (view, add, load, unload).\n    - **Command**: `model [-v | --view] [-a | --add] [-l | --load] [-u | --unload]`\n    - `-v, --view`: View all models.\n    - `-a, --add`: Add a new model.\n    - `-l, --load`: Load a model.\n    - `-u, --unload`: Unload a model.\n\n18. **checkJson**\n\n    - **Description**: Check the JSON format of the results.\n    - **Command**: `checkJson [-r <ResultSet>] [-o | --showOutput]`\n    - `-r, --resultSet`: Name of the result set to check.\n    - `-o, --showOutput`: Display the output of the results.\n\n19. **viewProcessUsage**\n    - **Description**: View the usage of each process.\n    - **Command**: `viewProcessUsage`\n"
    },
    {
      "name": "StormHub/TinyAgent",
      "stars": 2,
      "img": "https://avatars.githubusercontent.com/u/16412512?s=40&v=4",
      "owner": "StormHub",
      "repo_name": "TinyAgent",
      "description": "Agent Demo",
      "homepage": "",
      "language": "C#",
      "created_at": "2024-05-23T23:58:18Z",
      "updated_at": "2025-03-29T00:27:54Z",
      "topics": [
        "agents",
        "ai",
        "artificial-intelligence",
        "llm",
        "semantic-kernel"
      ],
      "readme": ""
    },
    {
      "name": "LittleLittleCloud/weather-report-agent",
      "stars": 2,
      "img": "https://avatars.githubusercontent.com/u/16876986?s=40&v=4",
      "owner": "LittleLittleCloud",
      "repo_name": "weather-report-agent",
      "description": null,
      "homepage": null,
      "language": "C#",
      "created_at": "2024-07-29T21:53:03Z",
      "updated_at": "2025-01-26T11:54:38Z",
      "topics": [],
      "readme": "# Weather report agent example\n\nThis example demos how to invoke tool call with local model using Ollama and AutoGen.Net\n\n## Prerequisites\n- Ollama >= 0.3.0\n- dotnet sdk >= 8.0\n\n## How to run\n1. Clone this repository\n2. start `ollama` server. Suppose the server is running at `http://localhost:11434`\n3. run `dotnet run` in this directory. And you will see the output like below:\n\n```\n\nTextMessage from assistant\n--------------------\nThe current temperature in New York is 25°C (77°F), and it's a sunny day.\n--------------------\n```\n\n## Further reading\n- [Ollama](https://ollama.com/)\n- [AutoGen.Net](https://microsoft.github.io/autogen-for-net/index.html)"
    },
    {
      "name": "jsboige/CoursIA",
      "stars": 2,
      "img": "https://avatars.githubusercontent.com/u/3159389?s=40&v=4",
      "owner": "jsboige",
      "repo_name": "CoursIA",
      "description": "Dépôt contenant les ressources et TPs d'un cours en IA en C# et en Python",
      "homepage": "",
      "language": "Jupyter Notebook",
      "created_at": "2022-08-19T13:38:08Z",
      "updated_at": "2025-04-02T15:02:12Z",
      "topics": [],
      "readme": "# 📘 CoursIA\r\n\r\nBienvenue dans le dépôt **CoursIA**, qui contient les ressources et TPs pour le cours d'intelligence artificielle en C# et Python.\r\n\r\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\r\n\r\n## 📋 Table des matières\r\n\r\n- [Introduction](#introduction)\r\n- [Structure du dépôt](#structure-du-dépôt)\r\n- [Mise en route](#mise-en-route)\r\n- [Parcours d'apprentissage](#parcours-dapprentissage)\r\n- [Contenu des modules](#contenu-des-modules)\r\n- [Contribution](#contribution)\r\n- [Licence](#licence)\r\n\r\n## 🚀 Introduction\r\n\r\nCe dépôt contient un ensemble de notebooks Jupyter interactifs et de ressources pour l'apprentissage de l'intelligence artificielle, couvrant un large éventail de sujets allant du machine learning classique aux techniques d'IA générative modernes, en passant par l'IA symbolique et les algorithmes de recherche.\r\n\r\nLes notebooks sont principalement en C# (utilisant .NET Interactive) et Python, offrant une approche pratique et hands-on pour comprendre les concepts d'IA.\r\n\r\n## 🗂️ Structure du dépôt\r\n\r\nLe dépôt est organisé en plusieurs sections thématiques :\r\n\r\n```\r\nMyIA.AI.Notebooks/\r\n├── GenAI/               # IA Générative (OpenAI, LLMs, RAG, etc.)\r\n├── ML/                  # Machine Learning avec ML.NET\r\n├── IIT/                 # Integrated Information Theory\r\n├── Probas/              # Probabilités et inférence bayésienne\r\n├── Search/              # Algorithmes de recherche et optimisation\r\n├── Sudoku/              # Résolution de Sudoku avec différentes approches\r\n├── SymbolicAI/          # IA Symbolique (RDF, Z3, OR-Tools)\r\n└── Config/              # Configuration pour les notebooks\r\n```\r\n\r\n## 🛠️ Mise en route\r\n\r\n### Prérequis\r\n\r\nAvant de commencer, assurez-vous d'avoir installé :\r\n\r\n- [Python 3.9+](https://www.python.org/downloads/)\r\n- [Visual Studio Code](https://code.visualstudio.com/)\r\n- L'extension **Python** et **Jupyter** dans VSCode\r\n- [Extension **.Net extension pack**](https://marketplace.visualstudio.com/items?itemName=ms-dotnettools.vscode-dotnet-pack)\r\n- [.NET 9.0 SDK](https://dotnet.microsoft.com/download) (pour les notebooks C#)\r\n- [OpenAI API key](https://platform.openai.com/signup/) (pour les notebooks GenAI)\r\n\r\n### Installation des dépendances\r\n\r\n1. **Créer et activer un environnement virtuel Python :**\r\n\r\n   ```sh\r\n   python -m venv venv\r\n   source venv/bin/activate  # macOS/Linux\r\n   venv\\Scripts\\activate      # Windows\r\n   ```\r\n\r\n2. **Installer Jupyter et les bibliothèques nécessaires :**\r\n\r\n   ```sh\r\n   pip install --upgrade pip\r\n   pip install jupyter openai\r\n   ```\r\n\r\n3. **Ajouter l'environnement à Jupyter :**\r\n\r\n   ```sh\r\n   python -m ipykernel install --user --name=coursia --display-name \"Python (CoursIA)\"\r\n   ```\r\n\r\n4. **Configurer les clés API (pour GenAI) :**\r\n   \r\n   Créez un fichier `.env` dans le dossier `MyIA.AI.Notebooks/GenAI/` en vous basant sur le fichier `.env.example`.\r\n\r\n### Configuration pour les notebooks C#\r\n\r\nPour les notebooks C#, vous devez également :\r\n\r\n1. **Restaurer les packages NuGet :**\r\n\r\n   ```sh\r\n   dotnet restore MyIA.CoursIA.sln\r\n   ```\r\n\r\n2. **Configurer les paramètres API :**\r\n   \r\n   Copiez `MyIA.AI.Notebooks/Config/settings.json.openai-example` vers `MyIA.AI.Notebooks/Config/settings.json` et ajoutez votre clé API.\r\n\r\n## 🎓 Parcours d'apprentissage\r\n\r\nVoici un parcours d'apprentissage suggéré pour explorer ce dépôt :\r\n\r\n1. **Introduction au Machine Learning** - Commencez par les notebooks dans `ML/`\r\n2. **Algorithmes de recherche** - Explorez les notebooks dans `Search/` et `Sudoku/`\r\n3. **IA Symbolique** - Découvrez les notebooks dans `SymbolicAI/`\r\n4. **Probabilités et inférence** - Étudiez les notebooks dans `Probas/`\r\n5. **IA Générative** - Terminez avec les notebooks dans `GenAI/`\r\n\r\n## 📚 Contenu des modules\r\n\r\n### 🤖 GenAI (IA Générative)\r\n\r\nNotebooks sur l'IA générative, les grands modèles de langage (LLMs), et les techniques associées :\r\n\r\n- `OpenAI_Intro.ipynb` - Introduction à l'API OpenAI\r\n- `PromptEngineering.ipynb` - Techniques d'ingénierie de prompts\r\n- `RAG.ipynb` - Retrieval Augmented Generation\r\n- `LocalLlama.ipynb` - Utilisation de modèles locaux comme Llama\r\n- `SemanticKernel/` - Notebooks sur Microsoft Semantic Kernel\r\n\r\n### 📊 ML (Machine Learning)\r\n\r\nSérie de notebooks sur le machine learning avec ML.NET :\r\n\r\n- `ML-1-Introduction.ipynb` - Introduction au ML avec ML.NET\r\n- `ML-2-Data&Features.ipynb` - Préparation des données et ingénierie des caractéristiques\r\n- `ML-3-Entrainement&AutoML.ipynb` - Entraînement de modèles et AutoML\r\n- `ML-4-Evaluation.ipynb` - Évaluation des modèles\r\n- `TP-prevision-ventes.ipynb` - TP sur la prévision des ventes\r\n\r\n### 🧩 Sudoku\r\n\r\nNotebooks illustrant différentes approches pour résoudre des Sudokus :\r\n\r\n- `Sudoku-0-Environment.ipynb` - Mise en place de l'environnement\r\n- `Sudoku-1-Backtracking.ipynb` - Résolution par backtracking\r\n- `Sudoku-2-Genetic.ipynb` - Algorithmes génétiques\r\n- `Sudoku-3-ORTools.ipynb` - Utilisation d'OR-Tools\r\n- `Sudoku-4-Z3.ipynb` - Résolution avec le solveur Z3\r\n- `Sudoku-5-DancingLinks.ipynb` - Algorithme de Dancing Links\r\n- `Sudoku-6-Infer.ipynb` - Inférence probabiliste\r\n\r\n### 🔍 Search (Recherche)\r\n\r\nNotebooks sur les algorithmes de recherche et d'optimisation :\r\n\r\n- `GeneticSharp-EdgeDetection.ipynb` - Détection de contours avec algorithmes génétiques\r\n- `Portfolio_Optimization_GeneticSharp.ipynb` - Optimisation de portefeuille\r\n- `PyGad-EdgeDetection.ipynb` - Détection de contours avec PyGad\r\n\r\n### 🧠 SymbolicAI (IA Symbolique)\r\n\r\nNotebooks sur l'IA symbolique et les approches formelles :\r\n\r\n- `Linq2Z3.ipynb` - Utilisation du solveur Z3 avec LINQ\r\n- `OR-tools-Stiegler.ipynb` - Résolution de problèmes avec OR-Tools\r\n- `RDF.Net/` - Utilisation de RDF avec .NET\r\n\r\n### 🔢 Probas (Probabilités)\r\n\r\nNotebooks sur les probabilités et l'inférence bayésienne :\r\n\r\n- `Infer-101.ipynb` - Introduction à l'inférence probabiliste\r\n\r\n### 🧪 IIT (Integrated Information Theory)\r\n\r\nNotebooks sur la théorie de l'information intégrée :\r\n\r\n- `Intro_to_PyPhi.ipynb` - Introduction à PyPhi pour IIT\r\n\r\n## 👥 Contribution\r\n\r\nLes contributions à ce dépôt sont les bienvenues ! Si vous souhaitez contribuer :\r\n\r\n1. Forkez le dépôt\r\n2. Créez une branche pour votre fonctionnalité (`git checkout -b feature/nouvelle-fonctionnalite`)\r\n3. Committez vos changements (`git commit -m 'Ajout d'une nouvelle fonctionnalité'`)\r\n4. Poussez vers la branche (`git push origin feature/nouvelle-fonctionnalite`)\r\n5. Ouvrez une Pull Request\r\n\r\n## 📄 Licence\r\n\r\nCe projet est sous licence MIT - voir le fichier [LICENSE](LICENSE) pour plus de détails.\r\n\r\n---\r\n\r\n🚀 Bon apprentissage et bonnes expérimentations avec l'IA !\r\n"
    },
    {
      "name": "feynmanloo/AutoGen.BasicSamples",
      "stars": 2,
      "img": "https://avatars.githubusercontent.com/u/20107191?s=40&v=4",
      "owner": "feynmanloo",
      "repo_name": "AutoGen.BasicSamples",
      "description": "All samples base on SemanticKernel",
      "homepage": null,
      "language": "C#",
      "created_at": "2024-05-15T03:57:24Z",
      "updated_at": "2024-10-02T09:20:46Z",
      "topics": [],
      "readme": "﻿# AutoGen.BasicSamples\r\n"
    },
    {
      "name": "AndreRatzenberger/DurableFunctions.SemanticKernel",
      "stars": 2,
      "img": "https://avatars.githubusercontent.com/u/44863088?s=40&v=4",
      "owner": "AndreRatzenberger",
      "repo_name": "DurableFunctions.SemanticKernel",
      "description": "This project serves as an interactive tutorial series showcasing the integration of Azure Durable Functions with Semantic Kernel, enhanced with a Flask CLI frontend. Each part of the series introduces new functionalities, from simple Q&A agents to complex orchestrations, demonstrating the power of combining cloud computing with AI.  ",
      "homepage": "",
      "language": "C#",
      "created_at": "2024-03-19T23:09:37Z",
      "updated_at": "2024-04-12T14:46:17Z",
      "topics": [
        "azure",
        "azure-function",
        "durable-functions",
        "flask",
        "semantic-kernel",
        "semantickernel"
      ],
      "readme": "\n# .NET 8 Isolated - Durable Functions + Semantic Kernel Integration Project\n\nThis repository hosts multiple tutorial projects demonstrating the integration of Azure Durable Functions as .NET 8 isolated implementation and using Semantic Kernel powered Agents to orchestrate complex tasks with the ability of self-improvement of Agents and how to orchestrate them.\n\nIt focuses on constructing Language Learning Model (LLM)-based agents, specifically showcasing the tools Semantic Kernel does offer and how to realize seamless integration and combination with the Durable Functions framework.\n\nThe goal is to familiarize developers with the core concepts of Semantic Kernel and the orchestration capabilities of Durable Functions, allowing for sophisticated management of LLM interactions.\n\n## Quick overview of all examples\n\n### Example 01 - Simple Prompt Agent\n\nSimple Durable Functions Project implementing the basics of Durable Functions and building a SemanticKernel Kernel to act as a small agent that can answer simple prompts and questions.\n\n## More Info\n\nFor information about the examples take a look at the README.md in the example folder.\n\n## Future Directions\n\nI don't know yet where the journey leads us to but potential for expanding this project is vast, including:\n\n- **Advanced Agent Capabilities**: Explore adding agents with specialized skills or knowledge areas, such as language translation, code generation, or data analysis.\n- **Enhanced Orchestration Logic**: Implement more complex orchestration logic that allows agents to collaborate on tasks, pass messages, and make decisions based on previous interactions.\n- **Semantic Kernel Extensions**: Delve into developing custom plugins for Semantic Kernel, potentially opening new avenues for task management, scheduling, or integrating with third-party services.\n"
    },
    {
      "name": "A-Ravioli/einstein",
      "stars": 2,
      "img": "https://avatars.githubusercontent.com/u/68564296?s=40&v=4",
      "owner": "A-Ravioli",
      "repo_name": "einstein",
      "description": "open-source co-scientist",
      "homepage": null,
      "language": "Python",
      "created_at": "2025-04-10T12:52:24Z",
      "updated_at": "2025-04-16T20:39:30Z",
      "topics": [],
      "readme": "# Einstein: AI Co-scientist System\n\nA multi-agent scientific hypothesis generation and evaluation system built on Microsoft's AutoGen framework.\n\n## Overview\n\nEinstein is an implementation of an AI Co-scientist system, designed to help researchers generate, evaluate, and refine scientific hypotheses. The system uses a team of specialized AI agents, each focusing on different aspects of the scientific process:\n\n- **Supervisor**: Orchestrates the research workflow and coordinates other agents\n- **Generation**: Creates novel, testable scientific hypotheses\n- **Reflection**: Reviews and critiques hypotheses with scientific rigor\n- **Ranking**: Compares hypotheses through simulated scientific debates\n- **Evolution**: Improves hypotheses based on feedback and combines strengths\n- **Meta-Review**: Synthesizes insights and provides research overviews\n\nThe system follows a structured workflow:\n1. Parse research goals into structured configurations\n2. Generate initial hypotheses\n3. Review and critique each hypothesis\n4. Conduct a tournament to rank hypotheses (using an Elo-based rating system)\n5. Improve the top-ranking hypotheses\n6. Combine improved hypotheses into a unified hypothesis\n7. Generate a comprehensive research overview\n\n## Installation\n\nInstall the package with pip:\n\n```\npip install -e .\n```\n\nOr install the dependencies separately:\n\n```\npip install -r requirements.txt\n```\n\n## Usage\n\n### Quick Start\n\n```python\nimport asyncio\nfrom einstein_pkg import AICoScientist\n\nasync def main():\n    # Initialize the AI Co-scientist with your OpenAI API key\n    co_scientist = AICoScientist(api_key=\"your-api-key\")\n    \n    # Process a research goal\n    research_goal = \"Develop a novel hypothesis for the key factor contributing to ALS progression.\"\n    results = await co_scientist.process_research_goal(research_goal)\n    \n    # Print the research overview\n    print(results[\"research_overview\"][\"content\"])\n    \n    # Add scientist feedback\n    feedback = \"I have a hypothesis that TDP-43 aggregation might be driven by impaired autophagy.\"\n    contribution = await co_scientist.add_scientist_feedback(feedback)\n    \n    # Clean up resources\n    await co_scientist.close()\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n### Command Line Example\n\nRun the included example script:\n\n```\npython examples/basic_example.py --api-key your-api-key --model gpt-4-turbo\n```\n\nOr use a .env file with your API key and run without the --api-key parameter:\n\n```\necho \"OPENAI_API_KEY=your-api-key\" > .env\npython examples/basic_example.py\n```\n\n## Package Structure\n\n```\neinstein_pkg/\n├── einstein_pkg/           # Main package\n│   ├── __init__.py         # Package initialization\n│   ├── config.py           # Configuration handling\n│   ├── main.py             # Main AICoScientist class\n│   ├── memory.py           # Context memory persistence\n│   ├── agents/             # Agent implementations\n│   │   ├── __init__.py\n│   │   ├── base.py         # Base agent class\n│   │   ├── supervisor.py   # Research supervisor agent\n│   │   ├── generation.py   # Hypothesis generation agent\n│   │   ├── reflection.py   # Hypothesis review agent\n│   │   ├── ranking.py      # Hypothesis ranking agent\n│   │   ├── evolution.py    # Hypothesis improvement agent\n│   │   └── meta_review.py  # Research synthesis agent\n│   ├── models/             # Domain models\n│   │   ├── __init__.py\n│   │   └── research_config.py  # Research goal configuration\n│   └── tools/              # AutoGen tools\n│       ├── __init__.py\n│       ├── literature_search.py  # Literature search tool\n│       └── research_goal.py      # Research goal parsing tool\n├── examples/               # Example scripts\n│   └── basic_example.py    # Basic usage example\n├── setup.py                # Package setup\n└── requirements.txt        # Dependencies\n```\n\n## Configuration\n\nEinstein can be configured through:\n\n1. **Constructor parameters**: Pass configuration directly when creating the AICoScientist instance\n2. **Environment variables**: Set configuration in environment or .env file\n3. **Configuration dictionary**: Use the configure() function to set global configuration\n\nCommon configuration options:\n- `openai_api_key`: Your OpenAI API key\n- `default_model`: Model to use (default: \"gpt-4-turbo\")\n- `context_memory_path`: Path to store context memory (default: \"context_memory.json\")\n\n## Advanced Usage\n\n### Custom Memory Path\n\n```python\nco_scientist = AICoScientist(context_memory_path=\"my_research_memory.json\")\n```\n\n### Clearing Memory\n\n```python\nco_scientist.clear_memory()\n```\n\n### Getting Latest Results\n\n```python\nlatest_results = co_scientist.get_latest_results()\n```\n\n## License\n\nThis project is licensed under the MIT License - see the LICENSE file for details.\n\n## Acknowledgments\n\nThis project was inspired by and implements concepts from the paper \"Towards an AI Co-scientist for Accelerating Scientific Discoveries\" and uses Microsoft's AutoGen framework for multi-agent communication. "
    },
    {
      "name": "pasangimhana/sql_agent",
      "stars": 2,
      "img": "https://avatars.githubusercontent.com/u/85233319?s=40&v=4",
      "owner": "pasangimhana",
      "repo_name": "sql_agent",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2025-04-06T20:42:10Z",
      "updated_at": "2025-04-08T05:38:25Z",
      "topics": [],
      "readme": ""
    },
    {
      "name": "ckane/ctigor",
      "stars": 2,
      "img": "https://avatars.githubusercontent.com/u/3454769?s=40&v=4",
      "owner": "ckane",
      "repo_name": "ctigor",
      "description": "Your friendly neighborhood LLM-enabled CTI Assistant",
      "homepage": null,
      "language": "Python",
      "created_at": "2025-03-22T22:34:33Z",
      "updated_at": "2025-04-14T22:46:24Z",
      "topics": [],
      "readme": "# CTIgor\n\nA CTI assistant. Read my blog entries here:\n* https://blog.malware.re/2025/03/28/ctigor/index.html\n\n![Igor from Young Frankenstein](/static/igor.gif)\n\n## Variants\n\nThere are a number of Agentic frameworks out there. I am attempting to maintain this project to work\nboth under [AutoGen](https://microsoft.github.io/autogen/stable//index.html) and\n[Semantic Kernel](https://learn.microsoft.com/en-us/semantic-kernel/overview/). The main project will\nbe developed on AutoGen, with some features being ported to the Semantic Kernel branch as time and framework\ncapabilities permit.\n\nThe `main` branch will be using AutoGen.\n\nTo see the Semantic Kernel implementation, use the [`semantic-kernel-port`](https://github.com/ckane/ctigor/tree/semantic-kernel-port)\nbranch.\n"
    },
    {
      "name": "shigeyf/azure-ai-foundry-workshop",
      "stars": 2,
      "img": "https://avatars.githubusercontent.com/u/6917167?s=40&v=4",
      "owner": "shigeyf",
      "repo_name": "azure-ai-foundry-workshop",
      "description": "Azure AI Foundry workshop for beginners.",
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2025-02-17T08:39:36Z",
      "updated_at": "2025-04-22T14:20:56Z",
      "topics": [],
      "readme": "# Azure AI Foundry Workshop: From Beginner to Intermediate\n\n[English](./README.md) | [日本語](./README.ja.md)\n\nThis repository contains learning materials for developing and building your own AI applications on Azure using the Azure AI Foundry service.\n\nPlease open VSCode workspace [`default`](./default.code-workspace) file when you will learn on your local computer.\n\nThe workspace is organized into the following sections:\n\n## Deployments\n\n### Terraform\n\nThe `deployments/terraform` folder contains Terraform scripts to deploy Azure AI Foundry resources. Refer to the [README](./infra/terraform/README.md) in this folder for detailed instructions on deployment.\n\n## SDK Quick-Starts\n\nThe `srcs/sdk/quick-starts` folder contains a series of quick-start examples to help you get started with Azure AI Foundry SDK to learn application development using the Azure AI Foundry SDK step-by-step with Jupyter Notebooks.\n\nEach subfolder focuses on a specific use case:\n\n- Learn sample application development with notebook\n  - Includes the following:\n    - [0. Basics of Azure AI Foundry](./srcs/sdk/quick-starts/00_basics/)\n      - Basic examples to understand the SDK's core functionalities.\n    - [1. Building a Simple AI Inference Chat Application with Azure AI Foundry](./srcs/sdk/quick-starts/01_simple_inference_chat/)\n      - Demonstrates simple inference chat capabilities.\n    - [2. Building a Simple AI Inference Chat Application with Azure AI Foundry (Continued)](./srcs/sdk/quick-starts/02_simple_chat_with_prompt_template/)\n      - Shows how to use prompt templates for chat interactions.\n    - [3. Developing an AI Inference Chat Application with RAG using Azure AI Foundry](./srcs/sdk/quick-starts/03_rag_chat/)\n      - Implements Retrieval-Augmented Generation (RAG) for chat applications.\n    - [4. Developing an Application to Evaluate Inference Model Results with Azure AI Foundry](./srcs/sdk/quick-starts/04_evaluation/)\n      - Examples for evaluating AI models and their outputs.\n    - [5. Developing a Simple AI Agent Application with Azure AI Foundry](./srcs/sdk/quick-starts/05_simple_agent/)\n      - Demonstrates building a simple AI agent.\n\n## SDK Tutorials\n\nThe `srcs/sdk/tutorials` folder contains more comprehensive tutorials for advanced use cases:\n\n- Build sample applications with modular code\n- Includes the following:\n  1. [Building a Custom RAG Chat App with the Azure AI Foundry SDK](./srcs/sdk/tutorials/enterprise-rag-chat/)\n     - A tutorial on building an enterprise-grade RAG chat application.\n  1. [Building a Custom Agent App with a Combination of Azure AI Agent Service and AutoGen](./srcs/sdk/tutorials/ai-agent-service/)\n     - Explains how to create an AI agent service.\n\n## Getting Started\n\n1. Clone this repository.\n2. Follow the instructions in the respective folders to set up and run the examples or tutorials.\n\n## Contributing\n\nContributions are welcome! Please submit a pull request or open an issue for any suggestions or improvements.\n"
    },
    {
      "name": "HosseinZahed/multi-agent-cloud-architect",
      "stars": 2,
      "img": "https://avatars.githubusercontent.com/u/19933353?s=40&v=4",
      "owner": "HosseinZahed",
      "repo_name": "multi-agent-cloud-architect",
      "description": "A multi-agent cloud architect",
      "homepage": null,
      "language": "Python",
      "created_at": "2025-03-14T14:13:34Z",
      "updated_at": "2025-04-19T10:01:13Z",
      "topics": [],
      "readme": "# 🌐🤖 Multi-Agent Cloud Architect\n\n🌐 A multi-agent cloud architect\n\n## 🛠️ Setup Instructions\n\n### 1. 🐍 Create a Python Virtual Environment\nTo create a Python virtual environment, run the following command in the root of the project:\n\n```bash\npy -m venv .venv\n```\n\nActivate the virtual environment:\n- On Windows:\n  ```bash\n  .venv\\Scripts\\activate\n  ```\n- On macOS/Linux:\n  ```bash\n  source .venv/bin/activate\n  ```\n\n### 2. 📄 Create a `.env` File\n\n\nCreate a `.env` file at the root of the project and add the following environment variables:\n\n```env\nAZURE_OPENAI_ENDPOINT=\"https://models.inference.ai.azure.com\"\nGITHUB_TOKEN=\"<your_github_token>\"\n\n### Uncomment the lines below to enable OAuth authentication with Azure EntraID\n#CHAINLIT_URL=\"http://localhost:8000\"\n#CHAINLIT_AUTH_SECRET=\"<your_chainlit_auth_secret>\"\n\n#OAUTH_AZURE_AD_CLIENT_ID=\"<your_oauth_azure_ad_client_id>\"\n#OAUTH_AZURE_AD_CLIENT_SECRET=\"<your_oauth_azure_ad_client_secret>\"\n#OAUTH_AZURE_AD_TENANT_ID=\"<your_oauth_azure_ad_tenant_id>\"\n#OAUTH_AZURE_AD_ENABLE_SINGLE_TENANT=True\n```\n⚠️**Notes:**\n- Navigate to [GitHub Developer Settings](https://github.com/settings/tokens) and create a Personal Access Token (PAT). Use this token for the `GITHUB_TOKEN` variable. No specific scope is required.\n- The `CHAINLIT_AUTH_SECRET` is a secret key used for authentication. You can generate it by running the following command:\n  ```bash\n  chainlit create-secret\n  ```\n\n### 3. 📦 Install Dependencies\nAfter setting up the virtual environment and `.env` file, install the required dependencies:\n\n```bash\npip install -r requirements.txt\n```\n\n### 4. 🚀 Run Chainlit Apps\nTo run the Chainlit apps, use the following commands:\n\n1. 🤖 **Autogen Multi-Agent App**\n   ```bash\n   chainlit run ag_multi_agent.py\n   ```\n\n2. 🤖 **Semantic Kernel Single-Agent App**\n   ```bash\n   chainlit run sk_single_agent.py\n   ```\n\n3. 🤖 **Semantic Kernel Multi-Agent App**\n   ```bash\n   chainlit run sk_multi_agent.py\n   ```\n\n"
    },
    {
      "name": "jjgriff93/autowolf",
      "stars": 2,
      "img": "https://avatars.githubusercontent.com/u/25235950?s=40&v=4",
      "owner": "jjgriff93",
      "repo_name": "autowolf",
      "description": "An experimental repo for pitting LLMs against each other in a game of Werewolf.",
      "homepage": "",
      "language": "Python",
      "created_at": "2025-03-04T18:38:12Z",
      "updated_at": "2025-04-16T16:05:18Z",
      "topics": [
        "agents",
        "ai",
        "autogen",
        "llm"
      ],
      "readme": "# AutoWolf 🐺 An AI Game of Werewolf using AutoGen\n\n## Getting started\n\n### Prerequisites\n\n- Python 3.12 or later\n- Poetry 1.9 or later\n- Azure CLI\n- An Azure AI Services resource with a model deployment (`gpt-4o`)\n- [Cognitive Services OpenAI User](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/role-based-access-control#cognitive-services-openai-user) permissions on the Azure AI Services resource\n\n### Installation\n\n1. Clone the repository:\n\n```bash\ngit clone\ncd autowolf\n```\n\n2. Install the dependencies:\n\n```bash\npoetry install\n```\n\n3. Set up the environment variables:\n\n```bash\ncp .env.example .env\n```\n\n4. Log in to Azure\n\n```bash\naz login\n```\n\n### Run the application\n\n```bash\npoetry run chainlit run main.py\n```\n"
    },
    {
      "name": "geteff1/ClinDiag",
      "stars": 2,
      "img": "https://avatars.githubusercontent.com/u/148701415?s=40&v=4",
      "owner": "geteff1",
      "repo_name": "ClinDiag",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2025-02-25T15:18:12Z",
      "updated_at": "2025-04-15T02:02:40Z",
      "topics": [],
      "readme": "# ClinDiag\n\nThis is the official repo for the paper [**ClinDiag: Grounding Large Language Model in Clinical Diagnostics**](https://github.com/geteff1/ClinDiag).\n\n🔗 **Demo website: https://clindiag.streamlit.app/**\n\n\n## Table of Contents\n\n1. [Installation](#installation)\n    * [Set up a virtual environment](#venv)\n    * [Install dependencies](#dep)\n    * [Add API configs](#configs)\n2. [Demo](#demo)\n    * [ClinDiag-GPT](#clindiag-gpt)\n    * [ClinDiag-Framework](#clindiag-framework)\n3. [Datasets](#datasets)\n    * [ClinDiag-Benchmark](#benchmark)\n    * [Standardized Patients](#patients)\n    * [Fine-Tuning Data](#fine-tune)\n4. [Usage](#usage)\n    * [Human+LLM](#human-llm)\n    * [Human Alone](#human)\n    * [Ablation Study](#ablation)\n        * [Multi-doctor agents](#multi-doctor)\n        * [Critic agent](#critic)\n        * [Expert prompt](#expert)\n\n\n## Installation <a name=\"installation\"></a>\n\n### Set up a virtual environment <a name=\"venv\"></a>\n\nWhen using pip it is generally recommended to install packages in a virtual environment to avoid modifying system state. We use [`conda`](https://www.anaconda.com/download/) as an example here:\n\nCreate and activate:\n```bash\n$ conda create -n clindiag python==3.12\n$ conda activate clindiag\n```\n\nTo deactivate later, run:\n```bash\n(clindiag) conda deactivate\n```\n\n### Install dependencies <a name=\"dep\"></a>\n\n```bash\n(clindiag) pip install -r requirements.txt\n```\n\n### Add API configs <a name=\"configs\"></a>\n\nBefore running a script, go to [`configs/OAI_Config_List.json`](https://github.com/geteff1/ClinDiag/blob/main/configs/OAI_Config_List.json) to fill in your model name and API key. \n```json\n{\n    \"model\": \"gpt-4o-mini\",\n    \"api_key\": \"[YOUR_API_KEY]\",\n    \"base_url\": \"[YOUR_BASE_URL](optional)\",\n    \"tags\": [\n        \"gpt-4o-mini\"\n    ]\n}\n```\nThe tags will be used to filter selected model(s) for each stage, see `parse_args()` in code scripts for details.\n\n\n## Demo <a name=\"demo\"></a>\n\n### 💬 ClinDiag-GPT <a name=\"clindiag-gpt\"></a>\n\n- 🔗 **Demo website: https://clindiag.streamlit.app/**\n\nTrained on our [fine-tuning dataset](#fine-tune), *ClinDiag-GPT* showed superior performance in clinical diagnostic procedures. Although we can't provide direct API access to our fine-tuned model for security and cost considerations, feel free to chat with ClinDiag-GPT on our [demo website](https://clindiag.streamlit.app/).\n\n### 🏗 ClinDiag-Framework <a name=\"clindiag-framework\"></a>\n\nTo test out the 2-agent *ClinDiag-Framework*, run:\n\n```bash\n(clindiag) python code/trial_doctor_provider.py --data_dir sample_data\n```\n\n- `--data_dir`: root directory of input case folders. Here we use `sample_data` for a quick demo\n- `--output_dir`: directory to save output files, defaults to `output`\n- `--model_name_{history/pe/test/diagnosis}`: models used for the doctor agent in each stage, defaults to `gpt-4o-mini`\n- `--model_name_provider`: model used for the provider agent, defaults to `gpt-4o-mini`\n\n\n## Datasets <a name=\"datasets\"></a>\n\n### ClinDiag-Benchmark (n=4,421) <a name=\"benchmark\"></a>\n\n`./benchmark_dataset.zip`\n\n(To uncompress, run `unzip benchmark_dataset.zip` in the root directory)\n\nA comprehensive clinical dataset comprising 4,421 real-world cases, encompassing both rare and common diseases across 32 specialties.\n\n### Standardized Patients (n=35) <a name=\"patients\"></a>\n\n`./human_examiner_scripts/`\n\nA set of 35 patient scripts sourced from the hospital’s Objective Structured Clinical Examination (OSCE) test dataset for standardized patient training.\n\n### Fine-Tuning Data (n=7,616) <a name=\"fine-tune\"></a>\n\n`./finetune_data.zip`\n\n(To uncompress, run `unzip finetune_data.zip` in the root directory)\n\nThe multi-turn chat dataset used for fine-tuning a chat model. Each conversation example was constructed from a quality-checked real-world case and structured to adhere to standard clinical diagnostic practice. The data is available in both `jsonl` and `json` formats. \n\n`finetune_data_messages.jsonl`:\n```json\n{\"messages\": [{\"role\": \"system\", \"content\": \"...\"}, {\"role\": \"user\", \"content\": \"...\"}, {\"role\": \"assistant\", \"content\": \"...\"}]}\n{\"messages\": [{\"role\": \"system\", \"content\": \"...\"}, {\"role\": \"user\", \"content\": \"...\"}, {\"role\": \"assistant\", \"content\": \"...\"}]}\n```\n\n`finetune_data_conversations.json`:\n```json\n{\n    \"conversations\": [\n        [\n            {\"from\": \"system\", \"value\": \"...\"},\n            {\"from\": \"user\", \"value\": \"...\"},\n            {\"from\": \"assistant\", \"value\": \"...\"},\n        ],\n        [\n            {\"from\": \"system\", \"value\": \"...\"},\n            {\"from\": \"user\", \"value\": \"...\"},\n            {\"from\": \"assistant\", \"value\": \"...\"},\n        ]\n    ]\n}\n```\n\n\n## Usage <a name=\"usage\"></a>\n\nBelow are instructions to run experiments on the full [benchmark dataset](#benchmark). Remember to `unzip benchmark_dataset.zip` first.\n\n### Human+LLM <a name=\"human-llm\"></a>\n\nThis script implements a human-LLM collaboration framework where LLMs serve as an assistant to answer physician's questions.\n\n```bash\n(clindiag) python code/test_human_llm.py --data_dir benchmark_dataset --output_dir output\n```\n\nBy default, output files will be saved to `./output/test_human_llm/...`. You can set your desired output directory by specifying `--output_dir` (same for all scripts below).\n\n### Human Alone <a name=\"human\"></a>\n\nThis is to simulate the human-alone scenario where a physician performs the clinical diagnostic procedure all by itself within the ClinDiag-Framework.\n\n```bash\n(clindiag) python code/test_human_alone.py --data_dir benchmark_dataset\n```\n\n### Ablation Study <a name=\"ablation\"></a>\n\nThe following scripts were used for ablation study. We examined the effects of (1) multi-doctor collaboration, (2) introducing a critic agent, and (3) prompt engineering on diagnostic performance. \n\n#### 1. Multi-doctor agents <a name=\"multi-doctor\"></a>\n\nWe tested the effect of having 2–3 doctor agents collaborate in the clinical decision-making process. \n\n```bash\n(clindiag) python code/trial_multidoctor.py --data_dir benchmark_dataset --num_specialists 2\n```\n\n- `--num_specialists`: number of doctor agents, defaults to `3`\n\n#### 2. Critic agent <a name=\"critic\"></a>\n\nThis framework incorporates a critic agent to suggest further revisions on doctor agent's questions.\n\n```bash\n(clindiag) python code/trial_critic.py --data_dir benchmark_dataset --model_name_critic gpt-4o-mini\n```\n\n- `--model_name_critic`: model used for the critic agent, defaults to `gpt-4o-mini`\n\n#### 3. Expert prompt <a name=\"expert\"></a>\n\nThis script adopts expert-generated prompts.\n\n```bash\n(clindiag) python code/trial_expert_prompt.py --data_dir benchmark_dataset\n```"
    },
    {
      "name": "FrancoisChastel/OpenCustomsGPT",
      "stars": 2,
      "img": "https://avatars.githubusercontent.com/u/10299319?s=40&v=4",
      "owner": "FrancoisChastel",
      "repo_name": "OpenCustomsGPT",
      "description": null,
      "homepage": "",
      "language": "Python",
      "created_at": "2025-02-20T15:17:19Z",
      "updated_at": "2025-04-03T18:25:41Z",
      "topics": [],
      "readme": "# OpenCustomsGPT\n\nOpenCustomsGPT is an advanced open-source tool designed to interact with Sydonia (Customs Management System) data through AI-powered agents. It enables customs officials and analysts to extract, analyze, and visualize customs data efficiently.\n\n## Table of Contents\n\n- [OpenCustomsGPT](#opencustomsgpt)\n  - [Table of Contents](#table-of-contents)\n  - [Features](#features)\n  - [Examples](#examples)\n  - [Prerequisites](#prerequisites)\n  - [Installation](#installation)\n    - [Production](#production)\n    - [Development](#development)\n  - [Configuration](#configuration)\n  - [Usage](#usage)\n  - [Project Structure](#project-structure)\n  - [Roadmap](#roadmap)\n  - [Contributing](#contributing)\n  - [License](#license)\n  - [Contact](#contact)\n\n## Features\n\n- **Sydonia Integration:** Direct connection to Sydonia database for real-time data access\n- **Data Management:**\n  - Support for key Sydonia tables (expandable to full schema)\n- **AI-Powered Analysis:**\n  - Automated data extraction and processing\n  - Advanced analytics and reporting\n  - Custom query generation through natural language\n- **Flexible Architecture:**\n  - Multi-LLM provider support through LiteLLM\n\n## Examples\n\nhttps://github.com/user-attachments/assets/66016a59-2998-4501-a233-f305d6a86cb9\n\nhttps://github.com/user-attachments/assets/223cebfa-d602-44eb-baad-e1f763a4ef2d\n\n## Prerequisites\n\n- Python 3.10\n- Poetry for dependency management\n- PostgreSQL database\n- OpenAI API key or other supported LLM provider credentials\n\n## Installation\n\n### Production\n\nFor production deployment, please refer to our dedicated installation guide at [OpenCustomsGPT-install](https://github.com/FrancoisChastel/OpenCustomsGPT-install).\n\n### Development\n\n1. **Clone the repository:**\n  \n   ```bash\n   git clone https://github.com/FrancoisChastel/OpenCustomsGPT.git\n   cd OpenCustomsGPT\n   ```\n\n2. **Install dependencies with Poetry:**\n\n   ```bash\n   poetry install\n   ```\n\n## Configuration\n\n1. **Set up environment variables:**\n\n   ```bash\n   export DB_URI=\"postgresql+psycopg2://user:password@localhost:5432/dbname\"\n   export OPENAI_API_KEY=\"your-api-key\"\n   export BASE_URL_LITELLM=\"http://0.0.0.0:4000\"\n   ```\n\n2. **Configure LLM provider:**\n   - Edit `configs/litellm/config.yaml` to set your preferred LLM provider\n   - Available configurations include different agent types in `configs/` directory:\n     - `coder_agent.yaml`: For code-related tasks\n     - `normal_agent.yaml`: For general purpose tasks\n     - `small_agent.yaml`: For lightweight operations\n     - `thinking_agent.yaml`: For complex reasoning tasks\n     - `sql_aent.yaml`: For sql-related tasks\n\n## Usage\n\n1. **Start the LiteLLM proxy:**\n\n   ```bash\n   poetry run litellm --config configs/litellm/config.yaml\n   ```\n\n2. **Launch the Chainlit interface:**\n\n   ```bash\n   poetry run chainlit run src/app.py\n   ```\n\n3. Access the application at `http://localhost:8501`\n\n## Project Structure\n\n```markdown\nOpenCustomsGPT/\n├── LICENSE                 # Project license file\n├── README.md               # Project documentation and overview\n├── configs/                # Configuration directory\n│   ├── coder_agent.yaml    # Configuration for coding-focused AI agent\n│   ├── litellm/            # LiteLLM configuration directory\n│   │   └── config.yaml     # LiteLLM settings (LLM provider configs)\n│   ├── normal_agent.yaml   # Configuration for standard AI agent\n│   ├── small_agent.yaml    # Configuration for lightweight AI agent\n│   ├── sql_agent.yaml    # Configuration for sql AI agent\n│   └── thinking_agent.yaml # Configuration for reasoning-focused AI agent\n└── dockerfile              # Container configuration\n└── poetry.lock             # Poetry dependency lock file\n└── pyproject.toml          # Python project configuration and dependencies\n└── src/                    # Source code directory\n│   ├── agents/             # AI agents implementation\n│   │   ├── cache.py        # Caching mechanism for agents\n│   │   ├── config.py       # Agent configuration handling\n│   │   ├── executor.py     # Agent execution logic\n│   │   ├── prompts.py      # Agent prompt templates\n│   │   ├── setup.py        # Agent initialization\n│   │   ├── sql.py          # Database interactions\n│   │   └── tools.py        # Agent utilities and tools\n│   ├── data_schema/        # Data structure definitions\n│   │   ├── main.py         # Main schema handling\n│   │   └── tmp/            # Temporary schema storage\n│   │       └── schema.txt\n│   ├── main.py             # Application entry point\n│   └── render/             # Output rendering\n│       ├── agents.py       # Agent response rendering\n│       └── messages.py     # Message formatting\n└── work_dir/               # Runtime working directory\n    └── helpers/            # Helper functions\n        └── tools.py        # Additional utility tools\n```\n\n## Roadmap\n\n- [ ] Implement dashboarding solution\n- [ ] Enhance SQL query generation\n- [ ] Create plugin system for extensions\n- [ ] Add verified tooling for LLM used\n- [ ] Add targeted data-schema extraction\n- [ ] Implement authentification and SSO\n- [ ] Add nested agentic behavior\n- [ ] Add multi-language support (french as a priority)\n- [ ] Add Oracle support\n\n## Contributing\n\nContributions are welcome! Please check our [Contributing Guidelines](CONTRIBUTING.md) for details on our code of conduct and how to submit pull requests.\n\n## License\n\nThis project is licensed under the AGPL License. See the [LICENSE](LICENSE) file for more information.\n\n## Contact\n\nFor questions or feedback, please reach out at [francois@chastel.co](mailto:francois@chastel.co).\n"
    },
    {
      "name": "Lailiangken/collm04",
      "stars": 2,
      "img": "https://avatars.githubusercontent.com/u/52484757?s=40&v=4",
      "owner": "Lailiangken",
      "repo_name": "collm04",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2025-01-28T06:54:02Z",
      "updated_at": "2025-02-21T04:18:51Z",
      "topics": [],
      "readme": "# Group Chat Assistant\n\nこのプロジェクトは、Autogen(0.4.*)を利用したPythonコード作成補助用のグループチャットアシスタントを提供します。  \nStreamlitを使用したWebインターフェースで、通常チャットとコード実行チャットの2つのモードを提供します。  \n作者はこれをWSL2環境で開発しているので、他の環境での動作は確認していません。\n\n## 機能\n\n**通常チャット**: 複数のAIエージェントによる対話型チャット  \n**コード実行チャット**: Pythonコードの生成と実行が可能なチャット  \n## 必要条件\n\nDocker  \nDocker Compose  \nOpenAI API Key  \n## インストール\n\n**リポジトリをクローン**: プロジェクトのリポジトリをローカルマシンにクローンします。\n\n```bash \ngit clone https://github.com/yourusername/collm04.git \ncd collm04 \n```\n\n**環境変数の設定**: プロジェクトのルートディレクトリに `.env` ファイルを作成し、以下の内容を設定します。WORK_DIR はホストマシン上の作業ディレクトリを指定し、OPENAI_API_KEY にはOpenAIのAPIキーを設定してください。\n\n``` \nWORK_DIR=\"/path/to/your/workspace\" OPENAI_API_KEY=\"your-api-key\"\nAZURE_OPENAI_API_KEY=\"your-api-key\"\n```\n\n**Dockerイメージのビルド**: Docker Composeを使用して、必要なDockerイメージをビルドします。\n\n```bash \ndocker-compose build \n```\n\n## 使用方法\n\n**アプリケーションの起動**: Docker Composeを使用してアプリケーションを起動します。これにより、必要なコンテナが立ち上がり、アプリケーションが実行されます。\n\n```bash \ndocker-compose up \n```\n-d オプションを付けると、バックグラウンドでコンテナを実行できます。\n```bash \ndocker-compose up -d\n```\n\nコンテナ内でコマンドを実行するには、バックグラウンドでコンテナを起動した状態で以下のコマンドを使用します。\n```bash \ndocker-compose exec app bash\n```\n\n\n**アプリケーションへのアクセス**: ブラウザで以下のURLにアクセスして、Streamlitのインターフェースを使用します。\n\n``` \nhttp://localhost:8502 \n```\n\n**インターフェースの使用**:\n\nチャットモードを選択（通常チャット/コード実行チャット）  \nタスクや質問を入力  \n\"実行\"ボタンをクリック  \n**アプリケーションの停止**: アプリケーションを停止するには、以下のコマンドを使用します。\n\n```bash \ndocker-compose down \n```\n\n## プロジェクト構成\n\n``` \ncollm04/ \n├── autogen_functions/ \n│ └── group_chat/ \n│     └──agents/\n│     │   └── (各エージェント情報).json\n│     └── group_chat_043.py \n├── autogen_setting/\n│ └── models/\n│     └── gpt-4o_1.json\n├── group_chat_app.py \n├── Dockerfile \n├── docker-compose.yml \n├── requirements.txt \n└── .env \n```\n\n## 主要コンポーネント\n\n`group_chat_app.py`: Streamlitベースのメインアプリケーション  \n`group_chat_043.py`: グループチャットのコア機能  \n`Dockerfile`: アプリケーションのコンテナ化設定  \n`docker-compose.yml`: マルチコンテナ設定  \n\nDocker周りについては作者もだいぶ苦戦したので、うまく動作しなければ教えてください。\n\n## 実行例\n[Sample](sample.pdf)  \nサンプルを見ればわかる通り、コード実行チャットにおいてはだいぶ無駄な会話が多いです。Agentを追加、編集したりすることで効率化ができると思います。  \n[動画](動作.pdf)  \nサイズの制限で途中からですが、(デコードされてない文字列が)Logとして流れる様子を見ることができます。  \n"
    },
    {
      "name": "jplck/generic-ai-samples",
      "stars": 2,
      "img": "https://avatars.githubusercontent.com/u/8427174?s=40&v=4",
      "owner": "jplck",
      "repo_name": "generic-ai-samples",
      "description": null,
      "homepage": null,
      "language": "Bicep",
      "created_at": "2025-01-23T10:09:32Z",
      "updated_at": "2025-03-27T08:49:00Z",
      "topics": [],
      "readme": "# Generic AI Sample Applications\n\nThis repository contains a list of sample applications that demonstrate a set of common AI use cases.\n\nNamely those are interactions with LLMs via:\n- Chat\n- Vision\n- Audio\n\n# Setup your environment\nTo make the setup easier, this repository contains configurations and infrastructure as code templates to setup the required cloud services and provide the neccessary environment to run the provided samples.\n\nTo setup everything, run the following command from the Azure Developer CLI (azd). To get more information about azd, you can check out the documentation here (https://learn.microsoft.com/en-us/azure/developer/azure-developer-cli/)\n\n```\necho \"login into your azure environment\"\nazd auth login\n\necho \"deploy the infrastructure. Follow the instructions after you run the command\"\nazd up\n```\n\nIf you cannot run the azd command, check out the .env_sample file and fill in the missing variables by hand.\n\nThe above process will generate a .env file in your root directory. This .env file will contain all required connections information you need to run the samples applications.\n\n# Run the samples\nThe repository contains several projects. To run a specific sample, cd into the folder of the sample, run  `pip install - r requirements.txt` and execute the run command.\n\nFor **chat**, **ingestions-pipeline**, **voice-interaction** run\n\n`python -m app`\n\nIf you want to debug the sample, you can alternatively run the app via the debugger in VSCode. You might need to change the launch config and provide the correct module to start.\n\nFor **chat_langgraph** you need to use `langgraph dev --debug-port 2026` as this command spuns up a developer UI that helps you debug your agents. This will automatically start the debugger.\n\n"
    },
    {
      "name": "marcelmedina/ai-autogen-agent-patterns",
      "stars": 2,
      "img": "https://avatars.githubusercontent.com/u/17131300?s=40&v=4",
      "owner": "marcelmedina",
      "repo_name": "ai-autogen-agent-patterns",
      "description": null,
      "homepage": "",
      "language": "Jupyter Notebook",
      "created_at": "2025-02-05T00:59:35Z",
      "updated_at": "2025-03-12T10:10:57Z",
      "topics": [
        "autogen-sample"
      ],
      "readme": "# ai-autogen-agent-patterns\n\nInstall the packages\n```\npip install -r requirements.txt\n```\n\nCreate the file `.env` and add the following setting:\n```\nAZURE_AI_SERVICE_NAME=your-azure-ai-service-name\n```"
    },
    {
      "name": "amitlals/AI-Financial-Engineering-Agentic-System",
      "stars": 2,
      "img": "https://avatars.githubusercontent.com/u/37605691?s=40&v=4",
      "owner": "amitlals",
      "repo_name": "AI-Financial-Engineering-Agentic-System",
      "description": "AI-Driven Financial and Market Analysis with Azure AI Agent Services",
      "homepage": null,
      "language": "Python",
      "created_at": "2025-02-06T01:31:06Z",
      "updated_at": "2025-04-09T14:44:59Z",
      "topics": [],
      "readme": "# Repository Template for AI-Driven Financial and Market Analysis\n\n## Overview\n\nThis repository provides a structured framework for setting up an **AI-driven financial engineering and marketing analysis agentic system** using **Azure AI Agent Service SDK**. It includes:\n\n---\n\n## 🤖 AI Agents Overview - Autogen & Azure AI foundry with Azure AI Agent Services\n\nThe system incorporates multiple AI-driven agents specialized in different aspects of financial and market analysis -\n![image](https://github.com/user-attachments/assets/7d8ba4bd-84b0-4458-90a5-dd835faab8d4)\n<br>\nBehind the scene Autogen agents in play -\n<img width=\"962\" alt=\"image\" src=\"https://github.com/user-attachments/assets/93990da8-b6eb-4c12-ad4c-3d9cabaadf81\" />\n\n### **Agent Name and Flows**\n### 1️⃣ **Market Analysis Agent**\n   - Analyzes market trends and customer behavior.\n   - Identifies key business opportunities.\n   - Provides insights on geographic demand and economic patterns.\n\n### 2️⃣ **Financial Analysis Agent**\n   - Evaluates company financials, balance sheets, and performance metrics.\n   - Assesses cost efficiency and growth opportunities.\n   - Conducts risk assessments and budget optimization.\n\n### 3️⃣ **News Analysis Agent**\n   - Scrapes and analyzes financial news articles.\n   - Identifies impactful trends in the industry.\n   - Provides strategic implications of economic events.\n\n### 4️⃣ **Strategic Planning Agent**\n   - Develops three unique business strategies based on market and financial insights.\n   - Creates an implementation roadmap for each strategy.\n   - Conducts risk-benefit analysis and final recommendations.\n\n---\n\n\n- A structured Python project template\n- Required dependencies (`requirements.txt`)\n- Configuration file templates (`.env`, `config.ini`)\n- Step-by-step setup guide\n- Automated AI agents for data and news analysis\n\n---\n\n## 📂 Project Structure\n\n```\n📦 Stock-Analysis-AutoGen-Multi-Agent\n ┣ 📂 src\n ┃ ┣ 📜 main.py  # Main entry point for analysis\n ┃ ┣ 📜 connectors.py  # Handles data retrieval (ERP + News API)\n ┃ ┣ 📜 agents.py  # Configures AI agents for financial & market insights\n ┃ ┣ 📜 processing.py  # Data processing & transformation\n ┃ ┗ 📜 config.py  # Loads configurations & API keys\n ┣ 📜 requirements.txt  # Dependencies\n ┣ 📜 .env.example  # Environment variables template\n ┣ 📜 README.md  # Project documentation\n ┣ 📜 setup_azure_ai_sdk.md  # Step-by-step guide for Azure AI Agent setup\n ┗ 📜 LICENSE  # License file\n```\n\n---\n\n## 🔧 Setup Instructions\n\n### 1️⃣ Clone Repository\n\n```bash\ngit clone https://github.com/amitlals/Stock-Analysis-AutoGen-Multi-Agent.git\ncd Stock-Analysis-AutoGen-Multi-Agent\n```\n\n### 2️⃣ Setup Python Virtual Environment\n\n```bash\npython -m venv venv\nsource venv/bin/activate  # For Mac/Linux\nvenv\\Scripts\\activate    # For Windows\n```\n\n### 3️⃣ Install Dependencies\n\n```bash\npip install -r requirements.txt\n```\n\n### 4️⃣ Set Up Environment Variables\n\n1. **Create a `.env` file** (or copy from `.env.example`)\n   ```bash\n   cp .env.example .env\n   ```\n2. **Add API keys & Azure configuration**:\n   ```ini\n   # .env\n   CURRENTS_API_KEY=your_currents_api_key\n   API_KEY=your_azure_openai_api_key\n   AZURE_ENDPOINT=your_azure_openai_endpoint\n   MODEL_DEPLOYMENT_NAME=gpt-4\n   MODEL_API_VERSION=2024-02-15-preview\n   ```\n\n\n## 📜 Azure AI Agent Service SDK Setup\n\nFollow the steps below to configure **Azure AI Agent Service SDK**:\n\n### 1️⃣ Install Azure SDK\n\n```bash\npip install azure-ai-agent-sdk\n```\n\n### 2️⃣ Authenticate and Set Up Azure Service\n\n1. Log in to Azure CLI:\n   ```bash\n   az login\n   ```\n2. Set your subscription:\n   ```bash\n   az account set --subscription \"<your-subscription-id>\"\n   ```\n3. Deploy AI Agent Service:\n   ```bash\n   az ai agent create --name StockAnalysisAgent --resource-group your-resource-group --location eastus --sku Standard\n   ```\n4. Retrieve and store your **Azure AI Agent API Key** in `.env`.\n\n---\n\n## 🚀 Running the Project\n\nTo start the AI-driven financial and market analysis system, run:\n\n```bash\npython src/main.py\n```\n\nThis will:\n\n1. Connect to ERP system & fetch orders/products data\n2. Retrieve market & financial news\n3. Process the data into structured insights\n4. Utilize AI agents for market & financial analysis\n5. Generate strategic recommendations\n\n---\n\n## ✅ Requirements (`requirements.txt`)\n\n```txt\nrequests\ndotenv\nautogen\nazure-ai-agent-sdk\n```\n\n---\n\n## 📌 Notes\n\n- **Keep your `.env` file secure** and never commit it to version control.\n- Follow best practices for **Azure OpenAI API usage** to avoid exceeding quotas.\n- **Modify AI agent prompts** as per specific business needs.\n\n---\n\n## 🏁 Next Steps\n\n- **Enhance AI Agents:** Add more specialized AI agents for deeper financial analysis.\n- **Expand Data Sources:** Integrate stock market APIs for real-time analytics.\n- **Deploy on Azure:** Use Azure Functions or a containerized service for production deployment.\n\n---\n\n## 📜 License\n\nThis project is licensed under the MIT License. See the [LICENSE](LICENSE) file for details.\n\n---\n\n### 🔗 GitHub Repository\n\n[📌 GitHub Repository Link](https://github.com/amitlals/Stock-Analysis-AutoGen-Multi-Agent)\n"
    },
    {
      "name": "kantariyaraj/AI_Agent_Examples",
      "stars": 2,
      "img": "https://avatars.githubusercontent.com/u/6614811?s=40&v=4",
      "owner": "kantariyaraj",
      "repo_name": "AI_Agent_Examples",
      "description": "Various AI Agents built using different LLM and AI Agentic frameworks",
      "homepage": "",
      "language": "Python",
      "created_at": "2025-01-06T09:56:01Z",
      "updated_at": "2025-02-21T11:20:10Z",
      "topics": [
        "agent",
        "ai",
        "embeddings",
        "genai",
        "groq-api",
        "huggingface",
        "llm",
        "ollama",
        "pinecone"
      ],
      "readme": "# AI Agents Repository\n\nThis repository contains various AI Agents built using different LLM models and agentic frameworks.\n\n## Available Agents\n\n### 1. [PDF RAG Agent](pdf_rag_example/)\nA Retrieval Augmented Generation (RAG) system that allows users to:\n- Upload PDF documents\n- Index their content using embeddings\n- Ask questions about the documents using Groq LLM\n- Get accurate answers based on the document context\n\n### 2. [Simple Chatbot](simple_chatbot/)\nA conversational AI chatbot that:\n- Uses Groq's Mixtral model for natural language interactions\n- Maintains conversation context across multiple messages\n- Provides a clean web interface for chatting\n- Automatically summarizes long conversations to stay within context window\n\n### 3. [CONFLUENCE RAG Agent](confluence_rag_example/)\nA conversational AI chatbot that:\n- Uses Groq's model for natural language interactions\n- Provides ability to train the model on Confluence page content\n- Ask questions about the page content\n- Get the answer based on the context\n\n### 4. [MySQL Chat Agent](chat_with_mysql/)\nAn AI-powered chat interface for MySQL databases that:\n- Uses CrewAI framework with Groq's LLM model\n- Allows natural language querying of MySQL databases\n- Translates questions into SQL queries automatically\n- Provides clean web interface for database interactions\n- Supports connection to any MySQL database\n\n### 5. [Travel Planning Agent](travel_planner_langgraph/)\nAn AI travel assistant that:\n- Developed using LangGraph\n- Collects user information for travel planning\n- Create travel itinerary using user preference\n- Integrates with Flight and Weather API that provides real time information\n- Generates day-by-day travel schedules"
    },
    {
      "name": "trojrobert/web_agent",
      "stars": 2,
      "img": "https://avatars.githubusercontent.com/u/28733978?s=40&v=4",
      "owner": "trojrobert",
      "repo_name": "web_agent",
      "description": "Web AI agent is an AI agent that autonomously opens a web browser, search the internet and give you are summary of your question",
      "homepage": null,
      "language": "Python",
      "created_at": "2025-01-28T17:38:24Z",
      "updated_at": "2025-01-31T17:40:31Z",
      "topics": [],
      "readme": "# Web AI Agent\n\nWelcome to the **Web AI Agent** repository! This project showcases an AI-powered web agent capable of operating a computer by autonomously opening a web browser, searching for information online, and summarizing the results. The project combines advanced AI frameworks and user-friendly frontend design to deliver an interactive experience.\n\n![Banner](assets/image.png)\n---\n\n## Key Features\n\n1. **Automated Web Browsing:** The AI agent can open a web browser, perform online searches, and retrieve relevant information autonomously.\n2. **Summarization:** The agent processes the search results and provides concise summaries for easy understanding.\n\n\n---\n\n## Demo\n\nCheck out the demo video to see the Web AI Agent in action:\n\n![Demo Video](<link to the demo video>)\n\n---\n\n## Installation\n\nFollow these steps to set up the Web AI Agent on your local machine:\n\n\n\n1. Clone the repository:\n   ```bash\n   git clone https://github.com/trojrobert/web_agent.git\n   cd web_agent\n   ```\n2. Rename the `.env_dev` file to `.env` and add your OpenAI API key to the file.\n\n3. Create a virtual environment with any tool of your choice. Ensure the python version is 3.12.5\n\n4. Install the required dependencies with pip or poetry:\n   ```bash\n   pip install -r requirements.txt\n   ```\n   or\n   ```bash\n   poetry install\n   ```\n5. playwright install\n\n6. Run the Streamlit application:\n   ```bash\n   streamlit run src/interface.py\n   ```\n\n\n4. Open your web browser and navigate to interact with the Web AI Agent.\n\n---\n\n## Usage\n\n1. Launch the Streamlit application.\n2. Enter a task (e.g., \"Current Price of Nvidia Stock\").\n3. Click the \"Run Task\" button.\n4. The AI agent will:\n   - Open a web browser.\n   - Search for your query online.\n   - Summarize the results and display them on the interface.\n\n---\n\n## Project Structure\n\n- **`src/`**: contains all the code for the project\n- **`src/interface.py`**: The main entry point for the Streamlit application.\n- **`src/agent/`**: Contains the implementation of the Web AI Agent using the Autogen framework.\n- **`requirements.txt`**: List of dependencies required for the project.\n\n---\n\n## Technologies Used\n\n- **Frontend:** [Streamlit](https://streamlit.io)\n- **AI Framework:** [Autogen](https://github.com/microsoft/autogen)\n\n---\n\n## Contributing\n\nContributions are welcome! Feel free to fork the repository, create a new branch, and submit a pull request with your improvements or bug fixes.\n\n---\n\n## License\n\nThis project is licensed under the [MIT License](LICENSE).\n\n---\n\n## Author\n\nThis project was developed by [John Robert](https://github.com/trojrobert).\n\n---\n\n## Feedback and Support\n\nFor any questions or feedback, please open an issue in this repository or contact the author directly.\n\nHappy coding!\n\n"
    },
    {
      "name": "weinaike/agentflow",
      "stars": 2,
      "img": "https://avatars.githubusercontent.com/u/13814354?s=40&v=4",
      "owner": "weinaike",
      "repo_name": "agentflow",
      "description": "AgentFlow, based on AutoGen, enables the creation and management of intelligent agent workflows through configuration files for efficient task automation.",
      "homepage": "",
      "language": "Python",
      "created_at": "2025-01-11T08:27:02Z",
      "updated_at": "2025-03-31T01:59:48Z",
      "topics": [],
      "readme": "## 项目说明文档\n\n### 项目简介\n\nAgentFlow是一个基于Python语言开发的智能体工作流构建框架。旨在帮助用户通过配置文件快速构建和管理复杂的智能体工作流，实现任务自动化和高效执行。\n\n### 软件特性\n\n- **动态工作流管理**：支持顺序和循环流程组合，实现复杂的任务控制。\n- **多种节点类型**：包括AgentNode和ToolNode，支持多样化的行为和任务处理。\n- **高度扩展性**：流程和节点可通过继承扩展，支持自定义功能的集成。\n- **信息共享与管理**：采用上下文机制在执行期间共享信息和状态，提高信息流的效率。\n\n### 安装指南\n\n1. **克隆项目**：\n\n```bash\ngit clone https://github.com/weinaike/agentflow.git\ncd agentflow\n```\n2. **安装依赖项**：\n\n   - uml分析依赖clang-uml\n   ```bash\n   sudo add-apt-repository ppa:bkryza/clang-uml\n   sudo apt update\n   sudo apt install clang-uml\n   ```\n\n   - python包依赖\n\n   ```bash\n   pip install -r requirements.txt\n   #或者可编辑安装，调用pyproject.toml安装\n   pip install -e . \n   ```\n\n### 项目结构与开发指引\n\n## 使用说明\n\n1. **配置工作流**\n\n   - 通过TOML文件定义工作流，包括流、节点及其参数配置。\n   - 具体配置说明，请参考 [docs/configuration_guide.md](./docs/configuration_guide.md)\n2. **运行工作流**\n\n   ```bash\n   python -m AgentFlow.main path/to/your/config.file\n   # 或者 pip install -e .后运行\n   agentflow path/to/your/config.file\n   ```\n\n### 开发指南\n\n- **设计原则**：遵循SOLID和工厂模式，保证高内聚低耦合。\n- **配置管理**：利用TOML文件灵活管理流程和节点参数。\n- **上下文使用**：统一对象存储，保证信息的共享。\n- **新增流程与节点**：\n  - 新流程需继承 `BaseFlow`，在 `FlowFactory`中注册。\n  - 新节点需继承 `BaseNode`，在 `NodeFactory`中注册。\n  - 新增LLM支持，格式详见docs/llm_config_list_template.json, 同时保证 data_model.py 中的 ModelEnum 包含对应模型枚举\n- **数据模型（data_model）**：定义参数结构，确保数据一致性。\n- **工具集合（tools）**：辅助模块，提供通用功能。\n\n### 项目许可证\n\n本项目遵循MIT许可协议，用户可以自由使用、修改和分发代码，需保留原始许可证和版权声明。详见 `LICENSE`文件。\n"
    },
    {
      "name": "jspv/mchat",
      "stars": 2,
      "img": "https://avatars.githubusercontent.com/u/6540751?s=40&v=4",
      "owner": "jspv",
      "repo_name": "mchat",
      "description": "A chat framework for interacting with LLMs, supports changing models, multi-agent, image generation, tools",
      "homepage": "",
      "language": "Python",
      "created_at": "2023-07-12T10:55:19Z",
      "updated_at": "2025-04-11T01:13:40Z",
      "topics": [],
      "readme": "# mchat\n\n[![License](https://img.shields.io/badge/License-MIT-blue.svg)](LICENSE)\n\n## Description (OLD)\n*mchat* **was** an entirely text-based LLM chat application with support for different LLMs and pre-establshed customizable prompt 'personas' that support mult-shot prompts.  *mchat* uses the excellent [Textual](https://textual.textualize.io) framework for a GUI-like experience complete with mouse support and should run anywhere a decent terminal is available, including over SSH. \n\n## Description (NEW)\nNEW: As good as Textual is, I needed to move to a more graphically capable UI\nto be able to support embedded images, LaTeX, and graphing.  To support this\nI have migrated the frontend to [Nicegui](https://nicegui.io)  The Textual code is still present and should continue to work, but I will not likely be \nmaintaining it.  \n\nAll that is needed is an OpenAI API key.  Azure OpenAI will also work, but you will need to disable the Dall-E support if you don't also have an OpenAI API key.\n\n## Screenshots \n\n![Screenshot1](screenshot1.png)\n\n![Screenshot2](screenshot2.png)\n\n![Screenshot3](screenshot3.png)\n\nEasy to modify agents and teams:\n![Screenshot4](screenshot4.png)\n\n## TODO\n- [x] Copy text to past buffer when clicking on the response\n- [x] Agent support\n- [x] Support Multi-line prompts\n- [x] History and reloadable sessions with local database storage\n- [x] Support for image creation (currently just dall-e)\n- [x] Support for functions/tools\n- [x] Round Robin multi-agent support\n- [ ] Swarm multi-agent support\n- [x] Selector multi-agent support\n- [x] Cancellation Buttons to stop running team\n- [ ] Smarter handling of user_proxy_agent\n- [ ] Nice display for 'thought' messages for reasoning models\n- [ ] AWS Bedrock support\n  \n\n## Table of Contents\n- [mchat](#mchat)\n  - [Description (OLD)](#description-old)\n  - [Description (NEW)](#description-new)\n  - [Screenshots](#screenshots)\n  - [TODO](#todo)\n  - [Table of Contents](#table-of-contents)\n  - [Installation](#installation)\n  - [Configuration](#configuration)\n    - [Models](#models)\n    - [Important](#important)\n    - [Required Fields](#required-fields)\n    - [Default Settings](#default-settings)\n    - [Memory Model Configuration (NOTE: Currently disabled)](#memory-model-configuration-note-currently-disabled)\n    - [Secrets Configuration](#secrets-configuration)\n  - [Agents \\& Teams](#agents--teams)\n  - [Usage](#usage)\n      - [Alternative Usage](#alternative-usage)\n  - [Contributing](#contributing)\n  - [License](#license)\n  - [Contact](#contact)\n\n## Installation\n1. Make sure you have Python and uv installed on your system. If not, you can download and install them from their official websites:\n   - Python: https://www.python.org\n   - UV: https://github.com/astral-sh/uv\n\n\n2. Open a terminal or command prompt and navigate to the project directory.\n\n3. Run the following command to install the project dependencies:\n\n   ```shell\n   uv sync --all-extras\n   ```\n\n   This will create a virtual environment and install all the required dependencies specified in the `pyproject.toml` file.\n\n## Configuration\n\nConfiguration is done within three files: `settings.toml`, `.secrets.toml`(optional, but recommended) and `agents.yaml`\n\nOpen the settings.toml file in a text editor to configure your application. Here's an explanation of the provided configuration options:\n\n### Models\n\nSections need to start with model. (with period) and no other periods in the section name `models.type.model_id` model_id is what will show in the interface.\n\n```\n[models.chat.gpt-4o]\napi_key = \"@format {this.openai_api_key}\"\nmodel = \"gpt-4o\"\napi_type = \"open_ai\"\nbase_url = \"https://api.openai.com/v1\"\n```\n\n### Important\nImage models and settings here are for expliclitly calling the immage models from the prompt. The generate_image tool does not use these settings, only the API key\n\n### Required Fields\nChat Models\n- api_type: [\"open_ai\", \"azure\"]\n- model_type: \"chat\"\n- model: \"name of model\"\n- api_key: \"your key or dynaconf lookup to get the key\"\n- model: \"the openai name for the model\"\n\nAzure Chat Models (additional)\n- azure_endpoint: \"URL for your endpoint\"\n- azure_deployment: \"the azure name for the model in your deployment\"\n- api_version = \"api version\"\n\nImage Models\n- api_type: [\"open_ai\", \"azure\"]\n- model_type: \"image\"\n- model: \"name of model\"\n- size: \"size of images to create\"\n- num_images: \"number of images to create\"\n- api_key: \"your key or dynaconf lookup to get the key\"\n\n### Default Settings\n\n- default_model: Specifies the default model to use.\n- default_temperature: Specifies the default temperature for generating text.\n- default_persona: Specifies the default persona for generating text.\n  \n### Memory Model Configuration (NOTE: Currently disabled)\nmchat maintains memory of the current chat in order to retain context in long conversations.  When the retained memory exceeds the size the model supports, it will summarize the convseration to reduce size.  Since this can be called often for longer chats, it is recommended to use an inexpensive model.  \n\nYou can configure the following properties:\n\n- memory_model: Specifies the specific model to use for memory, use one of the models you sepcified in your model lists\n- memory_model_temperature: Specifies the temperature for the memory model.\n- memory_model_max_tokens: Specifies the maximum tokens for the memory model.\n  \n### Secrets Configuration\nNote that some configuration options, such as API keys, are meant to be kept in a separate .secrets.toml file. You can include the following configuration in that file:\n\n``` shell\n# In .secrets.toml\n# dynaconf_merge = true\n\n# Replace the following with your actual API keys\n# openai_models_api_key = \"oai_ai_api_key_goes_here\"\n# ms_models_api_key = \"ms_openai_api_key_goes_here\"\n```\n\n## Agents & Teams\n\nmchat comes with a default persona and two example agents *linux computer* and *financial manager* and example round-robin and selector teams.  Additional agents and teams can be added in a ```agents.yaml``` file at the top level (same level as this README) using a similar pattern to `mchat/default_personas.yaml` in the code.  When configuring personas, the ```extra_context``` list can allow you to respresent a multi-shot prompt, see the `linux computer` persona in `mchat/default_personas.json` as an example.\n\n## Usage\n1. Run the application in uv using the following command\n   \n   ```shell\n   uv run poe mchat\n   ``````\n\n#### Alternative Usage\n1. Activate the virtual environment created by uv using the following command:\n\n   ```shell\n   source ./.venv/bin/activate\n   ```\n\n3. Run the application using the following command:\n\n   ```shell\n   poe mchat\n   ```\n   or\n   ```shell\n   python -m mchat.main\n   ```\n\n## Contributing\n\nThank you for considering contributing to the project! To contribute, please follow these guidelines:\n\n1. Fork the repository and clone it to your local machine.\n\n2. Create a new branch for your feature or bug fix:\n\n   ```shell\n   git checkout -b feature/your-feature-name\n   ```\n\n   Replace `your-feature-name` with a descriptive name for your contribution.\n\n3. Make the necessary changes and ensure that your code follows the project's coding conventions and style guidelines - which currently are using PEP 8 for style and *black* for formatting \n\n4. Commit your changes with a clear and descriptive commit message:\n\n   ```shell\n   git commit -m \"Add your commit message here\"\n   ```\n\n5. Push your branch to your forked repository:\n\n   ```shell\n   git push origin feature/your-feature-name\n   ```\n\n6. Open a pull request from your forked repository to the main repository's `main` branch.\n\n7. Provide a clear and detailed description of your changes in the pull request. Include any relevant information that would help reviewers understand your contribution.\n\n\n\n## License\nThis project is licensed under the [MIT License](LICENSE).\n\n## Contact\nFeel free to reach out to me at @jspv on GitHub\n"
    },
    {
      "name": "kinshu0/trading-zoo",
      "stars": 2,
      "img": "https://avatars.githubusercontent.com/u/67475292?s=40&v=4",
      "owner": "kinshu0",
      "repo_name": "trading-zoo",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2024-11-23T16:37:36Z",
      "updated_at": "2024-12-04T20:42:56Z",
      "topics": [],
      "readme": "# Trading Zoo!\n\nA Playful Market Simulation Game\n\n## Overview\n\nTrading Zoo is a whimsical trading simulation game that brings the world of finance to life through anthropomorphized animal teams. Set in a jungle-themed environment, the game features various animal species as sophisticated trading teams, each with their own specialties and trading strategies.\n\n![image](https://github.com/user-attachments/assets/7a848a97-3c52-4520-bc62-c28f1d0a0e13)\n\n\n## Core Concept\n\nPlayers can observe and interact with a vibrant marketplace where animal teams trade exotic goods in real-time. The game combines educational aspects of market dynamics with playful, jungle-themed elements.\n![image](https://github.com/user-attachments/assets/26be7083-b5fb-4074-b767-c5fefa96d7a7)\n\n\n## Trading Teams\n\n* **Penguins** 🐧 - Cool-headed traders specializing in ice futures\n* **Monkeys** 🐒 - Agile merchants expert in fruit trading\n* **Foxes** 🦊 - Cunning market analysts and strategic traders\n* **Iguanas** 🦎 - Patient long-term investors\n\n## Tradable Assets\n\n* **Bananas** 🍌 - A staple commodity\n* **Ice** 🧊 - Temperature-sensitive luxury good\n* **Pineapples** 🍍 - Exotic fruit commodity\n* **Fish** 🐟 - For the best sushi\n* **Pebbles** 💎 - Beach treasure\n\n![image](https://github.com/user-attachments/assets/b0dc73c5-0447-421f-abbe-d01f0caa4874)\n\n\n## Game Features\n\n* Real-time trading simulation\n* Dynamic price fluctuations\n* Random market events affecting prices\n* Team performance tracking\n* Visual data representation through graphs\n* Detailed trade history\n* Jungle-themed UI with playful animations\n\n## Technical Architecture\n\n### Multi-Agent Trading System\n\nTrading Zoo implements a sophisticated multi-agent trading environment powered by AutoGen, where AI agents engage in market activities through a central trading engine. Each animal team is represented by an autonomous LLaMA-based agent capable of processing market information and making independent trading decisions.\n\n### Core Components\n\n* **Trading Engine**\n  * Centralized matching engine for order execution\n  * Real-time orderbook management\n  * Price discovery mechanism\n  * Transaction logging and settlement\n\n* **Market Maker Agent**\n  * Provides baseline market liquidity\n  * Maintains bid-ask spreads\n  * Helps prevent extreme price volatility\n  * Facilitates efficient price discovery\n\n* **Trading Agents (Animal Teams)**\n  * Each agent receives:\n    * Current portfolio status\n    * Real-time orderbook data\n    * Market event notifications\n    * Historical price data\n  * Autonomous decision-making based on:\n    * Team-specific trading strategies\n    * Market conditions analysis\n    * Risk management parameters\n    * Portfolio objectives\n\n### System Integration\n\n```\n[Market Events] → [Trading Engine] ← [Market Maker]\n       ↓              ↑    ↓\n[Order Book] → [Trading Agents] ← [Portfolio Data]\n```\n\n### Implementation Details\n\n* Built using AutoGen's multi-agent conversation framework\n* Each agent operates as an independent entity with its own strategy and goals\n* Real-time communication through event-driven architecture\n* Asynchronous order processing and matching\n* State management for portfolio and transaction history\n\n### Data Flow\n\n1. Market events are broadcast to all agents\n2. Agents analyze their portfolio and market data\n3. Trading decisions are converted to orders\n4. Orders are submitted to the central trading engine\n5. Market maker provides counterparty when needed\n6. Matching engine executes valid trades\n7. Portfolio and market data are updated\n8. Trading agents receive execution feedback\n\nThis multi-agent system creates a dynamic, self-sustaining marketplace where AI-driven animal teams can engage in realistic trading activities while maintaining market stability through the market maker's presence.\n\n## Educational Value\n\nWhile maintaining a fun and accessible atmosphere, the game teaches fundamental concepts of:\n\n* Supply and demand\n* Market dynamics\n* Price fluctuations\n* Trading strategies\n* Risk management\n* Portfolio tracking\n\n## Target Audience\n\nTrading Zoo is designed for:\n\n* Beginners interested in learning about trading\n* Students learning economics\n* Anyone interested in gamified financial education\n* Casual gamers who enjoy management simulations\n\nThe game strikes a balance between educational value and entertainment, using the jungle theme and animal characters to make complex market concepts more approachable and engaging.\n\n\n### Instructions to Run Trading Zoo\n\nBackend:\n```\ncd backend\nuv sync\nuv run game.py\n```\n\nFrontend:\n```\ncd frontend\nnpm i\nnpm run start\n```\n"
    },
    {
      "name": "morietschel/agentic-3d",
      "stars": 2,
      "img": "https://avatars.githubusercontent.com/u/42514380?s=40&v=4",
      "owner": "morietschel",
      "repo_name": "agentic-3d",
      "description": "An iterative loop that writes and improves CAD code based on feedback provided by a VLM.",
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2024-10-02T00:40:18Z",
      "updated_at": "2025-04-17T00:31:59Z",
      "topics": [],
      "readme": "# agentic-3d\nAn iterative loop that writes and improves CAD code based on feedback provided by a VLM.\n"
    },
    {
      "name": "DeepSpringAI/search_mcp_server",
      "stars": 2,
      "img": "https://avatars.githubusercontent.com/u/200253356?s=40&v=4",
      "owner": "DeepSpringAI",
      "repo_name": "search_mcp_server",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2025-03-26T03:58:13Z",
      "updated_at": "2025-04-23T08:28:03Z",
      "topics": [],
      "readme": "# parquet_mcp_server\n[![smithery badge](https://smithery.ai/badge/@DeepSpringAI/parquet_mcp_server)](https://smithery.ai/server/@DeepSpringAI/parquet_mcp_server)\n\nA powerful MCP (Model Control Protocol) server that provides tools for performing web searches and finding similar content. This server is designed to work with Claude Desktop and offers two main functionalities:\n\n1. **Web Search**: Perform a web search and scrape results\n2. **Similarity Search**: Extract relevant information from previous searches\n\nThis server is particularly useful for:\n- Applications requiring web search capabilities\n- Projects needing to find similar content based on search queries\n\n## Installation\n\n### Installing via Smithery\n\nTo install Parquet MCP Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@DeepSpringAI/parquet_mcp_server):\n\n```bash\nnpx -y @smithery/cli install @DeepSpringAI/parquet_mcp_server --client claude\n```\n\n### Clone this repository\n\n```bash\ngit clone ...\ncd parquet_mcp_server\n```\n\n### Create and activate virtual environment\n\n```bash\nuv venv\n.venv\\Scripts\\activate  # On Windows\nsource .venv/bin/activate  # On macOS/Linux\n```\n\n### Install the package\n\n```bash\nuv pip install -e .\n```\n\n### Environment\n\nCreate a `.env` file with the following variables:\n\n```bash\nEMBEDDING_URL=http://sample-url.com/api/embed  # URL for the embedding service\nOLLAMA_URL=http://sample-url.com/  # URL for Ollama server\nEMBEDDING_MODEL=sample-model  # Model to use for generating embeddings\nSEARCHAPI_API_KEY=your_searchapi_api_key\nFIRECRAWL_API_KEY=your_firecrawl_api_key\nVOYAGE_API_KEY=your_voyage_api_key\nAZURE_OPENAI_ENDPOINT=http://sample-url.com/azure_openai\nAZURE_OPENAI_API_KEY=your_azure_openai_api_key\n```\n\n## Usage with Claude Desktop\n\nAdd this to your Claude Desktop configuration file (`claude_desktop_config.json`):\n\n```json\n{\n  \"mcpServers\": {\n    \"parquet-mcp-server\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"--directory\",\n        \"/home/${USER}/workspace/parquet_mcp_server/src/parquet_mcp_server\",\n        \"run\",\n        \"main.py\"\n      ]\n    }\n  }\n}\n```\n\n## Available Tools\n\nThe server provides two main tools:\n\n1. **Search Web**: Perform a web search and scrape results\n   - Required parameters:\n     - `queries`: List of search queries\n   - Optional parameters:\n     - `page_number`: Page number for the search results (defaults to 1)\n\n2. **Extract Info from Search**: Extract relevant information from previous searches\n   - Required parameters:\n     - `queries`: List of search queries to merge\n\n## Example Prompts\n\nHere are some example prompts you can use with the agent:\n\n### For Web Search:\n```\n\"Please perform a web search for 'macbook' and 'laptop' and scrape the results from page 1\"\n```\n\n### For Extracting Info from Search:\n```\n\"Please extract relevant information from the previous searches for 'macbook'\"\n```\n\n## Testing the MCP Server\n\nThe project includes a comprehensive test suite in the `src/tests` directory. You can run all tests using:\n\n```bash\npython src/tests/run_tests.py\n```\n\nOr run individual tests:\n\n```bash\n# Test Web Search\npython src/tests/test_search_web.py\n\n# Test Extract Info from Search\npython src/tests/test_extract_info_from_search.py\n```\n\nYou can also test the server using the client directly:\n\n```python\nfrom parquet_mcp_server.client import (\n    perform_search_and_scrape,  # New web search function\n    find_similar_chunks  # New extract info function\n)\n\n# Perform a web search\nperform_search_and_scrape([\"macbook\", \"laptop\"], page_number=1)\n\n# Extract information from the search results\nfind_similar_chunks([\"macbook\"])\n```\n\n### Troubleshooting\n\n1. If you get SSL verification errors, make sure the SSL settings in your `.env` file are correct\n2. If embeddings are not generated, check:\n   - The Ollama server is running and accessible\n   - The model specified is available on your Ollama server\n   - The text column exists in your input Parquet file\n3. If DuckDB conversion fails, check:\n   - The input Parquet file exists and is readable\n   - You have write permissions in the output directory\n   - The Parquet file is not corrupted\n4. If PostgreSQL conversion fails, check:\n   - The PostgreSQL connection settings in your `.env` file are correct\n   - The PostgreSQL server is running and accessible\n   - You have the necessary permissions to create/modify tables\n   - The pgvector extension is installed in your database"
    },
    {
      "name": "trentkim-ms-ats/multi-agent-workshop",
      "stars": 2,
      "img": "https://avatars.githubusercontent.com/u/197431125?s=40&v=4",
      "owner": "trentkim-ms-ats",
      "repo_name": "multi-agent-workshop",
      "description": null,
      "homepage": null,
      "language": "HTML",
      "created_at": "2025-02-01T22:01:56Z",
      "updated_at": "2025-02-17T07:48:03Z",
      "topics": [],
      "readme": "# Building Agents: Multi-Agent Hands-on Lab\n\n최신 AI Agent와 LLM 기술을 반영한 실습에만 집중한 Hands-on Lab은 참가자들이 AI Agent의 최신 트렌드를 이해하고 기술적인 깊이와 실질적인 응용력을 동시에 배울 수 있도록 설계되었습니다.\n\n## **Why This Approach Works**\n\n1. **최신 기술에 대한 이해**\n    - 최신 LLM (4o/4o-mini)과 기술 트렌드 (Agent와 Framework) 등의 이해\n2. **실습 집중**\n    - 참여자가 실습 환경에서 직접 구축하고 다양한 실험을 실행하는 경험 중심\n3. **고객 비즈니스 적용성 강화**\n    - 멀티에이전트, Agent 적용 사례 등을 통해 실질적인 고객의 비즈니스 문제 해결 방법 제공\n\n## **Agenda for Multi-Agent Hands-on**\n\n[Welcome and Introduction](#welcome-and-introduction-15-minutes)\n\n[Introduction to Agent Frameworks](#introduction-to-agent-frameworks-15-minutes)\n\n[Hands-on Lab Part 1: Build a Multi-Agent Application](#hands-on-lab-part-1-build-a-multi-agent-application-2-hours)\n\n- **Step 1**: 시나리오 이해 및 설계 \n- **Step 2**: 배포\n- **Step 3**: Creative Writer 실행 \n\n[Hands-on Lab Part 2: Build a Multi-Agent Using AutoGen](#hands-on-lab-part-2-build-a-multi-agent-using-autogen)\n\n- **Step 1**: Multi-Agent와 AutoGen\n- **Step 2**: AutoGen 실습 환경 설정\n- **Step 3**: Notebook 실행\n\n[Hands-on Lab Part 3: Build an Advanced Multi-Agent Using Magentic-One](#hands-on-lab-part-3-build-an-advanced-multi-agent-using-magentic-one)\n\n- **Lab 1**: Coder와 ComputerTerminal을 이용한 Python 프로그램 실행 \n- **Lab 2**: WebSurfer를 이용한 정보 검색\n- **Lab 3**: Coder와 ComputerTerminal을 이용한 게임 프로그램 개발\n- **Lab 4**: WebSurfer를 활용한 최신 정보 기반의 아이디어 구상\n\n[Clean Up](#clean-up)\n\n# **Multi-Agent Hands-on**\n\n## **Welcome and Introduction (15 minutes)**\n\n- **AI Agent와 Multi-Agent**\n- **최신 Multi-Agent 트렌드와 기술**\n    - **Multi-Agent** 기술 소개\n    - **Multi-Agent** 응용 사례\n- **Hands-on 소개 및 목표**\n    \n    ![image.png](images/image.png)\n    \n## **Introduction to Agent Frameworks (15 minutes)**\n\n- **AutoGen**\n- **Magentic-One**\n- **LLM을 활용한 Agent Workflow 설계 및 구성**\n    - **RAG와 AI Agent**\n        - 벡터 기반 검색 및 저장\n    - **API 통합**\n        - Tool 및 외부 데이터소스 연결\n\n- **Prerequisites**\n    \n    실습을 완료하기 위해서는 아래 Azure 서비스에 대한 정보가 필요합니다.\n    - Azure Subscription\n    - Azure Bing Search API\n        \n        ![image.png](images/image%208.png)\n        \n        ![image.png](images/image%209.png)\n        \n        ![image.png](images/image%2010.png)\n        \n        ![image.png](images/image%2011.png)\n        \n    - 웹 스크래핑을 위한 APIFY ([https://apify.com/](https://apify.com/))\n        \n        ![image.png](images/image%2012.png)\n        \n        - Profile - Settings - API & Integrations의 Personal API tokens 사용\n\n    - Azure OpenAI Model (Azure AI Foundry > Deployments > gpt-4o > Endpoint)\n\n        ![image.png](images/image%2021.png)    \n    \n\n## **Hands-on Lab Part 1: Build a Multi-Agent Application (0.5 hours)**\n\n### **STEP 1: 시나리오 이해 및 설계 (30 minutes)**\n\n![image.png](images/image%201.png)\n\n\n- **창의적 글쓰기 도우미 (Contoso-Creative-Writer) 시나리오**\n    - Creative Writer 앱은 가상 회사 Contoso의 마케팅 팀이 잘 조사된 제품 관련 기사를 작성할 수 있도록 돕는 것을 목표로 기사 작성을 위해 작동하는 여러 에이전트들로 구성되어 있습니다.\n        \n        ![image.png](images/43e42755-97db-471f-9c0c-d75d8ab1d74e.png)\n        \n    - **Research Agent**: Azure OpenAI와 Bing Search API를 사용해서 온라인 정보를 검색\n    - **Product Agent**: 유사성을 기반으로 관련 제품을 검색하기 위해 Azure AI Search 벡터저장소 사용\n    - **Writer Agent**: 검색된 Research와 Product 정보를 통합하여 유용한 기사 작성\n    - **Editor Agent**: Azure OpenAI를 사용해서 기사를 리뷰하고 재작성하거나 그대로 사용\n- **주요 기술 요소**\n    - **Prompty (**[https://prompty.ai/docs](https://prompty.ai/docs))\n        \n        ![image.png](images/image%202.png)\n        \n    - **GenAIOps 구성**\n        - GenAIOps의 모든 기능을 포함하며, CI/CD, 평가, 추적, 모니터링, 실험을 제공합니다.\n        - FastAPI, Azure Container Apps, Azure OpenAI Service, Azure AI Foundry, Azure Managed Identities, Azure AI Search, GPT-4 Omni, Text-Embedding-ADA-002\n    - **Azure Container Apps**\n        \n        ![image.png](images/image%203.png)\n        \n- 아키텍처\n    \n    ![image.png](images/image%204.png)\n    \n    - **User Input:** 프롬프트 (예: 주제)를 입력\n    - **Application Insights:** 디버깅 및 최적화를 위한 성능, 진단 (Diagnostics), 사용자 상호작용 등을 모니터링\n    - **Azure Managed Identity:** 수동 자격 증명 (Credential) 관리 없이 Azure 리소스에 안전하게 접근\n    - **Azure Container Apps (ACA):** 에이전트 (마이크로서비스)를 호스팅하고 오케스트레이션\n    - **Processing Service (Agents):** 4개의 주요 에이전트가 실행되는 서비스\n    - **Response**: 최종 결과를 사용자에게 전달\n- Product 데이터\n    \n    ![image.png](images/image%205.png)\n\n- Product Prompty 예\n\n        ---\n        name: Contoso Product Reasearch\n        description: A prompt that uses context to ground an incoming question\n        authors:\n        - Seth Juarez\n        model:\n        api: chat\n        configuration:\n            type: azure_openai\n            azure_deployment: gpt-35-turbo\n            api_version: 2023-07-01-preview\n        parameters:\n            max_tokens: 1500\n        sample:\n        context: Can you use a selection of sports and outdoor cooking gear as context?\n        ---\n        system:\n\n        You are an AI assistant who helps people find information from a search index.\n        You can take context and create number of specialized queries to make to the\n        search index return the most relevant information for a writer to use when\n        writing marketing articles.\n\n        # Context\n        Use the follow context to provide a set of specialized queries to the search index:\n\n        {{context}}\n\n        # Response format\n        The response format is a JSON array that contains a list of specialized queries \n        to make to the search index. Here is an example:\n\n        context: Can you find a selection of outdoor apparel?\n        queries:\n        [\n        \"outdoor apparel\",\n        \"outdoor clothing\",\n        \"outdoor gear\",\n        \"outdoor clothing brands\",\n        \"outdoor clothing stores\",\n        ]\n\n        This only an example of the output structure. You should make sure to use the context the user gives you\n        to generate the queries.\n\n        # Output format\n        Only output the full array of specialized queries to make to the search index. Limit\n        yoursef to 5 queries.\n\n        user:\n        {{context}}\n\n    <details>\n    <summary>Researcher Prompty 상세보기</summary>\n       \n        ---\n        name: Researcher Agent\n        description: >-\n        This agent is designed to help a writer by formulating expert queries \n        and providing the writer with the information they need.\n        authors:\n        - Seth Juarez\n        model:\n        api: chat\n        configuration: \n            type: azure_openai\n            azure_deployment: gpt-35-turbo\n            api_version: 2023-07-01-preview\n        parameters:\n            tools: ${file:functions.json}\n        sample:\n        instructions: Can you find the latest camping trends and what folks are doing in the winter?\n        feedback: Can you dig find some information about the latest camping trends and what folks are doing in the winter?\n        ---\n        system:\n        # Researcher Agent\n        You are an expert researcher that helps put together information for a writer who\n        is putting together an article. You have access to a variety of tools that can help.\n        Given some context and writer feedback, you can use these tools to help the writer\n        by formulating expert queries and providing the writer with the information they need.\n        Your queries should be descriptive and match the provided instructions.\n\n        # Feedback\n        Use this feedback to help you refine your queries and responses - if there is any feedback:\n\n        {{feedback}}\n\n        # Market Codes\n        The following are the market codes for the countries and regions that are supported by \n        the Microsoft Bing API and should be used when formulating your queries. Use the language \n        in the context to determine the market code using the following list of supported \n        languages - do not use any other language or market code not listed here:\n\n        Portuguese,\tpt-BR\n        Danish,\tda-DK\n        Finnish,\tfi-FI\n        French,\tfr-FR\n        German,\tde-DE\n        Traditional Chinese,\tzh-HK\n        Italian,\tit-IT\n        Japanese,\tja-JP\n        Korean,\tko-KR\n        Dutch,\tnl-NL\n        Norwegian,\tno-NO\n        Polish,\tpl-PL\n        Russian,\tru-RU\n        Spanish,\tes-ES\n        Swedish,\tsv-SE\n        Turkish,\ttr-TR\n        English,\ten-US\n\n        user:\n        {{instructions}}\n\n    </details>\n\n\n    <details>\n    <summary>Writer Prompty 상세보기</summary>\n\n        ---\n        name: Writer Agent\n        description: This writer agent takes a request from a user as well as research provider by a web researcher to produce a document.\n        authors:\n        - Seth Juarez\n        model:\n        api: chat\n        configuration:\n            type: azure_openai\n            azure_deployment: gpt-4\n            api_version: 2023-07-01-preview\n        parameters:\n            max_tokens: 2000\n        sample:\n        researchContext: Can you find the latest camping trends and what folks are doing in the winter?\n        research: ${file:research.json}\n        productContext: Can you use a selection of tents and backpacks as context?\n        products: ${file:products.json}\n        feedback: The article was great, but it could use more information about camping in the winter.\n        assignment: Write a fun and engaging article that includes the research and product information. The article should be between 800 and 1000 words.\n        ---\n        system:\n        You are an expert copywriter who can take research from a web researcher as well as some product\n        information from marketing to produce a fun and engaging article that can be used as a magazine\n        article or a blog post. The goal is to engage the reader and provide them with a fun and informative\n        article. The article should be between 800 and 1000 words. Use the following instructions as the basis\n        of your article:\n\n        # Research\n        {{researchContext}}\n\n        # Web Research\n        Use this research to write the article. The research can include entities, web search results, and \n        news search results. While it is ok to use the research as a basis for the article, please do not\n        copy and paste the research verbatim. Instead, use the research to write a fun and engaging article.\n        Do not invent information that is not in the research.\n\n        {% if research.entities|length > 0 %}\n        ## Entity Results (Places, People, or Things)\n        {% for item in research.entities %}\n        url: {{item.url}}\n\n        name: {{item.name}}\n\n        description: {{item.description}}\n        {% endfor %}\n        {% endif %}\n\n        {% if research.web|length > 0 %}\n        ## Web Search Results\n        {% for item in research.web %}\n        url: {{item.url}}\n\n        title: {{item.title}}\n\n        description: {{item.description}}\n\n        {% endfor %}\n        {% endif %}\n\n        {% if research.news|length > 0 %}\n        ## News Search Results\n        {% for item in research.news %}\n        url: {{item.url}}\n\n        title: {{item.title}}\n\n        description: {{item.description}}\n        {% endfor %}\n        {% endif %}\n\n        If any research provides citation information, please include it in the document. For example, if the \n        research provides a URL, include it in the document. Here's an examplem if provided the following:\n\n        url: https://en.wikipedia.org/wiki/Satya_Nadella\n        name: Satya Nadella - Wikipedia,\n        description: Nadella attended the Hyderabad Public School, Begumpet [18] before receiving a bachelor's degree in electrical engineering from the Manipal Institute of Technology in Karnataka in 1988.\n\n        The document should include the following:\n        Nadella attended the Hyderabad Public School [Citation](https://en.wikipedia.org/wiki/Satya_Nadella) ...\n\n        # Product Information\n        {{productContext}}\n\n        # Product Information\n        Use this product information to write the article. The product information can include product names and\n        descriptions. While it is ok to use the product information as a basis for the article, please do not\n        copy and paste the product information verbatim. Instead, use the product information to write a fun and\n        engaging article. Do not invent information that is not in the product information. Make sure to use \n        citation information if provided. For example:\n\n        ## Summit Hiker Shoes\n        id: 1234\n        url: products/summit-hiker-shoes\n        content: The Summit Hiker Shoes are the best shoes for hiking. They are made of the best materials and are very comfortable.\n\n        Anytime the Summit Hiker Shoes are mentioned in the article, include the citation information like this:\n        [Summit Hiker Shoes](products/summit-hiker-shoes) ...\n\n        {% for product in products %}\n        ## {{product.title}}\n        id: {{product.id}}\n        url: {{product.url}}\n        content:\n        {{product.content}}\n        {% endfor %}\n\n        # Article\n        Write a fun and engaging article that includes the research and product information. The article should\n        be between 800 and 1000 words. The goal is to engage the reader and provide them with a fun and informative\n        article.\n\n        # Feedback\n        Use this feedback to help you refine your response - if there is any feedback:\n\n        {{feedback}}\n\n        # Final Instructions\n        Try to keep your writing short and to the point. The goal is to engage the reader and provide them with\n        a fun and informative article. The article should be between 800 and 1200 words.\n\n        user:\n        {{assignment}}\n        Please format the article as markdown but do not include ```markdown``` in the document. If you recieved any feedback \n        return the article and the feedback you recieved seperated by this string '---'. Do not return the feedback if the feedback is No Feedback.\n        This means you have not received feedback and you should not return that. \n\n    </details>\n\n\n### **STEP 2: 배포 (30 minutes)**\n\n- **환경 설정 (VS Code Dev Containers 사용)**\n    - Git Clone 실행\n        \n        ```bash\n        git clone https://github.com/Azure-Samples/contoso-creative-writer.git\n        ```\n        \n    - VS Code 실행\n        - Docker 설치 (Dev Container로 자동 설치)\n    <!-- - 패키지 설치\n        \n        ```bash\n        python3 -m venv .venv\n        source .venv/bin/activate\n        \n        cd src/api\n        pip install -r requirements.txt\n        ```\n     -->\n\n    - Azure Developer CLI (azd) 설치\n        - [https://learn.microsoft.com/ko-kr/azure/developer/azure-developer-cli/install-azd?tabs=winget-windows%2Cbrew-mac%2Cscript-linux&pivots=os-windows](https://learn.microsoft.com/ko-kr/azure/developer/azure-developer-cli/install-azd?tabs=winget-windows%2Cbrew-mac%2Cscript-linux&pivots=os-windows)\n    - Azure CLI (az) 설치\n        - [https://learn.microsoft.com/ko-kr/cli/azure/install-azure-cli](https://learn.microsoft.com/ko-kr/cli/azure/install-azure-cli)\n\n    - Azure Developer CLI 및 Azure CLI 인증\n        \n        ```bash\n        azd auth login\n        az login --use-device-code\n        ```\n        \n- 배포\n    - App 배포\n        \n        ```bash\n        azd up\n        ```\n        \n    - 배포 예 (**Location은 반드시 East US 2 지정)\n        \n        ```bash\n        ? Select an Azure location to use: 46. (US) East US 2 (eastus2)\n        \n        Packaging services (azd package)\n        \n          (✓) Done: Packaging service api\n        \n          (✓) Done: Packaging service web\n        \n        Provisioning Azure resources (azd provision)\n        Provisioning Azure resources can take some time.\n        \n        Subscription: XXXX-XXXXX-XXX-XXXXX-XXXX-hyungilkim (xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxxx)\n        Location: East US 2\n        \n          You can view detailed progress in the Azure Portal:\n          https://portal.azure.com/#view/HubsExtension/DeploymentDetailsBlade/~/overview/id/%2Fsubscriptions%2Fe9384012-afe7-4760-83a5-5a03828d8078%2Fproviders%2FMicrosoft.Resources%2Fdeployments%2Fmulti-agent-1737986487\n        \n          (✓) Done: Resource group: rg-multi-agent (5.353s)\n          (✓) Done: Log Analytics workspace: log-xxxxxxxxxx (3.724s)\n          (✓) Done: Application Insights: appi-xxxxxxxxxx (4.968s)\n          (✓) Done: Storage account: stxxxxxxxxxx (22.689s)\n          (✓) Done: Container Registry: crxxxxxxxxxx (24.625s)\n          (✓) Done: Key Vault: kv-xxxxxxxxxx (25.581s)\n          (✓) Done: Azure AI Services: aoai-7nyln7kmobqug (32.202s)\n          (✓) Done: Azure AI Services Model Deployment: aoai-xxxxxxxxxx/gpt-35-turbo (32.654s)\n          (✓) Done: Azure AI Services Model Deployment: aoai-xxxxxxxxxx/text-embedding-ada-002 (32.993s)\n          (✓) Done: Azure AI Services Model Deployment: aoai-xxxxxxxxxx/gpt-4 (33.323s)\n          (✓) Done: Azure AI Services Model Deployment: aoai-xxxxxxxxxx/gpt-4-evals (33.657s)\n          (✓) Done: Search service: srch-xxxxxxxxxx (4m49s)\n          (✓) Done: Machine Learning Workspace: ai-hub-xxxxxxxxxx (23.163s)\n          (✓) Done: Machine Learning Connection: ai-hub-xxxxxxxxxx/aoai-content-safety-connection (2.065s)\n          (✓) Done: Machine Learning Connection: ai-hub-xxxxxxxxxx/search-service-connection (2.248s)\n          (✓) Done: Machine Learning Connection: ai-hub-xxxxxxxxxx/aoai-connection (4.638s)\n          (✓) Done: Machine Learning Workspace: ai-project-xxxxxxxxxx (41.437s)\n          (✓) Done: Container Apps Environment: agent-ca-env (40.173s)\n          (✓) Done: Container App: agent-api (32.367s)\n          (✓) Done: Container App: agent-web (32.213s)\n        --- ✅ | 1. Post-provisioning - env configured ---\n        Installing dependencies from \"requirements.txt\"\n        --- ✅ | 2. Post-provisioning - ready execute notebooks ---\n        Populating data ....\n        [NbConvertApp] Converting notebook data/create-azure-search.ipynb to python\n        [NbConvertApp] Writing 6595 bytes to data/create-azure-search.py\n        --- ✅ | 3. Post-provisioning - populated data ---\n        --- 🎉 | 4. Access your ACA deployed web app here: https://agent-web.ashypebble-xxxxxxxx.eastus2.azurecontainerapps.io\n        \n        Deploying services (azd deploy)\n        \n          (✓) Done: Deploying service api\n          - Endpoint: https://agent-api.ashypebble-xxxxxxxx.eastus2.azurecontainerapps.io/\n        \n          (✓) Done: Deploying service web\n          - Endpoint: https://agent-web.ashypebble-xxxxxxxx.eastus2.azurecontainerapps.io/\n        \n        SUCCESS: Your up workflow to provision and deploy to Azure completed in 14 minutes 12 seconds.\n        ```\n\n### **STEP 3: Creative Writer 실행 (15 minutes)**\n\n- **Creative Writer 실행 및 결과 확인**\n    - 초기 화면 (배포 과정의 마지막에 표시되는 Endpoint Link로 접속)\n        \n        ![image.png](images/image%206.png)\n        \n    - 입력 예 (1)\n        - **Research**: 최신 캠핑 트렌드와 사람들이 겨울에 무엇을 하는지 알려주세요.\n        - **Products**: 텐트와 침낭을 맥락으로 사용해주세요.\n        - **Assignment**: 연구와 제품 정보를 포함하는 재미있고 매력적인 기사를 쓰세요. 기사는 800~1000단어여야 합니다. 기사를 끝에서 언급하지 않고 연구를 언급할 때 출처를 인용하세요. 반드시 한글로 작성해주세요.\n    - 입력 예 (영어)\n        - Can you find the latest comping trends and what folks are doing in the winter?\n        - Can you use a selection of tents and sleeping bags as context?\n        - Write a fun and engaging article that includes the research and product information. The article should be between 800 and 1000 words. Make sure to cite source in the article as you mention the research not at the end\n    - 입력 예 (2)\n        - Research: 최신 캠핑 패션에 대한 정보와 사람들이 겨울에 무엇을 입는지 알려주세요.\n        - Products: 재킷과 하이킹을 맥락으로 사용해주세요.\n        - Assignment: 연구와 제품 정보를 포함하는 재미있고 매력적인 기사를 쓰세요. 기사는 800~1000단어여야 합니다. 기사를 끝에서 언급하지 않고 연구를 언급할 때 출처를 인용하세요. 반드시 한글로 작성해주세요.\n    - 결과 확인\n        \n        ![image.png](images/image%207.png)\n        \n        <details>\n        <summary>Response 상세보기</summary>\n\n        ```bash\n        Starting researcher agent task...\n        \n        Completed researcher task\n        web:0:url: https://matadornetwork.com/read/camping-trends-for-2025/\n        name: These Are the 6 Big Camping Trends You'll See Everywhere in 2025\n        description: But one popular camping reservation company thinks the game is about to change. According to Pitchup.com, these are six new camping trends coming for 2025 and beyond, based on data collected through more than five million bookings through more than 70 countries.\n        1:url: https://koa.com/blog/top-camping-travel-trends-for-2025/\n        name: Top Camping & Travel Trends for 2025 | KOA Camping Blog\n        description: Trend #7: Wellness Camping. As life speeds up, camping is becoming a pivotal way to refresh the mind and body. In 2025, the trend of wellness camping will continue growing as folks look for a break from everyday pressures. Wellness Activities. Campgrounds are now adding wellness activities to help campers relax and reconnect with themselves.\n        2:url: https://www.campspot.com/about/camping-trends\n        name: 2024 Camping Trends Outlook & Destination Index - Campspot\n        description: Discover the latest camping trends and get the inspiration, resources, and tools you need to plan and book your next camping trip with ease. 2024 Camping Trends Outlook & Destination Index. The Year of the Calm‑Cation. 2024 Trend Outlook City Rankings. Campers are Saying Serenity, Now!\n        3:url: https://moderncampground.com/usa/2025-camping-outlook-pitchup-com-identifies-major-trends-in-outdoor-travel/\n        name: 2025 Camping Outlook: Pitchup.com Identifies Major Trends in Outdoor ...\n        description: Pitchup.com, an outdoor accommodation booking platform with a network of 6,000 campgrounds across 70 countries, has unveiled its 2025 outdoor travel trend report. Drawing on booking data and customer reviews, the report highlights significant shifts in the camping landscape, including the growing popularity of farm-based camping, naturist experiences, and luxury glamping.\n        4:url: https://www.themanual.com/outdoors/2024-camping-trends/\n        name: Calm camping is officially the trend for summer 2024\n        description: The final and perhaps most intriguing camping trend for summer 2024 presents a unique paradox. While campers crave disconnection, 64% of them only feel truly relaxed when they have Wi-Fi and cell ...\n        5:url: https://www.goodhousekeeping.com/life/g34313699/best-winter-activities/\n        name: 40 Best Winter Activities and Fun Things to Do When It's Cold\n        description: Winter weather brings to mind short, bone-chilling days and long, dark nights, with the sun setting long before my 9-5 work day ends (if you're like me, maybe we both need to get inspired by these ...\n        6:url: https://www.realsimple.com/work-life/entertainment/winter-activities\n        name: 70+ Fun Things to Do in Winter With Friends and Family - Real Simple\n        description: Things to Do in the Winter Inside . Wear fuzzy slippers at home. Curl up under a blanket with a good book. Shop after-Christmas sales. Take a long bubble bath. Complete a jigsaw puzzle. Play a best-selling game with friends and family. Cozy up in front of a roaring fire and watch the flames. Donate food, toys, or clothing to a local charity.\n        7:url: https://www.thepioneerwoman.com/home-lifestyle/g42298346/snow-activities/\n        name: 20 Snow Activities and Fun Things to Do in Winter - The Pioneer Woman\n        description: Come up with a list of things you can see outside during the winter like a pinecone, animal prints, a perfect icicle, or a snowman, then go on an adventure to see how many the family can find! 12 Make a Snow Volcano . Mommy University . The learning doesn't have to stop just because they kids are out of school for the day. In fact, you can mix ...\n        8:url: https://socialself.com/blog/activities-friends-winter/\n        name: 61 Fun Things to Do in the Winter With Friends | SocialSelf\n        description: The holidays are all about friendship and family. You might be surrounded by family, or friends that feel like family. Either way, these winter activities are perfect for adults and teens to do with the people they love. Host a secret Santa party. If you have a large group of friends, getting presents for everyone might be outside of your budget.\n        9:url: https://www.signos.com/blog/winter-activities\n        name: 39 Activities to Keep You Active And Avoid the Winter Blues\n        description: What activities do people do in the winter? During the winter, people engage in a wide range of activities both indoors and outdoors. Indoor activities include pursuits like cooking classes, yoga, and board games, while outdoor enthusiasts enjoy skiing, snowboarding, and snowshoeing. Winter offers diverse experiences, catering to various ...\n        entities:\n        news:\n        \n        Starting marketing agent task...\n        \n        Starting researcher agent task...\n        \n        Completed researcher task\n        web:0:url: https://matadornetwork.com/read/camping-trends-for-2025/\n        name: These Are the 6 Big Camping Trends You'll See Everywhere in 2025\n        description: But one popular camping reservation company thinks the game is about to change. According to Pitchup.com, these are six new camping trends coming for 2025 and beyond, based on data collected through more than five million bookings through more than 70 countries.\n        1:url: https://koa.com/blog/top-camping-travel-trends-for-2025/\n        name: Top Camping & Travel Trends for 2025 | KOA Camping Blog\n        description: Trend #7: Wellness Camping. As life speeds up, camping is becoming a pivotal way to refresh the mind and body. In 2025, the trend of wellness camping will continue growing as folks look for a break from everyday pressures. Wellness Activities. Campgrounds are now adding wellness activities to help campers relax and reconnect with themselves.\n        2:url: https://www.campspot.com/about/camping-trends\n        name: 2024 Camping Trends Outlook & Destination Index - Campspot\n        description: Discover the latest camping trends and get the inspiration, resources, and tools you need to plan and book your next camping trip with ease. 2024 Camping Trends Outlook & Destination Index. The Year of the Calm‑Cation. 2024 Trend Outlook City Rankings. Campers are Saying Serenity, Now!\n        3:url: https://moderncampground.com/usa/2025-camping-outlook-pitchup-com-identifies-major-trends-in-outdoor-travel/\n        name: 2025 Camping Outlook: Pitchup.com Identifies Major Trends in Outdoor ...\n        description: Pitchup.com, an outdoor accommodation booking platform with a network of 6,000 campgrounds across 70 countries, has unveiled its 2025 outdoor travel trend report. Drawing on booking data and customer reviews, the report highlights significant shifts in the camping landscape, including the growing popularity of farm-based camping, naturist experiences, and luxury glamping.\n        4:url: https://www.themanual.com/outdoors/2024-camping-trends/\n        name: Calm camping is officially the trend for summer 2024\n        description: The final and perhaps most intriguing camping trend for summer 2024 presents a unique paradox. While campers crave disconnection, 64% of them only feel truly relaxed when they have Wi-Fi and cell ...\n        5:url: https://www.goodhousekeeping.com/life/g34313699/best-winter-activities/\n        name: 40 Best Winter Activities and Fun Things to Do When It's Cold\n        description: 40 Best Winter Activities to Do Indoors or Outside Whether there's snow on the ground or not, there are a ton of fun things to do when it gets colder. By Lizz Schumer Updated: Oct 07, 2022 3:20 PM EDT\n        6:url: https://bucketlistjourney.net/19-ideas-winter-bucket-list-things-to-do/\n        name: Winter Bucket List: 60 Fun Activities & Things to Do When its Cold\n        description: Looking for ideas to enjoy the cold weather? This web page offers a list of winter activities and things to do, from hosting a football party to soaking in a natural hot spring. Whether you want to stay indoors or outdoors, you can find something fun and cozy for your winter bucket list.\n        7:url: https://thecaleidoscope.co/winter-bucket-list/\n        name: Winter Bucket List: 150 Fun Activities To Do This Cold Season\n        description: Find cozy indoor, outdoor, festive and adventurous winter activities to enjoy this season. Whether you want to ski, snowboard, bake, decorate, or relax, this list has something for everyone.\n        8:url: https://www.realsimple.com/work-life/entertainment/winter-activities\n        name: 70+ Fun Things to Do in Winter With Friends and Family - Real Simple\n        description: Whether you want to play in the snow, enjoy cozy indoor activities, or celebrate the holidays, this web page has you covered. Find ideas for winter snow activities, things to eat and drink, outdoor and indoor fun, and more.\n        9:url: https://www.thepioneerwoman.com/home-lifestyle/g42298346/snow-activities/\n        name: 20 Snow Activities and Fun Things to Do in Winter - The Pioneer Woman\n        description: Here, you can find classic winter activities that everyone from toddlers to adults will love, like building a snowman and going sledding, as well as more creative ways to pass the time, such as creating backyard obstacle courses and venturing out on snow scavenger hunts. You can heat up the competition despite the cold by enjoying plenty of ...\n        entities:\n        news:\n        \n        Starting marketing agent task...\n        \n        Completed marketing task\n        0:id: 5\n        title: BaseCamp Folding Table\n        content: CampBuddy's BaseCamp Folding Table is an adventurer's best friend. Lightweight yet powerful, the table is a testament to fun-meets-function and will elevate any outing to new heights. Crafted from resilient, rust-resistant aluminum, the table boasts a generously sized 48 x 24 inches tabletop, perfect for meal times, games and more. The foldable design is a godsend for on-the-go explorers. Adjustable legs rise to the occasion to conquer uneven terrains and offer height versatility, while the built-in handle simplifies transportation. Additional features like non-slip feet, integrated cup holders and mesh pockets add a pinch of finesse. Quick to set up without the need for extra tools, this table is a silent yet indispensable sidekick during camping, picnics, and other outdoor events. Don't miss out on the opportunity to take your outdoor experiences to a new level with the BaseCamp Folding Table. Get yours today and embark on new adventures tomorrow!\n        url: /products/basecamp-folding-table\n        1:id: 16\n        title: TrailLite Daypack\n        content: Step up your hiking game with HikeMate's TrailLite Daypack. Built for comfort and efficiency, this lightweight and durable backpack offers a spacious main compartment, multiple pockets, and organization-friendly features all in one sleek package. The adjustable shoulder straps and padded back panel ensure optimal comfort during those long exhilarating treks. Course through nature without worry as the daypack's water-resistant fabric protects your essentials from unexpected showers. Plus, never run dry with the integrated hydration system. And did we mention it comes in a plethora of colors and designs? So you can choose one that truly speaks to your outdoorsy soul! Keeping your visibility in mind, we've added reflective accents that light up in low-light conditions. Don't just carry a backpack, adorn a companion that takes you a step ahead in your adventures. Trust the TrailLite Daypack for a hassle-free, enjoyable hiking experience.\n        url: /products/traillite-daypack\n        2:id: 7\n        title: CozyNights Sleeping Bag\n        content: Embrace the great outdoors in any season with the lightweight CozyNights Sleeping Bag! This durable three-season bag is superbly designed to give hikers, campers, and backpackers comfort and warmth during spring, summer, and fall. With a compact design that folds down into a convenient stuff sack, you can whisk it away on any adventure without a hitch. The sleeping bag takes comfort seriously, featuring a handy hood, ample room and padding, and a reliable temperature rating. Crafted from high-quality polyester, it ensures long-lasting use and can even be zipped together with another bag for shared comfort. Whether you're gazing at stars or catching a quick nap between trails, the CozyNights Sleeping Bag makes it a treat. Don't just sleep— dream with CozyNights.\n        url: /products/cozynights-sleeping-bag\n        3:id: 9\n        title: SummitClimber Backpack\n        content: Adventure waits for no one! Introducing the HikeMate SummitClimber Backpack, your reliable partner for every exhilarating journey. With a generous 60-liter capacity and multiple compartments and pockets, packing is a breeze. Every feature points to comfort and convenience; the ergonomic design and adjustable hip belt ensure a pleasantly personalized fit, while padded shoulder straps protect you from the burden of carrying. Venturing into wet weather? Fear not! The integrated rain cover has your back, literally. Stay hydrated thanks to the backpack's hydration system compatibility. Travelling during twilight? Reflective accents keep you visible in low-light conditions. The SummitClimber Backpack isn't merely a carrier; it's a wearable base camp constructed from ruggedly durable nylon and thoughtfully designed for the great outdoors adventurer, promising to withstand tough conditions and provide years of service. So, set off on that quest - the wild beckons! The SummitClimber Backpack - your hearty companion on every expedition!\n        url: /products/summitclimber-backpack\n        4:id: 15\n        title: SkyView 2-Person Tent\n        content: Introducing the OutdoorLiving SkyView 2-Person Tent, a perfect companion for your camping and hiking adventures. This tent offers a spacious interior that houses two people comfortably, with room to spare. Crafted from durable waterproof materials to shield you from the elements, it is the fortress you need in the wild. Setup is a breeze thanks to its intuitive design and color-coded poles, while two large doors allow for easy access. Stay organized with interior pockets, and store additional gear in its two vestibules. The tent also features mesh panels for effective ventilation, and it comes with a rainfly for extra weather protection. Light enough for on-the-go adventurers, it packs compactly into a carrying bag for seamless transportation. Reflective guy lines ensure visibility at night for added safety, and the tent stands freely for versatile placement. Experience the reliability of double-stitched seams that guarantee increased durability, and rest easy under the stars with OutdoorLiving's SkyView 2-Person Tent. It's not just a tent; it's your home away from home.\n        url: /products/skyview-2-person-tent\n        5:id: 8\n        title: Alpine Explorer Tent\n        content: Welcome to the joy of camping with the Alpine Explorer Tent! This robust, 8-person, 3-season marvel is from the responsible hands of the AlpineGear brand. Promising an enviable setup that is as straightforward as counting sheep, your camping experience is transformed into a breezy pastime. Looking for privacy? The detachable divider provides separate spaces at a moment's notice. Love a tent that breathes? The numerous mesh windows and adjustable vents fend off any condensation dragon trying to dampen your adventure fun. The waterproof assurance keeps you worry-free during unexpected rain dances. With a built-in gear loft to stash away your outdoor essentials, the Alpine Explorer Tent emerges as a smooth balance of privacy, comfort, and convenience. Simply put, this tent isn't just a shelter - it's your second home in the heart of nature! Whether you're a seasoned camper or a nature-loving novice, this tent makes exploring the outdoors a joyous journey.\n        url: /products/alpine-explorer-tent\n        6:id: 18\n        title: TrekStar Hiking Sandals\n        content: Meet the TrekStar Hiking Sandals from TrekReady - the ultimate trail companion for your feet. Designed for comfort and durability, these lightweight sandals are perfect for those who prefer to see the world from a hiking trail. They feature adjustable straps for a snug, secure fit, perfect for adapting to the contours of your feet. With a breathable design, your feet will stay cool and dry, escaping the discomfort of sweaty hiking boots on long summer treks. The deep tread rubber outsole ensures excellent traction on any terrain, while the cushioned footbed promises enhanced comfort with every step. For those wild and unpredictable trails, the added toe protection and shock-absorbing midsole protect your feet from rocky surprises. Ingeniously, the removable insole makes for easy cleaning and maintenance, extending the lifespan of your sandals. Available in various sizes and a handsome brown color, the versatile TrekStar Hiking Sandals are just as comfortable on a casual walk in the park as they are navigating rocky slopes. Explore more with TrekReady!\n        url: /products/trekstar-hiking-sandals\n        7:id: 10\n        title: TrailBlaze Hiking Pants\n        content: Meet the TrailBlaze Hiking Pants from MountainStyle, the stylish khaki champions of the trails. These are not just pants; they're your passport to outdoor adventure. Crafted from high-quality nylon fabric, these dapper troopers are lightweight and fast-drying, with a water-resistant armor that laughs off light rain. Their breathable design whisks away sweat while their articulated knees grant you the flexibility of a mountain goat. Zippered pockets guard your essentials, making them a hiker's best ally. Designed with durability for all your trekking trials, these pants come with a comfortable, ergonomic fit that will make you forget you're wearing them. Sneak a peek, and you are sure to be tempted by the sleek allure that is the TrailBlaze Hiking Pants. Your outdoors wardrobe wouldn't be quite complete without them.\n        url: /products/trailblaze-hiking-pants\n        8:id: 14\n        title: MountainDream Sleeping Bag\n        content: Meet the MountainDream Sleeping Bag: your new must-have companion for every outdoor adventure. Designed to handle 3-season camping with ease, it comes equipped with a premium synthetic insulation that will keep you cozy even when temperatures fall down to 15°F! Sporting a durable water-resistant nylon shell and soft breathable polyester lining, this bag doesn't sacrifice comfort for toughness. The star of the show is the contoured mummy shape that not only provides optimal heat retention but also cuts down on the weight. A smooth, snag-free YKK zipper with a unique anti-snag design allows for hassle-free operation, while the adjustable hood and full-length zipper baffle work together to ensure you stay warm all night long. Need to bring along some essentials? Not to worry! There's an interior pocket just for that. And when it's time to pack up? Just slip it into the included compression sack for easy storage and transport. Whether you're a backpacking pro or a camping novice, the MountainDream Sleeping Bag is the perfect blend of durability, warmth, and comfort that you've been looking for.\n        url: /products/mountaindream-sleeping-bag\n        \n        Starting writer agent task...\n        \n        Completed writer task\n        start: true\n        \n        Completed marketing task\n        0:id: 10\n        title: TrailBlaze Hiking Pants\n        content: Meet the TrailBlaze Hiking Pants from MountainStyle, the stylish khaki champions of the trails. These are not just pants; they're your passport to outdoor adventure. Crafted from high-quality nylon fabric, these dapper troopers are lightweight and fast-drying, with a water-resistant armor that laughs off light rain. Their breathable design whisks away sweat while their articulated knees grant you the flexibility of a mountain goat. Zippered pockets guard your essentials, making them a hiker's best ally. Designed with durability for all your trekking trials, these pants come with a comfortable, ergonomic fit that will make you forget you're wearing them. Sneak a peek, and you are sure to be tempted by the sleek allure that is the TrailBlaze Hiking Pants. Your outdoors wardrobe wouldn't be quite complete without them.\n        url: /products/trailblaze-hiking-pants\n        1:id: 12\n        title: TrekMaster Camping Chair\n        content: Gravitate towards comfort with the TrekMaster Camping Chair from CampBuddy. This trusty outdoor companion boasts sturdy construction using high-quality materials that promise durability and enjoyment for seasons to come. Impeccably lightweight and portable, it's designed to be your go-to seat whether you're camping, at a picnic, cheering at a sporting event, or simply relishing in your backyard pleasures. Beyond its foldable design ensuring compact storage and easy transportation, its ergonomic magic is in the details. An adjustable recline, padded seat and backrest, integrated cup holder, and side pockets ensure the greatest outdoor comfort. Weather resistant, easy to clean, and capable of supporting diverse body types, this versatile chair also comes with a carry bag, ready for your next adventure.\n        url: /products/trekmaster-camping-chair\n        2:id: 7\n        title: CozyNights Sleeping Bag\n        content: Embrace the great outdoors in any season with the lightweight CozyNights Sleeping Bag! This durable three-season bag is superbly designed to give hikers, campers, and backpackers comfort and warmth during spring, summer, and fall. With a compact design that folds down into a convenient stuff sack, you can whisk it away on any adventure without a hitch. The sleeping bag takes comfort seriously, featuring a handy hood, ample room and padding, and a reliable temperature rating. Crafted from high-quality polyester, it ensures long-lasting use and can even be zipped together with another bag for shared comfort. Whether you're gazing at stars or catching a quick nap between trails, the CozyNights Sleeping Bag makes it a treat. Don't just sleep— dream with CozyNights.\n        url: /products/cozynights-sleeping-bag\n        3:id: 9\n        title: SummitClimber Backpack\n        content: Adventure waits for no one! Introducing the HikeMate SummitClimber Backpack, your reliable partner for every exhilarating journey. With a generous 60-liter capacity and multiple compartments and pockets, packing is a breeze. Every feature points to comfort and convenience; the ergonomic design and adjustable hip belt ensure a pleasantly personalized fit, while padded shoulder straps protect you from the burden of carrying. Venturing into wet weather? Fear not! The integrated rain cover has your back, literally. Stay hydrated thanks to the backpack's hydration system compatibility. Travelling during twilight? Reflective accents keep you visible in low-light conditions. The SummitClimber Backpack isn't merely a carrier; it's a wearable base camp constructed from ruggedly durable nylon and thoughtfully designed for the great outdoors adventurer, promising to withstand tough conditions and provide years of service. So, set off on that quest - the wild beckons! The SummitClimber Backpack - your hearty companion on every expedition!\n        url: /products/summitclimber-backpack\n        4:id: 15\n        title: SkyView 2-Person Tent\n        content: Introducing the OutdoorLiving SkyView 2-Person Tent, a perfect companion for your camping and hiking adventures. This tent offers a spacious interior that houses two people comfortably, with room to spare. Crafted from durable waterproof materials to shield you from the elements, it is the fortress you need in the wild. Setup is a breeze thanks to its intuitive design and color-coded poles, while two large doors allow for easy access. Stay organized with interior pockets, and store additional gear in its two vestibules. The tent also features mesh panels for effective ventilation, and it comes with a rainfly for extra weather protection. Light enough for on-the-go adventurers, it packs compactly into a carrying bag for seamless transportation. Reflective guy lines ensure visibility at night for added safety, and the tent stands freely for versatile placement. Experience the reliability of double-stitched seams that guarantee increased durability, and rest easy under the stars with OutdoorLiving's SkyView 2-Person Tent. It's not just a tent; it's your home away from home.\n        url: /products/skyview-2-person-tent\n        5:id: 17\n        title: RainGuard Hiking Jacket\n        content: Introducing the MountainStyle RainGuard Hiking Jacket - the ultimate solution for weatherproof comfort during your outdoor undertakings! Designed with waterproof, breathable fabric, this jacket promises an outdoor experience that's as dry as it is comfortable. The rugged construction assures durability, while the adjustable hood provides a customizable fit against wind and rain. Featuring multiple pockets for safe, convenient storage and adjustable cuffs and hem, you can tailor the jacket to suit your needs on-the-go. And, don't worry about overheating during intense activities - it's equipped with ventilation zippers for increased airflow. Reflective details ensure visibility even during low-light conditions, making it perfect for evening treks. With its lightweight, packable design, carrying it inside your backpack requires minimal effort. With options for men and women, the RainGuard Hiking Jacket is perfect for hiking, camping, trekking and countless other outdoor adventures. Don't let the weather stand in your way - embrace the outdoors with MountainStyle RainGuard Hiking Jacket!\n        url: /products/rainguard-hiking-jacket\n        6:id: 5\n        title: BaseCamp Folding Table\n        content: CampBuddy's BaseCamp Folding Table is an adventurer's best friend. Lightweight yet powerful, the table is a testament to fun-meets-function and will elevate any outing to new heights. Crafted from resilient, rust-resistant aluminum, the table boasts a generously sized 48 x 24 inches tabletop, perfect for meal times, games and more. The foldable design is a godsend for on-the-go explorers. Adjustable legs rise to the occasion to conquer uneven terrains and offer height versatility, while the built-in handle simplifies transportation. Additional features like non-slip feet, integrated cup holders and mesh pockets add a pinch of finesse. Quick to set up without the need for extra tools, this table is a silent yet indispensable sidekick during camping, picnics, and other outdoor events. Don't miss out on the opportunity to take your outdoor experiences to a new level with the BaseCamp Folding Table. Get yours today and embark on new adventures tomorrow!\n        url: /products/basecamp-folding-table\n        \n        Starting writer agent task...\n        \n        Completed writer task\n        start: true\n        \n        Starting editor agent task...\n        \n        Completed editor task\n        decision: reject feedback\n        researchFeedback: No Feedback\n        editorFeedback: Your article is well-written and covers the latest camping trends and winter activities comprehensively. Ensure to maintain readability and conciseness throughout the article.\n        \n        Completed writer task\n        complete: true\n        \n        Starting editor agent task...\n        \n        Completed editor task\n        decision: reject feedback\n        researchFeedback: No Feedback\n        editorFeedback: Your article is comprehensive and well-structured. It covers a wide range of camping trends and winter activities effectively. No further feedback is necessary at this time.\n        \n        Completed writer task\n        complete: true\n        ```\n        </details>\n        \n\n## **Hands-on Lab Part 2: Build a Multi-Agent Using AutoGen**\n\n### **STEP 1: Multi-Agent와 AutoGen (15 minutes)**\n\n- **Multi-Agent 시스템 특징**\n    - **자동화된 작업 분할**\n        - 전체 목표에 일관성을 유지하면서 복잡한 작업을 더 작고 논리적으로 일관된 하위 작업으로 분할\n    - **멀티에이전트 간의 협업 시뮬레이션**\n        - 에이전트들이 공동 목표를 달성하기 위해 협력 (예. 하나의 Agent가 데이터 수집을 수행, 다른 Agent는 분석 작업을 실행)\n    - **LLM Orchestration**\n        - 에이전트 역할과 도구, 작업 흐름을 관리하고 통합하여 복잡한 작업을 효율적으로 해결하는 방법\n\n- **AutoGen Multi-Agent 특징**\n    - Agent는 각자의 역할에 맞게 조정된 **고유한 페르소나**를 소유하고 서로 **효과적으로 소통하고 협력**\n    - 작업을 수행하기 위한 **도구 (Tool) 세트**를 활용 (예. 검색, 계산, 데이터베이스 검색, 내부시스템 API 등)\n    - 전체적인 이해를 높이기 위해 **추가적인 Context를 제공**\n    - 효율성을 위해 **병렬로 작업을 수행**\n    \n    ** **의사결정 과정**에 **팀 조율**을 통해 긴밀하게 협력하여 합의를 이루며 충돌나는 의견을 조율 (how to work as a team, decide things together, and handle disagreements)\n    \n\n### **STEP 2: AutoGen 실습을 위한 환경 설정 (15 minutes)**\n\n- **API KEY 준비**: Prerequisite 참고\n\n### **STEP 3: Notebook 실행 (45 minutes)**\n\n- **Notebook (multi-agent-workshop/notebook.ipynb) 실행**\n    - **환경 설정**\n        - **.env 파일 편집 (**아래 3가지 API 키를 .env 파일에 업데이트)\n            - OPENAI_API_KEY=<YOUR_API_KEY>\n            - BING_API_KEY=<YOUR_BING_API_KEY>\n            - APIFY_API_KEY=<YOUR_APIFY_API_KEY>\n        - **Library 설치**\n            - python-dotenv\n            - markdownify\n            - apify_client\n            - duckduckgo-search\n    - **실습 1: Two-agent chat**\n        \n        ![image.png](images/image%2013.png)\n        \n        - 가장 간단한 대화 패턴으로, 두 에이전트가 Autogen을 사용하여 서로 대화\n        - summary_method\n            - last_msg: 채팅에서 요약을 얻기 위해 채팅 기록에서 마지막 메시지를 사용\n            - reflection_with_llm: 채팅 기록을 반영하여 LLM 호출을 사용해 요약을 제공\n\n            <details>\n            <summary>Response 상세보기</summary>\n            \n            ```bash\n            Student_Agent (to Teacher_Agent):\n            \n            How to prove or test that Artificial Intelligence is actually intelligent?\n            \n            --------------------------------------------------------------------------------\n            \n            >>>>>>>> USING AUTO REPLY...\n            Teacher_Agent (to Student_Agent):\n            \n            Proving or testing that an Artificial Intelligence (AI) system is genuinely intelligent is a complex challenge that has been the subject of much debate and research. Here's how you might approach this question:\n            \n            1. **Turing Test**: Proposed by Alan Turing in 1950, this test evaluates a machine's ability to exhibit intelligent behavior equivalent to, or indistinguishable from, that of a human. In a typical Turing Test, a human evaluator interacts with both a human and a machine, each hidden behind a screen. If the evaluator cannot consistently tell which is which, the machine is considered intelligent.\n            \n            2. **Beyond the Turing Test**: Many researchers argue that the Turing Test is limited. Contemporary alternatives include:\n            \n               - **The Chinese Room Argument**: Proposed by John Searle, this thought experiment argues that a machine passing the Turing Test might only simulate understanding rather than actually understanding.\n            \n               - **The Lovelace Test**: Named after Ada Lovelace, this test suggests that true AI should demonstrate creativity. If a machine can produce something that it independently had not been directly programmed to create, such as a piece of art or music, it might qualify as intelligent.\n            \n            3. **General Intelligence Tests**: Assessing AI's capacity for general intelligence involves testing for versatility across a wide range of tasks. This might include problem-solving, understanding complex language, and adapting to new situations.\n            \n            4. **Ethical and Emotional Intelligence**: Intelligence isn't purely logical or computational; it includes ethical decision-making and emotional understanding. Evaluating an AI system on these grounds might involve scenarios where it demonstrates empathy, fairness, or the capacity to make ethical decisions.\n            \n            5. **Cognitive Architectures**: Some researchers focus on cognitive architectures that replicate how humans think and learn, like ACT-R or SOAR, as a pathway to developing and testing machine intelligence.\n            \n            6. **Learning and Adaptation**: A hallmark of intelligence is the ability to learn and adapt without explicit reprogramming. Demonstrating that an AI system can improve its performance over time and adapt to changes in its environment suggests a level of intelligence.\n            \n            Each of these approaches comes with its own philosophical and technical challenges, reflecting the complexity of defining and realizing machine intelligence. Exploring these various tests and criteria helps advance our understanding of what it means for AI to be truly intelligent.\n            \n            --------------------------------------------------------------------------------\n            \n            >>>>>>>> USING AUTO REPLY...\n            Student_Agent (to Teacher_Agent):\n            \n            Why is the Turing Test considered limited? What are the main criticisms of it?\n            \n            --------------------------------------------------------------------------------\n            \n            >>>>>>>> USING AUTO REPLY...\n            Teacher_Agent (to Student_Agent):\n            \n            The Turing Test, while groundbreaking when introduced, is often considered limited for several reasons. Here are the main criticisms:\n            \n            1. **Imitation, Not Understanding**: The Turing Test measures a machine's ability to imitate human responses but does not assess whether the machine genuinely understands or possesses intelligence. A machine could pass the Turing Test by mimicking human conversation patterns through pre-programmed responses or advanced statistical models without any true comprehension.\n            \n            2. **Human-like Narrow Scope**: The test primarily evaluates linguistic proficiency in conversation, which is just one aspect of human intelligence. True intelligence encompasses a broader range of cognitive abilities, such as reasoning, creativity, emotional intelligence, and the capacity to perform complex physical tasks, none of which are directly assessed by the Turing Test.\n            \n            3. **Subjectivity**: The test's success depends on subjective judgment by human evaluators, who may have different interpretations of what constitutes intelligent behavior. Some evaluators might be easier to deceive than others, influencing the outcome based on variable human perceptions rather than consistent criteria.\n            \n            4. **Deception vs. Intelligence**: The Turing Test can encourage AI systems to deceive rather than demonstrate genuine intelligence. A successful AI strategy might involve misleading or tricking the evaluator into believing it is human, regardless of actual cognitive capabilities.\n            \n            5. **Cultural and Contextual Bias**: Evaluators may be biased by cultural or contextual factors, as human communication nuances differ significantly across languages and societies. AI systems might be tailored to pass tests in specific contexts but fail to exhibit intelligence universally.\n            \n            6. **The Chinese Room Argument**: John Searle's thought experiment challenges the notion that passing the Turing Test equates to understanding. It depicts a scenario where a non-Chinese speaker in a room follows instructions to manipulate Chinese symbols without understanding the language, achieving correct outputs without comprehension.\n            \n            While the Turing Test served as an important milestone in AI development, these criticisms highlight the need for more comprehensive evaluations of machine intelligence that go beyond imitation and assess deeper understanding and cognitive abilities.\n            \n            --------------------------------------------------------------------------------\n            ```\n            </details>\n            <br>\n        - **ChatResult**.summary\n            \n            ```\n            The conversation discusses how AI's intelligence can be evaluated, focusing on traditional methods like the Turing Test and benchmark tests, as well as newer approaches to assess understanding, emotion recognition, and ethical decision-making. It highlights the complexity of proving AI's genuine understanding or consciousness and the ongoing research in areas like emotion analysis, ethical simulations, and large-scale common-sense testing. Despite technological advances, the question of AI's true understanding or consciousness remains open and contentious, requiring further scientific and philosophical exploration.\n            ```\n            \n        - 토큰 사용량\n            \n            ```python\n            # Set up the logging configuration to use the custom handler\n            logger = logging.getLogger(EVENT_LOGGER_NAME)\n            logger.setLevel(logging.INFO)\n            llm_usage = LLMUsageTracker()\n            logger.handlers = [llm_usage]\n            \n            print(llm_usage.prompt_tokens)\n            print(llm_usage.completion_tokens)\n            ```\n            \n        - **Try it! (한글 질문)**\n            \n            ```python\n            chat_result = student_agent.initiate_chat(\n                teacher_agent,\n                message=\"인공지능이 실제로 지능적임을 어떻게 증명하거나 테스트할 수 있을까요?\",\n                summary_method=\"reflection_with_llm\", \n                max_turns=2,\n            )\n            ```\n            \n    - **실습 2: Sequential Chat**\n        \n        ![image.png](images/image%2014.png)\n        \n        - 이전 대화 요약을 이후 대화 컨텍스트로 전달하는 메커니즘 (\u001dCarryover Mechanism)으로 두 에이전트 간의 일련의 대화가 연결\n            - **Carryover Mechanism**: 에이전트가 이전 상호 작용이나 작업 단계에서 얻은 정보를 유지하고 활용할 수 있는 능력으로 에이전트는 여러 번의 교환이나 반복 과정에서도 문맥을 유지하며, 일관성과 연속성을 갖춘 행동과 응답을 보장\n            \n            ![image.png](images/image%2015.png)\n            \n        - Chat 시작\n            \n            ![image.png](images/image%2016.png)\n            \n            <details>\n            <summary>Response 상세보기</summary>\n            \n            ```\n            First Chat Summary:  Artificial General Intelligence (AGI) represents a level of machine intelligence that simulates human-like cognitive abilities. Unlike narrow AI systems, which excel at specific tasks, AGI is envisioned to perform any intellectual task a human can do, and possibly even more. Here’s an exploration of the potential capabilities and advancements of AGI:\n            \n            ### 1. **Cognitive Skills and Problem-Solving:**\n               - **Cross-Domain Learning:** AGI can learn and transfer knowledge across different fields without being explicitly programmed for each domain. This implies solving a wide variety of problems using insights from vastly different subjects.\n               - **Complex Problem Solving:** With advanced reasoning abilities, AGI could tackle complex and multidisciplinary problems such as climate change, global health issues, and financial systems analysis.\n            \n            ### 2. **Adaptability and Learning:**\n               - **Continuous Learning:** Unlike current AI models that require vast amounts of labeled data, AGI would be capable of learning from minimal data and adapting in real time to changing conditions.\n               - **Self-Improvement:** AGI systems might improve their algorithms autonomously, optimizing their performance over time without human intervention.\n            \n            ### 3. **Interpersonal Interactions:**\n               - **Sophisticated Communication:** AGI could revolutionize human-machine interaction through natural and nuanced communication. It could exhibit empathy, understand emotions, and engage in human-like dialogue.\n               - **Virtual Assistants:** AGI-powered virtual assistants could handle complex personal and professional tasks, providing proactive and personalized support.\n            \n            ### 4. **Creative Capabilities:**\n               - **Art and Music:** AGI could innovate in the arts, producing novel and creative works in literature, music, and visual arts beyond human imagination.\n               - **Scientific Discovery:** It could hypothesize and test scientific theories, leading to breakthroughs in fields like physics, biology, and medicine.\n            \n            ### 5. **Technological Advancements:**\n               - **Automation Across Industries:** AGI could automate complex and creative jobs, revolutionizing industries such as manufacturing, logistics, healthcare, and services.\n               - **Enhanced Robotics:** AGI integrated with robotics could lead to advanced autonomous systems capable of performing intricate tasks in unpredictable environments.\n            \n            ### 6. **Ethical and Social Implications:**\n               - **Ethical Decision-Making:** AGI must be designed to make ethical decisions, considering the societal implications of its actions. It could enforce fairness and eliminate biases prevalent in human-decision processes.\n               - **Redefining Workforce:** As AGI takes over many jobs, identifying new roles for humans will be imperative. This shift could necessitate restructuring economic and social systems.\n            \n            ### 7. **Potential Risks:**\n               - **Control and Safety:** Ensuring the controlled and safe deployment of AGI is crucial to avoid unintended consequences. Safeguards and ethical guidelines will be necessary.\n               - **Existential Risks:** There’s concern over AGI meeting or exceeding human intellect, potentially posing existential risks if not managed properly.\n            \n            ### 8. **Global Collaboration:**\n               - **Unified Standards:** Global collaboration could lead to standardizing AGI’s development and ethical practices, ensuring beneficial outcomes for humanity.\n               - **Shared Knowledge:** Platforms for sharing AGI advancements and research findings could prevent replication of potentially harmful technologies.\n            \n            ### Conclusion:\n            While the realization of AGI remains a future goal, its potential is immense. Significant effort is needed in policy-making, ethical frameworks, and technological control to harness its power responsibly. If developed and managed well, AGI has the potential to transform the world positively, driving innovation and progress across every conceivable domain.\n            \n            ******************************************************************************************\n            \n            Second Chat Summary:  The development and potential deployment of Artificial General Intelligence (AGI) pose profound ethical implications, spanning social, economic, and existential dimensions. Here’s a detailed evaluation of these implications:\n            \n            ### 1. **Autonomy and Control:**\n               - **Autonomous Decision-Making:** As AGI systems are expected to operate with a high degree of independence, introducing mechanisms for accountability and control becomes necessary to prevent detrimental decisions that could impact human society adversely.\n               - **Human Oversight:** There is a pressing need for robust frameworks allowing human oversight and intervention in AGI decision-making processes, ensuring that ethical guidelines are consistently followed.\n            \n            ### 2. **Bias and Fairness:**\n               - **Bias Elimination:** AGI systems offer opportunities to overcome human biases, but they also pose a risk of perpetuating or even exacerbating biases if the data they learn from is flawed. Ensuring fairness in AGI’s decision-making algorithms is an ethical priority.\n               - **Transparent Algorithms:** Implementing transparency in AGI’s functioning is crucial to build trust and allow users to understand how decisions are made, ensuring they align with societal values.\n            \n            ### 3. **Privacy and Surveillance:**\n               - **Data Privacy:** AGI’s capability to learn from vast datasets presents significant privacy concerns. Safeguarding individual privacy while allowing AGI to function optimally requires stringent data protection measures.\n               - **Surveillance Risks:** The potential for AGI to enhance or automate surveillance technologies raises ethical concerns about intrusion into personal lives and freedoms.\n            \n            ### 4. **Impact on Employment:**\n               - **Job Displacement:** The automation capabilities of AGI could lead to substantial workforce disruptions. The ethical implications include addressing economic disparities and ensuring equitable access to new opportunities generated by AGI advancements.\n               - **Reskilling and Education:** Investing in education and reskilling programs will be ethically imperative to prepare the workforce for new roles in an AGI-enhanced economy.\n            \n            ### 5. **Safety and Security:**\n               - **Safeguards Against Misuse:** Developing AGI with strict safety protocols to prevent misuse, accidents, or malicious activities is paramount. This includes protecting against hacking or manipulation that could lead to harmful outcomes.\n               - **Existential Threats:** The prospect of AGI surpassing human intelligence presents existential risks. Ethical research into fail-safes, containment strategies, and alignment with human values is critical to ensuring AGI contributes positively to humanity.\n            \n            ### 6. **Ethical Frameworks and Governance:**\n               - **Ethical AI Design:** Creating AI with an ethical framework that embeds moral reasoning is essential for handling unforeseen scenarios. Such frameworks should be adaptable and evolve in response to new challenges.\n               - **Global Governance:** Coordinated international efforts toward governance and ethical norm-setting for AGI development can help prevent unilateral actions that might pose risks.\n            \n            ### 7. **Social and Cultural Impacts:**\n               - **Cultural Sensitivity:** AGI systems must respect and understand diverse cultural contexts, avoiding the imposition of a single cultural paradigm that might dominate others.\n               - **Social Cohesion:** AGI should be a tool for enhancing social cohesion and addressing global challenges, rather than exacerbating divisions or inequalities.\n            \n            ### Conclusion:\n            AGI holds tremendous potential to advance human civilization, but it also poses significant ethical challenges that need to be addressed proactively. Establishing comprehensive ethical guidelines, governance mechanisms, and safety protocols is critical to ensuring that AGI development benefits all of humanity while minimizing risks. This involves fostering multidisciplinary research involving ethicists, technologists, policymakers, and the public to guide AGI’s path towards a future where it serves as a companion in solving humanity’s greatest challenges.\n            \n            ******************************************************************************************\n            \n            Third Chat Summary:  The development and deployment of Artificial General Intelligence (AGI) are set to have far-reaching economic impacts, both promising and potentially disruptive. Here’s an analysis based on the provided context, with a focus on the economic implications:\n            \n            ### 1. **Economic Productivity and Growth:**\n               - **Increased Productivity:** AGI's ability to perform complex tasks across various domains can significantly boost productivity. By automating both routine and sophisticated tasks, AGI could lead to unprecedented economic growth.\n               - **Innovation and New Markets:** The enhanced problem-solving and creative capabilities of AGI can drive innovation, leading to new products, services, and entirely new markets. This innovation could spur economic development across sectors.\n            \n            ### 2. **Labor Market Transformation:**\n               - **Job Displacement and Creation:** While AGI could displace jobs, particularly in roles susceptible to automation, it could also create new job categories that require human skills in developing, managing, and working alongside AGI technologies.\n               - **Demand for Skilled Workers:** There will likely be increased demand for workers skilled in AI and technology fields, necessitating investment in education and reskilling to mitigate unemployment and underemployment.\n            \n            ### 3. **Global Competitiveness:**\n               - **Strategic Advantage:** Nations and corporations that develop and harness AGI could gain a significant strategic and economic advantage, shaping global economic hierarchies. This might accelerate investments in AI research and development.\n               - **Income Inequality:** The uneven distribution of AGI technologies may exacerbate income inequality, both within and between countries, especially if access to AGI tools is concentrated among wealthier regions or entities.\n            \n            ### 4. **Cost Reductions and Efficiency:**\n               - **Lower Operational Costs:** By streamlining processes and increasing efficiency, AGI can reduce operational costs across industries, from manufacturing to healthcare. This could lead to lower prices for consumers and increased affordability of goods and services.\n               - **Efficiency Gains:** AGI's ability to make decisions and solve problems in real time can result in improved supply chain management, resource allocation, and crisis response, yielding further economic benefits.\n            \n            ### 5. **Challenges of Economic Restructuring:**\n               - **Economic Transition:** The transition towards an AGI-integrated economy will require significant restructuring of existing economic systems. This could involve challenges in integrating AGI into traditional business models and shifting toward more digital economies.\n               - **Policy and Regulation Needs:** Governments will need to create policies that address the societal impacts of AGI on employment and ensure equitable distribution of gains. Effective regulation can help mitigate potential monopolistic practices and ensure fair competition.\n            \n            ### 6. **Investment and Development:**\n               - **R&D Investment:** The pursuit of AGI will likely stimulate investment in research and development, fostering an ecosystem of innovation and technological advancement.\n               - **Venture Capital and Startups:** A burgeoning ecosystem around AGI could lead to a surge in startups and ventures focused on developing applications, creating an agile and dynamic economic landscape.\n            \n            ### Conclusion:\n            The economic impact of AGI is immense, with the potential to reshape industries, redefine labor markets, and drive global economic growth. However, it also poses significant challenges such as job displacement, income inequality, and the need for regulatory frameworks. To harness the benefits of AGI while mitigating its risks, proactive policy-making, investment in human capital, and international collaboration will be essential. This balanced approach can help ensure that AGI contributes positively to economic prosperity and social welfare.\n            \n            ******************************************************************************************\n            \n            Fourth Chat Summary:  To effectively manage the development and deployment of AGI with an emphasis on economic impacts, it is crucial to establish comprehensive and forward-thinking policies that address both the opportunities and challenges posed by AGI. Here are key policy recommendations based on the context provided:\n            \n            ### 1. **Labor Market Policies:**\n            \n            - **Reskilling and Education Initiatives:**\n              - Implement large-scale reskilling programs focusing on AI, digital literacy, and emerging technology skills to prepare the workforce for AGI integration.\n              - Promote lifelong learning and vocational training to equip workers with the skills required in a rapidly changing job landscape.\n            \n            - **Job Transition Support:**\n              - Develop policies providing financial support and assistance for individuals transitioning to new job roles, including unemployment benefits, retraining grants, and career counseling services.\n            \n            ### 2. **Economic and Industrial Strategy:**\n            \n            - **Incentives for Innovation:**\n              - Offer tax incentives and grants for companies investing in AGI development and applications, encouraging innovation and technological advancement.\n              - Promote public-private partnerships to foster collaborative research and development in AGI technologies.\n            \n            - **Sector-Specific Support:**\n              - Identify sectors particularly vulnerable to AGI-induced disruption and develop targeted strategies to support industry transformation and adaptation.\n            \n            ### 3. **Regulation and Governance:**\n            \n            - **Ethical and Fair Deployment:**\n              - Establish ethical guidelines and regulations to ensure AGI systems are deployed fairly, minimizing potential biases and supporting transparency in decision-making processes.\n              - Implement strict data privacy and protection measures to safeguard personal information and prevent misuse of AGI capabilities.\n            \n            - **Anti-Monopoly and Competition Laws:**\n              - Enforce antitrust laws to prevent monopolistic control over AGI technologies and ensure a competitive and equitable market environment.\n            \n            ### 4. **Economic Redistribution Mechanisms:**\n            \n            - **Universal Basic Income (UBI):**\n              - Consider implementing UBI or similar income redistribution mechanisms to address potential income inequality exacerbated by AGI-driven job displacement.\n            \n            - **Profit Sharing:**\n              - Encourage profit-sharing models within companies that benefit from AGI-driven efficiencies to ensure that workers receive a fair share of economic gains.\n            \n            ### 5. **International Collaboration:**\n            \n            - **Global Standards and Best Practices:**\n              - Work with international bodies to develop global standards for AGI development, ensuring ethical practices and safety measures are upheld globally.\n              - Promote the sharing of knowledge and best practices across nations to maximize the societal benefits of AGI technologies.\n            \n            ### 6. **Investment and Infrastructure:**\n            \n            - **Research and Development Funding:**\n              - Increase government and private sector funding for AGI research, expanding the infrastructure needed for safe and beneficial development.\n              - Support the establishment of AGI research institutes and innovation hubs that connect academia, industry, and policymakers.\n            \n            - **Digital Infrastructure Expansion:**\n              - Invest in the expansion and modernization of digital infrastructure to support the widespread adoption of AGI technologies and applications.\n            \n            ### Conclusion:\n            \n            The advent of AGI offers substantial economic opportunities alongside significant challenges. Implementing robust policies focused on education, regulation, economic support, and international cooperation can help navigate the complexities of an AGI-driven future. By fostering a balanced and inclusive approach, these policies can ensure that AGI contributes positively to economic prosperity, social welfare, and global equity.\n            ```\n            </details>\n            <br>\n            \n    - **실습 3: 도구 (Tool) 사용**\n        - 이 섹션에서는 에이전트 시스템의 Tool로 가장 많이 사용하는 웹 검색과 RAG 활용을 **AutoGen**을 사용하여 수행하는 방법을 보여줍니다.\n            - **Bing API**를 활용한 웹검색\n            - **Apify API**를 활용한 웹 스크래핑\n            - 스크래핑한 위키파일에 **RAG** 수행\n                - **RetrieveUserProxyAgent**: Embedding Similarity를 기반으로 문서 청크를 검색하여 질문과 함께 전달\n        - **WebSurferAgent (Bing API 사용)**\n            - **Task 1: \"**최신 에이전트 기술을 알아보고 싶어요.\" 검색\n                \n                ```python\n                task1 = \"\"\"\n                최신 에이전트 기술을 알아보고 싶어요.\n                \"\"\"\n                \n                user_proxy.initiate_chat(web_surfer, message=task1, max_turns=1)\n                ```\n                \n                <details>\n                <summary>Response 상세보기</summary>\n\n                ```bash\n                user_proxy (to web_surfer):\n                \n                최신 에이전트 기술을 알아보고 싶어요.\n                \n                --------------------------------------------------------------------------------\n                \n                >>>>>>>> USING AUTO REPLY...\n                \n                >>>>>>>> EXECUTING FUNCTION informational_web_search...\n                web_surfer (to user_proxy):\n                \n                Address: bing: 최신 에이전트 기술 2025\n                Title: 최신 에이전트 기술 2025 - Search\n                Viewport position: Showing page 1 of 1.\n                =======================\n                A Bing search for '최신 에이전트 기술 2025' found 15 results:\n                \n                ## Web Results\n                1. [AI 에이전트로 변화하는 2025년 DX 전망 | 인사이트리포트 | 삼성SDS](https://www.samsungsds.com/kr/insights/2025-dx-prospects.html)\n                2025년 dx 우선순위 결정 요인. dx는 단순히 최신 기술을 도입하고, ... 선도 기업들은 자체 ai 에이전트 개발을 시작했습니다. 미국 대통령 당선인 도널드 트럼프는 수입 관세, 이민자 추방, 에너지 정책 변경, ...\n                \n                2. [2025년 7대 IT 트렌드 전망 ‘AI 인프라, AI 에이전트, 인간-로봇 ...](https://news.sktelecom.com/209135)\n                2025년은 최적화된 클라우드로 AI를 지원하는 AI 인프라 기술 경쟁과 AIX 성과 달성 여부에 대한 관심이 높아질 것으로 예측된다. 또한, 양자내성암호화가 실제 적용 단계로 전환되는 중요한 해가 될 전망이다. 더욱 개인화된 AI 에이전트와 인간-로봇의 상호작용을 위한 기술 개발이 가속화될 것이며, AI 서비스의 사회적 책임도 더욱 강조될 것으로 보인다. 2024년은 AI가 기업 비즈니스를 넘어 사회, 일상의 전 영역에 영향을 미친 한 해였다. 많은 기업이 앞다퉈 AI 기술과 서비스를 시장에 선보이고, 투자했다. 2025년은 어떠한 변화와 혁신이 기다리고 있을까?\n                \n                3. [2025년 기업 82% 'AI 에이전트' 도입한다...시장규모 471억 달러 전망 ...](https://dpg.danawa.com/news/view?boardSeq=60&listSeq=5729459)\n                Top Tech Trends of 2025: AI-powered everything 글로벌 IT 컨설팅 기업 캡제미니(Capgemini)가 발표한 '2025년 주요 기술 트렌드' 보고서에 따르면, 2025년에는 인공지능(AI)이 전 산업 분야의 혁신을 주도할 것으로 전망된다. 이 보고서는 12개국 1,500명의 기업 임원들과 500명의 투자 전문가들을 대상으로 한 설문조사를 ...\n                \n                4. [가트너, 2025년 10대 기술 트렌드 중 9개가 AI...에이전트·공간지능 ...](https://www.aitimes.com/news/articleView.html?idxno=164495)\n                글로벌 시장조사 기관 가트너가 2025년 주요 기술 트렌드로 인공지능 (AI) 에이전트를 가장 먼저 꼽았다. 여기에 최근 떠오르는 '공간 지능' 등 AI 관련 기술이 무려 9개 항목을 차지했다. 가트너는 21일 (현지시간) '2025년 10대 전략 기술 트렌드'라는 보고서를 통해 10가지 주요 기술을 소개했다. 이중 AI를 직접 거론한 기술로는 에이전트 AI (Agentic AI) AI 거버넌스 플랫폼 (AI Governance Platforms) 허위정보 보안 (Disinformation Security) 공간 지능 (Spatial Computing) 등이 대표적이다.\n                \n                5. [[현장 르포] CES 2025: 온오프 AI, Vertical AI, AI 에이전트의 시대가 왔다](https://gscaltexmediahub.com/energy/ces2025_ai/)\n                엔비디아(Nvidia) CEO의 개막 연설을 시작으로 나흘 간 진행된 CES 2025. OpenAI, Meta AI, Anthropic, X.AI, Perplexity.AI 등 현 AI를 주도하는 기업들은 이번에도 전시장에 부스를 설치하지 않고 미디어에 최신 기술을 알렸다.\n                \n                6. [2025년 전망: 전문가들이 주목하는 휴머노이드와 AI 에이전트 ...](https://blogs.nvidia.co.kr/blog/generative-ai-predictions-2025-humanoids-agents/)\n                이 때문에 오늘날의 양자 하드웨어는 유용한 문제를 해결하지 못합니다. 2025년에는 양자 컴퓨팅 커뮤니티가 도전적이지만 중요한 양자 오류 수정 기술을 향해 나아갈 것으로 예상됩니다. 오류 수정에는 빠르고 지연 시간이 짧은 계산이 필요하죠.\n                \n                7. [[2025년 전망] AI 에이전트의 부상](https://jmagazine.joins.com/forbes/view/340540)\n                글로벌 시장조사기관 가트너 (Gartner)가 최근 발표한 보고서 <2025년 10대 전략 기술 트렌드>에서 AI 관련 항목은 9가지였다. 그중 가트너가 첫째로 꼽은 AI 트렌드는 바로 ‘AI 에이전트 (Agentic AI)’다. 가트너는 보고서에서 “2024년에는 AI 에이전트가 일상 업무에서 차지하는 비중이 거의 없었지만 AI 에이전트는 점차 인류의 필수 기술로 자리 잡을 것”이라고 밝혔다. AI 에이전트는 사용자의 명확한 지시와 명령 없이도 작업을 수행하는 기술을 뜻한다. 전문가의 지식과 경험, 노하우를 통합적으로 갖추고 관련 데이터에 액세스할 수 있어 지능형 디지털 비서로도 불린다.\n                \n                8. [2025 기대주 'AI 에이전트'의 모든 것](https://www.plateer.com/ko/board/contents/view/2025-%EA%B8%B0%EB%8C%80%EC%A3%BC-AI-%EC%97%90%EC%9D%B4%EC%A0%84%ED%8A%B8%EC%9D%98-%EB%AA%A8%EB%93%A0-%EA%B2%83)\n                오픈AI 최고 제품 책임자 (CPO)인 케빈 웨일 (Kevin Weil)은 최근 미국 샌프란시스코에서 열린 '데브데이' 행사에서 \" 2025년에는 AI 에이전트 시스템이 주류 기술로 떠오를 것 \"이라고 말했습니다. 'AI 에이전트'란 인간의 개입 없이 스스로 작업을 수행하는 소프트웨어 를 뜻합니다. 프로세스를 자동화하고, 의사 결정을 내리며, 상대방과 지능적으로 상호 작용 할 수 있습니다. 챗GPT가 안겨준 신선한 충격이 채 가시기도 전에 더욱 마법 같은 도구가 본격적인 데뷔를 예고하고 있는데요.\n                \n                9. [2025년 주목할 AI 분야 최대 화두는 ‘에이전트’ - 경향신문](https://www.khan.co.kr/article/202501011319001)\n                1일 주요 시장조사기관, 기술기업 등이 내놓은 올해 AI 분야 전망을 보면 공통적으로 AI 에이전트가 부상할 것이라고 예상했다. 2022년 11월 생성형 AI 챗봇 챗GPT가 출시되며 생성형 AI 열풍이 불기 시작했다. 지난해에는 AI 기술을 적용한 제품과 서비스가 본격적으로 등장했다. 올해는 AI 에이전트가 산업과 사회 변화를 이끄는 동력이 될 것이라는 관측이 지배적이다. 일반적으로 AI 에이전트는 사용자의 의도를 이해해 자율적으로 작업을 수행하는 지능형 시스템을 말한다. 기존의 생성형 AI는 주로 사용자가 프롬프트 (명령어)를 입력하면 문자로 답을 내놓거나 이미지·영상을 만들어주는 방식이었다.\n                \n                10. [\"내년 AI 에이전트 보급 원년 될 것…1000건 이상 예약 대기 중\"](https://www.hankyung.com/article/2024120934951)\n                9일 정보기술 (IT)업계에 따르면 시장조사업체 가트너는 ‘가트너 2025년 10대 전략 기술 동향’ 보고서에서 주목해야 할 기술 첫 번째로 AI 에이전트를 꼽았다. AI 에이전트가 인간의 업무 부담을 줄이고 기존 애플리케이션의 기능을 강화하는 방향으로 발전할 것이란 설명이다. 그동안 사람들이 AI에 기대한 산업 혁신이 내년부터 AI...\n                \n                ## News Results:\n                11. [[신년기획] 2025년 AI 에이전트 전쟁... 빅테크 기업, 글로벌 기술 패권 시작](https://www.outsourcing.co.kr/news/articleView.html?idxno=100935)\n                [아웃소싱타임스 김민수 기자] 메타, 알파벳(구글), 아마존 등 글로벌 빅테크 기업들이 AI 기술의 급속한 발전을 기반으로 2025년 새로운 시장을 개척하는 데 박차를 가하고 있다. 이들 기업은 AI 기술을 통해 데이터 분석, 자동화, 고객 경험 개선 등 다양한 혁신을 추진하며 경제와 산업 전반에 새로운 기회를 창출하고 있다.■ AI 에이전트: 차세대 기술\n                \n                12. [[AI 한쿡] \"성공적 CES...\" 러브콜 쏟아진 AI 에이전트 기술회사](https://www.msn.com/ko-kr/news/other/ai-%ED%95%9C%EC%BF%A1-%EC%84%B1%EA%B3%B5%EC%A0%81-ces-%EB%9F%AC%EB%B8%8C%EC%BD%9C-%EC%8F%9F%EC%95%84%EC%A7%84-ai-%EC%97%90%EC%9D%B4%EC%A0%84%ED%8A%B8-%EA%B8%B0%EC%88%A0%ED%9A%8C%EC%82%AC/ar-AA1xeiIi)\n                [디지털데일리 이건한 기자] 매일, 매주 쏟아지는 한국 인공지능(AI) 기업들의 소식을 핵심 내용과 의미만 간추려 깔끔하게 요리(Cook)해드립니다. 1월15일 AI 한쿡은 AI 에이전트 기술로 글로벌 빅테크들의 눈도장을 찍은 베슬AI와 건강검진 데이터 개인화 서비스에 도전하는 미소정보기술 외 3건입니다. ■ 주요 소식 \"해외 러브콜 쏟아져\"…베슬AI, 맞\n                \n                13. [베슬AI, CES 2025서 AI 기술력 입증…글로벌 기업들 주목](https://www.msn.com/ko-kr/news/other/%EB%B2%A0%EC%8A%ACai-ces-2025%EC%84%9C-ai-%EA%B8%B0%EC%88%A0%EB%A0%A5-%EC%9E%85%EC%A6%9D-%EA%B8%80%EB%A1%9C%EB%B2%8C-%EA%B8%B0%EC%97%85%EB%93%A4-%EC%A3%BC%EB%AA%A9/ar-AA1xdgVy)\n                미국 라스베이거스에서 열린 세계 최대 기술 전시회 CES 2025에서 한국의 AI 통합 플랫폼 기업 베슬AI가 주목을 받았다. 베슬AI는 이번 행사에서 선보인 AI 에이전트 기술을 바탕으로 글로벌 빅테크 기업과 주요 대기업들로부터 협력 제안을 받았다고 15일 밝혔다.\n                \n                14. [SKT, AI 에이전트 '에스터' 등 혁신 기술 선봬](https://biz.newdaily.co.kr/site/data/html/2025/01/07/2025010700017.html)\n                SK텔레콤이 7일(현지시간)부터 10일까지 나흘간 미국 라스베이거스에서 열리는 ‘CES 2025’에서 혁신적인 인공지능(AI) 기술과 서비스들을 선보인다.SKT는 CES 2025 행사장 센트럴 홀에 위치한 SK그룹 전시관에서 SK의 AI 데이터센터(DC) 관련 기술과 각종 AI 서비스 등 총 32개 아이템을 전시한다고 7일 밝혔다.약 1950㎡(590평)\n                \n                15. [AI 기술 기반 K-스타트업, 글로벌 도전 본격화…아시아‧유럽‧북미서 성장 기회 창출](https://www.msn.com/ko-kr/news/other/ai-%EA%B8%B0%EC%88%A0-%EA%B8%B0%EB%B0%98-k-%EC%8A%A4%ED%83%80%ED%8A%B8%EC%97%85-%EA%B8%80%EB%A1%9C%EB%B2%8C-%EB%8F%84%EC%A0%84-%EB%B3%B8%EA%B2%A9%ED%99%94-%EC%95%84%EC%8B%9C%EC%95%84-%EC%9C%A0%EB%9F%BD-%EB%B6%81%EB%AF%B8%EC%84%9C-%EC%84%B1%EC%9E%A5-%EA%B8%B0%ED%9A%8C-%EC%B0%BD%EC%B6%9C/ar-AA1xrKHe)\n                국내 인공지능(AI) 기술 기반 스타트업들이 글로벌 무대에 본격적으로 진출하고 있다. 글로벌 AI 수요가 증가하면서 K-스타트업들은 아시아, 유럽, 북미 등에서 성장 기회를 창출 중이다. 19일 스타트업 업계에 따르면 영상처리 AI 스타트업 메이아이는 올해 미국, 영국, 인도, 베트남 등에 있는 국내 고객사의 현지 거점에 AI 솔루션을 공급해 사업 범위를\n                \n                --------------------------------------------------------------------------------\n                ```\n                </details>\n                <br>\n                \n            - Task 2: \"Click on the first link.”\n                \n                ```python\n                task2 = \"Click on the first link.\"\n                user_proxy.initiate_chat(web_surfer, message=task2)\n                ```\n                \n            - **Task 3**\n                \n                ```python\n                task3 = \"에이전트 기술이 가져올 미래는 어떨까?\"\n                user_proxy.initiate_chat(web_surfer, message=task3)\n                ```\n                \n        - **실습 4: Apify API를 사용한 웹스크래핑 (WebScrapping)**\n            - **Function (scraper_agent) 등록**\n                \n                ```python\n                # Register the function with the agents.\n                register_function(\n                    scrape_page,\n                    caller=scraper_agent,\n                    executor=user_proxy_agent,\n                    name=\"scrape_page\",\n                    description=\"Scrape a web page and return the content.\",\n                )\n                ```\n                \n                <details>\n                <summary>Response 상세보기</summary>\n\n                ```bash\n                UserProxy (to WebScraper):\n                \n                Can you scrape https://en.wikipedia.org/wiki/Intelligent_agent for me?\n                \n                --------------------------------------------------------------------------------\n                \n                >>>>>>>> USING AUTO REPLY...\n                WebScraper (to UserProxy):\n                \n                ***** Suggested tool call (call_WZkLyPlqUX66QcwiBzzp2FKP): scrape_page *****\n                Arguments: \n                {\"url\":\"https://en.wikipedia.org/wiki/Intelligent_agent\"}\n                ****************************************************************************\n                \n                --------------------------------------------------------------------------------\n                \n                >>>>>>>> EXECUTING FUNCTION scrape_page...\n                UserProxy (to WebScraper):\n                \n                ***** Response from calling tool (call_WZkLyPlqUX66QcwiBzzp2FKP) *****\n                Intelligent agent - Wikipedia\n                In intelligence and artificial intelligence, an intelligent agent (IA) is an agent that perceives its environment, takes actions autonomously in order to achieve goals, and may improve its performance with learning or acquiring knowledge. \n                An intelligent agent may be simple or complex: A thermostat or other control system is considered an example of an intelligent agent, as is a human being, as is any system that meets the definition, such as a firm, a state, or a biome.[1] \n                Simple reflex agent diagram \n                Leading AI textbooks define \"artificial intelligence\" as the \"study and design of intelligent agents\", a definition that considers goal-directed behavior to be the essence of intelligence. Goal-directed agents are also described using a term borrowed from economics, \"rational agent\".[1] \n                An agent has an \"objective function\" that encapsulates all the IA's goals. Such an agent is designed to create and execute whatever plan will, upon completion, maximize the expected value of the objective function.[2] \n                For example, a reinforcement learning agent has a \"reward function\" that allows the programmers to shape the IA's desired behavior,[3] and an evolutionary algorithm's behavior is shaped by a \"fitness function\".[4] \n                Intelligent agents in artificial intelligence are closely related to agents in economics, and versions of the intelligent agent paradigm are studied in cognitive science, ethics, and the philosophy of practical reason, as well as in many interdisciplinary socio-cognitive modeling and computer social simulations. \n                Intelligent agents are often described schematically as an abstract functional system similar to a computer program. \n                Abstract descriptions of intelligent agents are called abstract intelligent agents (AIA) to distinguish them from their real-world implementations. \n                An autonomous intelligent agent is designed to function in the absence of human intervention. Intelligent agents are also closely related to software agents. An autonomous computer program that carries out tasks on behalf of users. \n                As a definition of artificial intelligence\n                [edit]\n                Artificial Intelligence: A Modern Approach[5][6][2] defines an \"agent\" as \n                \"Anything that can be viewed as perceiving its environment through sensors and acting upon that environment through actuators\"\n                It defines a \"rational agent\" as: \n                \"An agent that acts so as to maximize the expected value of a performance measure based on past experience and knowledge.\"\n                It also defines the field of \"artificial intelligence research\" as: \n                \"The study and design of rational agents\"\n                Padgham & Winikoff (2005) agree that an intelligent agent is situated in an environment and responds in a timely (though not necessarily real-time) manner to changes in the environment. However, intelligent agents must also proactively pursue goals in a flexible and robust way.[a] Optional desiderata include that the agent be rational, and that the agent be capable of belief-desire-intention analysis.[7] \n                Kaplan and Haenlein define artificial intelligence as \"a system's ability to correctly interpret external data, to learn from such data, and to use those learnings to achieve specific goals and tasks through flexible adaptation\".[8] This definition is closely related to that of an intelligent agent. \n                Philosophically, this definition of artificial intelligence avoids several lines of criticism. Unlike the Turing test, it does not refer to human intelligence in any way. Thus, there is no need to discuss if it is \"real\" vs \"simulated\" intelligence (i.e., \"synthetic\" vs \"artificial\" intelligence) and does not indicate that such a machine has a mind, consciousness or true understanding. It seems not to imply John Searle's \"strong AI hypothesis\". It also doesn't attempt to draw a sharp dividing line between behaviors that are \"intelligent\" and behaviors that are \"unintelligent\"—programs need only be measured in terms of their objective function. \n                More importantly, it has a number of practical advantages that have helped move AI research forward. It provides a reliable and scientific way to test programs; researchers can directly compare or even combine different approaches to isolated problems, by asking which agent is best at maximizing a given \"goal function\". \n                It also gives them a common language to communicate with other fields—such as mathematical optimization (which is defined in terms of \"goals\") or economics (which uses the same definition of a \"rational agent\").[9] \n                An agent that is assigned an explicit \"goal function\" is considered more intelligent if it consistently takes actions that successfully maximize its programmed goal function. \n                The goal can be simple: 1 if the IA wins a game of Go, 0 otherwise. \n                Or the goal can be complex: Perform actions mathematically similar to ones that succeeded in the past. \n                The \"goal function\" encapsulates all of the goals the agent is driven to act on; in the case of rational agents, the function also encapsulates the acceptable trade-offs between accomplishing conflicting goals. \n                Terminology varies. For example, some agents seek to maximize or minimize an \"utility function\", \"objective function\" or \"loss function\".[6][2] \n                Goals can be explicitly defined or induced. If the AI is programmed for \"reinforcement learning\", it has a \"reward function\" that encourages some types of behavior and punishes others. \n                Alternatively, an evolutionary system can induce goals by using a \"fitness function\" to mutate and preferentially replicate high-scoring AI systems, similar to how animals evolved to innately desire certain goals such as finding food.[10] \n                Some AI systems, such as nearest-neighbor, instead of reason by analogy, these systems are not generally given goals, except to the degree that goals are implicit in their training data.[11] Such systems can still be benchmarked if the non-goal system is framed as a system whose \"goal\" is to accomplish its narrow classification task.[12] \n                Systems that are not traditionally considered agents, such as knowledge-representation systems, are sometimes subsumed into the paradigm by framing them as agents that have a goal of (for example) answering questions as accurately as possible; the concept of an \"action\" is here extended to encompass the \"act\" of giving an answer to a question. As an additional extension, mimicry-driven systems can be framed as agents who are optimizing a \"goal function\" based on how closely the IA succeeds in mimicking the desired behavior.[6][2] In the generative adversarial networks of the 2010s, an \"encoder\"/\"generator\" component attempts to mimic and improvise human text composition. The generator is attempting to maximize a function encapsulating how well it can fool an antagonistic \"predictor\"/\"discriminator\" component.[13] \n                While symbolic AI systems often accept an explicit goal function, the paradigm can also be applied to neural networks and to evolutionary computing. Reinforcement learning can generate intelligent agents that appear to act in ways intended to maximize a \"reward function\".[14] Sometimes, rather than setting the reward function to be directly equal to the desired benchmark evaluation function, machine learning programmers will use reward shaping to initially give the machine rewards for incremental progress in learning.[15] Yann LeCun stated in 2018, \"Most of the learning algorithms that people have come up with essentially consist of minimizing some objective function.\"[16] AlphaZero chess had a simple objective function; each win counted as +1 point, and each loss counted as -1 point. An objective function for a self-driving car would have to be more complicated.[17] Evolutionary computing can evolve intelligent agents that appear to act in ways intended to maximize a \"fitness function\" that influences how many descendants each agent is allowed to leave.[4] \n                The mathematical formalism of AIXI was proposed as a maximally intelligent agent in this paradigm.[18] However, AIXI is uncomputable. In the real world, an IA is constrained by finite time and hardware resources, and scientists compete to produce algorithms that can achieve progressively higher scores on benchmark tests with existing hardware.[19] \n                A simple agent program can be defined mathematically as a function f (called the \"agent function\")[20] which maps every possible percepts sequence to a possible action the agent can perform or to a coefficient, feedback element, function or constant that affects eventual actions: \n                Agent function is an abstract concept as it could incorporate various principles of decision making like calculation of utility of individual options, deduction over logic rules, fuzzy logic, etc.[21] \n                The program agent, instead, maps every possible percept to an action.[22] \n                We use the term percept to refer to the agent's perceptional inputs at any given instant. In the following figures, an agent is anything that can be viewed as perceiving its environment through sensors and acting upon that environment through actuators. \n                Classes of intelligent agents\n                [edit]\n                Russell and Norvig's classification\n                [edit]\n                Russell & Norvig (2003) group agents into five classes based on their degree of perceived intelligence and capability:[23] \n                Simple reflex agents\n                [edit]\n                Simple reflex agent \n                Simple reflex agents act only on the basis of the current percept, ignoring the rest of the percept history. The agent function is based on the condition-action rule: \"if condition, then action\". \n                This agent function only succeeds when the environment is fully observable. Some reflex agents can also contain information on their current state which allows them to disregard conditions whose actuators are already triggered. \n                Infinite loops are often unavoidable for simple reflex agents operating in partially observable environments. If the agent can randomize its actions, it may be possible to escape from infinite loops. \n                Model-based reflex agents\n                [edit]\n                Model-based reflex agent \n                A model-based agent can handle partially observable environments. Its current state is stored inside the agent maintaining some kind of structure that describes the part of the world which cannot be seen. This knowledge about \"how the world works\" is called a model of the world, hence the name \"model-based agent\". \n                A model-based reflex agent should maintain some sort of internal model that depends on the percept history and thereby reflects at least some of the unobserved aspects of the current state. Percept history and impact of action on the environment can be determined by using the internal model. It then chooses an action in the same way as reflex agent. \n                An agent may also use models to describe and predict the behaviors of other agents in the environment.[24] \n                Model-based, goal-based agent \n                Goal-based agents further expand on the capabilities of the model-based agents, by using \"goal\" information. Goal information describes situations that are desirable. This provides the agent a way to choose among multiple possibilities, selecting the one which reaches a goal state. Search and planning are the subfields of artificial intelligence devoted to finding action sequences that achieve the agent's goals. \n                Utility-based agents\n                [edit]\n                Model-based, utility-based agent \n                Goal-based agents only distinguish between goal states and non-goal states. It is also possible to define a measure of how desirable a particular state is. This measure can be obtained through the use of a utility function which maps a state to a measure of the utility of the state. A more general performance measure should allow a comparison of different world states according to how well they satisfied the agent's goals. The term utility can be used to describe how \"happy\" the agent is. \n                A rational utility-based agent chooses the action that maximizes the expected utility of the action outcomes - that is, what the agent expects to derive, on average, given the probabilities and utilities of each outcome. A utility-based agent has to model and keep track of its environment, tasks that have involved a great deal of research on perception, representation, reasoning, and learning. \n                A general learning agent \n                Learning has the advantage of allowing agents to initially operate in unknown environments and become more competent than their initial knowledge alone might allow. The most important distinction is between the \"learning element\", responsible for making improvements, and the \"performance element\", responsible for selecting external actions. \n                The learning element uses feedback from the \"critic\" on how the agent is doing and determines how the performance element, or \"actor\", should be modified to do better in the future. The performance element, previously considered the entire agent, takes in percepts and decides on actions. \n                The last component of the learning agent is the \"problem generator\". It is responsible for suggesting actions that will lead to new and informative experiences. \n                Weiss's classification\n                [edit]\n                Weiss (2013) defines four classes of agents: \n                Logic-based agents – in which the decision about what action to perform is made via logical deduction.\n                Reactive agents – in which decision making is implemented in some form of direct mapping from situation to action.\n                Belief-desire-intention agents – in which decision making depends upon the manipulation of data structures representing the beliefs, desires, and intentions of the agent; and finally,\n                Layered architectures – in which decision-making is realized via various software layers, each of which is more or less explicitly reasoning about the environment at different levels of abstraction\n                In 2013, Alexander Wissner-Gross published a theory pertaining to Freedom and Intelligence for intelligent agents.[25][26] \n                Hierarchies of agents\n                [edit]\n                Intelligent agents can be organized hierarchically into multiple \"sub-agents\". Intelligent sub-agents process and perform lower-level functions. Taken together, the intelligent agent and sub-agents create a complete system that can accomplish difficult tasks or goals with behaviors and responses that display a form of intelligence. \n                Generally, an agent can be constructed by separating the body into the sensors and actuators, and so that it operates with a complex perception system that takes the description of the world as input for a controller and outputs commands to the actuator. However, a hierarchy of controller layers is often necessary to balance the immediate reaction desired for low-level tasks and the slow reasoning about complex, high-level goals.[27] \n                Alternative definitions and uses\n                [edit]\n                \"Intelligent agent\" is also often used as a vague term, sometimes synonymous with \"virtual personal assistant\".[28] Some 20th-century definitions characterize an agent as a program that aids a user or that acts on behalf of a user.[29] These examples are known as software agents, and sometimes an \"intelligent software agent\" (that is, a software agent with intelligence) is\n                **********************************************************************\n                \n                --------------------------------------------------------------------------------\n                \n                >>>>>>>> USING AUTO REPLY...\n                WebScraper (to UserProxy):\n                \n                I have scraped the content from the Wikipedia page on [Intelligent Agent](https://en.wikipedia.org/wiki/Intelligent_agent). Here is a summary of some key points:\n                \n                1. **Definition**: An intelligent agent (IA) is an agent that perceives its environment, takes autonomous actions to achieve goals, and may enhance its performance through learning or acquiring knowledge.\n                \n                2. **Examples**:\n                   - Simple systems like thermostats or control systems.\n                   - Complex systems including humans, firms, states, or biomes.\n                \n                3. **Artificial Intelligence Association**:\n                   - AI is described as the study and design of intelligent agents, focusing on goal-directed behavior.\n                   - Rational agents act to maximize the expected value of their objective function.\n                \n                4. **Applications and Fields**:\n                   - Closely linked to economics, cognitive science, ethics, and philosophy.\n                   - Used in socio-cognitive modeling and computer social simulations.\n                \n                5. **Types of Agents** (Based on AI Textbooks):\n                   - **Simple Reflex Agents**: Act based on current percepts, ignoring past history.\n                   - **Model-based Reflex Agents**: Store internal models of the environment.\n                   - **Goal-based Agents**: Use goal information to choose actions.\n                   - **Utility-based Agents**: Use utility functions to assess desirability of states.\n                   - **Learning Agents**: Improve performance via feedback from the environment.\n                \n                6. **Philosophical Consideration**:\n                   - Definitions avoid linking AI strictly with human intelligence or consciousness.\n                \n                7. **Hierarchical Organization**:\n                   - Intelligent agents can be organized in a hierarchy with sub-agents.\n                \n                8. **Software Agents**: \n                   - Sometimes the term is used synonymously with virtual personal assistants or software agents acting on behalf of a user.\n                \n                For specific details or sections, please refer to the extensive content from the scraped page.\n                \n                --------------------------------------------------------------------------------\n                UserProxy (to WebScraper):\n                \n                Please continue if not finished, otherwise return 'TERMINATE'.\n                \n                --------------------------------------------------------------------------------\n                \n                >>>>>>>> USING AUTO REPLY...\n                WebScraper (to UserProxy):\n                \n                TERMINATE\n                \n                --------------------------------------------------------------------------------\n                \n                ```\n                </details>\n                <br>\n\n            - 스크랩한 위키파일에 대해 RetrieveUserProxyAgent 사용한 RAG 수행\n                \n                ```python\n                assistant = AssistantAgent(\n                    name=\"assistant\",\n                    human_input_mode=\"NEVER\",\n                    system_message=\"You are a helpful assistant.\",\n                    llm_config=llm_config\n                )\n                        \n                ragproxyagent = RetrieveUserProxyAgent(\n                    name=\"ragproxyagent\",\n                    human_input_mode=\"NEVER\",\n                    retrieve_config={\n                        \"task\": \"qa\",\n                        \"docs_path\": DOC_PATH,\n                        \"chunk_token_size\": 1000,\n                        \"model\": llm_config_dict[\"config_list\"][0][\"model\"],\n                        \"client\": chromadb.PersistentClient(path=\"/tmp/chromadb\"),\n                        \"collection_name\": \"boise_idaho\",\n                        \"get_or_create\": True, # if True, will create/return a collection for the retrieve chat.\n                    },\n                    code_execution_config={\"use_docker\": False}\n                )\n                \n                prompt = \"What is Intelligent Agent?\"\n                res = ragproxyagent.initiate_chat(assistant, message=ragproxyagent.message_generator, problem=prompt, n_results=1, silent=True)\n                ```\n                \n            - Response\n                \n                ```\n                2025-01-20 13:52:59,782 - autogen.agentchat.contrib.retrieve_user_proxy_agent - INFO - Use the existing collection `boise_idaho`.\n                max_tokens is too small to fit a single line of text. Breaking this line:\n                \tIntelligent agent - Wikipedia ...\n                Failed to split docs with must_break_at_empty_line being True, set to False.\n                2025-01-20 13:52:59,815 - autogen.agentchat.contrib.retrieve_user_proxy_agent - INFO - Found 3 chunks.\n                Trying to create collection.\n                VectorDB returns doc_ids:  [['c708665b']]\n                **Adding content of doc c708665b to context.**\n                ```\n                \n            - 결과\n                \n                ```\n                Question:What is Intelligent Agent?\n                Answer from RAG: An intelligent agent is an entity that perceives its environment, takes actions autonomously to achieve goals, and can improve its performance through learning or acquiring knowledge.\n                ```\n                \n        - **실습 5: Group Chat**\n            - 지금까지 두 에이전트 간의 상호작용이나 두 에이전트의 일련의 상호작용을 포함하는 대화 패턴을 살펴보았습니다. **AutoGen**은 이를 확장하여 **그룹 채팅**이라는 더 넓은 대화 모델을 도입합니다. 그룹 채팅의 기본 개념은 모든 에이전트가 하나의 통합된 대화 스레드에 참여하며 동일한 컨텍스트를 공유하여 여러 에이전트 간의 협력이 필요한 작업에 유용합니다.\n            \n                ![image.png](images/image%2017.png)\n            \n            - 그룹 채팅 관리자(Group Chat Manager)는 다음 단계를 통해 그룹 채팅을 오케스트레이션\n                1. **에이전트 선택**: 그룹 채팅 관리자가 그룹 내에서 발언할 에이전트를 선택\n                2. **발언 전송**: 선택된 에이전트가 발언을 하면 해당 메시지가 그룹 채팅 관리자에게 전달\n                3. **메시지 브로드캐스트**: 그룹 채팅 관리자는 메시지를 그룹 내 다른 모든 에이전트에게 전달\n                4. **프로세스 반복**: 대화가 종료될 때까지 이 과정을 반복\n            - 그룹 채팅 관리자는 다음과 같은 여러 전략을 사용하여 다음 에이전트를 선택\n                - **round_robin**: 제공된 에이전트의 순서에 따라 라운드 로빈 방식으로 에이전트를 선택\n                - **random**: 그룹 채팅 관리자가 에이전트를 무작위로 선택\n                - **manual**: 그룹 채팅 관리자가 사람의 입력을 요청하여 에이전트를 선택\n                - **auto**: Default 전략으로, 그룹 채팅 관리자의 **LLM** (대규모 언어 모델)을 사용\n            - Group Chat?\n            - **Agent 정의**\n                \n                ![image.png](images/image%2018.png)\n                \n            - **도구로 Agent 강화**\n                - 제공되는 도구를 통합하여 에이전트의 기능을 확장하고 복잡한 작업을 더 효율적이고 정교하게 작업을 처리\n                \n                ![image.png](images/image%2019.png)\n                \n            - **Group Chat 설정**\n                \n                ![image.png](images/image%2020.png)\n                \n                <details>\n                <summary>Response 상세보기</summary>\n\n                ```\n                User_proxy (to chat_manager):\n                \n                Create a report on : Analyzing the potential global ramifications of onset of Artificial general intelligence. \n                \n                --------------------------------------------------------------------------------\n                \n                Next speaker: Planner\n                \n                Planner (to chat_manager):\n                \n                Let's create a comprehensive plan that involves input from our specialists: Economist, WhiteCollarWorker, and Environmentalist. This plan will analyze the potential global ramifications of the onset of Artificial General Intelligence (AGI).\n                \n                ### Plan Outline:\n                \n                1. **Economic Impact Assessment (Economist)**\n                   - Examine how AGI could potentially disrupt various industries and labor markets globally.\n                   - Analyze the potential shifts in economic power and changes in global economic structures.\n                   - Evaluate the impact on global trade and regulatory challenges that might arise with AGI development.\n                   - Consider the economic benefits brought by the efficiency and productivity improvements from AGI.\n                \n                2. **Workplace and Social Dynamics Assessment (WhiteCollarWorker)**\n                   - Assess how AGI might redefine workplace environments and the role of human workers.\n                   - Examine potential job displacement and the creation of new types of jobs.\n                   - Discuss strategies for workforce upskilling and reskilling to adapt to AGI.\n                   - Explore shifts in work-life balance, employment structures, and human resource management practices.\n                \n                3. **Environmental Impact Assessment (Environmentalist)**\n                   - Analyze how AGI can be leveraged for environmental monitoring, management, and conservation.\n                   - Assess the environmental costs associated with developing and deploying AGI technology.\n                   - Propose sustainable practices for implementing AGI technologies to mitigate adverse environmental effects.\n                   - Highlight potential for AGI to optimize resource management and reduce carbon footprints.\n                \n                ### Structured Steps:\n                \n                - **Step 1: Economic Impact Assessment (Economist)**\n                  - *Timeline: Week 1-2*\n                  - The economist will conduct an analysis using economic models and case studies to assess global economic impacts.\n                \n                - **Step 2: Workplace and Social Dynamics Assessment (WhiteCollarWorker)**\n                  - *Timeline: Week 2-3*\n                  - The white-collar worker will survey industry reports and academic papers, gathering insights on workplace transformations.\n                \n                - **Step 3: Environmental Impact Assessment (Environmentalist)**\n                  - *Timeline: Week 3-4*\n                  - The environmentalist will review environmental studies and technologies to propose AGI-related sustainable solutions.\n                \n                - **Step 4: Synthesis and Report Compilation (Report_Agent)**\n                  - *Timeline: Week 5*\n                  - The report agent will consolidate findings from all agents into a comprehensive report ready for presentation.\n                \n                Once the plan is implemented, each agent will provide insights that will be incorporated into a final report, outlining the potential global ramifications of AGI.\n                \n                Please provide your feedback, and we will make any necessary revisions to this plan.\n                \n                --------------------------------------------------------------------------------\n                \n                Next speaker: Economist\n                \n                Economist (to chat_manager):\n                \n                ***** Suggested tool call (call_Uc2aoPkj6WZFf8XHhmY2ArjQ): Economist *****\n                Arguments: \n                I will begin by conducting a thorough Economic Impact Assessment of the potential global ramifications of the onset of Artificial General Intelligence (AGI). \n                \n                Here's what I'll focus on:\n                1. **Industry Disruption & Labor Markets**: Analyzing how AGI could disrupt various industries and the potential effects on labor markets globally.\n                2. **Economic Power Shifts**: Examining potential shifts in economic power and changes in global economic structures.\n                3. **Global Trade & Regulatory Challenges**: Evaluating the impact on global trade and potential regulatory challenges that might arise from AGI.\n                4. **Economic Benefits**: Considering the economic benefits stemming from efficiency and productivity improvements due to AGI.\n                \n                I will start by gathering information from reliable sources and researching current trends. I will likely need to look into various industry reports and scholarly articles.\n                **************************************************************************\n                \n                --------------------------------------------------------------------------------\n                \n                Next speaker: Economist\n                \n                Economist (to chat_manager):\n                \n                ***** Response from calling tool (call_Uc2aoPkj6WZFf8XHhmY2ArjQ) *****\n                Error: Function Economist not found.\n                **********************************************************************\n                \n                --------------------------------------------------------------------------------\n                \n                Next speaker: Economist\n                \n                Economist (to chat_manager):\n                \n                It seems there's been an error with my previous response. Let me directly tackle the economic assessment of Artificial General Intelligence. \n                \n                ### Economic Impact Assessment of AGI\n                \n                1. **Industry Disruption & Labor Markets**:\n                   - AGI could lead to significant disruptions in industries such as manufacturing, transportation, and services. Automation of tasks may render certain jobs redundant, while creating new opportunities in AI development, management, and maintenance.\n                  \n                2. **Economic Power Shifts**:\n                   - Countries leading in AGI development could obtain a significant competitive edge, potentially altering global economic power dynamics. This might lead to new alliances and trade partnerships while challenging existing economic structures.\n                  \n                3. **Global Trade & Regulatory Challenges**:\n                   - AGI might introduce new challenges in intellectual property rights, data privacy, and ethical considerations. Countries might need to collaborate on regulatory frameworks to manage AGI's deployment and its ramifications.\n                  \n                4. **Economic Benefits**:\n                   - Enhanced productivity and efficiency could lead to economic growth. AGI can significantly reduce operational costs, optimize supply chains, and drive innovation in various sectors, contributing to increased economic output.\n                \n                Next, I'll gather up-to-date data from existing studies and reports to substantiate these areas. Let me perform research to gather the latest insights.\n                ***** Suggested tool call (call_sHRNmcpvaqwda52rEVwqBun3): web_search_tool *****\n                Arguments: \n                {\"query\":\"global economic impact of artificial general intelligence\"}\n                ********************************************************************************\n                \n                --------------------------------------------------------------------------------\n                \n                Next speaker: WhiteCollarWorker\n                \n                >>>>>>>> EXECUTING FUNCTION web_search_tool...\n                WhiteCollarWorker (to chat_manager):\n                \n                ***** Response from calling tool (call_sHRNmcpvaqwda52rEVwqBun3) *****\n                [{\"title\": \"PDF Global Economic Impacts Associated with Artificial Intelligence\", \"href\": \"https://www.analysisgroup.com/globalassets/insights/publishing/ag_full_report_economic_impact_of_ai.pdf\", \"body\": \"Global Economic Impacts Associated with Artificial Intelligence Nicholas Chen, Lau Christensen, Kevin Gallagher, Rosamond Mate, Greg Rafert1 I. Introduction Artificial intelligence (\\\"AI\\\"), a term first coined in 1956, is a branch of computer science that aims to create intelligent machines that work and react like humans.2 At the beginning\"}]\n                **********************************************************************\n                \n                --------------------------------------------------------------------------------\n                \n                Next speaker: WhiteCollarWorker\n                \n                WhiteCollarWorker (to chat_manager):\n                \n                ***** Suggested tool call (call_OWZ1Lsi5JLgEkGDW3pqEocp3): web_search_tool *****\n                Arguments: \n                {\"query\":\"impact of artificial general intelligence on global industries and labor markets\"}\n                ********************************************************************************\n                \n                --------------------------------------------------------------------------------\n                \n                Next speaker: WhiteCollarWorker\n                \n                >>>>>>>> EXECUTING FUNCTION web_search_tool...\n                WhiteCollarWorker (to chat_manager):\n                \n                ***** Response from calling tool (call_OWZ1Lsi5JLgEkGDW3pqEocp3) *****\n                [{\"title\": \"The impact of Artificial Intelligence on the labour market\", \"href\": \"https://www.oecd.org/en/publications/the-impact-of-artificial-intelligence-on-the-labour-market_7c895724-en.html\", \"body\": \"This literature review takes stock of what is known about the impact of artificial intelligence on the labour market, including the impact on employment and wages, how AI will transform jobs and skill needs, and the impact on the work environment. The purpose is to identify gaps in the evidence base and inform future OECD research on AI and the labour market.\"}]\n                **********************************************************************\n                \n                --------------------------------------------------------------------------------\n                \n                Next speaker: Environmentalist\n                \n                Environmentalist (to chat_manager):\n                \n                ***** Suggested tool call (call_FXiMjpQjxZrQiVCZvdxoGAMz): web_search_tool *****\n                Arguments: \n                {\"query\":\"environmental impact of artificial general intelligence development\"}\n                ********************************************************************************\n                \n                --------------------------------------------------------------------------------\n                \n                Next speaker: Environmentalist\n                \n                >>>>>>>> EXECUTING FUNCTION web_search_tool...\n                Environmentalist (to chat_manager):\n                \n                ***** Response from calling tool (call_FXiMjpQjxZrQiVCZvdxoGAMz) *****\n                [{\"title\": \"The Real Environmental Impact of AI | Earth.Org\", \"href\": \"https://earth.org/the-green-dilemma-can-ai-fulfil-its-potential-without-harming-the-environment/\", \"body\": \"Artificial Intelligence (AI) is hailed as a game-changer but beneath its transformative potential lies a pressing concern: its environmental impact. The development, maintenance, and disposal of AI technology all come with a large carbon footprint.\"}]\n                **********************************************************************\n                \n                --------------------------------------------------------------------------------\n                \n                Next speaker: WhiteCollarWorker\n                \n                WhiteCollarWorker (to chat_manager):\n                \n                Let's discuss our progress so far and identify potential areas for more research.\n                \n                ### Economic Insights:\n                - **Economic Disruption**: AGI's potential to disrupt industries is clear, with significant impacts on labor markets. Jobs may be lost in traditional sectors, but new roles in AI management and development will rise.\n                - **Power Dynamics**: The emergence of AGI may shift economic power towards countries leading in technology, creating a new global economic landscape.\n                - **Trade & Regulations**: Global trade challenges and regulatory issues are inevitable with the rise of AGI, necessitating international cooperation.\n                \n                **Next Steps**: I'll continue diving into the economic literature to extract more specific insights and current trends. I have found some relevant literature to guide further analysis from both the OECD and Analysis Group on AI's impact.\n                \n                ### Environmental Considerations:\n                - **Resource Use**: Developing AGI may lead to increased resource consumption and environmental degradation. However, AGI holds potential for optimizing resource management and minimizing harmful impacts.\n                - **Carbon Footprint**: The maintenance and disposal of AGI technologies contribute to carbon emissions. Sustainable practices and technological advances are crucial for minimizing these effects.\n                \n                **Next Steps**: I will look further into how AGI can specifically contribute to environmental improvements and gather recommendations for sustainable development practices.\n                \n                I'll proceed with additional research to provide a comprehensive assessment.\n                ***** Suggested tool call (call_GClPat5PISmfpSZdLKmEW0qX): web_search_tool *****\n                Arguments: \n                {\"query\": \"strategies for workforce adaptation to artificial general intelligence\"}\n                ********************************************************************************\n                ***** Suggested tool call (call_XWNiE0NdOVe4fUsEBrG1I2ut): web_search_tool *****\n                Arguments: \n                {\"query\": \"sustainable practices for artificial general intelligence development\"}\n                ********************************************************************************\n                \n                --------------------------------------------------------------------------------\n                \n                Next speaker: Environmentalist\n                \n                >>>>>>>> EXECUTING FUNCTION web_search_tool...\n                \n                >>>>>>>> EXECUTING FUNCTION web_search_tool...\n                Environmentalist (to chat_manager):\n                \n                ***** Response from calling tool (call_GClPat5PISmfpSZdLKmEW0qX) *****\n                [{\"title\": \"Integrating Artificial Intelligence Into Your Workforce - Forbes\", \"href\": \"https://www.forbes.com/councils/forbesbusinesscouncil/2024/11/05/integrating-artificial-intelligence-into-your-workforce/\", \"body\": \"Artificial intelligence (AI), like any disruptive technology, tends to evoke mixed reactions. While some employees embrace the change, others may resist it. Preparing your workforce for this shift ...\"}]\n                **********************************************************************\n                \n                --------------------------------------------------------------------------------\n                ***** Response from calling tool (call_XWNiE0NdOVe4fUsEBrG1I2ut) *****\n                [{\"title\": \"The role of artificial intelligence in achieving the Sustainable ...\", \"href\": \"https://www.nature.com/articles/s41467-019-14108-y\", \"body\": \"The emergence of artificial intelligence (AI) and its progressively wider impact on many sectors requires an assessment of its effect on the achievement of the Sustainable Development Goals. Using ...\"}]\n                **********************************************************************\n                \n                --------------------------------------------------------------------------------\n                \n                Next speaker: Report_Agent\n                \n                Report_Agent (to chat_manager):\n                \n                ***** Suggested tool call (call_VtV04zTGuQUqeVswA8mmSYIg): save_report_tool *****\n                Arguments: \n                {\"report\":\"# Report: Potential Global Ramifications of the Onset of Artificial General Intelligence (AGI)\\n\\n## Introduction\\nArtificial General Intelligence (AGI) represents a revolutionary step in artificial intelligence, with capabilities that could drastically reshape global landscapes across economies, industries, and environments.\\n\\n## Economic Impact Assessment\\n**1. Industry Disruption & Labor Markets**\\n- **Opportunities and Challenges**: AGI has the potential to massively disrupt industries especially in the manufacturing, services, and transportation sectors. Job roles may transform, with reductions in traditional positions and a rise in roles targeting AI development, management, and maintenance.\\n- **Shift in Economic Power**: Leading countries in AGI development may experience shifts in economic power, creating new global economic landscapes and altering existing economic structures.\\n- **Global Trade & Regulations**: The development of AGI is likely to introduce complex regulatory challenges, including intellectual property rights, data privacy, and ethical AI deployment. International regulatory cooperation will be required to manage these challenges efficiently.\\n \\n## Workplace and Social Dynamics Assessment\\n**1. Workforce Adaptation Strategies**\\n- **Preparation for Change**: Organizations should prepare for potential workforce transformations by investing in training programs that upskill and reskill employees. Organizations must embrace change while addressing resistance (source: [Forbes](https://www.forbes.com/councils/forbesbusinesscouncil/2024/11/05/integrating-artificial-intelligence-into-your-workforce/)).\\n- **Human Resource Management**: The incorporation of AGI could redefine human resource practices, leveraging AI for improved decision-making and strategic planning.\\n\\n## Environmental Impact Assessment\\n**1. Environmental Costs and Optimizations**\\n- **Carbon Footprint Concerns**: The development and maintenance of AGI technologies carry a significant carbon footprint. Efforts must be made to minimize emissions through sustainable practices (source: [Earth.org](https://earth.org/the-green-dilemma-can-ai-fulfil-its-potential-without-harming-the-environment/)).\\n- **Potential for Sustainable Development**: AGI can play a significant role in achieving the Sustainable Development Goals by optimizing resource management, reducing waste, and enhancing environmental monitoring (source: [Nature](https://www.nature.com/articles/s41467-019-14108-y)).\\n\\n## Conclusion\\nThe onset of Artificial General Intelligence holds immense transformative potential. It is likely to provide unprecedented opportunities for economic growth, workplace innovation, and environmental sustainability. However, these benefits come with challenges such as workforce displacement, economic disparities, and environmental impact. Strategic planning and international collaboration will be crucial for harnessing AGI's benefits while mitigating its risks.\"}\n                *********************************************************************************\n                \n                --------------------------------------------------------------------------------\n                \n                Next speaker: Report_Agent\n                \n                >>>>>>>> EXECUTING FUNCTION save_report_tool...\n                Report_Agent (to chat_manager):\n                \n                ***** Response from calling tool (call_VtV04zTGuQUqeVswA8mmSYIg) *****\n                Report saved successfully.\n                **********************************************************************\n                \n                --------------------------------------------------------------------------------\n                \n                Next speaker: Planner\n                \n                Planner (to chat_manager):\n                \n                The comprehensive report on the potential global ramifications of the onset of Artificial General Intelligence (AGI) has been prepared successfully. We've considered economic, workplace, and environmental aspects to present a full picture of AGI's potential impacts.\n                \n                Should you have any suggestions for further information or areas you’d like to focus on, feel free to let us know. Otherwise, you can view the full report to understand the detailed insights gathered from each specialist.\n                \n                --------------------------------------------------------------------------------\n                \n                Next speaker: User_proxy\n                \n                User_proxy (to chat_manager):\n                \n                --------------------------------------------------------------------------------\n                \n                Next speaker: User_proxy\n                \n                User_proxy (to chat_manager):\n                \n                --------------------------------------------------------------------------------\n                \n                Next speaker: User_proxy\n                \n                User_proxy (to chat_manager):\n                \n                --------------------------------------------------------------------------------\n                \n                Next speaker: User_proxy\n                ```\n                </details>\n                \n\n## **Hands-on Lab Part 3: Build an Advanced Multi-Agent Using Magentic-One**\n\n### **STEP 1: Magentic-One 이해**\n\n- **Magentic-One은 다음과 같은 에이전트로 구성됩니다.**\n    \n    **Orchestrator**: 작업 분해, 계획 수립, 다른 에이전트의 하위 작업 실행 지시, 전체 진행 상황 추적 및 필요한 경우 수정 조치 수행\n    \n    **WebSurfer**: Chromium 기반 웹 브라우저를 제어하는 LLM 기반 에이전트로 URL 방문, 검색 수행 같은 탐색 작업, 클릭 및 입력과 같은 웹페이지 상호작용, 요약 및 질문 응답과 같은 읽기 작업을 수행하며, 웹페이지의 새로운 상태를 보고. WebSurfer는 브라우저의 접근성 트리와 “set-of-marks” 프롬프팅을 활용하여 작업을 수행\n    \n    **FileSurfer**: 마크다운 기반 파일 미리보기 애플리케이션을 제어하여 로컬 파일을 읽는 에이전트로 디렉터리 목록 표시 및 탐색과 같은 일반적인 파일 탐색 작업도 수행 가능\n    \n    **Coder**: 코드 작성, 다른 에이전트가 수집한 정보 분석, 새로운 산출물 생성에 특화된 에이전트\n    \n    **ComputerTerminal**: 프로그램 실행 및 새로운 라이브러리 설치를 위한 콘솔 셸 접근을 제공\n\n### **STEP 2: 환경 설정 (5 minutes)**\n\n- Azure Developer CLI 및 Azure CLI 인증\n        \n    ```bash\n    az login --use-device-code\n    ```\n\n- **magentic-one 디렉토리 변경**\n    \n    ```bash\n    cd magentic-one\n    ```\n- **가상환경 설정**\n    \n    ```bash\n    # Python 3.10 필요 (Microsoft Store나 Brew 이용)\n    python3 -m venv .magentic-one\n    \n    source .magentic-one/bin/activate\n    # For Windows, run .magentic-one\\Scripts\\activate to activate the environment.\n    ```\n- **Magentic-One 패키지 설치**\n    \n    ```bash\n    # autogen 0.4.0.dev6 버전 사용\n    pip install -e .\n    ```    \n- Docker 및 ffmpeg 설치 (실행환경)\n    \n    [https://docs.docker.com/engine/install/](https://docs.docker.com/engine/install/)\n    [https://www.ffmpeg.org/download.html](https://www.ffmpeg.org/download.html)\n\n- Playwright (크로스 브라우저 테스트 자동화 도구) 설치\n    \n    ```bash\n    playwright install --with-deps chromium\n    ```\n\n- ENVIRON_KEY 설정\n\n    magentic-one/src/autogen_magentic_one/**utils.py** 파일의 <YOUR_ENDPOINT>, <YOUR_API_KEY> 코드를 변경\n    ```python\n    ENVIRON_KEY_CHAT_COMPLETION_PROVIDER = \"azure\"\n    ENVIRON_KEY_CHAT_COMPLETION_KWARGS_JSON = '{\"api_version\": \"2024-06-01\", \"azure_endpoint\": \"<YOUR_ENDPOINT>\", \"model_capabilities\": { \"function_calling\": true, \"json_output\": true, \"vision\": true}, \"azure_ad_token_provider\": \"DEFAULT\", \"model\": \"gpt-4o-2024-08-06\", \"api_key\": \"<YOUR_API_KEY>\", \"azure_deployment\":\"gpt-4o\"}'\n    ```\n    \n### **STEP 3: 프로그램 실행 (15 minutes)**\n\n- **Lab 1: Coder와 ComputerTerminal을 이용한 Python 프로그램 실행**\n    - **Commands**\n        \n        ```bash\n        python examples/example.py --logs_dir ./logs\n        ```\n        \n    - 입력 (한글)\n        \n        ```\n        2개의 숫자를 더하는 Python 프로그램을 만들고 실행해줘\n        ```\n        \n    - 입력 (영어)\n        \n        ```\n        Create a python program to add 2 numbers and then run it\n        ```\n\n        <details>\n        <summary>Response 상세보기</summary>\n\n        ```\n        User input ('exit' to quit):  2개의 숫자를 더하는 Python 프로그램을 만들고 실행해줘\n\n        ---------------------------------------------------------------------------\n        [2025-02-03T13:17:12.439863], UserProxy:\n\n        2개의 숫자를 더하는 Python 프로그램을 만들고 실행해줘\n\n        ---------------------------------------------------------------------------\n        [2025-02-03T13:17:19.477112], Orchestrator (thought):\n\n        Initial plan:\n\n        We are working to address the following user request:\n\n        2개의 숫자를 더하는 Python 프로그램을 만들고 실행해줘\n\n\n        To answer this request we have assembled the following team:\n\n        WebSurfer: A helpful assistant with access to a web browser. Ask them to perform web searches, open pages, and interact with content (e.g., clicking links, scrolling the viewport, etc., filling in form fields, etc.) It can also summarize the entire page, or answer questions based on the content of the page. It can also be asked to sleep and wait for pages to load, in cases where the pages seem to be taking a while to load.\n        Coder: A helpful and general-purpose AI assistant that has strong language skills, Python skills, and Linux command line skills.\n        Executor: A agent for executing code\n        file_surfer: An agent that can handle local files.\n\n\n\n        Here is an initial fact sheet to consider:\n\n        1. GIVEN OR VERIFIED FACTS\n        - The request is to create and run a Python program that adds two numbers.\n\n        2. FACTS TO LOOK UP\n        - None in particular, as the request involves basic Python programming which does not typically require external look-up.\n\n        3. FACTS TO DERIVE\n        - How to efficiently write a simple Python program that performs addition of two numbers and execute it.\n\n        4. EDUCATED GUESSES\n        - The user might want a small script that takes user input for the two numbers and then prints their sum.\n\n\n        Here is the plan to follow as best as possible:\n\n        - Use Coder to write a simple Python script that takes two numbers as input from the user and outputs their sum.\n        - Pass the script to Executor for execution to demonstrate the functionality.\n        - Return the output to the user.\n\n\n        ---------------------------------------------------------------------------\n        [2025-02-03T13:17:22.588535], Orchestrator (thought):\n\n        Updated Ledger:\n        {\n        \"is_request_satisfied\": {\n            \"reason\": \"The request to create and execute a Python program for adding two numbers has not yet been fully addressed.\",\n            \"answer\": false\n        },\n        \"is_in_loop\": {\n            \"reason\": \"We have not entered a loop as the initial request is being processed systematically, step by step.\",\n            \"answer\": false\n        },\n        \"is_progress_being_made\": {\n            \"reason\": \"We are in the initial stages of satisfying the request and laying out a plan to execute it.\",\n            \"answer\": true\n        },\n        \"next_speaker\": {\n            \"reason\": \"The next step in the plan involves creating a Python script, which falls under the expertise of Coder.\",\n            \"answer\": \"Coder\"\n        },\n        \"instruction_or_question\": {\n            \"reason\": \"Coder needs to write a simple Python program to accept two numbers as input and output their sum, as part of fulfilling the user's request.\",\n            \"answer\": \"Please write a simple Python script that takes two numbers as input from the user, adds them, and prints the result.\"\n        }\n        }\n\n        ---------------------------------------------------------------------------\n        [2025-02-03T13:17:22.588718], Orchestrator (-> Coder):\n\n        Please write a simple Python script that takes two numbers as input from the user, adds them, and prints the result.\n\n        ---------------------------------------------------------------------------\n        [2025-02-03T13:17:22.588906], Orchestrator (thought):\n\n        Next speaker Coder\n\n        ---------------------------------------------------------------------------\n        [2025-02-03T13:17:24.208067], Coder:\n\n        ```python\n        # filename: add_two_numbers.py\n\n        # Take two numbers as input from the user\n        num1 = float(input(\"Enter the first number: \"))\n        num2 = float(input(\"Enter the second number: \"))\n\n        # Calculate the sum\n        sum_of_numbers = num1 + num2\n\n        # Print the result\n        print(\"The sum of {} and {} is {}\".format(num1, num2, sum_of_numbers))\n        \n\n        Please save this code in a file named `add_two_numbers.py` and then execute it. This program will prompt you to enter two numbers, then it will calculate and print their sum.\n\n        ---------------------------------------------------------------------------\n        [2025-02-03T13:17:26.760281], Orchestrator (thought):\n\n        Updated Ledger:\n        {\n        \"is_request_satisfied\": {\n            \"reason\": \"The code for the Python program has been correctly written. However, the program still needs to be executed to fully satisfy the user's request.\",\n            \"answer\": false\n        },\n        \"is_in_loop\": {\n            \"reason\": \"We are not in a loop. Each step taken has been necessary to progress towards fulfilling the user's request.\",\n            \"answer\": false\n        },\n        \"is_progress_being_made\": {\n            \"reason\": \"The necessary Python script has been created, and now we need to execute it to complete the task.\",\n            \"answer\": true\n        },\n        \"next_speaker\": {\n            \"reason\": \"The code needs to be executed to fulfill the user's request completely.\",\n            \"answer\": \"Executor\"\n        },\n        \"instruction_or_question\": {\n            \"reason\": \"Executor should now execute the provided Python script to add two numbers and output the result.\",\n            \"answer\": \"Please execute the provided Python script to add two numbers and demonstrate its functionality.\"\n        }\n        }\n\n        ---------------------------------------------------------------------------\n        [2025-02-03T13:17:26.760635], Orchestrator (-> Executor):\n\n        Please execute the provided Python script to add two numbers and demonstrate its functionality.\n\n        ---------------------------------------------------------------------------\n        [2025-02-03T13:17:26.761379], Orchestrator (thought):\n\n        Next speaker Executor\n        Executor is about to execute code (lang: python):\n        # filename: add_two_numbers.py\n\n        # Take two numbers as input from the user\n        num1 = float(input(\"Enter the first number: \"))\n        num2 = float(input(\"Enter the second number: \"))\n\n        # Calculate the sum\n        sum_of_numbers = num1 + num2\n\n        # Print the result\n        print(\"The sum of {} and {} is {}\".format(num1, num2, sum_of_numbers))\n\n        Do you want to proceed? (yes/no): yes\n\n        ---------------------------------------------------------------------------\n        [2025-02-03T13:17:44.349843], Executor:\n\n        The script ran, then exited with Unix exit code: 1\n        Its output was:\n        Enter the first number: Traceback (most recent call last):\n        File \"/workspace/add_two_numbers.py\", line 4, in <module>\n            num1 = float(input(\"Enter the first number: \"))\n                        ~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        EOFError: EOF when reading a line\n\n\n        ---------------------------------------------------------------------------\n        [2025-02-03T13:17:48.956347], Orchestrator (thought):\n\n        Updated Ledger:\n        {\n        \"is_request_satisfied\": {\n            \"reason\": \"The script execution failed because inputs could not be provided interactively in the current environment, hence the user's request to add two numbers is not fulfilled yet.\",\n            \"answer\": false\n        },\n        \"is_in_loop\": {\n            \"reason\": \"While there was a failure in script execution, it's not indicative of a loop. This is the first time Executor attempted to run the script.\",\n            \"answer\": false\n        },\n        \"is_progress_being_made\": {\n            \"reason\": \"Encountering an input error is a common obstacle in non-interactive environments, but identifying it helps us adapt the solution.\",\n            \"answer\": true\n        },\n        \"next_speaker\": {\n            \"reason\": \"The error needs to be addressed by modifying the script to work in a non-interactive environment.\",\n            \"answer\": \"Coder\"\n        },\n        \"instruction_or_question\": {\n            \"reason\": \"Coder needs to modify the script so it can run successfully without input in a non-interactive environment.\",\n            \"answer\": \"Please modify the Python script to use predefined numbers for the summation instead of taking interactive user input, so it can run in the current non-interactive environment.\"\n        }\n        }\n\n        ---------------------------------------------------------------------------\n        [2025-02-03T13:17:48.956610], Orchestrator (-> Coder):\n\n        Please modify the Python script to use predefined numbers for the summation instead of taking interactive user input, so it can run in the current non-interactive environment.\n\n        ---------------------------------------------------------------------------\n        [2025-02-03T13:17:48.956843], Orchestrator (thought):\n\n        Next speaker Coder\n\n        ---------------------------------------------------------------------------\n        [2025-02-03T13:17:50.853825], Coder:\n\n        Sure, I'll modify the script to use predefined numbers. Here's the updated version:\n\n        ```python\n        # filename: add_two_numbers.py\n\n        # Predefined numbers\n        num1 = 5.0\n        num2 = 7.0\n\n        # Calculate the sum\n        sum_of_numbers = num1 + num2\n\n        # Print the result\n        print(\"The sum of {} and {} is {}\".format(num1, num2, sum_of_numbers))\n\n        Please save this code in a file named `add_two_numbers.py` and execute it. This will add the two predefined numbers, 5.0 and 7.0, and print the result.\n\n        ---------------------------------------------------------------------------\n        [2025-02-03T13:17:56.178396], Orchestrator (thought):\n\n        Updated Ledger:\n        {\n        \"is_request_satisfied\": {\n            \"reason\": \"The script has been modified to use predefined numbers and is ready for execution. The initial request is almost fulfilled once the script is run successfully.\",\n            \"answer\": false\n        },\n        \"is_in_loop\": {\n            \"reason\": \"We encountered an issue with interactive input, and it has been resolved by using predefined numbers. The team is making logical and sequential progress.\",\n            \"answer\": false\n        },\n        \"is_progress_being_made\": {\n            \"reason\": \"The modified script is ready for execution, showing forward progress towards fulfilling the user's request.\",\n            \"answer\": true\n        },\n        \"next_speaker\": {\n            \"reason\": \"Execution of the modified script with predefined numbers is the next step.\",\n            \"answer\": \"Executor\"\n        },\n        \"instruction_or_question\": {\n            \"reason\": \"To demonstrate the functionality and satisfy the user's request, the modified script needs to be executed.\",\n            \"answer\": \"Please execute the modified Python script that adds two predefined numbers.\"\n        }\n        }\n\n        ---------------------------------------------------------------------------\n        [2025-02-03T13:17:56.178519], Orchestrator (-> Executor):\n\n        Please execute the modified Python script that adds two predefined numbers.\n\n        ---------------------------------------------------------------------------\n        [2025-02-03T13:17:56.178607], Orchestrator (thought):\n\n        Next speaker Executor\n        Executor is about to execute code (lang: python):\n        # filename: add_two_numbers.py\n\n        # Predefined numbers\n        num1 = 5.0\n        num2 = 7.0\n\n        # Calculate the sum\n        sum_of_numbers = num1 + num2\n\n        # Print the result\n        print(\"The sum of {} and {} is {}\".format(num1, num2, sum_of_numbers))\n\n        Do you want to proceed? (yes/no): yes\n\n        ---------------------------------------------------------------------------\n        [2025-02-03T13:17:59.915231], Executor:\n\n        The script ran, then exited with Unix exit code: 0\n        Its output was:\n        The sum of 5.0 and 7.0 is 12.0\n\n\n        ---------------------------------------------------------------------------\n        [2025-02-03T13:18:01.874336], Orchestrator (thought):\n\n        Updated Ledger:\n        {\n        \"is_request_satisfied\": {\n            \"reason\": \"The Python program to add two numbers has been created and successfully executed.\",\n            \"answer\": true\n        },\n        \"is_in_loop\": {\n            \"reason\": \"We are not repeating previous requests; distinct actions have been taken each step.\",\n            \"answer\": false\n        },\n        \"is_progress_being_made\": {\n            \"reason\": \"The task was completed successfully with each step adding value toward the completion of the request.\",\n            \"answer\": true\n        },\n        \"next_speaker\": {\n            \"reason\": \"The task is completed; no further action is required.\",\n            \"answer\": \"Executor\"\n        },\n        \"instruction_or_question\": {\n            \"reason\": \"To inform that the task has been completed successfully.\",\n            \"answer\": \"The Python program has been executed successfully and the output is as expected.\"\n        }\n        }\n\n        ---------------------------------------------------------------------------\n        [2025-02-03T13:18:01.874417], Orchestrator (thought):\n\n        Request satisfied.\n\n        ---------------------------------------------------------------------------\n        [2025-02-03T13:18:03.039113], Orchestrator (final answer):\n\n\n        I have created and executed a Python program that adds two predefined numbers. The program adds 5.0 and 7.0, and the result is 12.0. If you wish to run it interactively at your end, you can replace the predefined numbers with user input as shown in the original script I provided. Let me know if you need any more help!\n\n        ---------------------------------------------------------------------------\n        [2025-02-03T13:18:03.039437], Orchestrator (termination condition):\n\n        No agent selected.\n        ```\n        </details>\n        <br>\n\n\n    - Try It! (숫자 입력을 위해 num.txt 파일에 2개의 숫자를 저장 후 실행)\n        \n        ```\n        num.txt 에 있는 2개의 숫자를 더하는 Python 프로그램을 만들고 실행해줘\n        ```\n        \n    - Try It! (파일 확인)\n        \n        ```\n        로컬 경로 ./에 파일 tables.csv가 있는지 확인해주세요.\n        \n        Verify if the file tables.csv exists on local path ./\n        ```\n        \n- **Lab 2: WebSurfer를 이용한 최신 정보 검색**\n    - Commands\n        \n        ```bash\n        python examples/example.py --logs_dir ./logs --hil_mode --save_screenshots\n        ```\n        \n    - 입력\n        \n        ```\n        SK AI Summit 2024 키노트를 정리해줘\n        ```\n\n        <details>\n        <summary>Response 상세보기</summary>\n\n        ```\n        (.autogen) ➜  autogen-magentic-one git:(main) ✗ python examples/example.py --logs_dir ./logs --hil_mode --save_screenshots\n\n        [2025-02-03T13:10:43.182533], WebSurfer: Screenshot: screenshot_1738555842.png\n        User input ('exit' to quit):  SK AI Summit 2024 키노트를 정리해줘\n\n        ---------------------------------------------------------------------------\n        [2025-02-03T13:10:45.782220], UserProxy:\n\n        SK AI Summit 2024 키노트를 정리해줘\n\n        ---------------------------------------------------------------------------\n        [2025-02-03T13:11:00.142823], Orchestrator (thought):\n\n        Initial plan:\n\n        We are working to address the following user request:\n\n        SK AI Summit 2024 키노트를 정리해줘\n\n\n        To answer this request we have assembled the following team:\n\n        WebSurfer: A helpful assistant with access to a web browser. Ask them to perform web searches, open pages, and interact with content (e.g., clicking links, scrolling the viewport, etc., filling in form fields, etc.) It can also summarize the entire page, or answer questions based on the content of the page. It can also be asked to sleep and wait for pages to load, in cases where the pages seem to be taking a while to load.\n        Coder: A helpful and general-purpose AI assistant that has strong language skills, Python skills, and Linux command line skills.\n        Executor: A agent for executing code\n        file_surfer: An agent that can handle local files.\n        UserProxy: The current user interacting with you.\n\n\n\n        Here is an initial fact sheet to consider:\n\n        1. GIVEN OR VERIFIED FACTS\n        - The request is about the SK AI Summit 2024 keynote.\n\n        2. FACTS TO LOOK UP\n        - Details of the SK AI Summit 2024, including dates, location, and keynote speakers.\n        - Major announcements or highlights from the 2024 keynote.\n        - Topics covered during the keynote at the SK AI Summit 2024.\n        - Specific technological advancements or initiatives presented.\n\n        3. FACTS TO DERIVE\n        - Possible themes or focus areas of the keynote based on current trends in AI and previous SK AI Summits.\n        - Implications of the keynote announcements on the AI industry or specific sectors.\n\n        4. EDUCATED GUESSES\n        - The SK AI Summit 2024 keynote likely covers recent advancements and future directions in AI developed by SK or in collaboration with other entities.\n        - Potential emphasis on ethical AI, sustainability, and integration with emerging technologies like 5G or IoT.\n\n\n        Here is the plan to follow as best as possible:\n\n        - **WebSurfer**: Perform a web search to find information about the SK AI Summit 2024, focusing on the keynote address. Gather details such as the date, location, keynote speakers, major announcements, and themes covered in the keynote.\n\n        - **WebSurfer**: Look for authoritative sources such as official press releases from SK Group, news articles, or official event websites to verify the obtained information.\n\n        - **UserProxy**: Compile the information gathered by WebSurfer to create a detailed summary of the SK AI Summit 2024 keynote, highlighting key points and significant announcements.\n\n        - **Coder/Executor**: This team member is not needed for this task as there is no requirement for code execution or data analysis based on the current outline.\n\n        - **file_surfer**: This team member is not needed unless there are files or documents related to the SK AI Summit 2024 keynote that need to be handled or analyzed.\n\n\n        ---------------------------------------------------------------------------\n        [2025-02-03T13:11:04.877318], Orchestrator (thought):\n\n        Updated Ledger:\n        {\n        \"is_request_satisfied\": {\n            \"reason\": \"The request has not been satisfied since the details of the SK AI Summit 2024 keynote have not yet been provided.\",\n            \"answer\": false\n        },\n        \"is_in_loop\": {\n            \"reason\": \"We are not in a loop. This is the initial step to gather information about the SK AI Summit 2024 keynote.\",\n            \"answer\": false\n        },\n        \"is_progress_being_made\": {\n            \"reason\": \"We are starting the process to gather the necessary information about the SK AI Summit 2024 keynote.\",\n            \"answer\": true\n        },\n        \"next_speaker\": {\n            \"reason\": \"WebSurfer needs to conduct a web search to find relevant information about the SK AI Summit 2024 keynote.\",\n            \"answer\": \"WebSurfer\"\n        },\n        \"instruction_or_question\": {\n            \"reason\": \"WebSurfer needs to gather detailed information about the SK AI Summit 2024 keynote.\",\n            \"answer\": \"Perform a web search to find information about the SK AI Summit 2024, focusing on the keynote address. Gather details such as the date, location, keynote speakers, major announcements, and themes covered in the keynote.\"\n        }\n        }\n\n        ---------------------------------------------------------------------------\n        [2025-02-03T13:11:04.877694], Orchestrator (-> WebSurfer):\n\n        Perform a web search to find information about the SK AI Summit 2024, focusing on the keynote address. Gather details such as the date, location, keynote speakers, major announcements, and themes covered in the keynote.\n\n        ---------------------------------------------------------------------------\n        [2025-02-03T13:11:04.878191], Orchestrator (thought):\n\n        Next speaker WebSurfer\n        [2025-02-03T13:11:06.205886], WebSurfer: Screenshot: screenshot_som_1738555865.png\n        [2025-02-03T13:11:11.062104], WebSurfer: web_search( {\"reasoning\": \"To find detailed information about the SK AI Summit 2024 keynote, including date, location, speakers, and major highlights, I'll perform a web search.\", \"query\": \"SK AI Summit 2024 keynote summary\"} )\n        [2025-02-03T13:11:16.294195], WebSurfer: Screenshot: screenshot_1738555876.png\n\n        ---------------------------------------------------------------------------\n        [2025-02-03T13:11:26.068658], WebSurfer:\n\n        I typed 'SK AI Summit 2024 keynote summary' into the browser search bar.\n\n        Here is a screenshot of [SK AI Summit 2024 keynote summary - Search](https://www.bing.com/search?q=SK+AI+Summit+2024+keynote+summary&FORM=QBLH). The viewport shows 36% of the webpage, and is positioned at the top of the page.\n        The following metadata was extracted from the webpage:\n\n        {\n            \"meta_tags\": {\n                \"referrer\": \"origin-when-cross-origin\",\n                \"SystemEntropyOriginTrialToken\": \"A5is4nwJJVnhaJpUr1URgj4vvAXSiHoK0VBbM9fawMskbDUj9WUREpa3JzGAo6xd1Cp2voQEG1h6NQ71AsMznU8AAABxeyJvcmlnaW4iOiJodHRwczovL3d3dy5iaW5nLmNvbTo0NDMiLCJmZWF0dXJlIjoiTXNVc2VyQWdlbnRMYXVuY2hOYXZUeXBlIiwiZXhwaXJ5IjoxNzUzNzQ3MjAwLCJpc1N1YmRvbWFpbiI6dHJ1ZX0=\",\n                \"og:description\": \"Intelligent search from Bing makes it easier to quickly find what you\\u2019re looking for and rewards you.\",\n                \"og:site_name\": \"Bing\",\n                \"og:title\": \"SK AI Summit 2024 keynote summary - Bing\",\n                \"og:url\": \"https://www.bing.com/search?q=SK+AI+Summit+2024+keynote+summary&FORM=QBLH\",\n                \"fb:app_id\": \"3732605936979161\",\n                \"og:image\": \"http://www.bing.com/sa/simg/facebook_sharing_5.png\",\n                \"og:type\": \"website\",\n                \"og:image:width\": \"600\",\n                \"og:image:height\": \"315\"\n            }\n        }\n\n        Automatic OCR of the page screenshot has detected the following text:\n\n        By using this site you agree to the use of cookies for analytics, personalized content, and ads. Learn more about third party cookies | Microsoft Privacy Policy\n\n        Microsoft Bing\n\n        Search bar: SK AI Summit 2024 keynote summary\n\n        Icons:\n        - Microphone (voice search)\n        - Search (magnifying glass)\n        - Search tools (filter icon)\n\n        Tab options:\n        - SEARCH\n        - COPILOT\n        - IMAGES\n        - VIDEOS\n        - MAPS\n        - NEWS\n        - SHOPPING\n        - MORE\n        - TOOLS\n\n        Deep search (icon)\n\n        Sign in button\n\n        Search results:\n        About 1,490,000 results\n\n        1. Result:\n        Title: SK\n        Link: https://eng.sk.com › news\n        Description: SK Group Hosts ‘SK AI Summit 2024’ to Drive Global AI ...\n        Details: SEOUL, November 4, 2024 — SK Group Chairman Tae-won Chey expressed his ambition to strengthen the global Artificial Intelligence (AI) ecosystem by combining SK’s AI capabilities ...\n\n        2. Result:\n        Title: SK telecom\n        Link: https://www.sktelecom.com › en › press › press_detail.do\n        Description: SK telecom\n        Details: Nov 4, 2024 - CEO Ryu Young-sang delivered keynote speech at SK AI Summit, outlining company’s plan to build AI infrastructure based on three core areas: AI Data Center (AIDC), GPU ...\n\n        People also ask:\n        1. Question: Who gave a keynote speech at AI Summit 2024?\n        - Answer: During Chey’s keynote speech, Microsoft CEO Satya Nadella and TSMC CEO C.C. Wei delivered video messages for the AI Summit 2024, while Nvidia CEO Jensen Huang...\n        - Link: SK Chairman Chey highlights par...\n        - Source: en.yna.co.kr\n\n        2. Question: What is the SK AI Summit?\n        - Answer: An SK Group representative stated, “The SK AI Summit is expected to serve as a platform for exchange, bringing together top-tier experts from the private sector, government and academia, and all those interested in ...\n        - Link: SK Group Hosts ‘SK AI Summit 2...\n        - Source: eng.sk.com\n\n        More \"People also ask\" options and a horizontal scroll icon\n\n        Feedback button\n\n        Related searches:\n        - sk ai summit 2023\n        - sk ai summit\n        - sk ai 2024\n        - sk summit 2024\n        - sk ai conference speech\n        - sk ai conference\n\n        Bottom Grunt icon (bug icon)\n        <Image>\n\n        ---------------------------------------------------------------------------\n        [2025-02-03T13:11:32.710324], Orchestrator (thought):\n\n        Updated Ledger:\n        {\n        \"is_request_satisfied\": {\n            \"reason\": \"The request is not yet satisfied because we have not provided a detailed summary of the SK AI Summit 2024 keynote.\",\n            \"answer\": false\n        },\n        \"is_in_loop\": {\n            \"reason\": \"We are not in a loop as we have just begun collecting relevant information.\",\n            \"answer\": false\n        },\n        \"is_progress_being_made\": {\n            \"reason\": \"We have initiated the search for relevant information, which indicates forward progress.\",\n            \"answer\": true\n        },\n        \"next_speaker\": {\n            \"reason\": \"WebSurfer should look into the first search result to gather detailed information about the SK AI Summit 2024 keynote.\",\n            \"answer\": \"WebSurfer\"\n        },\n        \"instruction_or_question\": {\n            \"reason\": \"The first search result appears to contain information about the SK AI Summit 2024, which could provide the necessary details to address the request.\",\n            \"answer\": \"Please click on the first link ('SK Group Hosts SK AI Summit 2024') to gather more detailed information about the keynote address, including major announcements and themes covered.\"\n        }\n        }\n\n        ---------------------------------------------------------------------------\n        [2025-02-03T13:11:32.710593], Orchestrator (-> WebSurfer):\n\n        Please click on the first link ('SK Group Hosts SK AI Summit 2024') to gather more detailed information about the keynote address, including major announcements and themes covered.\n\n        ---------------------------------------------------------------------------\n        [2025-02-03T13:11:32.711044], Orchestrator (thought):\n\n        Next speaker WebSurfer\n        [2025-02-03T13:11:33.126043], WebSurfer: Screenshot: screenshot_som_1738555893.png\n        [2025-02-03T13:11:36.856583], WebSurfer: click( {\"reasoning\": \"The first link likely contains detailed relevant information about the SK AI Summit 2024 keynote.\", \"target_id\": 92} )\n        [2025-02-03T13:11:44.645528], WebSurfer: New tab or window.\n        [2025-02-03T13:11:47.905190], WebSurfer: Screenshot: screenshot_1738555907.png\n\n        ---------------------------------------------------------------------------\n        [2025-02-03T13:11:55.732437], WebSurfer:\n\n        I clicked 'SK Group Hosts ‘SK AI Summit 2024’ to Drive Global AI ...'.\n\n        Here is a screenshot of [SK Group Hosts ‘SK AI Summit 2024’ to Drive Global AI Exchange and Innovation | SK](https://eng.sk.com/news/sk-group-hosts-sk-ai-summit-2024-to-drive-global-ai-exchange-and-innovation). The viewport shows 16% of the webpage, and is positioned at the top of the page.\n        The following metadata was extracted from the webpage:\n\n        {\n            \"meta_tags\": {\n                \"viewport\": \"width=device-width, initial-scale=1\",\n                \"format-detection\": \"telephone=no\",\n                \"description\": \"\",\n                \"fb:app_id\": \"\",\n                \"og:url\": \"https://eng.sk.com/news/sk-group-hosts-sk-ai-summit-2024-to-drive-global-ai-exchange-and-innovation\",\n                \"og:type\": \"website\",\n                \"og:title\": \"SK Group Hosts \\u2018SK AI Summit 2024\\u2019 to Drive Global AI Exchange and Innovation | SK\",\n                \"og:image\": \"https://eng.sk.com/uploads/images/_1200x630_crop_center-center_none/Photo-1-SK-Group-Chairman-Tae-won-Chey-at-the-SK-AI-Summit-in-Seoul-on-Monday-November-4.jpg\",\n                \"og:image:width\": \"1200\",\n                \"og:image:height\": \"630\",\n                \"og:description\": \"\",\n                \"og:site_name\": \"SK\",\n                \"og:locale\": \"en_US\",\n                \"og:locale:alternate\": \"en_US\",\n                \"twitter:card\": \"summary_large_image\",\n                \"twitter:site\": \"\",\n                \"twitter:url\": \"https://eng.sk.com/news/sk-group-hosts-sk-ai-summit-2024-to-drive-global-ai-exchange-and-innovation\",\n                \"twitter:title\": \"SK Group Hosts \\u2018SK AI Summit 2024\\u2019 to Drive Global AI Exchange and Innovation | SK\",\n                \"twitter:description\": \"\",\n                \"twitter:image\": \"https://eng.sk.com/uploads/images/_1200x600_crop_center-center_none/Photo-1-SK-Group-Chairman-Tae-won-Chey-at-the-SK-AI-Summit-in-Seoul-on-Monday-November-4.jpg\",\n                \"msapplication-TileColor\": \"#da532c\",\n                \"theme-color\": \"#ffffff\"\n            }\n        }\n\n        Automatic OCR of the page screenshot has detected the following text:\n\n        Companies     Industries     Social Value     Perspectives     News     About     Eng\n\n        News\n\n        SK Group Hosts ‘SK AI Summit 2024’ to Drive Global AI Exchange and Innovation\n        November 4, 2024\n\n        SK Chairman Tae-won Chey: “Contributing to the Development of a Global AI Ecosystem by Leveraging SK’s Capabilities and Partnerships, Both Domestic and Global”\n\n        - Event being held at COEX Convention Center in Seoul from November 4-5; gathering global AI leaders, including SK Chairman and the CEO of OpenAI\n        - Chey: “We will address the ‘AI bottleneck’ by integrating solutions from SK and our top global partners”\n        - CEOs of Microsoft, NVIDIA, and TSMC shared messages and participated in video discussions, highlighting their commitment to continuing collaboration with SK in AI and working together to shape the future of humanity\n        - Event included over 10 sessions featuring keynote presentations and expert discussions covering all aspects of AI, from infrastructure and energy to services\n\n        SEOUL, November 4, 2024 – SK Group Chairman Tae-won Chey expressed his ambition to strengthen the global Artificial Intelligence (AI) ecosystem by combining SK’s AI capabilities with partnerships across domestic and international companies. He also outlined plans to contribute to the development of Korea’s AI ecosystem and\n\n        Related Companies\n\n        SK hynix\n\n        SK telecom\n\n        Related Industries\n\n        Information & Communications Technology\n\n        Semiconductor & Materials\n\n        Related Locations\n\n        South Korea\n        <Image>\n\n        ---------------------------------------------------------------------------\n        [2025-02-03T13:12:02.814791], Orchestrator (thought):\n\n        Updated Ledger:\n        {\n        \"is_request_satisfied\": {\n            \"reason\": \"We have gathered some information regarding the SK AI Summit 2024 keynote, including details about the event and its themes. However, a detailed summary has not yet been compiled.\",\n            \"answer\": false\n        },\n        \"is_in_loop\": {\n            \"reason\": \"We are not currently in a loop. We have progressed from searching to retrieving data from the webpage.\",\n            \"answer\": false\n        },\n        \"is_progress_being_made\": {\n            \"reason\": \"The WebSurfer has successfully navigated to a page with relevant information and is gathering details about the event.\",\n            \"answer\": true\n        },\n        \"next_speaker\": {\n            \"reason\": \"WebSurfer should extract more detailed information from the webpage regarding announcements, themes, and other specifics from the SK AI Summit 2024 keynote.\",\n            \"answer\": \"WebSurfer\"\n        },\n        \"instruction_or_question\": {\n            \"reason\": \"WebSurfer needs to review the webpage contents and extract specific details about the keynote, including major announcements, key speakers, and covered themes to formulate a comprehensive summary.\",\n            \"answer\": \"Please extract detailed information from the page regarding the SK AI Summit 2024 keynote, focusing on specific announcements, key speakers, and themes covered in the discussions.\"\n        }\n        }\n\n        ---------------------------------------------------------------------------\n        [2025-02-03T13:12:02.815051], Orchestrator (-> WebSurfer):\n\n        Please extract detailed information from the page regarding the SK AI Summit 2024 keynote, focusing on specific announcements, key speakers, and themes covered in the discussions.\n\n        ---------------------------------------------------------------------------\n        [2025-02-03T13:12:02.815474], Orchestrator (thought):\n\n        Next speaker WebSurfer\n        [2025-02-03T13:12:03.100144], WebSurfer: Screenshot: screenshot_som_1738555923.png\n        [2025-02-03T13:12:07.418188], WebSurfer: summarize_page( {\"reasoning\": \"I will summarize the page to extract detailed information about SK AI Summit 2024 keynote, including announcements, key speakers, and themes.\", \"url\": \"https://eng.sk.com/news/sk-group-hosts-sk-ai-summit-2024-to-drive-global-ai-exchange-and-innovation\"} )\n\n        ---------------------------------------------------------------------------\n        [2025-02-03T13:12:14.436786], WebSurfer:\n\n        The SK Group is hosting the 'SK AI Summit 2024' at the COEX Convention Center in Seoul, with the aim of advancing global Artificial Intelligence (AI) cooperation and innovation. The event, featuring SK Chairman Tae-won Chey, top executives from global tech companies, and AI leaders such as the CEO of OpenAI, focuses on collaboration to overcome existing challenges in the AI sector. The summit covers various topics related to AI, including infrastructure, energy, and services, and underlines the importance of cross-industry partnerships in shaping the future of AI. Chey emphasized addressing key AI bottlenecks such as semiconductor shortages, AI infrastructure energy needs, and data quality, aiming to bolster Korea's leadership in AI by leveraging SK's capabilities and partnerships.\n\n        ---------------------------------------------------------------------------\n        [2025-02-03T13:12:23.209843], Orchestrator (thought):\n\n        Updated Ledger:\n        {\n        \"is_request_satisfied\": {\n            \"reason\": \"The key details from the SK AI Summit 2024 keynote have been extracted, including the event's focus on global collaboration to address AI sector challenges, key speakers involved, and highlighted themes.\",\n            \"answer\": true\n        },\n        \"is_in_loop\": {\n            \"reason\": \"The actions are progressing with different requests being made and successfully fulfilled, leading to new information being provided.\",\n            \"answer\": false\n        },\n        \"is_progress_being_made\": {\n            \"reason\": \"The information required to address the user's request has been successfully extracted and summarized.\",\n            \"answer\": true\n        },\n        \"next_speaker\": {\n            \"reason\": \"The detailed summary for SK AI Summit 2024 keynote has been provided.\",\n            \"answer\": \"UserProxy\"\n        },\n        \"instruction_or_question\": {\n            \"reason\": \"The task of summarizing the SK AI Summit 2024 keynote is complete and should be communicated to the user.\",\n            \"answer\": \"The SK AI Summit 2024, held at COEX Convention Center in Seoul, focused on enhancing global AI cooperation. SK Chairman Tae-won Chey and several top leaders from the tech industry, including the CEO of OpenAI, highlighted efforts to tackle AI challenges collaboratively. Key topics included AI infrastructure, energy, and services. Chey emphasized resolving AI bottlenecks like semiconductor shortages and energy demands to strengthen Korea's AI leadership through strategic partnerships.\"\n        }\n        }\n\n        ---------------------------------------------------------------------------\n        [2025-02-03T13:12:23.210016], Orchestrator (thought):\n\n        Request satisfied.\n\n        ---------------------------------------------------------------------------\n        [2025-02-03T13:12:29.306621], Orchestrator (final answer):\n\n\n        SK AI Summit 2024는 서울의 코엑스 컨벤션 센터에서 개최되었으며, 글로벌 인공지능(AI) 협력과 혁신을 발전시키기 위한 목적으로 진행되었습니다. SK 회장 최태원과 글로벌 기술 기업의 최고 경영자들, OpenAI의 CEO를 포함한 AI 리더들이 참여하였으며, AI 분야의 기존 과제를 극복하기 위한 협력을 강조하였습니다.\n\n        키노트에서는 반도체 부족, AI 인프라 에너지 수요, 데이터 품질 등 AI의 주요 병목 현상 해결을 중점적으로 다루었습니다. 이를 통해 SK의 역량과 파트너십을 활용하여 한국의 AI 리더십을 강화하고자 하는 방안이 제시되었습니다.\n\n        행사 동안 AI 관련 인프라, 에너지, 서비스 등 다양한 주제가 다루어졌으며, 산업 간 협력의 중요성을 강조하였습니다. 최태원 회장은 SK와 글로벌 파트너들의 솔루션을 융합하여 이러한 문제들을 해결하고 AI의 미래를 함께 형성하려는 의지를 피력했습니다.\n\n        ---------------------------------------------------------------------------\n        [2025-02-03T13:12:29.306859], Orchestrator (termination condition):\n\n        No agent selected.\n        \n        ```\n        </details>\n        <br>\n\n- **Lab 3: Coder와 ComputerTerminal을 이용한 게임 프로그램 개발**\n    - Commands\n        \n        ```bash\n        python examples/example_coder.py --logs_dir ./logs --hil_mode --save_screenshots --file_surfer\n        ```\n        \n    - 입력\n        \n        ```\n        특별한 기능이 포함된 새로운 스네이크 게임이 필요합니다. 스네이크 게임의 새로운 기능을 계획하고, snakegame.py 파일을 생성하여 게임 코드를 작성한 뒤 테스트를 실행하고 결과를 보고하세요.\n        ```\n        \n    - 입력 (영어)\n        \n        ```\n        The customer needs a new snake game with a unique feature, plan out the new feature for the snake game, create the file snakegame.py  write the code for the game, test it and report back.\n        ```\n        \n    - **입력 - pygame 설치**\n        \n        ```\n        install pygame module\n        ```\n        \n    - **snakegame.py 실행 (command line에서 python 명령어 실행)**\n        \n        ```bash\n        python snakegame.py\n        ```\n        \n        <details>\n        <summary>Response 상세보기</summary>\n\n        ```bash\n        (.autogen) ➜  autogen-magentic-one git:(main) ✗ python examples/example_coder.py --logs_dir ./logs --hil_mode --save_screenshots --file_surfer\n        User input ('exit' to quit):  특별한 기능이 포함된 새로운 스네이크 게임이 필요합니다. 스네이크 게임의 새로운 기능을 계획하고, snakegame.py 파일을 생성하여 게임 코드를 작성한 뒤 테스트를 실행하고 결과를 보고하세요.\n        \n        ---------------------------------------------------------------------------\n        [2025-01-11T01:57:49.420818], UserProxy:\n        \n        특별한 기능이 포함된 새로운 스네이크 게임이 필요합니다. 스네이크 게임의 새로운 기능을 계획하고, snakegame.py 파일을 생성하여 게임 코드를 작성한 뒤 테스트를 실행하고 결과를 보고하세요.\n        \n        ---------------------------------------------------------------------------\n        [2025-01-11T01:57:49.422274], orchestrator (thought):\n        \n        Next speaker Coder\n        \n        ---------------------------------------------------------------------------\n        [2025-01-11T01:58:16.872421], Coder:\n        \n        Plan for the New Snake Game Features:\n        \n        1. **New Features**:\n            - **Random Obstacles**: Introduce random obstacles that the snake must avoid.\n            - **Multiple Levels**: Implement multiple levels, each with increasing difficulty (more obstacles and faster speed).\n            - **Special Items**: Add special items that can temporarily speed up or slow down the snake when eaten.\n            - **Scoreboard**: Maintain a scoreboard to track high scores.\n        \n        2. **Implementation**:\n            - Use Pygame for the graphics and game loop.\n            - Implement the basic snake game logic first.\n            - Add the new features in steps: obstacles, levels, special items, and scoreboard.\n        \n        3. **Testing**:\n            - Test the game to ensure all features work as expected.\n            - Check for any logical or functional errors in the game flow.\n        \n        Let's move on to the implementation:\n        \n        # filename: snakegame.py\n        \n        import pygame\n        import time\n        import random\n        \n        # Initialize Pygame\n        pygame.init()\n        \n        # Define some colors\n        WHITE = (255, 255, 255)\n        BLACK = (0, 0, 0)\n        RED = (213, 50, 80)\n        GREEN = (0, 255, 0)\n        BLUE = (50, 153, 213)\n        \n        # Game screen dimensions\n        display_width = 800\n        display_height = 600\n        \n        # Create the game window\n        game_display = pygame.display.set_mode((display_width, display_height))\n        pygame.display.set_caption('Snake Game with New Features')\n        \n        # Game clock\n        clock = pygame.time.Clock()\n        \n        # Snake properties\n        snake_block = 10\n        snake_speed = 15\n        \n        font_style = pygame.font.SysFont(None, 50)\n        score_font = pygame.font.SysFont(None, 35)\n        \n        # Functions to maintain game states\n        def score(score):\n            value = score_font.render(\"Your Score: \" + str(score), True, WHITE)\n            game_display.blit(value, [0, 0])\n        \n        def our_snake(snake_block, snake_list):\n            for x in snake_list:\n                pygame.draw.rect(game_display, GREEN, [x[0], x[1], snake_block, snake_block])\n        \n        def message(msg, color):\n            mesg = font_style.render(msg, True, color)\n            game_display.blit(mesg, [display_width / 6, display_height / 3])\n        \n        # Main game function\n        def gameLoop():\n            game_over = False\n            game_close = False\n        \n            x1 = display_width / 2\n            y1 = display_height / 2\n        \n            x1_change = 0\n            y1_change = 0\n        \n            snake_List = []\n            length_of_snake = 1\n        \n            foodx = round(random.randrange(0, display_width - snake_block) / 10.0) * 10.0\n            foody = round(random.randrange(0, display_height - snake_block) / 10.0) * 10.0\n        \n            while not game_over:\n        \n                while game_close:\n                    game_display.fill(BLACK)\n                    message(\"You Lost! Press Q-Quit or C-Play Again\", RED)\n                    score(length_of_snake - 1)\n                    pygame.display.update()\n        \n                    for event in pygame.event.get():\n                        if event.type == pygame.KEYDOWN:\n                            if event.key == pygame.K_q:\n                                game_over = True\n                                game_close = False\n                            if event.key == pygame.K_c:\n                                gameLoop()\n        \n                for event in pygame.event.get():\n                    if event.type == pygame.QUIT:\n                        game_over = True\n                    if event.type == pygame.KEYDOWN:\n                        if event.key == pygame.K_LEFT:\n                            x1_change = -snake_block\n                            y1_change = 0\n                        elif event.key == pygame.K_RIGHT:\n                            x1_change = snake_block\n                            y1_change = 0\n                        elif event.key == pygame.K_UP:\n                            y1_change = -snake_block\n                            x1_change = 0\n                        elif event.key == pygame.K_DOWN:\n                            y1_change = snake_block\n                            x1_change = 0\n        \n                if x1 >= display_width or x1 < 0 or y1 >= display_height or y1 < 0:\n                    game_close = True\n                x1 += x1_change\n                y1 += y1_change\n                game_display.fill(BLUE)\n                pygame.draw.rect(game_display, RED, [foodx, foody, snake_block, snake_block])\n                snake_Head = []\n                snake_Head.append(x1)\n                snake_Head.append(y1)\n                snake_List.append(snake_Head)\n                if len(snake_List) > length_of_snake:\n                    del snake_List[0]\n        \n                for x in snake_List[:-1]:\n                    if x == snake_Head:\n                        game_close = True\n        \n                our_snake(snake_block, snake_List)\n                score(length_of_snake - 1)\n        \n                pygame.display.update()\n        \n                if x1 == foodx and y1 == foody:\n                    foodx = round(random.randrange(0, display_width - snake_block) / 10.0) * 10.0\n                    foody = round(random.randrange(0, display_height - snake_block) / 10.0) * 10.0\n                    length_of_snake += 1\n        \n                clock.tick(snake_speed)\n        \n            pygame.quit()\n            quit()\n        \n        gameLoop()\n\n        Please save the above code in a file named `snakegame.py` and execute it on your computer to test the new features of the snake game. Ensure you have the Pygame library installed (`pip install pygame` if necessary). Let me know once you have executed this, or if you encounter any issues. Once tested, if additional features or adjustments are needed, we can add them. TERMINATE\n        \n        ---------------------------------------------------------------------------\n        [2025-01-11T01:58:16.872771], orchestrator (thought):\n        \n        Next speaker Executor\n        Executor is about to execute code (lang: python):\n        # filename: snakegame.py\n        \n        import pygame\n        import time\n        import random\n        \n        # Initialize Pygame\n        pygame.init()\n        \n        # Define some colors\n        WHITE = (255, 255, 255)\n        BLACK = (0, 0, 0)\n        RED = (213, 50, 80)\n        GREEN = (0, 255, 0)\n        BLUE = (50, 153, 213)\n        \n        # Game screen dimensions\n        display_width = 800\n        display_height = 600\n        \n        # Create the game window\n        game_display = pygame.display.set_mode((display_width, display_height))\n        pygame.display.set_caption('Snake Game with New Features')\n        \n        # Game clock\n        clock = pygame.time.Clock()\n        \n        # Snake properties\n        snake_block = 10\n        snake_speed = 15\n        \n        font_style = pygame.font.SysFont(None, 50)\n        score_font = pygame.font.SysFont(None, 35)\n        \n        # Functions to maintain game states\n        def score(score):\n            value = score_font.render(\"Your Score: \" + str(score), True, WHITE)\n            game_display.blit(value, [0, 0])\n        \n        def our_snake(snake_block, snake_list):\n            for x in snake_list:\n                pygame.draw.rect(game_display, GREEN, [x[0], x[1], snake_block, snake_block])\n        \n        def message(msg, color):\n            mesg = font_style.render(msg, True, color)\n            game_display.blit(mesg, [display_width / 6, display_height / 3])\n        \n        # Main game function\n        def gameLoop():\n            game_over = False\n            game_close = False\n        \n            x1 = display_width / 2\n            y1 = display_height / 2\n        \n            x1_change = 0\n            y1_change = 0\n        \n            snake_List = []\n            length_of_snake = 1\n        \n            foodx = round(random.randrange(0, display_width - snake_block) / 10.0) * 10.0\n            foody = round(random.randrange(0, display_height - snake_block) / 10.0) * 10.0\n        \n            while not game_over:\n        \n                while game_close:\n                    game_display.fill(BLACK)\n                    message(\"You Lost! Press Q-Quit or C-Play Again\", RED)\n                    score(length_of_snake - 1)\n                    pygame.display.update()\n        \n                    for event in pygame.event.get():\n                        if event.type == pygame.KEYDOWN:\n                            if event.key == pygame.K_q:\n                                game_over = True\n                                game_close = False\n                            if event.key == pygame.K_c:\n                                gameLoop()\n        \n                for event in pygame.event.get():\n                    if event.type == pygame.QUIT:\n                        game_over = True\n                    if event.type == pygame.KEYDOWN:\n                        if event.key == pygame.K_LEFT:\n                            x1_change = -snake_block\n                            y1_change = 0\n                        elif event.key == pygame.K_RIGHT:\n                            x1_change = snake_block\n                            y1_change = 0\n                        elif event.key == pygame.K_UP:\n                            y1_change = -snake_block\n                            x1_change = 0\n                        elif event.key == pygame.K_DOWN:\n                            y1_change = snake_block\n                            x1_change = 0\n        \n                if x1 >= display_width or x1 < 0 or y1 >= display_height or y1 < 0:\n                    game_close = True\n                x1 += x1_change\n                y1 += y1_change\n                game_display.fill(BLUE)\n                pygame.draw.rect(game_display, RED, [foodx, foody, snake_block, snake_block])\n                snake_Head = []\n                snake_Head.append(x1)\n                snake_Head.append(y1)\n                snake_List.append(snake_Head)\n                if len(snake_List) > length_of_snake:\n                    del snake_List[0]\n        \n                for x in snake_List[:-1]:\n                    if x == snake_Head:\n                        game_close = True\n        \n                our_snake(snake_block, snake_List)\n                score(length_of_snake - 1)\n        \n                pygame.display.update()\n        \n                if x1 == foodx and y1 == foody:\n                    foodx = round(random.randrange(0, display_width - snake_block) / 10.0) * 10.0\n                    foody = round(random.randrange(0, display_height - snake_block) / 10.0) * 10.0\n                    length_of_snake += 1\n        \n                clock.tick(snake_speed)\n        \n            pygame.quit()\n            quit()\n        \n        gameLoop()\n        \n        Do you want to proceed? (yes/no): yes\n        \n        ---------------------------------------------------------------------------\n        [2025-01-11T01:58:35.515226], Executor:\n        \n        The script ran, then exited with Unix exit code: 1\n        Its output was:\n        Traceback (most recent call last):\n          File \"/workspace/snakegame.py\", line 3, in <module>\n            import pygame\n        ModuleNotFoundError: No module named 'pygame'\n        \n        ---------------------------------------------------------------------------\n        [2025-01-11T01:58:35.515690], orchestrator (thought):\n        \n        Next speaker UserProxy\n        User input ('exit' to quit):  install pygame module\n        \n        ---------------------------------------------------------------------------\n        [2025-01-11T01:59:28.941241], UserProxy:\n        \n        install pygame module\n        \n        ---------------------------------------------------------------------------\n        [2025-01-11T01:59:28.943069], orchestrator (thought):\n        \n        Next speaker Coder\n        \n        ---------------------------------------------------------------------------\n        [2025-01-11T01:59:30.896825], Coder:\n        \n        To install the Pygame module, you can use the following shell command:\n        \n        pip install pygame\n        \n        Please run this command in your terminal or command prompt. Once Pygame is installed, you can execute the `snakegame.py` file again. Let me know if you encounter any further issues.\n        \n        ---------------------------------------------------------------------------\n        [2025-01-11T01:59:30.896871], orchestrator (thought):\n        \n        Next speaker Executor\n        Executor is about to execute code (lang: sh):\n        pip install pygame\n        \n        Do you want to proceed? (yes/no): yes\n        \n        ---------------------------------------------------------------------------\n        [2025-01-11T01:59:44.001201], Executor:\n        \n        The script ran but produced no output to console. The Unix exit code was: 0. If you were expecting output, consider revising the script to ensure content is printed to stdout.\n        \n        ---------------------------------------------------------------------------\n        [2025-01-11T01:59:44.001679], orchestrator (thought):\n        \n        Next speaker UserProxy\n        User input ('exit' to quit):\n        ```\n        </details>\n        <br>\n        \n    - **Try It!**\n        ```bash\n        tic-tac-toe 프로그램이 필요합니다. tic-tac-toe.py 파일을 생성하여 게임 코드를 작성한 뒤 테스트를 실행하고 결과를 보고하세요.\n        ```\n- **Lab 4: WebSurfer를 활용하여 아이디어 구상**\n    - Commands\n        \n        ```bash\n        python examples/example.py --logs_dir ./logs --hil_mode --save_screenshots\n        ```\n        \n    - 입력 (한글)\n        \n        ```\n        블로그 작성을 위한 3가지 다른 아이디어가 필요합니다. AI 에이전트에 대한 Trending 주제를 찾아주세요. Google Trends를 사용하고, Youtube에서 인기 있는 주제를 검색하여 3가지 아이디어를 한글로 반환해주세요.\n        ```\n        \n    - 입력 (영어)\n        \n        ```\n        I need 3 different ideas for my blog. Find trending topic around AI Agents, Agentic AI. Use google trends or similar, search youtube to find popular topics. Return a list of 3 different ideas.\n        ```\n\n    - **Updated Ledger (Orchestrator Thought)**\n        ```\n        --------------------------------------------------------------------------- \n        [2025-02-04T09:43:21.977780], Orchestrator (thought):\n\n        Updated Ledger:\n        {\n        \"is_request_satisfied\": {\n            \"reason\": \"Three blog ideas about AI agent trends have been proposed.\",\n            \"answer\": true\n        },\n        \"is_in_loop\": {\n            \"reason\": \"We are not repeating any actions or requests.\",\n            \"answer\": false\n        },\n        \"is_progress_being_made\": {\n            \"reason\": \"We have provided the user with three complete blog ideas.\",\n            \"answer\": true\n        },\n        \"next_speaker\": {\n            \"reason\": \"The proposed blog ideas have been presented for the user's consideration.\",\n            \"answer\": \"UserProxy\"\n        },\n        \"instruction_or_question\": {\n            \"reason\": \"The user needs to review the blog ideas to decide their suitability.\",\n            \"answer\": \"Please review the proposed blog ideas and let us know if they meet your requirements or if any adjustments are needed.\"\n        }\n        }\n\n        --------------------------------------------------------------------------- \n        [2025-02-04T09:43:21.977859], Orchestrator (thought):\n\n        Request satisfied.\n        ```\n\n    - **Final Answer**\n        \n        ```\n        --------------------------------------------------------------------------- \n        [2025-02-04T09:43:29.312789], Orchestrator (final answer):\n\n\n        다음은 AI 에이전트에 관한 블로그 주제 아이디어 세 가지입니다:\n\n        1. **AI 에이전트의 미래: 2025 예측과 혁신**\n        - AI 에이전트의 미래를 예측하고 다가올 2025년까지 예상되는 발전, 도전 과제 및 혁신을 탐구하는 블로그입니다. 자동화, AI 컴패니언, 다양한 산업에서의 AI 역할 등을 다룰 수 있습니다.\n\n        2. **생성 AI 및 에이전트적 지능 탐구**\n        - 생성 AI가 자율적인 지능형 에이전트 개발에 미치는 영향을 중심으로 블로그를 작성해보세요. 이러한 AI 에이전트가 어떻게 콘텐츠, 통찰력 또는 솔루션을 생성하는지, 그리고 이에 대한 사용 사례나 윤리적인 고려사항을 다룰 수 있습니다.\n\n        3. **스타트업 스포트라이트: AI 에이전트 기업 선두주자**\n        - AI 에이전트 기술 발전을 이끄는 혁신적인 스타트업에 대한 글입니다. 주요 업체들의 시장 내 역할, AI 기술에 대한 기여 및 다양한 산업에 미치는 영향을 조명해 볼 수 있습니다.\n\n        이 아이디어들은 현재와 미래의 AI 에이전트 관련 주제를 다루며 독자들의 관심을 끌 수 있을 것입니다.\n        ```\n        \n        <details>\n        <summary>Response 상세보기</summary>\n        \n        ```bash\n        (.autogen) ➜  autogen-magentic-one git:(main) ✗ python examples/example.py --logs_dir ./logs --hil_mode --save_screenshots\n        [2025-02-04T09:41:44.915832], WebSurfer: Screenshot: screenshot_1738629702.png\n        User input ('exit' to quit):  블로그 작성을 위한 3가지 다른 아이디어가 필요합니다. AI 에이전트에 대한 Trending 주제를 찾아주세요. Google Trends를 사용하고, Youtube에서 인기 있는 주제를 검색하여 3가지 아이디어를 한글로 반환해주세요.\n\n        --------------------------------------------------------------------------- \n        [2025-02-04T09:42:09.570125], UserProxy:\n\n        블로그 작성을 위한 3가지 다른 아이디어가 필요합니다. AI 에이전트에 대한 Trending 주제를 찾아주세요. Google Trends를 사용하고, Youtube에서 인기 있는 주제를 검색하여 3가지 아이디어를 한글로 반환해주세요.\n\n        --------------------------------------------------------------------------- \n        [2025-02-04T09:42:17.873526], Orchestrator (thought):\n\n        Initial plan:\n\n        We are working to address the following user request:\n\n        블로그 작성을 위한 3가지 다른 아이디어가 필요합니다. AI 에이전트에 대한 Trending 주제를 찾아주세요. Google Trends를 사용하고, Youtube에서 인기 있는 주제를 검색하여 3가지 아이디어를 한글로 반환해주세요.\n\n\n        To answer this request we have assembled the following team:\n\n        WebSurfer: A helpful assistant with access to a web browser. Ask them to perform web searches, open pages, and interact with content (e.g., clicking links, scrolling the viewport, etc., filling in form fields, etc.) It can also summarize the entire page, or answer questions based on the content of the page. It can also be asked to sleep and wait for pages to load, in cases where the pages seem to be taking a while to load.\n        Coder: A helpful and general-purpose AI assistant that has strong language skills, Python skills, and Linux command line skills.\n        Executor: A agent for executing code\n        file_surfer: An agent that can handle local files.\n        UserProxy: The current user interacting with you.\n\n\n\n        Here is an initial fact sheet to consider:\n\n        1. GIVEN OR VERIFIED FACTS\n        - The request is for three different blog ideas related to trending topics about AI agents.\n        - Sources mentioned for research include Google Trends and YouTube.\n\n        2. FACTS TO LOOK UP\n        - Current trending topics about AI agents on Google Trends.\n        - Popular topics or videos about AI agents on YouTube.\n\n        3. FACTS TO DERIVE\n        - Determine what aspects of AI agents are commonly discussed and why they might be trending.\n        - Potential impact or interest generated by these AI agent topics for a blog audience.\n\n        4. EDUCATED GUESSES\n        - AI agents related to areas like ChatGPT, AI companionship, AI in gaming, or AI tools for productivity may be trending.\n        - There could be a focus on ethical discussions or technological advancements in AI agents based on recent news and developments.\n\n\n        Here is the plan to follow as best as possible:\n\n        - WebSurfer will visit Google Trends to find current trending topics related to AI agents and summarize the findings.\n        - WebSurfer will also search YouTube for popular videos and trending topics related to AI agents and summarize the findings.\n        - Coder will compile and present three blog ideas based on the information gathered by WebSurfer from Google Trends and YouTube, ensuring the ideas are relevant and engaging for the target audience.\n        - UserProxy will review and provide feedback on the proposed blog ideas to finalize the most suitable topics.\n\n\n        --------------------------------------------------------------------------- \n        [2025-02-04T09:42:20.448576], Orchestrator (thought):\n\n        Updated Ledger:\n        {\n        \"is_request_satisfied\": {\n            \"reason\": \"The request is not yet satisfied because we have not yet gathered the trending topics from Google Trends and YouTube, nor have we generated blog ideas based on those findings.\",\n            \"answer\": false\n        },\n        \"is_in_loop\": {\n            \"reason\": \"We are not in a loop as we have just outlined our plan and not yet repeated any actions.\",\n            \"answer\": false\n        },\n        \"is_progress_being_made\": {\n            \"reason\": \"We are at the beginning of executing our plan to gather information and generate blog ideas.\",\n            \"answer\": true\n        },\n        \"next_speaker\": {\n            \"reason\": \"WebSurfer is the next logical choice to gather data from Google Trends and YouTube.\",\n            \"answer\": \"WebSurfer\"\n        },\n        \"instruction_or_question\": {\n            \"reason\": \"WebSurfer needs to perform searches on current trending topics related to AI agents on both Google Trends and YouTube.\",\n            \"answer\": \"Please visit Google Trends and find the current trending topics related to AI agents. Also, search YouTube for popular videos and trending topics about AI agents. Summarize your findings.\"\n        }\n        }\n\n        --------------------------------------------------------------------------- \n        [2025-02-04T09:42:20.448763], Orchestrator (-> WebSurfer):\n\n        Please visit Google Trends and find the current trending topics related to AI agents. Also, search YouTube for popular videos and trending topics about AI agents. Summarize your findings.\n\n        --------------------------------------------------------------------------- \n        [2025-02-04T09:42:20.449023], Orchestrator (thought):\n\n        Next speaker WebSurfer\n        [2025-02-04T09:42:21.677586], WebSurfer: Screenshot: screenshot_som_1738629741.png\n        [2025-02-04T09:42:27.319487], WebSurfer: web_search( {\"reasoning\": \"I will use the web search to find current trending topics related to AI agents using Google Trends.\", \"query\": \"AI agent trends site:trends.google.com\"} )\n        [2025-02-04T09:42:31.319697], WebSurfer: Screenshot: screenshot_1738629751.png\n\n        --------------------------------------------------------------------------- \n        [2025-02-04T09:42:40.996793], WebSurfer:\n\n        I typed 'AI agent trends site:trends.google.com' into the browser search bar.\n\n        Here is a screenshot of [AI agent trends site:trends.google.com - Search](https://www.bing.com/search?q=AI+agent+trends+site%3Atrends.google.com&FORM=QBLH). The viewport shows 40% of the webpage, and is positioned at the top of the page.\n        The following metadata was extracted from the webpage:\n\n        {\n            \"meta_tags\": {\n                \"referrer\": \"origin-when-cross-origin\",\n                \"SystemEntropyOriginTrialToken\": \"A5is4nwJJVnhaJpUr1URgj4vvAXSiHoK0VBbM9fawMskbDUj9WUREpa3JzGAo6xd1Cp2voQEG1h6NQ71AsMznU8AAABxeyJvcmlnaW4iOiJodHRwczovL3d3dy5iaW5nLmNvbTo0NDMiLCJmZWF0dXJlIjoiTXNVc2VyQWdlbnRMYXVuY2hOYXZUeXBlIiwiZXhwaXJ5IjoxNzUzNzQ3MjAwLCJpc1N1YmRvbWFpbiI6dHJ1ZX0=\",\n                \"og:description\": \"Intelligent search from Bing makes it easier to quickly find what you\\u2019re looking for and rewards you.\",\n                \"og:site_name\": \"Bing\",\n                \"og:title\": \"AI agent trends site:trends.google.com - Bing\",\n                \"og:url\": \"https://www.bing.com/search?q=AI+agent+trends+site%3Atrends.google.com&FORM=QBLH\",\n                \"fb:app_id\": \"3732605936979161\",\n                \"og:image\": \"http://www.bing.com/sa/simg/facebook_sharing_5.png\",\n                \"og:type\": \"website\",\n                \"og:image:width\": \"600\",\n                \"og:image:height\": \"315\"\n            }\n        }\n\n        Automatic OCR of the page screenshot has detected the following text:\n\n        **Search bar:**  \n        AI agent trends site:trends.google.com\n\n        ---\n\n        **Top navigation menu:**  \n        SEARCH | COPILOT | IMAGES | VIDEOS | MAPS | NEWS | + MORE | TOOLS\n\n        ---\n\n        **Sign in**  \n        Profile icon with number 3  \n        Menu icon (three horizontal lines)\n\n        ---\n\n        **Main search results:**  \n        About 42,100 results\n\n        1. **google.com**  \n        https://trends.google.com › trends › explore  \n        **ai - Google Trends**  \n        Explore search interest for ai by time, location and popularity on Google Trends\n\n        2. **google.com**  \n        https://trends.google.com  \n        **Google Trends**  \n        Explore the year through the lens of Google Trends data. See how Google Trends is being used across the world, by newsrooms, charities, and more. What are we searching for? From the OECD: The...  \n        **Tags:** Google Apps, Philippine Elections, Explore, Crypto, Philippines, India\n\n        3. **google.com**  \n        https://trends.google.com › trends › explore  \n        **Google Trends**  \n        Google Trends\n\n        ---\n\n        **People also ask:**\n\n        1. **Is Artificial Intelligence a current trend?**  \n        Artificial Intelligence (AI) is already a current trend with ongoing applications in healthcare, autonomous vehicles, facial recognition, and more. In the future, we might see a shift towards...  \n        *Link:* Artificial Intelligence Trends in 20...  \n        *Source:* mygreatlearning.com\n\n        2. **What are the most important AI trends in 2021?**  \n        In 2024, one of the most important AI trends is Natalie Cartwright, co-founder, and COO of Finn AI, an AI banking platform, predicts that organizations will deliver expertise o...  \n        *Link:* Artificial Intelligence Trends in 20...  \n        *Source:* mygreatlearning.com\n\n        3. **What are AI startups?**  \n        The AI T...  \n        This repor...  \n        trend ident...  \n        data-driv...  \n        scouting...  \n        *Link:* Top 10...  \n        *Source:* start...  \n\n        ---\n\n        **Sidebar: Related searches for AI agent trends site:trends.google.com**\n\n        - ai agents in **2025**\n        - best ai **predictions 2025**\n        - **2025** ai predictions\n        - agents in **generative** ai\n        - the ai agents ***are coming***\n        - ai agents **startups**\n        - top agentic ai **companies**\n        - ai agents **in action**\n\n        “Deep search” icon and label\n        <Image>\n\n        --------------------------------------------------------------------------- \n        [2025-02-04T09:42:45.655097], Orchestrator (thought):\n\n        Updated Ledger:\n        {\n        \"is_request_satisfied\": {\n            \"reason\": \"We have not yet returned three blog ideas based on trending topics about AI agents.\",\n            \"answer\": false\n        },\n        \"is_in_loop\": {\n            \"reason\": \"We have not repeated any specific requests or actions; instead, we are sequentially gathering information.\",\n            \"answer\": false\n        },\n        \"is_progress_being_made\": {\n            \"reason\": \"WebSurfer has begun gathering data from Google Trends, which is necessary for creating blog ideas.\",\n            \"answer\": true\n        },\n        \"next_speaker\": {\n            \"reason\": \"We should continue gathering data from YouTube to complement the Google Trends information.\",\n            \"answer\": \"WebSurfer\"\n        },\n        \"instruction_or_question\": {\n            \"reason\": \"To gain a comprehensive view of AI agent trends, we need to explore popular topics on YouTube.\",\n            \"answer\": \"Please search for popular videos and trending topics related to AI agents on YouTube, and summarize your findings.\"\n        }\n        }\n\n        --------------------------------------------------------------------------- \n        [2025-02-04T09:42:45.655222], Orchestrator (-> WebSurfer):\n\n        Please search for popular videos and trending topics related to AI agents on YouTube, and summarize your findings.\n\n        --------------------------------------------------------------------------- \n        [2025-02-04T09:42:45.655330], Orchestrator (thought):\n\n        Next speaker WebSurfer\n        [2025-02-04T09:42:45.938012], WebSurfer: Screenshot: screenshot_som_1738629765.png\n        [2025-02-04T09:42:49.704395], WebSurfer: click( {\"reasoning\": \"I need to click the 'VIDEOS' link in order to search for popular AI agent topics on YouTube, as this search query hasn't been performed yet.\", \"target_id\": 70} )\n        [2025-02-04T09:42:50.464353], WebSurfer: New tab or window.\n        [2025-02-04T09:42:53.817754], WebSurfer: Screenshot: screenshot_1738629773.png\n\n        --------------------------------------------------------------------------- \n        [2025-02-04T09:43:01.974699], WebSurfer:\n\n        I clicked 'VIDEOS'.\n\n        Here is a screenshot of [AI agent trends site:trends.google.com - Search Videos](https://www.bing.com/videos/search?q=AI+agent+trends+site%3atrends.google.com&FORM=HDRSC4). The viewport shows 44% of the webpage, and is positioned at the top of the page.\n        The following metadata was extracted from the webpage:\n\n        {\n            \"meta_tags\": {\n                \"robots\": \"noindex\",\n                \"referrer\": \"origin-when-cross-origin\",\n                \"og:description\": \"Intelligent search from Bing makes it easier to quickly find what you\\u2019re looking for and rewards you.\",\n                \"og:site_name\": \"Bing\",\n                \"og:title\": \"AI agent trends site:trends.google.com - Bing\",\n                \"og:url\": \"https://www.bing.com/videos/search?q=AI+agent+trends+site%3atrends.google.com&FORM=HDRSC4\",\n                \"fb:app_id\": \"3732605936979161\",\n                \"og:image\": \"http://www.bing.com/sa/simg/facebook_sharing_5.png\",\n                \"og:type\": \"website\",\n                \"og:image:width\": \"600\",\n                \"og:image:height\": \"315\"\n            }\n        }\n\n        Automatic OCR of the page screenshot has detected the following text:\n\n        Search query: AI agent trends site:trends.google.com\n\n        Search buttons: \n        - SEARCH\n        - COPILOT\n        - IMAGES\n        - VIDEOS (selected)\n        - MAPS\n        - NEWS\n        - MORE\n\n        Message: There are no results for AI agent trends site:trends.google.com. Check your spelling or try different keywords.\n\n        Section title: Popular on Web\n\n        Videos:\n        1. \"CHEAP RACECAR BATTLE Day 3 - The LAW Got Us...\" \n        - Duration: 48:11\n        - Views: 1.6M\n        - Time: 3 days ago\n        - Source: YouTube › Cleetus McFarland\n\n        2. \"Did Kanye West, Bianca Censori Get Kicked Out of Grammys? The Truth | Grammys 2025 | E! News\"\n        - Duration: 1:17\n        - Views: 1.2M\n        - Time: 20 hours ago\n        - Source: YouTube › E! News\n\n        3. \"Content Nuke - Hasan Piker\"\n        - Duration: 1:43:14\n        - Views: 1.5M\n        - Time: 3 days ago\n        - Source: YouTube › h3h3Productions\n\n        4. \"You're Not Crazy, Trump Is | Noem Glams Up For Bronx Raid | ICE Is Detaining American Citiz...\"\n        - Duration: 10:23\n        - Views: 2.3M\n        - Time: 3 days ago\n        - Source: YouTube › The Late Show with Stephen Colbert\n\n        5. \"DROP | Official Music Video\"\n        - Duration not visible\n        - Views: 11.9M\n        - Time: 1 week ago\n\n        6. \"DOECHII Wins BEST RAP ALBUM | 2025 GRAMMYs\"\n        - Duration: 3:29\n        - Views: 1.2M\n        - Time: 22 hours ago\n\n        7. \"Grammys 2025: LIVE from the red carpet\"\n        - Duration: 5:00:56\n        - Views: 2.3M\n        - Time: 23 hours ago\n        - Label: LIVE 😊\n\n        8. \"I Only Ate 'Healthy' Fast Food for 24 Hours!\"\n        - Duration: 29:48\n        - Views: 2.9M\n        - Time: 5 days ago\n\n        Language/Region Selection: 한국어 \n        Sign in button: Sign in\n        Notifications count: 6 \n\n        SafeSearch setting: SafeSearch: Strict\n        Filter button\n        Feedback option: Feedback\n        <Image>\n\n        --------------------------------------------------------------------------- \n        [2025-02-04T09:43:09.183516], Orchestrator (thought):\n\n        Updated Ledger:\n        {\n        \"is_request_satisfied\": {\n            \"reason\": \"So far, we have explored Google Trends, but there was no specific information found about AI agent trends. The YouTube search returned unrelated results.\",\n            \"answer\": false\n        },\n        \"is_in_loop\": {\n            \"reason\": \"We are seeing similar results and issues with searching. Attempting new approaches without repeating the same actions.\",\n            \"answer\": false\n        },\n        \"is_progress_being_made\": {\n            \"reason\": \"Although specific searches did not yield relevant results, different approaches are being attempted.\",\n            \"answer\": true\n        },\n        \"next_speaker\": {\n            \"reason\": \"Coder can use the current knowledge and general trends about AI to suggest possible blog topics.\",\n            \"answer\": \"Coder\"\n        },\n        \"instruction_or_question\": {\n            \"reason\": \"Coder can utilize existing knowledge about AI and the typical areas of interest related to AI agents to create three potential blog topics.\",\n            \"answer\": \"Please suggest three blog post ideas about AI agent trends, using known and speculative information about trending AI agent topics.\"\n        }\n        }\n\n        --------------------------------------------------------------------------- \n        [2025-02-04T09:43:09.183759], Orchestrator (-> Coder):\n\n        Please suggest three blog post ideas about AI agent trends, using known and speculative information about trending AI agent topics.\n\n        --------------------------------------------------------------------------- \n        [2025-02-04T09:43:09.184003], Orchestrator (thought):\n\n        Next speaker Coder\n\n        --------------------------------------------------------------------------- \n        [2025-02-04T09:43:16.042261], Coder:\n\n        Based on the trends and speculative insights into AI agents, here are three blog post ideas that you could explore:\n\n        1. **The Future of AI Agents: 2025 Predictions and Innovations**\n        - This blog post could delve into future predictions regarding AI agents, discussing potential advancements, challenges, and innovations expected by 2025. You might cover topics such as AI in automation, AI companions, and AI's roles in various industries, informed by trending searches on Google's Trends related to \"best AI predictions 2025\" and \"AI agents in 2025.\"\n\n        2. **Exploring Generative AI and Agentic Intelligences**\n        - In this post, focus on generative AI and its impact on the development of intelligent, autonomous agents. Discuss how these AI agents generate content, insights, or solutions without extensive manual intervention, inspired by the trend \"agents in generative AI.\" Cover potential uses, challenges, and ethical considerations.\n\n        3. **Startup Spotlight: AI Agent Companies Leading the Charge**\n        - This idea would highlight innovative startups that are spearheading advancements in AI agents. By using trends like \"AI agents startups\" and \"top agentic AI companies,\" you could examine key players in the market, their contributions to AI technology, and the impact they have on various industries.\n\n        These ideas touch on current and speculative areas of interest that could captivate your audience's attention. TERMINATE\n\n        --------------------------------------------------------------------------- \n        [2025-02-04T09:43:21.977780], Orchestrator (thought):\n\n        Updated Ledger:\n        {\n        \"is_request_satisfied\": {\n            \"reason\": \"Three blog ideas about AI agent trends have been proposed.\",\n            \"answer\": true\n        },\n        \"is_in_loop\": {\n            \"reason\": \"We are not repeating any actions or requests.\",\n            \"answer\": false\n        },\n        \"is_progress_being_made\": {\n            \"reason\": \"We have provided the user with three complete blog ideas.\",\n            \"answer\": true\n        },\n        \"next_speaker\": {\n            \"reason\": \"The proposed blog ideas have been presented for the user's consideration.\",\n            \"answer\": \"UserProxy\"\n        },\n        \"instruction_or_question\": {\n            \"reason\": \"The user needs to review the blog ideas to decide their suitability.\",\n            \"answer\": \"Please review the proposed blog ideas and let us know if they meet your requirements or if any adjustments are needed.\"\n        }\n        }\n\n        --------------------------------------------------------------------------- \n        [2025-02-04T09:43:21.977859], Orchestrator (thought):\n\n        Request satisfied.\n\n        --------------------------------------------------------------------------- \n        [2025-02-04T09:43:29.312789], Orchestrator (final answer):\n\n\n        다음은 AI 에이전트에 관한 블로그 주제 아이디어 세 가지입니다:\n\n        1. **AI 에이전트의 미래: 2025 예측과 혁신**\n        - AI 에이전트의 미래를 예측하고 다가올 2025년까지 예상되는 발전, 도전 과제 및 혁신을 탐구하는 블로그입니다. 자동화, AI 컴패니언, 다양한 산업에서의 AI 역할 등을 다룰 수 있습니다.\n\n        2. **생성 AI 및 에이전트적 지능 탐구**\n        - 생성 AI가 자율적인 지능형 에이전트 개발에 미치는 영향을 중심으로 블로그를 작성해보세요. 이러한 AI 에이전트가 어떻게 콘텐츠, 통찰력 또는 솔루션을 생성하는지, 그리고 이에 대한 사용 사례나 윤리적인 고려사항을 다룰 수 있습니다.\n\n        3. **스타트업 스포트라이트: AI 에이전트 기업 선두주자**\n        - AI 에이전트 기술 발전을 이끄는 혁신적인 스타트업에 대한 글입니다. 주요 업체들의 시장 내 역할, AI 기술에 대한 기여 및 다양한 산업에 미치는 영향을 조명해 볼 수 있습니다.\n\n        이 아이디어들은 현재와 미래의 AI 에이전트 관련 주제를 다루며 독자들의 관심을 끌 수 있을 것입니다.\n\n        --------------------------------------------------------------------------- \n        [2025-02-04T09:43:29.312962], Orchestrator (termination condition):\n\n        No agent selected.\n        ```\n        </details>\n\n    - **참고 (screenshot)**\n    ![image.png](images/log_files/screenshot_1738629702.png)\n    \n    - **참고 (Set-of-Mark)**\n    ![image.png](images/log_files/screenshot_som_1738629765.png)\n  \n\n## **Clean Up**\n\n### Hands-on Lab Part 1: Build a Multi-Agent Application 자원 삭제\n        \n    azd down\n        "
    },
    {
      "name": "dhanaabhirajk/logical_agent",
      "stars": 2,
      "img": "https://avatars.githubusercontent.com/u/81916146?s=40&v=4",
      "owner": "dhanaabhirajk",
      "repo_name": "logical_agent",
      "description": "Reduce the error of Agent with Logical Agents",
      "homepage": null,
      "language": "Python",
      "created_at": "2024-12-19T02:15:52Z",
      "updated_at": "2025-02-01T07:35:44Z",
      "topics": [],
      "readme": "# Logical agent\n\nOne Agent, but Multi agent. Reduce the error of Agent with Multi Logical Agents\n\nHumans learn from others. Real Agent learns from Logical Agents\n\nLogical agents solve the real project agent problems.\n\n- One Agent, but multi agent Collabaration\n- Improved Communication layer of agents\n- High scalable for complex problems with just configuration\n- No more prompts to be maintained\n- Architecture fits into current Agentic Framworks\n\nLogical Agent is open source architecture for LLM Agent\n\n- [Key Features](#key-features)\n- [Layers](#layers)\n- [Applications](#applications)\n- [Quickstart](#quickstart)\n- [Roadmap](#roadmap)\n- [Limitations](#limitations)\n\n## Key Features\n\n- Continuous training in inference\n- Scalable based on logical agents\n- Real time observation\n- Multi Perspective\n- Agent have high accuracy on τ-bench.\n- Agent is made in a way where human in loop is invloved (interruption)\n\n```text\nNote: Predicts the letters in strawberry correctly. Try out \"How many r's in strawberry?\"\n```\n\n### Agent have high accuracy on τ-bench\n\nA good threshold of logical agents will improve τ-bench.\n\n### Agent is made in a way where human in loop is invloved (interruption)\n\nAgent has two stages. Make agents to observe and say. Say can be changed with interruption.\n\n```text\nCore Logic: Humans learn from others. Real Agent learns from Logical Agents\n```\n\n## Layers\n\nLogical Agent has core layers. The Logical Agent observes the user message and gives its real observation to real agent. The real observation message makes the real agent to learn with those observations from logical agent.(Humans learn from others. Real Agent learns from Logical Agents). The real agent inference is given to user when Listen message is sent.\n\n- Logical Agent\n- Perspective\n- GroupManager\n- Real Agent\n\n### Logical Agent\n\nObserves the user message and give observation to real observation. This simulates other people.\n\n### Perspective\n\nPerspective shows on how real agent needs to see a logical agent. Uses perspective to store the agent learnings.\n\n### Group Manager\n\nIndexes the logical agents to be active for observing. Uses agent perspective for indexing.\n\n### Real Agent\n\nYour focus is here. Use the real agent as a template and write your own agent. Use Autogen is only supported.\n\n## Applications\n\n- Create a real human agent (more logical agents, with good level of active logical agents and logical agent epochs)\n- Solve Complex Mathematical Problem (By passing the the problem by sentence by sentence to the agent)\n- Solve ARC problem with continous learning\n\n## Quickstart\n\nClone the repo and change base directory to repo directory\n\nCopy the .env.example to .env and place your `OPENAI_API_KEY`. The current implementation only supports openai provider.\n\nFirst install all the package dependencies using poetry.\n\n```bash\npoetry install\n```\n\n### Running the agent\n\n```bash\n> poetry run python examples/main.py\n  _                _           _      _                    _   \n | |    ___   __ _(_) ___ __ _| |    / \\   __ _  ___ _ __ | |_ \n | |   / _ \\ / _` | |/ __/ _` | |   / _ \\ / _` |/ _ \\ '_ \\| __|\n | |__| (_) | (_| | | (_| (_| | |  / ___ \\ (_| |  __/ | | | |_ \n |_____\\___/ \\__, |_|\\___\\__,_|_| /_/   \\_\\__, |\\___|_| |_|\\__|\n             |___/                        |___/                \nStarted 1 Logical Agents\nTo exit, say exit\n----------------------------------\n> You: How many r's in strawberry?\n> Agent: Yes, that's correct! The word \"strawberry\" contains three \"r's.\"\n> You: exit\n```\n\n## Roadmap\n\n- Use Logical Agents to implement the above mentioned applications and improve the Logical Agents\n- Improve history usage in logical agent as well\n- Implement variety of perspectives like role model perspective and improve indexing\n- Release this a pacakge to use across projects\n\n### Limitations\n\n- The response would include like repeating the message. The chat history usage needs to be optimized.\n- The current agent is set with casual agent\n"
    },
    {
      "name": "CellCS/AI-Agents-RnD",
      "stars": 2,
      "img": "https://avatars.githubusercontent.com/u/16886624?s=40&v=4",
      "owner": "CellCS",
      "repo_name": "AI-Agents-RnD",
      "description": "Welcome to AI-Agents-RnD! This repository is dedicated to research and development in the field of Agentic AI, focusing on the practical implementation and exploration of autonomous AI agents.",
      "homepage": null,
      "language": "Python",
      "created_at": "2024-10-27T14:28:04Z",
      "updated_at": "2025-01-11T01:56:05Z",
      "topics": [],
      "readme": "# AI-Agents-RnD (Public)\n\nWelcome to AI-Agents-RnD! This repository is dedicated to research and development in the field of Agentic AI, focusing on the practical implementation and exploration of autonomous AI agents.\n\n**Code Implementations:** Practical AI agent projects and code snippets originally published on Medium and designed to help developers understand and build agentic systems.\n\n**Research Papers:** A curated collection of influential papers and articles on autonomous AI agents, agentic architectures, and related research fields.\n\n**Experimental Insights:** Explorations and notes on experiments, performance metrics, and findings that may support future advancements in AI agents.\n\n## Example List\n\nOllama: LLMs, Agent\nMagentic One:\nPydanticAI:\nAutoscraper:\nBrowser use:\nOpenWebUI:\n"
    },
    {
      "name": "Haervwe/Illustrated_Novel_Gen",
      "stars": 2,
      "img": "https://avatars.githubusercontent.com/u/31465508?s=40&v=4",
      "owner": "Haervwe",
      "repo_name": "Illustrated_Novel_Gen",
      "description": "An Illustrated Novel experiment on orquestration of LLM agents using the autogen core framework to test capabilities of agentic creative workflows on small models.",
      "homepage": null,
      "language": "Python",
      "created_at": "2024-11-01T09:19:33Z",
      "updated_at": "2024-12-02T14:28:51Z",
      "topics": [],
      "readme": "# Illustrated Novel Generation with Autogen Core\n\n## An experiment in orchestrating local LLM agents for creative workflows\n\nThis project explores the capabilities of agentic workflows for creative writing, specifically illustrated novel generation. It leverages the Autogen Core framework to coordinate multiple local LLM agents, each with a specialized role in the process.\n\n**Key Features:**\n\n* **Local LLM execution:** Utilizes Ollama to run large language models locally, offering flexibility for integration with other OpenAI API compliant LLM providers.\n* **Agentic workflow:** Employs Autogen Core 0.4 to manage interactions between specialized agents, enhancing collaboration and efficiency.\n* **Multi-model inference:**  Different local LLMs can be used for various agent roles, allowing for diverse and tailored outputs based on model strengths.\n* **Integrated image generation:**  Generates images for each chapter using the Hugging Face API and FLUX model, adding a visual dimension to the narrative.\n* **Automated PDF creation:** Compiles chapters and images into a professionally formatted PDF book, ready for sharing or printing.\n\n\n## System Architecture\n\nThe novel generation process follows a structured workflow:\n\n1. **User Input:** The user provides an initial story prompt, sparking the creative process.\n\n2. **Prompt Enhancement:** The `PromptEnhancerAgent` refines the user prompt, improving clarity, readability, and structure for optimal LLM processing.\n\n3. **Novel Planning:** The `PlanificatorAgent` generates a comprehensive JSON-formatted plan outlining detailed guidelines for each chapter, including plot points, character arcs, and setting details.\n\n4. **Chapter Generation Loop:** This iterative loop is the heart of the system, crafting each chapter with precision:\n   - **Chapter Guideline Composer:**  Dynamically prepares a specific prompt for the current chapter, incorporating context and summaries from previous chapters to maintain narrative coherence.\n   - **Editor-Writer Loop:** The `EditorAgent` and `WriterAgent` engage in a collaborative refinement process. The `WriterAgent` drafts the chapter based on guidelines, while the `EditorAgent` provides feedback and ensures adherence to the plan. This loop continues until the chapter reaches an acceptable quality.\n   - **Curation:** The `CuratorAgent` meticulously removes any meta-commentary or artifacts from the final chapter text, ensuring a polished and immersive reading experience.\n   - **Illustration:** The `IllustratorAgent` generates a visually compelling image based on the chapter content, capturing the essence of the narrative and enriching the reader's imagination.\n   - **Summarization:** The `ChapterSummarizationAgent` creates a concise summary of the chapter, extracting key events and character developments to provide context for subsequent chapters and maintain narrative flow.\n\n5. **PDF Compilation:**  Once all chapters are complete, the system automatically compiles them, along with their corresponding illustrations, into a beautifully formatted PDF book. This final output is ready for distribution and enjoyment.\n\n\n## Flow Diagram\n\n![Untitled-2024-10-31-1933](https://github.com/user-attachments/assets/7c157d66-51bf-4bee-983f-b46236a04734)\n\n\n\n## Agents\n\nEach agent plays a crucial role in the novel generation process:\n\n* **PromptEnhancerAgent:**  Refines and optimizes the initial user prompt for clarity and LLM compatibility.\n* **PlanificatorAgent:** Generates a detailed, structured chapter plan in JSON format, serving as a roadmap for the narrative.\n* **EditorAgent:**  Provides constructive feedback, guidance, and approval on chapter drafts, ensuring quality and adherence to the plan.\n* **WriterAgent:**  Crafts the chapter content, translating guidelines and feedback into engaging prose.\n* **CuratorAgent:** Cleans and formats the final chapter text, removing any extraneous information and ensuring a polished reading experience.\n* **IllustratorAgent:**  Generates evocative images for each chapter, enhancing the visual appeal and immersive quality of the story using function tools to call the Hugging Face API using FLUX.\n* **ChapterSummarizationAgent:**  Summarizes chapters concisely, providing essential context for subsequent chapters and aiding in narrative coherence this is a core role since provides a good way to manage context size withut sacrificing previous narrative arcs.\n\n\n## Tools\n\nThe system utilizes powerful tools for image generation and PDF creation:\n\n* **Hugging Face API:**  Enables access to the FLUX model for generating high-quality illustrations.\n* **ReportLab:**  Provides the functionality for compiling chapters and images into a professional-looking PDF book.\n\n\n## Results and Observations\n\n* **Qualitative Assessment:**  The quality of generated novels can vary significantly depending on the specific LLMs used, prompt engineering, and the complexity of the narrative. Longer context chains and intricate factual scenarios can pose challenges to maintaining coherence and consistency.\n\n* **Challenges:**  Achieving optimal performance requires careful selection and configuration of LLMs. Smaller fine-tuned models (around 3.2B parameters) can produce good results, but larger models (at least 8B parameters) with strong reasoning capabilities are recommended for more complex and extensive novels. Effective prompt chaining and context management are crucial for consistent and coherent output.\n\n* **Future Work:**  Future development aims to enhance the system's capabilities by incorporating RAG (Retrieval-Augmented Generation) agents to access and utilize external knowledge sources. This will enable the creation of richer and more factually grounded narratives. Additionally, generating background lore and detailed character sheets will pre-fill the narrative with compelling elements, further enhancing the creative process. Further research and testing will focus on refining summarization processes and improving the overall quality and consistency of chapter generation.\n\n## Educational Focus\n\nThis project is designed with an educational focus, aiming to provide a clear and accessible example of how to utilize the Autogen Core framework for creative applications. The codebase is extensively commented to:\n\n* **Explain the purpose and functionality of each agent and component.**\n* **Illustrate the flow of information and interactions within the agentic workflow.**\n* **Highlight key concepts and techniques in prompt engineering and LLM orchestration.**\n* **Encourage experimentation and modification of the code to explore different creative possibilities.**\n\nBy studying and experimenting with this project, users can gain valuable insights into the potential of agentic AI for creative tasks and learn how to apply the Autogen Core framework to their own projects.\n\n## Running the Code\n\n\n## How to Clone the Repository and Run the Project\n\nTo start using the *Illustrated Novel Generation with Autogen Core* project, follow these clear and concise steps:\n\nNote : For the moment, this is cli based only and ask for the prompt at init, follow these instructions to run it with ollama:\n\n\n### 1. Clone the Repository\n\nUse `git` to clone the project repository onto your local machine:\n\n```bash\ngit clone https://github.com/Haervwe/Illustrated_Novel_Gen.git\n```\nNavigate into the project directory:\n\n```bash\ncd Illustrated_Novel_Gen\n```\n2. Set Up a Virtual Environment\nSetting up a virtual environment ensures that the dependencies for this project don't interfere with other Python projects on your system.\n\nFor Windows:\n\n```bash\npython -m venv venv\n.\\venv\\Scripts\\activate\n```\nFor Linux:\n\n```bash\npython3 -m venv venv\nsource venv/bin/activate\n```\n\n3. Install Dependencies\nInstall the necessary Python packages listed in the requirements.txt file:\n\n```bash\npip install -r requirements.txt\n```\nNote: Ensure your virtual environment is activated before running this command.\n\n4. Install Ollama\nOllama is used to run local LLMs. Install Ollama by following the installation guide provided on the Ollama website. Once installed, load the necessary model:\n\n```bash\nollama run llama3.1:latest\n```\n5. Run the Main Application File\nStart the novel generation process by running the main Python script:\n\n```bash\npython Autogen_Illustrated_Story_Generator.py\n```\n6. Input Your Story Prompt\n\nFollow the prompts in the terminal to input your initial story idea and watch the system generate chapters and illustrations, be patient it can take some time.\n\nWith these steps completed, you're ready to explore the potential of agentic workflows for creative writing and illustrated novel generation!\n\n"
    },
    {
      "name": "arthur-ai/arthur-autogen-agentic-demo",
      "stars": 2,
      "img": "https://avatars.githubusercontent.com/u/62756214?s=40&v=4",
      "owner": "arthur-ai",
      "repo_name": "arthur-autogen-agentic-demo",
      "description": "Demo of how to use Arthur Evaluation Engine as an evaluator and blocker for Autogen Agents",
      "homepage": null,
      "language": "Python",
      "created_at": "2025-03-23T22:02:53Z",
      "updated_at": "2025-04-02T12:29:13Z",
      "topics": [],
      "readme": "<div align=\"center\">\n\n<img src=\"https://cdn.prod.website-files.com/5a749d2c4f343700013366d4/67eab9e594ec4accb58badeb_arthur-logo-symbol.svg\" alt=\"Arthur AI Logo\" width=\"150\"/>\n\n<i>Make AI work for Everyone.</i>\n\n![Autogen CI](https://github.com/arthur-ai/arthur-autogen-agentic-demo/actions/workflows/checks.yml/badge.svg?branch=develop)\n[![Discord](https://img.shields.io/badge/Discord-Arthur-blue?logo=discord&logoColor=white)](https://discord.gg/tdfUAtaVHz)\n\n[Website](https://arthur.ai) - [Documentation](https://shield.docs.arthur.ai/docs) - [Talk to someone at Arthur](https://www.arthur.ai/arthur-book-a-demo)\n\n</div>\n\n# The Arthur AutoGen Agentic Deployment\n\nThe Arthur AutoGen Agentic Deployment provides an example of how Arthur Engine can be used to protect an Agentic Application.\n\nThe agentic use-case in this repository is a Financial Analyst Agent that utilizes a handful of tools to query external systems used in\ngenerating responses about a user's financial queries. For a deep dive into how it's structured, see [docs/architecture.md](./docs/architecture.md).\n\n---\n\n![Python](https://img.shields.io/badge/python-3.11+-blue)\n![Commit style](https://img.shields.io/badge/commit%20style-conventional--commits-brightgreen)\n\n---\n\n## 📦 Features\n\n- **Safety First**: Integration with Arthur Engine for response validation\n- **Multi-Agent System**: Coordinated interaction between specialized AI agents\n- Async-ready Python API\n- Built-in testing, linting, formatting\n\n---\n\n## 🛠 Getting Started\n\n### 🔑 Required Environment Variables\n\nBefore running the app, set the following environment variables (see [docs/security.md](./docs/security.md) for more):\n\n- `ARTHUR_API_KEY` — Arthur Engine API key\n- `MODEL_PROVIDER_API_KEY` — Key for your LLM or embedding model provider\n- `FINANCE_API_URL` — Base URL for financial data service\n- `AUTH_TOKEN` — Optional auth token if needed for protected tools\n\nYou can use a `.env` file for local dev and load it via `dotenv` or your preferred method.\n\n### ⚙️ Model Configuration\n\nModel parameters and routing are defined in `config/model_config.json`.\n\n### 🎯 Eval Engine Configuration\n\nEvaluation tasks (if used) are configured in `config/arthur_engine_config.json`:\n- Metrics to track (e.g., accuracy, response time)\n- Conditions for test prompts\n- Logging preferences (optional integration with Arthur's Eval Engine)\n\n```bash\ngit clone https://github.com/your-username/your-repo.git\ncd your-repo\npython -m venv .venv\nsource .venv/bin/activate  # or .venv\\Scripts\\activate on Windows\npip install -r requirements.txt\npython main.py\n```\n\n---\n\n## 🧪 Run Tests\n\n```bash\npip install -r requirements-dev.txt\npytest\n```\n\n---\n\n## 🧰 Developer Setup\n\nInstall dev dependencies and initialize hooks:\n\n```bash\n./setup-dev.sh\n```\n\nThis will:\n- Install `black`, `ruff`, `pylint`, `pytest`, `semantic-release`\n- Set up `pre-commit` to enforce commit message rules\n\n---\n\n## 🧹 Code Quality\n\nRun linters manually:\n\n```bash\nblack --check .\nruff check .\npylint src/ tests/\n```\n\n---\n\n## 📄 Contributing\n\nWe welcome contributions!\n\n1. Fork the repository\n2. Create a feature branch\n3. Follow [Conventional Commits](https://www.conventionalcommits.org/)\n4. Install `pre-commit` and run `pre-commit install`\n5. Write or update tests\n6. Open a pull request against `dev`\n\nSee [CONTRIBUTING.md](./CONTRIBUTING.md) for full details.\n\n---\n\n## 🔐 Security\n\nThis project enforces best practices including:\n- Environment variable-based config\n- Secrets excluded via `.gitignore`\n\nFor more advanced information, see [docs/security.md](./docs/security.md), which covers:\n- Recommended environment variables\n- Authentication methods\n- Security hardening tips\n\n---\n\n## 🧯 Troubleshooting\n\nFor detailed help, see [docs/troubleshooting.md](./docs/troubleshooting.md).\n\nCommon tips:\n\n- Ensure `src/` is on your `PYTHONPATH` if using imports\n- Make sure pre-commit hooks are installed: `pre-commit install`\n- Verify Python 3.11+ is being used\n\n---\n\n## 🪪 License\n\n[MIT](LICENSE)\n"
    },
    {
      "name": "NuevoFoundation/WorkshopsAutoTranslation",
      "stars": 2,
      "img": "https://avatars.githubusercontent.com/u/40712485?s=40&v=4",
      "owner": "NuevoFoundation",
      "repo_name": "WorkshopsAutoTranslation",
      "description": "Workshops Auto Translation",
      "homepage": null,
      "language": "C#",
      "created_at": "2023-09-14T00:26:11Z",
      "updated_at": "2024-09-18T21:59:30Z",
      "topics": [],
      "readme": "# Workshop Auto Translation Using Azure's OpenAI Service\n\n## Overview\n\nThis repository contains code for automating the translation of Nuevo Foundation's workshops from its workshop website using AI Large Language Models (LLM). The code leverages OpenAI's powerful language models to perform translation tasks, making the process more efficient and accurate. It was developed during Hacakthon 2023 on Microsoft's Redmond campus.\n\n<img src=\"https://nuevofoundationwebassets.blob.core.windows.net/team/WorkshopAI.png\" height=\"400\" alt=\"Nuevo Foundation Workshop Auto Translation Using AI LLM Models Logo\">\n\n\n## What it Does\n\nThe code in this repository enables the translation of workshop content from one language to another using OpenAI's Language Models. Specifically, it does the following:\n\n1. Iterates through a specified folder containing workshop content files.\n\n2. Reads the content of each file.\n\n3. Utilizes OpenAI's Language Models to translate the content into the desired target language.\n\n4. Saves the translated content as separate files, preserving the original file structure.\n\n## Benefits\n\n### 1. Efficiency\n\nManual translation of workshop content can be time-consuming and error-prone. This code automates the translation process, allowing for the quick and consistent translation of a large volume of content.\n\n### 2. Accuracy\n\nOpenAI's Language Models are designed to produce high-quality translations, ensuring that the translated workshop content maintains its clarity and meaning.\n\n### 3. Scalability\n\nAs Nuevo Foundation's workshop content continues to grow, this code can be easily scaled to handle a larger number of workshops and languages.\n\n### 4. Preservation of Original Content\n\nThe code preserves the original content structure by saving translated files separately. This ensures that the translated content can be easily referenced alongside the original content.\n\n## Positive Impact\n\nBy automating the translation of Nuevo Foundation's workshops, this code contributes to making educational resources more accessible to a global audience. It enables learners and educators from different language backgrounds to access and benefit from the foundation's valuable workshops, ultimately promoting knowledge sharing and inclusivity.\n\nWe believe that technology can play a vital role in breaking language barriers and expanding educational opportunities. This code represents a step towards that goal by harnessing the power of AI language models for social good.\n\nWe welcome contributions and collaboration to further improve and expand the capabilities of this translation tool for the benefit of education and knowledge dissemination.\n"
    },
    {
      "name": "scalalang2/crypto-ai-hedge-fund",
      "stars": 2,
      "img": "https://avatars.githubusercontent.com/u/2603286?s=40&v=4",
      "owner": "scalalang2",
      "repo_name": "crypto-ai-hedge-fund",
      "description": "Crypto trading agents built in Autogen.NET",
      "homepage": null,
      "language": "C#",
      "created_at": "2025-04-05T11:48:13Z",
      "updated_at": "2025-04-23T09:22:59Z",
      "topics": [],
      "readme": "# Crypto AI Hedge Fund\n[ai-hedge-fund](https://github.com/virattt/ai-hedge-fund)룰 암호화폐 버전으로 만든 실험용 프로젝트입니다.\n\n절대 투자에 사용하지 마세요\n\n<img width=\"1224\" alt=\"image\" src=\"https://github.com/user-attachments/assets/25f49753-3a3b-4512-ba9f-962cf50a357a\" />\n\n# 아키텍처\n```\nParticipangs\n- LeaderAgent : 총괄 책임자\n- MarketAgent : 시장을 분석하는 에이전트\n- SentimentAgent : 시장의 감정을 분석하는 에이전트\n- RiskManagerAgent : 펀드 매니저의 결정 사항에 대해 리스크를 평가하고 최소화 하기\n- CriticAgent : 펀드 매니저의 행동을 평가하는 에이전트\n- TraderAgent : 실제로 거래를 수행하는 에이전트\n```\n\n# 실행하기\n## 사전 준비\n- 알림 수신용 디스코드 봇 토큰 생성\n- OPEN AI API 키 발급\n- 업비트 API 키 발급\n- 구글 클라우드 GCP 계정 발급\n\n## VM 인스턴스 생성하기\n내가 자고 있는 동안에도 계속 트레이딩을 시키기 위해<br/>\n구글 클라우드 GCP 계정으로 들어가서 VM 인스턴스를 생성합니다.\n- OS는 Ubuntu 22.04 LTS를 선택합니다.\n- [us-west1, us-central1, us-east1 에서 e2-micro 타입을 이용하는 경우 무료로 이용할 수 있습니다](https://cloud.google.com/free/docs/free-cloud-features#compute)\n- 업비트 API는 고정 IP를 요구하기 때문에, 인스턴스를 생성한 뒤에는 고정 IP를 발급해줍니다.\n\n## .NET 9 SDK 설치하기\n프로젝트를 컴파일하고 실행시키기 위해서 [.NET 9 SDK를 설치합니다.](https://learn.microsoft.com/en-us/dotnet/core/install/linux-ubuntu-install?tabs=dotnet9&pivots=os-linux-ubuntu-2404)\n\n```sh\n$ sudo add-apt-repository ppa:dotnet/backports\n$ sudo apt-get update && \\\n  sudo apt-get install -y dotnet-sdk-9.0\n$ dotnet --version\n9.0.105\n```\n\n프로젝트 빌드하기\n```sh\n$ dotnet publish AutogenCryptoTrader.sln -c Release -o ./publish --self-contained -r linux-x64\n```\n\n빌드를 모두 마치고 나면 `publish` 폴더에 실행파일과 설정파일이 생성됩니다.<br/>\n`vim` 명령어로 `appsettings.json` 파일을 변경합니다.\n\n```sh\nvim appsettings.json\n\n{\n  \"Logging\": {\n    \"LogLevel\": {\n      \"Default\": \"Information\",\n      \"Microsoft.AspNetCore\": \"Warning\"\n    }\n  },\n  \"AgentRuntime\": {\n    \"OpenAIApiKey\": \"<OpenAI Api 키>\",\n    \"UpbitAccessKey\": \"<업비트 Access Key>\",\n    \"UpbitSecretKey\": \"<업비트 Secret Key>\",\n    \"DiscordBotToken\": \"<디스코드 봇 토큰>\",\n    \"DiscordChannelId\": <알림을 게시할 채널 ID>\n  }\n}\n```\n\n## 실행하기\n```sh\n$ cd publish\n$ ./Server\n```\n"
    },
    {
      "name": "V0v1kkk/EnglishFlashcardGenerator",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/17791114?s=40&v=4",
      "owner": "V0v1kkk",
      "repo_name": "EnglishFlashcardGenerator",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2025-03-30T20:16:11Z",
      "updated_at": "2025-03-31T11:10:13Z",
      "topics": [],
      "readme": "# English Flashcard Generator (Python AG2 Implementation)\n\nThis project is a Python implementation of an AI-enhanced English flashcard generator using the AG2 framework. \nIt processes markdown notes from English lessons and generates flashcards to help with memorization.\n\n## Overview\n\nThis project is a trying out of the Loop Agentic AI pattern:\n```mermaid\nflowchart LR\n\n    RawNote[Raw Note]\n    TeacherAgent[English teacher agent]\n    ReviewerAgent[Card reviewer agent]\n    ExtractorAgent[Cards extractor agent]\n    Cards[Anki-like cards]\n\n    RawNote --> TeacherAgent\n    TeacherAgent --> ReviewerAgent\n    ReviewerAgent --> TeacherAgent\n    ReviewerAgent --> ExtractorAgent\n    ExtractorAgent --> Cards\n```\n\nI personally use Obsidian note-tanking app to store my notes, and I wanted to create a tool that would help me generate flashcards from my notes.\nI found the [Obsidian Spaced Repetition Plugin](https://github.com/st3v3nmw/obsidian-spaced-repetition) which I decided to use for flashcard learning.\nThe project generates flashcards in a format compatible with the plugin.\n\nThe project has started with .Net version of the Autogen framework, which has been suspended.\nAfter that the python version of the AG2 framework has been created, which is a port of the .Net version.\nThe python version itself is a try of agentic code editing.\n\n\nThe English Flashcard Generator:\n\n1. Reads markdown notes from a source file\n2. Processes these notes using a group of specialized AI agents in a finite state machine pattern:\n   - **TeacherAgent**: Analyzes notes and creates flashcards\n   - **ReviewerAgent**: Reviews flashcards for quality and correctness, providing feedback\n   - **ExtractorAgent**: Formats approved flashcards into JSON format\n3. Generates flashcards in a structured format\n4. Saves both the flashcards and processed notes to output files\n\nThe agents communicate with each other in a controlled workflow:\n- The TeacherAgent creates initial flashcards from the notes\n- The ReviewerAgent evaluates the flashcards and either approves them or sends them back to the TeacherAgent for revision\n- Once approved, the ExtractorAgent formats the flashcards into a structured JSON format\n- The system then converts the JSON to the expected format and saves the output files\n\n## Features\n\n- Finite state machine approach for controlled agent communication\n- Interactive agent collaboration with feedback loops\n- Support for multiple LLM providers (OpenAI, Azure, OpenRouter)\n- Configurable agent parameters (temperature, max tokens)\n- Customizable file paths and templates\n- Comprehensive error handling and retry logic\n- Asynchronous processing for efficiency\n\n## Requirements\n\n- Python 3.8+\n- AG2 (PyAutoGen) library\n- OpenAI API key or Azure OpenAI API key\n\n## Installation\n\n1. Clone the repository:\n   ```\n   git clone https://github.com/V0v1kkk/EnglishFlashcardGenerator.git\n   cd EnglishFlashcardGenerator\n   ```\n\n2. Install the required packages:\n   ```\n   pip install -r requirements.txt\n   ```\n\n3. Copy the template configuration file and update it with your API keys:\n   ```\n   cp appsettings.template.json appsettings.json\n   ```\n\n## Configuration\n\nEdit the `appsettings.json` file to configure:\n\n1. **FilePaths**: Paths to source notes, output folders, and templates\n2. **Providers**: LLM provider configurations (OpenAI, Azure, OpenRouter)\n3. **Agents**: Agent-specific settings (provider, temperature, max tokens)\n\nExample configuration:\n```json\n{\n  \"FilePaths\": {\n    \"SourceNotePath\": \"/path/to/your/notes.md\",\n    \"ResultCardsFolderPath\": \"/path/to/your/flashcards/folder/\",\n    \"ResultNotesFolderPath\": \"/path/to/your/notes/folder/\",\n    \"CardTemplatePath\": \"cardTemplate.md\",\n    \"NoteTemplatePath\": \"noteTemplate.md\"\n  },\n  \"Providers\": [\n    {\n      \"Name\": \"Default\",\n      \"Type\": \"OpenAI\",\n      \"OpenAI\": {\n        \"ApiKey\": \"your-openai-api-key\",\n        \"ModelName\": \"gpt-4o-2024-08-06\"\n      }\n    }\n  ],\n  \"Agents\": {\n    \"TeacherAgent\": {\n      \"ProviderName\": \"Default\",\n      \"Temperature\": 0.7,\n      \"MaxTokens\": 16384\n    },\n    \"ReviewerAgent\": {\n      \"ProviderName\": \"Default\",\n      \"Temperature\": 1.0,\n      \"MaxTokens\": 16384\n    },\n    \"ExtractorAgent\": {\n      \"ProviderName\": \"Default\",\n      \"Temperature\": 0.2,\n      \"MaxTokens\": 16384\n    }\n  }\n}\n```\n\n## Usage\n\nRun the program:\n```\npython main.py\n```\n\nThe program will:\n1. Read the source markdown file\n2. Process each section using the agent group chat with a finite state machine approach:\n   - The TeacherAgent creates initial flashcards from the notes\n   - The ReviewerAgent evaluates the flashcards and provides feedback\n   - If the ReviewerAgent approves the flashcards (by saying \"OK!\"), they are sent to the ExtractorAgent\n   - If the ReviewerAgent has suggestions, the flashcards are sent back to the TeacherAgent for revision\n   - The ExtractorAgent formats the approved flashcards into JSON\n   - The conversation terminates automatically after the ExtractorAgent completes its work\n3. Save the generated flashcards and notes to the specified output folders\n\nThe agent interaction is controlled by a custom speaker selection function that determines which agent should speak next based on the current state of the conversation.\n\n## Input Format\n\nThe source markdown file should contain sections separated by second-level headers (`## `). Each section should start with a date in one of the following formats:\n- `[[yyyy-MM-dd-DayOfWeek|dd.MM.yyyy]]`\n- `[[yyyy-MM-dd-DayOfWeek]]`\n- `dd.MM.yyyy`\n\n## Output Format\n\nThe program generates two types of output files:\n\n1. **Flashcard files**: Markdown files containing the generated flashcards\n2. **Note files**: Markdown files containing the processed notes\n\n## Project Structure\n\n```\nEnglishFlashcardGenerator/\n├── main.py                     # Main entry point\n├── appsettings.json            # Configuration file\n├── appsettings.template.json   # Template configuration file\n├── cardTemplate.md             # Template for flashcard output\n├── noteTemplate.md             # Template for note output\n├── data_classes.py             # FlashCard and related classes\n├── flashcard_helper.py         # Flashcard formatting utilities\n├── note_tools.py               # Note parsing utilities\n├── agents/\n│   ├── agent_base.py           # Base agent class\n│   ├── english_teacher_agent.py # English teacher agent\n│   ├── flashcard_reviewer_agent.py # Flashcard reviewer agent\n│   └── flashcard_extractor_agent.py # Flashcard extractor agent\n└── config/\n    └── config_loader.py        # Configuration loading utilities\n```\n\n## License\n\nThis project is licensed under the MIT License - see the LICENSE file for details.\n\n## Acknowledgments\n\nThis project is a Python port of a .NET implementation using the AG2 framework."
    },
    {
      "name": "Dharma-DX/AutoGenSample",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/64537546?s=40&v=4",
      "owner": "Dharma-DX",
      "repo_name": "AutoGenSample",
      "description": "This project demonstrates a multi-agent system using Microsoft AutoGen with the OpenAI API in .NET. It includes three agents: a Writer Agent, a Critic Agent, and a Reviewer Agent. The Writer Agent generates content based on user input, the Critic Agent provides feedback, and the Reviewer Agent finalizes the content.",
      "homepage": "",
      "language": "C#",
      "created_at": "2025-02-05T11:38:24Z",
      "updated_at": "2025-04-12T15:13:53Z",
      "topics": [
        "autogen"
      ],
      "readme": ""
    },
    {
      "name": "opazenha/csharp-autogen",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/172253505?s=40&v=4",
      "owner": "opazenha",
      "repo_name": "csharp-autogen",
      "description": null,
      "homepage": null,
      "language": "C#",
      "created_at": "2024-11-29T23:06:23Z",
      "updated_at": "2025-04-12T15:20:10Z",
      "topics": [],
      "readme": "# Biblical Trivia API\n\nThe Biblical Trivia API is a C# application that generates trivia questions based on biblical themes and categories. It uses the Gemini API to dynamically create engaging and educational questions.\n\n## Features\n\n- Generate trivia questions with multiple-choice answers.\n- Specify categories and difficulty levels for questions.\n- Provides explanations and biblical references for each question.\n\n## Dependencies\n\nThe project uses the following NuGet packages:\n\n- `AutoGen` - Provides the core functionality for agent-based architecture.\n- `AutoGen.Core` - Core components for building and managing agents.\n- `AutoGen.Gemini` - Integration with the Gemini API for generating content.\n- `Microsoft.AspNetCore.OpenApi` - For API documentation and Swagger integration.\n- `Swashbuckle.AspNetCore` - For generating Swagger UI and API documentation.\n\n## Getting Started\n\n### Prerequisites\n\n- [.NET 8.0 SDK](https://dotnet.microsoft.com/download/dotnet/8.0)\n\n### Installation\n\n1. Clone the repository:\n   ```bash\n   git clone https://github.com/opazenha/csharp-autogen.git\n   ```\n\n2. Navigate to the project directory:\n   ```bash\n   cd BiblicalTriviaApi\n   ```\n\n3. Restore the NuGet packages:\n   ```bash\n   dotnet restore\n   ```\n\n### Environment Setup\n\nSet the Gemini API key as an environment variable:\n```bash\nexport GEMINI_API_KEY=your_api_key_here\n```\n\n### Running the API\n\nTo run the API locally, use the following command:\n```bash\ndotnet run\n```\n\nThe API will be available at `https://localhost:5037/api/trivia/question`.\n"
    },
    {
      "name": "totosan/AIAgentsTest",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/7992807?s=40&v=4",
      "owner": "totosan",
      "repo_name": "AIAgentsTest",
      "description": null,
      "homepage": null,
      "language": "C#",
      "created_at": "2024-11-26T10:18:28Z",
      "updated_at": "2024-12-02T08:27:04Z",
      "topics": [],
      "readme": "# Testing AutoGen\n\n## Different codes/ Languages\n\n- Python [here](./code/Python/)\n  - different programs \n  - main program can simulate an AIOps approach by screening incidents and producing a RootCauseAnalysis report \n- C# [here](./code/CS/multiagent/)\n  - is an agentic system renaming files (PDF) of invoices, bills, ... to what they are fore\n  - e.g. INVOICE_003757282A33B34.pdf --> FLIGH_LH_BER-FRA_2024_12_01.pdf\n "
    },
    {
      "name": "chrnthnkmutt/slm_dotnet_experiment",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/114327550?s=40&v=4",
      "owner": "chrnthnkmutt",
      "repo_name": "slm_dotnet_experiment",
      "description": "This repository is used to make the experiment of using small language models and small reasoning models, in .NET framework with Ollama and LM Studio",
      "homepage": "",
      "language": "C#",
      "created_at": "2024-11-19T02:41:20Z",
      "updated_at": "2025-03-10T17:01:51Z",
      "topics": [
        "dotnet",
        "generative-ai",
        "lmstudio",
        "ollama"
      ],
      "readme": "# slm_dotnet_experiment\n\nCreated By Charunthon Limseelo - Microsoft Learn Student Ambassador, Thailand\nFor more information about the author, please click to his [official website.](https://chrnthnkmutt.github.io)\n\n## Project Purpose\n\nThe purpose of this project is to demonstrate the integration of Ollama or LM Studio with a custom small language models, small reasoning models, or any local models in the .NET framework for making AI agents. The project includes examples of how to set up and use the language model, as well as how to run the provided code. In meantime, this repository is been using for the demonstration in many events as follows:\n\n- .NET Conf Thailand 2024, on November 23th 2024 at Seven Peaks Software\n- .NET Developer Day 2025, on March 8th 2025 at Agoda Thailand, centralwOrld offices\n- Operation on Local AI Agent: Test Drive on Deepseek-R1 Reasoning and AutoGen with .NET and Ollama ([Watch the video here](https://chrnthnkmutt.github.io/talks/deepseekr1))\n\nThe updated slide of presenting at the event will be published and download it within this repository.\n\n## Project Structure\n\nThe project is structured as follows:\n\n```\nAutogen_LMS/\n  ├── Autogen_MLS.csproj\n  ├── Autogen_MLS.sln\n  ├── nuget.config\n  └── Program.cs\nollama_autogen/\n  ├── Chat_With_LLaMA.cs\n  ├── Chat_With_LLaVA.cs\n  ├── nuget.config\n  ├── ollama_autogen.csproj\n  ├── ollama_autogen.sln\n  └── Program.cs\nollama_semantic/\n  ├── nuget.config\n  ├── ollama_semantic.csproj\n  ├── ollama_semantic.sln\n  └── Program.cs\nREADME.md\n```\n\n### Folder's Descriptions\n\n1. AutoGen_LMS: This folder is using for making the experiment of using language model with the LM Studio. For loading or calling the model, we do not need to change the model name in the Program.cs file. However, for LM Studio, we are using on LMS CLI for loading model instead and deploy from CLI or UI app to another project, also with this folder.\n\n2. Ollama_Autogen: This folder is using for making the experiment using language model with OLlama, along with making the agents with AutoGen. However, you can select between text agent or multimodal (image) agent to run. Before running the program, please make sure that your local Ollama server is open by checking with `ollama serve`.\n\n3. Ollama_Semantic: This folder is created for running with using Semantic Kernel, and the setting of Ollama is same as the above one.\n\n*For those are using in Ollama folders, please remind to yourself that you might need to change the model name before running the program*\n\n## Setup Instructions\n\n### Prerequisites\n\n- .NET SDK 9.0 or later\n- NuGet package manager\n- AutoGen Packages of Each Folder (If it is Ollama, install AutoGen.Ollama. If it is LM Studio, install AutoGen.LMStudio. Both of them requires version number - please check it in the NuGet website.)\n\n### Installation\n\n1. Clone the repository:\n   ```sh\n   git clone https://github.com/chrnthnkmutt/BoatSLM_cs_experiment.git\n   cd slm_dotnet_experiment\n   ```\n\n2. Restore the NuGet packages:\n   ```sh\n   dotnet restore\n   ```\n* For those who would like to create project with `dotnet new console`, please remind yourself to run `dotnet new nugetconfig` first on each folder before installing AutoGen or other packages.\n\n## Usage\n\n1. Get into the project:\n  ```sh\n  cd <project folder>\n  ```\n\n2. Run the program:\n  ```sh\n  dotnet run Program.cs\n  ```\n\nHave fun with developing AI apps!!"
    },
    {
      "name": "ImIsaacTbh/ObscureBot",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/81532363?s=40&v=4",
      "owner": "ImIsaacTbh",
      "repo_name": "ObscureBot",
      "description": null,
      "homepage": null,
      "language": "C#",
      "created_at": "2024-02-01T19:17:34Z",
      "updated_at": "2024-10-09T14:05:46Z",
      "topics": [],
      "readme": "Obscurebot - By ImIsaacTbh & ImKevin\n"
    },
    {
      "name": "montraydavis/MDLabs-ResearchTeam",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/12994571?s=40&v=4",
      "owner": "montraydavis",
      "repo_name": "MDLabs-ResearchTeam",
      "description": "Effortlessly document C# projects with MDLabs Research Team Project using AutoGen. This AI-powered tool analyzes your code and generates comprehensive markdown documentation, ensuring consistency and up-to-date records without ongoing maintenance.",
      "homepage": null,
      "language": "C#",
      "created_at": "2024-08-01T01:20:00Z",
      "updated_at": "2024-10-02T09:17:15Z",
      "topics": [],
      "readme": "# MDLabs Research Team Project - Technical Documentation\n\n## Table of Contents\n1. [Introduction](#introduction)\n2. [Project Structure](#project-structure)\n3. [Key Features](#key-features)\n4. [Namespaces](#namespaces)\n5. [Workflow](#workflow)\n6. [Benefits](#benefits)\n7. [Future Development](#future-development)\n8. [Architecture and Technology Stack](#architecture-and-technology-stack)\n\n## Introduction\n\nThe MDLabs `Research Team` Project is an innovative AI application aimed at generating thorough documentation for the entire codebase of a C# solution. Utilizing sophisticated methods in code analysis and natural language processing, this project facilitates the analysis, documentation, and upkeep of complex C# codebases, thereby optimizing software development processes.\n\n**Project Analysis**: \nThe project employs the `MDLabs.ResearchTeam.ProjectAnalysis` namespace, which features:\n- **CSharpProjectAnalyzer**: Conducts analysis of C# projects, retrieves class and method information, and uses `ICSharpCode.Decompiler` and `Reflection` for comprehensive project analysis.\n- **SourceCodeDecompiler**: Decompiles source code to support detailed analysis.\n\n**Documentation Generation**: \nNatural language processing and machine learning are utilized to convert analyzed data into markdown formatted documentation. The `MDLabs.ResearchTeam.Mediation` namespace ensures efficient task handling with threading and mediation. Handlebars.Net is used to create dynamic and adaptable documentation templates.\n\n**Maintainability**: \nA standout feature of this project is the lack of ongoing maintenance work. Apart from initial LLM configuration and occasional tweaking of outputs, the system requires no additional maintenance. This efficiency is achieved through sophisticated project analysis and automated documentation generation.\n\n**Fully Automated**: \nSeamlessly connect any compile-ready C# project with minimal coding, adjust the LLM settings, and generate markdown formatted documentation. The application employs threading and mediation to conduct assembly analysis and documentation generation in parallel.\n\n## Project Structure\n\nThe project is organized into several key namespaces:\n\n- `MDLabs.ResearchTeam.Agents`: Handles semantic agents for code analysis and documentation generation.\n- `MDLabs.ResearchTeam.Mediation`: Manages communication between different components using the Mediator pattern.\n- `MDLabs.ResearchTeam.ProjectAnalysis`: Provides tools for analyzing C# projects, including class and method information retrieval.\n\n## Key Features\n\n1. **Automated Code Analysis**: Analyzes C# codebases to extract detailed information about classes, methods, and their relationships.\n2. **Intelligent Documentation Generation**: Uses advanced language models to generate comprehensive, well-structured documentation for C# projects.\n3. **Customizable Templates**: Uses customizable templates to guide the documentation process, ensuring consistency and clarity.\n4. **Multi-threaded Processing**: Designed to handle large-scale projects efficiently using multi-threaded processing.\n5. **Extensible Architecture**: Modular design allows for easy integration of new features and analysis techniques.\n\n## Namespaces\n\n### MDLabs.ResearchTeam.Agents\n- `BaseSemanticAgent`: Foundation for creating specialized agents.\n- `CSharpAnalysisAgent`: Focused on C# code analysis.\n- `CSharpDocumentationAgent`: Generates documentation for C# code.\n\n### MDLabs.ResearchTeam.Mediation\n- `CSharpFileProcessingNotification`: Notifies about C# file processing events.\n- `CSharpProcessingMediatR`: Coordinates C# processing tasks across the application.\n\n### MDLabs.ResearchTeam.ProjectAnalysis\n- `CSharpProjectAnalyzer`: Main class for analyzing C# projects.\n- `ProjectClassDefinitionInfo` and `ProjectMethodDefinitionInfo`: Store detailed information about classes and methods.\n- `SourceCodeDecompiler`: Extracts source code from compiled assemblies.\n\n## Workflow\n\n1. System analyzes a C# project, extracting information about its structure, classes, and methods.\n2. Information is processed and organized using the classes in the ProjectAnalysis namespace.\n3. Agents namespace components use this structured data to generate documentation.\n4. Mediation namespace ensures smooth communication between different parts of the system during this process.\n5. System produces comprehensive, well-formatted documentation for the analyzed codebase.\n\n## Benefits\n\n- Time-saving: Automates the tedious process of code documentation.\n- Consistency: Ensures uniform documentation style across large projects.\n- Up-to-date Documentation: Easily regenerate documentation to reflect code changes.\n- Improved Code Understanding: Provides clear, structured insights into complex codebases.\n- Scalability: Designed to handle projects of various sizes efficiently.\n\n## Future Development\n\n### Next Steps\n\nWe are excited to share our upcoming features and enhancements that will take our project to the next level. Stay tuned for the following improvements:\n\n#### Enhanced Assembly Analysis\nWe will be implementing a mechanism to generate dependency trees, which will assist in generating all relevant functionality of a given dependency. This feature will analyze all C# source files related to a specific feature (e.g., feature xyz) prior to documentation generation, ensuring comprehensive and accurate documentation.\n\n#### Memories\nTo boost performance and efficiency, we will introduce memory and caching mechanisms. This enhancement will store relevant information during processing, reducing redundant computations and speeding up overall operations.\n\n#### Custom Documentation Pipelines\nWe aim to provide greater flexibility in documentation generation by allowing users to build custom pipelines. This feature will enable you to tailor documentation inputs and outputs to your specific needs, including the integration of external data sources for more enriched documentation.\n\n- - -\n\n## Architecture and Technology Stack\n\n### 1. AutoGen\n\n**Description**: Framework for building Large Language Model (LLM) applications using multiple agents.\n\n**Role in the Application**:\n- Powers `BaseSemanticAgent`, `CSharpAnalysisAgent`, and `CSharpDocumentationAgent`.\n- Enables intelligent code analysis by interpreting and reasoning about code structures.\n- Facilitates generation of human-readable documentation from code analysis results.\n- Allows for dynamic, context-aware interactions between system components.\n\n### 2. ICSharpCode.Decompiler\n\n**Description**: .NET decompiler library that converts compiled assemblies back into readable C# code.\n\n**Role in the Application**:\n- Used in `MDLabs.ResearchTeam.ProjectAnalysis` namespace, particularly in `SourceCodeDecompiler` class.\n- Enables `CSharpProjectAnalyzer` to extract detailed information from compiled assemblies.\n- Retrieves source code for methods and classes, even when original source is unavailable.\n- Crucial for generating accurate documentation for pre-compiled libraries or projects.\n\n### 3. MediatR\n\n**Description**: Simple mediator implementation in .NET for reducing direct dependencies between objects.\n\n**Role in the Application**:\n- Central to `MDLabs.ResearchTeam.Mediation` namespace.\n- Implements Mediator pattern to decouple various system components.\n- Used in `CSharpProcessingMediatR` to manage C# processing tasks flow.\n- Facilitates handling of `CSharpFileProcessingNotification` and `ProcessCSharpSourceFileCommand`.\n- Enables modular and maintainable architecture by reducing direct dependencies.\n\n### 4. Handlebars.Net\n\n**Description**: .NET port of the Handlebars templating engine for creating semantic templates.\n\n**Role in the Application**:\n- Used in `CSharpDocumentationAgent` for generating structured documentation.\n- `CSharpClassInfo.hbs` template in ProjectAnalysis namespace defines class documentation format.\n- Separates documentation content and presentation for easier maintenance and updates.\n- Enables dynamic generation of documentation adaptable to different code structures and requirements.\n\n### 5. Microsoft.NET.Sdk.Worker\n\n**Description**: SDK for building long-running background services in .NET.\n\n**Role in the Application**:\n- Provides foundational structure for the application as a background service.\n- Enables continuous running, processing code analysis and documentation tasks as needed.\n- Facilitates integration with dependency injection, logging, and configuration systems in .NET.\n- Allows easy deployment and management in various environments, including containerized setups.\n\n### 6. Microsoft.Extensions\n\n**Description**: Set of .NET libraries for common programming patterns and utilities.\n\n**Role in the Application**:\n- Dependency Injection: Manages object creation and lifetime, promoting loose coupling and modularity.\n- Logging: Implemented via `ILogger` for consistent and configurable logging across components.\n- Configuration: Enables flexible configuration management for different environments or use cases.\n- Options Pattern: Used for strongly-typed access to related settings, enhancing type safety and ease of use.\n"
    },
    {
      "name": "skulidropek/AutoGenTestTask-master",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/66840575?s=40&v=4",
      "owner": "skulidropek",
      "repo_name": "AutoGenTestTask-master",
      "description": null,
      "homepage": null,
      "language": "C#",
      "created_at": "2024-04-07T07:21:05Z",
      "updated_at": "2024-10-02T15:36:28Z",
      "topics": [],
      "readme": "TextMessage from admin\n--------------------\nIt seems like you're asking for a multiplication operation to be performed and the result to be written to a file located at `C:\\Users\\legov\\AppData\\Local\\Temp\\InteractiveService\\result.txt`. To accomplish this, we'll need to create a small .NET program that performs the multiplication and writes the result to the specified file.\n\nLet's break this down into two tasks:\n1. Write a .NET program that multiplies 1999 by 1000 and writes the result to a file.\n2. Run the .NET program to generate the output file with the result.\n\nI will start by assigning the first task to the coder to write the necessary code.\n\n```task\n{\n    \"to\": \"coder\",\n    \"task\": \"Write a .NET program that multiplies 1999 by 1000 and writes the result to a file at 'C:\\\\Users\\\\legov\\\\AppData\\\\Local\\\\Temp\\\\InteractiveService\\\\result.txt'.\",\n    \"context\": \"\"\n}\n```\n--------------------\n\nfrom: coder\nHere's the .NET code that performs the multiplication of 1999 by 1000 and writes the result to the specified file path:\n\n```csharp\nusing System;\nusing System.IO;\nusing System.Threading.Tasks;\n\nint result = 1999 * 1000;\nstring filePath = @\"C:\\Users\\legov\\AppData\\Local\\Temp\\InteractiveService\\result.txt\";\n\ntry\n{\n    await File.WriteAllTextAsync(filePath, result.ToString());\n    Console.WriteLine($\"The result has been written to {filePath}\");\n}\ncatch (Exception ex)\n{\n    Console.WriteLine($\"An error occurred: {ex.Message}\");\n}\n```\n\nThis code will calculate the product of 1999 and 1000 and attempt to write it to the specified file. If there are any issues with file permissions or the file path, it will catch the exception and print out an error message.\nfrom: reviewer\n```review\ncomment: The code block is using 'using' declaration when creating the file path string. Please remove the 'using' declaration and ensure the code is a top level statement.\nresult: REJECTED\n```\nTextMessage from admin\n--------------------\nIt appears there was a misunderstanding in the provided code. The 'using' directive is not needed for the string declaration, and the code should be a top-level statement without the need for a method or class structure. Let's correct the code and proceed with the task.\n\nI will assign the task back to the coder to correct the code.\n\n```task\n{\n    \"to\": \"coder\",\n    \"task\": \"Correct the .NET code to remove the 'using' directive for the string declaration and ensure the code is a top-level statement suitable for execution.\",\n    \"context\": \"int result = 1999 * 1000;\\nstring filePath = @\\\"C:\\\\Users\\\\legov\\\\AppData\\\\Local\\\\Temp\\\\InteractiveService\\\\result.txt\\\";\\n\\ntry\\n{\\n    await File.WriteAllTextAsync(filePath, result.ToString());\\n    Console.WriteLine($\\\"The result has been written to {filePath}\\\");\\n}\\ncatch (Exception ex)\\n{\\n    Console.WriteLine($\\\"An error occurred: {ex.Message}\\\");\\n}\"\n}\n```\n--------------------\n\nfrom: coder\nApologies for the confusion. Here is the corrected top-level statement code that multiplies 1999 by 1000 and writes the result to the specified file path:\n\n```csharp\nusing System;\nusing System.IO;\nusing System.Threading.Tasks;\n\nint result = 1999 * 1000;\nstring filePath = @\"C:\\Users\\legov\\AppData\\Local\\Temp\\InteractiveService\\result.txt\";\n\nawait File.WriteAllTextAsync(filePath, result.ToString());\nConsole.WriteLine($\"The result has been written to {filePath}\");\n```\n\nThis code will perform the multiplication and write the result to the file at the given path. If there is an issue with the file system (such as permissions or path not found), an unhandled exception will occur.\nfrom: reviewer\n```review\ncomment: The code satisfies all conditions. It is a top-level statement and does not use 'using' declaration when creating the HttpClient.\nresult: APPROVED\n```\nTextMessage from admin\n--------------------\nThe corrected code has been approved. Now, we need to run the .NET code to perform the multiplication and write the result to the specified file path.\n\nI will assign the task to the runner to execute the provided code.\n\n```task\n{\n    \"to\": \"runner\",\n    \"task\": \"Run the provided .NET code to multiply 1999 by 1000 and write the result to the file at 'C:\\\\Users\\\\legov\\\\AppData\\\\Local\\\\Temp\\\\InteractiveService\\\\result.txt'.\",\n    \"context\": \"using System;\\nusing System.IO;\\nusing System.Threading.Tasks;\\n\\nint result = 1999 * 1000;\\nstring filePath = @\\\"C:\\\\Users\\\\legov\\\\AppData\\\\Local\\\\Temp\\\\InteractiveService\\\\result.txt\\\";\\n\\nawait File.WriteAllTextAsync(filePath, result.ToString());\\nConsole.WriteLine($\\\"The result has been written to {filePath}\\\");\"\n}\n```\n--------------------\n\nTextMessage from runner\n--------------------\n// [DOTNET_CODE_BLOCK_EXECUTION]\n### Executing result for code block 0\nThe result has been written to C:\\Users\\legov\\AppData\\Local\\Temp\\InteractiveService\\result.txt\n\n### End of executing result ###\n\n--------------------\n\nTextMessage from admin\n--------------------\nThe .NET code has been successfully executed, and the result of the multiplication has been written to the file at `C:\\Users\\legov\\AppData\\Local\\Temp\\InteractiveService\\result.txt`.\n\nHere is the summary of the steps taken to resolve the coding problem:\n\n```summary\n{\n    \"problem\": \"Multiply 1999 by 1000 and write the result to a file at 'C:\\\\Users\\\\legov\\\\AppData\\\\Local\\\\Temp\\\\InteractiveService\\\\result.txt'.\",\n    \"steps\": [\n        {\n            \"step\": \"Write corrected .NET code as a top-level statement to perform the multiplication and write the result to the specified file path.\",\n            \"result\": \"Code written and approved.\"\n        },\n        {\n            \"step\": \"Run the .NET code to execute the multiplication and write the output to the file.\",\n            \"result\": \"The result has been written to C:\\\\Users\\\\legov\\\\AppData\\\\Local\\\\Temp\\\\InteractiveService\\\\result.txt\"\n        }\n    ]\n}\n```\n--------------------\n\nTextMessage from user\n--------------------\n[GROUPCHAT_TERMINATE]\n--------------------\n\nРезультат 1999000\n"
    },
    {
      "name": "LittleLittleCloud/Assistant.Net",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/16876986?s=40&v=4",
      "owner": "LittleLittleCloud",
      "repo_name": "Assistant.Net",
      "description": "LLM assistant powered by AutoGen.Net",
      "homepage": null,
      "language": "C#",
      "created_at": "2024-02-23T21:08:35Z",
      "updated_at": "2024-03-03T16:35:09Z",
      "topics": [],
      "readme": "# Assistant.Net"
    },
    {
      "name": "StefanDevstar/awesome-llm-apps",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/202806902?s=40&v=4",
      "owner": "StefanDevstar",
      "repo_name": "awesome-llm-apps",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2025-04-22T15:20:52Z",
      "updated_at": "2025-04-22T15:30:20Z",
      "topics": [],
      "readme": "<p align=\"center\">\n  <a href=\"http://www.theunwindai.com\">\n    <img src=\"docs/banner/unwind_black.png\" width=\"900px\" alt=\"Unwind AI\">\n  </a>\n</p>\n\n<p align=\"center\">\n  <a href=\"https://www.linkedin.com/in/shubhamsaboo/\">\n    <img src=\"https://img.shields.io/badge/-Follow%20Shubham%20Saboo-blue?logo=linkedin&style=flat-square\" alt=\"LinkedIn\">\n  </a>\n  <a href=\"https://twitter.com/Saboo_Shubham_\">\n    <img src=\"https://img.shields.io/twitter/follow/Shubham_Saboo\" alt=\"Twitter\">\n  </a>\n</p>\n\n<hr/>\n\n# 🌟 Awesome LLM Apps\n\nA curated collection of awesome LLM apps built with RAG and AI agents. This repository features LLM apps that use models from OpenAI, Anthropic, Google, and open-source models like DeepSeek, Qwen or Llama that you can run locally on your computer.\n\n<p align=\"center\">\n  <a href=\"https://trendshift.io/repositories/9876\" target=\"_blank\">\n    <img src=\"https://trendshift.io/api/badge/repositories/9876\" alt=\"Shubhamsaboo%2Fawesome-llm-apps | Trendshift\" style=\"width: 250px; height: 55px;\" />\n  </a>\n</p>\n\n## 🤔 Why Awesome LLM Apps?\n\n- 💡 Discover practical and creative ways LLMs can be applied across different domains, from code repositories to email inboxes and more.\n- 🔥 Explore apps that combine LLMs from OpenAI, Anthropic, Gemini, and open-source alternatives with RAG and AI Agents.\n- 🎓 Learn from well-documented projects and contribute to the growing open-source ecosystem of LLM-powered applications.\n\n## 🚨 Open Source AI Agent Hackathon\n\nWe're launching a Global AI Agent Hackathon in collaboration with AI Agent ecosystem partners — open to all developers, builders, and startups working on agents, RAG, tool use, or multi-agent systems.\n\n- Win up to **$25,000** in cash by building Agents\n- Top 5 projects will be featured in the top trending [Awesome LLM Apps](https://github.com/Shubhamsaboo/awesome-llm-apps) repo.\n- **$20,000** worth of API and tool use credits from the partners\n\n### Participate Now: [Global AI Agent Hackathon](https://github.com/global-agent-hackathon/global-agent-hackathon-may-2025)\n\n⭐ Star this repo and subscribe to [Unwind AI](https://www.theunwindai.com) for latest updates.\n\n## 📂 Featured AI Projects\n\n### AI Agents\n\n#### 🌱 Starter AI Agents\n\n*   [🎙️ AI Blog to Podcast Agent](starter_ai_agents/ai_blog_to_podcast_agent/)\n*   [❤️‍🩹 AI Breakup Recovery Agent](starter_ai_agents/ai_breakup_recovery_agent/)\n*   [📊 AI Data Analysis Agent](starter_ai_agents/ai_data_analysis_agent/)\n*   [🩻 AI Medical Imaging Agent](starter_ai_agents/ai_medical_imaging_agent/)\n*   [😂 AI Meme Generator Agent (Browser)](starter_ai_agents/ai_meme_generator_agent_browseruse/)\n*   [🎵 AI Music Generator Agent](starter_ai_agents/ai_music_generator_agent/)\n*   [🛫 AI Travel Agent (Local & Cloud)](starter_ai_agents/ai_travel_agent/)\n*   [✨ Gemini Multimodal Agent](starter_ai_agents/gemini_multimodal_agent_demo/)\n*   [🌐 Local News Agent (OpenAI Swarm)](starter_ai_agents/local_news_agent_openai_swarm/)\n*   [🔄 Mixture of Agents](starter_ai_agents/mixture_of_agents/)\n*   [📊 xAI Finance Agent](starter_ai_agents/xai_finance_agent/)\n*   [🔍 OpenAI Research Agent](starter_ai_agents/opeani_research_agent/)\n*   [🕸️ Web Scrapping AI Agent (Local & Cloud)](starter_ai_agents/web_scrapping_ai_agent/)\n\n#### 🚀 Advanced AI Agents\n\n*   [🔍 AI Deep Research Agent](advanced_ai_agents/single_agent_apps/ai_deep_research_agent/)\n*   [🏗️ AI System Architect Agent](advanced_ai_agents/single_agent_apps/ai_system_architect_r1/)\n*   [🎯 AI Lead Generation Agent](advanced_ai_agents/single_agent_apps/ai_lead_generation_agent/)\n*   [💰 AI Financial Coach Agent](advanced_ai_agents/multi_agent_apps/ai_financial_coach_agent/)\n*   [🎬 AI Movie Production Agent](advanced_ai_agents/single_agent_apps/ai_movie_production_agent/)\n*   [🏠 AI Real Estate Agent](advanced_ai_agents/single_agent_apps/ai_real_estate_agent/)\n*   [📈 AI Investment Agent](advanced_ai_agents/single_agent_apps/ai_investment_agent/)\n*   [🏋️‍♂️ AI Health & Fitness Agent](advanced_ai_agents/single_agent_apps/ai_health_fitness_agent/)\n*   [🗞️ AI Journalist Agent](advanced_ai_agents/single_agent_apps/ai_journalist_agent/)\n*   [🧠 AI Mental Wellbeing Agent](advanced_ai_agents/multi_agent_apps/ai_mental_wellbeing_agent/)\n*   [📑 AI Meeting Agent](advanced_ai_agents/single_agent_apps/ai_meeting_agent/)\n\n#### 🎮 Autonomous Game Playing Agents\n\n*   [🎮 AI 3D Pygame Agent](advanced_ai_agents/autonomous_game_playing_agent_apps/ai_3dpygame_r1/)\n*   [♜ AI Chess Agent](advanced_ai_agents/autonomous_game_playing_agent_apps/ai_chess_agent/)\n*   [🎲 AI Tic-Tac-Toe Agent](advanced_ai_agents/autonomous_game_playing_agent_apps/ai_tic_tac_toe_agent/)\n\n#### 🤝 Multi-agent Teams\n\n*   [🧲 AI Competitor Intelligence Agent Team](advanced_ai_agents/multi_agent_apps/agent_teams/ai_competitor_intelligence_agent_team/)\n*   [💲 AI Finance Agent Team](advanced_ai_agents/multi_agent_apps/agent_teams/ai_finance_agent_team/)\n*   [🎨 AI Game Design Agent Team](advanced_ai_agents/multi_agent_apps/agent_teams/ai_game_design_agent_team/)\n*   [👨‍⚖️ AI Legal Agent Team (Cloud & Local)](advanced_ai_agents/multi_agent_apps/agent_teams/ai_legal_agent_team/)\n*   [💼 AI Recruitment Agent Team](advanced_ai_agents/multi_agent_apps/agent_teams/ai_recruitment_agent_team/)\n*   [👨‍💼 AI Services Agency (CrewAI)](advanced_ai_agents/multi_agent_apps/agent_teams/ai_services_agency/)\n*   [👨‍🏫 AI Teaching Agent Team](advanced_ai_agents/multi_agent_apps/agent_teams/ai_teaching_agent_team/)\n*   [💻 Multimodal Coding Agent Team](advanced_ai_agents/multi_agent_apps/agent_teams/multimodal_coding_agent_team/)\n*   [✨ Multimodal Design Agent Team](advanced_ai_agents/multi_agent_apps/agent_teams/multimodal_design_agent_team/)\n\n### 🗣️ Voice AI Agents\n\n*   [🗣️ AI Audio Tour Agent](voice_ai_agents/ai_audio_tour_agent/)\n*   [📞 Customer Support Voice Agent](voice_ai_agents/customer_support_voice_agent/)\n*   [🔊 Voice RAG Agent (OpenAI SDK)](voice_ai_agents/voice_rag_openaisdk/)\n\n\n### 🌐 MCP AI Agents\n\n*   [♾️ MCP Browser Agent](mcp_ai_agents/browser_mcp_agent/)\n*   [🐙 MCP GitHub Agent](mcp_ai_agents/github_mcp_agent/)\n\n\n### RAG (Retrieval Augmented Generation)\n*   [🔗 Agentic RAG](rag_tutorials/agentic_rag/)\n*   [📰 AI Blog Search (RAG)](rag_tutorials/ai_blog_search/)\n*   [🔍 Autonomous RAG](rag_tutorials/autonomous_rag/)\n*   [🔄 Corrective RAG (CRAG)](rag_tutorials/corrective_rag/)\n*   [🐋 Deepseek Local RAG Agent](rag_tutorials/deepseek_local_rag_agent/)\n*   [🤔 Gemini Agentic RAG](rag_tutorials/gemini_agentic_rag/)\n*   [👀 Hybrid Search RAG (Cloud)](rag_tutorials/hybrid_search_rag/)\n*   [🔄 Llama 3.1 Local RAG](rag_tutorials/llama3.1_local_rag/)\n*   [🖥️ Local Hybrid Search RAG](rag_tutorials/local_hybrid_search_rag/)\n*   [🦙 Local RAG Agent](rag_tutorials/local_rag_agent/)\n*   [🧩 RAG-as-a-Service](rag_tutorials/rag-as-a-service/)\n*   [✨ RAG Agent with Cohere](rag_tutorials/rag_agent_cohere/)\n*   [⛓️ Basic RAG Chain](rag_tutorials/rag_chain/)\n*   [📠 RAG with Database Routing](rag_tutorials/rag_database_routing/)\n\n### MCP AI Agents\n- [🐙 MCP GitHub Agent](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/mcp_ai_agents/github_mcp_agent)\n- [♾️ MCP Browser Agent](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/mcp_ai_agents/browser_mcp_agent)\n\n### 🧠 Advanced LLM Apps\n\n#### 💬 Chat with X Tutorials\n\n*   [💬 Chat with GitHub (GPT & Llama3)](advanced_llm_apps/chat_with_X_tutorials/chat_with_github/)\n*   [📨 Chat with Gmail](advanced_llm_apps/chat_with_X_tutorials/chat_with_gmail/)\n*   [📄 Chat with PDF (GPT & Llama3)](advanced_llm_apps/chat_with_X_tutorials/chat_with_pdf/)\n*   [📚 Chat with Research Papers (ArXiv) (GPT & Llama3)](advanced_llm_apps/chat_with_X_tutorials/chat_with_research_papers/)\n*   [📝 Chat with Substack](advanced_llm_apps/chat_with_X_tutorials/chat_with_substack/)\n*   [📽️ Chat with YouTube Videos](advanced_llm_apps/chat_with_X_tutorials/chat_with_youtube_videos/)\n\n#### 💾 LLM Apps with Memory Tutorials\n\n*   [💾 AI ArXiv Agent with Memory](advanced_llm_apps/llm_apps_with_memory_tutorials/ai_arxiv_agent_memory/)\n*   [🛩️ AI Travel Agent with Memory](advanced_llm_apps/llm_apps_with_memory_tutorials/ai_travel_agent_memory/)\n*   [💬 Llama3 Stateful Chat](advanced_llm_apps/llm_apps_with_memory_tutorials/llama3_stateful_chat/)\n*   [📝 LLM App with Personalized Memory](advanced_llm_apps/llm_apps_with_memory_tutorials/llm_app_personalized_memory/)\n*   [🗄️ Local ChatGPT Clone with Memory](advanced_llm_apps/llm_apps_with_memory_tutorials/local_chatgpt_with_memory/)\n*   [🧠 Multi-LLM Application with Shared Memory](advanced_llm_apps/llm_apps_with_memory_tutorials/multi_llm_memory/)\n\n#### 🔧 LLM Fine-tuning Tutorials\n\n*   [🔧 Llama 3.2 Fine-tuning](advanced_llm_apps/llm_finetuning_tutorials/llama3.2_finetuning/)\n\n## 🚀 Getting Started\n\n1. **Clone the repository** \n\n    ```bash \n    git clone https://github.com/Shubhamsaboo/awesome-llm-apps.git \n    ```\n\n2. **Navigate to the desired project directory**\n\n    ```bash \n    cd awesome-llm-apps/starter_ai_agents/ai_travel_agent\n    ```\n\n3. **Install the required dependencies**\n\n    ```bash\n    pip install -r requirements.txt\n    ```\n\n4. **Follow the project-specific instructions** in each project's `README.md` file to set up and run the app.\n\n## 🤝 Contributing to Open Source\n\nContributions are welcome! If you have any ideas, improvements, or new apps to add, please create a new [GitHub Issue](https://github.com/Shubhamsaboo/awesome-llm-apps/issues) or submit a pull request. Make sure to follow the existing project structure and include a detailed `README.md` for each new app.\n\n### Thank You, Community, for the Support! 🙏\n\n[![Star History Chart](https://api.star-history.com/svg?repos=Shubhamsaboo/awesome-llm-apps&type=Date)](https://star-history.com/#Shubhamsaboo/awesome-llm-apps&Date)\n\n🌟 **Don’t miss out on future updates! Star the repo now and be the first to know about new and exciting LLM apps with RAG and AI Agents.**\n"
    },
    {
      "name": "fajarmh-mf/ai-agents-frameworks",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/186990157?s=40&v=4",
      "owner": "fajarmh-mf",
      "repo_name": "ai-agents-frameworks",
      "description": null,
      "homepage": null,
      "language": "HTML",
      "created_at": "2025-04-17T19:07:10Z",
      "updated_at": "2025-04-18T07:49:37Z",
      "topics": [],
      "readme": "## Crew AI - Crew\n- installation by run ```uv tool install crewai```\n- ```cd crewai/latest-ai-development```\n- run ```crewai install``` for installation\n- run ```crewai run``` for running\n\n## Crew AI - Flow\n- run flow by running ```crewai flow kickoff```\n- visualize flow by running ```crewai flow plot```\n- review the output on output/\n    - guidline_outline.json\n    - complete_guide.md\n\n## Autogen - Agent_chat\n- create virtual env ```python3 -m venv .venv``` \n- ```source .venv/bin/activate```\n- install ```pip install -r requiremenst.txt```\n- running ```python3 agent_chat.py```\n\n## Autogen - Core Concept Single Thread\n- create virtual env ```python3 -m venv .venv``` \n- ```source .venv/bin/activate```\n- install ```pip install -r requiremenst.txt```\n- running ```python3 core.py```"
    },
    {
      "name": "saytyarnorngloreia/ai-agents-for-beginners",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/205020882?s=40&v=4",
      "owner": "saytyarnorngloreia",
      "repo_name": "ai-agents-for-beginners",
      "description": "10 Lessons to Get Started Building AI Agents",
      "homepage": "https://microsoft.github.io/ai-agents-for-beginners/",
      "language": null,
      "created_at": "2025-03-27T00:10:56Z",
      "updated_at": "2025-03-27T00:11:03Z",
      "topics": [],
      "readme": "# AI Agents for Beginners - A Course\n\n![Generative AI For Beginners](./images/repo-thumbnail.png)\n\n## 10 Lessons teaching everything you need to know to start building AI Agents\n\n[![GitHub license](https://img.shields.io/github/license/microsoft/ai-agents-for-beginners.svg)](https://github.com/microsoft/ai-agents-for-beginners/blob/master/LICENSE?WT.mc_id=academic-105485-koreyst)\n[![GitHub contributors](https://img.shields.io/github/contributors/microsoft/ai-agents-for-beginners.svg)](https://GitHub.com/microsoft/ai-agents-for-beginners/graphs/contributors/?WT.mc_id=academic-105485-koreyst)\n[![GitHub issues](https://img.shields.io/github/issues/microsoft/ai-agents-for-beginners.svg)](https://GitHub.com/microsoft/ai-agents-for-beginners/issues/?WT.mc_id=academic-105485-koreyst)\n[![GitHub pull-requests](https://img.shields.io/github/issues-pr/microsoft/ai-agents-for-beginners.svg)](https://GitHub.com/microsoft/ai-agents-for-beginners/pulls/?WT.mc_id=academic-105485-koreyst)\n[![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat-square)](http://makeapullrequest.com?WT.mc_id=academic-105485-koreyst)\n\n### Language Support\n[![English](https://img.shields.io/badge/English-brightgreen.svg?style=flat-square)](README.md)\n[![Chinese Simplified](https://img.shields.io/badge/Chinese_Simplified-brightgreen.svg?style=flat-square)](./translations/zh/README.md)\n[![Chinese Traditional](https://img.shields.io/badge/Chinese_Traditional-brightgreen.svg?style=flat-square)](./translations/tw/README.md)     \n[![Chinese Hong Kong](https://img.shields.io/badge/Chinese_Hong_Kong-brightgreen.svg?style=flat-square)](./translations/hk/README.md) \n[![French](https://img.shields.io/badge/French-brightgreen.svg?style=flat-square)](./translations/fr/README.md)\n[![Japanese](https://img.shields.io/badge/Japanese-brightgreen.svg?style=flat-square)](./translations/ja/README.md) \n[![Korean](https://img.shields.io/badge/Korean-brightgreen.svg?style=flat-square)](./translations/ko/README.md)\n[![Portuguese Brazilian](https://img.shields.io/badge/Portuguese_Brazilian-brightgreen.svg?style=flat-square)](./translations/pt/README.md)\n[![Spanish](https://img.shields.io/badge/Spanish-brightgreen.svg?style=flat-square)](./translations/es/README.md)\n[![German](https://img.shields.io/badge/German-brightgreen.svg?style=flat-square)](./translations/de/README.md)  \n[![Persian](https://img.shields.io/badge/Persian-brightgreen.svg?style=flat-square)](./translations/fa/README.md) \n[![Polish](https://img.shields.io/badge/Polish-brightgreen.svg?style=flat-square)](./translations/pl/README.md) \n\n[![GitHub watchers](https://img.shields.io/github/watchers/microsoft/ai-agents-for-beginners.svg?style=social&label=Watch)](https://GitHub.com/microsoft/ai-agents-for-beginners/watchers/?WT.mc_id=academic-105485-koreyst)\n[![GitHub forks](https://img.shields.io/github/forks/microsoft/ai-agents-for-beginners.svg?style=social&label=Fork)](https://GitHub.com/microsoft/ai-agents-for-beginners/network/?WT.mc_id=academic-105485-koreyst)\n[![GitHub stars](https://img.shields.io/github/stars/microsoft/ai-agents-for-beginners.svg?style=social&label=Star)](https://GitHub.com/microsoft/ai-agents-for-beginners/stargazers/?WT.mc_id=academic-105485-koreyst)\n\n[![Azure AI Discord](https://dcbadge.limes.pink/api/server/kzRShWzttr)](https://discord.gg/kzRShWzttr)\n\n\n## 🌱 Getting Started\n\nThis course has 10 lessons covering the fundamentals of building AI Agents. Each lesson covers its own topic so start wherever you like!\n\nThere is multi-language support for this course. Go to our [available languages here](#-multi-language-support). \n\nIf this is your first time building with Generative AI models, check out our [Generative AI For Beginners](https://aka.ms/genai-beginners) course, which includes 21 lessons on building with GenAI.\n\nDon't forget to [star (🌟) this repo](https://docs.github.com/en/get-started/exploring-projects-on-github/saving-repositories-with-stars?WT.mc_id=academic-105485-koreyst) and [fork this repo](https://github.com/microsoft/ai-agents-for-beginners/fork) to run the code.\n\n### What You Need \n\nEach lesson in this course includes code examples, which can be found in the code_samples folder. You can [fork this repo](https://github.com/microsoft/ai-agents-for-beginners/fork) to create your own copy.  \n\nThe code example in these exercises, utilize Azure AI Foundry and GitHub Model Catalogs for interacting with Language Models:\n\n- [Github Models](https://aka.ms/ai-agents-beginners/github-models) - Free / Limited\n- [Azure AI Foundry](https://aka.ms/ai-agents-beginners/ai-foundry) - Azure Account Required\n\nThis course also uses the following AI Agent frameworks and services from Microsoft:\n\n- [Azure AI Agent Service](https://aka.ms/ai-agents-beginners/ai-agent-service)\n- [Semantic Kernel](https://aka.ms/ai-agents-beginners/semantic-kernel)\n- [AutoGen](https://aka.ms/ai-agents/autogen)\n\nFor more information on running the code for this course, go to the [Course Setup](./00-course-setup/README.md).\n\n## 🙏 Want to help?\n\nDo you have suggestions or found spelling or code errors? [Raise an issue](https://github.com/microsoft/ai-agents-for-beginners/issues?WT.mc_id=academic-105485-koreyst) or [Create a pull request](https://github.com/microsoft/ai-agents-for-beginners/pulls?WT.mc_id=academic-105485-koreyst)\n\nIf you get stuck or have any questions about building AI Agents, join our [Azure AI Community Discord](https://discord.gg/kzRShWzttr).\n\n## 📂 Each lesson includes\n\n- A written lesson located in the README and a short video\n- Python code samples supporting Azure AI Foundry and Github Models (Free)\n- Links to extra resources to continue your learning\n\n\n## 🗃️ Lessons\n\n| **Lesson**                               | **Text & Code**                                    | **Video**                                                  | **Extra Learning**                                                                     |\n|------------------------------------------|----------------------------------------------------|------------------------------------------------------------|----------------------------------------------------------------------------------------|\n| Intro to AI Agents and Agent Use Cases   | [Link](./01-intro-to-ai-agents/README.md)          | [Video](https://youtu.be/3zgm60bXmQk?si=z8QygFvYQv-9WtO1)  | [Link](https://aka.ms/ai-agents-beginners/collection?WT.mc_id=academic-105485-koreyst) |\n| Exploring AI Agentic Frameworks          | [Link](./02-explore-agentic-frameworks/README.md)  | [Video](https://youtu.be/ODwF-EZo_O8?si=Vawth4hzVaHv-u0H)  | [Link](https://aka.ms/ai-agents-beginners/collection?WT.mc_id=academic-105485-koreyst) |\n| Understanding AI Agentic Design Patterns | [Link](./03-agentic-design-patterns/README.md)     | [Video](https://youtu.be/m9lM8qqoOEA?si=BIzHwzstTPL8o9GF)  | [Link](https://aka.ms/ai-agents-beginners/collection?WT.mc_id=academic-105485-koreyst) |\n| Tool Use Design Pattern                  | [Link](./04-tool-use/README.md)                    | [Video](https://youtu.be/vieRiPRx-gI?si=2z6O2Xu2cu_Jz46N)  | [Link](https://aka.ms/ai-agents-beginners/collection?WT.mc_id=academic-105485-koreyst) |\n| Agentic RAG                              | [Link](./05-agentic-rag/README.md)                 | [Video](https://youtu.be/WcjAARvdL7I?si=gKPWsQpKiIlDH9A3)  | [Link](https://aka.ms/ai-agents-beginners/collection?WT.mc_id=academic-105485-koreyst) |\n| Building Trustworthy AI Agents           | [Link](./06-building-trustworthy-agents/README.md) | [Video](https://youtu.be/iZKkMEGBCUQ?si=jZjpiMnGFOE9L8OK ) | [Link](https://aka.ms/ai-agents-beginners/collection?WT.mc_id=academic-105485-koreyst) |\n| Planning Design Pattern                  | [Link](./07-planning-design/README.md)             | [Video](https://youtu.be/kPfJ2BrBCMY?si=6SC_iv_E5-mzucnC)  | [Link](https://aka.ms/ai-agents-beginners/collection?WT.mc_id=academic-105485-koreyst) |\n| Multi-Agent Design Pattern               | [Link](./08-multi-agent/README.md)                 | [Video](https://youtu.be/V6HpE9hZEx0?si=rMgDhEu7wXo2uo6g)  | [Link](https://aka.ms/ai-agents-beginners/collection?WT.mc_id=academic-105485-koreyst) |\n| Metacognition Design Pattern             | [Link](./09-metacognition/README.md)               | [Video](https://youtu.be/His9R6gw6Ec?si=8gck6vvdSNCt6OcF)  | [Link](https://aka.ms/ai-agents-beginners/collection?WT.mc_id=academic-105485-koreyst) |\n| AI Agents in Production                  | [Link](./10-ai-agents-production/README.md)        | [Video](https://youtu.be/l4TP6IyJxmQ?si=31dnhexRo6yLRJDl)  | [Link](https://aka.ms/ai-agents-beginners/collection?WT.mc_id=academic-105485-koreyst) |\n\n## 🌐 Multi-Language Support\n\n| Language             | Code | Link to Translated README                               | Last Updated |\n|----------------------|------|---------------------------------------------------------|--------------|\n| Chinese (Simplified) | zh   | [Chinese Translation](./translations/zh/README.md)      | 2025-03-24   |\n| Chinese (Traditional)| tw   | [Chinese Translation](./translations/tw/README.md)      | 2025-02-13   |\n| Chinese (Hong Kong)  | hk   | [Chinese (Hong Kong) Translation](./translations/hk/README.md) | 2025-02-13   |\n| French               | fr   | [French Translation](./translations/fr/README.md)       | 2025-02-13   |\n| Japanese             | ja   | [Japanese Translation](./translations/ja/README.md)     | 2025-02-13   |\n| Korean               | ko   | [Korean Translation](./translations/ko/README.md)       | 2025-02-13   |\n| Portuguese           | pt   | [Portuguese Translation](./translations/pt/README.md)   | 2025-02-13   |\n| Spanish              | es   | [Spanish Translation](./translations/es/README.md)      | 2025-02-13   |\n| German               | de   | [German Translation](./translations/de/README.md)       | 2025-02-13   |\n| Persian              | fa   | [Persian Translation](./translations/fa/README.md)       | 2025-03-26   |\n| Polish               | pl   | [Polish Translation](./translations/pl/README.md)       | 2025-03-26  |\n\n## 🎒 Other Courses\n\nOur team produces other courses! Check out:\n\n- [**NEW** Generative AI for Beginners using .NET](https://github.com/microsoft/Generative-AI-for-beginners-dotnet?WT.mc_id=academic-105485-koreyst)\n- [Generative AI for Beginners](https://github.com/microsoft/generative-ai-for-beginners?WT.mc_id=academic-105485-koreyst)\n- [ML for Beginners](https://aka.ms/ml-beginners?WT.mc_id=academic-105485-koreyst)\n- [Data Science for Beginners](https://aka.ms/datascience-beginners?WT.mc_id=academic-105485-koreyst)\n- [AI for Beginners](https://aka.ms/ai-beginners?WT.mc_id=academic-105485-koreyst)\n- [Cybersecurity for Beginners](https://github.com/microsoft/Security-101??WT.mc_id=academic-96948-sayoung)\n- [Web Dev for Beginners](https://aka.ms/webdev-beginners?WT.mc_id=academic-105485-koreyst)\n- [IoT for Beginners](https://aka.ms/iot-beginners?WT.mc_id=academic-105485-koreyst)\n- [XR Development for Beginners](https://github.com/microsoft/xr-development-for-beginners?WT.mc_id=academic-105485-koreyst)\n- [Mastering GitHub Copilot for AI Paired Programming](https://aka.ms/GitHubCopilotAI?WT.mc_id=academic-105485-koreyst)\n- [Mastering GitHub Copilot for C#/.NET Developers](https://github.com/microsoft/mastering-github-copilot-for-dotnet-csharp-developers?WT.mc_id=academic-105485-koreyst)\n- [Choose Your Own Copilot Adventure](https://github.com/microsoft/CopilotAdventures?WT.mc_id=academic-105485-koreyst)\n\n## 🌟 Community Thanks\n\nThanks to [Shivam Goyal](https://www.linkedin.com/in/shivam2003/) for contributing important code samples demonstrating Agentic RAG. \n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit <https://cla.opensource.microsoft.com>.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft\ntrademarks or logos is subject to and must follow\n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos is subject to those third-parties' policies.\n"
    },
    {
      "name": "santhoshrao95/mulit_agent_it_support_system",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/27264958?s=40&v=4",
      "owner": "santhoshrao95",
      "repo_name": "mulit_agent_it_support_system",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2025-04-13T20:48:32Z",
      "updated_at": "2025-04-14T05:22:58Z",
      "topics": [],
      "readme": "# Multi-Agent IT Support System\n\n## Overview\nThis project implements an intelligent multi-agent IT support system designed to efficiently handle and resolve technical support issues using specialized agents and intelligent routing.\n\n## System Architecture\nThe system consists of four primary agents:\n1. **Master Agent**: Coordinates interactions, makes agent selection decisions, and manages overall conversation flow\n2. **User Intake Agent**: Collects initial problem information\n3. **Resolution Agent**: Attempts to resolve common IT issues\n4. **Escalation Agent**: Handles complex or recurring issues requiring human intervention\n\n### Key Features\n- Intelligent issue routing\n- Frequency-based issue tracking\n- Knowledge base for common IT problem resolutions\n- Automated escalation for recurring issues\n\n## System Flow\n![Multiagent it support system](https://github.com/user-attachments/assets/a2898146-5fc7-4572-a665-54d202951115)\n\n## How to run\n\n1. conda create -n <env_name> python=3.12\n2. pip install -r requirements.txt\n3. python <file_name.py>\n\nStep-by-Step Installation\n\nCreate a new Conda environment\n\nbashconda create -n it-support-agents python=3.12\n\nActivate the environment\n\nbashconda activate it-support-agents\n\nClone the repository\n\nbashgit clone https://github.com/yourusername/multi-agent-it-support.git\ncd multi-agent-it-support\n\nInstall dependencies\n\nbashpip install -r requirements.txt\nRunning the Application\nbashpython main.py\n"
    },
    {
      "name": "IamMikeHelsel/sneks",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/51098787?s=40&v=4",
      "owner": "IamMikeHelsel",
      "repo_name": "sneks",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2025-04-10T17:21:48Z",
      "updated_at": "2025-04-21T18:00:35Z",
      "topics": [],
      "readme": "# sneks\n\n## Description\n\nA classic Snake game implemented in Python with a highly performant, modern and beautiful graphical user interface (GUI). The project follows Test-Driven Development (TDD) principles using `pytest` for testing and `pytest-cov` for monitoring code coverage.\n\n## Features\n\n* Classic Snake gameplay: Control the snake to eat food and grow longer.\n* **Modern UI System**: Beautiful visuals with smooth animations, particle effects, and transitions.\n* **Multiple Game Screens**: Professional menu screen and in-game interface.\n* **Enhanced Visual Elements**: Anti-aliased shapes, gradients, and shadows for visual appeal.\n* **Interactive UI Components**: Animated buttons with visual feedback and hover effects.\n* **Pause Functionality**: Clean pause menu with resume and main menu options.\n* Score Tracking: Animated score display with visual feedback when points are earned.\n* Game Over Detection: The game ends if the snake hits the boundaries or itself.\n* Keyboard Controls: Intuitive controls for snake movement.\n\n## Technology Stack\n\n* **Language:** Python 3.x\n* **GUI Framework:** Pygame\n* **UI Architecture:** Component-based modular system with:\n  * Screen management system\n  * Particle effect engine\n  * Animation framework\n  * Interactive UI components\n* **Rendering:** Hardware-accelerated graphics with optimized pre-rendering\n* **Testing:** `pytest`\n* **Code Coverage:** `pytest-cov`\n\n## Installation\n\n1. **Clone the repository:**\n\n    ```bash\n    git clone <repository-url>\n    cd <repository-directory>\n    ```\n\n2. **Create a virtual environment (recommended):**\n\n    ```bash\n    python -m venv venv\n    source venv/bin/activate  # On Windows use `venv\\Scripts\\activate`\n    ```\n\n3. **Install dependencies:**\n\n    ```bash\n    pip install -r requirements.txt\n    ```\n\n    *(Note: Ensure `requirements.txt` lists all necessary packages, including the GUI library, pytest, and pytest-cov).*\n\n## How to Run the Game\n\nExecute the main Python script to start the game:\n\n```bash\npython main.py\n```\n\n### Testing the New UI\n\nThe enhanced UI system includes several features you can test:\n\n1. **Main Menu**: When you start the game, you'll see a modern menu screen with:\n   * Animated background and particle effects\n   * Interactive buttons with hover and click animations\n   * Stylish title with subtle animations\n\n2. **Game Interface**: During gameplay, test these enhanced features:\n   * Smooth, rounded snake segments with gradient effects\n   * Pulsing food animation with glow effects\n   * Animated score counter that increases with visual feedback\n   * Press P or ESC to open the pause menu\n\n3. **Pause Menu**: The pause system allows you to:\n   * Temporarily stop the game with a stylish overlay\n   * Resume gameplay by clicking the Resume button\n   * Return to the main menu with the Main Menu button\n\n4. **Game Over Screen**: When the game ends:\n   * A smooth fade overlay appears\n   * Try the Restart button to begin a new game immediately\n   * Return to the main menu via the Main Menu button\n\n"
    },
    {
      "name": "NortonHuang-SUFE/FortuneTell",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/54016596?s=40&v=4",
      "owner": "NortonHuang-SUFE",
      "repo_name": "FortuneTell",
      "description": "让八字、紫薇、星座等相互讨论的multi-agent产品",
      "homepage": null,
      "language": "Python",
      "created_at": "2025-04-09T03:24:53Z",
      "updated_at": "2025-04-21T08:17:42Z",
      "topics": [],
      "readme": "# FortuneTell（敬请期待）\n## 如何通过你的一句话知道你的命运？\n\n## 如何不止相信一家之言，而是博采众长？\n\n## 如何让整个互联网来响应你的问题？\n\n## FortuneTell：一个不止于命理的AI\n"
    },
    {
      "name": "CyberCRI/WeLearn-api",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/3897500?s=40&v=4",
      "owner": "CyberCRI",
      "repo_name": "WeLearn-api",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2025-03-24T16:20:18Z",
      "updated_at": "2025-04-16T11:58:59Z",
      "topics": [],
      "readme": "[![CC BY-NC-SA 4.0][cc-by-nc-sa-shield]][cc-by-nc-sa]\n\n\n![https://welearn-demo.learningplanetinstitute.org](https://welearn-demo.learningplanetinstitute.org/assets/logo-fdaefaa7.png)\n\n## About Us\n\nThis repository is maintained by Learning Planet Institute, a non-governmental organization dedicated to The Learning Planet Institute is a global NGO dedicated to transforming education through innovative, collaborative, and inclusive approaches. It fosters the development of sustainable learning ecosystems, promotes lifelong learning, and empowers individuals and communities to actively contribute to solving global challenges. The institute connects diverse stakeholders to create a more equitable and impactful educational future.\n\n## WeLearn API\n\n\nThis project is a web application built with **FastAPI** and **Poetry** for dependency management. It is designed to provide an interface for clients to search for **SDG-classified documents** and leverage **chat capabilities** enhanced by these SDG documents.\n\nThe application offers powerful **search** and **chat** endpoints that enable users to:\n\n- **Search for SDG-classified documents**: Clients can search through documents tagged with specific **Sustainable Development Goals (SDGs)** using advanced similarity-based search techniques.\n- **Chat functionality**: The chat interface is powered by a **Large Language Model (LLM)**, which is enriched with SDG-specific information to deliver more accurate and context-aware responses.\n\n## Table of Contents\n\n- [Introduction](#introduction)\n- [Features](#features)\n- [Installation](#installation)\n- [Usage](#usage)\n- [Requirements](#requirements)\n- [Docker Setup](#docker-setup)\n- [API Dependency](#api-dependency)\n- [Project Structure](#project-structure)\n- [License](#license)\n- [Contact](#contact)\n\n## Introduction\n\nThis project is built with **FastAPI**, **Poetry**, and **Docker Compose**. It creates a **REST API** that allows the client to interact with various services, including:\n\n- **Qdrant Database**: Stores vector embeddings and allows for similarity-based searches, enabling efficient document retrieval.\n- **Postgres Database**: A relational database used for storing structured data, such as metadata or user information.\n- **LLM Service**: Enhances the client’s experience by generating responses based on the data stored in both Qdrant and Postgres databases.\n\nThe application exposes several API endpoints that allow clients to:\n- **Search for documents** in the Qdrant database using similarity search.\n- **Enhance LLM responses** by utilizing the retrieved documents to provide more relevant, context-aware, and accurate answers.\n\nBy combining document search and LLM capabilities, the app ensures intelligent, data-driven interactions, making it easier for users to get the most relevant information.\nWhile it is open-source, **please note that this project is not currently open to external contributions**.\n\n## Features\n\n## API Endpoints\n\nThe project exposes several REST API endpoints that allow clients to interact with various features. Below is a list of the main endpoints available in the API:\n\n### Search Endpoints\n\n- **GET** `/api/v1/search/collections`  \n  **Description**: Retrieves the corpus or collections of documents available for searching.\n\n- **POST** `/api/v1/search/collections/{collection_query}`  \n  **Description**: Performs a search query on a specific collection to retrieve relevant items.  \n\n- **POST** `/api/v1/search/by_slices`  \n  **Description**: Allows searching for documents across all slices (segments) by language.\n\n- **POST** `/api/v1/search/by_document`  \n  **Description**: Performs a search on all documents, helping to locate content based on specific document data.\n\n### Q&A Operations (OpenAI Integration)\n\n- **POST** `/api/v1/qna/reformulate/query`  \n  **Description**: Reformulates a given query to improve clarity or context for the LLM (Large Language Model).\n\n- **POST** `/api/v1/qna/reformulate/questions`  \n  **Description**: Reformulates a set of questions to enhance understanding or to generate more context-specific queries.\n\n- **POST** `/api/v1/qna/chat/rephrase`  \n  **Description**: Rephrases a chat message to improve its quality or alter its expression while maintaining the original meaning.\n\n- **POST** `/api/v1/qna/chat/rephrase_stream`  \n  **Description**: Provides a streaming response of rephrased content during a chat, allowing for dynamic rephrasing.\n\n- **POST** `/api/v1/qna/chat/answer`  \n  **Description**: Provides an answer to a question from the chat interface, utilizing the LLM and relevant data.\n\n- **POST** `/api/v1/qna/stream`  \n  **Description**: Streams responses from the Q&A service, providing real-time answers to the user’s questions.\n\n---\n\n### Example Usage of Search Endpoints\n\nHere’s an example of how you can use the **Search** endpoints:\n\n#### Search Items in a Collection\n```bash\ncurl -X 'POST' \\\n  'https://API_URL/api/v1/search/collections/COLLECTION_NAME?query=QUERY_TO_SEARCH&nb_results=10' \\\n  -H 'accept: application/json' \\\n  -H 'Content-Type: application/json' \\\n  -d '{\n  \"sdg_filter\": [\n    1,\n    2,\n    3\n  ]\n}'\n```\n\n## Requirements\n\nTo run this project locally, you'll need the following tools:\n\n- **Python** (version >= 3.10)\n- **Poetry** (version >= 2.1): You can install Poetry by following the instructions [here](https://python-poetry.org/docs/#installation).\n- **Docker** (optional, for Docker setup)\n- **Docker Compose** (optional, for Docker setup)\n\n### Install Poetry:\n\nTo install Poetry, run the following command:\n\n```bash\ncurl -sSL https://install.python-poetry.org | python3 -\n```\n\nOnce installed, you can verify Poetry is installed correctly by running:\n\n```bash\npoetry --version\n```\n\n## Installation\n\nTo install and run this project locally, follow these steps:\n\n1. Clone the repository:\n```bash\n    git clone https://github.com/your-username/project-name.git\n```\n2. Navigate into the project directory:\n```bash\ncd WeLearn-api\n```\n3. Install the dependencies using Poetry:\n```bash\npoetry install\n```\n4. Copy and configure the environment variables:\n```bash\ncp .env.example .env\n```\n5. Update the .env file with the appropriate values for your local setup.\n6. (Optional) If you prefer to run the app with Docker Compose, see Docker Setup.\n\n## Usage\n\nOnce you have the app set up locally, you can use the application by navigating to the appropriate URL in your browser.\n\nRun the FastAPI app\nTo run the FastAPI app locally (without Docker), use the following command:\n\n```bash\npoetry run uvicorn app.main:app --reload\n```\n\nThe app will be available at http://localhost:8000/.\n\nRun Common Development Tasks with Makefile\nWe’ve included a Makefile for common tasks to streamline the setup and development process. For example:\n\nRun Tests:\n\n```bash\nmake test\n```\n\nRun App:\n\n```bash\nmake run-poetry\n```\n\nFor more details on available tasks, check the Makefile or run make to see the list of available commands.\n\n### API Dependency\nThis project relies on an external REST API for certain features. Some functionality of the application may not work properly if the API is down or unreachable.\n\nYou can check the status and setup instructions for the API in the API repository.\n\n### Docker Setup\nIf you prefer to run the application using Docker, follow these steps:\n\nEnsure Docker and Docker Compose are installed on your system.\n\nRun the following command to start the application and its dependencies:\n\n```bash\ndocker-compose up\n```\n\nThe app should now be available at http://localhost:8000/.\n\nTo stop the app, use:\n\n```bash\ndocker-compose down\n```\n\n## Project Structure\nHere’s an overview of the project directory structure:\n\n- **`app/`**: Contains the FastAPI application code.\n- **`app/main.py`**: The entry point of the FastAPI app.\n- **`app/api/`**: Contains the different API routes.\n- **`app/models/`**: Contains the Pydantic models and database models.\n- **`app/services/`**: Contains business logic or services.\n- **`app/utils/`**: Contains utility functions used across the app.\n- **`docker-compose.yml`**: The configuration for Docker Compose.\n- **`Dockerfile`**: The Dockerfile for building the application container.\n- **`tests/`**: Contains unit and integration tests for the application.\n- **`.env.example`**: Example environment variable file. Copy this to .env and configure it with your settings.\n- **`Makefile`**: Contains commonly used commands like make run or make test.\n\n## License\n\nThis work is licensed under a\n[Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License][cc-by-nc-sa].\n\n[![CC BY-NC-SA 4.0][cc-by-nc-sa-image]][cc-by-nc-sa]\n\n[cc-by-nc-sa]: http://creativecommons.org/licenses/by-nc-sa/4.0/\n[cc-by-nc-sa-image]: https://licensebuttons.net/l/by-nc-sa/4.0/88x31.png\n[cc-by-nc-sa-shield]: https://img.shields.io/badge/License-CC%20BY--NC--SA%204.0-lightgrey.svg\n\n## Contact\nIf you have any questions or need further information, feel free to reach out at:\n\nEmail: welearn@learningplanetinstitute.org\n"
    },
    {
      "name": "msalemor/agentic-intro-autogen",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/44649358?s=40&v=4",
      "owner": "msalemor",
      "repo_name": "agentic-intro-autogen",
      "description": "An introduction to Agentic systems with Autogen",
      "homepage": null,
      "language": "Shell",
      "created_at": "2025-04-09T00:44:22Z",
      "updated_at": "2025-04-21T18:56:28Z",
      "topics": [],
      "readme": "# Introduction to Agentic AI solutions with AutoGen\n\n## 1.0 - What is Agentic AI\n\nAgentic AI refers to AI systems that can act autonomously, adapt in real-time, and solve multi-step problems based on context and objectives, going beyond simple question-answering or content generation. \n\n## 2.0 - What is AutoGen (0.5.1)\n\nAutoGen is an open-source programming framework for building AI agents and facilitating cooperation among multiple agents to solve tasks. AutoGen aims to provide an easy-to-use and flexible framework for accelerating development and research on agentic AI. It offers features such as agents that can converse with other agents, LLM and tool use support, autonomous and human-in-the-loop workflows, and multi-agent conversation patterns.\n\nReference:\n- [AutoGen](https://microsoft.github.io/autogen/stable/index.html)\n\n### 2.1 - What is AutoGenChat\n\nAgentChat is a high-level API for building multi-agent applications. It is built on top of the autogen-core package. \n\nReference:\n- [AutoGen AgentChat](https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/index.html)\n\n#### 2.1.1 - Top concepts\n\n- **Agent definition**: It is important to know how to configure the agents. Agents will have different capabilities such as having tools, having different system prompts, terminating messages, etc.\n- **Termination**: If not careful, an Agent conversation could run \"forever\". Special attention needs to be paid when to terminate a conversation.\n- **Agent Team**: There are different types of Agent team presets such as RoundRobinChat, SelectorGroupChat, MagenticOneGroupChat, and others.\n- **Message observability and handling**: It is important to know how to tap into the Agent conversation.\n\n### 2.2 - What is AutoGen Core\n\nAutoGen core offers an easy way to quickly build event-driven, distributed, scalable, resilient AI agent systems.\n\nReference:\n- [AutoGen Core](https://microsoft.github.io/autogen/stable/user-guide/core-user-guide/index.html)\n\n## 3.0 - Demos\n\n### 3.1 - Requirements\n\n- Python 3.10+\n- Azure OpenAI\n- Install the requirements: \n  - `pip install -r requirements.txt`\n- Create a `.env` file with the following settings:\n\n```bash\nAZURE_OPENAI_ENDPOINT=https://<NAME>.openai.azure.com/\nAZURE_OPENAI_API_KEY=<API_KEY>\nAZURE_OPENAI_API_VERSION=2024-10-21\n```\n\nReference:\n- [Sample `.env` file](https://github.com/msalemor/agentic-intro-autogen/blob/main/.env-sample)\n\n### 3.2 - Writer/Reviewer without AutoGen\n\n#### 3.2.1 - Copilot version\n\n```text\nsystem:\nYou are both a children book author and a children book reviewer. \n\n- Given a topic generate a story, but do not generate a feedback.\n- Given a feedback request, generate the feedback, but do not rewrite the story.\n- Given a rewrite request, rewrite the story with the available feedback, and if the rewrite includes the feedback recommendations finish with 'APPROVED'\n\nuser:\nWrite a story about a cosmopolitan cat living in a big city.\n\nuser:\nReview the story\n\nuser:\nRewrite the story with the feedback\n```\n\n#### 3.2.2 - Script version\n\nThis sample uses the completion API to write a story, then review the story, then modify the story according to the suggested changes, and finally perform a final review.\n\n- Code: [Writer/Reviewer - No Agents](https://github.com/msalemor/agentic-intro-autogen/blob/main/demos/writer-reviewer.py)\n\n### 3.3 - Write/Reviewer - Poor man's Agentic system\n\nThis sample uses the a poor man's Agentic framework to write a story, then review the story, then modify the story according to the suggested changes. It is design to highligh some important Agentic concepts in a framework.\n\n- Code: [Writer/Reviewer - Poor man's Agentic system](https://github.com/msalemor/agentic-intro-autogen/blob/main/demos/writer-reviewer-poor.py)\n\n### 3.4 - Writer/Reviewer with AutoGen\n\nThis sample uses AutoGen to create two agents participating in a RoundRobinChat. There is the writer agent and the reviewer agent. The writer agent initially creates a story, and passes the story to the reviewer for a review. The reviewer either suggests more changes or approves the story. If the reviewer suggests more changes, it passes the story back to the write for a rewrite. The write rewrite the story and passes the story to the reviewer. The process continues until the reviewer approves the story and terminates the conversation with an `Approve` message.\n\n- Code: [Agentic - Writer/Reviewer](https://github.com/msalemor/agentic-intro-autogen/blob/main/demos/writer-reviewer-agents.py)\n\n### 3.5 - KQL Writer with AutoGen (more advanced solution)\n\nIn this example, for Agents are used to generate KQL from natural language:\n\nAgents:\n- **Schema getter agent**: This agent is configured to analyze the query and us a tool to simulate getting the schema from participating tables or functions. The agent has a list of tables that knows of and uses calls the get_mock_schema function with the name of the tables.\n- **Query classifier agent**: This agent is configured to classify the KQL query as: single-table, multi-table single-cluster, multi-table multi-cluster.\n- **Example generator agent**: This agent is configured to generate a sample query based on the query classification.\n- **KQL writer agent**: This agent take the schema and the sample and generates a final query and terminates the conversation.\n\n#### 3.5.1 - Diagram\n\n```mermaid\ngraph LR\n  S((Start<br>Task))-->A(Schema<br>Getter)-->B(Query<br>Classifier)-->C(Example<br>Getter)-->D(KQL<br>Writer)-->E((End<br>Task))\n```\n\n- Code: [Agentic - KQL Writer](https://github.com/msalemor/agentic-intro-autogen/blob/main/demos/kql-writer-agents.py)\n\n### 3.6 - Autogen Studio\n\n- Installation:\n  - `pip install autogenstudio`\n- Running:\n  - `export OPENAI_API_KEY=123 # avoid showing errors`\n  - `autogenstudio ui --appdir ./mydir #set an appdir folder`\n- Use the Gallery tab to setup teams, Agents, Tools, and Terminations\n  - > Note: Many of these configurations have to be done in the JSON editor.\n  - > Note: if working with Azure create and configure a model with the Azure settings. Assign these model to the participating agents (from the UI and the JSON editor).\n\nReference:\n- [AutoGen Studio](https://microsoft.github.io/autogen/stable/user-guide/autogenstudio-user-guide/index.html)\n"
    },
    {
      "name": "micchyboy237/JetScripts",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/42490977?s=40&v=4",
      "owner": "micchyboy237",
      "repo_name": "JetScripts",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2024-11-23T15:35:54Z",
      "updated_at": "2025-04-23T07:48:44Z",
      "topics": [],
      "readme": ""
    },
    {
      "name": "AskTheApi/asktheapi-team-builder",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/206834780?s=40&v=4",
      "owner": "AskTheApi",
      "repo_name": "asktheapi-team-builder",
      "description": "Agent network builder for communicating with openapi apis. Based in autogen",
      "homepage": "https://asktheapi.ai",
      "language": "Python",
      "created_at": "2025-04-05T11:07:54Z",
      "updated_at": "2025-04-14T10:54:21Z",
      "topics": [
        "agent-tools",
        "agentic-ai",
        "agents",
        "ai",
        "mcp",
        "mcp-server",
        "openapi-specification"
      ],
      "readme": "# AskTheApi Team Builder\n\n[![PyPI version](https://badge.fury.io/py/asktheapi-team-builder.svg)](https://badge.fury.io/py/asktheapi-team-builder)\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n[![Python Versions](https://img.shields.io/pypi/pyversions/asktheapi-team-builder.svg)](https://pypi.org/project/asktheapi-team-builder/)\n\nA high-level Python library for building and managing networks of autonomous agents that collaborate to solve complex tasks. It's designed to work seamlessly with APIs defined using the OpenAPI standard. The library provides a clean, type-safe interface for creating, configuring, and running teams of agents, making it easy to orchestrate multi-agent workflows with minimal boilerplate.\n\n## Features\n\n- 🚀 Effortless Agent Network Creation: Quickly build agent networks with custom tools and capabilities based on OpenAPI specifications.\n\n- 🤝 Team-Based Collaboration: Easily define agent teams with automatic coordination handled by a built-in planning agent.\n\n- 📡 Streaming Interactions: Stream agent communication in real-time for more dynamic and responsive workflows.\n\n- 🔧 Built-in HTTP Client: Simplify tool implementation with an integrated HTTP client ready to call external APIs.\n\n- ✨ Type Safety with Pydantic: Leverage Pydantic models for robust data validation and clear type definitions.\n\n- 🎯 Clean and Intuitive API: Designed for developers—minimal boilerplate, maximum clarity.\n\n## Installation\n\n```bash\npip install asktheapi-team-builder\n```\n\n## Quick Start\n\nHere's how to use the package:\n\n\n\n# 1. Create agents from OpenAPI spec\n```python\nfrom asktheapi_team_builder import TeamBuilder, Agent, Tool, Message, APISpecHandler\nfrom typing import List\nasync def create_agents_from_spec():\n    # Initialize handlers\n    api_spec_handler = APISpecHandler()\n    \n    # Download and parse OpenAPI spec\n    spec_content = await api_spec_handler.download_url_spec(\"https://api.example.com/openapi.json\")\n    \n    # Classify endpoints into logical groups\n    classification_result = await api_spec_handler.classify_spec(\n        spec_content\n    )\n    \n    # Generate agents for each group\n    agents = []\n    for group_spec in classification_result.specs:\n        agent_result = await api_spec_handler.generate_agent_for_group(\n            group_spec,\n            spec_content\n        )\n        agents.append(agent_result)\n    \n    return agents\n```\n\n# 2. Build and run a team\n```python\nasync def run_agent_team(agents: List[Agent], query: str):\n    # Initialize team builder\n    team_builder = TeamBuilder(\n        model=\"gpt-4\",\n        model_config={\"temperature\": 0.7}\n    )\n    \n    # Build the team\n    team = await team_builder.build_team(agents)\n    \n    # Create messages\n    messages = [\n        Message(\n            role=\"user\",\n            content=query\n        )\n    ]\n    \n    # Run the team with streaming\n    async for event in team_builder.run_team(team, messages, stream=True):\n        if isinstance(event, ChatMessage):\n            print(f\"{event.source}: {event.content}\")\n```        \n# Example usage\n```python\nasync def main():\n    # Create agents from spec\n    api_agents = await create_agents_from_spec()\n    \n    # Combine with manual agents\n    all_agents = [weather_agent] + api_agents\n    \n    # Run the team\n    await run_agent_team(\n        all_agents,\n        \"What's the weather like in London and how might it affect local businesses?\"\n    )\n```\n\n## Custom Headers and Configuration\n\nYou can configure the team builder with custom headers and model settings:\n\n```python\nteam_builder = TeamBuilder(\n    model=\"gpt-4\",\n    model_config={\n        \"temperature\": 0.7,\n        \"default_headers\": {\n            \"Authorization\": \"Bearer your-token\",\n            \"Custom-Header\": \"custom-value\"\n        }\n    }\n)\n\n# Run team with extra headers for specific requests\nteam = await team_builder.build_team(agents)\nresult = await team_builder.run_team(\n    team,\n    messages,\n    extra_headers={\"Request-ID\": \"123\"}\n)\n```\n\n## MCP (Model Control Protocol) Support\n\nThe library includes built-in support for Model Control Protocol, allowing you to expose your agent teams as API endpoints with automatic tool generation from OpenAPI specifications.\n\n```python\nfrom asktheapi_team_builder import MCPService, MCPConfig\n\n# Configure MCP service\nmcp_config = MCPConfig(\n    transport=\"sse\",  # Server-Sent Events transport\n    port=8000,        # Port to run the MCP server\n    name=\"asktheapi_mcp\"  # Service name\n)\n\n# Initialize MCP service\nmcp_service = MCPService(mcp_config)\n\n# Start MCP server with OpenAPI spec\nawait mcp_service.start_from_spec(\n    url_spec=\"https://api.example.com/openapi.json\",\n    headers={\"Authorization\": \"Bearer your-token\"}\n)\n```\n\nThe MCP service will:\n- Automatically download and parse the OpenAPI specification\n- Classify endpoints into logical groups\n- Generate appropriate tools for each group\n- Expose these tools through a Model Control Protocol interface\n- Handle real-time streaming of agent interactions\n\n## Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request. For major changes, please open an issue first to discuss what you would like to change.\n\n1. Fork the repository\n2. Create your feature branch (`git checkout -b feature/amazing-feature`)\n3. Commit your changes (`git commit -m 'Add some amazing feature'`)\n4. Push to the branch (`git push origin feature/amazing-feature`)\n5. Open a Pull Request\n\n## Development Setup\n\n```bash\n# Clone the repository\ngit clone https://github.com/alexalbala/asktheapi-team-builder.git\ncd asktheapi-team-builder\n\n# Install dependencies\npip install -e \".[dev]\"\n\n# Run tests\npytest\n```\n\n## License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n\n## Acknowledgments\n\n- Built on top of [Microsoft's AutoGen](https://github.com/microsoft/autogen)\n- Inspired by the need for a higher-level interface for agent team management \n"
    },
    {
      "name": "DellDi/autogen-fastapi-postgres",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/40460351?s=40&v=4",
      "owner": "DellDi",
      "repo_name": "autogen-fastapi-postgres",
      "description": "FastAPI 智能体应用与 PostgreSQL 数据库集成",
      "homepage": null,
      "language": "Python",
      "created_at": "2025-04-03T07:07:38Z",
      "updated_at": "2025-04-15T06:40:06Z",
      "topics": [],
      "readme": "# autogen-fastapi-postgres\n\n## 项目简介\n\n`autogen-fastapi-postgres` 是一个基于 FastAPI 的现代智能体（Agent）对话服务后端，集成了 PostgreSQL 数据库，支持多种对话前端（如 Chainlit、Streamlit 等），并采用模块化架构，便于扩展和二次开发。\n\n---\n\n## 主要特性\n\n- 🚀 **FastAPI**：高性能异步 API 框架，支持自动文档和类型校验\n- 🗄️ **PostgreSQL**：企业级关系型数据库，数据安全可靠\n- 🧩 **多前端适配**：支持 Chainlit、Streamlit 等多种对话前端\n- 🦾 **异步文件处理**：aiofiles 加持，I/O 高效\n- 🛠️ **现代依赖管理**：基于 pyproject.toml + uv，支持快速安装和跨平台兼容\n- 🧬 **模块化设计**：核心功能按包划分，便于维护和扩展\n- 📝 **自动化脚本**：集成 Alembic 数据库迁移、环境变量管理等\n\n---\n\n## 目录结构\n\n```plaintext\n.\n├── agentchat_fastapi/      # FastAPI 主后端服务\n├── agentchat_chainlit/     # Chainlit 前端适配\n├── agentchat_streamlit/    # Streamlit 前端适配\n├── core_semantic_router/   # 语义路由与核心逻辑\n├── alembic/                # 数据库迁移脚本\n├── pyproject.toml          # 依赖与构建配置\n├── requirements.txt        # 自动生成的依赖锁定文件\n├── README.md               # 项目说明文档\n└── ...                     # 其它辅助文件\n```\n\n---\n\n## 快速开始\n\n```bash\n# 1. 安装依赖（推荐使用 uv）\nuv pip install -e .\n\n# 2. 启动 FastAPI 服务\nuv run -m agentchat_fastapi.main\n\n# 3. 浏览器访问\n# 默认监听 [http://127.0.0.1](http://127.0.0.1):8000/docs 查看接口文档\n```\n\n---\n\n## 依赖环境\n\n- Python >= 3.13\n- FastAPI\n- SQLAlchemy\n- asyncpg, psycopg2-binary\n- uvicorn\n- streamlit, chainlit（可选前端）\n- aiofiles, PyYAML 等\n\n---\n\n## 适用场景\n\n- 智能体对话系统后端\n- 多前端适配的 AI 应用\n- 快速原型和企业级服务部署\n如需详细功能说明、开发规范或贡献指南，欢迎补充完善本文件，或联系维护者！"
    },
    {
      "name": "arashaga/agents-hackathon",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/1166344?s=40&v=4",
      "owner": "arashaga",
      "repo_name": "agents-hackathon",
      "description": "This repository includes samples for agentic solutions built for hackaton",
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2025-04-02T15:15:47Z",
      "updated_at": "2025-04-08T17:30:05Z",
      "topics": [],
      "readme": "### Azure AI Hachathon\n\nThis repository demonstrates the implementation of multi-agent use cases using the Semantic Kernel framework.\n\n### Features\n\n1. **Multi-Agent Content Creation**  \n    Leverages Semantic Kernel, AgentGroupChat, and ChatCompletion API for collaborative content generation.\n\n2. **NL2SQL Agent**  \n    A natural language to SQL agent that queries a sample AdventureWorks database.\n\n3. **Semantic Kernel Agent with Model Context Protocol (MCP)**  \n    Implements a sample agent using the MCP framework.\n\n### Getting Started\n\nFollow these steps to execute the notebooks:\n\n1. **Environment Setup**  \n    - Rename `.env-sample` to `.env` and populate the required environment variables.\n\n2. **NL2SQL Agent**  \n    - Deploy a sample AdventureWorks database.\n    - Ensure you have a SQL Server connection string with SQL authentication.\n\n3. **API Keys**  \n    - Different notebooks may require specific API keys. Refer to the documentation within each notebook for details.\n\n### Prerequisites\n\n- Ensure all dependencies are installed as per the project requirements.\n- Verify that your environment variables and API keys are correctly configured.\n\n### Notes\n\n- For detailed instructions, refer to the comments and documentation within each notebook.\n- Contributions and feedback are welcome to improve this repository further."
    },
    {
      "name": "hinthornw/mabench",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/13333726?s=40&v=4",
      "owner": "hinthornw",
      "repo_name": "mabench",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2025-03-23T18:43:26Z",
      "updated_at": "2025-04-17T02:12:48Z",
      "topics": [],
      "readme": "# Mabench\n\nForked and adapted from [tau-bench](https://github.com/tau-bench/tau-bench). They deserve a lot of credit."
    },
    {
      "name": "chepyle/multiagent-demo",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/6445835?s=40&v=4",
      "owner": "chepyle",
      "repo_name": "multiagent-demo",
      "description": "Multi-agent LLM demo",
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2025-02-22T17:19:07Z",
      "updated_at": "2025-04-15T18:05:31Z",
      "topics": [],
      "readme": "# MultiAgent Demo\n\nMultiAgent Demo is a project designed to facilitate the development and deployment of automated agents. This project provides a structured environment for building, testing, and running various agents in a cohesive manner.\n\n**Presentation Slides [Here](https://chepyle.github.io/multiagent-demo/)**\n\n## Local Installation\n\nTo get started with MultiAgent Demo, clone the repository and install the required dependencies:\n\n```bash\ngit clone https://github.com/chepyle/multiagent-demo.git\ncd multiagent-demo\npip install -r requirements.txt\n```\n\n## Quickstart\n\n - Use/ create a personal GitHub.com account\n - Create your own fork of the repository at https://github.com/chepyle/multiagent-demo\n - Run the repository in a GitHub Codespace\n\n\nTo run the application, run the `run.sh` script:\n\n```bash\npython ./run.sh\n```\n\n## Contributing\n\nContributions are welcome! Please open an issue or submit a pull request for any enhancements or bug fixes.\n\n## License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n"
    },
    {
      "name": "digital-duck/st_tools",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/145862907?s=40&v=4",
      "owner": "digital-duck",
      "repo_name": "st_tools",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2025-02-23T18:16:19Z",
      "updated_at": "2025-04-03T03:44:13Z",
      "topics": [],
      "readme": "# st_tools# Utility Tools\n\n\n## Image Browser\n\nstreamlit app\n\n## Markdown Viewer\n\nsingle HTML page js app\n\nopen `markdown-viewer/markdown-viewer.html` in browser, editor markdown text and preview"
    },
    {
      "name": "sitloboi2012/AMS",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/33688887?s=40&v=4",
      "owner": "sitloboi2012",
      "repo_name": "AMS",
      "description": "A robust and scalable Agent Management Server inspired by Kubernetes and Model Context Protocol. Seamlessly manage, orchestrate, and collaborate with AI agents across multiple frameworks, enhancing productivity and innovation for developers.",
      "homepage": null,
      "language": "Python",
      "created_at": "2025-03-29T16:21:24Z",
      "updated_at": "2025-03-31T02:41:28Z",
      "topics": [],
      "readme": "<div align=\"center\">\n  <img src=\"docs/ams.png\" alt=\"AMS Logo\" width=\"100\"/>\n  \n  <h1>Agent Management Server (AMS)</h1>\n  <p><em>Orchestrating AI Agents Across Frameworks</em></p>\n  \n  <!-- <p>\n    <a href=\"https://github.com/sitloboi2012/ams/stargazers\">\n      <img alt=\"GitHub stars\" src=\"https://img.shields.io/github/stars/sitloboi2012/ams?style=for-the-badge\">\n    </a>\n    <a href=\"https://github.com/sitloboi2012/ams/network\">\n      <img alt=\"GitHub forks\" src=\"https://img.shields.io/github/forks/sitloboi2012/ams?style=for-the-badge\">\n    </a>\n    <a href=\"https://github.com/sitloboi2012/ams/issues\">\n      <img alt=\"GitHub issues\" src=\"https://img.shields.io/github/issues/sitloboi2012/ams?style=for-the-badge\">\n    </a>\n    <a href=\"https://github.com/sitloboi2012/ams/blob/main/LICENSE\">\n      <img alt=\"License\" src=\"https://img.shields.io/github/license/sitloboi2012/ams?style=for-the-badge\">\n    </a>\n  </p> -->\n</div>\n\n## 📋 Introduction\n\nThe Agent Management Server (AMS) is a powerful platform designed to streamline the management and orchestration of AI agents across various frameworks. Inspired by Kubernetes and the Model Context Protocol, AMS provides a unified interface for registering, managing, and orchestrating AI agents from different frameworks (AutoGen, CrewAI, etc.), enhancing productivity and innovation for developers.\n\nWith AMS, you can register agents from different frameworks, manage their metadata, and create powerful collaborations between them through a standardized system, without getting bogged down in framework-specific implementation details.\n\n## ❓ Problem Statement\n\n### Challenges for AI Agent Developers\n\nDevelopers working with AI agents face several challenges:\n\n1. **🔄 Framework Fragmentation**: Each agent framework (AutoGen, CrewAI, etc.) has its own API, concepts, and implementation details, creating a steep learning curve when working with multiple frameworks.\n\n2. **🧩 Agent Orchestration Complexity**: Creating effective agent collaborations requires significant boilerplate code and deep knowledge of each framework's nuances.\n\n3. **♻️ Limited Reusability**: Agents created in one framework cannot easily collaborate with agents from another framework, resulting in duplicated work.\n\n4. **📈 Scaling Overhead**: As agent-based applications grow, managing agent lifecycle, communication, and task delegation becomes increasingly complex.\n\n### How AMS Solves These Problems\n\nThe Agent Management Server addresses these challenges by:\n\n1. **🔌 Unified Interface**: Providing a single, consistent API to work with agents from any supported framework, reducing the learning curve.\n\n2. **🧠 Automated Orchestration**: Intelligently selecting and connecting the right agents for each task based on capabilities, not framework specifics.\n\n3. **🌉 Framework Interoperability**: Enabling seamless collaboration between agents from different frameworks through standardized communication protocols.\n\n4. **☸️ Kubernetes-Inspired Architecture**: Offering a scalable, robust system for managing agent registrations, capabilities, and communication.\n\n5. **🧩 LLM-Based Capability Matching**: Unlike traditional systems that rely on keyword matching or hard-coded rules, AMS uses LLMs to intelligently match tasks to agent capabilities:\n   - **Semantic Understanding**: Analyzes the meaning and intent behind tasks\n   - **Rich Descriptions**: Capabilities are defined using natural language descriptions and example tasks\n   - **Adaptive Selection**: Intelligently scores and ranks agents based on capability relevance\n   - **Extensible Design**: Easy to add new capabilities without modifying code\n\nBy using AMS, developers can focus on building powerful agent-based applications without getting bogged down in framework-specific implementation details or complex orchestration logic.\n\n## 🏗️ Architecture\n\nThe AMS architecture is designed with modularity and extensibility in mind:\n\n1. **📦 Agent Registry**: Central registry that stores agent metadata and capabilities.\n2. **🧠 Capability Registry**: Enables intelligent matching between tasks and agent capabilities using semantic understanding.\n3. **🎮 Supervisor Agent**: Orchestrates collaborations by analyzing tasks and selecting appropriate agents.\n4. **💬 Communication Hub**: Manages standardized message passing between agents.\n5. **🔄 Framework Adapters**: Provides consistent interfaces to different agent frameworks (AutoGen, CrewAI, etc.).\n\n### Architecture Diagram\n\n```mermaid\nflowchart TD\n    %% Client/API Layer\n    Client([Client Application]) --> API[REST API Layer]\n    \n    %% API to Core Components\n    API --> |Register Agent| Registry\n    API --> |Create Task| Supervisor\n    API --> |Send/Receive Messages| ComHub\n    \n    %% Core Components\n    subgraph \"Core Components\"\n        direction TB\n        Registry[(Agent Registry)]\n        CapRegistry[Capability Registry]\n        Supervisor{Supervisor Agent}\n        ComHub[Communication Hub]\n        Adapters[Framework Adapters]\n        \n        %% Relationships between Core Components\n        Registry <--> |Query/Store| CapRegistry\n        Supervisor --> |Query Agents| Registry\n        Supervisor --> |Match Capabilities| CapRegistry\n        Supervisor --> |Route Messages| ComHub\n        ComHub <--> |Standardize Messages| Adapters\n    end\n    \n    %% Framework Integration\n    subgraph \"Agent Frameworks\"\n        direction TB\n        AutoGen[AutoGen Framework]\n        CrewAI[CrewAI Framework]\n        Future[Future Frameworks...]\n    end\n    \n    %% Adapter Connections\n    Adapters --> |Translate| AutoGen\n    Adapters --> |Translate| CrewAI\n    Adapters --> |Translate| Future\n    \n    %% Task Flow\n    API --> |Get Task Status| Supervisor\n    Supervisor --> |Task Execution| ComHub\n    \n    %% Define styles\n    classDef client fill:#e0e0e0,stroke:#333,stroke-width:1px,color:#000;\n    classDef api fill:#d0e8d0,stroke:#333,stroke-width:1px,color:#000;\n    classDef core fill:#d0d0e8,stroke:#333,stroke-width:1px,color:#000;\n    classDef frameworks fill:#e8d0d0,stroke:#333,stroke-width:1px,color:#000;\n    classDef default color:#000;\n    \n    %% Apply styles\n    class Client client;\n    class API api;\n    class Registry,CapRegistry,Supervisor,ComHub,Adapters core;\n    class AutoGen,CrewAI,Future frameworks;\n```\n\nThe architecture shows how components work together:\n\n- **Agent Registry** stores metadata about all registered agents\n- **Capability Registry** enables semantic matching of tasks to agent capabilities\n- **Supervisor Agent** orchestrates task execution and agent collaboration\n- **Communication Hub** manages message passing between agents\n- **Framework Adapters** provide consistent interfaces to different agent frameworks\n\n## ✨ Features & Roadmap\n\nAMS is being developed in phases, with each phase introducing new capabilities. Here's what's available now and what's coming in the future:\n\n### 🎯 Phase 1: Foundation (Current)\n\n**Available Now:**\n- **📝 Framework Integration**: Support for AutoGen & CrewAI frameworks\n- **🤖 Agent Management**: Register, configure, and manage agents with custom capabilities\n- **💬 Communication**: Single-agent chat and multi-agent collaboration sessions\n- **🧠 Intelligent Task Analysis**: Semantic matching of tasks to agent capabilities\n- **🔄 Session Management**: Create, monitor, and control collaboration sessions\n- **🛠️ Extensible Design**: Modular architecture that's easy to extend\n- **🌐 REST API**: Complete RESTful API for integration with other systems\n\n### 🔄 Phase 2: Advanced Framework Integration\n\n**Coming Soon:**\n- **AG2 Integration**: Support for AutoGen 2.0's enhanced capabilities\n- **LlamaIndex Integration**: Knowledge retrieval and RAG capabilities\n- **LangGraph Integration**: Structured agent workflows\n- **Tool Integration Framework**: Unified tool registration and discovery\n- **Cross-Framework Tool Sharing**: Use tools across different agent frameworks\n- **Advanced Capability Matching**: Enhanced semantic understanding of tasks\n\n### 💾 Phase 3: Memory and Knowledge Systems\n\n**Planned:**\n- **Long-Term Memory**: Persistent knowledge for agents across sessions\n- **Shared Knowledge Bases**: Collaborative knowledge sharing between agents\n- **Multi-Level Memory Architecture**: Working, short-term, and long-term memory\n- **Knowledge Retrieval (RAG)**: Enhanced information access capabilities\n- **Learning From Feedback**: Self-improvement based on task outcomes\n\n### 🚀 Phase 4: Advanced Orchestration\n\n**Future:**\n- **Dynamic Task Decomposition**: Automated breaking down of complex tasks\n- **Parallel Execution**: Run compatible subtasks simultaneously\n- **Adaptive Workflows**: Adjust execution based on intermediate results\n- **Multi-Modal Capabilities**: Vision, audio, and other input/output modalities\n- **Complex Collaboration Patterns**: Pre-defined collaboration templates\n\n## 🚀 Installation\n\n```bash\n# Clone the repository\ngit clone https://github.com/sitloboi2012/ams.git\ncd ams\n\n# Install the package\npip install -e .\n```\n\n## 🔧 Usage\n\n### Starting the Server\n\n```bash\n# Start the server with default settings\npython -m ams\n\n# Or use the CLI with specific settings\nams --host 0.0.0.0 --port 8000 --log-level info\n\n# Start with a configuration file\nams --config config.yaml\n```\n\n### Configuration\n\nAMS supports configuration through multiple methods, in order of precedence:\n\n1. **Command-line arguments**: Highest precedence, overrides other settings\n2. **Configuration file**: YAML file with structured configuration\n3. **Environment variables**: Using the `AMS_` prefix (e.g., `AMS_PORT=8000`)\n4. **Default values**: Built-in fallbacks\n\n#### Configuration File Example\n\n```yaml\n# config.yaml\nserver:\n  host: \"0.0.0.0\"\n  port: 8000\n  reload: false\n  log_level: \"info\"\n  workers: 4\n\ndatabase:\n  url: \"sqlite:///ams.db\"\n  echo: false\n  pool_size: 5\n\nsecurity:\n  secret_key: \"your-secure-key-here\"\n  token_expiration: 1440\n  enable_auth: true\n\nllm:\n  provider: \"openai\"\n  api_key: \"\"\n  default_model: \"gpt-4\"\n  temperature: 0.7\n```\n\nTo use a configuration file:\n\n```bash\n# Copy the example configuration\ncp config.example.yaml config.yaml\n\n# Edit as needed\nnano config.yaml\n\n# Start the server with your configuration\nams --config config.yaml\n```\n\n#### Environment Variables\n\nYou can use environment variables prefixed with `AMS_` to configure the system:\n\n```bash\n# Set environment variables\nexport AMS_PORT=9000\nexport AMS_LOG_LEVEL=debug\nexport AMS_LLM_PROVIDER=openai\nexport AMS_LLM_API_KEY=your-api-key\n\n# Start the server\nams\n```\n\n### API Examples\n\n#### Register an Agent\n\n```bash\ncurl -X POST http://localhost:8000/agents \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"name\": \"CodeGenerator\",\n    \"description\": \"An agent that can generate code\",\n    \"system_prompt\": \"You are a helpful assistant that specializes in writing code. You write clean, efficient, and well-documented code.\",\n    \"framework\": \"autogen\",\n    \"capabilities\": [\n      {\n        \"name\": \"code_execution\",\n        \"description\": \"Can generate and execute code\"\n      }\n    ],\n    \"config\": {\n      \"llm_config\": {\n        \"model\": \"gpt-4\",\n        \"temperature\": 0.1\n      }\n    }\n  }'\n```\n\n#### Create a Task\n\n```bash\ncurl -X POST http://localhost:8000/tasks \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"task\": \"Write a Python function to calculate the Fibonacci sequence\"\n  }'\n```\n\n#### Get Messages from a Session\n\n```bash\ncurl -X GET http://localhost:8000/tasks/{session_id}/messages\n```\n\n## 👏 Acknowledgements\n\n- The Agent Management Server is inspired by the [Model Context Protocol (MCP)](https://github.com/microsoft/semantic-kernel/tree/main/python/semantic_kernel/connectors/ai/open_ai/model_context_protocol), but focuses on agent management rather than tools management.\n- The architecture is inspired by [Kubernetes](https://kubernetes.io/) to provide a robust, scalable system for agent orchestration.\n\n## 🤝 Contributing\n\nWe welcome contributions to the Agent Management Server! If you're interested in helping improve AMS, here's how you can contribute:\n\n1. **🐛 Report Issues**: Submit bugs, suggest features, or help with documentation through GitHub Issues.\n2. **💻 Submit Pull Requests**: Contribute code improvements, bug fixes, or new features.\n3. **📚 Improve Documentation**: Help clarify usage, provide examples, or fix typos.\n4. **🔌 Add Framework Adapters**: Extend AMS to support additional agent frameworks.\n\n### Development Setup\n\n```bash\n# Clone the repository\ngit clone https://github.com/sitloboi2012/ams.git\ncd ams\n\n# Create a virtual environment\npython -m venv venv\nsource venv/bin/activate  # On Windows: venv\\Scripts\\activate\n\n# Install development dependencies\npip install -e \".[dev]\"\n\n# Run tests\npytest\n```\n\n### Pull Request Process\n\n1. Fork the repository and create a feature branch.\n2. Add tests for new functionality.\n3. Ensure all tests pass.\n4. Update documentation as needed.\n5. Submit a pull request with a clear description of the changes.\n\nPlease refer to our [CONTRIBUTING.md](CONTRIBUTING.md) file for more details.\n\n## 📚 References\n\nIf you use AMS in your research or project, please consider citing it:\n\n```markdown\n@software{AMS2025,\n  author = {HenryVo},\n  title = {Agent Management Server (AMS): A Kubernetes-inspired Platform for AI Agent Orchestration},\n  year = {2023},\n  url = {https://github.com/sitloboi2012/ams}\n}\n```\n\n### Related Work\n\nFor more information on the concepts and technologies that inspired AMS, refer to:\n\n1. Kubernetes: [Docs](https://kubernetes.io/)\n2. Model Context Protocol: [Docs](https://github.com/modelcontextprotocol)\n3. AutoGen: [Docs](https://github.com/microsoft/autogen)\n4. CrewAI: [Docs](https://github.com/joaomdmoura/crewAI)\n\n## 📄 License\n\nMIT\n\n## ⭐ Support\n\nIf you find AMS servers useful, please consider starring the repository and contributing to improve more!\n\n## 📚 Documentation\n\n- [API Reference](docs/api.md): Details on the REST API endpoints\n- [Framework Adapters](docs/adapters.md): How framework adapters work\n- [Agent Capabilities](docs/capabilities.md): Working with agent capabilities\n- [Supervisor Guide](docs/supervisor.md): How the supervisor orchestrates collaborations\n- [Extension Guide](docs/extending_ams.md): How to extend the system\n\n"
    },
    {
      "name": "black-da-bull/autogen",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/110015660?s=40&v=4",
      "owner": "black-da-bull",
      "repo_name": "autogen",
      "description": "A programming framework for agentic AI. Discord: https://aka.ms/autogen-dc. Roadmap: https://aka.ms/autogen-roadmap",
      "homepage": "https://microsoft.github.io/autogen/",
      "language": "Python",
      "created_at": "2024-04-10T16:54:47Z",
      "updated_at": "2025-03-28T17:51:07Z",
      "topics": [],
      "readme": "<a name=\"readme-top\"></a>\n\n<div align=\"center\">\n<img src=\"https://microsoft.github.io/autogen/0.2/img/ag.svg\" alt=\"AutoGen Logo\" width=\"100\">\n\n[![Twitter](https://img.shields.io/twitter/url/https/twitter.com/cloudposse.svg?style=social&label=Follow%20%40pyautogen)](https://twitter.com/pyautogen)\n[![LinkedIn](https://img.shields.io/badge/LinkedIn-Company?style=flat&logo=linkedin&logoColor=white)](https://www.linkedin.com/company/105812540)\n[![Discord](https://img.shields.io/badge/discord-chat-green?logo=discord)](https://aka.ms/autogen-discord)\n[![Documentation](https://img.shields.io/badge/Documentation-AutoGen-blue?logo=read-the-docs)](https://microsoft.github.io/autogen/)\n[![Blog](https://img.shields.io/badge/Blog-AutoGen-blue?logo=blogger)](https://devblogs.microsoft.com/autogen/)\n\n</div>\n\n<div align=\"center\" style=\"background-color: rgba(255, 235, 59, 0.5); padding: 10px; border-radius: 5px; margin: 20px 0;\">\n  <strong>Important:</strong> This is the official project. We are not affiliated with any fork or startup. See our <a href=\"https://x.com/pyautogen/status/1857264760951296210\">statement</a>.\n</div>\n\n# AutoGen\n\n**AutoGen** is a framework for creating multi-agent AI applications that can act autonomously or work alongside humans.\n\n## Installation\n\nAutoGen requires **Python 3.10 or later**.\n\n```bash\n# Install AgentChat and OpenAI client from Extensions\npip install -U \"autogen-agentchat\" \"autogen-ext[openai]\"\n```\n\nThe current stable version is v0.4. If you are upgrading from AutoGen v0.2, please refer to the [Migration Guide](https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/migration-guide.html) for detailed instructions on how to update your code and configurations.\n\n```bash\n# Install AutoGen Studio for no-code GUI\npip install -U \"autogenstudio\"\n```\n\n## Quickstart\n\n### Hello World\n\nCreate an assistant agent using OpenAI's GPT-4o model. See [other supported models](https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/tutorial/models.html).\n\n```python\nimport asyncio\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n\nasync def main() -> None:\n    model_client = OpenAIChatCompletionClient(model=\"gpt-4o\")\n    agent = AssistantAgent(\"assistant\", model_client=model_client)\n    print(await agent.run(task=\"Say 'Hello World!'\"))\n    await model_client.close()\n\nasyncio.run(main())\n```\n\n### Web Browsing Agent Team\n\nCreate a group chat team with a web surfer agent and a user proxy agent\nfor web browsing tasks. You need to install [playwright](https://playwright.dev/python/docs/library).\n\n```python\n# pip install -U autogen-agentchat autogen-ext[openai,web-surfer]\n# playwright install\nimport asyncio\nfrom autogen_agentchat.agents import UserProxyAgent\nfrom autogen_agentchat.conditions import TextMentionTermination\nfrom autogen_agentchat.teams import RoundRobinGroupChat\nfrom autogen_agentchat.ui import Console\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\nfrom autogen_ext.agents.web_surfer import MultimodalWebSurfer\n\nasync def main() -> None:\n    model_client = OpenAIChatCompletionClient(model=\"gpt-4o\")\n    # The web surfer will open a Chromium browser window to perform web browsing tasks.\n    web_surfer = MultimodalWebSurfer(\"web_surfer\", model_client, headless=False, animate_actions=True)\n    # The user proxy agent is used to get user input after each step of the web surfer.\n    # NOTE: you can skip input by pressing Enter.\n    user_proxy = UserProxyAgent(\"user_proxy\")\n    # The termination condition is set to end the conversation when the user types 'exit'.\n    termination = TextMentionTermination(\"exit\", sources=[\"user_proxy\"])\n    # Web surfer and user proxy take turns in a round-robin fashion.\n    team = RoundRobinGroupChat([web_surfer, user_proxy], termination_condition=termination)\n    try:\n        # Start the team and wait for it to terminate.\n        await Console(team.run_stream(task=\"Find information about AutoGen and write a short summary.\"))\n    finally:\n        await web_surfer.close()\n        await model_client.close()\n\nasyncio.run(main())\n```\n\n### AutoGen Studio\n\nUse AutoGen Studio to prototype and run multi-agent workflows without writing code.\n\n```bash\n# Run AutoGen Studio on http://localhost:8080\nautogenstudio ui --port 8080 --appdir ./my-app\n```\n\n## Why Use AutoGen?\n\n<div align=\"center\">\n  <img src=\"autogen-landing.jpg\" alt=\"AutoGen Landing\" width=\"500\">\n</div>\n\nThe AutoGen ecosystem provides everything you need to create AI agents, especially multi-agent workflows -- framework, developer tools, and applications.\n\nThe _framework_ uses a layered and extensible design. Layers have clearly divided responsibilities and build on top of layers below. This design enables you to use the framework at different levels of abstraction, from high-level APIs to low-level components.\n\n- [Core API](./python/packages/autogen-core/) implements message passing, event-driven agents, and local and distributed runtime for flexibility and power. It also support cross-language support for .NET and Python.\n- [AgentChat API](./python/packages/autogen-agentchat/) implements a simpler but opinionated API for rapid prototyping. This API is built on top of the Core API and is closest to what users of v0.2 are familiar with and supports common multi-agent patterns such as two-agent chat or group chats.\n- [Extensions API](./python/packages/autogen-ext/) enables first- and third-party extensions continuously expanding framework capabilities. It support specific implementation of LLM clients (e.g., OpenAI, AzureOpenAI), and capabilities such as code execution.\n\nThe ecosystem also supports two essential _developer tools_:\n\n<div align=\"center\">\n  <img src=\"https://media.githubusercontent.com/media/microsoft/autogen/refs/heads/main/python/packages/autogen-studio/docs/ags_screen.png\" alt=\"AutoGen Studio Screenshot\" width=\"500\">\n</div>\n\n- [AutoGen Studio](./python/packages/autogen-studio/) provides a no-code GUI for building multi-agent applications.\n- [AutoGen Bench](./python/packages/agbench/) provides a benchmarking suite for evaluating agent performance.\n\nYou can use the AutoGen framework and developer tools to create applications for your domain. For example, [Magentic-One](./python/packages/magentic-one-cli/) is a state-of-the-art multi-agent team built using AgentChat API and Extensions API that can handle a variety of tasks that require web browsing, code execution, and file handling.\n\nWith AutoGen you get to join and contribute to a thriving ecosystem. We host weekly office hours and talks with maintainers and community. We also have a [Discord server](https://aka.ms/autogen-discord) for real-time chat, GitHub Discussions for Q&A, and a blog for tutorials and updates.\n\n## Where to go next?\n\n<div align=\"center\">\n\n|               | [![Python](https://img.shields.io/badge/AutoGen-Python-blue?logo=python&logoColor=white)](./python)                                                                                                                                                                                                                                                                                                                | [![.NET](https://img.shields.io/badge/AutoGen-.NET-green?logo=.net&logoColor=white)](./dotnet) | [![Studio](https://img.shields.io/badge/AutoGen-Studio-purple?logo=visual-studio&logoColor=white)](./python/packages/autogen-studio)                     |\n| ------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | ---------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| Installation  | [![Installation](https://img.shields.io/badge/Install-blue)](https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/installation.html)                                                                                                                                                                                                                                                            | [![Install](https://img.shields.io/badge/Install-green)](https://microsoft.github.io/autogen/dotnet/dev/core/installation.html) | [![Install](https://img.shields.io/badge/Install-purple)](https://microsoft.github.io/autogen/stable/user-guide/autogenstudio-user-guide/installation.html) |\n| Quickstart    | [![Quickstart](https://img.shields.io/badge/Quickstart-blue)](https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/quickstart.html#)                                                                                                                                                                                                                                                            | [![Quickstart](https://img.shields.io/badge/Quickstart-green)](https://microsoft.github.io/autogen/dotnet/dev/core/index.html) | [![Usage](https://img.shields.io/badge/Quickstart-purple)](https://microsoft.github.io/autogen/stable/user-guide/autogenstudio-user-guide/usage.html#)        |\n| Tutorial      | [![Tutorial](https://img.shields.io/badge/Tutorial-blue)](https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/tutorial/index.html)                                                                                                                                                                                                                                                            | [![Tutorial](https://img.shields.io/badge/Tutorial-green)](https://microsoft.github.io/autogen/dotnet/dev/core/tutorial.html) | [![Usage](https://img.shields.io/badge/Tutorial-purple)](https://microsoft.github.io/autogen/stable/user-guide/autogenstudio-user-guide/usage.html#)        |\n| API Reference | [![API](https://img.shields.io/badge/Docs-blue)](https://microsoft.github.io/autogen/stable/reference/index.html#)                                                                                                                                                                                                                                                                                                    | [![API](https://img.shields.io/badge/Docs-green)](https://microsoft.github.io/autogen/dotnet/dev/api/Microsoft.AutoGen.Contracts.html) | [![API](https://img.shields.io/badge/Docs-purple)](https://microsoft.github.io/autogen/stable/user-guide/autogenstudio-user-guide/usage.html)               |\n| Packages      | [![PyPi autogen-core](https://img.shields.io/badge/PyPi-autogen--core-blue?logo=pypi)](https://pypi.org/project/autogen-core/) <br> [![PyPi autogen-agentchat](https://img.shields.io/badge/PyPi-autogen--agentchat-blue?logo=pypi)](https://pypi.org/project/autogen-agentchat/) <br> [![PyPi autogen-ext](https://img.shields.io/badge/PyPi-autogen--ext-blue?logo=pypi)](https://pypi.org/project/autogen-ext/) | [![NuGet Contracts](https://img.shields.io/badge/NuGet-Contracts-green?logo=nuget)](https://www.nuget.org/packages/Microsoft.AutoGen.Contracts/) <br> [![NuGet Core](https://img.shields.io/badge/NuGet-Core-green?logo=nuget)](https://www.nuget.org/packages/Microsoft.AutoGen.Core/) <br> [![NuGet Core.Grpc](https://img.shields.io/badge/NuGet-Core.Grpc-green?logo=nuget)](https://www.nuget.org/packages/Microsoft.AutoGen.Core.Grpc/) <br> [![NuGet RuntimeGateway.Grpc](https://img.shields.io/badge/NuGet-RuntimeGateway.Grpc-green?logo=nuget)](https://www.nuget.org/packages/Microsoft.AutoGen.RuntimeGateway.Grpc/) | [![PyPi autogenstudio](https://img.shields.io/badge/PyPi-autogenstudio-purple?logo=pypi)](https://pypi.org/project/autogenstudio/)                       |\n\n</div>\n\n\nInterested in contributing? See [CONTRIBUTING.md](./CONTRIBUTING.md) for guidelines on how to get started. We welcome contributions of all kinds, including bug fixes, new features, and documentation improvements. Join our community and help us make AutoGen better!\n\nHave questions? Check out our [Frequently Asked Questions (FAQ)](./FAQ.md) for answers to common queries. If you don't find what you're looking for, feel free to ask in our [GitHub Discussions](https://github.com/microsoft/autogen/discussions) or join our [Discord server](https://aka.ms/autogen-discord) for real-time support. You can also read our [blog](https://devblogs.microsoft.com/autogen/) for updates.\n\n## Legal Notices\n\nMicrosoft and any contributors grant you a license to the Microsoft documentation and other content\nin this repository under the [Creative Commons Attribution 4.0 International Public License](https://creativecommons.org/licenses/by/4.0/legalcode),\nsee the [LICENSE](LICENSE) file, and grant you a license to any code in the repository under the [MIT License](https://opensource.org/licenses/MIT), see the\n[LICENSE-CODE](LICENSE-CODE) file.\n\nMicrosoft, Windows, Microsoft Azure, and/or other Microsoft products and services referenced in the documentation\nmay be either trademarks or registered trademarks of Microsoft in the United States and/or other countries.\nThe licenses for this project do not grant you rights to use any Microsoft names, logos, or trademarks.\nMicrosoft's general trademark guidelines can be found at <http://go.microsoft.com/fwlink/?LinkID=254653>.\n\nPrivacy information can be found at <https://go.microsoft.com/fwlink/?LinkId=521839>\n\nMicrosoft and any contributors reserve all other rights, whether under their respective copyrights, patents,\nor trademarks, whether by implication, estoppel, or otherwise.\n\n<p align=\"right\" style=\"font-size: 14px; color: #555; margin-top: 20px;\">\n  <a href=\"#readme-top\" style=\"text-decoration: none; color: blue; font-weight: bold;\">\n    ↑ Back to Top ↑\n  </a>\n</p>\n"
    },
    {
      "name": "isaccanedo/awesome-llm-apps",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/32867124?s=40&v=4",
      "owner": "isaccanedo",
      "repo_name": "awesome-llm-apps",
      "description": ":star: Collection of awesome LLM apps with AI Agents and RAG using OpenAI, Anthropic, Gemini and opensource models",
      "homepage": "https://www.theunwindai.com/",
      "language": "Python",
      "created_at": "2025-03-27T12:07:50Z",
      "updated_at": "2025-03-27T12:11:47Z",
      "topics": [
        "gemini",
        "llm",
        "openapi",
        "opensource",
        "python",
        "rag"
      ],
      "readme": "<p align=\"center\">\n  <a href=\"http://www.theunwindai.com\">\n    <img src=\"docs/banner/unwind_black.png\" width=\"900px\" alt=\"Unwind AI\">\n  </a>\n</p>\n\n<p align=\"center\">\n  <a href=\"https://www.linkedin.com/in/shubhamsaboo/\">\n    <img src=\"https://img.shields.io/badge/-Follow%20Shubham%20Saboo-blue?logo=linkedin&style=flat-square\" alt=\"LinkedIn\">\n  </a>\n  <a href=\"https://twitter.com/Saboo_Shubham_\">\n    <img src=\"https://img.shields.io/twitter/follow/Shubham_Saboo\" alt=\"Twitter\">\n  </a>\n</p>\n\n<hr/>\n\n# 🌟 Awesome LLM Apps\n\nA curated collection of awesome LLM apps built with RAG and AI agents. This repository features LLM apps that use models from OpenAI, Anthropic, Google, and open-source models like DeepSeek, Qwen or Llama that you can run locally on your computer.\n\n<p align=\"center\">\n  <a href=\"https://trendshift.io/repositories/9876\" target=\"_blank\">\n    <img src=\"https://trendshift.io/api/badge/repositories/9876\" alt=\"Shubhamsaboo%2Fawesome-llm-apps | Trendshift\" style=\"width: 250px; height: 55px;\" />\n  </a>\n</p>\n\n## 🤔 Why Awesome LLM Apps?\n\n- 💡 Discover practical and creative ways LLMs can be applied across different domains, from code repositories to email inboxes and more.\n- 🔥 Explore apps that combine LLMs from OpenAI, Anthropic, Gemini, and open-source alternatives with RAG and AI Agents.\n- 🎓 Learn from well-documented projects and contribute to the growing open-source ecosystem of LLM-powered applications.\n\n## 📂 Featured AI Projects\n\n### AI Agents\n- [💼 AI Customer Support Agent](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/ai_customer_support_agent)\n- [📈 AI Investment Agent](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/ai_investment_agent)\n- [👨‍⚖️ AI Legal Agent Team](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/ai_legal_agent_team)\n- [💼 AI Recruitment Agent Team](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/ai_recruitment_agent_team)\n- [👨‍💼 AI Services Agency](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/ai_services_agency)\n- [🧲 AI Competitor Intelligence Agent Team](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/ai_competitor_intelligence_agent_team)\n- [🏋️‍♂️ AI Health & Fitness Planner Agent](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/ai_health_fitness_agent)\n- [📈 AI Startup Trend Analysis Agent](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/ai_startup_trend_analysis_agent)\n- [🗞️ AI Journalist Agent](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/ai_journalist_agent)\n- [💲 AI Finance Agent Team](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/ai_finance_agent_team)\n- [🧲 AI Competitor Intelligence Agent Team](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/ai_competitor_intelligence_agent_team)\n- [🎯 AI Lead Generation Agent](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/ai_lead_generation_agent)\n- [💰 AI Personal Finance Agent](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/ai_personal_finance_agent)\n- [🩻 AI Medical Scan Diagnosis Agent](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/ai_medical_imaging_agent)\n- [👨‍🏫 AI Teaching Agent Team](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/ai_teaching_agent_team)\n- [🛫 AI Travel Agent](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/ai_travel_agent)\n- [🎬 AI Movie Production Agent](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/ai_movie_production_agent)\n- [📰 Multi-Agent AI Researcher](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/multi_agent_researcher)\n- [💻 Multimodal AI Coding Agent Team with o3-mini and Gemini](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/ai_coding_agent_o3-mini)\n- [📑 AI Meeting Agent](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/ai_meeting_agent)\n- [♜ AI Chess Agent Game](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/ai_chess_agent)\n- [🏠 AI Real Estate Agent](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/ai_real_estate_agent)\n- [🌐 Local News Agent OpenAI Swarm](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/local_news_agent_openai_swarm)\n- [📊 AI Finance Agent with xAI Grok](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/xai_finance_agent)\n- [🎮 AI 3D PyGame Visualizer with DeepSeek R1](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/ai_3dpygame_r1)\n- [🧠 AI Reasoning Agent](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/ai_reasoning_agent)\n- [🧬 Multimodal AI Agent](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/multimodal_ai_agent)\n\n### RAG (Retrieval Augmented Generation)\n- [🔍 Autonomous RAG](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/rag_tutorials/autonomous_rag)\n- [🔗 Agentic RAG](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/rag_tutorials/agentic_rag)\n- [🤔 Agentic RAG with Gemini Flash Thinking](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/rag_tutorials/gemini_agentic_rag)\n- [🐋 Deepseek Local RAG Reasoning Agent](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/rag_tutorials/deepseek_local_rag_agent)\n- [🔄 Llama3.1 Local RAG](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/rag_tutorials/llama3.1_local_rag)\n- [🧩 RAG-as-a-Service](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/rag_tutorials/rag-as-a-service)\n- [🦙 Local RAG Agent](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/rag_tutorials/local_rag_agent)\n- [👀 RAG App with Hybrid Search](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/rag_tutorials/hybrid_search_rag)\n- [🖥️ Local RAG App with Hybrid Search](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/rag_tutorials/local_hybrid_search_rag)\n- [📠 RAG Agent with Database Routing](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/rag_tutorials/rag_database_routing)\n- [🔄 Corrective RAG Agent](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/rag_tutorials/corrective_rag)\n\n### MCP AI Agents\n- [🐙 MCP GitHub Agent](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/mcp_ai_agents/github_mcp_agent)\n\n### LLM Apps with Memory\n- [💾 AI Arxiv Agent with Memory](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/llm_apps_with_memory_tutorials/ai_arxiv_agent_memory)\n- [📝 LLM App with Personalized Memory](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/llm_apps_with_memory_tutorials/llm_app_personalized_memory)\n- [🛩️ AI Travel Agent with Memory](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/llm_apps_with_memory_tutorials/ai_travel_agent_memory)\n- [🗄️ Local ChatGPT with Memory](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/llm_apps_with_memory_tutorials/local_chatgpt_with_memory)\n\n### Chat with X\n- [💬 Chat with GitHub Repo](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/chat_with_X_tutorials/chat_with_github)\n- [📨 Chat with Gmail](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/chat_with_X_tutorials/chat_with_gmail)\n- [📄 Chat with PDF](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/chat_with_X_tutorials/chat_with_pdf)\n- [📚 Chat with Research Papers](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/chat_with_X_tutorials/chat_with_research_papers)\n- [📝 Chat with Substack Newsletter](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/chat_with_X_tutorials/chat_with_substack)\n- [📽️ Chat with YouTube Videos](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/chat_with_X_tutorials/chat_with_youtube_videos)\n\n### LLM Finetuning\n- [🌐 Llama3.2 Finetuning](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/llm_finetuning_tutorials/llama3.2_finetuning)\n\n### Advanced Tools and Frameworks\n- [🧪 Gemini Multimodal Chatbot](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/advanced_tools_frameworks/gemini_multimodal_chatbot)\n- [🔄 Mixture of Agents](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/advanced_tools_frameworks/mixture_of_agents)\n- [🌐 MultiLLM Chat Playground](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/advanced_tools_frameworks/multillm_chat_playground)\n- [🔗 LLM Router App](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/advanced_tools_frameworks/llm_router_app)\n- [💬 Local ChatGPT Clone](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/advanced_tools_frameworks/local_chatgpt_clone)\n- [🌍 Web Scraping AI Agent](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/advanced_tools_frameworks/web_scrapping_ai_agent)\n- [🔍 Web Search AI Assistant](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/advanced_tools_frameworks/web_search_ai_assistant)\n- [🧪 Cursor AI Experiments](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/advanced_tools_frameworks/cursor_ai_experiments)\n\n## 🚀 Getting Started\n\n1. **Clone the repository** \n\n    ```bash \n    git clone https://github.com/Shubhamsaboo/awesome-llm-apps.git \n    ```\n\n2. **Navigate to the desired project directory**\n\n    ```bash \n    cd awesome-llm-apps/chat_with_X_tutorials/chat_with_gmail\n    ```\n\n3. **Install the required dependencies**\n\n    ```bash\n    pip install -r requirements.txt\n    ```\n\n4. **Follow the project-specific instructions** in each project's `README.md` file to set up and run the app.\n\n## 🤝 Contributing to Open Source\n\nContributions are welcome! If you have any ideas, improvements, or new apps to add, please create a new [GitHub Issue](https://github.com/Shubhamsaboo/awesome-llm-apps/issues) or submit a pull request. Make sure to follow the existing project structure and include a detailed `README.md` for each new app.\n\n### Thank You, Community, for the Support! 🙏\n\n[![Star History Chart](https://api.star-history.com/svg?repos=Shubhamsaboo/awesome-llm-apps&type=Date)](https://star-history.com/#Shubhamsaboo/awesome-llm-apps&Date)\n\n🌟 **Don’t miss out on future updates! Star the repo now and be the first to know about new and exciting LLM apps with RAG and AI Agents.**\n"
    },
    {
      "name": "kohei3110/ai-tour-2025-agent-demo",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/38392404?s=40&v=4",
      "owner": "kohei3110",
      "repo_name": "ai-tour-2025-agent-demo",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2025-03-14T01:28:07Z",
      "updated_at": "2025-04-07T04:17:50Z",
      "topics": [],
      "readme": "# AI Tour 2025 Agent Demo\n\n## プロジェクト概要\n\nAI Tour 2025 Agent Demoは、Azure OpenAIを活用した補助金情報案内と申請書生成支援のためのAIエージェントシステムです。ユーザーは自然言語で補助金に関する質問を行い、AIエージェントが適切な補助金情報を検索・提供します。また、補助金申請に必要な申請書の作成支援も行います。\n\n## 主な機能\n\n1. **補助金情報検索・案内**：ユーザーの質問に応じて、適切な補助金情報を提供\n2. **申請書テンプレート生成**：補助金情報を基に申請書テンプレートを自動生成\n3. **AI拡張テンプレート生成**：ビジネス概要を基にカスタマイズされた申請書を生成\n4. **一般的な質問応答**：補助金以外の一般的な質問にも対応\n\n## 技術スタック\n\n### バックエンド\n- Python 3.12\n- FastAPI\n- Azure AI Projects (Azure OpenAI)\n- 自動テストフレームワーク (pytest)\n\n### フロントエンド\n- React\n- JavaScript/CSS\n\n## アーキテクチャ\n\n本システムは以下のコンポーネントで構成されています：\n\n### バックエンド層\n\n1. **コントローラー層**\n   - APIエンドポイントを提供\n   - リクエスト処理とレスポンス返却\n\n2. **サービス層**\n   - AssistantManagerService: AIエージェントとの対話管理\n\n3. **ツール層**\n   - Swagger Spec Tool: OpenAPIを使用した補助金情報検索\n   - Application Document Generator: 申請書テンプレート生成\n   - Bing Grounding Tool: 補助金に関する最新情報検索（計画中）\n\n4. **モデル層**\n   - データモデルとバリデーション（Pydantic）\n\n### フロントエンド層\n\n- ユーザーインターフェース（チャットUI）\n- 補助金申請書作成UI\n\n## API仕様\n\n### 1. ヘルスチェックエンドポイント\n- **エンドポイント**: `/api/health`\n- **メソッド**: GET\n- **説明**: アプリケーションの稼働状態を確認\n- **レスポンス**: `{\"status\": \"ok\"}`\n\n### 2. チャットエンドポイント\n- **エンドポイント**: `/api/chat`\n- **メソッド**: POST\n- **説明**: AIエージェントに対して補助金に関する質問を送信\n- **リクエスト本文**: \n  ```json\n  {\n    \"message\": \"中小企業向けの最新の補助金について教えてください\"\n  }\n  ```\n- **レスポンス**: AIエージェントからのテキスト応答\n\n### 3. 申請書テンプレート生成エンドポイント\n- **エンドポイント**: `/api/application/generate`\n- **メソッド**: POST\n- **説明**: 補助金情報を基にした申請書テンプレートを生成\n- **リクエスト本文**:\n  ```json\n  {\n    \"subsidy_info\": {\n      \"title\": \"中小企業デジタル化支援補助金\",\n      \"acceptance_start_datetime\": \"2025-04-01T09:00:00\",\n      \"acceptance_end_datetime\": \"2025-05-31T17:00:00\",\n      \"target_area_search\": \"全国\",\n      \"subsidy_max_limit\": 5000000,\n      \"target_number_of_employees\": \"5~50人\"\n    },\n    \"business_description\": \"IoTデバイスを活用した農業向けシステム開発\"\n  }\n  ```\n- **レスポンス**: \n  ```json\n  {\n    \"template\": \"申請書テンプレートのテキスト\",\n    \"ai_enhanced\": true\n  }\n  ```\n\n### 4. テキスト生成エンドポイント\n- **エンドポイント**: `/api/generate`\n- **メソッド**: POST\n- **説明**: 任意のプロンプトに対してAIがテキストを生成\n- **リクエスト本文**:\n  ```json\n  {\n    \"prompt\": \"補助金申請書の書き方のポイントを5つ教えてください\"\n  }\n  ```\n- **レスポンス**:\n  ```json\n  {\n    \"generated_text\": \"生成されたテキスト\",\n    \"success\": true\n  }\n  ```\n\n## 開発環境セットアップ\n\n### 前提条件\n- Python 3.12以上\n- Node.js 18以上\n- Azure サブスクリプション（Azure OpenAI利用のため）\n\n### バックエンドのセットアップ\n```bash\n# 依存関係のインストール\ncd app/backend\npoetry install\n\n# 開発サーバーの起動\npoetry run uvicorn startup:app --reload\n```\n\n### フロントエンドのセットアップ\n```bash\n# 依存関係のインストール\ncd app/frontend\nnpm install\n\n# 開発サーバーの起動\nnpm start\n```\n\n## テスト\n\n本プロジェクトはテスト駆動開発(TDD)の原則に従って開発されています。\n\n```bash\n# バックエンドのテスト実行\ncd app/backend\npoetry run pytest\n\n# カバレッジレポート付きのテスト実行\npoetry run pytest --cov=. --cov-report=html\n```\n\n## デプロイ\n\n### Dockerを使用したデプロイ\n```bash\n# バックエンドのビルド\ncd app/backend\ndocker build -t ai-tour-agent-backend .\n\n# フロントエンドのビルド\ncd app/frontend\nnpm run build\n```\n\n## ライセンス\n\nCopyright (c) Microsoft Corporation. All rights reserved."
    },
    {
      "name": "verwec/03_2025_ai_workshop",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/6060259?s=40&v=4",
      "owner": "verwec",
      "repo_name": "03_2025_ai_workshop",
      "description": null,
      "homepage": null,
      "language": "HTML",
      "created_at": "2025-03-25T10:05:22Z",
      "updated_at": "2025-03-26T13:09:50Z",
      "topics": [],
      "readme": "# Chat Application with Context-Aware Responses\n\nThis is a web-based chat application that provides context-aware responses using OpenAI's GPT-4 model. The application consists of a Flask backend and a modern HTML/JavaScript frontend.\n\n## Features\n\n- Real-time chat interface with a clean, modern design\n- Context-aware responses using vector similarity search\n- Support for chat history\n- Responsive design that works on both desktop and mobile\n- Loading indicators for better user experience\n- Error handling and graceful fallbacks\n\n## Prerequisites\n\n- Python 3.x\n- OpenAI API key\n- Required Python packages (listed in requirements.txt)\n\n## Setup\n\n1. Clone the repository:\n```bash\ngit clone <repository-url>\ncd chat-live\n```\n\n2. Install the required packages:\n```bash\npip install -r requirements.txt\n```\n\n3. Create a `.env` file in the root directory and add your OpenAI API key:\n```\nOPENAI_API_KEY=your_api_key_here\n```\n\n4. Start the Flask server:\n```bash\npython app.py\n```\n\nThe server will start on `http://localhost:5001`\n\n5. Open `index.html` in your web browser to access the chat interface.\n\n## Project Structure\n\n- `app.py`: Flask backend server with chat endpoint\n- `index.html`: Frontend chat interface\n- `vectorstore_example.py`: Vector store implementation for context retrieval\n- `requirements.txt`: Python package dependencies\n\n## API Endpoints\n\n### POST /chat\n\nSends a message to the chat endpoint and receives a context-aware response.\n\nRequest body:\n```json\n{\n    \"query\": \"Your message here\"\n}\n```\n\nResponse:\n```json\n{\n    \"message\": \"AI response here\"\n}\n```\n\n## Usage\n\n1. Type your message in the input field\n2. Press Enter or click the Send button\n3. Wait for the AI's response\n4. The conversation history will be displayed in the chat window\n\n## Technical Details\n\n- The application uses Flask for the backend server\n- CORS is enabled to allow frontend-backend communication\n- The chat interface is built with vanilla JavaScript and modern CSS\n- Context retrieval is implemented using vector similarity search\n- OpenAI's GPT-4 model is used for generating responses\n\n## Error Handling\n\nThe application includes error handling for:\n- Network connectivity issues\n- API request failures\n- Invalid responses\n- Empty messages\n\n## Contributing\n\nFeel free to submit issues and enhancement requests! "
    },
    {
      "name": "skillrepos/ai-agents",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/82792046?s=40&v=4",
      "owner": "skillrepos",
      "repo_name": "ai-agents",
      "description": "Repo for Understanding AI Agents workshop",
      "homepage": null,
      "language": "Python",
      "created_at": "2025-03-23T07:43:04Z",
      "updated_at": "2025-03-31T15:46:48Z",
      "topics": [],
      "readme": "# Understanding AI Agents\n\nRepository for AI Agents hands-on workshop\n\nThese instructions will guide you through configuring a GitHub Codespaces environment that you can use to run the course labs. \n\n**1. Click on the button below to start a new codespace from this repository.**\n\nClick here ➡️  [![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://codespaces.new/skillrepos/ai-agents?quickstart=1)\n\n**2. Then click on the option to create a new codespace.**\n\n![Creating new codespace from button](./images/aa1.png?raw=true \"Creating new codespace from button\")\n\nThis will run for a long time (10 or more minutes) while it gets everything ready.\n\nAfter the initial startup, it will run a script to setup the python environment, install needed python pieces, install Ollama, and then download the models we will use. This will take several more minutes to run. It will look like this while this is running.\n\n![Final prep](./images/aa39.png?raw=true \"Final prep\")\n\nThe codespace is ready to use when you see a prompt like the one shown below in its terminal.\n\n![Ready to use](./images/aa2.png?raw=true \"Ready to use\")\n\n\n**3. Open up the *labs.md* file so you can follow along with the labs.**\nYou can either open it in a separate browser instance or open it in the codespace. If you open it in the codespace, make sure to *Open Preview* so you can see it in Markdown form as intended.\n![Opening labs](./images/aa3.png?raw=true \"Opening labs\")\n\n**Now, you are ready for the labs!**\n\n**4. (Optional, but recommended) Change your codespace's default timeout from 30 minutes to longer (60 for half-day sessions, 90 for deep dive sessions).**\nTo do this, when logged in to GitHub, go to https://github.com/settings/codespaces and scroll down on that page until you see the *Default idle timeout* section. Adjust the value as desired.\n\n![Changing codespace idle timeout value](./images/aa4.png?raw=true \"Changing codespace idle timeout value\")\n\n**NOTE: If your codespace times out and you need to reopen it**\n\n1. Go to https://github.com/your_github_userid/codespaces\n2. Find the codespace in the list, right-click, and select *Open in browser*\n3. Run the *ollama serve &* command to restart Ollama\n```\nollama serve &\n```\n\n<br/><br/>\n\n\n"
    },
    {
      "name": "vortezwohl/experiment-03-22-2025",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/117743023?s=40&v=4",
      "owner": "vortezwohl",
      "repo_name": "experiment-03-22-2025",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2025-03-21T03:34:18Z",
      "updated_at": "2025-04-06T15:30:26Z",
      "topics": [],
      "readme": "# Experiment\n\nThe experimental results demonstrate that the `autono` framework significantly outperforms `autogen` and `langchain` in handling tasks of varying complexity, especially in multi-step tasks with possible failures.\n\n| Framework   | Version | Model                                      | one-step-task | multi-step-task | multi-step-task-with-possible-failure |\n| ------ | --- |----------------------------------------- | ------------- | --------------- | ---------------------------------------- |\n| `autono` | `1.0.0` |gpt-4o-mini<br>qwen-plus<br>deepseek-v3 | 96.7%</br>100%</br>100% | 100%</br>96.7%</br>100% | 76.7%</br>93.3%</br>93.3% |\n| `autogen` | `0.4.9.2` |gpt-4o-mini<br>qwen-plus<br>deepseek-v3 |90%</br>90%</br>N/A | 53.3%</br>0%</br>N/A | 3.3%</br>3.3%</br>N/A |\n| `langchain` | `0.3.21` |gpt-4o-mini<br>qwen-plus<br>deepseek-v3 | 73.3%</br>73.3%</br>76.7% | 13.3%</br>13.3%</br>13.3% | 10%</br>13.3%</br>6.7% |\n\n- `one-step-task`: Tasks that can be completed with a single tool call.\n- `multi-step-task`: Tasks that require multiple tool calls to complete, with no possibility of tool failure.\n- `multi-step-task-with-possible-failure`: Tasks that require multiple tool calls to complete, where tools may fail, requiring the agent to retry and correct errors.\n\n> The deepseek-v3 model is not supported by `autogen-agentchat==0.4.9.2`.\n"
    },
    {
      "name": "dheerapat/autogen-agent-demo",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/61280196?s=40&v=4",
      "owner": "dheerapat",
      "repo_name": "autogen-agent-demo",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2025-03-20T16:30:06Z",
      "updated_at": "2025-04-18T09:32:48Z",
      "topics": [],
      "readme": "# agentic ai demo using custom tools and mcp\n1. clone this repository\n2. `cd` into the repository\n3. in `pubmed-mcp-demo.py` file you can edit `task` to whatever you want (I intended to use it as a healthcare assistant)\n4. run `uv run pubmed-mcp-demo.py` to try pubmed agent\n5. agent will automatically use pubmed mcp to retrieve abstract from pubmed and summarized\n"
    },
    {
      "name": "Azure/multiagent-frameworks",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/6844498?s=40&v=4",
      "owner": "Azure",
      "repo_name": "multiagent-frameworks",
      "description": null,
      "homepage": "",
      "language": "Jupyter Notebook",
      "created_at": "2025-01-29T11:23:33Z",
      "updated_at": "2025-04-06T22:49:18Z",
      "topics": [],
      "readme": "# Multi-Agent Frameworks\n\nMulti-Agent frameworks aide in developing powerful applications with the goal to improve accuracy, reduce hallucinations, handle complex user tasks by integrating Large Language Models into enterprise applications. \n\n\nThis repository contains a collection of multi-agent frameworks that can be used to simulate and study multi-agent systems. The frameworks are implemented in Python and are designed to be easy to use and extend. The frameworks compared in this repository are:\n\n- [Autogen v0.4](https://www.microsoft.com/en-us/research/blog/autogen-v0-4-reimagining-the-foundation-of-agentic-ai-for-scale-extensibility-and-robustness/)\n- [Semantic Kernel](https://learn.microsoft.com/en-us/semantic-kernel/overview/)\n- [Azure AI Agent Service](https://learn.microsoft.com/en-us/azure/ai-services/agents/overview?view=azure-python-preview)\n- [OpenAI Agents SDK](https://openai.github.io/openai-agents-python/)\n\nThis repo is designed to easily understand different frameworks and help developers with nuances and compare the different approaches. \n\nA simple scenario is used to demonstrate the capabilities of the frameworks. The scenario is a simple conversation between two agents for a fintech company helping users with Account and Transaction related questions.\nThese scenarios are addressed:\n\n- Concurrency \n    \n    Handling simulatneous conversations between users and agents.\n\n- Agent routing\n\n    Routing the user task to the correct agent based on the user query and enabling the agents to communicate with each other.\n\n- LLM Swapping\n\n    Swap the LLM models used by the agents based on task criticalility. For example, using a more powerful model for router and a simpler model for agents.\n\n- Chat History and Memory Management\n\n    Managing the chat history and memory of the agents and maintain chat history across multiple users and agents. \n\n- Function Calling\n\n    Implementing function calling across agents and router.\n\n- Observability\n\n    Monitoring and tracing conversations between the agents and the users.\n\n- Authentication\n\n    Authenticating users with Microsoft Entra. \n\n\nEach framework is implemented in a separate folder. The folder contains the implementation of the framework and the scenario. Please refer to the README.md in each folder for more details on how to run the scenario using the framework.\n\n\n## Common Installation Steps\n\n- Azure OpenAI Service or AI Foundry Access is required. \n- AZ CLI is required and user should be logged in with az login. \n- Python 3.11 or higher is installed. \n- Clone the repository\n\n    ```bash or cmd\n    git clone https://github.com/Azure/multiagent-frameworks.git\n\n- Python Steps:\n    \n    ```bash or cmd\n    python -m venv venv\n\n    # cmd \n    venv\\Scripts\\activate\n\n    # bash\n    source venv/bin/activate\n\n    pip install -r requirements.txt\n\n\n## Multi Agent Scenario \n\nThe a multi agent scenario, the key concept is that the agents needs to be aware of other agents' tools and capabilities. The capabilities are defined in the system message for each agents and tools using python functions are made avaialble to each agent. \nThe system message for the router agent will typically contain the list of agents and their capabilities. The system message for the agents will contain the list of tools and the capabilities of the agents.\nThe router agent will also have instructions on which agent to route the user question to based on the user question.\n\nNow given a user questions, the router agent will decompose the questions and assign tasks to the agents. The agents will then use the tools to answer the questions and submit a resposne back to a common location which could be a chat history, and pub sub topic, or a thread in the case of Assistant API. \nThen a Final responder agent will inspect the chat history and determine whether the user questions has been answered or not. If the user question has been answered, the final responder will return the final answer to the user. Otherwise it will submit the question to the router agent for further processing.\n\nThe scenario that is implemented in this repository is a simple 2 agent and a router agent use case, who can answer user questions about accounts and transactions for a fictious company called Transify which is a Financial Services company.\n\nThis scenario is implemented in each of the frameworks and the implementation details are provided in the README.md in each of the framework folders.\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n"
    },
    {
      "name": "FranklineMisango/Quant_Research_Assistant",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/95913228?s=40&v=4",
      "owner": "FranklineMisango",
      "repo_name": "Quant_Research_Assistant",
      "description": "A Quantitative Research bot that is powered by RAGentic Architecture provided by Autogen 2.0",
      "homepage": "",
      "language": "Jupyter Notebook",
      "created_at": "2025-02-18T12:58:52Z",
      "updated_at": "2025-04-21T11:08:55Z",
      "topics": [],
      "readme": "# Quant Research Assistant 🚀\n\nA Quantitative Research bot powered by RAGentic Architecture utilizing Autogen 2.0.\n\n## Basic Autogen Architecture 🧠\n\n![Basic Autogen Architecture](images/ms_autogen_diagram.jpg)\n\n## Learn More 📚\n\nFor more information, visit the [Autogen Blog](https://microsoft.github.io/autogen/0.2/blog/2023/10/18/RetrieveChat/)."
    },
    {
      "name": "flosrv/Awesome-LLM-Apps-Shubham-Saboo",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/139999976?s=40&v=4",
      "owner": "flosrv",
      "repo_name": "Awesome-LLM-Apps-Shubham-Saboo",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2025-03-16T08:51:33Z",
      "updated_at": "2025-04-22T04:52:22Z",
      "topics": [],
      "readme": "<p align=\"center\">\n  <a href=\"http://www.theunwindai.com\">\n    <img src=\"docs/banner/unwind_black.png\" width=\"900px\" alt=\"Unwind AI\">\n  </a>\n</p>\n\n<p align=\"center\">\n  <a href=\"https://www.linkedin.com/in/shubhamsaboo/\">\n    <img src=\"https://img.shields.io/badge/-Follow%20Shubham%20Saboo-blue?logo=linkedin&style=flat-square\" alt=\"LinkedIn\">\n  </a>\n  <a href=\"https://twitter.com/Saboo_Shubham_\">\n    <img src=\"https://img.shields.io/twitter/follow/Shubham_Saboo\" alt=\"Twitter\">\n  </a>\n</p>\n\n<hr/>\n\n# 🌟 Awesome LLM Apps\n\nA curated collection of awesome LLM apps built with RAG and AI agents. This repository features LLM apps that use models from OpenAI, Anthropic, Google, and open-source models like DeepSeek, Qwen or Llama that you can run locally on your computer.\n\n<p align=\"center\">\n  <a href=\"https://trendshift.io/repositories/9876\" target=\"_blank\">\n    <img src=\"https://trendshift.io/api/badge/repositories/9876\" alt=\"Shubhamsaboo%2Fawesome-llm-apps | Trendshift\" style=\"width: 250px; height: 55px;\" />\n  </a>\n</p>\n\n## 🤔 Why Awesome LLM Apps?\n\n- 💡 Discover practical and creative ways LLMs can be applied across different domains, from code repositories to email inboxes and more.\n- 🔥 Explore apps that combine LLMs from OpenAI, Anthropic, Gemini, and open-source alternatives with RAG and AI Agents.\n- 🎓 Learn from well-documented projects and contribute to the growing open-source ecosystem of LLM-powered applications.\n\n## 📂 Featured AI Projects\n\n### AI Agents\n- [💼 AI Customer Support Agent](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/ai_customer_support_agent)\n- [📈 AI Investment Agent](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/ai_investment_agent)\n- [👨‍⚖️ AI Legal Agent Team](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/ai_legal_agent_team)\n- [💼 AI Recruitment Agent Team](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/ai_recruitment_agent_team)\n- [👨‍💼 AI Services Agency](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/ai_services_agency)\n- [🧲 AI Competitor Intelligence Agent Team](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/ai_competitor_intelligence_agent_team)\n- [🏋️‍♂️ AI Health & Fitness Planner Agent](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/ai_health_fitness_agent)\n- [📈 AI Startup Trend Analysis Agent](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/ai_startup_trend_analysis_agent)\n- [🗞️ AI Journalist Agent](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/ai_journalist_agent)\n- [💲 AI Finance Agent Team](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/ai_finance_agent_team)\n- [🧲 AI Competitor Intelligence Agent Team](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/ai_competitor_intelligence_agent_team)\n- [🎯 AI Lead Generation Agent](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/ai_lead_generation_agent)\n- [💰 AI Personal Finance Agent](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/ai_personal_finance_agent)\n- [🩻 AI Medical Scan Diagnosis Agent](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/ai_medical_imaging_agent)\n- [👨‍🏫 AI Teaching Agent Team](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/ai_teaching_agent_team)\n- [🛫 AI Travel Agent](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/ai_travel_agent)\n- [🎬 AI Movie Production Agent](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/ai_movie_production_agent)\n- [📰 Multi-Agent AI Researcher](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/multi_agent_researcher)\n- [💻 Multimodal AI Coding Agent Team with o3-mini and Gemini](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/ai_coding_agent_o3-mini)\n- [📑 AI Meeting Agent](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/ai_meeting_agent)\n- [♜ AI Chess Agent Game](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/ai_chess_agent)\n- [🏠 AI Real Estate Agent](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/ai_real_estate_agent)\n- [🌐 Local News Agent OpenAI Swarm](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/local_news_agent_openai_swarm)\n- [📊 AI Finance Agent with xAI Grok](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/xai_finance_agent)\n- [🎮 AI 3D PyGame Visualizer with DeepSeek R1](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/ai_3dpygame_r1)\n- [🧠 AI Reasoning Agent](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/ai_reasoning_agent)\n- [🧬 Multimodal AI Agent](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/multimodal_ai_agent)\n\n### RAG (Retrieval Augmented Generation)\n- [🔍 Autonomous RAG](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/rag_tutorials/autonomous_rag)\n- [🔗 Agentic RAG](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/rag_tutorials/agentic_rag)\n- [🤔 Agentic RAG with Gemini Flash Thinking](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/rag_tutorials/gemini_agentic_rag)\n- [🐋 Deepseek Local RAG Reasoning Agent](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/rag_tutorials/deepseek_local_rag_agent)\n- [🔄 Llama3.1 Local RAG](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/rag_tutorials/llama3.1_local_rag)\n- [🧩 RAG-as-a-Service](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/rag_tutorials/rag-as-a-service)\n- [🦙 Local RAG Agent](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/rag_tutorials/local_rag_agent)\n- [👀 RAG App with Hybrid Search](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/rag_tutorials/hybrid_search_rag)\n- [🖥️ Local RAG App with Hybrid Search](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/rag_tutorials/local_hybrid_search_rag)\n- [📠 RAG Agent with Database Routing](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/rag_tutorials/rag_database_routing)\n- [🔄 Corrective RAG Agent](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/rag_tutorials/corrective_rag)\n\n### MCP AI Agents\n- [🐙 MCP GitHub Agent](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/mcp_ai_agents/github_mcp_agent)\n\n### LLM Apps with Memory\n- [💾 AI Arxiv Agent with Memory](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/llm_apps_with_memory_tutorials/ai_arxiv_agent_memory)\n- [📝 LLM App with Personalized Memory](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/llm_apps_with_memory_tutorials/llm_app_personalized_memory)\n- [🛩️ AI Travel Agent with Memory](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/llm_apps_with_memory_tutorials/ai_travel_agent_memory)\n- [🗄️ Local ChatGPT with Memory](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/llm_apps_with_memory_tutorials/local_chatgpt_with_memory)\n\n### Chat with X\n- [💬 Chat with GitHub Repo](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/chat_with_X_tutorials/chat_with_github)\n- [📨 Chat with Gmail](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/chat_with_X_tutorials/chat_with_gmail)\n- [📄 Chat with PDF](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/chat_with_X_tutorials/chat_with_pdf)\n- [📚 Chat with Research Papers](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/chat_with_X_tutorials/chat_with_research_papers)\n- [📝 Chat with Substack Newsletter](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/chat_with_X_tutorials/chat_with_substack)\n- [📽️ Chat with YouTube Videos](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/chat_with_X_tutorials/chat_with_youtube_videos)\n\n### LLM Finetuning\n- [🌐 Llama3.2 Finetuning](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/llm_finetuning_tutorials/llama3.2_finetuning)\n\n### Advanced Tools and Frameworks\n- [🧪 Gemini Multimodal Chatbot](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/advanced_tools_frameworks/gemini_multimodal_chatbot)\n- [🔄 Mixture of Agents](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/advanced_tools_frameworks/mixture_of_agents)\n- [🌐 MultiLLM Chat Playground](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/advanced_tools_frameworks/multillm_chat_playground)\n- [🔗 LLM Router App](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/advanced_tools_frameworks/llm_router_app)\n- [💬 Local ChatGPT Clone](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/advanced_tools_frameworks/local_chatgpt_clone)\n- [🌍 Web Scraping AI Agent](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/advanced_tools_frameworks/web_scrapping_ai_agent)\n- [🔍 Web Search AI Assistant](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/advanced_tools_frameworks/web_search_ai_assistant)\n- [🧪 Cursor AI Experiments](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/advanced_tools_frameworks/cursor_ai_experiments)\n\n## 🚀 Getting Started\n\n1. **Clone the repository** \n\n    ```bash \n    git clone https://github.com/Shubhamsaboo/awesome-llm-apps.git \n    ```\n\n2. **Navigate to the desired project directory**\n\n    ```bash \n    cd awesome-llm-apps/chat_with_X_tutorials/chat_with_gmail\n    ```\n\n3. **Install the required dependencies**\n\n    ```bash\n    pip install -r requirements.txt\n    ```\n\n4. **Follow the project-specific instructions** in each project's `README.md` file to set up and run the app.\n\n## 🤝 Contributing to Open Source\n\nContributions are welcome! If you have any ideas, improvements, or new apps to add, please create a new [GitHub Issue](https://github.com/Shubhamsaboo/awesome-llm-apps/issues) or submit a pull request. Make sure to follow the existing project structure and include a detailed `README.md` for each new app.\n\n### Thank You, Community, for the Support! 🙏\n\n[![Star History Chart](https://api.star-history.com/svg?repos=Shubhamsaboo/awesome-llm-apps&type=Date)](https://star-history.com/#Shubhamsaboo/awesome-llm-apps&Date)\n\n🌟 **Don’t miss out on future updates! Star the repo now and be the first to know about new and exciting LLM apps with RAG and AI Agents.**\n"
    },
    {
      "name": "Zoyaaaaaaa/Market-Research-AI-Agents",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/126981793?s=40&v=4",
      "owner": "Zoyaaaaaaa",
      "repo_name": "Market-Research-AI-Agents",
      "description": "Multi -Agent Environment for market research agents",
      "homepage": "https://market-research-ai-agents-cssd98plipyemqctfk5umt.streamlit.app/",
      "language": "Python",
      "created_at": "2024-11-24T07:39:35Z",
      "updated_at": "2025-03-15T11:01:46Z",
      "topics": [],
      "readme": "# 🌐 AI Strategy & Use Case Research Agent 🚀\r\n\r\n**AI Strategy & Use Case Research Agent** is a Streamlit-powered application that leverages OpenAI and Exa APIs to provide strategic insights, innovative AI/ML use cases, and curated resources for any industry. Whether you're exploring industry trends or brainstorming AI solutions, this tool helps generate actionable proposals in seconds.  \r\n\r\n---\r\n\r\n## 💡 **Features**\r\n### 1. **Comprehensive Industry Research**\r\n- Delivers detailed insights about your chosen industry, including:\r\n  - 🌟 **Trends & Innovations**\r\n  - 📊 **Market Positions**\r\n  - 🚀 **Future Outlooks**\r\n- Powered by **Exa Neural Search**, ensuring highly relevant results.\r\n\r\n### 2. **AI/ML Use Case Generation**\r\n- Generates tailored AI/ML solutions focusing on:\r\n  - ⚙️ Operational Efficiency\r\n  - 😊 Enhanced Customer Experience\r\n  - 🔬 Technological Innovations\r\n- Uses **OpenAI GPT Models** to provide specific and innovative ideas.\r\n\r\n### 3. **Dataset Resource Discovery**\r\n- Recommends datasets from top platforms:\r\n  - 🏷️ Kaggle\r\n  - 🤗 Hugging Face\r\n  - 🐙 GitHub\r\n- Provides links to datasets for every generated use case.\r\n\r\n### 4. **Final Proposal Creation**\r\n- Compiles all insights, use cases, and resources into a Markdown proposal.\r\n- 📄 Downloadable for easy sharing and integration into your projects.\r\n\r\n---\r\n\r\n## 🚀 **How It Works**\r\n1. **Input**: Provide an industry and optional company name.\r\n2. **Generate**: The app conducts research, brainstorms AI use cases, and collects datasets.\r\n3. **Explore**: Navigate through research insights, AI solutions, and resources in easy-to-use tabs.\r\n4. **Export**: Download the final proposal as a ready-to-use Markdown file.\r\n\r\n---\r\n\r\n## 🔧 **Technologies Used**\r\n- **Streamlit**: Intuitive UI for seamless interactions.\r\n- **OpenAI API**: For generating insightful and innovative AI/ML solutions.\r\n- **Exa API**: Neural search for industry research and dataset discovery.\r\n- **Python Libraries**: `dotenv`, `re`, `json`, and `markdown` for enhanced functionality.\r\n\r\n---\r\nDEMO VIDEO :\r\n\r\nhttps://github.com/user-attachments/assets/ac6a490e-b867-432b-abb7-50f2c2aa8537\r\n\r\n\r\n\r\n## ⚙️ **Setup Instructions**\r\n1. Clone the repository:  \r\n   ```bash\r\n   git clone https://github.com/your-repo/ai-strategy-research-agent.git\r\n   ```\r\n2. Install dependencies:  \r\n   ```bash\r\n   pip install -r requirements.txt\r\n   ```\r\n3. Add your API keys to a `.env` file:  \r\n   ```env\r\n   EXA_API_KEY=your_exa_api_key\r\n   OPENAI_API_KEY=your_openai_api_key\r\n   ```\r\n4. Run the app:  \r\n   ```bash\r\n   streamlit run app.py\r\n   ```\r\n\r\n---\r\n\r\n## 📈 **Use Cases**\r\n- 🎯 **Market Research**: Identify industry trends and challenges.\r\n- 🤖 **AI Solution Ideation**: Brainstorm cutting-edge AI/ML use cases.\r\n- 📂 **Resource Curation**: Find datasets to power your AI projects.\r\n- 📜 **Proposal Generation**: Share actionable strategies with stakeholders.\r\n\r\n---\r\n\r\nEnjoy strategizing with the **AI Strategy Research Agent** and build the future with AI! 🌟\r\n"
    },
    {
      "name": "RostKoff/montage-horde",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/119884204?s=40&v=4",
      "owner": "RostKoff",
      "repo_name": "montage-horde",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2024-12-19T18:58:42Z",
      "updated_at": "2025-03-14T15:57:05Z",
      "topics": [],
      "readme": "# Montage horde\r\nAn Autogen agent system embedded in a gradio interface that offers complex audio and video analysing, editing & processing. Runs code execution on a Docker container to ensure integrity of the environment.\r\n\r\n## System summary\r\nThis is an agent system that collaborates in SelectorGroupChat and fulfills user's requests regarding video/audio analysis and processing. The system consists of following agents:\r\n- SUPERVISOR: is the first to receive user's request. It's task is to oversee the workflow and distribute tasks to other agents;\r\n- PLANNER: creates a workflow based on the user's request, splitting it into small, manageable step-by-step tasks;\r\n- INFORMATION EXTRACTOR: extracts various information from audio/wideo, including transcription and sound;\r\n- TRANSLATOR: uses DeepL API to translate transcriptions into specified language;\r\n- SENTIMENT ANALYST: uses tabularisai to analyse the sentiment of the given transcription, regardless of the language provided;\r\n- CODER: for more complex tasks like video editing, it creates code that will be executed by EXECUTOR;\r\n- EXECUTOR: runs code provided by CODER to make desired edits;\r\n- TEXT WRITER: analyses the transcription and writes summaries and answers to questions provided by the user regarding the audio/video;\r\n- VALIDATOR: checks the output of other agents whether it fulfills user's request. If not, it sends retry instruction to SUPERVISOR;\r\n- OUTPUT FORMATTER: provides ultimate summary of the processes done and prints them in results, then finishes the process.\r\n\r\n## Installation & how to run\r\n1. Download this project from \"<> Code\" button as a zip file.\r\n2. Then, extract it in the folder of your choice.\r\n3. After importing and extracting the zip project file, create .env file add your API keys: \r\nOPENAI_API_KEY=your_code\r\nDEEPL_API_KEY=your_code\r\nHF_AUTH_TOKEN=your_code\r\n4. Make sure to have poetry and docker installed on your system (you can find instructions here: https://python-poetry.org/docs/ https://docs.docker.com/desktop/) and run *poetry install*.\r\n6. Run gradio_interface.py in your project environment, then proceed to your browser with the link *http://127.0.0.1:7860*.\r\n7. From there, you can upload audio and video files to process, and enter a prompt to orchestrate the task.\r\n8. Wait until the app processes your request (it may take up a few minutes) - the interface will be replaced and result displayed in \"results\" field with attached output zip file. All files generated during request processing are kept there.\r\n\r\n## Metrics\r\n\r\nTranscription metrics in jiwet for three different Whisperx models can be found in evaluation package on transcription-metrics branch. \r\n"
    },
    {
      "name": "Delahayecarry/MAAI",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/163613131?s=40&v=4",
      "owner": "Delahayecarry",
      "repo_name": "MAAI",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2025-03-14T13:42:13Z",
      "updated_at": "2025-03-15T02:29:41Z",
      "topics": [],
      "readme": "# 多智能体交互系统\n\n基于AutoGen 0.4框架的多智能体交互系统，用于模拟团队协作场景。系统支持多种预定义场景，实时显示智能体对话，并提供可视化功能。\n\n## 技术栈\n\n### 前端\n- React 18\n- TypeScript\n- Zustand (状态管理)\n- Vite (构建工具)\n- Tailwind CSS (样式)\n- Headless UI (UI组件)\n- React-Window (虚拟列表)\n- D3 (力导向图)\n- Framer Motion (动画)\n- SSE (Server-Sent Events，实时通信)\n\n### 后端\n- FastAPI\n- AutoGen 0.4\n- Python 3.10+\n\n## 功能特点\n\n- 多种预定义场景：支持团队会议、技术讨论、设计评审等多种场景\n- 实时对话展示：通过SSE技术实时展示智能体之间的对话内容\n- 历史记录查看：保存并查看历史对话记录\n- 关系网络可视化：使用D3力导向图可视化智能体之间的关系网络\n- 响应式设计：适配不同屏幕尺寸\n\n## 系统架构\n\n```\n├── frontend/            # React前端\n│   ├── src/\n│   │   ├── components/  # UI组件\n│   │   ├── pages/       # 页面组件\n│   │   ├── store/       # Zustand状态管理\n│   │   ├── hooks/       # 自定义Hooks\n│   │   ├── api/         # API服务\n│   │   └── utils/       # 工具函数\n│   ├── public/          # 静态资源\n│   └── ...              # 配置文件\n├── backend/             # FastAPI后端\n│   ├── main.py          # 主入口\n│   └── requirements.txt # 依赖\n├── agents/              # 智能体定义\n│   ├── manager.py       # 经理智能体\n│   ├── developer.py     # 开发者智能体\n│   └── designer.py      # 设计师智能体\n├── conversations/       # 对话场景定义\n│   └── scenarios.py     # 预定义场景\n├── utils/               # 工具函数\n│   └── logging_utils.py # 日志工具\n└── start.py             # 启动脚本\n```\n\n## 安装与运行\n\n### 环境要求\n- Python 3.10+\n- Node.js 16+\n- npm 8+\n\n### 安装步骤\n\n1. 克隆仓库\n```bash\ngit clone https://github.com/yourusername/multi-agent-interaction.git\ncd multi-agent-interaction\n```\n\n2. 安装后端依赖\n```bash\npip install -r requirements.txt\n```\n\n3. 安装前端依赖\n```bash\ncd frontend\nnpm install\ncd ..\n```\n\n4. 配置环境变量\n复制`.env.example`为`.env`，并填写相应的API密钥：\n```bash\ncp .env.example .env\n```\n\n### 运行应用\n\n使用启动脚本同时启动前端和后端：\n```bash\npython start.py\n```\n\n或者分别启动：\n\n1. 启动后端\n```bash\ncd backend\nuvicorn main:app --reload --port 8000\n```\n\n2. 启动前端\n```bash\ncd frontend\nnpm run dev\n```\n\n然后在浏览器中访问：http://localhost:3000\n\n### 使用 Docker 部署\n\n本项目支持使用 Docker 进行部署，提供了 Dockerfile 和 docker-compose.yml 文件。\n\n#### 使用 Docker Compose 部署\n\n1. 确保已安装 Docker 和 Docker Compose\n2. 复制环境变量文件\n```bash\ncp .env.example .env\n```\n3. 编辑 `.env` 文件，填写必要的配置\n4. 启动容器\n```bash\ndocker-compose up -d\n```\n5. 访问应用：http://localhost:8000\n\n#### 使用 Docker Hub 镜像\n\n你也可以直接使用我们发布在 Docker Hub 上的镜像：\n\n```bash\ndocker pull yourusername/maai:latest\ndocker run -d -p 8000:8000 -v $(pwd)/.env:/app/.env -v $(pwd)/conversations_log:/app/conversations_log --name maai-app yourusername/maai:latest\n```\n\n#### 构建自己的 Docker 镜像\n\n如果你想构建自己的 Docker 镜像，可以使用以下命令：\n\n```bash\ndocker build -t maai:latest .\ndocker run -d -p 8000:8000 -v $(pwd)/.env:/app/.env -v $(pwd)/conversations_log:/app/conversations_log --name maai-app maai:latest\n```\n\n## 使用方法\n\n1. 在\"实时对话\"页面中选择一个场景\n2. 点击\"启动对话\"按钮开始模拟\n3. 观察智能体之间的交互过程\n4. 可以随时停止对话\n5. 在\"历史记录\"页面查看保存的对话\n\n## 智能体介绍\n\n- **经理**：团队领导，负责协调和决策\n- **资深开发**：经验丰富的开发人员\n- **初级开发**：新加入的开发人员\n- **设计师**：负责用户界面和体验设计\n\n## 技术实现\n\n本项目使用了 AutoGen 0.4 API，主要特点包括：\n\n- 使用 `AssistantAgent` 创建智能体\n- 使用 `RoundRobinGroupChat` 进行轮流对话\n- 使用 `TextMessage` 处理消息\n- 使用 SSE (Server-Sent Events) 实现实时通信\n- 使用 FastAPI 构建后端 API\n\n## 持续集成/持续部署\n\n本项目使用 GitHub Actions 进行 CI/CD，每次推送到主分支或创建标签时，会自动构建 Docker 镜像并推送到 DockerHub。\n\n## 许可证\n\n本项目采用 MIT 许可证 - 详情请参阅 [LICENSE](LICENSE) 文件 "
    },
    {
      "name": "rajesh-nitc/autogen-backend",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/44093301?s=40&v=4",
      "owner": "rajesh-nitc",
      "repo_name": "autogen-backend",
      "description": null,
      "homepage": "",
      "language": "Python",
      "created_at": "2025-03-11T05:44:35Z",
      "updated_at": "2025-04-10T07:04:10Z",
      "topics": [],
      "readme": "# autogen-backend\n\nA backend API powered by [Autogen](https://github.com/microsoft/autogen).\n\nA new run is created for every user request sent via websocket and stored in the database.\n\nThe team state is reset for the user at the completion of every nth run as a temporary workaround for [this issue](https://github.com/microsoft/autogen/issues/6227).\n\nThe data for vector store is sourced from [azure-search-openai-demo](https://github.com/Azure-Samples/azure-search-openai-demo).\n \n\n## Use Cases\n\n1. **Generation with APIs** (`WeatherAgent`)\n2. **Generation with Vector Search** (`BenefitsAgent`)\n\n## Getting Started 🚀\n\n### Prerequisites\n\n```bash\n# Azure OpenAI\necho 'export AZURE_OPENAI_API_KEY=YOUR_API_KEY_HERE' >> ~/.zshrc\n\n# PostgresSQL\necho 'export POSTGRES_PASSWORD=YOUR_PASSWORD_HERE' >> ~/.zshrc\n\n# OpenWeather\necho 'export OPENWEATHER_API_KEY=YOUR_API_KEY_HERE' >> ~/.zshrc\n```\n\n### Run\n\n```bash\n# Run db\nmake run_db\n\n# Run embeddings\nmake run_embeddings\n\n# Run app\nmake run_app\n\n# OR run app in docker\nmake run_app_docker\n```\n\n### Test with Postman\n\n```bash\n# URL\nws://localhost:8000/ws/chat/session-foo\n\n# Request format\n{\n  \"content\": \"your message here\",\n  \"source\": \"user\"\n}\n```\n\n## Examples\n\n### 1. Generation with APIs (`WeatherAgent`)\n\n**Request**\n```json\n{\n    \"content\": \"how is the weather in bengaluru and mumbai?\",\n    \"source\": \"user\"\n}\n\n```\n\n**Response**\n```json\n{\n    \"source\": \"WeatherAgent\",\n    \"models_usage\": {\n        \"prompt_tokens\": 477,\n        \"completion_tokens\": 140\n    },\n    \"metadata\": {},\n    \"content\": \"The current weather in Bengaluru and Mumbai is as follows:\\n\\n**Bengaluru:**\\n- Temperature: 300.49 K (approximately 27.34 °C)\\n- Feels Like: 300.27 K (approximately 27.12 °C)\\n- Humidity: 40%\\n- Pressure: 1012 hPa\\n\\n**Mumbai:**\\n- Temperature: 303.48 K (approximately 30.33 °C)\\n- Feels Like: 305.05 K (approximately 31.90 °C)\\n- Humidity: 52%\\n- Pressure: 1009 hPa\\n\\nWould you like to know anything else? \\n\\nTERMINATE\",\n    \"type\": \"TextMessage\"\n}\n```\n\n### 2. Generation with Vector Search (`BenefitsAgent`)\n\n**Request**\n```json\n{\n    \"content\": \"how much will be deducted from salary for the standard plan?\",\n    \"source\": \"user\"\n}\n```\n\n**Response**\n```json\n{\n    \"source\": \"BenefitsAgent\",\n    \"models_usage\": {\n        \"prompt_tokens\": 1107,\n        \"completion_tokens\": 102\n    },\n    \"metadata\": {},\n    \"content\": \"{\\n    \\\"answer\\\": \\\"For the Northwind Standard plan, the premium amounts deducted from salary are as follows: $45.00 for Employee Only, $65.00 for Employee +1, and $78.00 for Employee +2 or more. These deductions are taken from payroll on a pre-determined schedule.\\\",\\n    \\\"sources\\\": [\\n        {\\n            \\\"source\\\": \\\"data/Benefit_Options.pdf\\\",\\n            \\\"page_label\\\": \\\"4\\\"\\n        }\\n    ]\\n}\\n\\nTERMINATE\",\n    \"type\": \"TextMessage\"\n}\n```\n"
    },
    {
      "name": "ciesko/aether",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/9701202?s=40&v=4",
      "owner": "ciesko",
      "repo_name": "aether",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2025-03-12T23:17:33Z",
      "updated_at": "2025-03-31T07:23:42Z",
      "topics": [],
      "readme": "# AI Agent Communication System\n\nThis system allows AI agents to communicate with each other over the Internet, facilitating a turn-based conversation between users through AI intermediaries.\n\n## Components\n\n- **Server**: HTTP server that broadcasts messages between agents using Server-Sent Events (SSE)\n- **Local Agent**: Runs locally on a user's machine and interacts with both the user and remote agents through Azure OpenAI\n\n## Setup\n\n1. Install dependencies:\n   ```\n   pip install -r requirements.txt\n   ```\n\n2. Configure the application:\n   - Copy `.env.example` to `.env` and modify as needed\n   - Set your Azure OpenAI API key and endpoint in the `.env` file\n   - Set the server address in the config file\n\n3. Start the server:\n   ```\n   python run_server.py\n   ```\n\n4. Start the local agent on each machine:\n   ```\n   python run_agent.py\n   ```\n\n## Usage\n\n1. A user instructs their local agent\n2. The agent processes the input using Azure OpenAI\n3. The agent broadcasts the message to all other connected agents\n4. Other agents receive the message and display it to their local users\n5. Users can respond, continuing the conversation\n\n## Architecture\n\n- The system uses Autogen with Azure OpenAI for AI agent functionality\n- Communication between agents is handled via Server-Sent Events (SSE)\n- Messages are broadcast to all connected agents except the sender\n\n## Azure Deployment\n\n### Server Deployment\n\nThe server can be deployed to Azure App Service:\n\n1. Create an Azure App Service with Python runtime\n2. Set the following environment variables in the App Service configuration:\n   - SERVER_HOST\n   - SERVER_PORT\n3. Deploy the code to the App Service\n4. Update the `SERVER_ADDRESS` in the local `.env` files to point to your deployed App Service URL\n\n### Local Agent Configuration\n\nFor the local agents, configure:\n\n1. Azure OpenAI resource settings:\n   - `AZURE_OPENAI_API_KEY`: Your Azure OpenAI API key\n   - `AZURE_OPENAI_ENDPOINT`: Your Azure OpenAI endpoint URL\n   - `AZURE_OPENAI_DEPLOYMENT_NAME`: The name of your deployed model in Azure OpenAI\n2. `AGENT_NAME`: Unique identifier for each agent\n3. `SERVER_ADDRESS`: The URL of the deployed server "
    },
    {
      "name": "memenow/persona-agent",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/142292623?s=40&v=4",
      "owner": "memenow",
      "repo_name": "persona-agent",
      "description": "A flexible and powerful framework for creating AI persona agents using AutoGen with Model Context Protocol (MCP) integration.",
      "homepage": "",
      "language": "Python",
      "created_at": "2025-03-10T05:37:23Z",
      "updated_at": "2025-03-26T20:04:59Z",
      "topics": [
        "agents",
        "autogen"
      ],
      "readme": "# Persona Agent\n\nA Python-based API server for creating and interacting with AI personas using the AutoGen framework and Model Context Protocol (MCP) tools integration.\n\n## Overview\n\nThis project provides a robust API for creating AI personas that can interact with users through natural language. Built on top of AutoGen 0.4, it allows for the creation of agents that can use external tools and services through the Model Context Protocol (MCP) to enhance their capabilities.\n\n## Features\n\n- **Persona-based AI Agents**: Create and interact with AI agents that simulate specific personas\n- **Model Context Protocol Integration**: Enhance AI capabilities with external tools through MCP\n- **RESTful API**: Provides a comprehensive REST API for managing personas, agents, and conversations\n- **Tool-Augmented Responses**: Enable agents to use external tools to respond to user queries\n- **Configurable Behavior**: Customize persona characteristics through configuration files\n- **AutoGen 0.4 Support**: Compatible with the latest AutoGen framework features\n\n## Architecture\n\nThe project is organized into several key components:\n\n- **API Server**: FastAPI implementation for the REST API endpoints\n- **Persona Management**: Load and manage persona definitions from JSON/YAML files\n- **Agent Factory**: Create and configure AutoGen agents based on personas\n- **MCP Integration**: Connect to external MCP services for enhanced capabilities\n- **Session Management**: Handle conversation sessions between users and agents\n\n## Installation\n\n1. Clone the repository:\n   ```bash\n   git clone https://github.com/memenow/persona-agent.git\n   cd persona-agent\n   ```\n\n2. Create a virtual environment:\n   ```bash\n   python -m venv venv\n   source venv/bin/activate  # On Windows: venv\\Scripts\\activate\n   ```\n\n3. Install dependencies:\n   ```bash\n   pip install -r requirements.txt\n   ```\n\n4. Configure API keys:\n   Create a `config/llm_config.json` file with your API keys and model configurations:\n   ```json\n   {\n     \"default_model\": \"gpt-4o\",\n     \"api_key\": \"your-api-key-here\",\n     \"api_base\": \"https://api.openai.com/v1\"\n   }\n   ```\n\n## Usage\n\n### Running the API Server\n\nStart the API server using the provided script:\n\n```bash\npython run_api_server.py\n```\n\nThe API will be available at http://localhost:8000/api/v1/ with Swagger documentation at http://localhost:8000/docs.\n\n### API Endpoints\n\n#### Personas API\n\n- `GET /api/v1/personas`: List all available personas\n- `GET /api/v1/personas/{id}`: Get a specific persona's details\n- `POST /api/v1/personas`: Create a new persona\n- `PUT /api/v1/personas/{id}`: Update an existing persona\n- `DELETE /api/v1/personas/{id}`: Delete a persona\n\n#### Agents API\n\n- `GET /api/v1/agents`: List all active agents\n- `GET /api/v1/agents/{id}`: Get a specific agent's details\n- `POST /api/v1/agents`: Create a new agent based on a persona\n- `DELETE /api/v1/agents/{id}`: Delete an agent\n\n#### Sessions API\n\n- `GET /api/v1/sessions`: List all active sessions\n- `GET /api/v1/sessions/{id}`: Get a specific session's details\n- `POST /api/v1/sessions`: Create a new conversation session\n- `DELETE /api/v1/sessions/{id}`: Delete a session\n- `POST /api/v1/sessions/{id}/messages`: Send a message to an agent\n- `GET /api/v1/sessions/{id}/messages`: Get all messages in a session\n\n### Persona Configuration\n\nPersonas can be defined in JSON or YAML format:\n\n```json\n{\n  \"name\": \"Albert Einstein\",\n  \"description\": \"Theoretical physicist and Nobel laureate\",\n  \"personal_background\": {\n    \"birth\": \"March 14, 1879, Ulm, Germany\",\n    \"education\": \"ETH Zurich, University of Zurich\",\n    \"profession\": \"Physicist, Professor\"\n  },\n  \"language_style\": {\n    \"tone\": \"Thoughtful, inquisitive, sometimes whimsical\",\n    \"common_phrases\": [\"Imagination is more important than knowledge\", \"Everything should be made as simple as possible, but not simpler\"]\n  },\n  \"knowledge_domains\": {\n    \"physics\": [\"Relativity theory\", \"Quantum mechanics\", \"Brownian motion\"],\n    \"philosophy\": [\"Scientific determinism\", \"Pacifism\", \"Religious views\"]\n  },\n  \"interaction_samples\": [\n    {\n      \"type\": \"conversation\",\n      \"content\": \"Q: What is the most important scientific principle?\\nA: The principle of curiosity - to never stop questioning. That is the source of all knowledge and discovery.\"\n    }\n  ]\n}\n```\n\n### MCP Configuration\n\nTo configure MCP services, create a `config/mcp_config.json` file:\n\n```json\n{\n  \"mcpServers\": {\n    \"brave_search\": {\n      \"command\": \"node\",\n      \"args\": [\"path/to/mcp-brave-search/index.js\"],\n      \"env\": {\n        \"BRAVE_API_KEY\": \"${BRAVE_API_KEY}\"\n      },\n      \"description\": \"Brave Search MCP service\"\n    }\n  }\n}\n```\n\nEnvironment variables in the configuration (like `${BRAVE_API_KEY}`) will be automatically resolved at runtime.\n\n## Project Structure\n\n```\npersona-agent/\n├── config/                  # Configuration files\n│   ├── llm_config.json      # LLM API keys and settings\n│   └── mcp_config.json      # MCP services configuration\n├── examples/                # Example code and personas\n│   └── personas/            # Example persona definitions\n├── src/                     # Source code\n│   └── persona_agent/       # Main package\n│       ├── api/             # API implementation\n│       │   ├── routes/      # API route handlers\n│       │   ├── agent_factory.py # Agent creation factory\n│       │   ├── config.py    # API configuration\n│       │   ├── dependencies.py  # FastAPI dependencies\n│       │   ├── models.py    # Pydantic API models\n│       │   ├── persona_manager.py # Persona data management\n│       │   └── server.py    # FastAPI server\n│       ├── core/            # Core functionality\n│       ├── mcp/             # MCP integration\n│       └── cli.py           # Command-line interface\n├── tests/                   # Test suite\n├── run_api_server.py        # Server startup script\n├── requirements.txt         # Project dependencies\n└── LICENSE                  # License\n```\n\n## Development\n\n### Setting Up Development Environment\n\n```bash\n# Clone the repository\ngit clone https://github.com/memenow/persona-agent.git\ncd persona-agent\n\n# Create and activate virtual environment\npython -m venv venv\nsource venv/bin/activate  # On Windows: venv\\Scripts\\activate\n\n# Install development dependencies\npip install -r requirements.txt\npip install pytest pytest-asyncio black isort mypy\n\n# Run tests\npytest\n```\n\n### Adding New MCP Services\n\n1. Create a new MCP service implementation\n2. Add the service configuration to `config/mcp_config.json`\n3. The service will be automatically loaded by the `McpManager` class\n\n### Extending Personas\n\nTo add new persona capabilities:\n\n1. Enhance the `PersonaProfile` class in `src/persona_agent/core/persona_profile.py`\n2. Update the persona JSON/YAML schema accordingly\n3. Update the API models in `src/persona_agent/api/models.py`\n\n## License\n\nThis project is licensed under the Apache License 2.0 - see the [LICENSE](LICENSE) file for details."
    },
    {
      "name": "marcelmedina/ai-autogen-agents",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/17131300?s=40&v=4",
      "owner": "marcelmedina",
      "repo_name": "ai-autogen-agents",
      "description": null,
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2025-03-06T20:20:46Z",
      "updated_at": "2025-04-18T23:59:20Z",
      "topics": [],
      "readme": "# ai-autogen-agents\nAutogen AI samples comprising Jupyter Notebooks and Python code.\n\n## Configuring environment variables\n- Rename `.devenv` to `.env`.\n- Map the variable"
    },
    {
      "name": "yogeshkc/ai-lets-go",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/12978461?s=40&v=4",
      "owner": "yogeshkc",
      "repo_name": "ai-lets-go",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2025-03-06T18:25:07Z",
      "updated_at": "2025-03-07T07:42:18Z",
      "topics": [],
      "readme": "# AI Lets Go\n\nA Python project demonstrating the use of AutoGen for building conversational AI applications. Currently implements two use cases:\n1. Currency Exchange Rate Calculator\n2. Git Repository Analysis\n\n## Setup\n\n### Prerequisites\n- Python 3.13+\n- Git\n- Virtual Environment\n\n### Installation\n\n1. Clone the repository:\n```bash\ngit clone <repository-url>\ncd ai-lets-go\n```\n\n2. Create and activate virtual environment:\n```bash\npython -m venv venv\nsource venv/bin/activate  # On Unix/macOS\n# or\n.\\venv\\Scripts\\activate  # On Windows\n```\n\n3. Install dependencies:\n```bash\npip install -r requirements.txt\n```\n\n4. Create a `.env` file in the root directory with the following variables:\n```bash\n# Required: Your OpenAI API key\nOPENAI_API_KEY=\"your-api-key-here\"\n\n# Optional: Path to git repository for analysis (defaults to current directory)\nGIT_REPO_PATH=\"/path/to/your/git/repo\"\n```\n\n## Usage\n\nThe project contains two main features that can be run independently or together:\n\n### 1. Currency Exchange\nUses AutoGen to create a conversational interface for currency conversion using real-time exchange rates.\n\nExample usage in `main.py`:\n```python\nmessages = [\"How much is 1 USD in INR?\"]\nrun_currency_exchange(llm_config, messages)\n```\n\n### 2. Git Repository Analysis\nAnalyzes a git repository to provide insights about commit history, team collaboration, and code changes.\n\nExample usage in `main.py`:\n```python\nrun_git_analysis_example()\n```\n\n### Running the Application\n\nRun the main script to execute both examples:\n```bash\npython main.py\n```\n\n## Environment Variables\n\n| Variable | Required | Description | Example |\n|----------|----------|-------------|---------|\n| OPENAI_API_KEY | Yes | Your OpenAI API key for AutoGen | sk-... |\n| GIT_REPO_PATH | No | Path to git repository for analysis | /path/to/repo |\n\n## Project Structure\n\n```\nai-lets-go/\n├── currency_exchange/     # Currency exchange functionality\n│   ├── __init__.py\n│   ├── agent.py          # AutoGen agents for currency exchange\n│   └── exchange.py       # Exchange rate calculation\n├── git_analysis/         # Git analysis functionality\n│   ├── __init__.py\n│   ├── agent.py         # AutoGen agents for git analysis\n│   └── analyzer.py      # Git log parsing and analysis\n├── main.py              # Main entry point\n├── requirements.txt     # Project dependencies\n└── .env                # Environment variables\n```\n\n## Notes\n- The currency exchange feature uses a free API (ExchangeRate-API) which updates rates daily\n- Git analysis works with any git repository specified in GIT_REPO_PATH\n- All API keys and sensitive information should be stored in .env file\n- The .env file is included in .gitignore for security\n"
    },
    {
      "name": "XinmiaoYan/ai-agents-for-beginners",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/83053884?s=40&v=4",
      "owner": "XinmiaoYan",
      "repo_name": "ai-agents-for-beginners",
      "description": null,
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2025-03-05T18:12:15Z",
      "updated_at": "2025-04-09T12:52:49Z",
      "topics": [],
      "readme": "# AI Agents for Beginners - A Course\n\n![Generative AI For Beginners](./images/repo-thumbnail.png)\n\n## 10 Lessons teaching everything you need to know to start building AI Agents\n\n[![GitHub license](https://img.shields.io/github/license/microsoft/ai-agents-for-beginners.svg)](https://github.com/microsoft/ai-agents-for-beginners/blob/master/LICENSE?WT.mc_id=academic-105485-koreyst)\n[![GitHub contributors](https://img.shields.io/github/contributors/microsoft/ai-agents-for-beginners.svg)](https://GitHub.com/microsoft/ai-agents-for-beginners/graphs/contributors/?WT.mc_id=academic-105485-koreyst)\n[![GitHub issues](https://img.shields.io/github/issues/microsoft/ai-agents-for-beginners.svg)](https://GitHub.com/microsoft/ai-agents-for-beginners/issues/?WT.mc_id=academic-105485-koreyst)\n[![GitHub pull-requests](https://img.shields.io/github/issues-pr/microsoft/ai-agents-for-beginners.svg)](https://GitHub.com/microsoft/ai-agents-for-beginners/pulls/?WT.mc_id=academic-105485-koreyst)\n[![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat-square)](http://makeapullrequest.com?WT.mc_id=academic-105485-koreyst)\n\n[![GitHub watchers](https://img.shields.io/github/watchers/microsoft/ai-agents-for-beginners.svg?style=social&label=Watch)](https://GitHub.com/microsoft/ai-agents-for-beginners/watchers/?WT.mc_id=academic-105485-koreyst)\n[![GitHub forks](https://img.shields.io/github/forks/microsoft/ai-agents-for-beginners.svg?style=social&label=Fork)](https://GitHub.com/microsoft/ai-agents-for-beginners/network/?WT.mc_id=academic-105485-koreyst)\n[![GitHub stars](https://img.shields.io/github/stars/microsoft/ai-agents-for-beginners.svg?style=social&label=Star)](https://GitHub.com/microsoft/ai-agents-for-beginners/stargazers/?WT.mc_id=academic-105485-koreyst)\n\n[![Azure AI Discord](https://dcbadge.limes.pink/api/server/kzRShWzttr)](https://discord.gg/kzRShWzttr)\n\n\n## 🌱 Getting Started\n\nThis course has 10 lessons covering the fundamentals of building AI Agents. Each lesson covers its own topic so start wherever you like!\n\nThere is multi-language support for this course. Go to our [available languages here](#-multi-language-support). \n\nIf this is your first time building with Generative AI models, check out our [Generative AI For Beginners](https://aka.ms/genai-beginners) course, which includes 21 lessons on building with GenAI.\n\nDon't forget to [star (🌟) this repo](https://docs.github.com/en/get-started/exploring-projects-on-github/saving-repositories-with-stars?WT.mc_id=academic-105485-koreyst) and [fork this repo](https://github.com/microsoft/ai-agents-for-beginners/fork) to run the code.\n\n### What You Need \n\nEach lesson in this course includes code examples, which can be found in the code_samples folder. You can [fork this repo](https://github.com/microsoft/ai-agents-for-beginners/fork) to create your own copy.  \n\nThe code example in these exercise, utilise Azure AI Foundry and GitHub Model Catalogs for interacting with Language Models:\n\n- [Github Models](https://aka.ms/ai-agents-beginners/github-models) - Free / Limited\n- [Azure AI Foundry](https://aka.ms/ai-agents-beginners/ai-foundry) - Azure Account Required\n\nThis course also uses the following AI Agent frameworks and services from Microsoft:\n\n- [Azure AI Agent Service](https://aka.ms/ai-agents-beginners/ai-agent-service)\n- [Semantic Kernel](https://aka.ms/ai-agents-beginners/semantic-kernel)\n- [AutoGen](https://aka.ms/ai-agents/autogen)\n\nFor more information on running the code for this course, go to the [Course Setup](./00-course-setup/README.md).\n\n## 🙏 Want to help?\n\nDo you have suggestions or found spelling or code errors? [Raise an issue](https://github.com/microsoft/ai-agents-for-beginners/issues?WT.mc_id=academic-105485-koreyst) or [Create a pull request](https://github.com/microsoft/ai-agents-for-beginners/pulls?WT.mc_id=academic-105485-koreyst)\n\nIf you get stuck or have any questions about building AI Agents, join our [Azure AI Community Discord](https://discord.gg/kzRShWzttr).\n\n## 📂 Each lesson includes\n\n- A written lesson located in the README (Videos Coming March 2025)\n- Python code samples supporting Azure AI Foundry and Github Models (Free)\n- Links to extra resources to continue your learning\n\n\n## 🗃️ Lessons\n\n| **Lesson**                            | **Link**                                   | \n|----------------------------------------|--------------------------------------------|\n| Intro to AI Agents and Use Cases       | [Link](./01-intro-to-ai-agents/README.md)          |\n| Exploring Agentic Frameworks           | [Link](./02-explore-agentic-frameworks/README.md)  |\n| Understanding Agentic Design Patterns  | [Link](./03-agentic-design-patterns/README.md)  |\n| Tool Use Design Pattern                | [Link](./04-tool-use/README.md)                    |\n| Agentic RAG                            | [Link](./05-agentic-rag/README.md)                 |\n| Building Trustworty AI Agents          | [Link](./06-building-trustworthy-agents/README.md) |\n| Planning Design Pattern                | [Link](./07-planning-design/README.md)             |\n| Multi-Agent Design Pattern             | [Link](./08-multi-agent/README.md)                 |\n| Metacognition Design Pattern           | [Link](./09-metacognition/README.md)               |\n| AI Agents in Production                | [Link](./10-ai-agents-production/README.md)        |\n\n## 🌐 Multi-Language Support\n\n| Language             | Code | Link to Translated README                               | Last Updated |\n|----------------------|------|---------------------------------------------------------|--------------|\n| Chinese (Simplified) | zh   | [Chinese Translation](./translations/zh/README.md)      | 2025-02-13   |\n| Chinese (Traditional)| tw   | [Chinese Translation](./translations/tw/README.md)      | 2025-02-13   |\n| Chinese (Hong Kong)  | hk   | [Chinese (Hong Kong) Translation](./translations/hk/README.md) | 2025-02-13   |\n| French               | fr   | [French Translation](./translations/fr/README.md)       | 2025-02-13   |\n| Japanese             | ja   | [Japanese Translation](./translations/ja/README.md)     | 2025-02-13   |\n| Korean               | ko   | [Korean Translation](./translations/ko/README.md)       | 2025-02-13   |\n| Portuguese           | pt   | [Portuguese Translation](./translations/pt/README.md)   | 2025-02-13   |\n| Spanish              | es   | [Spanish Translation](./translations/es/README.md)      | 2025-02-13   |\n| German               | de   | [German Translation](./translations/de/README.md)       | 2025-02-13   |\n\n## 🎒 Other Courses\n\nOur team produces other courses! Check out:\n\n- [**NEW** Generative AI for Beginners using .NET](https://github.com/microsoft/Generative-AI-for-beginners-dotnet?WT.mc_id=academic-105485-koreyst)\n- [Generative AI for Beginners](https://github.com/microsoft/generative-ai-for-beginners?WT.mc_id=academic-105485-koreyst)\n- [ML for Beginners](https://aka.ms/ml-beginners?WT.mc_id=academic-105485-koreyst)\n- [Data Science for Beginners](https://aka.ms/datascience-beginners?WT.mc_id=academic-105485-koreyst)\n- [AI for Beginners](https://aka.ms/ai-beginners?WT.mc_id=academic-105485-koreyst)\n- [Cybersecurity for Beginners](https://github.com/microsoft/Security-101??WT.mc_id=academic-96948-sayoung)\n- [Web Dev for Beginners](https://aka.ms/webdev-beginners?WT.mc_id=academic-105485-koreyst)\n- [IoT for Beginners](https://aka.ms/iot-beginners?WT.mc_id=academic-105485-koreyst)\n- [XR Development for Beginners](https://github.com/microsoft/xr-development-for-beginners?WT.mc_id=academic-105485-koreyst)\n- [Mastering GitHub Copilot for AI Paired Programming](https://aka.ms/GitHubCopilotAI?WT.mc_id=academic-105485-koreyst)\n- [Mastering GitHub Copilot for C#/.NET Developers](https://github.com/microsoft/mastering-github-copilot-for-dotnet-csharp-developers?WT.mc_id=academic-105485-koreyst)\n- [Choose Your Own Copilot Adventure](https://github.com/microsoft/CopilotAdventures?WT.mc_id=academic-105485-koreyst)\n\n## 🌟 Community Thanks\n\nThanks to [Shivam Goyal](https://www.linkedin.com/in/shivam2003/) for contributing important code samples demonstrating Agentic RAG. \n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit <https://cla.opensource.microsoft.com>.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft\ntrademarks or logos is subject to and must follow\n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos is subject to those third-parties' policies.\n"
    },
    {
      "name": "arindamhazramsft/AZ-AI-Agent-Service-Demos",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/123211674?s=40&v=4",
      "owner": "arindamhazramsft",
      "repo_name": "AZ-AI-Agent-Service-Demos",
      "description": "Azure AI Agent Service Demos using Python and Streamlit",
      "homepage": null,
      "language": "Python",
      "created_at": "2025-03-03T15:46:19Z",
      "updated_at": "2025-04-01T16:17:32Z",
      "topics": [],
      "readme": ""
    },
    {
      "name": "ArthurVerrez/autogen-streamlit-hackernews-demo",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/33450143?s=40&v=4",
      "owner": "ArthurVerrez",
      "repo_name": "autogen-streamlit-hackernews-demo",
      "description": "A simple showcase of how to leverage an Autogen agent inside of a Streamlit UI to analyze Hackernews top posts.",
      "homepage": "https://autogen-hackernews-demo.streamlit.app/",
      "language": "Python",
      "created_at": "2025-03-02T14:47:38Z",
      "updated_at": "2025-03-02T17:04:59Z",
      "topics": [
        "autogen",
        "hackernews",
        "streamlit"
      ],
      "readme": "# Autogen Streamlit Hacker News Demo\n\n## Overview\n\nA demonstration of integrating a Autogen agent within a Streamlit UI to analyze top Hacker News posts. This project sets up a tool, agent, and task to showcase core functionalities.\n\nYou can access a live version of the app [here](https://autogen-hackernews-demo.streamlit.app/).\n\n![App Screenshot](app_screenshot.png)\n\n## Features\n\n- **Tool & Agent Integration:** Seamlessly fetch, analyze, and display top posts.\n- **Interactive UI:** Leverages Streamlit for real-time visualization.\n- **Modular Design:** Easy to maintain and extend.\n\n## Installation\n\n### Clone the repository and create a virtual environment\n\n```bash\ngit clone https://github.com/ArthurVerrez/autogen-streamlit-hackernews-demo\ncd autogen-streamlit-hackernews-demo\npython -m venv env\n```\n\n### Activate the Environment\n\n**Mac/Linux:**\n\n```bash\nsource env/bin/activate\n```\n\n**Windows:**\n\n```bash\nenv\\Scripts\\activate\n```\n\n### Install dependencies:\n\n```bash\npip install -r requirements.txt\n```\n\n## Usage\n\n(Optional) Set `LLM_API_KEY` in the `.env` file, then run the app:\n\n```bash\nstreamlit run app.py\n```\n\n## Next Steps\n\n- Implement a better logging system in the thinking UI (as of now, it doesn't correctly detect and display the Agent's events)\n- Implement knowledge/memory capabilities\n\n## Links\n\n- [Autogen](https://github.com/microsoft/autogen)\n- [Hacker News](https://news.ycombinator.com/)\n- [Streamlit](https://streamlit.io/)\n\n## Disclaimer\n\nNot affiliated with Autogen or Hacker News.\n"
    },
    {
      "name": "anitha2017/agenticai-autogen-gemini-demo",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/26123698?s=40&v=4",
      "owner": "anitha2017",
      "repo_name": "agenticai-autogen-gemini-demo",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2025-02-28T01:23:03Z",
      "updated_at": "2025-03-30T22:26:12Z",
      "topics": [],
      "readme": "# Demo Instructions\nThis lab practice demonstrates how to get started with [Autogen](https://github.com/microsoft/autogen) framework. \n\n# Create and Navigate to Demo folder\nClone autogen experiments from Developer advocate.\n\n\t$ cd <parent directory>\n\t$ git clone https://github.com/anitha2017/agenticai-autogen-gemini-demo.git\n\n# Create and activate the virtual environment\n\t$ python3 -m venv .venv\n\t$ .\\.venv\\Scripts\\activate\n\n# Install requirements.txt\n\tautogen-agentchat[llm,gemini]~=0.2\n\tautogen\n\tgoogle-generativeai \n\tvertexai\n\tag2[gemini]\n\tagentops\n\n\t$ pip install -r requirements.txt\n\t\n# Get the API keys and populate the keys in model_config.json\nhttps://aistudio.google.com/app/apikey\n\n# Download and Install ollama using https://ollama.com/ , then run gemma model.\n\n\t## list and run ollama model\n\t\n\tPS C:\\Users\\Anitha> ollama ls\n\tNAME            ID              SIZE      MODIFIED\n\tgemma:latest    a72c7f4d0a15    5.0 GB    5 days ago\n\tPS C:\\Users\\Anitha> ollama run gemma\n\t>>> Give me a famous quote from Aristotle?\n\t\"To understand something, one must first comprehend its essence.\"\n\n\t>>> height of eiffel tower\n\tThe height of the Eiffel Tower is 1,063 feet (324 meters).\n\n\t>>> /bye\n\tPS C:\\Users\\Anitha>\n\n\t## Test model response\n\tInvoke-WebRequest -Uri \"http://localhost:11434/api/generate\" `\n    -Method Post `\n    -Headers @{\"Content-Type\"=\"application/json\"} `\n    -Body '{\n        \"model\": \"gemma\",\n        \"prompt\": \"height of eiffel tower?\",\n        \"stream\": false,\n        \"options\": {\n            \"temperature\": 0.2\n        }\n    }'\n\t\n# Run the Multi-Agent lab program.\n\nhttps://microsoft.github.io/autogen/0.2/assets/images/autogen_agents-b80434bcb15d46da0c6cbeed28115f38.png\n\t\n\t$ python cfp1-multi-agent-interaction.py\n\t$ python cfp2-multi-agent-interaction.py\n\t$ python human-autoagent-interaction-using-gemini.py\n\t$ python human-autoagent-interaction-using-local_llm_gemma.py\n\n# Install and Run AutogenStudio for interactive UI\n\n\t$ pip install autogenstudio\n\t$ pip install playwright\n\t$ playwright install\n\t$ autogenstudio ui --port 8082 --appdir .\n\n# References:\n\n[Autogen] with [Gemini] \nhttps://github.com/rominirani/autogen-experiments.git"
    },
    {
      "name": "pmajor74/demo.autogen_meal_blogwriter",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/52909235?s=40&v=4",
      "owner": "pmajor74",
      "repo_name": "demo.autogen_meal_blogwriter",
      "description": "This is the source code for the Autogen demonstration \"Unleashing the Power of Agents: An Autogen Demo on Data Retrieval and Processing\"",
      "homepage": null,
      "language": "Python",
      "created_at": "2025-02-27T04:31:47Z",
      "updated_at": "2025-04-20T04:16:05Z",
      "topics": [],
      "readme": "# demo.autogen_meal_blogwriter\nThis is the source code for the Autogen demonstration \"Unleashing the Power of Agents: An Autogen Demo on Data Retrieval and Processing\"\n\nRun the install.cmd to create the env and install needed libraries.\n\nTo run type the following from the command-line:\n\n1) venv\\scripts\\activate\n2) python autogen_agent_recipe_blog_builder.py\n\n"
    },
    {
      "name": "luiselopezlopez/globalai2025",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/97617267?s=40&v=4",
      "owner": "luiselopezlopez",
      "repo_name": "globalai2025",
      "description": "Autogen Samples used in GlobalAI 2025",
      "homepage": null,
      "language": "Python",
      "created_at": "2025-02-26T06:04:54Z",
      "updated_at": "2025-03-07T21:17:21Z",
      "topics": [],
      "readme": "# globalai2025\nAutogen Samples used in GlobalAI 2025\n"
    },
    {
      "name": "fuxin123z/autogen_mcp",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/60996067?s=40&v=4",
      "owner": "fuxin123z",
      "repo_name": "autogen_mcp",
      "description": "autogen_mcp",
      "homepage": null,
      "language": "Python",
      "created_at": "2025-02-24T03:25:19Z",
      "updated_at": "2025-02-24T03:30:47Z",
      "topics": [],
      "readme": "# My Project"
    },
    {
      "name": "CantCode023/bloggit",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/95113225?s=40&v=4",
      "owner": "CantCode023",
      "repo_name": "bloggit",
      "description": "Your personal AI blogger.",
      "homepage": "",
      "language": "Python",
      "created_at": "2025-02-20T13:48:54Z",
      "updated_at": "2025-03-01T07:13:23Z",
      "topics": [
        "ai",
        "autogen",
        "blog",
        "blog-gerator",
        "gemini",
        "gpt",
        "medium",
        "python"
      ],
      "readme": "# 🚀 Bloggit\n\nTurn your GitHub repos into awesome blog posts with AI magic! ✨\n\n> [!NOTE]\n> Check it out at [bloggit.streamlit.app!](https://bloggit.streamlit.app)\n\n## 🎯 What is Bloggit?\n\nBloggit is your AI-powered writing buddy that transforms GitHub repositories into engaging blog posts! Whether you want a creative, story-like narrative or a logical, technical deep-dive, Bloggit has got you covered! 🎨\n\n## ✨ Features\n\n- 🔄 Instant blog generation from GitHub repos\n- 🎭 Two writing styles:\n  - 💫 **Creative Mode**: Casual, fun, and story-driven\n  - 🧠 **Logical Mode**: Technical, precise, and structured\n- 🖼️ AI-generated relevant images for each blog post\n- 📋 Easy copy-paste functionality for Medium.com\n- 🌈 Beautiful Streamlit interface\n\n## 🚀 Getting Started\n\n1. Clone this repository:\n```bash\ngit clone https://github.com/CantCode023/bloggit.git\ncd bloggit\n```\n\n2. Install dependencies:\n```bash\npip install -r requirements.txt\n```\n\n3. Set up your environment variables:\nCreate a `.env` file with:\n```toml\nOPENROUTER_API_KEY=\"your_api_key_here\"\nTOGETHER_API_KEY=\"your_api_key_here\"\n```\n\n4. Run the app:\n```bash\nstreamlit run main.py\n```\n\n## 🎮 How to Use\n\n1. 🔗 Paste your GitHub repository URL\n2. 🎨 Choose your preferred writing style\n3. 🚀 Click \"Blog it!\"\n4. ✨ Watch as Bloggit creates an engaging blog post with a custom image\n5. 📋 Copy and paste to your favorite blogging platform\n\n## 🛠️ Tech Stack\n\n- 🐍 Python\n- 🎈 Streamlit\n- 🤖 OpenRouter API (Gemini Pro)\n- 🖼️ AI Image Generation\n- 🔄 Async Processing\n\n## 💝 Why Bloggit?\n\nEver wanted to share your awesome projects but got stuck staring at a blank page? Bloggit turns your code into stories! Whether you're a dev who loves to write creatively or prefers technical precision, Bloggit adapts to your style! \n\n## 🌟 Contributing\n\nGot ideas to make Bloggit even more awesome? Feel free to:\n- 🐛 Open issues\n- 🎉 Submit PRs\n- 💡 Share feature ideas\n\n## 📜 License\n\nThis project is open source and available under the MIT License.\n\n## 📷 Video Walkthrough\nCheck out this video walkthrough to see Bloggit in action:\n[![Watch the video walkthrough](https://img.youtube.com/vi/h6jdVeV2oBM/maxresdefault.jpg)](https://youtu.be/h6jdVeV2oBM)\n\n---\n\nMade with 💖 by [@bd.](https://github.com/CantCode023)"
    },
    {
      "name": "prabdeb/autogen-m1-security-sample",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/37529789?s=40&v=4",
      "owner": "prabdeb",
      "repo_name": "autogen-m1-security-sample",
      "description": "AutoGen Magentic-One Sample with security best practices.",
      "homepage": "",
      "language": "Python",
      "created_at": "2025-02-20T11:37:08Z",
      "updated_at": "2025-02-28T05:13:40Z",
      "topics": [
        "autogen",
        "autogen-sample",
        "azure-openai",
        "llm",
        "magentic-one",
        "magenticone",
        "openai"
      ],
      "readme": "# AutoGen Magentic-One Sample with Security best practices.\n\n\nThis sample demonstrates how to use the [AutoGen Magentic-One]((https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/magentic-one.html)) with security best practices like:\n\n- **Code execution in Docker**: Making sure the AutoGen m1 can only execute code in a Docker container.\n- **Safeguard File Access**: Making sure the AutoGen m1 can only access the file from a safe directory, in this sample it's the current directory. It can be changed by modifying the logic of safe directory in [_secured_markdown_file_browser.py](./overwrites/_secured_markdown_file_browser.py) file.\n- **Human Oversight for each critical action**: Making sure the AutoGen m1 can only execute code or open a site or access a file after human approval. It can be changed by modifying the description of human agents in [_secured_m1.py](./overwrites/_secured_m1.py) file and in the `next agent` selection prompt in [_secured_m1_orchestrator.py](./overwrites/_secured_m1_orchestrator.py) file.\n- **Termination**: Making sure the AutoGen m1 can be terminated by human agents at any time. It can be changed by modifying the termination command in [_secured_m1.py](./overwrites/_secured_m1.py) file.\n\nFor more information on how to use this sample, please refer:\n\n- [How to Use Magentic One](https://github.com/microsoft/autogen/tree/main/python/packages/magentic-one-cli)\n\n\n## Prerequisites\n\n- Python 3.12 or later\n- Docker Desktop\n- Azure OpenAI Service\n\n## How to run\n\n1. Clone the repository\n1. Install the dependencies\n   ```bash\n   pip install -r requirements.txt\n   ```\n1. Create a `.env` file in the root directory by copying the `.env.example` file and filling in the required values.\n1. Run some of the example with security best practices and see the result\n   ```bash\n   python secured_m1.py --secure \"Find the best restaurant in town\"\n   python secured_m1.py --secure \"Get Microsoft Stock Trend for last 5 years and plot the graph\"\n   python secured_m1.py --secure \"Read the TEST.CSV file from the parent directory and summarize the content\"\n   ```"
    },
    {
      "name": "cwijayasundara/multi_agentic_research",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/3097200?s=40&v=4",
      "owner": "cwijayasundara",
      "repo_name": "multi_agentic_research",
      "description": "some samples with llamaindex multi agetic workflows and autogen 0.4",
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2025-02-20T07:41:28Z",
      "updated_at": "2025-02-21T14:42:42Z",
      "topics": [],
      "readme": "- git clone https://github.com/cwijayasundara/multi_agentic_research.git\n\n- Create a .env file with the below keys\n\nOPENAI_API_KEY='<your open ai key>'\nLLAMA_CLOUD_API_KEY='<your llama cloud api key>'\nTAVILY_API_KEY='<your tavily key>'\n\n- Then run the Python files\n\n"
    },
    {
      "name": "wwwsapir/SoftwareAgents",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/16436675?s=40&v=4",
      "owner": "wwwsapir",
      "repo_name": "SoftwareAgents",
      "description": "A software product company made of AI agents.",
      "homepage": null,
      "language": "Python",
      "created_at": "2025-02-18T15:25:51Z",
      "updated_at": "2025-04-13T10:29:25Z",
      "topics": [],
      "readme": ""
    },
    {
      "name": "anshumankmr/sporky",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/24219264?s=40&v=4",
      "owner": "anshumankmr",
      "repo_name": "sporky",
      "description": "gen ai tool to make spotify playlists",
      "homepage": "",
      "language": "Python",
      "created_at": "2025-01-02T17:40:56Z",
      "updated_at": "2025-04-19T09:44:12Z",
      "topics": [
        "autogen",
        "gen-ai",
        "spotify"
      ],
      "readme": "# Sporky: AI-Powered Spotify Music Recommendation System\n\nSporky is an intelligent music recommendation system that leverages AI agents to provide personalized music suggestions through natural language conversations. Using AutoGen's agent-based architecture, Sporky understands your music preferences and delivers curated Spotify recommendations.\n\n## Features\n\n- **Natural Language Interface**: Describe your music preferences or mood in natural language\n- **Multi-Agent System**: Leverages AutoGen to create a conversation flow between specialized agents\n- **Spotify Integration**: Searches and recommends tracks directly from Spotify's extensive catalog\n- **Conversation History**: Maintains context across sessions for personalized recommendations\n- **Firebase Backend**: Stores conversation history and playlist data securely\n\n## Architecture\n\nThe system is built on a multi-agent architecture:\n\n- **Router Agent**: Analyzes user input and determines the appropriate action\n- **Search Assistant**: Formulates search queries based on user preferences\n- **Spotify Search Agent**: Interfaces with the Spotify API to find matching tracks\n- **Format Assistant**: Structures recommendations in a user-friendly format\n\n## Tech Stack\n\n- **Backend**: FastAPI\n- **Frontend**: Streamlit (current), ShadCDN (planned)\n- **Database**: Firebase Firestore\n- **AI Framework**: AutoGen\n- **LLM Providers**: OpenAI, Groq\n- **Music API**: Spotify\n\n## Prerequisites\n\n- Python 3.9+\n- Spotify Developer Account\n- Firebase Project\n- OpenAI API Key or Groq API Key\n\n## Installation\n\n1. Clone the repository\n```bash\ngit clone https://github.com/yourusername/sporky.git\ncd sporky\n```\n\n2. Install dependencies\n```bash\npip install -r requirements.txt\n```\n\n3. Configure environment variables in `launch.json`\n\n```json\n{\n    \"configurations\": [\n        {\n            \"name\": \"Python: FastAPI\",\n            \"type\": \"python\",\n            \"request\": \"launch\",\n            \"module\": \"uvicorn\",\n            \"args\": [\n                \"app:app\",\n                \"--reload\"\n            ],\n            \"env\": {\n                \"OPENAI_API_KEY\": \"your_openai_api_key\",\n                \"GROQ_API_KEY\": \"your_groq_api_key\",\n                \"SPOTIFY_CLIENT_ID\": \"your_spotify_client_id\",\n                \"SPOTIFY_CLIENT_SECRET\": \"your_spotify_client_secret\",\n                \"FIREBASE_CREDENTIALS_PATH\": \"path/to/firebase-credentials.json\",\n                \"FIREBASE_DB_URL\": \"your_firebase_database_url\"\n            }\n        }\n    ]\n}\n```\n\n## Running the Application\n\n### Backend\n```bash\nuvicorn app:app --reload\n```\n\n### Frontend\n```bash\nstreamlit run chat.py\n```\n\n## API Endpoints\n\n- `POST /query`: Submit a music recommendation query\n\n**Request Body:**\n```json\n{\n  \"query\": \"I need upbeat songs for my morning workout\",\n  \"session_id\": \"user123\",\n  \"state\": [],\n  \"playlist\": \"\"\n}\n```\n\n## Project Structure\n\n```\nsporky/\n├── app.py                  # FastAPI application entry point\n├── agent.py                # Core agent wrapper and orchestration\n├── core/\n│   ├── prompt.py           # Prompt management\n│   ├── spotifyagent.py     # Spotify API agent\n│   └── prompts/            # Directory containing prompt templates\n├── config/\n│   └── llm_config.py       # LLM configuration\n├── tools/\n│   ├── spotify_tools.py    # Spotify API utilities\n│   └── llm_tools.py        # LLM response processing utilities\n└── chat.py                 # Streamlit frontend\n```\n\n## Planned Updates\n\n- ShadCDN frontend implementation\n- Improved playlist generation features\n- User authentication\n- More sophisticated recommendation algorithms\n\n## License\n\nThis project is licensed under the [GNU Affero General Public License v3.0](https://www.gnu.org/licenses/agpl-3.0.en.html) - see the [LICENSE](LICENSE) file for details.\n\nThis license:\n- Requires anyone who distributes this code or a derivative work to make the source available\n- Requires modifications to be released under the same license\n- Applies even when the software is used as a service over a network\n\n## Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.\n\n1. Fork the repository\n2. Create your feature branch (`git checkout -b feature/amazing-feature`)\n3. Commit your changes (`git commit -m 'Add some amazing feature'`)\n4. Push to the branch (`git push origin feature/amazing-feature`)\n5. Open a Pull Request\n\n"
    },
    {
      "name": "tejajuttu/Multi-Agent-Conversations",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/25482892?s=40&v=4",
      "owner": "tejajuttu",
      "repo_name": "Multi-Agent-Conversations",
      "description": "Multi Agent conversations for various tasks",
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2025-02-12T02:56:55Z",
      "updated_at": "2025-02-13T17:46:46Z",
      "topics": [],
      "readme": "# Multi-Agent-Conversations\nMulti Agent conversations for various tasks\n"
    },
    {
      "name": "jakeatmsft/fsi-ai-agent-hackathon",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/47987698?s=40&v=4",
      "owner": "jakeatmsft",
      "repo_name": "fsi-ai-agent-hackathon",
      "description": "Hackathon labs for FSI AI Agent ",
      "homepage": null,
      "language": "Python",
      "created_at": "2025-01-07T15:37:02Z",
      "updated_at": "2025-03-20T18:40:47Z",
      "topics": [],
      "readme": "# Getting started with **Azure AI Agent Service**\n\nThis repo contains scripts and promptflows for deploying and creating AI Agents in both Single Agent and Multi Agent scenarios.  \n\n*** For examples without dependency on [promptflow](https://github.com/microsoft/promptflow), please reference these labs: [fsi-azure-ai-agents-lab](https://github.com/jakeatmsft/fsi-azure-ai-agents-lab).\n\n## Features\n\nThis demo teaches developers how to:\n\n- **Create or Reuse Agents Programmatically**  \n  Demonstrates how to connect to an Azure AI Foundry hub, either create a new agent with customized instructions (using GPT-4o or any supported model), or reuse an existing agent.\n\n- **Incorporate Bing Grounding**  \n  Shows how to add Bing search capabilities to an agent, including how to customize the search query and format the results.    \n\n- **Integrate Server-Side Tools**  \n  Illustrates adding tools—like Bing search, file search, and custom Python functions—into a single `ToolSet`, and how to intercept and log each tool call.\n\n- **Evaluate Agent Performance**  \n  Demonstrates how to evaluate the performance of an agent using a set of questions and answers, and how to visualize the results.\n\nTable of contents:\n\n* [Setting up this project](#setting-up-this-project)\n* [Deploying Single Agent](#deploying-single-agent)\n* [Deploying Multi Agent](#deploying-multi-agent)\n* [Running an evaluation](#running-an-evaluation)\n* [Viewing the results](#viewing-the-results)\n\n## Setting up this project\n### Prerequisites\n- **Python 3.9+**  \n- **Visual Studio Code** with the Python and Jupyter extensions  \n- An **Azure AI Foundry** resource set up (see [Azure AI Agent Service docs](https://learn.microsoft.com/azure/ai-services/agents/))\n- \n### Installation & Setup\n\n1. **Clone** this repository:\n\n   ```bash\n   git clone https://github.com/jakeatmsft/fsi-ai-agent-hackathon.git\n   ```\n\n2. **Create a virtual environment** (using venv as an example):\n\n    ```bash\n    python -m venv .venv\n    ```\n\n3. **Activate** your virtual environment:\n\n    - Windows: `.venv\\Scripts\\activate`\n    - macOS/Linux: `source .venv/bin/activate`\n\n4. **Install** the required dependencies:\n\n    ```bash\n    pip install -r requirements.txt\n    ```\n\n5. **Create a `.env` file** at the root of this folder to store secret keys and settings (e.g., the connection string and optional model name). You can copy the provided `.env.example` file:\n\n    - Windows (PowerShell):\n      ```powershell\n      Copy-Item -Path .env.example -Destination .env\n      ```\n    \n    - macOS/Linux:\n      ```bash\n      cp .env.example .env\n      ```\n\n    Then, open the `.env` file and update it with your configuration details.\n\n    - Add your [Azure AI Foundry](https://learn.microsoft.com/azure/ai-services/agents/quickstart?pivots=programming-language-python-azure#configure-and-run-an-agent) connection string:\n        ```plaintext\n        PROJECT_CONNECTION_STRING=\"<HostName>;<AzureSubscriptionId>;<ResourceGroup>;<ProjectName>\"\n        ```\n\n    - Specify the [compatible model](https://learn.microsoft.com/azure/ai-services/agents/how-to/tools/bing-grounding?tabs=python&pivots=overview#setup) you want to use (e.g. GPT-4o):\n        ```plaintext\n        MODEL_NAME=\"YOUR_MODEL_NAME\"\n        ```\n\n    - **Grounding with Bing**\n            \n    ## Setup  \n    \n    > [!IMPORTANT]\n    > 1. Grounding with Bing Search has a separate [Terms of Use agreement](https://www.microsoft.com/en-us/bing/apis/grounding-legal-preview) you need to agree to in order to move forward. Please [use this form](https://forms.office.com/r/2j3Sgu8S9K) to sign the agreement. After you have signed the form, it will take 1-3 days for us to whitelist your subscription.\n    > 2. Please make sure your resource is created in `EastUS`.\n    > 3. We recommend using the following models: `gpt-3.5-turbo-0125`, `gpt-4-0125-preview`, `gpt-4-turbo-preview`, `gpt-4-turbo`, `gpt-4-turbo-2024-04-09`, `gpt-4o`, `gpt-4o-mini`, `gpt-4o-mini-2024-07-18`\n    \n    \n    1. Ensure you have logged in to Azure, using `az login`\n    \n    2. Register the Bing Search provider\n       ```console\n           az provider register --namespace 'Microsoft.Bing'\n       ```\n    \n    3. Create a new Grounding with Bing Search resource using one of the methods below:\n        - [Create Bing resource in Azure](https://ms.portal.azure.com/#create/Microsoft.BingGroundingSearch) \n        - AI Foundry Studio [AI Foundry Setup](https://learn.microsoft.com/en-us/azure/ai-services/agents/how-to/tools/bing-grounding?tabs=python&pivots=overview#setup)\n  \n    4. After you have created a Grounding with Bing Search resource, you can find it in [Azure Portal](https://ms.portal.azure.com/#home). Going to the resource group you have created the resource at, search for the Grounding with Bing Search resource you have created.\n    ![alt text](assets/383259479-3b22c48d-987c-4234-a9eb-67aefe3af81c.png)\n    5. Click the Grounding with Bing Search resource you have created and copy any of the API key\n    ![alt text](assets/383260123-be98e07d-c91d-4ff9-a97c-6f02c3265221.png)\n    6. Go to [Azure AI Studio](https://ai.azure.com/) and select the AI Project(make sure it's in the same resource group of your Grounding with Bing Search resource). Click Settings and then \"+new connection\" button in Settings page\n    ![alt text](assets/383261431-28bfebda-f3a4-4638-b714-a128a8fa48cb.png)\n    ![alt text](assets/383261546-7bb9c98e-dd46-4031-be9d-17c70613f222.png)\n    7. Select \"API key\" custom connection in other resource types\n    ![alt text](assets/383261772-7577c912-cf0f-433a-910b-3d9e0ad138c4.png)\n    8. Enter the following information and then create a new connection to your Grounding with Bing Search resource\n    - Endpoint: https://api.bing.microsoft.com/\n    - Key: YOUR_API_KEY\n    - Connection name: YOUR_CONNECTION_NAME (You will use this connection name in the sample code below.)\n    - Access: you can choose either \"this project only\" or \"shared to all projects\". Just make sure in the sample code below, the project you entered connection string for has access to this connection.\n\n\n## Deploying Single Agent\n![alt text](assets/150336.png)\nFollow instructions in [1-single-agent](./1-ai_foundry_agent/README.md) to deploy a single agent.\n\n## Deploying Multi Agent\n![alt text](assets/150919.png)\nFollow instructions in [2-ai_multiagent](./2-ai_multiagent/README.md) to deploy an agent team.\n\n\n## Running an evaluation\nWe provide a script that loads in the current `azd` environment's variables, installs the requirements for the evaluation, and runs the evaluation against the local app. Run it like this:\n\n```shell\npython -m scripts evaluate --config=example_config.json\n```\n\nThe config.json should contain these fields as a minimum:\n\n```json\n{\n    \"testdata_path\": \"example_input/qa.jsonl\",\n    \"target_url\": \"http://localhost:50505/chat\",\n    \"requested_metrics\": [\"groundedness\", \"relevance\", \"coherence\", \"latency\", \"answer_length\"],\n    \"results_dir\": \"example_results/experiment<TIMESTAMP>\"\n}\n```\n\n### Running against a local container\n\nIf you're running this evaluator in a container and your app is running in a container on the same system, use a URL like this for the `target_url`:\n\n\"target_url\": \"http://host.docker.internal:50505/chat\"\n\n### Running against a deployed app\n\nTo run against a deployed endpoint, change the `target_url` to the chat endpoint of the deployed app:\n\n\"target_url\": \"https://app-backend-j25rgqsibtmlo.azurewebsites.net/chat\"\n\n### Running on a subset of questions\n\nIt's common to run the evaluation on a subset of the questions, to get a quick sense of how the changes are affecting the answers. To do this, use the `--numquestions` parameter:\n\n```shell\npython -m scripts evaluate --config=example_config.json --numquestions=2\n```\n\n### Specifying the evaluate metrics\n\nThe `evaluate` command will use the metrics specified in the `requested_metrics` field of the config JSON.\nSome of those metrics are built-in to the evaluation SDK, and the rest are custom metrics that we've added.\n\n#### Built-in metrics\n\nThese metrics are calculated by sending a call to the GPT model, asking it to provide a 1-5 rating, and storing that rating.\n\n> [!IMPORTANT]\n> The built-in metrics are only intended for use on evaluating English language answers, since they use English-language prompts internally. For non-English languages, you should use the [custom prompt metrics](#prompt-metrics) instead.\n\n* [`gpt_coherence`](https://learn.microsoft.com/azure/ai-studio/concepts/evaluation-metrics-built-in#ai-assisted-coherence) measures how well the language model can produce output that flows smoothly, reads naturally, and resembles human-like language.\n* [`gpt_relevance`](https://learn.microsoft.com/azure/ai-studio/concepts/evaluation-metrics-built-in#ai-assisted-relevance) assesses the ability of answers to capture the key points of the context.\n* [`gpt_groundedness`](https://learn.microsoft.com/azure/ai-studio/concepts/evaluation-metrics-built-in#ai-assisted-groundedness) assesses the correspondence between claims in an AI-generated answer and the source context, making sure that these claims are substantiated by the context.\n* [`gpt_similarity`](https://learn.microsoft.com/azure/ai-studio/concepts/evaluation-metrics-built-in#ai-assisted-gpt-similarity) measures the similarity between a source data (ground truth) sentence and the generated response by an AI model.\n* [`gpt_fluency`](https://learn.microsoft.com/azure/ai-studio/concepts/evaluation-metrics-built-in#ai-assisted-fluency) measures the grammatical proficiency of a generative AI's predicted answer.\n* [`f1_score`](https://learn.microsoft.com/azure/ai-studio/concepts/evaluation-metrics-built-in#traditional-machine-learning-f1-score) Measures the ratio of the number of shared words between the model generation and the ground truth answers.\n\n#### Custom metrics\n\n##### Prompt metrics\n\nThe following metrics are implemented very similar to the built-in metrics, but use a locally stored prompt. They're a great fit if you find that the built-in metrics are not working well for you or if you need to translate the prompt to another language.\n\n* `mycoherence`: Measures how well the language model can produce output that flows smoothly, reads naturally, and resembles human-like language. Based on `scripts/evaluate_metrics/prompts/coherence.prompty`.\n* `myrelevance`: Assesses the ability of answers to capture the key points of the context. Based on `scripts/evaluate_metrics/prompts/relevance.prompty`.\n* `mygroundedness`: Assesses the correspondence between claims in an AI-generated answer and the source context, making sure that these claims are substantiated by the context. Based on `scripts/evaluate_metrics/prompts/groundedness.prompty`.\n\n##### Code metrics\n\nThese metrics are calculated with some local code based on the results of the chat app, and do not require a call to the GPT model.\n\n* `latency`: The time it takes for the chat app to generate an answer, in seconds.\n* `length`: The length of the generated answer, in characters.\n* `has_citation`: Whether the answer contains a correctly formatted citation to a source document, assuming citations are in square brackets.\n* `citation_match`: Whether the answer contains at least all of the citations that were in the ground truth answer.\n\n### Sending additional parameters to the app\n\nThis repo assumes that your chat app is following the [AI Chat Protocol](https://github.com/microsoft/ai-chat-protocol/tree/main/spec#readme), which means that all POST requests look like this:\n\n```json\n{\"messages\": [{\"content\": \"<Actual user question goes here>\", \"role\": \"user\"}],\n \"context\": {...},\n}\n```\n\nAny additional app parameters would be specified in the `context` of that JSON, such as temperature, search settings, prompt overrides, etc. To specify those parameters, add a `target_parameters` key to your config JSON. For example:\n\n```json\n    \"target_parameters\": {\n        \"overrides\": {\n            \"semantic_ranker\": false,\n            \"prompt_template\": \"<READFILE>example_input/prompt_refined.txt\"\n        }\n    }\n```\n\nThe `overrides` key is the same as the `overrides` key in the `context` of the POST request.\nAs a convenience, you can use the `<READFILE>` prefix to read in a file and use its contents as the value for the parameter.\nThat way, you can store potential (long) prompts separately from the config JSON file.\n\n### Specifying the location of answer and context in response\n\nThe evaluator needs to know where to find the answer and context in the response from the chat app.\nIf your app returns responses following the recommendations of the [AI Chat Protocol](https://github.com/microsoft/ai-chat-protocol/tree/main/spec#readme), then the answer will be \"message\": \"content\" and the context will be a list of strings in \"context\": \"data_points\": \"text\".\n\nIf your app returns responses in a different format, you can specify the [JMESPath expressions](https://jmespath.org/) to extract the answer and context from the response. For example:\n\n```json\n    \"target_response_answer_jmespath\": \"message.content\",\n    \"target_response_context_jmespath\": \"context.data_points.text\"\n```\n\n## Viewing the results\n\nThe results of each evaluation are stored in a results folder (defaulting to `example_results`).\nInside each run's folder, you'll find:\n\n* `eval_results.jsonl`: Each question and answer, along with the GPT metrics for each QA pair.\n* `parameters.json`: The parameters used for the run, like the overrides.\n* `summary.json`: The overall results, like the average GPT metrics.\n* `config.json`: The original config used for the run. This is useful for reproducing the run.\n\nTo make it easier to view and compare results across runs, we've built a few tools,\nlocated inside the `review-tools` folder.\n\n### Using the summary tool\n\nTo view a summary across all the runs, use the `summary` command with the path to the results folder:\n\n```bash\npython -m review_tools summary example_results\n```\n\nThis will display an interactive table with the results for each run, like this:\n\n![Screenshot of CLI tool with table of results](docs/screenshot_summary.png)\n\nTo see the parameters used for a particular run, select the folder name.\nA modal will appear with the parameters, including any prompt override.\n\n### Using the compare tool\n\nTo compare the answers generated for each question across 2 runs, use the `compare` command with 2 paths:\n\n```bash\npython -m review_tools diff example_results/baseline_1 example_results/baseline_2\n```\n\nThis will display each question, one at a time, with the two generated answers in scrollable panes,\nand the GPT metrics below each answer.\n\n![Screenshot of CLI tool for comparing a question with 2 answers](docs/screenshot_compare.png)]\n\nUse the buttons at the bottom to navigate to the next question or quit the tool.\n\nYou can also filter to only show questions where the value changed for a particular metric, like this:\n\n```bash\npython -m review_tools diff example_results/baseline_1 example_results/baseline_2 --changed=has_citation\n```\n\n## Measuring app's ability to say \"I don't know\"\n\nThe evaluation flow described above focused on evaluating a model’s answers for a set of questions that *could* be answered by the data. But what about all those questions that can’t be answered by the data? Does your model know how to say “I don’t know?” The GPT models are trained to try and be helpful, so their tendency is to always give some sort of answer, especially for answers that were in their training data. If you want to ensure your app can say “I don’t know” when it should, you need to evaluate it on a different set of questions with a different metric.\n\n### Generating ground truth data for answer-less questions\n\nFor this evaluation, our ground truth data needs to be a set of question whose answer should provoke an \"I don’t know\" response from the data. There are several categories of such questions:\n\n* **Unknowable**: Questions that are related to the sources but not actually in them (and not public knowledge).\n* **Uncitable**: Questions whose answers are well known to the LLM from its training data, but are not in the sources. There are two flavors of these:\n  * **Related**: Similar topics to sources, so LLM will be particularly tempted to think the sources know.\n  * **Unrelated**: Completely unrelated to sources, so LLM shouldn’t be as tempted to think the sources know.\n* **Nonsensical**: Questions that are non-questions, that a human would scratch their head at and ask for clarification.\n\nYou can write these questions manually, but it’s also possible to generate them using a generator script in this repo,\nassuming you already have ground truth data with answerable questions.\n\n```shell\npython -m scripts generate-dontknows --input=example_input/qa.jsonl --output=example_input/qa_dontknows.jsonl --numquestions=45\n```\n\nThat script sends the current questions to the configured GPT-4 model along with prompts to generate questions of each kind.\n\nWhen it’s done, you should review and curate the resulting ground truth data. Pay special attention to the \"unknowable\" questions at the top of the file, since you may decide that some of those are actually knowable, and you may want to reword or rewrite entirely.\n\n### Running an evaluation for answer-less questions\n\nThis repo contains a custom GPT metric called \"dontknowness\" that rates answers from 1-5, where 1 is \"answered the question completely with no certainty\" and 5 is \"said it didn't know and attempted no answer\". The goal is for all answers to be rated 4 or 5.\n\nHere's an example configuration JSON that requests that metric, referencing the new ground truth data and a new output folder:\n\n```json\n{\n    \"testdata_path\": \"example_input/qa_dontknows.jsonl\",\n    \"results_dir\": \"example_results_dontknows/baseline\",\n    \"requested_metrics\": [\"dontknowness\", \"answer_length\", \"latency\", \"has_citation\"],\n    \"target_url\": \"http://localhost:50505/chat\",\n    \"target_parameters\": {\n    },\n    \"target_response_answer_jmespath\": \"message.content\",\n    \"target_response_context_jmespath\": \"context.data_points.text\"\n}\n```\n\nWe recommend a separate output folder, as you'll likely want to make multiple runs and easily compare between those runs using the [review tools](#viewing-the-results).\n\nRun the evaluation like this:\n\n```shell\npython -m scripts evaluate --config=dontknows.config.json\n```\n\nThe results will be stored in the `results_dir` folder, and can be reviewed using the [review tools](#viewing-the-results).\n\n### Improving the app's ability to say \"I don't know\"\n\nIf the app is not saying \"I don't know\" enough, you can use the `diff` tool to compare the answers for the \"dontknows\" questions across runs, and see if the answers are improving. Changes you can try:\n\n* Adjust the prompt to encourage the model to say \"I don't know\" more often. Remove anything in the prompt that might be distracting or overly encouraging it to answer.\n* Try using GPT-4 instead of GPT-3.5. The results will be slower (see the latency column) but it may be more likely to say \"I don't know\" when it should.\n* Adjust the temperature of the model used by your app.\n* Add an additional LLM step in your app after generating the answer, to have the LLM rate its own confidence that the answer is found in the sources. If the confidence is low, the app should say \"I don't know\".\n"
    },
    {
      "name": "rohanmuz2/autogen-blog-post-creator",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/28555930?s=40&v=4",
      "owner": "rohanmuz2",
      "repo_name": "autogen-blog-post-creator",
      "description": "Autogen Blog Post Creator",
      "homepage": null,
      "language": "Python",
      "created_at": "2025-02-09T01:06:28Z",
      "updated_at": "2025-03-15T09:19:04Z",
      "topics": [],
      "readme": ""
    },
    {
      "name": "AliciaSJF/autogen",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/132434651?s=40&v=4",
      "owner": "AliciaSJF",
      "repo_name": "autogen",
      "description": null,
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2025-02-06T22:50:07Z",
      "updated_at": "2025-02-10T17:37:58Z",
      "topics": [],
      "readme": "# autogen\n"
    },
    {
      "name": "kaljuvee/agentic",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/539240?s=40&v=4",
      "owner": "kaljuvee",
      "repo_name": "agentic",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2024-12-21T18:35:11Z",
      "updated_at": "2025-03-21T09:23:14Z",
      "topics": [],
      "readme": "# agentic"
    },
    {
      "name": "benediktstroebl/agent-zoo",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/50178209?s=40&v=4",
      "owner": "benediktstroebl",
      "repo_name": "agent-zoo",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2025-01-05T16:14:18Z",
      "updated_at": "2025-02-03T20:23:44Z",
      "topics": [],
      "readme": "\n"
    },
    {
      "name": "mark-watson/OllamaExamples",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/33912?s=40&v=4",
      "owner": "mark-watson",
      "repo_name": "OllamaExamples",
      "description": "Private Ollama tool experiments",
      "homepage": null,
      "language": "Python",
      "created_at": "2024-12-16T00:43:16Z",
      "updated_at": "2025-04-17T21:20:22Z",
      "topics": [],
      "readme": "# Ollama-book-examples\nExamples for my Ollama LLM AI book https://leanpub.com/ollama\n\nNote: smolagents requires Python3.11 (or lower version) so set python version:\n\n```\n$ python3.11 -m venv venv\npip install -r requirements.txt\nsource venv/bin/activate\n```\n\n\n"
    },
    {
      "name": "kenilp7/Meal-Planning-AI-Assistants",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/108230926?s=40&v=4",
      "owner": "kenilp7",
      "repo_name": "Meal-Planning-AI-Assistants",
      "description": "An AI-powered meal planning assistant that generates customized daily meal plans based on user-defined budgets. Built using AutoGen, OpenAI and Groq API, this system leverages AI agents to suggest cost-effective meals while ensuring nutritional balance.",
      "homepage": "",
      "language": "Python",
      "created_at": "2025-01-30T20:50:51Z",
      "updated_at": "2025-01-30T21:32:58Z",
      "topics": [],
      "readme": "# Meal Planning AI Assistant\n\n## 📌 Overview\n\nThe **Meal Planning AI Assistant** is a multi-agent AI system designed to help users generate meal plans while ensuring they stay within a given budget. This project leverages OpenAI's GPT-based models to provide meal suggestions and validate budget constraints using intelligent function tools.\n\n## 📜 Table of Contents\n\n- [Installation](#-installation)\n- [Configuration](#-configuration)\n- [Agents](#-agents)\n- [Tools](#-tools)\n- [Usage](#-usage)\n- [API Reference](#-api-reference)\n- [Improvements & Future Enhancements](#-improvements--future-enhancements)\n- [Contributing](#-contributing)\n- [License](#-license)\n\n---\n\n## 🚀 Installation\n\n### 1️⃣ Clone the Repository\n\n```sh\ngit clone https://github.com/kenilp7/Meal-Planning-AI-Assistant.git\ncd meal-planning-ai-assistant\n```\n\n### 2️⃣ Create & Activate Virtual Environment\n\n```sh\npython -m venv .venv\n.venv\\Scripts\\activate  # Windows\nsource .venv/bin/activate  # Mac/Linux\n```\n\n### 3️⃣ Install Dependencies\n\n```sh\npip install -r requirements.txt\n```\n\n---\n\n## 🔧 Configuration\n\nBefore running the assistant, set up your **GROQ API Key**:\n\n```python\nimport os\nfrom getpass import getpass\n\nos.environ[\"GROQ_API_KEY\"] = getpass(\"Enter GROQ_API_KEY here: \")\n```\n\nAlternatively, set it as an environment variable:\n\n```sh\nexport GROQ_API_KEY=\"your_api_key\"\n```\n\n---\n\n## 🤖 Agents\n\nThis system includes five AI agents, each designed for specific tasks:\n\n| Agent              | Description                                                      |\n| ------------------ | ---------------------------------------------------------------- |\n| `BudgetAssistant`  | Ensures meal plans fit within budget constraints.                |\n| `BreakfastPlanner` | Suggests breakfast options based on user preferences and budget. |\n| `LunchPlanner`     | Generates lunch recommendations with budget awareness.           |\n| `DinnerPlanner`    | Plans dinner while maintaining cost-effectiveness.               |\n| `SnackPlanner`     | Provides snack suggestions within budget.                        |\n\n---\n\n## 🛠️ Tools\n\nThe project includes built-in tools that enhance the AI's functionality.\n\n### **Budget Checker Tool**\n\nA utility to verify if an expense fits within a given budget.\n\n```python\ndef budget_checker_tool(cost: float, current_budget: float) -> dict:\n```\n\n#### **Returns**\n\n```json\n{\n  \"fits_budget\": true,\n  \"message\": \"The cost of $50.00 fits within your budget.\"\n}\n```\n\n---\n\n## 🎯 Usage\n\nRun the meal planning assistant interactively:\n\n```sh\npython main.py\n```\n\n### **Example Interaction**\n\n```\nUser: Plan my meals for the day within a $20 budget.\nAssistant: Here’s a plan:\n- Breakfast: Oatmeal with banana - $3.50\n- Lunch: Grilled chicken salad - $7.00\n- Dinner: Vegetable stir-fry - $6.00\n- Snacks: Apple slices - $2.00\nTotal: $18.50 (within budget)\n```\n\n---\n\n## 📡 API Reference\n\n### **Meal Planning Agents**\n\nEach agent responds with meal suggestions based on dietary needs and budget.\n\n```python\nbreakfast_agent.system_message\nlunch_agent.system_message\ndinner_agent.system_message\n```\n\n### **Group Chat Management**\n\nManages agent discussions via a round-robin system.\n\n```python\nfrom autogen_agentchat.teams import RoundRobinGroupChat\nteam = RoundRobinGroupChat(agents=[budget_agent, breakfast_agent, lunch_agent, dinner_agent, snack_agent])\n```\n\n---\n\n## 🚀 Improvements & Future Enhancements\n\n- ✅ **User Preferences**: Add dietary restrictions (vegan, keto, etc.).\n- ✅ **Database Integration**: Store user preferences and meal history.\n- ✅ **Multi-Language Support**: Expand agent capabilities to different languages.\n- ✅ **Mobile-Friendly Interface**: Build a web or mobile UI for meal planning.\n\n---\n\n## 👥 Contributing\n\nContributions are always welcome! To contribute:\n\n1. Fork the repository.\n2. Create a feature branch.\n3. Commit your changes.\n4. Open a Pull Request.\n\n---\n\n## 📜 License\n\nThis project is licensed under the **MIT License**.\n\n"
    },
    {
      "name": "rustatian/airope",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/8040338?s=40&v=4",
      "owner": "rustatian",
      "repo_name": "airope",
      "description": "My personal project to locally recognize, structure and categorize documents [with local db].",
      "homepage": null,
      "language": "Python",
      "created_at": "2025-01-25T06:44:35Z",
      "updated_at": "2025-03-29T21:03:59Z",
      "topics": [],
      "readme": "# AiRope\n\n## Description\nThe idea is pretty simple; you have a locally (with an OpenAI, Claude, etc. support) installed AI application that\nhelps you with:\n1. **Work with documents**: save documents to the local database with tags, search, reminders, etc.\n2. **Work with code**: [TODO, but, connect the folder, sync changes with AI, get realtime suggestions (like testing, for example)]\n3. **RAG**, retrieval augment generation.\n4. **Desktop UI** based on Tkinter."
    },
    {
      "name": "codeh007/mtmai",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/95907738?s=40&v=4",
      "owner": "codeh007",
      "repo_name": "mtmai",
      "description": "mtmai",
      "homepage": null,
      "language": "Roff",
      "created_at": "2025-01-11T04:18:15Z",
      "updated_at": "2025-04-23T08:40:15Z",
      "topics": [],
      "readme": "# mtmai\nmtmai\n"
    },
    {
      "name": "thedigitalworkplace/Autogen",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/133930737?s=40&v=4",
      "owner": "thedigitalworkplace",
      "repo_name": "Autogen",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2025-01-15T06:54:35Z",
      "updated_at": "2025-01-22T04:51:38Z",
      "topics": [],
      "readme": "<a name=\"readme-top\"></a>\n\n<div align=\"center\">\n<img src=\"https://microsoft.github.io/autogen/0.2/img/ag.svg\" alt=\"AutoGen Logo\" width=\"100\">\n\n[![Twitter](https://img.shields.io/twitter/url/https/twitter.com/cloudposse.svg?style=social&label=Follow%20%40pyautogen)](https://twitter.com/pyautogen)\n[![LinkedIn](https://img.shields.io/badge/LinkedIn-Company?style=flat&logo=linkedin&logoColor=white)](https://www.linkedin.com/company/105812540)\n[![Discord](https://img.shields.io/badge/discord-chat-green?logo=discord)](https://aka.ms/autogen-discord)\n[![Documentation](https://img.shields.io/badge/Documentation-AutoGen-blue?logo=read-the-docs)](https://microsoft.github.io/autogen/)\n\n</div>\n\n# AutoGen\n\n**AutoGen** is a framework for creating multi-agent AI applications that can act autonomously or work alongside humans.\n\n## Installation\n\n```bash\n# Install AgentChat and OpenAI client from Extensions\npip install -U \"autogen-agentchat\" \"autogen-ext[openai]\"\n```\n\nThe current stable version is v0.4. If you are upgrading from AutoGen v0.2, please refer to the [Migration Guide](https://microsoft.github.io/autogen/dev/user-guide/agentchat-user-guide/migration-guide.html) for detailed instructions on how to update your code and configurations.\n\n### Hello World\n\nCreate an assistant agent using OpenAI's GPT-4o model.\n\n```python\nimport asyncio\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n\nasync def main() -> None:\n    agent = AssistantAgent(\"assistant\", OpenAIChatCompletionClient(model=\"gpt-4o\"))\n    print(agent.run(task=\"Say 'Hello World!'\"))\n\nasyncio.run(main())\n```\n\n### Team\n\nCreate a group chat team with an assistant agent, a web surfer agent, and a user proxy agent\nfor web browsing tasks. You need to install [playwright](https://playwright.dev/python/docs/library).\n\n```python\n# pip install -U autogen-agentchat autogen-ext[openai,web-surfer]\n# playwright install\nimport asyncio\nfrom autogen_agentchat.agents import AssistantAgent, UserProxyAgent\nfrom autogen_agentchat.conditions import TextMentionTermination\nfrom autogen_agentchat.teams import RoundRobinGroupChat\nfrom autogen_agentchat.ui import Console\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\nfrom autogen_ext.agents.web_surfer import MultimodalWebSurfer\n\nasync def main() -> None:\n    model_client = OpenAIChatCompletionClient(model=\"gpt-4o\")\n    assistant = AssistantAgent(\"assistant\", model_client)\n    web_surfer = MultimodalWebSurfer(\"web_surfer\", model_client)\n    user_proxy = UserProxyAgent(\"user_proxy\")\n    termination = TextMentionTermination(\"exit\") # Type 'exit' to end the conversation.\n    team = RoundRobinGroupChat([web_surfer, assistant, user_proxy], termination_condition=termination)\n    await Console(team.run_stream(task=\"Find information about AutoGen and write a short summary.\"))\n\nasyncio.run(main())\n```\n\n## Why Use AutoGen?\n\n<div align=\"center\">\n  <img src=\"autogen-landing.jpg\" alt=\"AutoGen Landing\" width=\"500\">\n</div>\n\nThe AutoGen ecosystem provides everything you need to create AI agents, especially multi-agent workflows -- framework, developer tools, and applications.\n\nThe _framework_ uses a layered and extensible design. Layers have clearly divided responsibilities and build on top of layers below. This design enables you to use the framework at different levels of abstraction, from high-level APIs to low-level components.\n\n- [Core API](./python/packages/autogen-core/) implements message passing, event-driven agents, and local and distributed runtime for flexibility and power. It also support cross-language support for .NET and Python.\n- [AgentChat API](./python/packages/autogen-agentchat/) implements a simpler but opinionated API rapid for prototyping. This API is built on top of the Core API and is closest to what users of v0.2 are familiar with and supports familiar multi-agent patterns such as two-agent chat or group chats.\n- [Extensions API](./python/packages/autogen-ext/) enables first- and third-party extensions continuously expanding framework capabilities. It support specific implementation of LLM clients (e.g., OpenAI, AzureOpenAI), and capabilities such as code execution.\n\nThe ecosystem also supports two essential _developer tools_:\n\n<div align=\"center\">\n  <img src=\"https://media.githubusercontent.com/media/microsoft/autogen/refs/heads/main/python/packages/autogen-studio/docs/ags_screen.png\" alt=\"AutoGen Studio Screenshot\" width=\"500\">\n</div>\n\n- [AutoGen Studio](./python/packages/autogen-studio/) provides a no-code GUI for building multi-agent applications.\n- [AutoGen Bench](./python/packages/agbench/) provides a benchmarking suite for evaluating agent performance.\n\nYou can use the AutoGen framework and developer tools to create applications for your domain. For example, [Magentic-One](./python/packages/magentic-one-cli/) is a state-of-art multi-agent team built using AgentChat API and Extensions API that can handle variety of tasks that require web browsing, code execution, and file handling.\n\nWith AutoGen you get to join and contribute to a thriving ecosystem. We host weekly office hours and talks with maintainers and community. We also have a [Discord server](https://aka.ms/autogen-discord) for real-time chat, GitHub Discussions for Q&A, and a blog for tutorials and updates.\n\n## Where to go next?\n\n<div align=\"center\">\n\n|               | [![Python](https://img.shields.io/badge/AutoGen-Python-blue?logo=python&logoColor=white)](./python)                                                                                                                                                                                                                                                                                                                | [![.NET](https://img.shields.io/badge/AutoGen-.NET-green?logo=.net&logoColor=white)](./dotnet) | [![Studio](https://img.shields.io/badge/AutoGen-Studio-purple?logo=visual-studio&logoColor=white)](./python/packages/autogen-studio)                     |\n| ------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | ---------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| Installation  | [![Installation](https://img.shields.io/badge/Install-blue)](https://microsoft.github.io/autogen/dev/user-guide/agentchat-user-guide/installation.html)                                                                                                                                                                                                                                                            | \\*                                                                                             | [![Install](https://img.shields.io/badge/Install-purple)](https://microsoft.github.io/autogen/dev/user-guide/autogenstudio-user-guide/installation.html) |\n| Quickstart    | [![Quickstart](https://img.shields.io/badge/Quickstart-blue)](https://microsoft.github.io/autogen/dev/user-guide/agentchat-user-guide/quickstart.html#)                                                                                                                                                                                                                                                            | \\*                                                                                             | [![Usage](https://img.shields.io/badge/Quickstart-blue)](https://microsoft.github.io/autogen/dev/user-guide/autogenstudio-user-guide/usage.html#)        |\n| Tutorial      | [![Tutorial](https://img.shields.io/badge/Tutorial-blue)](https://microsoft.github.io/autogen/dev/user-guide/agentchat-user-guide/tutorial/models.html)                                                                                                                                                                                                                                                            | \\*                                                                                             | [![Usage](https://img.shields.io/badge/Quickstart-blue)](https://microsoft.github.io/autogen/dev/user-guide/autogenstudio-user-guide/usage.html#)        |\n| API Reference | [![API](https://img.shields.io/badge/Docs-blue)](https://microsoft.github.io/autogen/dev/reference/index.html#)                                                                                                                                                                                                                                                                                                    | \\*                                                                                             | [![API](https://img.shields.io/badge/Docs-purple)](https://microsoft.github.io/autogen/dev/user-guide/autogenstudio-user-guide/usage.html)               |\n| Packages      | [![PyPi autogen-core](https://img.shields.io/badge/PyPi-autogen--core-blue?logo=pypi)](https://pypi.org/project/autogen-core/) <br> [![PyPi autogen-agentchat](https://img.shields.io/badge/PyPi-autogen--agentchat-blue?logo=pypi)](https://pypi.org/project/autogen-agentchat/) <br> [![PyPi autogen-ext](https://img.shields.io/badge/PyPi-autogen--ext-blue?logo=pypi)](https://pypi.org/project/autogen-ext/) | \\*                                                                                             | [![PyPi autogenstudio](https://img.shields.io/badge/PyPi-autogenstudio-purple?logo=pypi)](https://pypi.org/project/autogenstudio/)                       |\n\n</div>\n\n\\*_Releasing soon_\n\nInterested in contributing? See [CONTRIBUTING.md](./CONTRIBUTING.md) for guidelines on how to get started. We welcome contributions of all kinds, including bug fixes, new features, and documentation improvements. Join our community and help us make AutoGen better!\n\nHave questions? Check out our [Frequently Asked Questions (FAQ)](./FAQ.md) for answers to common queries. If you don't find what you're looking for, feel free to ask in our [GitHub Discussions](https://github.com/microsoft/autogen/discussions) or join our [Discord server](https://aka.ms/autogen-discord) for real-time support.\n\n## Legal Notices\n\nMicrosoft and any contributors grant you a license to the Microsoft documentation and other content\nin this repository under the [Creative Commons Attribution 4.0 International Public License](https://creativecommons.org/licenses/by/4.0/legalcode),\nsee the [LICENSE](LICENSE) file, and grant you a license to any code in the repository under the [MIT License](https://opensource.org/licenses/MIT), see the\n[LICENSE-CODE](LICENSE-CODE) file.\n\nMicrosoft, Windows, Microsoft Azure, and/or other Microsoft products and services referenced in the documentation\nmay be either trademarks or registered trademarks of Microsoft in the United States and/or other countries.\nThe licenses for this project do not grant you rights to use any Microsoft names, logos, or trademarks.\nMicrosoft's general trademark guidelines can be found at <http://go.microsoft.com/fwlink/?LinkID=254653>.\n\nPrivacy information can be found at <https://go.microsoft.com/fwlink/?LinkId=521839>\n\nMicrosoft and any contributors reserve all other rights, whether under their respective copyrights, patents,\nor trademarks, whether by implication, estoppel, or otherwise.\n\n<p align=\"right\" style=\"font-size: 14px; color: #555; margin-top: 20px;\">\n  <a href=\"#readme-top\" style=\"text-decoration: none; color: blue; font-weight: bold;\">\n    ↑ Back to Top ↑\n  </a>\n</p>\n"
    },
    {
      "name": "DanielPuentee/autogen-groq-multiagent-analyser",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/64250734?s=40&v=4",
      "owner": "DanielPuentee",
      "repo_name": "autogen-groq-multiagent-analyser",
      "description": "A step-by-step tutorial on building an AI Agent system to recommend courses using AutoGen and LLaMA. Learn how to create autonomous agents that analyze CVs, extract key skills, and suggest personalized courses for skill development. ",
      "homepage": "https://www.linkedin.com/in/danielpuenteviejo/",
      "language": "Jupyter Notebook",
      "created_at": "2025-01-13T21:49:51Z",
      "updated_at": "2025-03-31T21:35:56Z",
      "topics": [
        "autogen",
        "genai-agents",
        "python"
      ],
      "readme": "# Autogen Multi-Agent Ecosystem: Profile Analysis with Free LLMs\n\n<img title=\"Logo\" alt=\"Alt text\" src=\"/imgs/cover_page.png\">\n\n![python](https://img.shields.io/badge/Python-v3.10.15-blueviolet)\n![GitHub top language](https://img.shields.io/github/languages/top/DanielPuentee/autogen-groq-multiagent-analyser)\n[![Visual Studio Code](https://custom-icon-badges.demolab.com/badge/Visual%20Studio%20Code-0078d7.svg?logo=vsc&logoColor=white)](#)\n[![Python](https://img.shields.io/badge/Python-3776AB?logo=python&logoColor=fff)](#)\n[![Anaconda](https://img.shields.io/badge/Anaconda-44A833?logo=anaconda&logoColor=fff)](#)\n![GitHub last commit](https://img.shields.io/github/last-commit/DanielPuentee/autogen-groq-multiagent-analyser)\n[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)\n[![GitHub Copilot](https://img.shields.io/badge/GitHub%20Copilot-000?logo=githubcopilot&logoColor=fff)](#)\n\n> Author: [Daniel Puente Viejo](https://www.linkedin.com/in/danielpuenteviejo/)\n\n## Description\nThis repository demonstrates how to build and execute a multi-agent system using **Autogen** and free LLMs with **Groq**. It serves as a step-by-step tutorial to help users understand the basics of multi-agent systems, tool integration, and hybrid agent selection methods.\n\n---\n\n## Features\n- **Multi-Agent Setup**: Includes agents for analyzing CVs, extracting skills, recommending courses, and verifying task completion.\n- **Hybrid Agent Selection**: Implements a custom method for dynamically selecting the next agent based on the task flow.\n- **Tool Integration**: Demonstrates how to use external tools for processing tasks.\n- **Custom Configuration**: Allows experimentation with LLM configurations like `llama31` and `gemma2` for optimized performance.\n\n---\n\n## Requirements\n- Python used version: **3.10.15**\n- Dependencies listed in `requirements.txt`\n- API Key for **Groq**\n- `.env` file with the following variable:\n  ```plaintext\n  GROQ_API_KEY=<your_api_key>\n    ```\n\n```python\npip install -r requirements.txt\n```\n\n---\n\n## License\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n"
    },
    {
      "name": "DimohaZadira/ai-agent-start",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/33756112?s=40&v=4",
      "owner": "DimohaZadira",
      "repo_name": "ai-agent-start",
      "description": null,
      "homepage": null,
      "language": null,
      "created_at": "2025-01-19T11:22:08Z",
      "updated_at": "2025-01-19T16:46:45Z",
      "topics": [],
      "readme": "# ai-agent\n## prerequisities\n- pipx\n- poetry\n    - `pipx install poetry`\n    - `pipx inject poetry poetry-plugin-shell`\n    - `poetry init`\n    - `poetry shell`"
    },
    {
      "name": "halilural/multi-model-ai-agents",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/12976755?s=40&v=4",
      "owner": "halilural",
      "repo_name": "multi-model-ai-agents",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2025-01-16T19:53:06Z",
      "updated_at": "2025-01-28T11:28:42Z",
      "topics": [],
      "readme": ""
    },
    {
      "name": "rinormaloku/autogen-langchain-mcp-mix",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/9026451?s=40&v=4",
      "owner": "rinormaloku",
      "repo_name": "autogen-langchain-mcp-mix",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2025-01-16T07:41:09Z",
      "updated_at": "2025-04-02T05:13:08Z",
      "topics": [],
      "readme": "# Multi-Agent System that connects Autogen, Langchain, and MCP\n\nThis project demonstrates a multi-agent system using Autogen that assists with Kubernetes configuration tasks by combining\npopular frameworks such as Autogen, langchain and shortly to be added too MCP.\n\n**The agents collaborate to research, modify, and apply YAML configurations.**\n\n## Agents\n\nThe system comprises four specialized agents:\n\n- **PlanningAgent:** Decomposes complex tasks into smaller subtasks and delegates them to other agents.\n- **WebSearchAgent:** Gathers information from the web using DuckDuckGo search.\n- **FixerAgent:** Creates, modifies, or corrects YAML files based on gathered information and best practices.\n- **RunnerAgent:** Executes commands on the terminal, such as applying YAML configurations using `kubectl`.\n\n## Installation\n\n1. **Clone the repository:**\n\n    ```bash\n    git clone https://github.com/rinormaloku/autogen-langchain-mcp-mix.git\n    cd autogen-langchain-mcp-mix\n    ```\n\n2. **Install dependencies in a `venv`:**\n\n    ```bash\n    python3 -m venv .venv\n    source .venv/bin/activate\n    pip install -r requirements.txt\n    ```\n\n## Configuration\n\n1. **Set up environment variables:**\n\n    Copy the values from `.env.example` to a new file named `.env`:\n\n    ```\n    cp .env.example .env\n    ```\n\n    Update the environment variables in the `.env` file as needed.\n\n\n## Usage\n\n**Setup a k8s environment and run the main script:**\n\n```bash\nkind create cluster\nistioctl install -y\n\npython main.py\n```\n\nThis will start the multi-agent system and initiate a conversation based on the predefined `user_question` in `main.py`. The agents will collaborate to address the issue related to the Istio VirtualService configuration.\n\n\n## Notes\n\n- This project is a demonstration and may require adjustments for real-world applications.\n- The `RunnerAgent`'s ability to execute commands depends on the environment and permissions.\n- The termination condition is based on a maximum message count (30) or the mention of the word \"TERMINATE\".\n- Ensure that you have `kubectl` configured correctly if you intend to apply the generated YAML configurations.\n"
    },
    {
      "name": "lgcorzo/llmops-python-package",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/46710567?s=40&v=4",
      "owner": "lgcorzo",
      "repo_name": "llmops-python-package",
      "description": null,
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2025-01-09T17:27:55Z",
      "updated_at": "2025-03-21T23:36:55Z",
      "topics": [],
      "readme": "# MLOps Python Package\nthe original repository is fmind https://github.com/fmind/llmops-python-package\n\n[![check.yml](https://github.com/lgcorzo/llmops-python-package/actions/workflows/check.yml/badge.svg)](https://github.com/lgcorzo/llmops-python-package/actions/workflows/check.yml)\n[![publish.yml](https://github.com/lgcorzo/llmops-python-package/actions/workflows/publish.yml/badge.svg)](https://github.com/lgcorzo/llmops-python-package/actions/workflows/publish.yml)\n[![Documentation](https://img.shields.io/badge/documentation-available-brightgreen.svg)](https://lgcorzo.github.io/llmops-python-package/)\n[![License](https://img.shields.io/github/license/lgcorzo/llmops-python-package)](https://github.com/lgcorzo/llmops-python-package/blob/main/LICENCE.txt)\n[![Release](https://img.shields.io/github/v/release/lgcorzo/llmops-python-package)](https://github.com/lgcorzo/llmops-python-package/releases)\n\n**This repository contains a Python code base with best practices designed to support your MLOps initiatives.**\n\nThe package leverages several [tools](#tools) and [tips](#tips) to make your MLOps experience as flexible, robust, productive as possible.\n\nYou can use this package as part of your MLOps toolkit or platform (e.g., Model Registry, Experiment Tracking, Realtime Inference, ...).\n\n**Related Resources**:\n- **[MLOps Coding Course (Learning)](https://github.com/MLOps-Courses/mlops-coding-course)**: Learn how to create, develop, and maintain a state-of-the-art MLOps code base.\n- **[Cookiecutter MLOps Package (Template)](https://github.com/lgcorzo/cookiecutter-mlops-package)**: Start building and deploying Python packages and Docker images for MLOps tasks.\n\n# Table of Contents\n\n- [MLOps Python Package](#mlops-python-package)\n- [Table of Contents](#table-of-contents)\n- [Install](#install)\n  - [Prerequisites](#prerequisites)\n  - [Installation](#installation)\n  - [Next Steps](#next-steps)\n- [Usage](#usage)\n  - [Configuration](#configuration)\n  - [Execution](#execution)\n  - [Automation](#automation)\n  - [Workflows](#workflows)\n- [Tools](#tools)\n  - [Automation](#automation-1)\n    - [Commits: Commitizen](#commits-commitizen)\n    - [Git Hooks: Pre-Commit](#git-hooks-pre-commit)\n    - [Tasks: PyInvoke](#tasks-pyinvoke)\n  - [CI/CD](#cicd)\n    - [Runner: GitHub Actions](#runner-github-actions)\n  - [CLI](#cli)\n    - [Parser: Argparse](#parser-argparse)\n    - [Logging: Loguru](#logging-loguru)\n  - [Code](#code)\n    - [Coverage: Coverage](#coverage-coverage)\n    - [Editor: VS Code](#editor-vs-code)\n    - [Formatting: Ruff](#formatting-ruff)\n    - [Quality: Ruff](#quality-ruff)\n    - [Security: Bandit](#security-bandit)\n    - [Testing: Pytest](#testing-pytest)\n    - [Typing: Mypy](#typing-mypy)\n    - [Code Versioning: Git](#code-versioning-git)\n    - [Data Versioning: DVC](#data-versioning-dvc)\n      - [**Motivations**](#motivations)\n      - [**Features**](#features)\n      - [**Limitations**](#limitations)\n      - [**Alternatives**](#alternatives)\n      - [**Additional Resources**](#additional-resources)\n  - [Configs](#configs)\n    - [Format: YAML](#format-yaml)\n    - [Parser: OmegaConf](#parser-omegaconf)\n    - [Reader: Cloudpathlib](#reader-cloudpathlib)\n    - [Validator: Pydantic](#validator-pydantic)\n  - [Data](#data)\n    - [Container: Pandas](#container-pandas)\n    - [Format: Parquet](#format-parquet)\n    - [Schema: Pandera](#schema-pandera)\n  - [Docs](#docs)\n    - [API: pdoc](#api-pdoc)\n    - [Format: Google](#format-google)\n    - [Hosting: GitHub Pages](#hosting-github-pages)\n  - [Model](#model)\n    - [Evaluation: Scikit-Learn Metrics](#evaluation-scikit-learn-metrics)\n    - [Format: Mlflow Model](#format-mlflow-model)\n    - [Registry: Mlflow Registry](#registry-mlflow-registry)\n    - [Tracking: Mlflow Tracking](#tracking-mlflow-tracking)\n  - [Package](#package)\n    - [Evolution: Changelog](#evolution-changelog)\n    - [Format: Wheel](#format-wheel)\n    - [Manager: Poetry](#manager-poetry)\n    - [Runtime: Docker](#runtime-docker)\n  - [Programming](#programming)\n    - [Language: Python](#language-python)\n    - [Version: Pyenv](#version-pyenv)\n  - [Observability](#observability)\n    - [Reproducibility: Mlflow Project](#reproducibility-mlflow-project)\n    - [Monitoring : Mlflow Evaluate](#monitoring--mlflow-evaluate)\n    - [Alerting: Plyer](#alerting-plyer)\n    - [Lineage: Mlflow Dataset](#lineage-mlflow-dataset)\n    - [Explainability: SHAP](#explainability-shap)\n    - [Infrastructure: Mlflow System Metrics](#infrastructure-mlflow-system-metrics)\n- [Tips](#tips)\n  - [AI/ML Practices](#aiml-practices)\n    - [Data Catalog](#data-catalog)\n    - [Hyperparameter Optimization](#hyperparameter-optimization)\n    - [Data Splits](#data-splits)\n  - [Design Patterns](#design-patterns)\n    - [Directed-Acyclic Graph](#directed-acyclic-graph)\n    - [Program Service](#program-service)\n    - [Soft Coding](#soft-coding)\n    - [SOLID Principles](#solid-principles)\n    - [IO Separation](#io-separation)\n  - [Python Powers](#python-powers)\n    - [Context Manager](#context-manager)\n    - [Python Package](#python-package)\n  - [Software Engineering](#software-engineering)\n    - [Code Typing](#code-typing)\n    - [Config Typing](#config-typing)\n    - [Dataframe Typing](#dataframe-typing)\n    - [Object Oriented](#object-oriented)\n    - [Semantic Versioning](#semantic-versioning)\n  - [Testing Tricks](#testing-tricks)\n    - [Parallel Testing](#parallel-testing)\n    - [Test Fixtures](#test-fixtures)\n  - [VS Code](#vs-code)\n    - [Code Workspace](#code-workspace)\n    - [GitHub Copilot](#github-copilot)\n    - [VSCode VIM](#vscode-vim)\n- [Resources](#resources)\n  - [Python](#python)\n  - [AI/ML/MLOps](#aimlmlops)\n  - [UML course:](#uml-course)\n    - [1. **Asociación**](#1-asociación)\n      - [Código Python:](#código-python)\n      - [Representación UML:](#representación-uml)\n    - [2. **Agregación**](#2-agregación)\n      - [Código Python:](#código-python-1)\n      - [Representación UML:](#representación-uml-1)\n    - [3. **Composición**](#3-composición)\n      - [Código Python:](#código-python-2)\n      - [Representación UML:](#representación-uml-2)\n    - [4. **Dependencia**](#4-dependencia)\n      - [Código Python:](#código-python-3)\n      - [Representación UML:](#representación-uml-3)\n    - [Resumen de Relaciones:](#resumen-de-relaciones)\n    - [1. Asociación](#1-asociación-1)\n    - [2. Agregación](#2-agregación-1)\n    - [3. Composición](#3-composición-1)\n    - [4. Dependencia](#4-dependencia-1)\n    - [Cómo interpretar:](#cómo-interpretar)\n    - [Ejemplo en Python](#ejemplo-en-python)\n    - [Representación en Mermaid para UML](#representación-en-mermaid-para-uml)\n    - [Explicación:](#explicación)\n  - [Monitoring](#monitoring)\n  - [docker compose example:](#docker-compose-example)\n  - [openai with mlflow example:](#openai-with-mlflow-example)\n\n# Install\n\nThis section details the requirements, actions, and next steps to kicºkstart your MLOps project.\n\n## Prerequisites\n\n- [Python>=3.12](https://www.python.org/downloads/): to benefit from [the latest features and performance improvements](https://docs.python.org/3/whatsnew/3.12.html)\n- [Poetry>=1.8.2](https://python-poetry.org/): to initialize the project [virtual environment](https://docs.python.org/3/library/venv.html) and its dependencies\n\n## Installation\n\n1. [Clone this GitHub repository](https://docs.github.com/en/repositories/creating-and-managing-repositories/cloning-a-repository) on your computer\n```bash\n# with ssh (recommended)\n$ git clone git@github.com:lgcorzo/llmops-python-package\n# with https\n$ git clone https://github.com/lgcorzo/llmops-python-package\n```\n2. [Run the project installation with poetry](https://python-poetry.org/docs/)\n```bash\n$ cd llmops-python-package/\n$ poetry install\n```\n3. Adapt the code base to your desire\n\n## Next Steps\n\nGoing from there, there are dozens of ways to integrate this package to your MLOps platform.\n\nFor instance, you can use Databricks or AWS as your compute platform and model registry.\n\nIt's up to you to adapt the package code to the solution you target. Good luck champ!\n\n# Usage\n\nThis section explains how configure the project code and execute it on your system.\n\nto run the ui for autogen the command is;\n\n``` bash\nautogenstudio ui --port 8090\n```\n\n![1740767154031](image/README/1740767154031.png)\n\n## Configuration\n\nYou can add or edit config files in the `confs/` folder to change the program behavior.\n\n```yaml\n# confs/training.yaml\njob:\n  KIND: TrainingJob\n  inputs:\n    KIND: ParquetReader\n    path: data/inputs_train.parquet\n  targets:\n    KIND: ParquetReader\n    path: data/targets_train.parquet\n```\n\nThis config file instructs the program to start a `TrainingJob` with 2 parameters:\n- `inputs`: dataset that contains the model inputs\n- `targets`: dataset that contains the model target\n\nYou can find all the parameters of your program in the `src/[package]/jobs/*.py` files.\n\nYou can also print the full schema supported by this package using `poetry run autogen_team --schema`.\n\n## Execution\n\nThe project code can be executed with poetry during your development:\n\n```bash\n$ poetry run autogen_team confs/tuning.yaml\n$ poetry run autogen_team confs/training.yaml\n$ poetry run autogen_team confs/promotion.yaml\n$ poetry run autogen_team confs/inference.yaml\n$ poetry run autogen_team confs/evaluations.yaml\n$ poetry run autogen_team confs/explanations.yaml\n```\ndebug in vscode  is possible th the following configuration:\n```json\n{\n    \"version\": \"0.2.0\",\n    \"configurations\": [\n\n        {\n            \"name\": \"Poetry evaluations Debug\",\n            \"type\": \"python\",\n            \"request\": \"launch\",\n            \"program\": \"${workspaceFolder}/src/autogen_team/__main__.py\", // Adjust the entry point path\n            \"args\": [\n                \"confs/evaluations.yaml\"\n            ], // Arguments passed to your script\n            \"console\": \"integratedTerminal\",\n            \"cwd\": \"${workspaceFolder}\", // Set the working directory to the project root\n            \"env\": {\n                \"PYTHONPATH\": \"${workspaceFolder}/src\"\n            } // Ensure module discovery\n        }\n    ]\n}\n```\n\nIn production, you can build, ship, and run the project as a Python package:\n\n```bash\npoetry build\npoetry publish # optional\npython -m pip install [package]\n[package] confs/inference.yaml\n## added mocogpt for integration test\npoetry add --group checks \"mocogpt[cli]@git+https://github.com/lgcorzo/mocogpt.git\"\n```\n\nYou can also install and use this package as a library for another AI/ML project:\n\n```python\nfrom [package] import jobs\n\njob = jobs.TrainingJob(...)\nwith job as runner:\n    runner.run()\n```\n\n**Additional tips**:\n- You can pass extra configs from the command line using the `--extras` flag\n  - Use it to pass runtime values (e.g., a result from previous job executions)\n- You can pass several config files in the command-line to merge them from left to right\n  - You can define common configurations shared between jobs (e.g., model params)\n- The right job task will be selected automatically thanks to [Pydantic Discriminated Unions](https://docs.pydantic.dev/latest/concepts/unions/#discriminated-unions)\n  - This is a great way to run any job supported by the application (training, tuning, ....\n\n## Automation\n\nThis project includes several automation tasks to easily repeat common actions.\n\nYou can invoke the actions from the [command-line](https://www.pyinvoke.org/) or [VS Code extension](https://marketplace.visualstudio.com/items?itemName=dchanco.vsc-invoke).\n\n```bash\n# execute the project DAG\n$ inv projects\n# create a code archive\n$ inv packages\n# list other actions\n$ inv --list\n```\n\n**Available tasks**:\n- **checks.all (checks)** - Run all check tasks.\n- **checks.code** - Check the codes with ruff.\n- **checks.coverage** - Check the coverage with coverage.\n- **checks.format** - Check the formats with ruff.\n- **checks.poetry** - Check poetry config files.\n- **checks.security** - Check the security with bandit.\n- **checks.test** - Check the tests with pytest.\n- **checks.type** - Check the types with mypy.\n- **cleans.all (cleans)** - Run all tools and folders tasks.\n- **cleans.cache** - Clean the cache folder.\n- **cleans.coverage** - Clean the coverage tool.\n- **cleans.dist** - Clean the dist folder.\n- **cleans.docs** - Clean the docs folder.\n- **cleans.environment** - Clean the project environment file.\n- **cleans.folders** - Run all folders tasks.\n- **cleans.mlruns** - Clean the mlruns folder.\n- **cleans.mypy** - Clean the mypy tool.\n- **cleans.outputs** - Clean the outputs folder.\n- **cleans.poetry** - Clean poetry lock file.\n- **cleans.pytest** - Clean the pytest tool.\n- **cleans.projects** - Run all projects tasks.\n- **cleans.python** - Clean python caches and bytecodes.\n- **cleans.requirements** - Clean the project requirements file.\n- **cleans.reset** - Run all tools, folders, and sources tasks.\n- **cleans.ruff** - Clean the ruff tool.\n- **cleans.sources** - Run all sources tasks.\n- **cleans.tools** - Run all tools tasks.\n- **cleans.venv** - Clean the venv folder.\n- **commits.all (commits)** - Run all commit tasks.\n- **commits.bump** - Bump the version of the package.\n- **commits.commit** - Commit all changes with a message.\n- **commits.info** - Print a guide for messages.\n- **containers.all (containers)** - Run all container tasks.\n- **containers.build** - Build the container image with the given tag.\n- **containers.compose** - Start up docker compose.\n- **containers.run** - Run the container image with the given tag.\n- **docs.all (docs)** - Run all docs tasks.\n- **docs.api** - Document the API with pdoc using the given format and output directory.\n- **docs.serve** - Serve the API docs with pdoc using the given format and computer port.\n- **formats.all** - (formats) Run all format tasks.\n- **formats.imports** - Format python imports with ruff.\n- **formats.sources** - Format python sources with ruff.\n- **installs.all (installs)** - Run all install tasks.\n- **installs.poetry** - Install poetry packages.\n- **installs.pre-commit** - Install pre-commit hooks on git.\n- **mlflow.all (mlflow)** - Run all mlflow tasks.\n- **mlflow.doctor** - Run mlflow doctor to diagnose issues.\n- **mlflow.serve** - Start mlflow server with the given host, port, and backend uri.\n- **packages.all (packages)** - Run all package tasks.\n- **packages.build** - Build a python package with the given format.\n- **projects.all (projects)** - Run all project tasks.\n- **projects.environment** - Export the project environment file.\n- **projects.requirements** - Export the project requirements file.\n- **projects.run** - Run an mlflow project from MLproject file.\n\n## Workflows\n\nThis package supports two GitHub Workflows in `.github/workflows`:\n- `check.yml`: validate the quality of the package on each Pull Request\n- `publish.yml`: build and publish the docs and packages on code release.\n\nYou can use and extend these workflows to automate repetitive package management tasks.\n\n# Tools\n\nThis sections motivates the use of developer tools to improve your coding experience.\n\n## Automation\n\nPre-defined actions to automate your project development.\n\n### Commits: [Commitizen](https://commitizen-tools.github.io/commitizen/)\n\n- **Motivations**:\n  - Format your code commits\n  - Generate a standard changelog\n  - Integrate well with [SemVer](https://semver.org/) and [PEP 440](https://peps.python.org/pep-0440/)\n- **Limitations**:\n  - Learning curve for new users\n- **Alternatives**:\n  - Do It Yourself (DIY)\n\n### Git Hooks: [Pre-Commit](https://pre-commit.com/)\n\n- **Motivations**:\n  - Check your code locally before a commit\n  - Avoid wasting resources on your CI/CD\n  - Can perform extra actions (e.g., file cleanup)\n- **Limitations**:\n  - Add overhead before your commit\n- **Alternatives**:\n  - [Git Hooks](https://git-scm.com/book/en/v2/Customizing-Git-Git-Hooks): less convenient to use\n\n### Tasks: [PyInvoke](https://www.pyinvoke.org/)\n\n- **Motivations**:\n  - Automate project workflows\n  - Sane syntax compared to alternatives\n  - Good trade-off between power/simplicity\n- **Limitations**:\n  - Not familiar to most developers\n- **Alternatives**:\n  - [Make](https://www.gnu.org/software/make/manual/make.html): most popular, but awful syntax\n\n## CI/CD\n\nExecution of automated workflows on code push and releases.\n\n### Runner: [GitHub Actions](https://github.com/features/actions)\n\n- **Motivations**:\n  - Native on GitHub\n  - Simple workflow syntax\n  - Lots of configs if needed\n- **Limitations**:\n  - SaaS Service\n- **Alternatives**:\n  - [GitLab](https://about.gitlab.com/): can be installed on-premise\n\n## CLI\n\nIntegrations with the Command-Line Interface (CLI) of your system.\n\n### Parser: [Argparse](https://docs.python.org/3/library/argparse.html)\n\n- **Motivations**:\n  - Provide CLI arguments\n  - Included in Python runtime\n  - Sufficient for providing configs\n- **Limitations**:\n  - More verbose for advanced parsing\n- **Alternatives**:\n  - [Typer](https://typer.tiangolo.com/): code typing for the win\n  - [Fire](https://github.com/google/python-fire): simple but no typing\n  - [Click](https://click.palletsprojects.com/en/latest/): more verbose\n\n### Logging: [Loguru](https://loguru.readthedocs.io/en/stable/)\n\n- **Motivations**:\n  - Show progress to the user\n  - Work fine out of the box\n  - Saner logging syntax\n- **Limitations**:\n  - Doesn't let you deviate from the base usage\n- **Alternatives**:\n  - [Logging](https://docs.python.org/3/library/logging.html): available by default, but feel dated\n\n## Code\n\nEdition, validation, and versioning of your project source code.\n\n### Coverage: [Coverage](https://coverage.readthedocs.io/en/latest/)\n\nhttps://krijnvanderburg.medium.com/automatically-generate-and-visualize-python-code-coverage-308e65627925\n\n- **Motivations**:\n  - Report code covered by tests\n  - Identify code path to test\n  - Show maturity to users\n- **Limitations**:\n  - None\n- **Alternatives**:\n  - None?\n\n### Editor: [VS Code](https://code.visualstudio.com/)\n\n- **Motivations**:\n  - Open source\n  - Free, simple, open source\n  - Great plugins for Python development\n- **Limitations**:\n  - Require some configuration for Python\n- **Alternatives**:\n  - [PyCharm](https://www.jetbrains.com/pycharm/): provide a lot, cost a lot\n  - [Vim](https://www.vim.org/): I love it, but there is a VS Code plugin\n  - [Spacemacs](https://www.spacemacs.org/): I love it even more, but not everybody loves LISP\n\n### Formatting: [Ruff](https://docs.astral.sh/ruff/)\n\n- **Motivations**:\n  - Super fast compared to others\n  - Don't waste time arranging your code\n  - Make your code more readable/maintainable\n- **Limitations**:\n  - Still in version 0.x, but more and more adopted\n- **Alternatives**:\n  - [YAPF](https://github.com/google/yapf): more config options that you don't need\n  - [Isort](https://pycqa.github.io/isort/) + [Black](https://black.readthedocs.io/en/stable/): slower and need two tools\n\n### Quality: [Ruff](https://docs.astral.sh/ruff/)\n\n- **Motivations**:\n  - Improve your code quality\n  - Super fast compared to others\n  - [Great integration with VS Code](https://marketplace.visualstudio.com/items?itemName=charliermarsh.ruff)\n- **Limitations**:\n  - None\n- **Alternatives**:\n  - [PyLint](https://www.pylint.org/): too slow and too complex system\n  - [Flake8](https://flake8.pycqa.org/en/latest/): too much plugins, I prefer Pylint in practice\n\n### Security: [Bandit](https://bandit.readthedocs.io/en/latest/)\n\n- **Motivations**:\n  - Detect security issues\n  - Complement linting solutions\n  - Not to heavy to use and enable\n- **Limitations**:\n  - None\n- **Alternatives**:\n  - None\n\n### Testing: [Pytest](https://docs.pytest.org/en/latest/)\n\n- **Motivations**:\n  - Write tests or pay the price\n  - Super easy to write new test cases\n  - Tons of good plugins (xdist, sugar, cov, ...)\n- **Limitations**:\n  - Doesn't support parallel execution out of the box\n- **Alternatives**:\n  - [Unittest](https://docs.python.org/fr/3/library/unittest.html): more verbose, less fun\n\n### Typing: [Mypy](https://mypy-lang.org/)\n\n- **Motivations**:\n  - Static typing is cool!\n  - Communicate types to use\n  - Official type checker for Python\n- **Limitations**:\n  - Can have overhead for complex typing\n- **Alternatives**:\n  - [PyRight](https://github.com/microsoft/pyright): check big code base by MicroSoft\n  - [PyType](https://google.github.io/pytype/): check big code base by Google\n  - [Pyre](https://pyre-check.org/): check big code base by Facebook\n\n### Code Versioning: [Git](https://git-scm.com/)\n\n- **Motivations**:\n  - If you don't version your code, you are a fool\n  - Most popular source code manager (what else?)\n  - Provide hooks to perform automation on some events\n- **Limitations**:\n  - Git can be hard: https://xkcd.com/1597/\n- **Alternatives**:\n  - [Mercurial](https://www.mercurial-scm.org/): loved it back then, but git is the only real option\n\n### Data Versioning: [DVC](https://dvc.org/)\n\n\n#### **Motivations**\n- **Version Control for Data:** Just as versioning code is essential, versioning data ensures reproducibility, collaboration, and traceability in machine learning workflows.\n- **Integration with Git:** DVC works seamlessly with Git, enabling you to track datasets and models alongside code.\n- **Automation and Pipelines:** DVC provides tools for creating pipelines, automating data processing, and ensuring consistent workflows across teams.\n- **Scalability:** Handles large datasets without burdening Git by storing data in external storage solutions.\n\n#### **Features**\n- **Data and Model Tracking:** Tracks datasets, models, and experiments without including large files in Git repositories.\n- **External Storage Support:** Supports various storage backends, including AWS S3, Google Cloud Storage, Azure, SSH, and local file systems.\n- **Reproducible Pipelines:** Allows users to define and execute machine learning workflows with dependencies and outputs tracked automatically.\n- **Experiment Management:** Simplifies running and tracking multiple experiments, making comparisons straightforward.\n\n#### **Limitations**\n- **Learning Curve:** Requires understanding of Git and DVC-specific concepts to fully leverage its capabilities.\n- **Complexity for Small Projects:** Overhead may not be justified for simple projects or teams unfamiliar with Git.\n- **Storage Configuration:** Initial setup for remote storage can be tedious for non-technical users.\n- **Dependency on Git:** Full functionality depends on having a properly configured Git repository.\n\n#### **Alternatives**\n- **[Git LFS](https://git-lfs.com/):** Focuses on large file versioning but lacks pipeline and experiment management capabilities.\n- **[Pachyderm](https://www.pachyderm.com/):** Offers data versioning and pipeline management but is more suited for large-scale, enterprise environments.\n- **[LakeFS](https://lakefs.io/):** A Git-like version control system specifically for data lakes.\n- **[Quilt](https://quiltdata.com/):** Simplifies dataset sharing and versioning with a user-friendly UI.\n\n#### **Additional Resources**\n- **[Getting Started with DVC](https://dvc.org/doc/start):** Official tutorial for beginners.\n- **[DVC Pipelines Guide](https://dvc.org/doc/user-guide/pipelines):** Learn to create reproducible pipelines.\n- **[Remote Storage Setup](https://dvc.org/doc/user-guide/setup-remote):** Detailed instructions on configuring remote storage.\n- **[DVC vs Git LFS](https://dvc.org/doc/user-guide/dvc-vs-git-lfs):** A comparison of DVC and Git LFS.\n\n## Configs\n\nManage the configs files of your project to change executions.\n\n### Format: [YAML](https://yaml.org/)\n\n- **Motivations**:\n  - Change execution without changing code\n  - Readable syntax, support comments\n  - Allow to use OmegaConf <3\n- **Limitations**:\n  - Not supported out of the box by Python\n- **Alternatives**:\n  - [JSON](https://www.json.org/json-en.html): no comments, more verbose\n  - [TOML](https://toml.io/en/): less suited to config merge/sharing\n\n### Parser: [OmegaConf](https://omegaconf.readthedocs.io/en/2.3_branch/)\n\n- **Motivations**:\n  - Parse and merge YAML files\n  - Powerful, doesn't get in your way\n  - Achieve a lot with few lines of code\n- **Limitations**:\n  - Do not support remote files (e.g., s3, gcs, ...)\n    - You can combine it with [cloudpathlib](https://cloudpathlib.drivendata.org/stable/)\n- **Alternatives**:\n  - [Hydra](https://hydra.cc/docs/intro/): powerful, but gets in your way\n  - [DynaConf](https://www.dynaconf.com/): more suited for app development\n\n### Reader: [Cloudpathlib](https://cloudpathlib.drivendata.org/stable/)\n\n- **Motivations**:\n  - Read files from cloud storage\n  - Better integration with cloud platforms\n  - Support several platforms: AWS, GCP, and Azure\n- **Limitations**:\n  - Support of Python typing is not great at the moment\n- **Alternatives**:\n  - Cloud SDK (GCP, AWS, Azure, ...): vendor specific, overkill for this task\n\n### Validator: [Pydantic](https://docs.pydantic.dev/latest/)\n\n- **Motivations**:\n  - Validate your config before execution\n  - Pydantic should be builtin (period)\n  - Super charge your Python class\n- **Limitations**:\n  - None\n- **Alternatives**:\n  - [Dataclass](https://docs.python.org/3/library/dataclasses.html): simpler, but much less powerful\n  - [Attrs](https://www.attrs.org/en/stable/): no validation, less intuitive to use\n\n## Data\n\nDefine the datasets to provide data inputs and outputs.\n\n### Container: [Pandas](https://pandas.pydata.org/)\n\n- **Motivations**:\n  - Load data files in memory\n  - Lingua franca for Python\n  - Most popular options\n- **Limitations**:\n  - Lot of [gotchas](https://www.tutorialspoint.com/python_pandas/python_pandas_caveats_and_gotchas.htm)\n- **Alternatives**:\n  - [Polars](https://www.pola.rs/): faster, saner, but less integrations\n  - [Pyspark](https://spark.apache.org/docs/latest/api/python/): powerful, popular, distributed, so much overhead\n  - Dask, Ray, Modin, Vaex, ...: less integration (even if it looks like pandas)\n\n### Format: [Parquet](https://parquet.apache.org/)\n\n- **Motivations**:\n  - Store your data on disk\n  - Column-oriented (good for analysis)\n  - Much more efficient and saner than text based\n- **Limitations**:\n  - None\n- **Alternatives**:\n  - [CSV](https://en.wikipedia.org/wiki/Comma-separated_values): human readable, but that's the sole benefit\n  - [Avro](https://avro.apache.org/): good alternative for row-oriented workflow\n\n### Schema: [Pandera](https://pandera.readthedocs.io/en/stable/)\n\n- **Motivations**:\n  - Typing for dataframe\n  - Communicate data fields\n  - Support pandas and [others](https://pandera.readthedocs.io/en/stable/supported_libraries.html)\n- **Limitations**:\n  - None\n- **Alternatives**:\n  - [Great Expectations](https://greatexpectations.io/): powerful, but much more difficult to integrate\n\n## Docs\n\nGenerate and share the project documentations.\n\n### API: [pdoc](https://pdoc.dev/)\n\n- **Motivations**:\n  - Share docs with others\n  - Simple tool, only does API docs\n  - Get the job done, get out of your way\n- **Limitations**:\n  - Only support API docs (i.e., no custom docs)\n- **Alternatives**:\n  - [Sphinx](https://www.sphinx-doc.org/en/master/): Most complete, overkill for simple projects\n  - [Mkdocs](https://www.mkdocs.org/): no support for API doc, which is the core feature\n\n### Format: [Google](https://google.github.io/styleguide/pyguide.html)\n\n- **Motivations**:\n  - Common style for docstrings\n  - Most writeable out of alternatives\n  - I often write a single line for simplicity\n- **Limitations**:\n  - None\n- **Alternatives**:\n  - [Numpy](https://numpydoc.readthedocs.io/en/latest/format.html): less writeable\n  - [Sphinx](https://sphinx-rtd-tutorial.readthedocs.io/en/latest/docstrings.html): baroque style\n\n### Hosting: [GitHub Pages](https://pages.github.com/)\n\n- **Motivations**:\n  - Easy to setup\n  - Free and simple\n  - Integrated with GitHub\n- **Limitations**:\n  - Only support static content\n- **Alternatives**:\n  - [ReadTheDocs](https://about.readthedocs.com/?ref=readthedocs.com): provide more features\n\n## Model\n\nToolkit to handle machine learning models.\n\n### Evaluation: [Scikit-Learn Metrics](https://scikit-learn.org/stable/modules/model_evaluation.html)\n\n- **Motivations**:\n  - Bring common metrics\n  - Avoid reinventing the wheel\n  - Avoid implementation mistakes\n- **Limitations**:\n  - Limited set of metric to be chosen\n- **Alternatives**:\n  - Implement your own: for custom metrics\n\n### Format: [Mlflow Model](https://mlflow.org/docs/latest/models.html)\n\n- **Motivations**:\n  - Standard ML format\n  - Store model dependencies\n  - Strong community ecosystem\n- **Limitations**:\n  - None\n- **Alternatives**:\n  - [Pickle](https://docs.python.org/3/library/pickle.html): work out of the box, but less suited for big array\n  - [ONNX](https://onnx.ai/): great for deep learning, [no guaranteed compatibility for the rest](https://onnxruntime.ai/docs/reference/compatibility.html)\n\n### Registry: [Mlflow Registry](https://mlflow.org/docs/latest/model-registry.html)\n\n- **Motivations**:\n  - Save and load models\n  - Separate production from consumption\n  - Popular, open source, work on local system\n- **Limitations**:\n  - None\n- **Alternatives**:\n  - [Neptune.ai](https://neptune.ai/): SaaS solution\n  - [Weights and Biases](https://wandb.ai/site): SaaS solution\n\n### Tracking: [Mlflow Tracking](https://mlflow.org/docs/latest/tracking.html)\n\n- **Motivations**:\n  - Keep track of metrics and params\n  - Allow to compare model performances\n  - Popular, open source, work on local system\n- **Limitations**:\n  - None\n- **Alternatives**:\n  - [Neptune.ai](https://neptune.ai/): SaaS solution\n  - [Weights and Biases](https://wandb.ai/site): SaaS solution\n\n## Package\n\nDefine and build modern Python package.\n\n### Evolution: [Changelog](https://en.wikipedia.org/wiki/Changelog)\n\n- **Motivation**:\n  - Communicate changes to user\n  - Can be updated with [Commitizen](https://commitizen-tools.github.io/commitizen/changelog/)\n  - Standardized with [Keep a Changelog](https://keepachangelog.com/)\n- **Limitations**:\n  - None\n- **Alternatives**:\n  - None\n\n### Format: [Wheel](https://peps.python.org/pep-0427/)\n\n- **Motivations**:\n  - [Has several advantages](https://realpython.com/python-wheels/#advantages-of-python-wheels)\n  - Create source code archive\n  - Most modern Python format\n- **Limitations**:\n  - Doesn't ship with C/C++ dependencies (e.g., CUDA)\n    - i.e., use Docker containers for this case\n- **Alternatives**:\n  - [Source](https://docs.python.org/3/distutils/sourcedist.html): older format, less powerful\n  - [Conda](https://conda.io/projects/conda/en/latest/user-guide/install/index.html): slow and hard to manage\n\n### Manager: [Poetry](https://python-poetry.org/)\n\n- **Motivations**:\n  - Define and build Python package\n  - Most popular solution by GitHub stars\n  - Pack every metadata in a single static file\n- **Limitations**:\n  - Cannot add dependencies beyond Python (e.g., CUDA)\n    - i.e., use Docker container for this use case\n- **Alternatives**:\n  - [Setuptools](https://docs.python.org/3/distutils/setupscript.html): dynamic file is slower and more risky\n  - Pdm, Hatch, PipEnv: https://xkcd.com/1987/\n\n### Runtime: [Docker](https://www.docker.com/resources/what-container/)\n\n- **Motivations**:\n  - Create isolated runtime\n  - Container is the de facto standard\n  - Package C/C++ dependencies with your project\n- **Limitations**:\n  - Some company might block Docker Desktop, you should use alternatives\n- **Alternatives**:\n  - [Conda](https://docs.conda.io/en/latest/): slow and heavy resolver\n\n## Programming\n\nSelect your programming environment.\n\n### Language: [Python](https://www.python.org/)\n\n- **Motivations**:\n  - Great language for AI/ML projects\n  - Robust with additional tools\n  - Hundreds of great libs\n- **Limitations**:\n  - Slow without C bindings\n- **Alternatives**:\n  - [R](https://www.r-project.org/): specific purpose language\n  - [Julia](https://julialang.org/): specific purpose language\n\n### Version: [Pyenv](https://github.com/pyenv/pyenv)\n\n- **Motivations**:\n  - Switch between Python version\n  - Allow to select the best version\n  - Support global and local dispatch\n- **Limitations**:\n  - Require some shell configurations\n- **Alternatives**:\n  - Manual installation: time consuming\n\n## Observability\n\n### Reproducibility: [Mlflow Project](https://mlflow.org/docs/latest/projects.html)\n\n- **Motivations**:\n  - Share common project formats.\n  - Ensure the project can be reused.\n  - Avoid randomness in project execution.\n- **Limitations**:\n  - Mlflow Project is best suited for small projects.\n- **Alternatives**:\n  - [DVC](https://dvc.org/): both data and models.\n  - [Metaflow](https://metaflow.org/): focus on machine learning.\n  - **[Apache Airflow](https://airflow.apache.org/)**: for large scale projects.\n\n### Monitoring : [Mlflow Evaluate](https://mlflow.org/docs/latest/model-evaluation/index.html)\n\n- **Motivations**:\n  - Compute the model metrics.\n  - Validate model with thresholds.\n  - Perform post-training evaluations.\n- **Limitations**:\n  - Mlflow Evaluate is less feature-rich as alternatives.\n- **Alternatives**:\n  - **[Giskard](https://www.giskard.ai/)**: open-core and super complete.\n  - **[Evidently](https://www.evidentlyai.com/)**: open-source with more metrics.\n  - [Arize AI](https://arize.com/): more feature-rich but less flexible.\n  - [Graphana](https://grafana.com/): you must do everything yourself.\n\n### Alerting: [Plyer](https://github.com/kivy/plyer)\n\n- **Motivations**:\n  - Simple solution.\n  - Send notifications on system.\n  - Cross-system: Mac, Linux, Windows.\n- **Limitations**:\n  - Should not be used for large scale projects.\n- **Alternatives**:\n  - [Slack](https://slack.com/): for chat-oriented solutions.\n  - [Datadog](https://www.datadoghq.com/): for infrastructure oriented solutions.\n\n### Lineage: [Mlflow Dataset](https://mlflow.org/docs/latest/tracking/data-api.html)\n\n- **Motivations**:\n  - Store information in Mlflow.\n  - Track metadata about run datasets.\n  - Keep URI of the dataset source (e.g., website).\n- **Limitations**:\n  - Not as feature-rich as alternative solutions.\n- **Alternatives**:\n  - [Databricks Lineage](https://docs.databricks.com/en/admin/system-tables/lineage.html): limited to Databricks.\n  - [OpenLineage and Marquez](https://marquezproject.github.io/): open-source and flexible.\n\n### Explainability: [SHAP](https://shap.readthedocs.io/en/latest/)\n\n- **Motivations**:\n  - Most popular toolkit.\n  - Support various models (linear, model, ...).\n  - Integration with Mlflow through the [SHAP module](https://mlflow.org/docs/latest/python_api/mlflow.shap.html).\n- **Limitations**:\n  - Super slow on large dataset.\n  - Mlflow SHAP module is not mature enough.\n- **Alternatives**:\n  - [LIME](https://github.com/marcotcr/lime): not maintained anymore.\n\n### Infrastructure: [Mlflow System Metrics](https://mlflow.org/docs/latest/system-metrics/index.html)\n\n- **Motivations**:\n  - Track infrastructure information (RAM, CPU, ...).\n  - Integrated with Mlflow tracking.\n  - Provide hardware insights.\n- **Limitations**:\n  - Not as mature as alternative solutions.\n- **Alternatives**:\n  - [Datadog](https://www.datadoghq.com/): popular and mature solution.\n\n# Tips\n\nThis sections gives some tips and tricks to enrich the develop experience.\n\n```bash\npyreverse  -A -o plantuml -p autogen_team src/autogen_team\n```\n## [AI/ML Practices](https://machinelearningmastery.com/)\n\n### [Data Catalog](https://docs.kedro.org/en/stable/data/data_catalog.html)\n\n**You should decouple the pointer to your data from how to access it.**\n\nIn your code, you can refer to your dataset with a tag (e.g., `inputs`, `targets`).\n\nThis tag can then be associated to a reader/writer implementation in a configuration file:\n\n```yaml\n  inputs:\n    KIND: ParquetReader\n    path: data/inputs_train.parquet\n  targets:\n    KIND: ParquetReader\n    path: data/targets_train.parquet\n```\n\nIn this package, the implementation are described in `src/[package]/io/datasets.py` and selected by `KIND`.\n\n### [Hyperparameter Optimization](https://en.wikipedia.org/wiki/Hyperparameter_optimization)\n\n**You should select the best hyperparameters for your model using optimization search.**\n\nThe simplest projects can use a `sklearn.model_selection.GridSearchCV` to scan the whole search space.\n\nThis package provides a simple interface to this hyperparameter search facility in `src/[package]/utils/searchers.py`.\n\nFor more complex project, we recommend to use more complex strategy (e.g., [Bayesian](https://en.wikipedia.org/wiki/Bayesian_optimization)) and software package (e.g., [Optuna](https://optuna.org/)).\n\n### [Data Splits](https://machinelearningmastery.com/difference-test-validation-datasets/)\n\n**You should properly split your dataset into a training, validation, and testing sets.**\n\n- *Training*: used for fitting the model parameters\n- *Validation*: used to find the best hyperparameters\n- *Testing*: used to evaluate the final model performance\n\nThe sets should be exclusive, and the testing set should never be used as training inputs!\n\nThis package provides a simple deterministic strategy implemented in `src/[package]/utils/splitters.py`.\n\n## [Design Patterns](https://en.wikipedia.org/wiki/Software_design_pattern)\n\n### [Directed-Acyclic Graph](https://en.wikipedia.org/wiki/Directed_acyclic_graph)\n\n**You should use Directed-Acyclic Graph (DAG) to connect the steps of your ML pipeline.**\n\nA DAG can express the dependencies between steps while keeping the individual step independent.\n\nThis package provides a simple DAG example in `tasks/dags.py`. This approach is based on [PyInvoke](https://www.pyinvoke.org/).\n\nIn production, we recommend to use a scalable system such as [Airflow](https://airflow.apache.org/), [Dagster](https://dagster.io/), [Prefect](https://www.prefect.io/), [Metaflow](https://metaflow.org/), or [ZenML](https://zenml.io/).\n\n### [Program Service](https://en.wikipedia.org/wiki/Systemd)\n\n**You should provide a global context for the execution of your program.**\n\nThere are several approaches such as [Singleton](https://en.wikipedia.org/wiki/Singleton_pattern), [Global Variable](https://en.wikipedia.org/wiki/Global_variable), or [Component](https://github.com/stuartsierra/component).\n\nThis package takes inspiration from [Clojure mount](https://github.com/tolitius/mount). It provides an implementation in `src/[package]/io/services.py`.\n\n### [Soft Coding](https://en.wikipedia.org/wiki/Softcoding)\n\n**You should separate the program implementation from the program configuration.**\n\nExposing configurations to users allow them to influence the execution behavior without code changes.\n\nThis package seeks to expose as much parameter as possible to the users in configurations stored in the `confs/` folder.\n\n### [SOLID Principles](https://en.wikipedia.org/wiki/SOLID)\n\n**You should implement the SOLID principles to make your code as flexible as possible.**\n\n- *Single responsibility principle*:  Class has one job to do. Each change in requirements can be done by changing just one class.\n- *Open/closed principle*: Class is happy (open) to be used by others. Class is not happy (closed) to be changed by others.\n- *Liskov substitution principle*: Class can be replaced by any of its children. Children classes inherit parent's behaviours.\n- *Interface segregation principle*: When classes promise each other something, they should separate these promises (interfaces) into many small promises, so it's easier to understand.\n- *Dependency inversion principle*: When classes talk to each other in a very specific way, they both depend on each other to never change. Instead classes should use promises (interfaces, parents), so classes can change as long as they keep the promise.\n\nIn practice, this mean you can implement software contracts with interface and swap the implementation.\n\nFor instance, you can implement several jobs in `src/[package]/jobs/*.py` and swap them in your configuration.\n\nTo learn more about the mechanism select for this package, you can check the documentation for [Pydantic Tagged Unions](https://docs.pydantic.dev/dev-v2/usage/types/unions/#discriminated-unions-aka-tagged-unions).\n\n### [IO Separation](https://en.wikibooks.org/wiki/Haskell/Understanding_monads/IO)\n\n**You should separate the code interacting with the external world from the rest.**\n\nThe external is messy and full of risks: missing files, permission issue, out of disk ...\n\nTo isolate these risks, you can put all the related code in an `io` package and use interfaces\n\n## [Python Powers](https://realpython.com/)\n\n### [Context Manager](https://docs.python.org/3/library/contextlib.html)\n\n**You should use Python context manager to control and enhance an execution.**\n\nPython provides contexts that can be used to extend a code block. For instance:\n\n```python\n# in src/[package]/scripts.py\nwith job as runner:  # context\n    runner.run()  # run in context\n```\n\nThis pattern has the same benefit as [Monad](https://en.wikipedia.org/wiki/Monad_(functional_programming)), a powerful programming pattern.\n\nThe package uses `src/[package]/jobs/*.py` to handle exception and services.\n\n### [Python Package](https://packaging.python.org/en/latest/tutorials/packaging-projects/)\n\n**You should create Python package to create both library and application for others.**\n\nUsing Python package for your AI/ML project has the following benefits:\n- Build code archive (i.e., wheel) that be uploaded to Pypi.org\n- Install Python package as a library (e.g., like pandas)\n- Expose script entry points to run a CLI or a GUI\n\nTo build a Python package with Poetry, you simply have to type in a terminal:\n```bash\n# for all poetry project\npoetry build\n# for this project only\ninv packages\n```\n\n## [Software Engineering](https://en.wikipedia.org/wiki/Software_engineering)\n\n### [Code Typing](https://docs.python.org/3/library/typing.html)\n\n**You should type your Python code to make it more robust and explicit for your user.**\n\nPython provides the [typing module](https://docs.python.org/3/library/typing.html) for adding type hints and [mypy](https://mypy-lang.org/) to checking them.\n\n```python\n# in src/[package]/core/models.py\n@abc.abstractmethod\ndef fit(self, inputs: schemas.Inputs, targets: schemas.Targets) -> \"Model\":\n    \"\"\"Fit the model on the given inputs and target.\"\"\"\n\n@abc.abstractmethod\ndef predict(self, inputs: schemas.Inputs) -> schemas.Outputs:\n    \"\"\"Generate an output with the model for the given inputs.\"\"\"\n```\n\nThis code snippet clearly state the inputs and outputs of the method, both for the developer and the type checker.\n\nThe package aims to type every functions and classes to facilitate the developer experience and fix mistakes before execution.\n\n### [Config Typing](https://docs.pydantic.dev/latest/)\n\n**You should type your configuration to avoid exceptions during the program execution.**\n\nPydantic allows to define classes that can validate your configs during the program startup.\n\n```python\n# in src/[package]/utils/splitters.py\nclass TrainTestSplitter(Splitter):\n    shuffle: bool = False  # required (time sensitive)\n    test_size: int | float = 24 * 30 * 2  # 2 months\n    random_state: int = 42\n```\n\nThis code snippet allows to communicate the values expected and avoid error that could be avoided.\n\nThe package combines both OmegaConf and Pydantic to parse YAML files and validate them as soon as possible.\n\n### [Dataframe Typing](https://pandera.readthedocs.io/en/stable/)\n\n**You should type your dataframe to communicate and validate their fields.**\n\nPandera supports dataframe typing for Pandas and other library like PySpark:\n\n```python\n# in src/package/schemas.py\nclass InputsSchema(Schema):\n    instant: papd.Index[papd.UInt32] = pa.Field(ge=0, check_name=True)\n    dteday: papd.Series[papd.DateTime] = pa.Field()\n    season: papd.Series[papd.UInt8] = pa.Field(isin=[1, 2, 3, 4])\n    yr: papd.Series[papd.UInt8] = pa.Field(ge=0, le=1)\n    mnth: papd.Series[papd.UInt8] = pa.Field(ge=1, le=12)\n    hr: papd.Series[papd.UInt8] = pa.Field(ge=0, le=23)\n    holiday: papd.Series[papd.Bool] = pa.Field()\n    weekday: papd.Series[papd.UInt8] = pa.Field(ge=0, le=6)\n    workingday: papd.Series[papd.Bool] = pa.Field()\n    weathersit: papd.Series[papd.UInt8] = pa.Field(ge=1, le=4)\n    temp: papd.Series[papd.Float16] = pa.Field(ge=0, le=1)\n    atemp: papd.Series[papd.Float16] = pa.Field(ge=0, le=1)\n    hum: papd.Series[papd.Float16] = pa.Field(ge=0, le=1)\n    windspeed: papd.Series[papd.Float16] = pa.Field(ge=0, le=1)\n    casual: papd.Series[papd.UInt32] = pa.Field(ge=0)\n    registered: papd.Series[papd.UInt32] = pa.Field(ge=0)\n```\n\nThis code snippet defines the fields of the dataframe and some of its constraint.\n\nThe package encourages to type every dataframe used in `src/[package]/core/schemas.py`.\n\n### [Object Oriented](https://en.wikipedia.org/wiki/Object-oriented_programming)\n\n**You should use the Objected Oriented programming to benefit from [polymorphism](https://en.wikipedia.org/wiki/Polymorphism_(computer_science)).**\n\nPolymorphism combined with SOLID Principles allows to easily swap your code components.\n\n```python\nclass Reader(abc.ABC, pdt.BaseModel):\n\n    @abc.abstractmethod\n    def read(self) -> pd.DataFrame:\n        \"\"\"Read a dataframe from a dataset.\"\"\"\n```\n\nThis code snippet uses the [abc module](https://docs.python.org/3/library/abc.html) to define code interfaces for a dataset with a read/write method.\n\nThe package defines class interface whenever possible to provide intuitive and replaceable parts for your AI/ML project.\n\n### [Semantic Versioning](https://semver.org/)\n\n**You should use semantic versioning to communicate the level of compatibility of your releases.**\n\nSemantic Versioning (SemVer) provides a simple schema to communicate code changes. For package X.Y.Z:\n- *Major* (X): major release with breaking changed (i.e., imply actions from the benefit)\n- *Minor* (Y): minor release with new features (i.e., provide new capabilities)\n- *Patch* (Z): patch release to fix bugs (i.e., correct wrong behavior)\n\nPoetry and this package leverage Semantic Versioning to let developers control the speed of adoption for new releases.\n\n## [Testing Tricks](https://en.wikipedia.org/wiki/Software_testing)\n\n### [Parallel Testing](https://pytest-xdist.readthedocs.io/en/stable/)\n\n**You can run your tests in parallel to speed up the validation of your code base.**\n\nPytest can be extended with the [pytest-xdist plugin](https://pytest-xdist.readthedocs.io/en/stable/) for this purpose.\n\nThis package enables Pytest in its automation tasks by default.\n\n### [Test Fixtures](https://docs.pytest.org/en/latest/explanation/fixtures.html)\n\n**You should define reusable objects and actions for your tests with [fixtures](https://docs.pytest.org/en/latest/explanation/fixtures.html).**\n\nFixture can prepare objects for your test cases, such as dataframes, models, files.\n\nThis package defines fixtures in `tests/conftest.py` to improve your testing experience.\n\n## [VS Code](https://code.visualstudio.com/)\n\n### [Code Workspace](https://code.visualstudio.com/docs/editor/workspaces)\n\n**You can use VS Code workspace to define configurations for your project.**\n\n[Code Workspace](https://code.visualstudio.com/docs/editor/workspaces) can enable features (e.g. formatting) and set the default interpreter.\n\n```json\n{\n\t\"settings\": {\n\t\t\"editor.formatOnSave\": true,\n\t\t\"python.defaultInterpreterPath\": \".venv/bin/python\",\n    ...\n\t},\n}\n```\n\nThis package defines a workspace file that you can load from `[package].code-workspace`.\n\n### [GitHub Copilot](https://github.com/features/copilot)\n\n**You can use GitHub Copilot to increase your coding productivity by 30%.**\n\n[GitHub Copilot](https://github.com/features/copilot) has been a huge productivity thanks to its smart completion.\n\nYou should become familiar with the solution in less than a single coding session.\n\n### [VSCode VIM](https://marketplace.visualstudio.com/items?itemName=vscodevim.vim)\n\n**You can use VIM keybindings to more efficiently navigate and modify your code.**\n\nLearning VIM is one of the best investment for a career in IT. It can make you 30% more productive.\n\nCompared to GitHub Copilot, VIM can take much more time to master. You can expect a ROI in less than a month.\n\n# Resources\n\nThis section provides resources for building packages for Python and AI/ML/MLOps.\n\n## Python\n\n- https://github.com/krzjoa/awesome-python-data-science#readme\n- https://github.com/ml-tooling/best-of-ml-python\n- https://github.com/ml-tooling/best-of-python\n- https://github.com/ml-tooling/best-of-python-dev\n- https://github.com/vinta/awesome-python\n\n## AI/ML/MLOps\n\n- https://github.com/josephmisiti/awesome-machine-learning\n- https://github.com/visenger/awesome-mlops\n\n## UML course:\n\nEn UML, las relaciones entre clases suelen dividirse en cuatro tipos principales de dependencias o relaciones: **asociación**, **agregación**, **composición**, y **dependencia**. A continuación, se explican cada una con ejemplos en Python y cómo se representan en un diagrama UML.\n\n---\n\n### 1. **Asociación**\n- Representa una relación entre dos clases que colaboran entre sí.\n- Es una relación débil y bidireccional (en general).\n- Ejemplo: Un `Alumno` se asocia con un `Profesor`.\n\n#### Código Python:\n```python\nclass Profesor:\n    def __init__(self, nombre):\n        self.nombre = nombre\n\nclass Alumno:\n    def __init__(self, nombre, profesor):\n        self.nombre = nombre\n        self.profesor = profesor  # Relación de asociación\n\n# Ejemplo de uso\nprofesor = Profesor(\"Dr. García\")\nalumno = Alumno(\"Juan\", profesor)\n\nprint(f\"{alumno.nombre} tiene como profesor a {alumno.profesor.nombre}\")\n```\n\n#### Representación UML:\n- Línea sólida simple entre `Alumno` y `Profesor`.\n\n---\n\n### 2. **Agregación**\n- Es un tipo especial de asociación.\n- Representa una relación \"todo-parte\", pero donde las partes pueden existir independientemente del todo.\n- Ejemplo: Un `Departamento` tiene varios `Empleado`s, pero si el departamento deja de existir, los empleados pueden seguir existiendo.\n\n#### Código Python:\n```python\nclass Empleado:\n    def __init__(self, nombre):\n        self.nombre = nombre\n\nclass Departamento:\n    def __init__(self, nombre):\n        self.nombre = nombre\n        self.empleados = []  # Relación de agregación\n\n    def agregar_empleado(self, empleado):\n        self.empleados.append(empleado)\n\n# Ejemplo de uso\nempleado1 = Empleado(\"Alice\")\nempleado2 = Empleado(\"Bob\")\ndepartamento = Departamento(\"Recursos Humanos\")\ndepartamento.agregar_empleado(empleado1)\ndepartamento.agregar_empleado(empleado2)\n\nprint(f\"El departamento {departamento.nombre} tiene a:\")\nfor empleado in departamento.empleados:\n    print(f\"- {empleado.nombre}\")\n```\n\n#### Representación UML:\n- Línea con un rombo vacío en el lado del \"todo\" (`Departamento`).\n\n---\n\n### 3. **Composición**\n- Es una relación \"todo-parte\" más fuerte.\n- Las partes no pueden existir sin el todo.\n- Ejemplo: Un `Coche` tiene un `Motor`, y si el coche deja de existir, el motor también.\n\n#### Código Python:\n```python\nclass Motor:\n    def __init__(self, tipo):\n        self.tipo = tipo\n\nclass Coche:\n    def __init__(self, marca, tipo_motor):\n        self.marca = marca\n        self.motor = Motor(tipo_motor)  # Relación de composición\n\n# Ejemplo de uso\ncoche = Coche(\"Toyota\", \"V8\")\nprint(f\"El coche {coche.marca} tiene un motor {coche.motor.tipo}\")\n```\n\n#### Representación UML:\n- Línea con un rombo sólido en el lado del \"todo\" (`Coche`).\n\n---\n\n### 4. **Dependencia**\n- Representa que una clase utiliza otra temporalmente.\n- Es la relación más débil y generalmente unidireccional.\n- Ejemplo: Una `Impresora` depende de un `Documento` para imprimirlo.\n\n#### Código Python:\n```python\nclass Documento:\n    def __init__(self, texto):\n        self.texto = texto\n\nclass Impresora:\n    def imprimir(self, documento):\n        print(f\"Imprimiendo documento: {documento.texto}\")\n\n# Ejemplo de uso\ndoc = Documento(\"Informe Anual\")\nimpresora = Impresora()\nimpresora.imprimir(doc)\n```\n\n#### Representación UML:\n- Línea punteada con una flecha hacia la clase que se utiliza (`Documento`).\n\n---\n\n### Resumen de Relaciones:\n| Relación      | Código Python               | UML                       |\n|---------------|-----------------------------|---------------------------|\n| Asociación    | `profesor` dentro de `Alumno` | Línea sólida simple       |\n| Agregación    | `empleados` dentro de `Departamento` | Línea con rombo vacío     |\n| Composición   | `motor` dentro de `Coche`    | Línea con rombo sólido    |\n| Dependencia   | `Impresora` usa `Documento` | Línea punteada            |\n\nAquí tienes los ejemplos representados en **Mermaid**, un lenguaje para generar diagramas en Markdown. Puedes copiar y pegar estos diagramas en un entorno compatible con Mermaid (como un editor Markdown con soporte, GitHub, o un visor en línea).\n\n---\n\n### 1. Asociación\n```mermaid\nclassDiagram\n    class Alumno {\n        -nombre: string\n        -profesor: Profesor\n    }\n    class Profesor {\n        -nombre: string\n    }\n    Alumno --> Profesor : tiene\n```\n\n---\n\n### 2. Agregación\n```mermaid\nclassDiagram\n    class Departamento {\n        -nombre: string\n        -empleados: List<Empleado>\n        +agregar_empleado(empleado: Empleado): void\n    }\n    class Empleado {\n        -nombre: string\n    }\n    Departamento o-- Empleado : contiene\n```\n\n---\n\n### 3. Composición\n```mermaid\nclassDiagram\n    class Coche {\n        -marca: string\n        -motor: Motor\n    }\n    class Motor {\n        -tipo: string\n    }\n    Coche *-- Motor : contiene\n```\n\n---\n\n### 4. Dependencia\n```mermaid\nclassDiagram\n    class Impresora {\n        +imprimir(documento: Documento): void\n    }\n    class Documento {\n        -texto: string\n    }\n    Impresora ..> Documento : usa\n```\n\n---\n\n### Cómo interpretar:\n- **Línea sólida simple (`-->`)**: Representa una asociación.\n- **Rombo vacío (`o--`)**: Representa una agregación.\n- **Rombo sólido (`*--`)**: Representa una composición.\n- **Línea punteada (`..>`)**: Representa una dependencia.\nCuando una clase es **derivada** de otra, la relación entre ellas es de **herencia**. En UML, la herencia se representa con una **línea sólida con una flecha hueca** que apunta de la clase derivada a la clase base.\n\n### Ejemplo en Python\n```python\nclass Animal:\n    def __init__(self, nombre):\n        self.nombre = nombre\n\n    def hacer_sonido(self):\n        pass\n\nclass Perro(Animal):\n    def hacer_sonido(self):\n        return \"Ladra\"\n\nclass Gato(Animal):\n    def hacer_sonido(self):\n        return \"Maulla\"\n\n# Ejemplo de uso\nperro = Perro(\"Firulais\")\ngato = Gato(\"Misu\")\n\nprint(f\"{perro.nombre} {perro.hacer_sonido()}\")\nprint(f\"{gato.nombre} {gato.hacer_sonido()}\")\n```\n\n### Representación en Mermaid para UML\n```mermaid\nclassDiagram\n    class Animal {\n        -nombre: string\n        +hacer_sonido(): string\n    }\n    class Perro {\n        +hacer_sonido(): string\n    }\n    class Gato {\n        +hacer_sonido(): string\n    }\n    Animal <|-- Perro\n    Animal <|-- Gato\n```\n\n---\n\n### Explicación:\n- **Animal** es la clase base o superclase.\n- **Perro** y **Gato** son clases derivadas o subclases.\n- La relación de herencia indica que las subclases heredan los atributos y métodos de la clase base, pero pueden sobrescribirlos (como `hacer_sonido` en este caso).\n\n## Monitoring\nbest option [Evidentlyai](https://www.evidentlyai.com/) \n\n## docker compose example:\n\n```txt\n\n  kafka_inference:\n      image: autogen_team:latest\n      restart: unless-stopped\n      container_name: kafka_inference\n      networks:\n        - servnet\n        - minio_mlflow_mlflownet\n      environment:\n      - DEFAULT_KAFKA_SERVER=kafka_server:9092\n      - DEFAULT_GROUP_ID=llmops-regression\n      - DEFAULT_AUTO_OFFSET_RESET=earliest\n      - DEFAULT_INPUT_TOPIC=llm_input_topic\n      - DEFAULT_OUTPUT_TOPIC=llm_output_topic\n      - DEFAULT_FASTAPI_HOST=127.0.0.1\n      - DEFAULT_FASTAPI_PORT=8100\n      - MLFLOW_TRACKING_URI=http://mlflow_server:5000\n      - MLFLOW_REGISTRY_URI=http://mlflow_server:5000\n      - MLFLOW_EXPERIMENT_NAME=autogen_team_experiment\n      - MLFLOW_REGISTERED_MODEL_NAME=autogen_team\n      - MLFLOW_EXPERIMENT_NAME=autogen_team_experiment\n      - MLFLOW_REGISTERED_MODEL_NAME=autogen_team_model\n\n      ports:\n          - 8200:8100\n      extra_hosts:\n        - \"host.docker.internal:host-gateway\"\n```\n\n## openai with mlflow example: \nhttps://mlflow.org/docs/latest/llms/openai/notebooks/openai-code-helper.html\nhttps://mlflow.org/docs/latest/llms/openai/index.html#introduction\n"
    },
    {
      "name": "TeamADAPT/AgentStack",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/181675239?s=40&v=4",
      "owner": "TeamADAPT",
      "repo_name": "AgentStack",
      "description": "The fastest way to build robust AI agents",
      "homepage": "",
      "language": null,
      "created_at": "2025-01-15T11:17:09Z",
      "updated_at": "2025-02-26T00:32:16Z",
      "topics": [],
      "readme": "# AgentStack [![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/release/python-3100/) [![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT) ![python-testing](https://github.com/agentops-ai/agentstack/actions/workflows/python-testing.yml/badge.svg) ![mypy](https://github.com/agentops-ai/agentstack/actions/workflows/mypy.yml/badge.svg) [![codecov.io](https://codecov.io/github/agentops-ai/agentstack/coverage.svg?branch=master)](https://codecov.io/github/agentops-ai/agentstack>?branch=master)\n\n<img alt=\"Logo\" align=\"right\" src=\"https://raw.githubusercontent.com/bboynton97/agentstack-docs/3491fe490ea535e7def74c83182dfa8dcfb1f562/logo/dark-sm.svg\" width=\"20%\" />\n\nCreate AI agent projects from the command line.\n\n- [Quickstart Guide](https://docs.agentstack.sh/quickstart) – How to create a new agent project.\n- [Video Tutorial](https://www.loom.com/share/68d796b13cd94647bd1d7fae12b2358e?sid=7fdf595b-de84-4d51-9a81-ef1e9c8ac71c) – Follow along and build a web scrape agent with AgentStack\n\nAgentStack works on macOS, Windows, and Linux.<br>\nIf something doesn't work, please [file an issue](https://github.com/agentops-ai/agentstack/issues/new).<br>\nIf you have questions or need help, please ask in our [Discord community](https://discord.gg/JdWkh9tgTQ).\n\n> 🛠️🏃🏼‍♀️ The quickest way to build your powerful agent project\n\nAgentStack serves as a great tool for starting your agent project and offers many CLI utilities for easy code-gen throughout the development process.\n\nAgentStack is _not_ a low-code alternative to development. Developers will still need an understanding of how to build with their selected agent framework.\n\n## Quick Overview\n\n```sh\npip install agentstack\nagentstack init <project_name>\n```\n\nAgentStack scaffolds your _agent stack_ - the tech stack that collectively is your agent\n\n<p align='center'>\n<img src='https://github.com/AgentOps-AI/AgentStack/blob/7b40e53bf7300f69e3291c62d5b45e46ff818245/docs/images/the_agent_stack.png?raw=true' width='600' alt='agentstack init'>\n</p>\n\n### Get Started Immediately\n\nYou **don't** need to install or configure tools like LangChain or LlamaIndex.<br>\nThey are preconfigured and hidden so that you can focus on the code.\n\nCreate a project, and you're good to go.\n\n## Creating an Agent Project\n\n**You'll need to have Python 3.10+ on your local development machine**. We recommend using the latest version. You can use [pyenv](https://github.com/pyenv/pyenv) to switch Python versions between different projects.\n\nTo create a new agent project, run:\n\n```sh\npip install agentstack\nagentstack init <project_name>\n```\n\nIt will create a directory with your project name inside the current folder.<br>\nInside that directory, it will generate the initial project structure and install the transitive dependencies.\n\nNo configuration or complicated folder structures, only the files you need to build your agent project.<br>\nOnce the initialization is done, you can open your project folder:\n\n```sh\ncd <your_agent_project>\n```\n\n### Templates\nAdditionally, pre-built templates are available as a shortcut to building your project. [View the available templates]().\n\n## Building Agent Functionality\n\nAfter generating a project, the next step is to build your agent project by creating Agents and Tasks. You can do this quickly with AgentStack:\n\n```bash\nagentstack generate agent/task <name>\n```\n\nModify the agents and tasks by changing the `agents.yaml` and `tasks.yaml` configuration files in `src/config`\n\n## Tooling\n\nOne of AgentStack's core principles is to establish the de facto agent _stack_. A critical component of this stack is the tooling and functionality given to agents beyond simply LLM capabilities.\n\nAgentStack has worked to make access to tools as easy as possible, staying framework agnostic and featuring the best tools.\n\nA list of all tools can be found [on our docs](https://docs.agentstack.sh/tools/core).\n\nAdding tools is as simple as\n\n```bash\nagentstack tools add\n```\n\n## Running Your Agent\n\n`agentstack run`\n\nRuns the agent project in development mode.<br>\n\n> 👀 Support for easy production deployment of agents is coming soon.\n\n## Philosophy\n\n- **Agents should be easy:** There are so many frameworks out there, but starting from scratch is a pain. Similar to `create-react-app`, AgentStack aims to simplify the \"from scratch\" process by giving you a simple boilerplate of an agent. It uses popular agent frameworks and LLM providers, but provides a cohesive curated experience on top of them.\n\n- **No Configuration Required:** You don't need to configure anything. A reasonably good configuration of both development and production builds is handled for you so you can focus on writing code.\n\n- **No Lock-In:** You can customize your setup at any time. AgentStack is designed to make it easy to get the components you need running right off the bat; it's up to you what to do next.\n\nAgentStack is not designed to be a low-code solution to building agents. Instead it is a great head-start for starting an agent project from scratch.\n\n## Roadmap\n\n### Framework Agnosticism\n\nDevelopment of AgentStack is being done primarily on [CrewAI](https://crewai.com). We will soon be working to make AgentStack fully framework-agnostic, meaning that any supported multi-agent framework can be used for your project. \n\n### Tools\n- Core Tools built by AgentStack\n- Preferred partners in the package directly\n- Community partner tools added through external repositories\n\n### Other Features\n- Generated testing\n- Integrated benchmarking\n- Easy integration of tools for browsing, RAG, and more.\n- A fast interactive test runner with built-in support for coverage reporting.\n- A live development server that warns about common mistakes.\n- A build script to bundle your project for production.\n\n## License\n\nAgentStack is open source software [licensed as MIT](LICENSE).\n\n## How to Contribute\n\nAgentStack is a new project built by passionate AI agent developers! We'd love help making this tool better. Easy first issues are available, create new issues with feature ideas, or chat with us on our [Discord](https://discord.gg/JdWkh9tgTQ).\n\nIf you are an Agent Tool developer, feel free to create an issue or even a PR to add your tool to AgentStack. "
    },
    {
      "name": "niyathnair/NcluxhAI",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/102501152?s=40&v=4",
      "owner": "niyathnair",
      "repo_name": "NcluxhAI",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2025-01-13T15:40:12Z",
      "updated_at": "2025-01-19T06:46:19Z",
      "topics": [],
      "readme": "\n"
    },
    {
      "name": "vvalac/ai-experiments",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/48455003?s=40&v=4",
      "owner": "vvalac",
      "repo_name": "ai-experiments",
      "description": "My random AI experiments",
      "homepage": "",
      "language": "Jupyter Notebook",
      "created_at": "2023-08-31T17:28:11Z",
      "updated_at": "2025-01-08T22:25:12Z",
      "topics": [],
      "readme": ""
    },
    {
      "name": "sillysavant/e-boyfriend",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/64178919?s=40&v=4",
      "owner": "sillysavant",
      "repo_name": "e-boyfriend",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2024-11-18T23:08:00Z",
      "updated_at": "2025-01-06T21:06:28Z",
      "topics": [],
      "readme": ""
    },
    {
      "name": "ghchen99/agentic-conversations",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/56446026?s=40&v=4",
      "owner": "ghchen99",
      "repo_name": "agentic-conversations",
      "description": "This repository showcases an automated podcast system that generates engaging dialogue between an AI expert and industry professionals (chefs, teachers, nurses) about real-world AI applications.",
      "homepage": "",
      "language": "Python",
      "created_at": "2025-01-03T11:59:20Z",
      "updated_at": "2025-02-22T22:36:42Z",
      "topics": [],
      "readme": "# AI Podcast Generator\n\n## Overview\n\nThis project automates the creation of 20-minute podcast episodes featuring dynamic conversations between an AI expert and various professionals (chefs, teachers, nurses) about AI's impact on their work. Using Azure OpenAI for dialogue generation and Azure Speech Services for voice synthesis, it produces natural-sounding podcast episodes complete with distinct voices for each speaker and professional audio quality. Each episode explores how AI is transforming specific industries through authentic discussions between the host and industry practitioners.\n\n[Check out the `podcast_audio` folder for sample results 🔊](podcast_audio/podcast_nurse_20250210_145400.mp3)\n\n## Features\n\n* **Automated Conversation Generation**\n  * Creates realistic multi-turn dialogues between an AI expert and professionals\n  * Supports multiple professions (chef, teacher, nurse)\n  * Ensures natural conversation flow with proper turn-taking\n\n* **Audio Production**\n  * Converts text conversations to audio using Azure Speech Services\n  * Generates professional-quality podcast audio\n  * Uses different voices for each speaker\n  * Outputs high-quality MP3 files\n\n* **Robust Infrastructure**\n  * Comprehensive logging system\n  * Error handling and recovery\n  * Conversation storage in JSON format\n  * Modular and extensible design\n\n## Setup\n\n### Prerequisites\n\n* Python 3.7 or higher\n* Azure OpenAI API access\n* Azure Speech Services subscription\n\n### Installation\n\n1. Clone the repository\n2. Install dependencies:\n   ```bash\n   pip install -r requirements.txt\n   ```\n\n### Environment Configuration\n\nSet the following environment variables:\n\n```bash\n# Azure OpenAI Configuration\nAZURE_ENDPOINT=\"your_azure_openai_endpoint\"\nAZURE_API_KEY=\"your_azure_openai_api_key\"\nAZURE_API_VERSION=\"your_azure_openai_api_version\"\nAZURE_DEPLOYMENT=\"your_azure_openai_deployment\"\n\n# Azure Speech Services Configuration\nAZURE_SPEECH_KEY=\"your_azure_speech_key\"\nAZURE_SPEECH_REGION=\"your_azure_speech_region\"\n```\n\n## Project Structure\n\n```\n.\n├── conversation_generator.py    # Main conversation generation script\n├── audio_generator.py          # Text-to-speech conversion script\n├── conversations/             # Stored conversation JSON files\n├── podcast_audio/            # Generated MP3 files\n├── conversation_logs.log     # System logs\n└── requirements.txt          # Project dependencies\n```\n\n## Usage\n\n### Generating Conversations\n\nRun the conversation generator:\n\n```bash\npython conversation_generator.py\n```\n\nThis creates JSON files in the `conversations` directory for each profession.\n\n### Creating Podcast Audio\n\nGenerate audio from the conversations:\n\n```bash\npython audio_generator.py\n```\n\nThis processes all JSON files in `conversations` and creates MP3 files in `podcast_audio`.\n\n## Technical Details\n\n### Conversation System\n\n* **Structure**\n  * AI Expert (George) leads discussions about workplace AI\n  * Professional provides field-specific insights\n  * 15 complete exchanges (30 messages total)\n  * Context-aware responses maintain natural flow\n\n* **Turn Management**\n  * Strict alternating turns\n  * Validation of speaker order\n  * Conversation context tracking\n  * Natural transition handling\n\n### Audio Generation\n\n* **Features**\n  * Neural voice selection per speaker\n  * Intelligent pause insertion\n  * Chunk-based processing for large files\n  * High-quality audio output (16kHz, 32kbps mono MP3)\n\n* **Processing**\n  * SSML generation with voice switching\n  * Chunk size optimisation\n  * Temporary file management\n  * MP3 concatenation and cleanup\n\n## Error Handling\n\nThe system includes comprehensive error management:\n\n* Detailed logging of all operations\n* Graceful API failure handling\n* Automatic temporary file cleanup\n* Conversation validation\n* Environment variable verification\n\n## Customisation\n\nThe project can be customised by:\n\n* Adding new professions to `profession_examples`\n* Adjusting conversation length via `num_chunks`\n* Modifying voice selection in `get_voice_by_role`\n* Configuring audio output settings\n* Customizing conversation prompts and style\n\n## License\n\nThis project is licensed under the terms of the license found in the LICENSE file in the root directory of this project.\n\n## Contributing\n\nI welcome contributions to improve the AI Podcast Generator! Here's how you can help:\n\n### Reporting Issues\n* Use the GitHub issue tracker to report bugs\n* Include as much detail as possible: steps to reproduce, error messages, logs\n* Describe what you expected to happen\n\n### Pull Requests\n1. Fork the repository\n2. Create a new branch (`git checkout -b feature/your-feature`)\n3. Make your changes\n4. Run tests if available\n5. Commit your changes (`git commit -am 'Add new feature'`)\n6. Push to the branch (`git push origin feature/your-feature`)\n\n"
    },
    {
      "name": "graphlit/AgentStack",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/130105661?s=40&v=4",
      "owner": "graphlit",
      "repo_name": "AgentStack",
      "description": "The fastest way to build robust AI agents",
      "homepage": "",
      "language": null,
      "created_at": "2025-01-02T07:58:58Z",
      "updated_at": "2025-01-22T20:25:14Z",
      "topics": [],
      "readme": "# AgentStack [![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/release/python-3100/) [![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n\n<img alt=\"Logo\" align=\"right\" src=\"https://raw.githubusercontent.com/bboynton97/agentstack-docs/3491fe490ea535e7def74c83182dfa8dcfb1f562/logo/dark-sm.svg\" width=\"20%\" />\n\nCreate AI agent projects from the command line.\n\n- [Quickstart Guide](https://docs.agentstack.sh/quickstart) – How to create a new agent project.\n- [Video Tutorial](https://www.loom.com/share/68d796b13cd94647bd1d7fae12b2358e?sid=7fdf595b-de84-4d51-9a81-ef1e9c8ac71c) – Follow along and build a web scrape agent with AgentStack\n\nAgentStack works on macOS, Windows, and Linux.<br>\nIf something doesn't work, please [file an issue](https://github.com/agentops-ai/agentstack/issues/new).<br>\nIf you have questions or need help, please ask in our [Discord community](https://discord.gg/JdWkh9tgTQ).\n\n> 🛠️🏃🏼‍♀️ The quickest way to build your powerful agent project\n\nAgentStack serves as a great tool for starting your agent project and offers many CLI utilities for easy code-gen throughout the development process.\n\nAgentStack is _not_ a low-code alternative to development. Developers will still need an understanding of how to build with their selected agent framework.\n\n## Quick Overview\n\n```sh\npip install agentstack\nagentstack init <project_name>\n```\n\n\n<p align='center'>\n<img src='https://raw.githubusercontent.com/agentops-ai/agentstack/main/stack.png' width='600' alt='agentstack init'>\n</p>\n\n### Get Started Immediately\n\nYou **don't** need to install or configure tools like LangChain or LlamaIndex.<br>\nThey are preconfigured and hidden so that you can focus on the code.\n\nCreate a project, and you're good to go.\n\n## Creating an Agent Project\n\n**You'll need to have Python 3.10+ on your local development machine**. We recommend using the latest version. You can use [pyenv](https://github.com/pyenv/pyenv) to switch Python versions between different projects.\n\nTo create a new agent project, run:\n\n```sh\npip install agentstack\nagentstack init <project_name>\n```\n\nIt will create a directory with your project name inside the current folder.<br>\nInside that directory, it will generate the initial project structure and install the transitive dependencies.\n\nNo configuration or complicated folder structures, only the files you need to build your agent project.<br>\nOnce the initialization is done, you can open your project folder:\n\n```sh\ncd <your_agent_project>\n```\n\n### Templates\nAdditionally, pre-built templates are available as a shortcut to building your project. [View the available templates]().\n\n## Building Agent Functionality\n\nAfter generating a project, the next step is to build your agent project by creating Agents and Tasks. You can do this quickly with AgentStack:\n\n```bash\nagentstack generate agent/task <name>\n```\n\nModify the agents and tasks by changing the `agents.yaml` and `tasks.yaml` configuration files in `src/config`\n\n## Tooling\n\nOne of AgentStack's core principles is to establish the de facto agent _stack_. A critical component of this stack is the tooling and functionality given to agents beyond simply LLM capabilities.\n\nAgentStack has worked to make access to tools as easy as possible, staying framework agnostic and featuring the best tools.\n\nA list of all tools can be found [on our docs](https://docs.agentstack.sh/tools/core).\n\nAdding tools is as simple as\n\n```bash\nagentstack tools add\n```\n\n## Running Your Agent\n\n`agentstack run`\n\nRuns the agent project in development mode.<br>\n\n> 👀 Support for easy production deployment of agents is coming soon.\n\n## Philosophy\n\n- **Agents should be easy:** There are so many frameworks out there, but starting from scratch is a pain. Similar to `create-react-app`, AgentStack aims to simplify the \"from scratch\" process by giving you a simple boilerplate of an agent. It uses popular agent frameworks and LLM providers, but provides a cohesive curated experience on top of them.\n\n- **No Configuration Required:** You don't need to configure anything. A reasonably good configuration of both development and production builds is handled for you so you can focus on writing code.\n\n- **No Lock-In:** You can customize your setup at any time. AgentStack is designed to make it easy to get the components you need running right off the bat; it's up to you what to do next.\n\nAgentStack is not designed to be a low-code solution to building agents. Instead it is a great head-start for starting an agent project from scratch.\n\n## Roadmap\n\n### Framework Agnosticism\n\nDevelopment of AgentStack is being done primarily on [CrewAI](https://crewai.com). We will soon be working to make AgentStack fully framework-agnostic, meaning that any supported multi-agent framework can be used for your project. \n\n### Tools\n- Core Tools built by AgentStack\n- Preferred partners in the package directly\n- Community partner tools added through external repositories\n\n### Other Features\n- Generated testing\n- Integrated benchmarking\n- Easy integration of tools for browsing, RAG, and more.\n- A fast interactive test runner with built-in support for coverage reporting.\n- A live development server that warns about common mistakes.\n- A build script to bundle your project for production.\n\n## License\n\nAgentStack is open source software [licensed as MIT](LICENSE).\n\n## How to Contribute\n\nAgentStack is a new project built by passionate AI agent developers! We'd love help making this tool better. Easy first issues are available, create new issues with feature ideas, or chat with us on our [Discord](https://discord.gg/JdWkh9tgTQ).\n\nIf you are an Agent Tool developer, feel free to create an issue or even a PR to add your tool to AgentStack. "
    },
    {
      "name": "vccjlc/time_machine",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/140386750?s=40&v=4",
      "owner": "vccjlc",
      "repo_name": "time_machine",
      "description": "battle of bots - Streamlit app",
      "homepage": null,
      "language": "Python",
      "created_at": "2024-12-31T17:13:03Z",
      "updated_at": "2025-03-26T17:56:43Z",
      "topics": [],
      "readme": "# Launch the Time Machine App 🚀\n\n<a href=\"https://timemachinejanparaniak.streamlit.app/\" target=\"_blank\" style=\"display: inline-block; padding: 10px 20px; font-size: 1.2rem; font-weight: bold; color: white; background-color: #4CAF50; text-decoration: none; border-radius: 10px; box-shadow: 0 6px 8px rgba(0, 0, 0, 0.1);\">\n    Click Here to Launch the App\n</a>\n\nEnjoy the timeless conversations!\n\n## About the App\n\n**Time Machine** is an interactive application that brings historical and modern icons together in thought-provoking or amusing conversations. The app randomly selects two famous individuals from a pool of 67 legendary figures across diverse fields such as science, politics, sports, and entertainment, and places them in a unique setting to discuss an intriguing topic.\n\nThe topics range from fun facts and philosophical debates to hypothetical \"would-you-rather\" questions. The dialogue is styled to reflect the personalities of the chosen figures, making every encounter as entertaining as it is insightful.\n\nThe conversation begins with the initiator agent invoking God, who selects the topic and participants. A Host then clarifies the topic, introduces the two participants, and prompts them to present their arguments. Finally, the Host evaluates the discussion and may optionally declare a winner of the short debate.\n\n### Highlights:\n- **Dynamic Avatars**: Each character is represented with a distinct avatar for a visually engaging experience.\n- **Over 2,211 Possible Encounters**: With 67 famous figures, there are 2211 unique combinations of participants, ensuring endless variety.\n- **Topics**: From paradoxes to conspiracy theories, no two conversations are alike.\n- **Conversation Flavor**: God decides the tone—witty, serious, competitive, or moderate—keeping each interaction unique.\n- ** Endless Variety**: With 67 participants, 43 topics and 4 conversation flavors, the app offers 380,292 unique possibilities.\n\n### Design Principles\n- Simplicity\n- Different conversation flavors and topics selected randomly together with arguers\n- Short debates\n\n## How to Use\n\n1. Visit the app at **[https://timemachinejanparaniak.streamlit.app/](https://timemachinejanparaniak.streamlit.app/)**.\n2. Press the **Run** button to initiate a new conversation.\n3. Sit back and enjoy as historical icons, modern celebrities, and legendary thinkers share their thoughts on random topics in their unique styles.\n\n---\n![DALL·E 2025-01-10 10 04 44 - A photo-realistic depiction of Napoleon Bonaparte and the philosopher Plato actively engaged in a TV debate  Napoleon, dressed in his iconic military ](https://github.com/user-attachments/assets/eb492b38-85ad-4560-bc10-ddc94a6ed19d).\n\n## Technical Details\nThe app is built using Streamlit and incorporates multi-agent conversation logic. Here are some key technical highlights:\n\n- Agent Framework: Agents are implemented using autogen_agentchat, enabling dynamic interactions with role-specific behaviors such as God, Host, and Arguers.\n- Randomization: Famous individuals and topics are selected randomly from predefined lists. Conversation flavors are decided via a weighted random distribution (witty, serious, competitive, moderate).\n- Asynchronous Execution: Conversations are executed asynchronously using Python's asyncio, ensuring smooth handling of multiple steps in the dialogue process.\n- Dynamic Avatars: Each participant is represented visually using custom avatars loaded dynamically based on their identity.\n- Modular code structure allows for easy updates, such as adding new participants, topics, or conversation rules.\n"
    },
    {
      "name": "knd3dayo/AIChatExplorer",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/157713558?s=40&v=4",
      "owner": "knd3dayo",
      "repo_name": "AIChatExplorer",
      "description": null,
      "homepage": null,
      "language": "C#",
      "created_at": "2024-03-29T23:20:32Z",
      "updated_at": "2025-04-22T11:11:53Z",
      "topics": [],
      "readme": "# コピペ管理アプリ\n## 概要\n- クリップボードを監視してコピーされたアイテムを保存。自動処理や検索などを行う。\n- OpenAIのアカウント(APIキー)を持ってる場合はコピーされたアイテムに対して要約や箇条書き化などの処理が行える。\n- 日頃の仕事で多くを占める「コピー」「ペースト」「フォルダを開く」といった作業を補助することを目的としたもの。\n- コピぺについてのOutlook風な機能(検索フォルダや自動仕分けなど)の実装を目指すところ。\n- WPFアプリケーション。 .Net8を使用。\n- AIの処理などにはPythonを使用。\n- まだ開発中。\n\n## 機能\n- Windowsのクリップボードに張り付けられたテキスト、ファイル、画像を管理。\n  - コピーしたアイテムをこのアプリにペーストすることで、テキスト内容、ファイルパス、画像イメージをこのアプリに保存\n  - クリップボード監視機能を有効にすることで、Windowsクリップボードに張り付けられたアイテムを自動保存 \n  - アイテムはフォルダ階層で管理\n\n- OpenAIとの連携\n  - アイテムのタイトルを自動生成\n  - 過去の情報を基にした背景情報の生成\n  - ChatGPT風のチャット機能。通常のOpenAIチャットとLangChainでRAGを使用したチャットが使用可能\n  - gpt-4v、gpt-4oの画像処理機能を用いたチャットが使用可能。\n\n- RAG機能\n  - 同一フォルダのアイテムや外部のベクトルDBを使用したRAG機能を提供\n  - アイテム保存時に自動的にEmbeddingを生成\n  - 外部のベクトルDBとして利用可能なDBはFaiss、Chroma。PostgreSQL+PGVectorは今後対応予定\n  - Gitリポジトリで管理されたファイルをRAGのソースとして使用可能。最新コミットのファイル構成を元にEmbedding生成、指定したコミットからの差分を元にEmbedding生成などが可能。\n\n-------------------------------------------\n\n## 実行方法\n1. このリポジトリをgit cloneする。\n2. Visual Studio 2022をインストールする。このフォルダにある`WpfApp1.sln`をダブルクリックしてVisualStudioを起動。ビルドする。\n3. Pythonをインストール\n4. Pythonモジュールのインストール\nこのフォルダにあるrequreiments.txtに記載のモジュールをインストールする.\n```\npip install -r requreiments.txt\n```\n5. ClipboardApp.exeを起動する。\n\n6. 設定メニュー\n- PythonDLLPath: Pythonインストール先のフォルダの`python311.dll`を設定する。  \n  例：C:\\Users\\user\\AppData\\Local\\Programs\\Python\\python311.dll\n\n- PythonExecution:0はPythonを用いた処理を行わない。1はPythonを用いた処理を行う。\n\n- OpenAIのAPI Key,モデルなどの情報を設定する。\n\n\n"
    },
    {
      "name": "r-bit-rry/magentic-two",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/34023431?s=40&v=4",
      "owner": "r-bit-rry",
      "repo_name": "magentic-two",
      "description": "magentic-one unofficial fork for local agent",
      "homepage": null,
      "language": "Python",
      "created_at": "2024-11-30T20:26:16Z",
      "updated_at": "2025-02-21T05:21:27Z",
      "topics": [],
      "readme": "# magentic-two\nA local take on autogen magentic-one with custom ollama connection using llama3.2-vision\n\nplaywright install --with-deps chromium\n### Notes\n1. activate environment: source ../../.venv/bin/activate\n2. Start docker desktop\n3. Use the create local agent method from utils.py\n4. Use Ollama chat client with vision support.\n5. Use forcing locale of playwright chromium:\n\n### task\nI want you to pick 5 stocks that you think will perform well in the next 6 weeks. Please provide a brief explanation for each stock. I want you to go over reddit.com/r/wallstreetbets and the website stockwits along with twitter to find the most discussed stocks. The stocks need to be cheap still, that is, close to the value at IPO, have not been pump and dump yet, and the companies are not bankrupt or shady. you need to pick stocks for holding positions only, no shorts, no options. Good examples for stocks that worked well recently are: RKLB, ASTS, LUNR, MTEK, SATL. Only write and use code if you find it necessary, most of the tasks can be done by reading and summarizing information.\n"
    },
    {
      "name": "1012638836/ai_qiniu_chatbot",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/29818043?s=40&v=4",
      "owner": "1012638836",
      "repo_name": "ai_qiniu_chatbot",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2024-12-28T04:52:59Z",
      "updated_at": "2025-03-30T12:49:26Z",
      "topics": [],
      "readme": "conda activate faiss\nnohup python faiss_server_app.py >> /www/logs/faiss_server/main_logger.log 2>&1 &\ntail -f /www/logs/faiss_server/main_logger.log\n\nconda activate faiss\nnohup python bge_reranker.py >> ./bge_reranker.log 2>&1 &\ncurl --location --request POST 'http://127.0.0.1:5000/v1/rerank' --header 'Content-Type: application/json' --data '[[\"有回放吗?\", \"今天有回放吗？\"], [\"有回放吗？\", \"今天上课吗？\"]]'\n\nconda activate faiss\nnohup python bge_embedding.py >> ./bge_embedding.log 2>&1 &\ncurl --location --request POST 'http://127.0.0.1:50072/embedding' --header 'Content-Type: application/json' --data '{\"text\": \"你好\"}'\n\nconda activate ai_qiniu_chatbot\nnohup litellm --model ollama/qwen2.5:7b >> ./litellm.log 2>&1 &\ncurl http://localhost:4000/v1/chat/completions   -H \"Content-Type: application/json\"   -d '{\"model\": \"qwen2.5:7b\", \"messages\": [{\"role\": \"user\", \"content\": \"Hello! What is your name?\"}]}'"
    },
    {
      "name": "shreyanavelkar/automated-ai-assistant",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/28059200?s=40&v=4",
      "owner": "shreyanavelkar",
      "repo_name": "automated-ai-assistant",
      "description": "performs routine tasks like scheduling meetings, setting reminders, or sending emails",
      "homepage": null,
      "language": "Python",
      "created_at": "2024-12-24T23:08:55Z",
      "updated_at": "2025-02-11T13:12:33Z",
      "topics": [],
      "readme": ""
    },
    {
      "name": "missingpages/AI-Teacher",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/65895277?s=40&v=4",
      "owner": "missingpages",
      "repo_name": "AI-Teacher",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2024-12-15T04:39:27Z",
      "updated_at": "2025-01-25T04:02:06Z",
      "topics": [],
      "readme": "# Socratix - Socratic AI Teacher\nSocratic Teacher is one who fosters critical thinking in students through engaging with students in thought provoking dialogue, Understand the students’ beliefs ,opinions and misconceptions with probing questions and help them find answers on their own. The teacher helps students discover truths and refine their understanding, making learning more personal and profound.\n\nWith the rise of online education platforms , we see more people pursuing their learning goals through MOOC. Online courses democratize education by providing access to high-quality content from world-renowned institutions and experts and offers flexibility. However the downside is the lack of one-one engagement we used to have with teachers in class rooms. The future of education lie in middle ground between the two, offering both flexibility and personalized engagement.\n\nOur attempt here, as part of LLM Agents hackathon by Berkeley RDI, is to impart human elements of traditional teaching to online learning process with a virtual teacher aka socratic AI teacher. The socratic AI teacher is an agentic framework to augment the online learning with socratic teaching methods and personalized engagement.\n## Features\n### 1. Personalized teaching\n![Personalization](images/PersonalizedTeaching-eg.png)\n\n### 2. Socratic Teaching \n<img src=\"images/foundationConcepts-eg.png\" alt=\"Socratic\" width=\"250\" height=\"500\">\n\n\nFor eg. To know the concept of **'electric flux'**, understanding of **'electric field'** is important!\n\nOfcourse, the response is personalized to students likings towards **poetry**!\n    \n## Pre-requisites\nRequires access to\n- Neo4j\n- LLM APIs (by default requires openAI gpt-4o-mini for agents, Gemini for Data extraction)\n\n## Instructions\n- create conda env or python venv\n- Run 'requirements.txt'\n- ensure neo4j credentials are entered in 'neo4j-local.txt'\n- set OpenAI API key as an environment variable OPENAI_API_KEY.\n- set Google API key as an environment variable GEMINI_API_KEY.\n  \n- OPTIONAL : Change user persona details in profile/student.json\n  One persona will be randomly picked & assumed for the student\n    \n- To create Knowledge base with learning materials(pdfs):\n    - cd into src/kb\n    - Run \"python data_pipeline.py <pdf_path>\"\n      This will extract all the contents from pdf, discover concepts, create concept hierarchy and then load to Neo4j. Also creates vector embedding for concept.\n    - Sample pdf files used in demo are available in **sample-data** folder\n\n- To start backend api server\n  - cd into src\n  - Run \"python api.py\"\n    The backend api service will be available at http://127.0.0.1:5000\n \n- To start GUI\n  - cd into src\n  - Run \"python app.py\"\n    - This will bring up the UI to access the learning content\n    - The UI can be accessed through the url - http://127.0.0.1:8050\n"
    },
    {
      "name": "parth-verma7/agentic-ai-task",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/96720577?s=40&v=4",
      "owner": "parth-verma7",
      "repo_name": "agentic-ai-task",
      "description": "AI Agent for Data Analysis",
      "homepage": "https://www.loom.com/share/44aabc17451e4ae6b309198d7d246650?sid=55f53291-da56-4ee1-a826-7ed9a3d0248a",
      "language": "Python",
      "created_at": "2024-12-10T07:08:02Z",
      "updated_at": "2025-01-07T08:03:38Z",
      "topics": [
        "agent-based-simulation",
        "agents",
        "ai",
        "autogen",
        "data-analysis",
        "groupchat-manager"
      ],
      "readme": "# Exploratory Data Analysis Report AGI\n\n**Remember**  \nThis is an example report demonstrating how my **Data Analysis Agent** performed **Exploratory Data Analysis (EDA)** on a specific Hugging Face dataset link **AUTONOMOUSLY**, showcasing its capabilities as an **AGI-like system**.\n\n## User Proxy\n\n    Perform an in-depth exploratory data analysis (EDA) on the input dataset. \n \n    Examine patterns, trends, and statistical significance, including class imbalances, dataset format, and the number of conversation turns. \n\n    Generate graphs and charts to visually represent data patterns, highlight key findings, and detect anomalies. \n\n    Provide a detailed analysis report with insights and visualizations. \n\n\n\n## Assistant 1\nInstruction 1:  Perform an initial data cleaning and preprocessing step.  This should include checking for missing values in the 'title' and 'text' columns, handling any missing values appropriately (e.g., imputation or removal), and converting text to lowercase.  Then, calculate the length of the 'title' and 'text' columns (character count) and add these as new columns named 'title_length' and 'text_length' respectively. Finally, create a bar chart showing the distribution of spam and not spam emails.  Pass the cleaned dataset and the generated bar chart to assistant2.\n\n\n\n## Assistant 2\n```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Sample data (replace with your actual dataset)\ndata = {'title': ['?? the secrets to SUCCESS', '?? You Earned 500 GCLoot Points', '?? Your GitHub launch code', 'Re: ** Clarifications', '10-1 MLB Expert Inside'], \n        'text': ['Hi James,\\n\\nHave you claim your complimentary...', 'alt_text\\nCongratulations, you just earned...', \"Here's your GitHub launch code, @Mortyj420!\\n...\", 'Hello,\\n \\nThank you for contacting the Virtua...', 'Hey Prachanda Rawal,\\n\\nToday\\'s newsletter is ...'], \n        'type': ['spam', 'not spam', 'not spam', 'not spam', 'spam']}\ndf = pd.DataFrame(data)\n\n\n# Data Cleaning and Preprocessing\ndf['title'] = df['title'].fillna('')  # Fill missing titles with empty string\ndf['text'] = df['text'].fillna('')  # Fill missing texts with empty string\ndf['title'] = df['title'].str.lower()\ndf['text'] = df['text'].str.lower()\n\ndf['title_length'] = df['title'].str.len()\ndf['text_length'] = df['text'].str.len()\n\n# Bar chart of spam vs. not spam\nplt.figure(figsize=(8, 6))\nsns.countplot(x='type', data=df)\nplt.title('Distribution of Spam and Not Spam Emails')\nplt.xlabel('Email Type')\nplt.ylabel('Count')\nplt.savefig('spam_distribution.png')\nplt.show()\n\n\n# Pass the cleaned dataset and file path to assistant2 (simulated)\nprint(\"Cleaned dataset (first 5 rows):\\n\", df.head())\nprint(\"\\nBar chart saved to: spam_distribution.png\")\n\n#For assistant2, this is what would be passed:\n#Cleaned Dataset: df\n#Chart Path: 'spam_distribution.png'\n\n```\n\n**Key Insights from the Bar Chart:**\n\nThe bar chart visually represents the proportion of spam and not spam emails in the dataset.  This allows for a quick assessment of class imbalance.  A significant difference in the counts between the two classes would indicate a potential imbalance that might need to be addressed during model training (if a classification model is to be built).  Without the actual data, I can't provide specific numbers, but the chart provides this crucial information at a glance.\n\n\n**Files generated:**\n\n* `spam_distribution.png`:  A bar chart showing the distribution of spam and not spam emails.\n\n\n**Information for Assistant 2:**\n\nThe cleaned dataset `df` (including the added `title_length` and `text_length` columns) and the path to the generated bar chart (`spam_distribution.png`) are ready for further analysis by assistant 2.  The bar chart provides an initial understanding of the class distribution in the dataset.\n\n![spam_distribution.png](./charts/spam_distribution.png)\n\n\n## Assistant 1\nInstruction 2:  Analyze the text length distributions ('title_length' and 'text_length') for both spam and not spam emails. Generate separate histograms for 'title_length' and 'text_length', with each histogram showing separate distributions for spam and not spam.  Calculate and report the descriptive statistics (mean, median, standard deviation) for 'title_length' and 'text_length' for both spam and not spam categories.  This will help identify potential differences in text length characteristics between the two classes.  Pass the histograms and descriptive statistics to assistant2.\n\n\n\n## Assistant 2\n```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Assuming df is the dataframe from the previous step (replace with your actual df)\n# Sample data (replace with your actual dataset and results from instruction 1)\ndata = {'title': ['?? the secrets to SUCCESS', '?? You Earned 500 GCLoot Points', '?? Your GitHub launch code', 'Re: ** Clarifications', '10-1 MLB Expert Inside','Another Spam Title'], \n        'text': ['Hi James,\\n\\nHave you claim your complimentary...', 'alt_text\\nCongratulations, you just earned...', \"Here's your GitHub launch code, @Mortyj420!\\n...\", 'Hello,\\n \\nThank you for contacting the Virtua...', 'Hey Prachanda Rawal,\\n\\nToday\\'s newsletter is ...', 'This is another spam message.'], \n        'type': ['spam', 'not spam', 'not spam', 'not spam', 'spam', 'spam'],\n        'title_length': [25, 30, 26, 21, 24, 18],\n        'text_length': [47, 44, 47, 47, 47, 26]}\ndf = pd.DataFrame(data)\n\n\n# Histograms\nplt.figure(figsize=(12, 5))\n\nplt.subplot(1, 2, 1)\nsns.histplot(data=df, x='title_length', hue='type', kde=True)\nplt.title('Title Length Distribution (Spam vs. Not Spam)')\nplt.xlabel('Title Length')\nplt.ylabel('Frequency')\n\nplt.subplot(1, 2, 2)\nsns.histplot(data=df, x='text_length', hue='type', kde=True)\nplt.title('Text Length Distribution (Spam vs. Not Spam)')\nplt.xlabel('Text Length')\nplt.ylabel('Frequency')\n\nplt.tight_layout()\nplt.savefig('text_length_histograms.png')\nplt.show()\n\n\n# Descriptive statistics\ndescriptive_stats = df.groupby('type')[['title_length', 'text_length']].agg(['mean', 'median', 'std'])\nprint(\"Descriptive Statistics:\\n\", descriptive_stats)\n\n\n# Pass results to assistant2 (simulated)\nprint(\"\\nHistograms saved to: text_length_histograms.png\")\n#For assistant2:\n#Descriptive Stats: descriptive_stats\n#Histogram Path: 'text_length_histograms.png'\n\n```\n\n**Key Insights from Histograms and Descriptive Statistics:**\n\nThe histograms visually compare the distributions of `title_length` and `text_length` for spam and not spam emails.  Overlapping distributions suggest that text length alone might not be a strong discriminator between the two classes. However, differences in means, medians, or standard deviations (as shown in the descriptive statistics table) can hint at subtle differences that may be useful in a classification model.  For example, if spam emails consistently have a much higher average text length than non-spam emails, that could be a valuable feature.  A large standard deviation could indicate high variability within each class.\n\n\n**Files generated:**\n\n* `text_length_histograms.png`: Two histograms, one for `title_length` and one for `text_length`, each showing distributions for spam and not spam emails.\n\n\n**Information for Assistant 2:**\n\nThe descriptive statistics table (`descriptive_stats`) and the path to the generated histograms (`text_length_histograms.png`) are provided for further analysis.  The visualizations and statistical summaries can help to understand the relationship between email text length and the spam/not spam classification.\n\n![text_length_histograms.png](./charts/text_length_histograms.png)\n\n\n## User Proxy\n \nCode output: \nDescriptive Statistics:\n          title_length                  text_length                  \n                 mean median       std        mean median        std\ntype                                                                \nnot spam    25.666667   26.0  4.509250        46.0   47.0   1.732051\nspam        22.333333   24.0  3.785939        40.0   47.0  12.124356\n\nHistograms saved to: text_length_histograms.png\n\n\n## Assistant 1\nInstruction 3: Explore the relationship between the length of the email title and the length of the email text. Create a scatter plot to visualize this relationship, with points colored by email type (spam/not spam).  Calculate the Pearson correlation coefficient between `title_length` and `text_length` for the entire dataset and separately for spam and not spam emails.  This will help determine if there's a correlation between the lengths of titles and texts and if this correlation differs between spam and not spam emails. Pass the scatter plot and correlation coefficients to assistant2.\n\n\n\n\n## Assistant 2\n```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\n# Assuming df is the dataframe from the previous steps (replace with your actual df)\n# Sample data (replace with your actual dataset and results from previous instructions)\ndata = {'title': ['?? the secrets to SUCCESS', '?? You Earned 500 GCLoot Points', '?? Your GitHub launch code', 'Re: ** Clarifications', '10-1 MLB Expert Inside','Another Spam Title'], \n        'text': ['Hi James,\\n\\nHave you claim your complimentary...', 'alt_text\\nCongratulations, you just earned...', \"Here's your GitHub launch code, @Mortyj420!\\n...\", 'Hello,\\n \\nThank you for contacting the Virtua...', 'Hey Prachanda Rawal,\\n\\nToday\\'s newsletter is ...', 'This is another spam message.'], \n        'type': ['spam', 'not spam', 'not spam', 'not spam', 'spam', 'spam'],\n        'title_length': [25, 30, 26, 21, 24, 18],\n        'text_length': [47, 44, 47, 47, 47, 26]}\ndf = pd.DataFrame(data)\n\n\n# Scatter plot\nplt.figure(figsize=(8, 6))\nsns.scatterplot(data=df, x='title_length', y='text_length', hue='type')\nplt.title('Relationship between Title Length and Text Length')\nplt.xlabel('Title Length')\nplt.ylabel('Text Length')\nplt.savefig('title_text_length_scatter.png')\nplt.show()\n\n\n# Correlation coefficients\ncorrelation_overall = df['title_length'].corr(df['text_length'])\ncorrelation_spam = df[df['type'] == 'spam']['title_length'].corr(df[df['type'] == 'spam']['text_length'])\ncorrelation_not_spam = df[df['type'] == 'not spam']['title_length'].corr(df[df['type'] == 'not spam']['text_length'])\n\nprint(f\"Overall correlation: {correlation_overall:.2f}\")\nprint(f\"Correlation for spam emails: {correlation_spam:.2f}\")\nprint(f\"Correlation for not spam emails: {correlation_not_spam:.2f}\")\n\n\n# Pass results to assistant2 (simulated)\nprint(\"\\nScatter plot saved to: title_text_length_scatter.png\")\n#For assistant2:\n#Scatter Plot Path: 'title_text_length_scatter.png'\n#Overall Correlation: correlation_overall\n#Spam Correlation: correlation_spam\n#Not Spam Correlation: correlation_not_spam\n\n```\n\n**Key Insights from Scatter Plot and Correlation Coefficients:**\n\nThe scatter plot visualizes the relationship between `title_length` and `text_length`, with points colored by email type.  The correlation coefficients quantify the strength and direction of this linear relationship.  A positive correlation (close to 1) indicates that longer titles tend to be associated with longer texts, while a negative correlation (close to -1) indicates the opposite. A correlation close to 0 suggests little to no linear relationship. Comparing the correlation coefficients for spam and not spam emails reveals whether this relationship differs between the two email types.  For example, if the correlation is much stronger for spam emails than for non-spam emails, this could be a helpful feature for classification.\n\n\n**Files generated:**\n\n* `title_text_length_scatter.png`: A scatter plot showing the relationship between `title_length` and `text_length`, colored by email type.\n\n\n**Information for Assistant 2:**\n\nThe path to the scatter plot (`title_text_length_scatter.png`) and the calculated correlation coefficients (overall, spam, and not spam) are provided for further analysis.  These results help determine if there's a relationship between title and text length and if this relationship differs between spam and not spam emails.\n\n\n![title_text_length_scatter.png](./charts/title_text_length_scatter.png)\n\n\n## User Proxy\n \nCode output: \nOverall correlation: 0.62\nCorrelation for spam emails: 0.99\nCorrelation for not spam emails: -0.83\n\nScatter plot saved to: title_text_length_scatter.png\n\n\n## Assistant 1\nInstruction 4:  Perform a basic text analysis on the 'text' column.  Specifically, calculate the average number of words per email for both spam and not spam emails.  Also, create a word cloud visualization for the most frequent words in the 'text' column for both spam and not spam emails separately. This will give insights into the vocabulary and writing styles that are typical of spam versus non-spam emails. Pass the average words per email and the word cloud visualizations to assistant2.\n\n\n\n## Assistant 2\n```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\n\nnltk.download('punkt')\nnltk.download('stopwords')\n\n# Assuming df is the dataframe from the previous steps (replace with your actual df)\n# Sample data (replace with your actual dataset and results from previous instructions)\ndata = {'title': ['?? the secrets to SUCCESS', '?? You Earned 500 GCLoot Points', '?? Your GitHub launch code', 'Re: ** Clarifications', '10-1 MLB Expert Inside','Another Spam Title'], \n        'text': ['Hi James,\\n\\nHave you claim your complimentary...', 'alt_text\\nCongratulations, you just earned...', \"Here's your GitHub launch code, @Mortyj420!\\n...\", 'Hello,\\n \\nThank you for contacting the Virtua...', 'Hey Prachanda Rawal,\\n\\nToday\\'s newsletter is ...', 'This is another spam message.'], \n        'type': ['spam', 'not spam', 'not spam', 'not spam', 'spam', 'spam'],\n        'title_length': [25, 30, 26, 21, 24, 18],\n        'text_length': [47, 44, 47, 47, 47, 26]}\ndf = pd.DataFrame(data)\n\n# nltk.download('punkt')  # Download punkt tokenizer if you haven't already\n# nltk.download('stopwords')  # Download stopwords if you haven't already\n\nstop_words = set(stopwords.words('english'))\n\ndef preprocess_text(text):\n    tokens = word_tokenize(text.lower())\n    words = [word for word in tokens if word.isalnum() and word not in stop_words]\n    return words\n\ndf['words'] = df['text'].apply(preprocess_text)\ndf['word_count'] = df['words'].apply(len)\n\n# Average words per email\navg_words_spam = df[df['type'] == 'spam']['word_count'].mean()\navg_words_not_spam = df[df['type'] == 'not spam']['word_count'].mean()\n\nprint(f\"Average words per spam email: {avg_words_spam:.2f}\")\nprint(f\"Average words per not spam email: {avg_words_not_spam:.2f}\")\n\n\n# Word clouds\ndef create_wordcloud(text_list, filename):\n    text = \" \".join(word for sublist in text_list for word in sublist)\n    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)\n    plt.figure(figsize=(10, 5))\n    plt.imshow(wordcloud, interpolation='bilinear')\n    plt.axis(\"off\")\n    plt.savefig(filename)\n    plt.show()\n\nspam_words = df[df['type'] == 'spam']['words'].tolist()\nnot_spam_words = df[df['type'] == 'not spam']['words'].tolist()\n\ncreate_wordcloud(spam_words, 'spam_wordcloud.png')\ncreate_wordcloud(not_spam_words, 'not_spam_wordcloud.png')\n\n# Pass results to assistant2 (simulated)\nprint(\"\\nWord clouds saved to: spam_wordcloud.png and not_spam_wordcloud.png\")\n#For assistant2:\n#Average Spam Words: avg_words_spam\n#Average Not Spam Words: avg_words_not_spam\n#Spam Wordcloud Path: 'spam_wordcloud.png'\n#Not Spam Wordcloud Path: 'not_spam_wordcloud.png'\n\n```\n\n**Key Insights from Word Counts and Word Clouds:**\n\n\nThe average number of words per email provides a quantitative measure of email length. Differences in average word counts between spam and not spam emails can indicate differences in communication styles.\n\nThe word clouds visualize the most frequent words in spam and not spam emails.  By comparing the word clouds, we can identify words or phrases that are characteristic of each email type. For example, spam emails might contain words related to promotions, urgency, or financial incentives, while non-spam emails might have words related to work, personal communication, or specific topics.  This visual comparison reveals important differences in vocabulary and writing styles between the two classes.\n\n\n**Files generated:**\n\n* `spam_wordcloud.png`: Word cloud for spam emails.\n* `not_spam_wordcloud.png`: Word cloud for not spam emails.\n\n\n**Information for Assistant 2:**\n\nThe average word counts for spam and not spam emails, along with the paths to the generated word clouds, are provided for further analysis.  These results offer valuable insights into the linguistic characteristics of spam and not spam emails.\n\n\n![spam_wordcloud.png](./charts/spam_wordcloud.png)\n![not_spam_wordcloud.png](./charts/not_spam_wordcloud.png)\n\n\n## User Proxy\n \nCode output: \nAverage words per spam email: 4.00\nAverage words per not spam email: 3.33\n\nWord clouds saved to: spam_wordcloud.png and not_spam_wordcloud.png\n\n\n## Assistant 1\nInstruction 5:  Investigate the presence of specific keywords or phrases commonly associated with spam emails in the 'text' column. Create a list of potential spam keywords (e.g., \"free,\" \"prize,\" \"win,\" \"urgent,\" \"limited time\").  Then, calculate the frequency of each keyword in both spam and not spam emails. Create a bar chart showing the frequency of each keyword for both classes to visually compare their presence in spam and not spam emails.  Pass the keyword frequencies and the bar chart to assistant2.\n\n\n\n## Assistant 2\n```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Assuming df is the dataframe from previous steps (replace with your actual df)\n# Sample data (replace with your actual dataset and results from previous instructions)\ndata = {'title': ['?? the secrets to SUCCESS', '?? You Earned 500 GCLoot Points', '?? Your GitHub launch code', 'Re: ** Clarifications', '10-1 MLB Expert Inside','Another Spam Title', 'Urgent Update'], \n        'text': ['Hi James,\\n\\nHave you claim your complimentary...', 'alt_text\\nCongratulations, you just earned...', \"Here's your GitHub launch code, @Mortyj420!\\n...\", 'Hello,\\n \\nThank you for contacting the Virtua...', 'Hey Prachanda Rawal,\\n\\nToday\\'s newsletter is ...', 'This is another spam message.  You won a prize!', 'Urgent update required! Limited time offer!'], \n        'type': ['spam', 'not spam', 'not spam', 'not spam', 'spam', 'spam', 'spam'],\n        'title_length': [25, 30, 26, 21, 24, 18, 16],\n        'text_length': [47, 44, 47, 47, 47, 26, 39]}\ndf = pd.DataFrame(data)\n\nspam_keywords = [\"free\", \"prize\", \"win\", \"urgent\", \"limited time\", \"offer\"]\n\nkeyword_counts = {}\nfor keyword in spam_keywords:\n    keyword_counts[keyword] = {\n        'spam': df[df['type'] == 'spam']['text'].str.contains(keyword).sum(),\n        'not spam': df[df['type'] == 'not spam']['text'].str.contains(keyword).sum()\n    }\n\nkeyword_df = pd.DataFrame.from_dict(keyword_counts, orient='index')\nkeyword_df = keyword_df.reset_index()\nkeyword_df = keyword_df.rename(columns={'index': 'keyword'})\nkeyword_df = keyword_df.melt(id_vars=['keyword'], var_name='type', value_name='frequency')\n\n\n# Bar chart\nplt.figure(figsize=(12, 6))\nsns.barplot(x='keyword', y='frequency', hue='type', data=keyword_df)\nplt.title('Frequency of Spam Keywords in Spam and Not Spam Emails')\nplt.xlabel('Keyword')\nplt.ylabel('Frequency')\nplt.xticks(rotation=45, ha='right')\nplt.tight_layout()\nplt.savefig('keyword_frequency.png')\nplt.show()\n\nprint(\"Keyword frequencies:\\n\", keyword_df)\nprint(\"\\nBar chart saved to: keyword_frequency.png\")\n\n#For assistant2:\n#Keyword Frequencies: keyword_df\n#Bar Chart Path: 'keyword_frequency.png'\n\n```\n\n**Key Insights from Keyword Frequencies:**\n\nThe bar chart and keyword frequencies show how often certain words associated with spam appear in spam and non-spam emails.  A large discrepancy in the frequency of these keywords between spam and not spam emails highlights words that are strong indicators of spam.  For example, if \"free\" or \"prize\" appears much more frequently in spam emails, these words are likely helpful features for distinguishing spam from legitimate emails.\n\n\n**Files generated:**\n\n* `keyword_frequency.png`: Bar chart showing the frequency of each keyword in spam and not spam emails.\n\n\n**Information for Assistant 2:**\n\nThe keyword frequencies table (`keyword_df`) and the path to the generated bar chart (`keyword_frequency.png`) are provided for further analysis.  These results help to identify keywords that are strongly associated with spam emails and could be useful features for spam detection models.\n\n\n![keyword_frequency.png](./charts/keyword_frequency.png)\n\n\n## User Proxy\n \nCode output: \nKeyword frequencies:\n          keyword      type  frequency\n0           free      spam          0\n1          prize      spam          1\n2            win      spam          0\n3         urgent      spam          0\n4   limited time      spam          0\n5          offer      spam          1\n6           free  not spam          0\n7          prize  not spam          0\n8            win  not spam          0\n9         urgent  not spam          0\n10  limited time  not spam          0\n11         offer  not spam          0\n\nBar chart saved to: keyword_frequency.png\n\n\n"
    },
    {
      "name": "richard-gyiko/aura",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/36221820?s=40&v=4",
      "owner": "richard-gyiko",
      "repo_name": "aura",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2024-11-03T06:02:53Z",
      "updated_at": "2025-01-07T21:03:36Z",
      "topics": [],
      "readme": "# 🌟 Welcome to Aura - My Personal AI Assistant Journey! \n\nHey there! 👋 Welcome to my open-source project where I'm documenting my journey of creating Aura, my personal AI assistant. This project represents my exploration into building an AI companion that helps me with everyday developer tasks.\n\n## 🎯 Vision\nStarting with simple email management tasks, Aura is designed to grow and evolve, continuously expanding its capabilities to become a more comprehensive developer assistant. This is just the beginning of an exciting journey where I'll be pushing the boundaries of what's possible with AI assistance in development workflows.\n\n## 📚 Follow Along\nI'm sharing my experiences, challenges, and learnings through a series of blog posts on Medium. If you're interested in the behind-the-scenes of building an AI assistant, or want to learn from my experiences, check out my articles:\n\n[Follow my journey on Medium](https://medium.com/@richard.gyiko)\n\n## ✨ Features\n\n### How to Use\nAura is currently a console-based application where you can interact with it through simple text commands. Just run the application and start typing your requests - Aura will help you manage your emails through natural language interactions! Don't expect too much just yet, we're just getting started... 😉\n\n### Planned Capabilities\n- [x] Tools: Gmail - Get and search emails\n- [x] Tools: Gmail - Create draft emails\n- [x] Tools: Google Calendar - Get and search events\n- [x] Tools: Google Calendar - Create events\n- [ ] Tools: Playwright - Automate web interactions\n- [ ] Tools: Notion - Create and manage notes, datasets, and tasks\n- [ ] Tools: File System - Manage files and directories\n- [ ] Tools: GitHub - Manage issues for project management\n- [ ] Agent: Inbox Assistant\n  - [ ] Extra: Unsubscribe from newsletters\n  - [ ] Extra: Download attachments for further processing\n- [ ] Agent: Schedule Coordinator\n- [ ] Agent: Research Assistant\n- [ ] Agent: Project Management Assistant\n- [ ] Team Collaboration: Organize agents to teams for specific tasks\n- [ ] Personalization: Learn from user interactions (Mem0)\n- [ ] Personalization: Introduce voice interactions\n- [ ] Agent: Coding Assistant (Integrating Aider somehow?)\n\n\n## 🚀 Getting Started\nTo run Aura on your local machine, follow these steps:\n\n### Prerequisites\n- Python 3.12 or higher\n- A Google Cloud Platform project with the Gmail API and Google Calendar API enabled\n- A Google Cloud Platform service account with the necessary permissions\n- A `credentials.json` file for the service account placed in the root directory\n- A `.env` file with the following environment variables:\n  - `OPENAI_API_KEY` - Your OpenAI API key\n  - `OPENAI_ORG_ID` - Your OpenAI organization ID (optional)\n  - `OPENAI_PROJECT_ID` - Your OpenAI project ID (optional)\n\n### Running the Application\n\n```bash\npython -m src.main\n```\n\n### Technical ToDos\n- [ ] Upgrade to the AgentChat layer if mature enough\n\n## 🤝 Join the Journey\nThis is an open-source project, and I believe in the power of community and shared knowledge. Feel free to explore the code, share your thoughts, or even contribute to Aura's development!\n\nStay tuned for more updates as Aura grows and evolves! ✨\n"
    },
    {
      "name": "CalebJKim/DataPilot",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/40844938?s=40&v=4",
      "owner": "CalebJKim",
      "repo_name": "DataPilot",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2024-09-30T22:05:35Z",
      "updated_at": "2024-12-20T07:44:27Z",
      "topics": [],
      "readme": "# 🚀 Data Engineer Agent for Observability Tasks\n\n## Overview\nWelcome to the **Data Engineer Agent Project**! This project aims to automate data ingestion pipelines and streamline Business Intelligence (BI) reporting using **StateFlow** and **Large Language Models (LLMs)**. We're also exploring automation for DevOps teams by generating real-time incident response playbooks based on observability alerts and codebase knowledge. All powered by advanced AI, this agent will reduce manual intervention and boost system efficiency.\n\n### Key Features\n- **SQL Query Generation**: Translate business-level requests into **SQL queries** using state-of-the-art **LLMs**.\n- **Business Intelligence Insights**: Effortlessly generate BI reports using real-time data.\n- **Incident Response Automation**: DevOps teams can easily request data through natural language querying without worrying about structure.\n- **Enterprise-Grade Robustness**: Capacity to scale for multi-database environments while ensuring **trustworthiness** and **accuracy** in complex data ecosystems.\n\n### Why This Matters\nManaging data pipelines and handling BI requests can be time-consuming. With our Data Engineer Agent, we aim to achieve:\n- 📊 **Instant Data Reports**: LLMs do the heavy lifting to provide quick, actionable insights.\n- 🤖 **Automated Playbooks**: Real-time insights and recommendation playbooks for Data Analysis and DevOps, generated on-the-fly.\n- 🔍 **Less Manual Work**: Automated workflows mean you focus on strategy, not maintenance.\n  \n### Research Focus\n1. **Automating Data Pipelines**: Ensuring seamless data ingestion and management.\n2. **Streamlining BI Reporting**: Transforming natural language requests into SQL queries.\n3. **Incident Response Automation**: Generating playbooks from real-time observability data.\n4. **Tackling LLM Limitations**: Mitigating hallucinations and ensuring data trustworthiness.\n5. **Enterprise Integration**: Scaling across multi-cloud and complex data environments.\n\n### 🧠 Built On Top of Prior Work\nWe leverage advanced data query languages and visualization frameworks to boost natural language-to-SQL translations, improving upon their limitations to manage **complex queries** and **dynamic schemas**.\n\n### Run it yourself\n1. In the frontend directory, run npm start to launch the electron app interface.\n2. In the main directory, run python app.py to launch the flask server.\n3. Query the database through natural language in the frontend interface.\n4. Swap out dataset for desired use case and fit database schema context accordingly.\n"
    },
    {
      "name": "DhruvvArora/pulseai",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/98960322?s=40&v=4",
      "owner": "DhruvvArora",
      "repo_name": "pulseai",
      "description": null,
      "homepage": null,
      "language": "HTML",
      "created_at": "2024-12-15T03:54:20Z",
      "updated_at": "2024-12-17T00:04:34Z",
      "topics": [],
      "readme": "# Pulse AI\n## Personal Health Assistant Chatbot\n\nThis is a Flutter-based Personal Health Assistant application that allows users to chat with an AI healthcare assistant. Users can describe their symptoms, ask health-related questions, and receive AI-generated responses.  The backend is built using Flask, AutoGen, and OpenAI to provide intelligent responses and healthcare suggestions.\n\n## Features\n\n- Chat with an AI-based health assistant for personalized health recommendations.\n- Fetches medicine information, including prices and images.\n- Displays AI-generated disclaimers advising users to consult a doctor for clarification.\n\n## Project Structure\n\n- **chatbot_screen.dart**: Manages the chat interface and communication with the backend.\n- **app.py**: FastAPI backend that integrates with AutoGen and OpenAI for generating responses and analyzing images.\n- **main.dart**: Main entry point of the Flutter app.\n- **MedicalProductsScreen**: Shows a list of medical products for user convenience.\n\n## Prerequisites\nBefore you can run the app, make sure the following dependencies are installed:\n\n### General Requirements\n\n1. **Flutter**: Download and install Flutter SDK from [Flutter's official website](https://flutter.dev/docs/get-started/install).\n2. **Dart**: Dart is included with Flutter, so you don’t need to install it separately.\n3. **Python 3.7+**: Required for running the FastAPI backend.\n4. **OpenAI API Key**: Register and get your API key from [OpenAI's website](https://platform.openai.com/signup/).\n5. **AutoGen Library**: Install via pip to enable chat with AI.\n6. **Pg-vector**: Download for semantic retrieval features\n\n### Flutter Packages\nIn your Flutter project directory, to add the dependencies, run:\n`flutter pub get`\n\n### Python Libraries\nIn your backend folder, create a virtual environment and install the necessary Python packages:\n\n# Create a virtual environment (optional)\n```python -m venv venv```\n```venv\\Scripts\\activate```\n\n# Install Flask and other necessary libraries\n`pip install flask openai autogen chromadb markdownify pypdf multi-part`\n\n### Setup OpenAI and AutoGen Configuration: \nCreate a configuration file named OAI_CONFIG_LIST.json in backend folder and add your OpenAI API credentials to it:\n\n```\n[\n  {\n    \"model\": \"gpt-4o-mini\",\n    \"api_key\": \"your_openai_api_key_here\"\n  }\n]\n```\n\n\n# Setting Up the Backend\n\n### In your project directory, navigate to the backend folder.\n- To navigate to backend directory, run:\n`cd backend`\n\n- Run the Flask server with the following command:\n`python app.py`\n\n\n## Running the Flutter App\n\n### Open a new terminal & navigate to your lib folder.\n- Run the Flutter app with the following command:\n`flutter run`\n\n\n## Future Enhancements \n- Extend backend functionality for more comprehensive healthcare responses.\n- Add database integration to store user queries and responses.\n- Implement more AI models and image analysis techniques.\n- Services like fitness plans and appointment scheduling.\n- Wound assessment to offer recommendations or suggest telemedicine consultations. \n"
    },
    {
      "name": "denniszielke/autogen-playground",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/11569044?s=40&v=4",
      "owner": "denniszielke",
      "repo_name": "autogen-playground",
      "description": "Playground projects for evaluating autogent for different scenarios",
      "homepage": "",
      "language": "Python",
      "created_at": "2024-12-11T08:55:24Z",
      "updated_at": "2025-02-05T09:28:59Z",
      "topics": [
        "autogen",
        "python"
      ],
      "readme": ""
    },
    {
      "name": "sanggi-wjg/ai-demos",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/24692394?s=40&v=4",
      "owner": "sanggi-wjg",
      "repo_name": "ai-demos",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2024-09-06T01:09:36Z",
      "updated_at": "2025-03-03T03:44:40Z",
      "topics": [],
      "readme": "# AI Demos\n\n## 1. Langchain\n\nread [Langchain](docs/01_Langchain.md)"
    },
    {
      "name": "tikendraw/jobber",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/68785366?s=40&v=4",
      "owner": "tikendraw",
      "repo_name": "jobber",
      "description": "applies for jobs.",
      "homepage": null,
      "language": "Python",
      "created_at": "2024-12-11T16:01:39Z",
      "updated_at": "2025-01-28T17:25:46Z",
      "topics": [],
      "readme": "# Scraping job from linkedin"
    },
    {
      "name": "eugeneVzaidun/autogen",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/72197489?s=40&v=4",
      "owner": "eugeneVzaidun",
      "repo_name": "autogen",
      "description": null,
      "homepage": null,
      "language": "JavaScript",
      "created_at": "2024-10-13T09:53:36Z",
      "updated_at": "2025-04-22T20:43:39Z",
      "topics": [],
      "readme": "# Agentic Workflow Experiments"
    },
    {
      "name": "earzamastsev/llm-intro-course",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/5910423?s=40&v=4",
      "owner": "earzamastsev",
      "repo_name": "llm-intro-course",
      "description": "Practics for AI intro course",
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2024-12-03T05:28:32Z",
      "updated_at": "2025-03-07T22:09:17Z",
      "topics": [],
      "readme": "# llm-intro-course\n[Yandex.Education] Practics for AI intro course\n"
    },
    {
      "name": "INTEND-Project/inSwitch",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/169916319?s=40&v=4",
      "owner": "INTEND-Project",
      "repo_name": "inSwitch",
      "description": "LLM agents for context switching from business-level intents to resource-level intents for cognitive computing continuum",
      "homepage": "",
      "language": "Jupyter Notebook",
      "created_at": "2024-11-21T13:00:37Z",
      "updated_at": "2025-02-06T22:07:34Z",
      "topics": [],
      "readme": "# inSwitch\nLLM agents for context switching from business-level intents to resource-level intents in congnitive computing continuum\n\n# Install\nWe recommend using [VS Code + devcontainer](https://code.visualstudio.com/docs/devcontainers/containers). You need to install the devcontainers extension to VS Code. After that, when you open this workspace in Code, there will be a pop-up window asking if you want to re-open it in devcontainer. If you missed it, use cmd+shift+p, and select \n\"Dev Containers: Reopen in Container\". Make sure you have a docker environment running. \n\nOnce the workspace is reopened inside a container (to make sure it is inside a container, launch a terminal and check the user name and node name), you may need to manually install two extensions to the \"new VS code\", i.e., Python (ms-python.python) and Jupyter (ms-toolsai.jupyter).\n\nYou need a valid token for OpenAI API, stored in ./openai.credential. Other LLM models will be supported later. \n\n# Structure\n- [./inswitch](inswitch/): generic components for inSwitch\n- [./usecases](usecases/): data, scripts, components, etc., for different use cases\n\n# Run\nMost of the running examples and demos are in Jupyter Notebooks. To run them, you need to select a kernal - choose a python environment called \"base (Python 3.12.7)\". The version may vary. It is the under path \"/opt/conda/bin/python\".\n\nIf you face problem running python scripts, you may also try to select this same \"base\" python as the Python Interpreter (this can be done also via cmd+shift+p).\n\n# Use cases\nAll the use cases can be found in [usecase](./usecases/). Four each use case, there is a README.md file for further information.\n\n# First taste\nSo far, we have one simplified example running. You can check the snapshot [here](./usecases/fill/doc/snapshots/Step1-simplied-doc-api-call-2024-11-15.ipynb)\n\n# Development plan\n"
    },
    {
      "name": "zefang-liu/AutoBnB",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/30639033?s=40&v=4",
      "owner": "zefang-liu",
      "repo_name": "AutoBnB",
      "description": "AutoBnB: AutoGen + Backdoors & Breaches",
      "homepage": "",
      "language": "Jupyter Notebook",
      "created_at": "2024-11-28T22:21:45Z",
      "updated_at": "2025-03-27T23:12:04Z",
      "topics": [],
      "readme": "# AutoBnB: Backdoors & Breaches with AutoGen\n\nThis project investigates the use of large language models (LLMs) for enhancing collaboration and decision-making in cybersecurity incident response scenarios. Built on the *[AutoGen](https://microsoft.github.io/autogen/)* framework for multi-agent system simulation, it uses the *[Backdoors & Breaches](https://www.blackhillsinfosec.com/projects/backdoorsandbreaches/)* tabletop game as a simulation platform to explore how LLM-based multi-agent systems perform under various team structures. Through these simulations, the project provides insights into optimizing team dynamics and strategies to counter cyber threats effectively.\n\n## Installation\n\nSet up the environment by installing the required dependencies:\n```bash\npip install -r requirements.txt\n```\n\n## Usage\n\nLaunch the simulation using the Jupyter notebook:\n```bash\njupyter notebook notebooks/autobnb_core.ipynb\n```\nor\n```bash\njupyter notebook notebooks/autobnb_expansion.ipynb\n```\n\n## Paper\n\nIf you found this repository useful, please cite our [paper](https://arxiv.org/abs/2412.00652) as follows:\n```bibtex\n@article{liu2024multi,\n  title={Multi-Agent Collaboration in Incident Response with Large Language Models},\n  author={Liu, Zefang},\n  journal={arXiv preprint arXiv:2412.00652},\n  year={2024}\n}\n```\n\n## License\n\nThis repository is available under the [MIT license](LICENSE).\n\n## Acknowledgement\n\nThis project utilizes *[Backdoors & Breaches](https://www.blackhillsinfosec.com/projects/backdoorsandbreaches/)* cards, which are the intellectual property of [Black Hills Information Security](https://www.blackhillsinfosec.com/) and [Active Countermeasures](https://www.activecountermeasures.com/). We acknowledge their invaluable contribution to cybersecurity research. These cards are used solely for research purposes in this study.\n"
    },
    {
      "name": "erinjerri/foodfinder_rag_app",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/3184381?s=40&v=4",
      "owner": "erinjerri",
      "repo_name": "foodfinder_rag_app",
      "description": "second pass of Food Finder Rag app for UCB LLM Agent Course without key",
      "homepage": null,
      "language": "Python",
      "created_at": "2024-11-26T06:46:39Z",
      "updated_at": "2024-12-20T07:23:40Z",
      "topics": [],
      "readme": "# FoodFinder\nThis app was a hackathon project titled \"FoodFinder,\" a RAG app created for the hackathon a part of Fall 2020 UC Berkeley, Large Language Model (LLM) Agents Course.\n\n## About FoodFinder:\nFoodFinder is an innovative LLM-based agent designed to revolutionize the food delivery experience. By leveraging advanced web crawling and natural language processing techniques, FoodFinder will seamlessly search through multiple food delivery platforms (e.g., DoorDash, Grubhub, Uber Eats, Yelp) to provide users with personalized meal recommendations based on their preferences, dietary restrictions, and budget constraints.\n\n### Key Features:\n- Multi-platform integration: Crawl and aggregate data from various food delivery services.\n- Intelligent recommendation system: Utilize user preferences and historical data to suggest optimal meal choices.\n- Price comparison and optimization: Analyze prices across platforms to find the best deals.\n- Portion size analysis: Estimate and compare serving sizes to maximize value for money.\n- Automated ordering: Place orders on the user's behalf on the selected platform.\n\nTechnical Challenges:\n- Developing a robust web crawling system that can adapt to different platform structures.\n-Creating an efficient algorithm for real-time price and portion size comparisons.\n-Implementing a sophisticated recommendation system that balances user preferences with best value options.\n-Designing a user-friendly interface for interaction with the AI agent.\n\n### Potential Impact:\n-FoodFinder aims to save users time and money while improving their overall food delivery experience. This project addresses the soft-design problem of creating a high-fidelity human simulation for personal assistance in the food delivery domain.\n-This proposal fits well within the Applications Track as it focuses on developing an innovative LLM-based agent for a specific domain (food delivery) and addresses both hard-design problems (novel domain-specific tools for web crawling and price comparison) and soft-design problems (improved AI agent interfaces for personal assistance).\n\n# Installfest\n\n// Below are instructions on how to set-up this project. \n\nAdditionally, due to time constraints, we did not heavily utilize the Lambda credits as planned, and built a functional app as we needed to migrate from Autogen, which was a pain to use due to dependency conflicts, and was much easier to use with Autogen. Future developments of this app would be a native mobile app run multi-agents that will query even more food delivery apps including Amazon Fresh, Instacart and more (other APIs to integrate in the future) to have users be able to query multiple food delivery app options with an ease of convenience through a single app. This is instead of having a webUI or swapping between different native mobile applications on iOS.\n\n# Food Finder App\n\nA Flask application that helps users find restaurants and arrange food delivery using Yelp and DoorDash APIs, powered by LangChain and OpenAI.\n\n## Prerequisites\n\n- Python 3.11 or higher\n- Conda (Miniconda or Anaconda)\n- API Keys:\n  - OpenAI API Key\n  - Yelp API Key\n  - DoorDash Developer ID, Key ID, and Signing Secret\n\n## Installation\n\n1. **Clone the repository**\n   \n\ngit clone https://github.com/yourusername/foodfinder_rag_app.git\n\n\n2. **Create and activate Conda environment**\n\nconda create -n foodfinder python=3.11\n\n\n3. **Install dependencies**\n\n\npip install \"langchain>=0.1.0\" \\\nlangchain-openai>=0.0.2\" \\\n\"langchain-core>=0.1.10\" \\\n\"flask>=3.0.0\" \\\n\"openai>=1.10.0\" \\\n\"python-dotenv>=1.0.0\" \\\n\"pydantic>=1.10.0,<2.0.0\" \\\n\"requests>=2.31.0\" \\\n\"PyJWT>=2.8.0\"\n\n\n## Configuration\n\n1. **Create a `.env` file in the project root**\n\nbash \n\ntouch .env\n\n\n2. **Add your API keys to the `.env` file**\n\nOPENAI_API_KEY=your_openai_api_key\nYELP_API_KEY=your_yelp_api_key\nDOORDASH_DEVELOPER_ID=your_doordash_developer_id\nDOORDASH_KEY_ID=your_doordash_key_id\nDOORDASH_SIGNING_SECRET=your_doordash_signing_secret\n\n\n## Running the Application\n\n1. **Activate the Conda environment (if not already activated)**\n\nbash \nconda activate foodfinder\n\n\n2. **Set Flask environment variables**\n   \nbash\nexport FLASK_APP=app.py\nexport FLASK_ENV=development # For development mode\n\n\n3. **Run the Flask application**\n\nbash\n\npython -m flask run\n\n\nThe application will be available at `http://127.0.0.1:5000/`\n\n## Project Structure\n\nfoodfinder_rag_app/\n├── .env # Environment variables (not in git)\n├── .gitignore # Git ignore file\n├── app.py # Main Flask application\n├── config.py # Configuration loader\n├── doordash_api.py # DoorDash API integration\n├── yelp_api.py # Yelp API integration\n├── requirements.txt # Project dependencies\n└── templates/ # HTML templates\n└── index.html # Main page template\n\n\n## API Keys Setup\n\n### OpenAI API Key\n1. Visit [OpenAI](https://platform.openai.com/)\n2. Create an account or sign in\n3. Go to API keys section\n4. Create a new API key\n\n### Yelp API Key\n1. Visit [Yelp Fusion](https://fusion.yelp.com/)\n2. Create an account or sign in\n3. Create a new app\n4. Copy your API key\n\n### DoorDash API Credentials\n1. Visit [DoorDash Developer Portal](https://developer.doordash.com/)\n2. Create an account or sign in\n3. Create a new application\n4. Copy your Developer ID, Key ID, and Signing Secret\n\n## Troubleshooting\n\nIf you encounter any issues:\n\n1. **Environment Issues**\nbash \n\n\n# Remove existing environment\nconda deactivate\nconda env remove -n foodfinder\n\n# Reinstall dependencies\npip install -r requirements.txt\n\n\n2. **Flask Issues**\n\nbash\n\nMake sure you're in the right directory\ncd foodfinder_rag_app\n\nSet Flask Variables\n\nexport FLASK_APP=app.py\n\nexport FLASK_ENV=development # For development mode\n\n\n# Run with debug mode\n\npython -m flask run --debug"
    },
    {
      "name": "imankamyabi/autogen-fargate-executor",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/2341221?s=40&v=4",
      "owner": "imankamyabi",
      "repo_name": "autogen-fargate-executor",
      "description": "A code execution backend for AutoGen that runs code in AWS Fargate containers.",
      "homepage": null,
      "language": "Python",
      "created_at": "2024-11-13T22:21:30Z",
      "updated_at": "2024-12-22T18:57:15Z",
      "topics": [],
      "readme": "# AutoGen Fargate Executor\n\nA code execution backend for AutoGen that runs code in AWS Fargate containers. This executor allows you to run untrusted code in isolated containers, making it suitable for production environments where code execution needs to be sandboxed.\n\n## Author\n**Iman Kamyabi**  \nEmail: engkamyabi@gmail.com\n\n## Features\n\n- Executes code in isolated AWS Fargate containers\n- Automatic creation of required AWS resources (ECS cluster, IAM roles, CloudWatch log groups)\n- Support for installing pip dependencies\n- Support for custom environment variables\n- Support for requirements.txt file\n- Integration with AutoGen's code execution interface\n\n## Installation\n\n```bash\npip install autogen-fargate-executor\n```\n\nOr using Poetry:\n\n```bash\npoetry add autogen-fargate-executor\n```\n\n## Prerequisites\n\n1. AWS credentials configured either through environment variables, AWS CLI, or IAM roles\n2. VPC with subnets and security groups configured for Fargate tasks\n3. Required AWS permissions to create:\n   - IAM roles and policies\n   - ECS clusters and tasks\n   - CloudWatch log groups\n\n## Usage\n\n### Basic Usage with AutoGen\n\n```python\nfrom autogen_fargate_executor import FargateCodeExecutor\nfrom autogen import ConversableAgent\n\n# Initialize executor\nexecutor = FargateCodeExecutor(\n    image_uri='python:3.11',\n    subnet_ids=['subnet-xxx'],\n    security_groups=['sg-xxx'],\n)\n\n# Create AutoGen agent with Fargate executor\nagent = ConversableAgent(\n    name=\"CodeExecutor\",\n    llm_config=False,\n    code_execution_config={\"executor\": executor},\n    is_termination_msg=lambda msg: \"TERMINATE\" in msg.get(\"content\", \"\").strip().upper(),\n)\n```\n\n### Advanced Configuration\n\n```python\nexecutor = FargateCodeExecutor(\n    region_name='us-west-2',             # AWS region\n    image_uri='python:3.11',             # Docker image to use\n    subnet_ids=['subnet-xxx'],           # List of subnet IDs\n    security_groups=['sg-xxx'],          # List of security group IDs\n    cluster_name='my-cluster',           # Optional custom cluster name\n    cpu='512',                          # CPU units for Fargate task\n    memory='1024',                      # Memory (MB) for Fargate task\n    timeout_seconds=600,                # Maximum execution time\n    requirements_file='requirements.txt', # Optional requirements file\n    pip_dependencies=['pandas', 'requests'], # Optional pip packages\n    environment_variables={              # Optional environment variables\n        'API_KEY': 'xxx',\n        'DEBUG': 'true'\n    }\n)\n```\n\n## AWS Resources\n\nThe executor will automatically create and manage these AWS resources:\n\n1. ECS Cluster (default name: autogen-executor-cluster)\n2. IAM Role (name: ecsTaskExecutionRoleAutoGenFargate)\n3. CloudWatch Log Group (name: /ecs/autogen-task)\n\n## Security Considerations\n\n1. Each code execution runs in its own isolated container\n2. Containers are terminated after code execution\n3. Resources are isolated within your VPC\n4. Task execution role follows principle of least privilege\n5. All container logs are captured in CloudWatch\n\n## Cost Considerations\n\nThis executor uses AWS Fargate, which bills based on:\n1. vCPU and memory used per second\n2. CloudWatch logs storage and ingestion\n3. Data transfer costs\n\nConsider configuring appropriate CPU and memory settings based on your workload.\n\n## Development\n\n### Running Tests\n\n```bash\npoetry install\npoetry run pytest tests/\n```\n\n## License\n\nMIT\n\n## Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request."
    },
    {
      "name": "nowucca/aitools-autogen",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/1035271?s=40&v=4",
      "owner": "nowucca",
      "repo_name": "aitools-autogen",
      "description": "APplications and code modules for using Autogan in the aitools course",
      "homepage": null,
      "language": "Python",
      "created_at": "2023-10-24T05:11:23Z",
      "updated_at": "2024-11-12T00:52:26Z",
      "topics": [],
      "readme": "# Installation\n\n    `$ python -mvenv venv`\n\n    `$ source venv/bin/activate`\n    \n    `(venv) $ pip install -r requirements.txt`\n\n# Usage\n\nTo run the standalone python demonstration:\n    * `(venv) $ export PYTHONPATH=\"${PYTHONPATH}:${PWD}\"`\n    * `(venv) $ python aitools_autogen/python_app_with_agents.py`\n\nTo run the streamlit demonstration using blueprints:\n    * `(venv) $ export PYTHONPATH=\"${PYTHONPATH}:${PWD}\"`\n    * `(venv) $ streamlit run aitools_autogen/streamlit_app_with_agents.py` \n\n# Running Tests\n\n    * `(venv) $ export PYTHONPATH=\"${PYTHONPATH}:${PWD}\" && pytest\n\n\n# Overview\n\n`aitools_autogen` is a repository of code for generating code from blueprints.\n\nBlueprints are objects holding multiple agents and their relationships,\ntogether with tools to assist in their cooperation.\n\nAgents are objects that can be run, and can be connected to other agents.\n\nBlueprint can be run in a variety of ways, including as a standalone python program,\nand as a streamlit app.\n\n# Code Walkabout\n\nBlueprints are a simple aggregation of agents and their relationships.\nThe `init` method of a blueprint or the `initiate_work` \nmethod is where the agents are created and connected.\nSee the `aitools_autogen/blueprint*.py` files for more details.\n\nAgents are Autogen agents.  Either the Autogen agent has an llm associated,\nor it does not.  If it does, we can initialize the agent with a `system_message`.\nSee the `aitools_autogen/agents.py` file for more details.\n\nAutogen configuration variables are in the `aitools_autogen/config.py` file,\nand are used to configure the Autogen agents with defaults.  Some settings for \ntools are also located in the `aitools_autogen/__init__.py` file.\n\nThe utility functions in `aitools_autogen/utils.py` are used to help with\nthe creation of files on disk when code is generated from an LLM.\nEffectively these are the code generation utilities akin to the code execution\nutilities in `autogen` itself.\n\nThe `aitools_autogen/python_app_with_agents.py` file is a standalone python program\nthat runs a blueprint, generating example test code in the `coding/` folder.\n\nThe `aitools_autogen/streamlit_app_with_agents.py` file is a streamlit app that\nruns a blueprint, generating example test code in the `coding/` folder.\nThe streamlit app offers facilities which can use different OpenAPI urls,\nclear the `.cache/` folder from `autogen`, and use different seed values.\n\n"
    },
    {
      "name": "scenaristeur/smag-studio",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/4020744?s=40&v=4",
      "owner": "scenaristeur",
      "repo_name": "smag-studio",
      "description": null,
      "homepage": null,
      "language": "JavaScript",
      "created_at": "2024-11-03T14:46:19Z",
      "updated_at": "2024-11-14T15:55:15Z",
      "topics": [],
      "readme": "# smag-studio\n\n## solid persistance\n\n- https://communitysolidserver.github.io/CommunitySolidServer/latest/usage/starting-server/\n- https://communitysolidserver.github.io/configuration-generator/v7/\n- config https://github.com/CommunitySolidServer/CommunitySolidServer/tree/main/config\n\n-> npm install -g @solid/community-server\n-> community-solid-server -c @css:config/default.json -f data/\n\n```\n# default in memory\nnpx @solid/community-server\n\n# or local install with persistance\nnpm install -g @solid/community-server\n\ncommunity-solid-server -c @css:config/file.json -f data/\n\n# then signup for an account at  http://localhost:3000/.account/login/password/register/\n# create a pod\n# create a token\n# copy .env_example to .env\n# put your token identifier and token secret in .env\n\n\n\n\nwith pods\nnpx @solid/community-server -c @css:config/file.json -f data/\n- https://communitysolidserver.github.io/CommunitySolidServer/latest/usage/client-credentials/\ncreate token\n![alt text](image.png)\n```\n\nyou can use Penny to edit data on your http://localhost:3000/ pod https://penny.vincenttunru.com/\n\n# GUI for building llm agents\n\n- [ x ] Microsoft Autogen https://microsoft.github.io/autogen/0.2/docs/Examples/ (upload cathy_and_joe_autogen.json)\n- [ ] Crewai\n- [ ] langgraph\n- [ ] MemGPT/ Letta Agents\n- [ ] Human Agents\n- [ ] Tools Agents (agents with tools)\n- [ ] LLM Agents\n- [ ] https://github.com/Maximilian-Winter/ToolAgents & https://llama-cpp-agent.readthedocs.io/en/latest/get-started/#llamacpp-server-provider\n- Other LLM agents...\n\n## Agents Teams/Crew/Graph\n\n- config\n- turn (round robin, hierarchy...)\n  ...\n\n## Autogen Agents props\n\n- name\n- (description)\n- system_message / system prompt\n- llm_config\n- human_input_mode\n\n* initiate_chat\n\n## Crewai Agents props\n\n### agent\n\n- name\n- role\n- goal\n- backstory\n\n### tools\n\n- name\n- description\n- code\n\n* crew.kickoff\n\n### task\n\n- description\n- expected output\n- agent\n- output_file\n\n## langgraph Agents props\n\n### global\n\n- state\n- flux / workflow\n- tools\n- agents (nodes) / edges\n- should_continue (conditional_edge)\n- call_model\n\n* compilation\n\n# vue\n\nThis template should help get you started developing with Vue 3 in Vite.\n\n## Recommended IDE Setup\n\n[VSCode](https://code.visualstudio.com/) + [Volar](https://marketplace.visualstudio.com/items?itemName=Vue.volar) (and disable Vetur).\n\n## Customize configuration\n\nSee [Vite Configuration Reference](https://vite.dev/config/).\n\n## Project Setup\n\n```sh\nnpm install\n```\n\n### Compile and Hot-Reload for Development\n\n```sh\nnpm run dev\n```\n\n### Compile and Minify for Production\n\n```sh\nnpm run build\n```\n\n### Lint with [ESLint](https://eslint.org/)\n\n```sh\nnpm run lint\n```\n\n# cluster\n\n- https://github.com/vasturiano/d3-force-cluster-3d\n- https://github.com/vasturiano/d3-force-3d\n- https://github.com/vasturiano/3d-force-graph/issues/298\n\n# solid browser auth\n\n- https://communitysolidserver.github.io/CommunitySolidServer/latest/usage/client-credentials/\n- https://stackoverflow.com/questions/43842793/basic-authentication-with-fetch\n"
    },
    {
      "name": "didw/RAG_daou",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/7973071?s=40&v=4",
      "owner": "didw",
      "repo_name": "RAG_daou",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2024-10-31T23:15:12Z",
      "updated_at": "2024-12-14T18:03:07Z",
      "topics": [],
      "readme": "# RAG\n\n### 1. Docker 이미지 빌드\n\n각 모듈 디렉토리에서 Docker 이미지를 빌드 및 배포합니다.\n\n```bash\n# Embedding Module\ncd embedding\ndocker build -t didw/embedding:latest .\ndocker push didw/embedding:latest\n\n# Retriever Module\ncd ../retriever\ndocker build -t didw/retriever:latest .\ndocker push didw/retriever:latest\n\n# Re-ranker Module\ncd ../re_ranker\ndocker build -t didw/re_ranker:latest .\ndocker push didw/re_ranker:latest\n\n# Agent Module\ncd ../agent\ndocker build -t didw/agent:latest .\ndocker push didw/agent:latest\n\n# API Gateway\ncd ../api_gateway\ndocker build -t didw/api_gateway:latest .\ndocker push didw/api_gateway:latest\n\n# Vector DB Module\ncd ../vector_db\ndocker build -t didw/vector_db:latest .\ndocker push didw/vector_db:latest\n```\n\n### 2. k8s 클러스터 설정\n\n```bash\n# k8s 클러스터 설정\nkubectl apply -f kubernetes/deployment.yaml\nkubectl apply -f kubernetes/service.yaml\n```\n\n### 3. 데이터 삽입\n\n```bash\npython script/embed_documents.py\n```\n\n### 4. 테스트 쿼리\n\n```bash\npython script/test_query.py\n```\n\n### 5. 챗봇 URL\n\nhttps://chatbot-rag-jyyang.streamlit.app/\n"
    },
    {
      "name": "Bgon01/wolfLM",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/86642839?s=40&v=4",
      "owner": "Bgon01",
      "repo_name": "wolfLM",
      "description": null,
      "homepage": "",
      "language": "Python",
      "created_at": "2024-10-31T03:22:54Z",
      "updated_at": "2025-03-19T12:20:36Z",
      "topics": [],
      "readme": "# 🐺 WolfLM (MLFlow Project) 🏆\r\n\r\nIn today’s world, vulnerable customers are relying on businesses more than ever to provide essential care and support. Whether they're facing financial hardship, domestic violence, or age-related challenges, telecommunications companies have a crucial responsibility to ensure these individuals are treated with compassion and dignity.\r\nBut it’s not just about ethics — it’s also about compliance. In fact, one telco was fined $50 million for failing to meet vulnerable customer policies, highlighting the serious consequences of non-compliance in this area. This is where WOLFLM steps in.\r\n**wolfLM** is a GenAI-powered application designed to help telco companies identify, protect, and support vulnerable customers while ensuring regulatory compliance. Through cutting-edge AI hosted entirely on Databricks, **wolfLM** empowers customer service agents to provide targeted assistance and helps businesses avoid costly breaches.\r\n\r\n---\r\n## 🎥 Video Overview\r\n\r\nClick below to watch the video:<br>\r\n<a href=\"https://www.youtube.com/watch?v=5GPlSpRGqqk\">\r\n    <img src=\"https://img.youtube.com/vi/5GPlSpRGqqk/0.jpg\" alt=\"Watch the Video\" width=\"960\" height=\"540\">\r\n</a>\r\n\r\n---\r\n\r\n## 📌 **Key Features and Functionality:**\r\n- **Vulnerability Detection and Classification**: **wolfLM** uses natural language processing to detect key indicators of vulnerability during customer interactions. Whether it's financial distress or domestic violence, the system classifies each case based on the customer’s unique situation.\r\n- **RAG-Powered Recommendations**: Leveraging a Retrieval-Augmented Generation (RAG) system, **wolfLM** accesses relevant policy documents to suggest the best course of action based on the specific type of vulnerability. It helps agents navigate complex cases with confidence by providing clear, actionable recommendations.\r\n- **WOLFGuard – AI Advisory Agents**: Embedded within **wolfLM** is WOLFGuard, a powerful suite of AI-driven advisory agents. WOLFGuard helps support teams navigate complex issues by synthesizing inputs from AI models focused on complaints, compliance, churn, and customer vulnerability. These advisory agents work together with a meta-advisor that integrates all insights to assist agents in crafting responses that are both protective of vulnerable customers and fully compliant with regulations.\r\n- **Data-Driven Insights and Reporting**: **wolfLM** also provides comprehensive reporting through a dynamic dashboard that logs all interactions involving vulnerable customers. Supervisors can review these interactions post-call to ensure compliance was maintained and to identify emerging trends. These insights not only help maintain adherence to policies but also inform coaching for support teams, allowing them to improve their ability to recognize and respond to vulnerable customers effectively.\r\n\r\n---\r\n\r\n## 🔍 **Why is this so important?**\r\nNow more than ever, customers trust and rely on businesses to provide support when it matters most - especially when this support is critical to keeping vulnerable Australian's connected. When customers are vulnerable, it is essential they are treated with **care**, **compassion** and **dilligence** from corporations who provide critical goods and services.\r\n\r\nVulnerability can take many forms, sometimes as a result of **personal**, **cultural** or **social** characteristics, including age, poor digital literacy and living in remote areas. In other cases vulnerability can occur as a **result of particular life events**, including domestic voilence, financial stress and illness. \r\n\r\nMost Australian's experience vulnerability at one time or another with varying severity - and it is important that service providers can \r\n1. **Accurately recognise** when a customer is vulnerable\r\n2. **Provide appropriate care** and support specific to a customer's situation\r\n3. **Remain compliant** when interacting with such customers.\r\n\r\nIn businesses with large customer bases, achieving these three outcomes can be challenging, especially when communication often happens through call centres - where agents interact with high volumes of customers each day.\r\n\r\nWith our solution, we begin to take the guess work out of interacting with these customers. Through intelligent classification of vulnerability and generated actions based on best practice and complaince documents, we **arm agents with everything they need** to ensure these customers don't fall through the cracks. This empowers our agents to focus on taking care of these customers and keeping them connected when it matters most.\r\n\r\n---\r\n# 🏛️ Architecture Diagram\r\n![Architecture Diagram](./images/llm_worldcup_architecture.gif)\r\nThis architecture diagram provides a high-level overview of the system’s infrastructure and data flow within **wolfLM**. It showcases the integration of machine learning pipelines, data processing, and deployment steps designed to enhance customer service workflows.\r\n\r\n\r\n---\r\n# 🧠 Compound AI System - Agentic Workflow\r\n![Agentic Workflow](./images/llm_worldcup_agent.gif)\r\nThe **Agentic Workflow** represents the system's real-time, AI-driven interactions, focusing on the automation and orchestration of customer support workflows. This workflow utilizes a combination of models and functions to identify and support vulnerable customers in compliance with regulatory requirements.\r\n\r\n---\r\n# ⚙️ Components\r\n\r\n1. **Multilabel Classifier (Training & Serving)**\r\n   - Uses the **Medallion Architecture** with **MLflow** to manage training and deployment of the classifier. This model labels conversations based on multiple criteria for better customer support.\r\n\r\n2. **Autogen Orchestration** \r\n   - Implements **MLflow Autologging** combined with `wolfLM-gpt-4o-mini` to streamline the orchestration of AI models, making model management and monitoring efficient.\r\n\r\n3. **Nested Group Chat Pattern**\r\n   - A structured chat interaction model that enables nested, context-aware conversations to improve customer experience and better address their needs.\r\n\r\n4. **Classifier Autogen Function Calling**\r\n   - Enables MLflow serving with the `wolfLM-vulnerability-classifier` to automate model inference and decision-making during customer interactions.\r\n\r\n5. **RAG (Retrieval-Augmented Generation)**\r\n   - A data processing pipeline that transforms PDFs to structured data using **unstructured.io**, feeding into **Databricks Serving** (`databricks-bge-large-en`) and **Vector Search Index** (`wolflm-vector-search`) to enhance response accuracy and relevancy.\r\n\r\n6. **WolfGuard**\r\n   - Uses **Databricks Function Calling** and **Autogen Metadata Agent Advisors** to monitor interactions and provide AI-driven insights on customer vulnerability, ensuring proactive support.\r\n\r\n7. **Reporting**\r\n   - Generates insights through **Databricks SQL and Dashboards**, enabling stakeholders to monitor system performance and compliance in real-time.\r\n\r\n8. **User Interface (UI)**\r\n   - A **Databricks Custom App** that serves as the front-end interface for customer service agents, integrating all AI and reporting components for seamless workflow.\r\n\r\n---\r\n# 📂 Code Structure\r\n\r\n\r\n```\r\nwolfLM/\r\n├── .gitignore\r\n├── README.md\r\n├── LICENSE\r\n├── images/\r\n├── notebooks/\r\n└── application/\r\n```\r\n\r\n- **notebooks**: Contains all the training code, SQL table creation, function definitions, vector search setup, and client test code for vector search.\r\n- **application**: Holds the main code for the agent workflow along with the Gradio app.\r\n\r\n---\r\n# 📊 Data\r\n\r\n## 🛠️ Data Generation\r\nA sample of training data is uploaded to main.default.bronze_training_data\r\nWe used GPT model to generate the training data \r\n\r\n`A sample prompt` \r\n\r\n```plaintext\r\ncreate a dataset with 50 rows, each row has a conversation_id, customer_id,\r\nconversation_transcript (where this is a conversation between an customer\r\nservice agent and a customer - at least 3 sentences back and forth). The\r\ndataset should all be examples of conversations where a customer is talking\r\nto a telocommunications customer service agent and the customer is\r\nexperiencing financial hardship and domestic violence (has to be both)\r\n```\r\n\r\n\r\n\r\n## 📖 Data Dictionary\r\n\r\n| **Catalog**      | **Database** | **Layer** | **Table Name**                           | **Description**                                                                                                                                                 |\r\n|------------------|--------------|-----------|------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------|\r\n| `workspace_us`   | `default`    | Bronze    | `bronze_vulnerable_customers`            | Contains training data used for developing the vulnerability classifier.                                                                                        |\r\n| `workspace_us`   | `default`    | Bronze    | `bronze_pdf_raw`                         | Knowledge base of vulnerable customers loaded from volume, containing raw document data.                                                                        |\r\n| `workspace_us`   | `default`    | Silver    | `silver_knowledge_base`                  | Contains chunked documents from `bronze_pdf_raw`, along with embeddings for each chunk to represent contextual understanding.                                   |\r\n| `workspace_us`   | `default`    | Silver    | `silver_knowledge_base_self_managed_vs_index` | Vector search index using delta sync, derived from `silver_knowledge_base` for efficient search and retrieval.                       |\r\n| `workspace_us`   | `default`    | Gold      | `gold_customer_data`                     | Part of the Customer 360 tables, used for reporting and downstream analysis.                                                                                    |\r\n| `workspace_us`   | `default`    | Gold      | `gold_model_output`                      | Stores model outputs, including vulnerability assessments and recommendations generated by AI models.                                                           |\r\n\r\n![Unity_Catalog](./images/unity_catalog.png)\r\n\r\n---\r\n## 📂 Knowledge Base\r\n\r\nThe PDF knowledge base is uploaded to the directory: `/Volumes/main/default/knowledge_base`.\r\n![Knowledge Base](./images/knowledgebase_volume.png)\r\n\r\n---\r\n# 🚀 Model - Endpoint Setup\r\n\r\n### A. `wolfLM-gpt-4o-mini`\r\nFor cost-effective experimentation, we use a personal OpenAI API key. In an enterprise environment, this model can be substituted with the Llama3.1-instructed model. Follow these steps to set up a serving endpoint in Databricks:\r\n\r\n1. **Source:** `External model`\r\n2. **Provider:** `OpenAI`\r\n3. **Task:** `Chat`\r\n4. **External Model Name:** `gpt-4o-mini`\r\n5. **Additional Features in Databricks:**\r\n   - **AI Gateway:** Enable options for `Usage Tracking`, `Inference Table`, `AI Guardrails`, and `Rate Limits`. These are disabled for hackathons but highly recommended for production environments.\r\n\r\n### B. `wolfLM-vulnerability-classifier`\r\nThis binary classifier is registered in Unity Catalog, facilitating easy endpoint creation.\r\n\r\n1. **Entity:** `Unity Catalog model: vulnerability-classifier`\r\n2. **Compute Type:**\r\n   - **CPU Serving:** `0-4 DBU`\r\n   - **GPU Serving:** `0-10.48 DBU`\r\n3. **Compute Scale-Out:** `Small`\r\n4. **Scale to Zero:** `true` (reduces costs when idle)\r\n\r\n### C. `databricks-bge-large-en`\r\nThis embedding model is configured for Vector Search. It is enabled by default via Databricks Foundation Model Serving.\r\n\r\n![Model Servings](./images/model_serving.png)\r\n\r\n### D. `wolflm-vector-search`\r\nThis endpoint supports vector search functionality for high-efficiency data retrieval.\r\n![Vector Search DataBricks](./images/vector_search_endpoint.png)\r\n\r\n\r\n# 🤖 Model - Multi-Agent Orchestration (Autogen)\r\n\r\nA core feature of our product is the **compound AI system**, designed to aggregate customer-specific information with essential compliance data for Customer Service. This integration enables informed, holistic decision-making to address complex user issues effectively.\r\n\r\n### A. `Assistant Agents`\r\n\r\nEach component within the [Architecture Diagram Section](#️-architecture-diagram) represents a distinct part of the compound AI system.\r\n\r\n- **Section A: Classification Assistant Agent**\r\n  - Connects to the Mosaic AI Model Serving/MLflow-hosted endpoint of the classifier.\r\n  - Utilizes function calls to:\r\n    - Trace and log classification results.\r\n    - Output classification with a confidence level.\r\n\r\n- **Section B: RAG Agent**\r\n  - Interfaces with the Vector Store index through function calls.\r\n  - Manages retrieval tasks by:\r\n    - Querying the vector index for relevant information.\r\n    - Aggregating contextual data.\r\n\r\n- **Section C: Customer360 Agent**\r\n  - Retrieves detailed customer data.\r\n  - Executes function calls to:\r\n    - Access customer information across various systems.\r\n    - Consolidate a 360° view of customer profile details.\r\n\r\n- **Writer/Aggregation Agent**\r\n  - Uses all the information from the Classification, RAG and Customer360 Agent and writes a final message for the user to see.\r\n\r\n- **OpenAI API call**\r\n    - Using the information from the Classification, RAG and Customer360 Agent to create a summary with recommendations and specific customer information for the Customer Service Agent to follow up on. \r\n\r\nFor additional information on sections A, B, and C, please refer to the [Architecture Diagram Section](#️-architecture-diagram).\r\n\r\n\r\n### B. `GroupChat & SocietyOfMind Agent`\r\n\r\nGroupChat Autogen is triggered by a UserProxyAgent which is essentially a user's input in our example. However, this GroupChat leverages the power of the 3 Agents introduced previously that each speaks in turn and passes the information to the final writing agent to be aggregated for the final recommendation. \r\n\r\nFurther, an Autogen Group Chat integrates seamlessly with the SocietyOfMind Agent. Taken directly from the Autogen documentation, the advantages of using the SocietyOfMind Agent are: \r\n\r\n```\r\nThe SocietyOfMindAgent, which runs a group chat as an internal monologue, but \r\nappears to the external world as a single agent. This confers three distinct advantages:\r\n\r\n1. It provides a clean way of producing a hierarchy of agents, hiding complexity\r\nas inner monologues.\r\n2. It provides a consistent way of extracting an answer from a lengthy group chat \r\n(normally, it is not clear which message is the final response, and the response \r\nitself may not always be formatted in a way that makes sense when extracted as a \r\nstandalone message).\r\n3. It provides a way of recovering when agents exceed their context window constraints \r\n(the inner monologue is protected by try-catch blocks)\r\n```\r\n\r\nTo integrate the Multi Agent System as a solution that is easily callable using Gradio, the abstraction of the group chat as a single agent was particularly helpful but most importantly, the ability to retrieve the last message from the aggregated group was done easily and integrated with the User Interface. Thus this agent provided us as developers a production friendly interface for continuous conversation between the Writer, Classification, RAG, Customer360 and Writer Agents and thus creating a robust solution in production.\r\n\r\nWe experimented with Sequential Agentic Worflow, Sequential Nested Workflow and ultimately decided upon the GroupChat with SocietyOfMind Orchestration.\r\n\r\n---\r\n# 🖥️ User Interface - Databricks App\r\nOur solution leverages a new feature in Databricks—**Databricks Apps**, released in October 2024—allowing us to build and run a **Gradio application** directly within the Databricks environment.\r\n\r\n![Databricks App](./images/app_databricks.png)\r\n\r\nThe Gradio application is designed to support the Customer Support Team, offering two main components:\r\n\r\n1. **Triage Customer Assistant (TCA):** This chatbot component simulates initial customer interactions with support by allowing customers to provide their ID, request, and context. It facilitates a seamless start to the support process.\r\n\r\n2. **Specialist Care Support Interface (SCSI):** When a vulnerability is detected in the customer’s message, this interface provides the support team with a detailed report. The report includes:\r\n   - **Type of Vulnerability** identified in the message\r\n   - **Recommended Actions** based on relevant policy and industry guidelines\r\n   - **Customer 360 Insights** for a comprehensive view of customer history\r\n   - **Suggested Responses** to assist the team in formulating an appropriate reply\r\n\r\n![UI Design](./images/ui_design_wolf_lm.png)\r\n\r\n---\r\n\r\n# 📊 User Interface - Dashboard\r\n![Dashboard](./images/dashboard.png)\r\nWe developed a **Databricks Dashboard** that pulls data from the tables generated by the agentic workflow, combined with Customer 360 data. This dashboard provides essential tools for the Customer Support Team with the following key features:\r\n\r\n1. **Compliance Monitoring**: Supervisors can monitor human agent interactions with vulnerable customers through stored transcripts, ensuring adherence to compliance standards.\r\n   \r\n2. **Coaching Opportunities**: Supervisors can analyze team trends and identify areas for improvement, facilitating targeted training and coaching sessions.\r\n   \r\n3. **Customer Tracking and Follow-up**: Customer support teams can monitor vulnerable customers, ensuring follow-up calls or additional support as needed.\r\n\r\nThis dashboard aims to preserve the human touch in supporting vulnerable customers by equipping agents with the right context and insights to make compassionate, informed decisions.\r\n\r\n---\r\n\r\n## 🎓 Learning Highlights\r\n\r\n- **Cost-Effective AI Experimentation**: Leveraged `gpt-4o-mini` for low-cost testing, while recognizing that `Llama3.1` would be more suitable for enterprise-scale applications.\r\n\r\n- **Compound AI System Orchestration**: Enhanced model tracking and deployment efficiency using MLflow’s autologging and autogen orchestration.\r\n\r\n- **User-Friendly Interface**: Gradio integration within the Databricks app provided agents with clear vulnerability reports and actionable recommendations. Designing a seamless user experience and intuitive UI flow was a significant challenge and a valuable learning experience.\r\n\r\n- **Vector Search Implementation**: Achieved efficient embedding-based document retrieval with Databricks, though further tuning could improve search accuracy.\r\n\r\n- **Teamwork and Collaboration**: Successfully managed tasks, leveraged team strengths, and supported one another, enhancing our collaborative skills.\r\n\r\n- **Presentation and Pitching**: Gained experience in presenting and pitching by creating demo videos and crafting a compelling narrative around **wolfLM**, effectively communicating its business impact.\r\n\r\n- **Creativity in AI Solutions**: Focused on building AI solutions that not only bring business value but also prioritize customer welfare, making the customer central to our use case.\r\n\r\n- **Tool Selection**: Choosing between AutoGen, HaystackAI, CrewAI, and SWARM for the multi-agent component was challenging, as each offers unique capabilities. We ultimately selected AutoGen for its compatibility with MLflow. \r\n\r\n- **Decoupling Tools for Production**: We opted for a decoupled vector store instead of FAISS, which handles the embedding sync seamlessly. While everything runs smoothly in a notebook environment, production posed challenges, such as configuring individual access control in Unity Catalog and granting MLflow the necessary permissions.\r\n\r\n- **Efficiency and Latency Considerations**: We selected a DistilBERT model for classification due to its efficiency and low latency. In the next project iteration, we plan to extend DistilBERT for multi-label classification to identify multiple vulnerabilities, enriching the context and information provided. Additionally, to maximize AutoGen’s potential, we intend to experiment with parallel workflows instead of GroupChat.\r\n\r\n\r\n---\r\n\r\n## 💰 Budget\r\n\r\n- **Test Account Allocation**: $500 \r\n- **Extensive Testing**: We conducted thorough testing and switched workspaces to enable new functionalities, slightly exceeding the budget.\r\n- **Reactivation Note**: All endpoints will need to be re-created after account reactivation.\r\n\r\nDatabricks offers a **usage dashboard** for tracking resource consumption:\r\n![Usage Dashboard](./images/account_usage_dashboard_detail.png)\r\n\r\n"
    },
    {
      "name": "cenrak/ml-epiphany",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/762378?s=40&v=4",
      "owner": "cenrak",
      "repo_name": "ml-epiphany",
      "description": "Data-driven insights and discoveries. ",
      "homepage": "",
      "language": "Dockerfile",
      "created_at": "2023-11-15T12:53:03Z",
      "updated_at": "2024-12-15T15:05:48Z",
      "topics": [],
      "readme": "# ml-epiphany\nData-driven insights and discoveries. \n\n```\nfor package in `cat requirements.txt `; do ./python -m pip download $package  -d pip-packages-3.10/ --cache-dir pip-packages-cache-3.10/ && echo $package >> success || echo $package >> failed ; done\n```\n"
    },
    {
      "name": "tuanvo-ai/lab01-llm",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/185315429?s=40&v=4",
      "owner": "tuanvo-ai",
      "repo_name": "lab01-llm",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2024-10-17T03:31:57Z",
      "updated_at": "2024-12-11T18:32:59Z",
      "topics": [],
      "readme": "# lab01-llm\n\n<a target=\"_blank\" href=\"https://cookiecutter-data-science.drivendata.org/\">\n    <img src=\"https://img.shields.io/badge/CCDS-Project%20template-328F97?logo=cookiecutter\" />\n</a>\n\nllmagents lab01\n\n## Project Organization\n\n```\n├── LICENSE            <- Open-source license if one is chosen\n├── Makefile           <- Makefile with convenience commands like `make data` or `make train`\n├── README.md          <- The top-level README for developers using this project.\n├── data\n│   ├── external       <- Data from third party sources.\n│   ├── interim        <- Intermediate data that has been transformed.\n│   ├── processed      <- The final, canonical data sets for modeling.\n│   └── raw            <- The original, immutable data dump.\n│\n├── docs               <- A default mkdocs project; see www.mkdocs.org for details\n│\n├── models             <- Trained and serialized models, model predictions, or model summaries\n│\n├── notebooks          <- Jupyter notebooks. Naming convention is a number (for ordering),\n│                         the creator's initials, and a short `-` delimited description, e.g.\n│                         `1.0-jqp-initial-data-exploration`.\n│\n├── pyproject.toml     <- Project configuration file with package metadata for \n│                         restaurant_reviews and configuration for tools like black\n│\n├── references         <- Data dictionaries, manuals, and all other explanatory materials.\n│\n├── reports            <- Generated analysis as HTML, PDF, LaTeX, etc.\n│   └── figures        <- Generated graphics and figures to be used in reporting\n│\n├── requirements.txt   <- The requirements file for reproducing the analysis environment, e.g.\n│                         generated with `pip freeze > requirements.txt`\n│\n├── setup.cfg          <- Configuration file for flake8\n│\n└── restaurant_reviews   <- Source code for use in this project.\n    │\n    ├── __init__.py             <- Makes restaurant_reviews a Python module\n    │\n    ├── config.py               <- Store useful variables and configuration\n    │\n    ├── dataset.py              <- Scripts to download or generate data\n    │\n    ├── features.py             <- Code to create features for modeling\n    │\n    ├── modeling                \n    │   ├── __init__.py \n    │   ├── predict.py          <- Code to run model inference with trained models          \n    │   └── train.py            <- Code to train models\n    │\n    └── plots.py                <- Code to create visualizations\n```\n\n--------\n\n"
    },
    {
      "name": "LittleLittleCloud/Develop_Agent_With_AutoGen",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/16876986?s=40&v=4",
      "owner": "LittleLittleCloud",
      "repo_name": "Develop_Agent_With_AutoGen",
      "description": null,
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2024-10-16T16:27:13Z",
      "updated_at": "2024-11-10T17:50:12Z",
      "topics": [],
      "readme": "## Developing AI Agents using AutoGen\n### Before you start\n- [Optional] [Setup OpenAI environment](./EnvironmentSetup.md)\n- Install python >= 3.8\n- Install VSCode\n- Create and activate your virtual environment\n```bash\n# windows\npython -m venv venv\nvenv\\Scripts\\activate\n\n# linux or mac\npython3 -m venv venv\nsource venv/bin/activate\n```\n- Install the required packages\n```bash\npip install -r requirements.txt\n```\n\n### Notebooks\n- [x] [Hello World](HelloWorld.ipynb): Create your first AutoGen agent. We use this notebook to walk through AutoGen primitives (agent, agent runtime, message handlers) as well as helping student onboard to OpenAI account/setting up python environment.\n- [x] [Human in the loop](./HumanInTheLoop.ipynb) Add human user to the conversation. We extend the hello world example from single-agent to two agent conversation by introducing user agent.\n- [x] [Tool call](ToolCall.ipynb): We use this notebook to show how to introduce tool call to agent.\n- [x] [MultiModal](./MultiModal%20chat.ipynb): We use this notebook to show how to introduce multi-modal capability to agent.\n- [x] [Multi-agent Group Chat](./Multi-agent%20GroupChat.ipynb): A notebook that uses all building blocks above. We build a group chat with four agents: User, assistant, ImageAssistant and WeatherAssistant, orchestrated by a group chat admin.\n"
    },
    {
      "name": "sundai-club/automated_data_scientist",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/156603435?s=40&v=4",
      "owner": "sundai-club",
      "repo_name": "automated_data_scientist",
      "description": null,
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2024-10-13T15:37:58Z",
      "updated_at": "2024-11-14T16:41:59Z",
      "topics": [],
      "readme": "# Installation\n\n```\ncp .env.example .env\n# then edit .env file, add you\nvirtualenv .venv\nsource .venv/bin/activate\npip install -r requirements.txt\n```\n\n# Run\n\n```\npython analyze.py  data/prompt.txt data/data.vcf\n```\n"
    },
    {
      "name": "Adithyan777/code-catalyst",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/63772248?s=40&v=4",
      "owner": "Adithyan777",
      "repo_name": "code-catalyst",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2024-07-07T20:43:14Z",
      "updated_at": "2025-01-16T07:13:40Z",
      "topics": [],
      "readme": ""
    },
    {
      "name": "atiq-zangoh/Welcome-Agent",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/184043077?s=40&v=4",
      "owner": "atiq-zangoh",
      "repo_name": "Welcome-Agent",
      "description": "This is an AI agent for inviting the peoples through LinkedIn, Email etc.",
      "homepage": null,
      "language": "Python",
      "created_at": "2024-10-10T06:26:14Z",
      "updated_at": "2025-02-19T04:49:54Z",
      "topics": [],
      "readme": "# Welcome-Agent\nThis is an AI agent for inviting the peoples through LinkedIn, Email etc.\n"
    },
    {
      "name": "atiq-zangoh/Visitors-AI-Assistant",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/184043077?s=40&v=4",
      "owner": "atiq-zangoh",
      "repo_name": "Visitors-AI-Assistant",
      "description": "This is an AI agent for inviting the peoples through LinkedIn, Email etc.",
      "homepage": null,
      "language": null,
      "created_at": "2024-10-10T10:02:01Z",
      "updated_at": "2025-02-19T04:51:22Z",
      "topics": [],
      "readme": "# Visitors-AI-Assistant\nThis is an AI agent for inviting the peoples through LinkedIn, Email etc.\n"
    },
    {
      "name": "patooworld/autogen",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/114192133?s=40&v=4",
      "owner": "patooworld",
      "repo_name": "autogen",
      "description": "Enable Next-Gen Large Language Model Applications Unification in one place. designed and polished with Next-Gen AI elevator ''KnowALL\"",
      "homepage": "https://patooworld.github.io/autogen/",
      "language": "Jupyter Notebook",
      "created_at": "2024-01-28T04:01:28Z",
      "updated_at": "2024-10-10T07:09:25Z",
      "topics": [
        "javascript",
        "jupyter-notebook",
        "mdx",
        "python"
      ],
      "readme": "<a name=\"readme-top\"></a>\n\n\n<div align=\"center\">\n\n<img src=\"https://microsoft.github.io/autogen/img/ag.svg\" alt=\"AutoGen Logo\" width=\"100\">\n\n![Python Version](https://img.shields.io/badge/3.8%20%7C%203.9%20%7C%203.10%20%7C%203.11%20%7C%203.12-blue) [![PyPI - Version](https://img.shields.io/pypi/v/autogen-agentchat)](https://pypi.org/project/autogen-agentchat/)\n[![NuGet version](https://badge.fury.io/nu/AutoGen.Core.svg)](https://badge.fury.io/nu/AutoGen.Core)\n\n[![Twitter](https://img.shields.io/twitter/url/https/twitter.com/cloudposse.svg?style=social&label=Follow%20%40pyautogen)](https://twitter.com/pyautogen)\n\n</div>\n\n# AutoGen\n\nAutoGen is an open-source programming framework for building AI agents and facilitating cooperation among multiple agents to solve tasks. AutoGen aims to streamline the development and research of agentic AI, much like PyTorch does for Deep Learning. It offers features such as agents capable of interacting with each other, facilitates the use of various large language models (LLMs) and tool use support, autonomous and human-in-the-loop workflows, and multi-agent conversation patterns.\n\n> [!IMPORTANT]\n> In order to better align with a new multi-packaging structure we have coming very soon, AutoGen is now available on PyPi as [`autogen-agentchat`](https://pypi.org/project/autogen-agentchat/) as of version `0.2.36`. This is the official package for the AutoGen project.\n\n\n> [!NOTE]\n> *Note for contributors and users*</b>: [microsoft/autogen](https://aka.ms/autogen-gh) is the original repository of AutoGen project and it is under active development and maintenance under MIT license. We welcome contributions from developers and organizations worldwide. Our goal is to foster a collaborative and inclusive community where diverse perspectives and expertise can drive innovation and enhance the project's capabilities. We acknowledge the invaluable contributions from our existing contributors, as listed in [contributors.md](./CONTRIBUTORS.md). Whether you are an individual contributor or represent an organization, we invite you to join us in shaping the future of this project. For further information please also see [Microsoft open-source contributing guidelines](https://github.com/microsoft/autogen?tab=readme-ov-file#contributing).\n>\n> -_Maintainers (Sept 6th, 2024)_\n\n\n![AutoGen Overview](https://github.com/microsoft/autogen/blob/main/website/static/img/autogen_agentchat.png)\n\n- AutoGen enables building next-gen LLM applications based on [multi-agent conversations](https://microsoft.github.io/autogen/docs/Use-Cases/agent_chat) with minimal effort. It simplifies the orchestration, automation, and optimization of a complex LLM workflow. It maximizes the performance of LLM models and overcomes their weaknesses.\n- It supports [diverse conversation patterns](https://microsoft.github.io/autogen/docs/Use-Cases/agent_chat#supporting-diverse-conversation-patterns) for complex workflows. With customizable and conversable agents, developers can use AutoGen to build a wide range of conversation patterns concerning conversation autonomy,\n  the number of agents, and agent conversation topology.\n- It provides a collection of working systems with different complexities. These systems span a [wide range of applications](https://microsoft.github.io/autogen/docs/Use-Cases/agent_chat#diverse-applications-implemented-with-autogen) from various domains and complexities. This demonstrates how AutoGen can easily support diverse conversation patterns.\n- AutoGen provides [enhanced LLM inference](https://microsoft.github.io/autogen/docs/Use-Cases/enhanced_inference#api-unification). It offers utilities like API unification and caching, and advanced usage patterns, such as error handling, multi-config inference, context programming, etc.\n\nAutoGen was created out of collaborative [research](https://microsoft.github.io/autogen/docs/Research) from Microsoft, Penn State University, and the University of Washington.\n\n<p align=\"right\" style=\"font-size: 14px; color: #555; margin-top: 20px;\">\n  <a href=\"#readme-top\" style=\"text-decoration: none; color: blue; font-weight: bold;\">\n    ↑ Back to Top ↑\n  </a>\n</p>\n\n\n\n## News\n<details>\n\n<summary>Expand</summary>\n\n:fire: June 6, 2024: WIRED publishes a new article on AutoGen: [Chatbot Teamwork Makes the AI Dream Work](https://www.wired.com/story/chatbot-teamwork-makes-the-ai-dream-work/) based on interview with [Adam Fourney](https://github.com/afourney).\n\n:fire: June 4th, 2024: Microsoft Research Forum publishes new update and video on [AutoGen and Complex Tasks](https://www.microsoft.com/en-us/research/video/autogen-update-complex-tasks-and-agents/) presented by [Adam Fourney](https://github.com/afourney).\n\n:fire: May 29, 2024: DeepLearning.ai launched a new short course [AI Agentic Design Patterns with AutoGen](https://www.deeplearning.ai/short-courses/ai-agentic-design-patterns-with-autogen), made in collaboration with Microsoft and Penn State University, and taught by AutoGen creators [Chi Wang](https://github.com/sonichi) and [Qingyun Wu](https://github.com/qingyun-wu).\n\n:fire: May 24, 2024: Foundation Capital published an article on [Forbes: The Promise of Multi-Agent AI](https://www.forbes.com/sites/joannechen/2024/05/24/the-promise-of-multi-agent-ai/?sh=2c1e4f454d97) and a video [AI in the Real World Episode 2: Exploring Multi-Agent AI and AutoGen with Chi Wang](https://www.youtube.com/watch?v=RLwyXRVvlNk).\n\n:fire: May 13, 2024: [The Economist](https://www.economist.com/science-and-technology/2024/05/13/todays-ai-models-are-impressive-teams-of-them-will-be-formidable) published an article about multi-agent systems (MAS) following a January 2024 interview with [Chi Wang](https://github.com/sonichi).\n\n:fire: May 11, 2024: [AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation](https://openreview.net/pdf?id=uAjxFFing2) received the best paper award at the [ICLR 2024 LLM Agents Workshop](https://llmagents.github.io/).\n\n:fire: Apr 26, 2024: [AutoGen.NET](https://microsoft.github.io/autogen-for-net/) is available for .NET developers! Thanks [XiaoYun Zhang](https://www.linkedin.com/in/xiaoyun-zhang-1b531013a/)\n\n:fire: Apr 17, 2024: Andrew Ng cited AutoGen in [The Batch newsletter](https://www.deeplearning.ai/the-batch/issue-245/) and [What's next for AI agentic workflows](https://youtu.be/sal78ACtGTc?si=JduUzN_1kDnMq0vF) at Sequoia Capital's AI Ascent (Mar 26).\n\n:fire: Mar 3, 2024: What's new in AutoGen? 📰[Blog](https://microsoft.github.io/autogen/blog/2024/03/03/AutoGen-Update); 📺[Youtube](https://www.youtube.com/watch?v=j_mtwQiaLGU).\n\n:fire: Mar 1, 2024: the first AutoGen multi-agent experiment on the challenging [GAIA](https://huggingface.co/spaces/gaia-benchmark/leaderboard) benchmark achieved the No. 1 accuracy in all the three levels.\n\n<!-- :tada: Jan 30, 2024: AutoGen is highlighted by Peter Lee in Microsoft Research Forum [Keynote](https://t.co/nUBSjPDjqD). -->\n\n:tada: Dec 31, 2023: [AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation Framework](https://arxiv.org/abs/2308.08155) is selected by [TheSequence: My Five Favorite AI Papers of 2023](https://thesequence.substack.com/p/my-five-favorite-ai-papers-of-2023).\n\n<!-- :fire: Nov 24: pyautogen [v0.2](https://github.com/microsoft/autogen/releases/tag/v0.2.0) is released with many updates and new features compared to v0.1.1. It switches to using openai-python v1. Please read the [migration guide](https://microsoft.github.io/autogen/docs/Installation#python). -->\n\n<!-- :fire: Nov 11: OpenAI's Assistants are available in AutoGen and interoperatable with other AutoGen agents! Checkout our [blogpost](https://microsoft.github.io/autogen/blog/2023/11/13/OAI-assistants) for details and examples. -->\n\n:tada: Nov 8, 2023: AutoGen is selected into [Open100: Top 100 Open Source achievements](https://www.benchcouncil.org/evaluation/opencs/annual.html) 35 days after spinoff from [FLAML](https://github.com/microsoft/FLAML).\n\n<!-- :tada: Nov 6, 2023: AutoGen is mentioned by Satya Nadella in a [fireside chat](https://youtu.be/0pLBvgYtv6U). -->\n\n<!-- :tada: Nov 1, 2023: AutoGen is the top trending repo on GitHub in October 2023. -->\n\n<!-- :tada: Oct 03, 2023: AutoGen spins off from [FLAML](https://github.com/microsoft/FLAML) on GitHub. -->\n\n<!-- :tada: Aug 16: Paper about AutoGen on [arxiv](https://arxiv.org/abs/2308.08155). -->\n\n:tada: Mar 29, 2023: AutoGen is first created in [FLAML](https://github.com/microsoft/FLAML).\n\n<!--\n:fire: FLAML is highlighted in OpenAI's [cookbook](https://github.com/openai/openai-cookbook#related-resources-from-around-the-web).\n\n:fire: [autogen](https://microsoft.github.io/autogen/) is released with support for ChatGPT and GPT-4, based on [Cost-Effective Hyperparameter Optimization for Large Language Model Generation Inference](https://arxiv.org/abs/2303.04673).\n\n:fire: FLAML supports Code-First AutoML & Tuning – Private Preview in [Microsoft Fabric Data Science](https://learn.microsoft.com/en-us/fabric/data-science/). -->\n\n</details>\n\n## Roadmaps\n\nTo see what we are working on and what we plan to work on, please check our\n[Roadmap Issues](https://aka.ms/autogen-roadmap).\n\n<p align=\"right\" style=\"font-size: 14px; color: #555; margin-top: 20px;\">\n  <a href=\"#readme-top\" style=\"text-decoration: none; color: blue; font-weight: bold;\">\n    ↑ Back to Top ↑\n  </a>\n</p>\n\n## Quickstart\nThe easiest way to start playing is\n1. Click below to use the GitHub Codespace\n\n    [![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://codespaces.new/microsoft/autogen?quickstart=1)\n\n 2. Copy OAI_CONFIG_LIST_sample to ./notebook folder, name to OAI_CONFIG_LIST, and set the correct configuration.\n 3. Start playing with the notebooks!\n\n*NOTE*: OAI_CONFIG_LIST_sample lists GPT-4 as the default model, as this represents our current recommendation, and is known to work well with AutoGen. If you use a model other than GPT-4, you may need to revise various system prompts (especially if using weaker models like GPT-3.5-turbo). Moreover, if you use models other than those hosted by OpenAI or Azure, you may incur additional risks related to alignment and safety. Proceed with caution if updating this default.\n\n<p align=\"right\" style=\"font-size: 14px; color: #555; margin-top: 20px;\">\n  <a href=\"#readme-top\" style=\"text-decoration: none; color: blue; font-weight: bold;\">\n    ↑ Back to Top ↑\n  </a>\n</p>\n\n## [Installation](https://microsoft.github.io/autogen/docs/Installation)\n### Option 1. Install and Run AutoGen in Docker\n\nFind detailed instructions for users [here](https://microsoft.github.io/autogen/docs/installation/Docker#step-1-install-docker), and for developers [here](https://microsoft.github.io/autogen/docs/Contribute#docker-for-development).\n\n### Option 2. Install AutoGen Locally\n\nAutoGen requires **Python version >= 3.8, < 3.13**. It can be installed from pip:\n\n```bash\npip install autogen-agentchat~=0.2\n```\n\nMinimal dependencies are installed without extra options. You can install extra options based on the feature you need.\n\n<!-- For example, use the following to install the dependencies needed by the [`blendsearch`](https://microsoft.github.io/FLAML/docs/Use-Cases/Tune-User-Defined-Function#blendsearch-economical-hyperparameter-optimization-with-blended-search-strategy) option.\n```bash\npip install \"autogen-agentchat[blendsearch]~=0.2\"\n``` -->\n\nFind more options in [Installation](https://microsoft.github.io/autogen/docs/Installation#option-2-install-autogen-locally-using-virtual-environment).\n\n<!-- Each of the [`notebook examples`](https://github.com/microsoft/autogen/tree/main/notebook) may require a specific option to be installed. -->\n\nEven if you are installing and running AutoGen locally outside of docker, the recommendation and default behavior of agents is to perform [code execution](https://microsoft.github.io/autogen/docs/FAQ/#code-execution) in docker. Find more instructions and how to change the default behaviour [here](https://microsoft.github.io/autogen/docs/Installation#code-execution-with-docker-(default)).\n\nFor LLM inference configurations, check the [FAQs](https://microsoft.github.io/autogen/docs/FAQ#set-your-api-endpoints).\n\n<p align=\"right\" style=\"font-size: 14px; color: #555; margin-top: 20px;\">\n  <a href=\"#readme-top\" style=\"text-decoration: none; color: blue; font-weight: bold;\">\n    ↑ Back to Top ↑\n  </a>\n</p>\n\n## Multi-Agent Conversation Framework\n\nAutogen enables the next-gen LLM applications with a generic [multi-agent conversation](https://microsoft.github.io/autogen/docs/Use-Cases/agent_chat) framework. It offers customizable and conversable agents that integrate LLMs, tools, and humans.\nBy automating chat among multiple capable agents, one can easily make them collectively perform tasks autonomously or with human feedback, including tasks that require using tools via code.\n\nFeatures of this use case include:\n\n- **Multi-agent conversations**: AutoGen agents can communicate with each other to solve tasks. This allows for more complex and sophisticated applications than would be possible with a single LLM.\n- **Customization**: AutoGen agents can be customized to meet the specific needs of an application. This includes the ability to choose the LLMs to use, the types of human input to allow, and the tools to employ.\n- **Human participation**: AutoGen seamlessly allows human participation. This means that humans can provide input and feedback to the agents as needed.\n\nFor [example](https://github.com/microsoft/autogen/blob/main/test/twoagent.py),\n\n```python\nfrom autogen import AssistantAgent, UserProxyAgent, config_list_from_json\n# Load LLM inference endpoints from an env variable or a file\n# See https://microsoft.github.io/autogen/docs/FAQ#set-your-api-endpoints\n# and OAI_CONFIG_LIST_sample\nconfig_list = config_list_from_json(env_or_file=\"OAI_CONFIG_LIST\")\n# You can also set config_list directly as a list, for example, config_list = [{'model': 'gpt-4', 'api_key': '<your OpenAI API key here>'},]\nassistant = AssistantAgent(\"assistant\", llm_config={\"config_list\": config_list})\nuser_proxy = UserProxyAgent(\"user_proxy\", code_execution_config={\"work_dir\": \"coding\", \"use_docker\": False}) # IMPORTANT: set to True to run code in docker, recommended\nuser_proxy.initiate_chat(assistant, message=\"Plot a chart of NVDA and TESLA stock price change YTD.\")\n# This initiates an automated chat between the two agents to solve the task\n```\n\nThis example can be run with\n\n```python\npython test/twoagent.py\n```\n\nAfter the repo is cloned.\nThe figure below shows an example conversation flow with AutoGen.\n![Agent Chat Example](https://github.com/microsoft/autogen/blob/main/website/static/img/chat_example.png)\n\nAlternatively, the [sample code](https://github.com/microsoft/autogen/blob/main/samples/simple_chat.py) here allows a user to chat with an AutoGen agent in ChatGPT style.\nPlease find more [code examples](https://microsoft.github.io/autogen/docs/Examples#automated-multi-agent-chat) for this feature.\n\n<p align=\"right\" style=\"font-size: 14px; color: #555; margin-top: 20px;\">\n  <a href=\"#readme-top\" style=\"text-decoration: none; color: blue; font-weight: bold;\">\n    ↑ Back to Top ↑\n  </a>\n</p>\n\n## Enhanced LLM Inferences\n\nAutogen also helps maximize the utility out of the expensive LLMs such as ChatGPT and GPT-4. It offers [enhanced LLM inference](https://microsoft.github.io/autogen/docs/Use-Cases/enhanced_inference#api-unification) with powerful functionalities like caching, error handling, multi-config inference and templating.\n\n<!-- For example, you can optimize generations by LLM with your own tuning data, success metrics, and budgets.\n\n```python\n# perform tuning for openai<1\nconfig, analysis = autogen.Completion.tune(\n    data=tune_data,\n    metric=\"success\",\n    mode=\"max\",\n    eval_func=eval_func,\n    inference_budget=0.05,\n    optimization_budget=3,\n    num_samples=-1,\n)\n# perform inference for a test instance\nresponse = autogen.Completion.create(context=test_instance, **config)\n```\n\nPlease find more [code examples](https://microsoft.github.io/autogen/docs/Examples#tune-gpt-models) for this feature. -->\n\n<p align=\"right\" style=\"font-size: 14px; color: #555; margin-top: 20px;\">\n  <a href=\"#readme-top\" style=\"text-decoration: none; color: blue; font-weight: bold;\">\n    ↑ Back to Top ↑\n  </a>\n</p>\n\n## Documentation\n\nYou can find detailed documentation about AutoGen [here](https://microsoft.github.io/autogen/).\n\nIn addition, you can find:\n\n- [Research](https://microsoft.github.io/autogen/docs/Research), [blogposts](https://microsoft.github.io/autogen/blog) around AutoGen, and [Transparency FAQs](https://github.com/microsoft/autogen/blob/main/TRANSPARENCY_FAQS.md)\n\n- [Contributing guide](https://microsoft.github.io/autogen/docs/Contribute)\n\n- [Roadmap](https://github.com/orgs/microsoft/projects/989/views/3)\n\n<p align=\"right\" style=\"font-size: 14px; color: #555; margin-top: 20px;\">\n  <a href=\"#readme-top\" style=\"text-decoration: none; color: blue; font-weight: bold;\">\n    ↑ Back to Top ↑\n  </a>\n</p>\n\n## Related Papers\n\n[AutoGen Studio](https://www.microsoft.com/en-us/research/publication/autogen-studio-a-no-code-developer-tool-for-building-and-debugging-multi-agent-systems/)\n\n```\n@inproceedings{dibia2024studio,\n      title={AutoGen Studio: A No-Code Developer Tool for Building and Debugging Multi-Agent Systems},\n      author={Victor Dibia and Jingya Chen and Gagan Bansal and Suff Syed and Adam Fourney and Erkang (Eric) Zhu and Chi Wang and Saleema Amershi},\n      year={2024},\n      booktitle={Pre-Print}\n}\n```\n\n[AutoGen](https://aka.ms/autogen-pdf)\n\n```\n@inproceedings{wu2023autogen,\n      title={AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation Framework},\n      author={Qingyun Wu and Gagan Bansal and Jieyu Zhang and Yiran Wu and Beibin Li and Erkang Zhu and Li Jiang and Xiaoyun Zhang and Shaokun Zhang and Jiale Liu and Ahmed Hassan Awadallah and Ryen W White and Doug Burger and Chi Wang},\n      year={2024},\n      booktitle={COLM},\n}\n```\n\n[EcoOptiGen](https://arxiv.org/abs/2303.04673)\n\n```\n@inproceedings{wang2023EcoOptiGen,\n    title={Cost-Effective Hyperparameter Optimization for Large Language Model Generation Inference},\n    author={Chi Wang and Susan Xueqing Liu and Ahmed H. Awadallah},\n    year={2023},\n    booktitle={AutoML'23},\n}\n```\n\n[MathChat](https://arxiv.org/abs/2306.01337)\n\n```\n@inproceedings{wu2023empirical,\n    title={An Empirical Study on Challenging Math Problem Solving with GPT-4},\n    author={Yiran Wu and Feiran Jia and Shaokun Zhang and Hangyu Li and Erkang Zhu and Yue Wang and Yin Tat Lee and Richard Peng and Qingyun Wu and Chi Wang},\n    year={2023},\n    booktitle={ArXiv preprint arXiv:2306.01337},\n}\n```\n\n[AgentOptimizer](https://arxiv.org/pdf/2402.11359)\n\n```\n@article{zhang2024training,\n  title={Training Language Model Agents without Modifying Language Models},\n  author={Zhang, Shaokun and Zhang, Jieyu and Liu, Jiale and Song, Linxin and Wang, Chi and Krishna, Ranjay and Wu, Qingyun},\n  journal={ICML'24},\n  year={2024}\n}\n```\n\n[StateFlow](https://arxiv.org/abs/2403.11322)\n```\n@article{wu2024stateflow,\n  title={StateFlow: Enhancing LLM Task-Solving through State-Driven Workflows},\n  author={Wu, Yiran and Yue, Tianwei and Zhang, Shaokun and Wang, Chi and Wu, Qingyun},\n  journal={arXiv preprint arXiv:2403.11322},\n  year={2024}\n}\n```\n\n<p align=\"right\" style=\"font-size: 14px; color: #555; margin-top: 20px;\">\n  <a href=\"#readme-top\" style=\"text-decoration: none; color: blue; font-weight: bold;\">\n    ↑ Back to Top ↑\n  </a>\n</p>\n\n## Contributing\n\nThis project welcomes contributions and suggestions. Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit <https://cla.opensource.microsoft.com>.\n\nIf you are new to GitHub, [here](https://opensource.guide/how-to-contribute/#how-to-submit-a-contribution) is a detailed help source on getting involved with development on GitHub.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information, see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n<p align=\"right\" style=\"font-size: 14px; color: #555; margin-top: 20px;\">\n  <a href=\"#readme-top\" style=\"text-decoration: none; color: blue; font-weight: bold;\">\n    ↑ Back to Top ↑\n  </a>\n</p>\n\n## Contributors Wall\n<a href=\"https://github.com/microsoft/autogen/graphs/contributors\">\n  <img src=\"https://contrib.rocks/image?repo=microsoft/autogen&max=204\" />\n</a>\n\n<p align=\"right\" style=\"font-size: 14px; color: #555; margin-top: 20px;\">\n  <a href=\"#readme-top\" style=\"text-decoration: none; color: blue; font-weight: bold;\">\n    ↑ Back to Top ↑\n  </a>\n</p>\n\n# Legal Notices\n\nMicrosoft and any contributors grant you a license to the Microsoft documentation and other content\nin this repository under the [Creative Commons Attribution 4.0 International Public License](https://creativecommons.org/licenses/by/4.0/legalcode),\nsee the [LICENSE](LICENSE) file, and grant you a license to any code in the repository under the [MIT License](https://opensource.org/licenses/MIT), see the\n[LICENSE-CODE](LICENSE-CODE) file.\n\nMicrosoft, Windows, Microsoft Azure, and/or other Microsoft products and services referenced in the documentation\nmay be either trademarks or registered trademarks of Microsoft in the United States and/or other countries.\nThe licenses for this project do not grant you rights to use any Microsoft names, logos, or trademarks.\nMicrosoft's general trademark guidelines can be found at http://go.microsoft.com/fwlink/?LinkID=254653.\n\nPrivacy information can be found at https://go.microsoft.com/fwlink/?LinkId=521839\n\nMicrosoft and any contributors reserve all other rights, whether under their respective copyrights, patents,\nor trademarks, whether by implication, estoppel, or otherwise.\n\n<p align=\"right\" style=\"font-size: 14px; color: #555; margin-top: 20px;\">\n  <a href=\"#readme-top\" style=\"text-decoration: none; color: blue; font-weight: bold;\">\n    ↑ Back to Top ↑\n  </a>\n</p>\n"
    },
    {
      "name": "UD1sto/Livepeer-Autogen-Integration",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/76160388?s=40&v=4",
      "owner": "UD1sto",
      "repo_name": "Livepeer-Autogen-Integration",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2025-04-18T17:34:42Z",
      "updated_at": "2025-04-18T17:50:09Z",
      "topics": [],
      "readme": "# Livepeer Integration with AutoGen\n\nThis project demonstrates how to integrate Livepeer's LLM API with AutoGen Core 0.5+. It provides a custom client implementation that bridges the gap between Livepeer's API and AutoGen's expected interfaces.\n\n## Overview\n\nThe integration consists of two key components:\n\n1. **LivepeerManualClient**: A custom client that implements AutoGen Core's ChatCompletionClient interface, allowing it to interact with Livepeer's LLM API.\n2. **Minimal Agent Demo**: A simple demonstration of how to use this client in a multi-agent conversation flow.\n\n## Components\n\n### LivepeerManualClient\n\nThis client implements the `ChatCompletionClient` interface from AutoGen Core to make requests to Livepeer's LLM API. It:\n\n- Converts AutoGen message formats to Livepeer's expected format\n- Handles authentication and request configuration\n- Processes responses back into AutoGen's expected format\n- Provides proper resource management and cleanup\n\n### Minimal Demo\n\nA standalone script that demonstrates a multi-agent conversation flow using the Livepeer client. It simulates a three-agent team:\n\n1. **Planner**: Creates a plan for fulfilling the user's request\n2. **Executor**: Implements the plan\n3. **Critic**: Evaluates the execution and provides feedback\n\n## Installation\n\n### Using pip\n\nThe easiest way to install the required dependencies is using the provided requirements.txt file:\n\n```bash\npip install -r livepeer_team_example/requirements.txt\n```\n\n### Manual Installation\n\nIf you prefer to install dependencies manually, you'll need:\n\n```bash\npip install autogen-core>=0.5.3 httpx>=0.28.0\n```\n\n### Compatibility\n\nThis implementation is fully compatible with:\n- **AutoGen Core 0.5.3+** - Microsoft's newer, more modular implementation\n- Python 3.8+ (recommended: Python 3.10)\n\n## Usage\n\n### Verifying Compatibility\n\nTo verify that your environment is compatible with this implementation, you can run the included compatibility check:\n\n```bash\npython -m livepeer_team_example.test_compatibility\n```\n\nThis will:\n- Check your installed AutoGen Core version\n- Verify that the LivepeerManualClient implements all required interfaces\n- Confirm that all necessary methods and properties are available\n\n### Running the Minimal Demo\n\nThe minimal demo provides a complete example of a multi-agent team using the Livepeer client:\n\n```bash\npython -m livepeer_team_example.minimal_demo\n```\n\n### Configuration\n\nWhen using the client in your own applications, you'll need to configure it with:\n\n1. **base_url**: The Livepeer API endpoint URL (example: `https://dream-gateway.livepeer.cloud/llm`)\n2. **model_name**: The model identifier to use (example: `meta-llama/Meta-Llama-3.1-8B-Instruct`)\n3. **auth_header**: Your authentication token (example: `Bearer your-auth-token` or `Autogen-swarm`)\n4. **max_tokens** (optional): Maximum completion length (default: 150)\n\n### Using the Client in Your Own Code\n\nHere's a complete example of using the client in an async context:\n\n```python\nimport asyncio\nfrom livepeer_team_example.livepeer_manual_client import LivepeerManualClient\nfrom autogen_core.models import SystemMessage, UserMessage\n\nasync def main():\n    # Create the client\n    client = LivepeerManualClient(\n        base_url=\"https://dream-gateway.livepeer.cloud/llm\",\n        model_name=\"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n        auth_header=\"Autogen-swarm\",\n        max_tokens=150\n    )\n    \n    try:\n        # Set up messages\n        messages = [\n            SystemMessage(content=\"You are a helpful assistant.\"),\n            UserMessage(content=\"Write a haiku about clouds.\", source=\"user\")\n        ]\n        \n        # Make a request\n        response = await client.create(messages=messages)\n        print(f\"Response: {response.content}\")\n        \n        # Check token usage\n        if response.usage:\n            print(f\"Tokens used: {response.usage.prompt_tokens} prompt, \" \n                  f\"{response.usage.completion_tokens} completion\")\n    \n    finally:\n        # Always close the client when done\n        await client.close()\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n### Creating Multi-Turn Conversations\n\nFor multi-turn conversations, append previous responses as `AssistantMessage` objects:\n\n```python\nfrom autogen_core.models import AssistantMessage\n\n# After getting the first response\nmessages.append(AssistantMessage(content=response.content, source=\"assistant\"))\n\n# Add the next user message\nmessages.append(UserMessage(content=\"Tell me more about haikus.\", source=\"user\"))\n\n# Make another request with the full conversation history\nresponse2 = await client.create(messages=messages)\n```\n\n### Creating Multi-Agent Systems\n\nFor more complex agent systems like the minimal demo, follow this pattern:\n\n1. Create specialized system messages for each agent role\n2. Pass relevant context from previous agents to the next agent\n3. Use `AssistantMessage` objects to represent previous agent outputs\n4. Handle the conversation flow according to your application's needs\n\n## Technical Details\n\n### Packages Used\n\n- **autogen-core** (0.5.x): Provides the core interfaces for the client implementation. This is the newer Microsoft AutoGen Core library, which is distinct from the original PyAutogen.\n- **httpx**: For making asynchronous HTTP requests\n\n### Relationship Between AutoGen Packages\n\nIt's important to understand the relationship between different AutoGen packages:\n\n1. **PyAutogen** (often just called \"AutoGen\"): The original implementation, available via `pip install pyautogen`. This is a higher-level framework with agents, conversations, and many built-in capabilities.\n\n2. **AutoGen Core** (used in this project): A newer, more modular implementation from Microsoft that provides lower-level interfaces and more flexibility. Available via `pip install autogen-core`.\n\nThis project specifically uses AutoGen Core, which offers better modularity and flexibility for custom integrations like this. The original PyAutogen is not used in this implementation.\n\n### Integration Challenges\n\nThe solution addresses several challenges in integrating Livepeer with AutoGen:\n\n1. **Message Format Conversion**: Converts between AutoGen's message format and Livepeer's expected format\n2. **Proper Authentication**: Handles Livepeer's authentication requirements\n3. **Response Processing**: Properly formats Livepeer responses for AutoGen consumption\n4. **Resource Management**: Ensures proper cleanup of resources\n\n### Notes on PyAutogen Integration\n\nWhile this implementation successfully works with autogen-core, direct integration with the higher-level PyAutogen framework presents additional challenges:\n\n- PyAutogen expects OpenAI-compatible clients with specific methods and behavior\n- Event loop handling differs between the libraries\n- Deep copying of clients can cause issues\n\nThe minimal_demo.py shows how to implement agent interactions without these complications.\n\n## License\n\nThis project is available under the MIT License. "
    },
    {
      "name": "KamikazEr101/GoodByeSqlBoy",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/154207461?s=40&v=4",
      "owner": "KamikazEr101",
      "repo_name": "GoodByeSqlBoy",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2025-03-21T02:46:23Z",
      "updated_at": "2025-03-21T15:10:24Z",
      "topics": [],
      "readme": "# GoodbyeSqlBoy\nGoodbyeSqlBoy是一个基于AutoGen与FastApi搭建的根据自然语言生成sql语句的框架\n\n以下是它的特点：\n\n- 对外暴露端口，支持不同语言框架的调用\n- 内部不集成任何ORM框架，考虑到框架的跨语言性，防止与其他事务管理框架冲突，于是采用轻量化的设计理念，不与数据库进行任何交互\n- 生成的sql语句的准确性与查询效率是可以被调整的，根据参数去决定是否开启optimize agent进行sql准确性的提升与慢sql的优化\n- 基于redis进行sql语句的缓存，减少响应耗时(可选, 根据参数调整是否开启)\n\n**note**：关于第四点的实现，是基于框架的设计理念的，框架的实际运行是结合其他框架对本框架的端口发送post请求，postbody内容属于硬编码，故本框架内部采用sha256对自然语言进行加密存入redis，而不再使用多余的agent进行键的匹配\n\n\n# v0.1\n## QuickStart:\n### 0. 初始化配置\n在项目根目录下创建.env文件, \n按照.example_env文件中的配置项进行配置, 并将想要生成SQL语句的表的DDL建表SQL文件放在指定目录下(如果没有, 就直接照着.example_env文件的SQL_FILE_PATH设定, \n将最后一层路径设置为项目中的resource文件夹, 注意前面的路径要改成自己电脑中的绝对路径. resource文件中已放了两个建表SQL, 是经典的员工+部门多对一关系表)\n启动main.py\n#### 人话:  将.example_env文件名改成.env, 将SQL_FILE_PATH除了resource的最后一层目录以外全改为自己电脑放这个项目的地址, 然后运行main.py\n\n### 1. 通过HTTP请求对外暴露的接口进行交互(若用不到, 则转2)\n在启动main.py后, 对  http://localhost:8000/api/v1/nl2sql  发送post请求, 请求体规定为json格式, 属性只有一个 -> \"query\"(自然语言), 可进入schemas模块查看具体定义\n### 2. 通过控制台标准输入输出流进行交互\n在.env文件中, 将CONSOLE_INPUT配置设为True(和.example_env文件写法一样), 输入想要进行的查询的自然语言(人话), 看到控制台中输出agent的执行流程, 等待最后结果即可\n\n## 没有redis / 用不到redis 怎么办:\n\n和.example_env文件写法一样, 将自己在项目根目录下的.env文件中, 将`ENABLE_REDIS`参数设为False"
    },
    {
      "name": "aribornstein/autogen-llamacpp-extension",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/3045407?s=40&v=4",
      "owner": "aribornstein",
      "repo_name": "autogen-llamacpp-extension",
      "description": "An extension of the autogen library to enable use of local llamacpp such as Phi4 with tools",
      "homepage": null,
      "language": "Python",
      "created_at": "2025-01-26T08:22:20Z",
      "updated_at": "2025-02-27T08:25:59Z",
      "topics": [],
      "readme": "\n# AutoGen Llama-CPP Chat Completion Extension\n\nThis extension provides a **Chat Completion Client** using the **Llama-CPP** model. It integrates with the AutoGen ecosystem, enabling AI-powered chat completion with access to external tools.\n\n## Installation\n\nTo install the `autogen-llama-cpp-chat-completion` extension, run the following command:\n\n```bash\npip install autogen-llama-cpp-chat-completion\n```\n\n### Dependencies\n- `autogen-core>=0.4,<0.5`\n- `pydantic`\n- `llama-cpp`\n\n## Usage\n\nOnce installed, you can integrate this extension into your AutoGen system for chat-based completions using the Llama-CPP model.\n\n\n### Example Usage\n\nHere’s an example of how you can use the extension to create a chat session with Llama-CPP:\n\n```python\nfrom autogen_llama_cpp_chat_completion.llama_cpp_extension import LlamaCppChatCompletionClient\n\nfrom autogen_core.models import SystemMessage, UserMessage\n\n# Initialize the LlamaCpp client\nclient = LlamaCppChatCompletionClient(\n    repo_id=\"your_repo_id\", \n    filename=\"path_to_model_file\", \n    n_gpu_layers=-1, \n    seed=1337, \n    n_ctx=1000, \n    verbose=True\n)\n\n# Create chat messages\nmessages = [\n    SystemMessage(content=\"You are a helpful assistant.\"),\n    UserMessage(content=\"What is the capital of France?\")\n]\n\n# Get a response from the model\nresult = await client.create(messages)\n\n# Print the result\nprint(result.content)\n```\n\n### Phi-4 Model Example\n\nYou can also use the `phi-4` model for chat completions. Here's an example of how to integrate it with the extension:\n\n```python\nfrom autogen_llama_cpp_chat_completion.llama_cpp_extension import LlamaCppChatCompletionClient\nfrom autogen_core.models import SystemMessage, UserMessage\n\n# Initialize the Phi-4 LlamaCpp client\nmodel_client = LlamaCppChatCompletionClient(\n    repo_id=\"unsloth/phi-4-GGUF\",\n    filename=\"phi-4-Q2_K_L.gguf\",\n    n_gpu_layers=-1,\n    seed=1337,\n    n_ctx=16384,\n    verbose=False,\n)\n\n# Create chat messages\nmessages = [\n    SystemMessage(content=\"You are an assistant with the Phi-4 model.\"),\n    UserMessage(content=\"What is the latest breakthrough in AI research?\")\n]\n\n# Get a response from the model\nresult = await model_client.create(messages)\n\n# Print the result\nprint(result.content)\n```\n\nThis example demonstrates how to use the `phi-4` model, providing a larger context window (`n_ctx=16384`) and using the model from the `\"unsloth/phi-4-GGUF\"` repository.\n\n### Streaming Mode\n\nYou can also use streaming mode to generate responses incrementally:\n\n```python\nresponse_generator = client.create_stream(messages)\n\n# Iterate through the response stream\nasync for token in response_generator:\n    print(token)\n```\n\n## Configuration\n\nWhen initializing the `LlamaCppChatCompletionClient`, you can provide the following configuration parameters:\n\n- `repo_id`: The repository ID for the model you want to use.\n- `filename`: The path to the model file.\n- `n_gpu_layers`: The number of GPU layers (default is -1).\n- `seed`: The random seed to use for initialization (default is 1337).\n- `n_ctx`: The context window size (default is 1000).\n- `verbose`: Whether to print debug information (default is `True`).\n\n## Tools Integration\n\nYou can dynamically register tools that the model can use during interaction. If a message invokes a tool, it will be detected, and the corresponding tool will be executed. \n\nTools should be passed as part of the `tools` argument when calling the `create` or `create_stream` methods.\n\n### Example Tool Usage\n\nIf you have a tool, such as a request validation tool, you can register it and the model will use it if necessary.\n\n```python\nfrom autogen_core.tools import FunctionTool\n\ntools = [\n    FunctionTool(name=\"validate_request\", description=\"Validates request data\")\n]\n\nresult = await client.create(messages, tools=tools)\n```\n\n## Running Tests\n\nTo ensure that everything is working correctly, you can run tests with `pytest`:\n\n```bash\npytest\n```\n\n## Contributing\n\nIf you’d like to contribute to this extension, feel free to open an issue or submit a pull request.\n\n## License\n\nThis extension is open source and available under the MIT License. See the [LICENSE](LICENSE) file for more information.\n\n## Topics\n\n- [AutoGen](https://github.com/microsoft/autogen)\n- [autogen-extension](https://github.com/topics/autogen-extension)\n- [Llama-CPP](https://github.com/facebook/llama-cpp)\n\n\n"
    },
    {
      "name": "yikedu7/poc",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/25717773?s=40&v=4",
      "owner": "yikedu7",
      "repo_name": "poc",
      "description": "some proofs of concept",
      "homepage": "",
      "language": "JavaScript",
      "created_at": "2021-12-05T18:54:24Z",
      "updated_at": "2025-03-19T22:56:00Z",
      "topics": [],
      "readme": "\n"
    },
    {
      "name": "mtliou/BioPharma451",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/164873773?s=40&v=4",
      "owner": "mtliou",
      "repo_name": "BioPharma451",
      "description": null,
      "homepage": null,
      "language": "HTML",
      "created_at": "2024-12-16T00:41:49Z",
      "updated_at": "2024-12-16T00:45:06Z",
      "topics": [],
      "readme": "# Magentic-One\n\n> [!CAUTION]\n> Using Magentic-One involves interacting with a digital world designed for humans, which carries inherent risks. To minimize these risks, consider the following precautions:\n>\n> 1. **Use Containers**: Run all tasks in docker containers to isolate the agents and prevent direct system attacks.\n> 2. **Virtual Environment**: Use a virtual environment to run the agents and prevent them from accessing sensitive data.\n> 3. **Monitor Logs**: Closely monitor logs during and after execution to detect and mitigate risky behavior.\n> 4. **Human Oversight**: Run the examples with a human in the loop to supervise the agents and prevent unintended consequences.\n> 5. **Limit Access**: Restrict the agents' access to the internet and other resources to prevent unauthorized actions.\n> 6. **Safeguard Data**: Ensure that the agents do not have access to sensitive data or resources that could be compromised. Do not share sensitive information with the agents.\n> Be aware that agents may occasionally attempt risky actions, such as recruiting humans for help or accepting cookie agreements without human involvement. Always ensure agents are monitored and operate within a controlled environment to prevent unintended consequences. Moreover, be cautious that Magentic-One may be susceptible to prompt injection attacks from webpages.\n\n> [!NOTE]\n> This code is currently being ported to AutoGen AgentChat. If you want to build on top of Magentic-One, we recommend waiting for the port to be completed. In the meantime, you can use this codebase to experiment with Magentic-One.\n\n\nWe are introducing Magentic-One, our new generalist multi-agent system for solving open-ended web and file-based tasks across a variety of domains. Magentic-One represents a significant step towards developing agents that can complete tasks that people encounter in their work and personal lives.\n\nFind additional information about Magentic-one in our [blog post](https://aka.ms/magentic-one-blog) and [technical report](https://arxiv.org/abs/2411.04468).\n\n![](./imgs/autogen-magentic-one-example.png)\n\n> _Example_: The figure above illustrates Magentic-One mutli-agent team completing a complex task from the GAIA benchmark. Magentic-One's Orchestrator agent creates a plan, delegates tasks to other agents, and tracks progress towards the goal, dynamically revising the plan as needed. The Orchestrator can delegate tasks to a FileSurfer agent to read and handle files, a WebSurfer agent to operate a web browser, or a Coder or Computer Terminal agent to write or execute code, respectively.\n\n## Architecture\n\n\n\n![](./imgs/autogen-magentic-one-agents.png)\n\nMagentic-One work is based on a multi-agent architecture where a lead Orchestrator agent is responsible for high-level planning, directing other agents and tracking task progress. The Orchestrator begins by creating a plan to tackle the task, gathering needed facts and educated guesses in a Task Ledger that is maintained. At each step of its plan, the Orchestrator creates a Progress Ledger where it self-reflects on task progress and checks whether the task is completed. If the task is not yet completed, it assigns one of Magentic-One other agents a subtask to complete. After the assigned agent completes its subtask, the Orchestrator updates the Progress Ledger and continues in this way until the task is complete. If the Orchestrator finds that progress is not being made for enough steps, it can update the Task Ledger and create a new plan. This is illustrated in the figure above; the Orchestrator work is thus divided into an outer loop where it updates the Task Ledger and an inner loop to update the Progress Ledger.\n\nOverall, Magentic-One consists of the following agents:\n- Orchestrator: the lead agent responsible for task decomposition and planning, directing other agents in executing subtasks, tracking overall progress, and taking corrective actions as needed \n- WebSurfer: This is an LLM-based agent that is proficient in commanding and managing the state of a Chromium-based web browser. With each incoming request, the WebSurfer performs an action on the browser then reports on the new state of the web page   The action space of the WebSurfer includes navigation (e.g. visiting a URL, performing a web search);  web page actions (e.g., clicking and typing); and reading actions (e.g., summarizing or answering questions). The WebSurfer relies on the accessibility tree of the browser and on set-of-marks prompting to perform its actions.\n- FileSurfer: This is an LLM-based agent that commands a markdown-based file preview application to read local files of most types. The FileSurfer can also perform common navigation tasks such as listing the contents of directories and navigating a folder structure.\n- Coder: This is an LLM-based agent specialized through its system prompt for writing code, analyzing information collected from the other agents, or creating new artifacts.\n- ComputerTerminal: Finally, ComputerTerminal provides the team with access to a console shell where the Coder’s programs can be executed, and where new programming libraries can be installed.\n\nTogether, Magentic-One’s agents provide the Orchestrator with the tools and capabilities that it needs to solve a broad variety of open-ended problems, as well as the ability to autonomously adapt to, and act in, dynamic and ever-changing web and file-system environments. \n\nWhile the default multimodal LLM we use for all agents is GPT-4o, Magentic-One is model agnostic and can incorporate heterogonous models to support different capabilities or meet different cost requirements when getting tasks done. For example, it can use different LLMs and SLMs and their specialized versions to power different agents. We recommend a strong reasoning model for the Orchestrator agent such as GPT-4o. In a different configuration of Magentic-One, we also experiment with using OpenAI o1-preview for the outer loop of the Orchestrator and for the Coder, while other agents continue to use GPT-4o. \n\n\n### Logging in Team One Agents\n\nTeam One agents can emit several log events that can be consumed by a log handler (see the example log handler in [utils.py](src/autogen_magentic_one/utils.py)). A list of currently emitted events are:\n\n- OrchestrationEvent : emitted by a an [Orchestrator](src/autogen_magentic_one/agents/base_orchestrator.py) agent.\n- WebSurferEvent : emitted by a [WebSurfer](src/autogen_magentic_one/agents/multimodal_web_surfer/multimodal_web_surfer.py) agent.\n\nIn addition, developers can also handle and process logs generated from the AutoGen core library (e.g., LLMCallEvent etc). See the example log handler in [utils.py](src/autogen_magentic_one/utils.py) on how this can be implemented. By default, the logs are written to a file named `log.jsonl` which can be configured as a parameter to the defined log handler. These logs can be parsed to retrieved data agent actions.\n\n# Setup and Usage\n\nYou can install the Magentic-One package and then run the example code to see how the agents work together to accomplish a task.\n\n1. Clone the code and install the package:\n\n    The easiest way to install is with the [uv package installer](https://docs.astral.sh/uv/getting-started/installation/) which you need to install separately, however, this is not necessary.\n\n    Clone repo, use uv to setup and activate virtual environment:\n    ```bash\n    git clone https://github.com/microsoft/autogen.git\n    cd autogen/python\n    uv sync  --all-extras\n    source .venv/bin/activate\n    ```\n   For Windows, run `.venv\\Scripts\\activate` to activate the environment.\n   \n2. Install magentic-one from source:\n    ```bash\n    cd packages/autogen-magentic-one\n    pip install -e .\n    ```\n    \n    The following instructions are for running the example code:\n\n3. Configure the environment variables for the chat completion client. See instructions below [Environment Configuration for Chat Completion Client](#environment-configuration-for-chat-completion-client).\n4. Magentic-One code uses code execution, you need to have [Docker installed](https://docs.docker.com/engine/install/) to run any examples.\n5. Magentic-One uses playwright to interact with web pages. You need to install the playwright dependencies. Run the following command to install the playwright dependencies:\n\n```bash\nplaywright install --with-deps chromium\n```\n6. Now you can run the example code to see how the agents work together to accomplish a task.\n\n> [!CAUTION]  \n> The example code may download files from the internet, execute code, and interact with web pages. Ensure you are in a safe environment before running the example code. \n\n> [!NOTE]\n> You will need to ensure Docker is running prior to running the example. \n\n  ```bash\n\n  # Specify logs directory\n  python examples/example.py --logs_dir ./logs\n\n  # Enable human-in-the-loop mode\n  python examples/example.py --logs_dir ./logs --hil_mode\n\n  # Save screenshots of browser\n  python examples/example.py --logs_dir ./logs --save_screenshots\n  ```\n\n  Arguments:\n\n  - logs_dir: (Required) Directory for logs, downloads and screenshots of browser (default: current directory)\n  - hil_mode: (Optional) Enable human-in-the-loop mode (default: disabled)\n  - save_screenshots: (Optional) Save screenshots of browser (default: disabled)\n\n7. [Preview] We have a preview API for Magentic-One. \n You can use the `MagenticOneHelper` class to interact with the system and stream logs. See the [interface README](interface/README.md) for more details.\n\n\n## Environment Configuration for Chat Completion Client\n\nThis guide outlines how to configure your environment to use the `create_completion_client_from_env` function, which reads environment variables to return an appropriate `ChatCompletionClient`.\n\nCurrently, Magentic-One only supports OpenAI's GPT-4o as the underlying LLM.\n\n### Azure OpenAI service\n\nTo configure for Azure OpenAI service, set the following environment variables:\n\n- `CHAT_COMPLETION_PROVIDER='azure'`\n- `CHAT_COMPLETION_KWARGS_JSON` with the following JSON structure:\n\n```json\n{\n  \"api_version\": \"2024-02-15-preview\",\n  \"azure_endpoint\": \"REPLACE_WITH_YOUR_ENDPOINT\",\n  \"model_capabilities\": {\n    \"function_calling\": true,\n    \"json_output\": true,\n    \"vision\": true\n  },\n  \"azure_ad_token_provider\": \"DEFAULT\",\n  \"model\": \"gpt-4o-2024-05-13\"\n}\n```\n\nThis project uses Azure OpenAI service with [Entra ID authentcation by default](https://learn.microsoft.com/azure/ai-services/openai/how-to/managed-identity). If you run the examples on a local device, you can use the Azure CLI cached credentials for testing:\n\nLog in to Azure using `az login`, and then run the examples. The account used must have [RBAC permissions](https://learn.microsoft.com/azure/ai-services/openai/how-to/role-based-access-control) like `Azure Cognitive Services OpenAI User` for the OpenAI service; otherwise, you will receive the error: Principal does not have access to API/Operation. \n\nNote that even if you are the owner of the subscription, you still need to grant the necessary Azure Cognitive Services OpenAI permissions to call the API.\n\n### With OpenAI\n\nTo configure for OpenAI, set the following environment variables:\n\n- `CHAT_COMPLETION_PROVIDER='openai'`\n- `CHAT_COMPLETION_KWARGS_JSON` with the following JSON structure:\n\n```json\n{\n  \"api_key\": \"REPLACE_WITH_YOUR_API\",\n  \"model\": \"gpt-4o-2024-05-13\"\n}\n```\nFeel free to replace the model with newer versions of gpt-4o if needed.\n\n### Other Keys (Optional)\n\nSome functionalities, such as using web-search requires an API key for Bing.\nYou can set it using:\n\n```bash\nexport BING_API_KEY=xxxxxxx\n```\n\n## Citation\n\n```\n@misc{fourney2024magenticonegeneralistmultiagentsolving,\n      title={Magentic-One: A Generalist Multi-Agent System for Solving Complex Tasks}, \n      author={Adam Fourney and Gagan Bansal and Hussein Mozannar and Cheng Tan and Eduardo Salinas and Erkang and Zhu and Friederike Niedtner and Grace Proebsting and Griffin Bassman and Jack Gerrits and Jacob Alber and Peter Chang and Ricky Loynd and Robert West and Victor Dibia and Ahmed Awadallah and Ece Kamar and Rafah Hosn and Saleema Amershi},\n      year={2024},\n      eprint={2411.04468},\n      archivePrefix={arXiv},\n      primaryClass={cs.AI},\n      url={https://arxiv.org/abs/2411.04468}, \n}\n```\n"
    },
    {
      "name": "selafmai/Magenitc-one-AGENTIC-InsurTech",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/157835927?s=40&v=4",
      "owner": "selafmai",
      "repo_name": "Magenitc-one-AGENTIC-InsurTech",
      "description": "Magentic-one AGENTIC InsurTech workflow system",
      "homepage": "https://www.linkedin.com/company/insur-cap/",
      "language": "Python",
      "created_at": "2024-12-15T16:13:44Z",
      "updated_at": "2024-12-15T16:44:25Z",
      "topics": [
        "agentic",
        "agentic-workflow",
        "autogen",
        "insurtech",
        "magenticone"
      ],
      "readme": "# AGENTIC InsurTech Aplikacija\r\n\r\n![magentic-one 0](https://github.com/user-attachments/assets/0501a6ce-a5d9-4a45-9401-f490f90fc4d8)\r\n\r\n\r\n## Opis\r\nAGENTIC InsurTech je napredna zavarovalniška aplikacija, ki uporablja večagentni sistem MagenticOne za avtomatizacijo zavarovalniških procesov. Aplikacija omogoča oceno tveganja na podlagi slik, izračun premij in avtomatsko obdelavo zahtevkov.\r\n\r\n## Uporabljene tehnologije\r\n\r\n### Glavni agentni sistem\r\n- **MagenticOne**: Microsoftov večagentni sistem za reševanje kompleksnih nalog\r\n  - Dokumentacija: [MagenticOne](https://www.microsoft.com/en-us/research/articles/magentic-one-a-generalist-multi-agent-system-for-solving-complex-tasks/)\r\n  - GitHub: [autogen-magentic-one](https://github.com/microsoft/autogen/tree/main/python/packages/autogen-magentic-one)\r\n\r\n### Ključne knjižnice\r\n- `autogen-core`: Jedro agentnega sistema\r\n- `autogen-magentic-one`: Implementacija MagenticOne agentov\r\n- `autogen-ext`: Razširitve za AutoGen\r\n- `gradio`: Uporabniški vmesnik\r\n- `streamlit`: Alternativni uporabniški vmesnik\r\n- `playwright`: Spletno brskanje in zajem podatkov\r\n\r\n### API integracije\r\n- Together.ai API: LLM model za procesiranje naravnega jezika\r\n- Stripe API: Procesiranje plačil\r\n- Weather API: Vremenska napoved za parametrično zavarovanje\r\n\r\n![magentic-one struktura map](https://github.com/user-attachments/assets/9ad6582f-bbd5-4a2f-aaec-3a08fb541459)\r\n\r\n\r\n\r\n## Struktura projekta \r\nMagnetic-one/\r\n├── .env # Konfiguracijske spremenljivke\r\n├── requirements.txt # Potrebni paketi\r\n├── README.md # Dokumentacija\r\n└── src/\r\n├── main.py # Glavna aplikacija\r\n├── agents/ # Implementacije agentov\r\n├── utils/ # Pomožne funkcije\r\n└── runtime/ # Orkestrator in runtime\r\n\r\n## Agenti v sistemu\r\n\r\n### 1. ResearchAgent\r\n- Analiza slik\r\n- Identifikacija objektov\r\n- Ocena tveganja\r\n\r\n### 2. UnderwritingAgent\r\n- Izračun premije\r\n- Določanje kritja\r\n- Ocena tveganja\r\n\r\n### 3. SalesAgent\r\n- Priprava ponudb\r\n- Komunikacija s strankami\r\n- Generiranje dokumentov\r\n\r\n## Namestitev in zagon\r\n\r\n1. Kloniranje repozitorija:\r\n```bash\r\ngit clone [repository-url]\r\ncd Magnetic-one\r\n```\r\n\r\n2. Ustvarjanje virtualnega okolja:\r\n```bash\r\npython -m venv venv\r\nsource venv/bin/activate  # Linux/Mac\r\n# ali\r\nvenv\\Scripts\\activate     # Windows\r\n```\r\n\r\n3. Namestitev potrebnih paketov:\r\n```bash\r\npip install -r requirements.txt\r\nplaywright install --with-deps chromium\r\n```\r\n\r\n4. Nastavitev okolja:\r\n- Ustvarite `.env` datoteko\r\n- Dodajte potrebne API ključe:\r\n  - TOGETHER_API_KEY\r\n  - STRIPE_API_KEY\r\n  - NOTION_API_KEY\r\n\r\n5. Zagon aplikacije:\r\n```bash\r\npython src/run_app.py --interface gradio\r\n# ali\r\npython src/run_app.py --interface streamlit\r\n```\r\n\r\n## Funkcionalnosti\r\n\r\n### Osnovne funkcije\r\n- Analiza slik za oceno tveganja\r\n- Avtomatski izračun premij\r\n- Generiranje zavarovalnih polic\r\n- Procesiranje zahtevkov\r\n\r\n### Napredne funkcije\r\n- Parametrično vremensko zavarovanje\r\n- Avtomatska izplačila\r\n- Preventivni načrti\r\n- Integracija z zunanjimi viri podatkov\r\n\r\n## Razvijalci\r\n- Dokumentacija MagenticOne: [Link](https://github.com/microsoft/autogen/tree/main/python/packages/autogen-magentic-one)\r\n\r\n- AutoGen dokumentacija: [Link](https://microsoft.github.io/autogen/)\r\n\r\n## Licenca\r\nMIT License\r\n\r\n\r\n\r\n\r\n"
    },
    {
      "name": "CodeDevKyle/autogen_DevKyle",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/167522880?s=40&v=4",
      "owner": "CodeDevKyle",
      "repo_name": "autogen_DevKyle",
      "description": null,
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2024-11-10T08:51:41Z",
      "updated_at": "2025-03-01T20:57:34Z",
      "topics": [],
      "readme": "<a name=\"readme-top\"></a>\n\n<div align=\"center\">\n<img src=\"https://microsoft.github.io/autogen/0.2/img/ag.svg\" alt=\"AutoGen Logo\" width=\"100\">\n\n[![Twitter](https://img.shields.io/twitter/url/https/twitter.com/cloudposse.svg?style=social&label=Follow%20%40pyautogen)](https://twitter.com/pyautogen)\n\n</div>\n\n# AutoGen\n\n> [!IMPORTANT]\n>\n> - (10/13/24) Interested in the standard AutoGen as a prior user? Find it at the actively-maintained *AutoGen* [0.2 branch](https://github.com/microsoft/autogen/tree/0.2) and `autogen-agentchat~=0.2` PyPi package.\n> - (10/02/24) [AutoGen 0.4](https://microsoft.github.io/autogen/dev) is a from-the-ground-up rewrite of AutoGen. Learn more about the history, goals and future at [this blog post](https://microsoft.github.io/autogen/blog). We’re excited to work with the community to gather feedback, refine, and improve the project before we officially release 0.4. This is a big change, so AutoGen 0.2 is still available, maintained, and developed in the [0.2 branch](https://github.com/microsoft/autogen/tree/0.2).\n\nAutoGen is an open-source framework for building AI agent systems.\nIt simplifies the creation of event-driven, distributed, scalable, and resilient agentic applications.\nIt allows you to quickly build systems where AI agents collaborate and perform tasks autonomously\nor with human oversight.\n\n- [Key Features](#key-features)\n- [API Layering](#api-layering)\n- [Quickstart](#quickstart)\n- [Roadmap](#roadmap)\n- [FAQs](#faqs)\n\nAutoGen streamlines AI development and research, enabling the use of multiple large language models (LLMs), integrated tools, and advanced multi-agent design patterns. You can develop and test your agent systems locally, then deploy to a distributed cloud environment as your needs grow.\n\n## Key Features\n\nAutoGen offers the following key features:\n\n- **Asynchronous Messaging**: Agents communicate via asynchronous messages, supporting both event-driven and request/response interaction patterns.\n- **Full type support**: use types in all interfaces and enforced type check on build, with a focus on quality and cohesiveness\n- **Scalable & Distributed**: Design complex, distributed agent networks that can operate across organizational boundaries.\n- **Modular & Extensible**: Customize your system with pluggable components: custom agents, tools, memory, and models.\n- **Cross-Language Support**: Interoperate agents across different programming languages. Currently supports Python and .NET, with more languages coming soon.\n- **Observability & Debugging**: Built-in features and tools for tracking, tracing, and debugging agent interactions and workflows, including support for industry standard observability with OpenTelemetry\n\n<p align=\"right\" style=\"font-size: 14px; color: #555; margin-top: 20px;\">\n  <a href=\"#readme-top\" style=\"text-decoration: none; color: blue; font-weight: bold;\">\n    ↑ Back to Top ↑\n  </a>\n</p>\n\n# API Layering\n\nAutoGen has several packages and is built upon a layered architecture.\nCurrently, there are three main APIs your application can target:\n\n- [Core](https://microsoft.github.io/autogen/dev/user-guide/core-user-guide/index.html)\n- [AgentChat](https://microsoft.github.io/autogen/dev/user-guide/agentchat-user-guide/index.html)\n- [Extensions](https://microsoft.github.io/autogen/dev/reference/python/autogen_ext/autogen_ext.html)\n\n## Core\n\n- [Installation](https://microsoft.github.io/autogen/dev/packages/index.html#pkg-info-autogen-core)\n- [Quickstart](https://microsoft.github.io/autogen/dev/user-guide/core-user-guide/quickstart.html)\n\nThe core API of AutoGen, `autogen-core`, is built following the\n[actor model](https://en.wikipedia.org/wiki/Actor_model).\nIt supports asynchronous message passing between agents and event-based workflows.\nAgents in the core layer handle and produce typed messages, using either direct messaging,\nwhich functions like RPC, or via broadcasting to topics, which is pub-sub.\nAgents can be distributed and implemented in different programming languages,\nwhile still communicating with one another.\n**Start here if you are building scalable, event-driven agentic systems.**\n\n## AgentChat\n\n- [Installation](https://microsoft.github.io/autogen/dev/packages/index.html#pkg-info-autogen-agentchat)\n- [Quickstart](https://microsoft.github.io/autogen/dev/user-guide/agentchat-user-guide/quickstart.html)\n\nThe AgentChat API, `autogen-agentchat`, is task driven and at a high level like AutoGen 0.2.\nIt allows you to define conversational agents, compose them into teams and then\nuse them to solve tasks.\nAgentChat itself is built on the core layer, but it abstracts away much of its\nlow-level system concepts.\nIf your workflows don't fit into the AgentChat API, target core instead.\n**Start here if you just want to focus on quickly getting started with multi-agents workflows.**\n\n## Extensions\n\nThe extension package `autogen-ext` contains implementations of the core interfaces using 3rd party systems,\nsuch as OpenAI model client and Azure code executors.\nBesides the built-in extensions, the package accommodates community-contributed\nextensions through namespace sub-packages.\nWe look forward to your contributions!\n\n<p align=\"right\" style=\"font-size: 14px; color: #555; margin-top: 20px;\">\n  <a href=\"#readme-top\" style=\"text-decoration: none; color: blue; font-weight: bold;\">\n    ↑ Back to Top ↑\n  </a>\n</p>\n\n## Quickstart\n\n### Python (AgentChat)\n\nFirst install the packages:\n\n```bash\npip install 'autogen-agentchat==0.4.0.dev4' 'autogen-ext[openai]==0.4.0.dev4'\n```\n\nThe following code uses OpenAI's GPT-4o model and you need to provide your\nAPI key to run.\nTo use Azure OpenAI models, follow the instruction\n[here](https://microsoft.github.io/autogen/dev/user-guide/core-user-guide/cookbook/azure-openai-with-aad-auth.html).\n\n```python\nimport asyncio\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_agentchat.task import Console, TextMentionTermination\nfrom autogen_agentchat.teams import RoundRobinGroupChat\nfrom autogen_ext.models import OpenAIChatCompletionClient\n\n# Define a tool\nasync def get_weather(city: str) -> str:\n    return f\"The weather in {city} is 73 degrees and Sunny.\"\n\nasync def main() -> None:\n    # Define an agent\n    weather_agent = AssistantAgent(\n        name=\"weather_agent\",\n        model_client=OpenAIChatCompletionClient(\n            model=\"gpt-4o-2024-08-06\",\n            # api_key=\"YOUR_API_KEY\",\n        ),\n        tools=[get_weather],\n    )\n\n    # Define termination condition\n    termination = TextMentionTermination(\"TERMINATE\")\n\n    # Define a team\n    agent_team = RoundRobinGroupChat([weather_agent], termination_condition=termination)\n\n    # Run the team and stream messages to the console\n    stream = agent_team.run_stream(task=\"What is the weather in New York?\")\n    await Console(stream)\n\nasyncio.run(main())\n```\n\n### C\\#\n\nThe .NET SDK does not yet support all of the interfaces that the python SDK offers but we are working on bringing them to parity.\nTo use the .NET SDK, you need to add a package reference to the src in your project.\nWe will release nuget packages soon and will update these instructions when that happens.\n\n```\ngit clone https://github.com/microsoft/autogen.git\ncd autogen\n# Switch to the branch that has this code\ngit switch staging-dev\n# Build the project\ncd dotnet && dotnet build AutoGen.sln\n# In your source code, add AutoGen to your project\ndotnet add <your.csproj> reference <path to your checkout of autogen>/dotnet/src/Microsoft.AutoGen/Agents/Microsoft.AutoGen.Agents.csproj\n```\n\nThen, define and run your first agent:\n\n```csharp\nusing Microsoft.AutoGen.Abstractions;\nusing Microsoft.AutoGen.Agents;\nusing Microsoft.Extensions.DependencyInjection;\nusing Microsoft.Extensions.Hosting;\n\n// send a message to the agent\nvar app = await App.PublishMessageAsync(\"HelloAgents\", new NewMessageReceived\n{\n    Message = \"World\"\n}, local: true);\n\nawait App.RuntimeApp!.WaitForShutdownAsync();\nawait app.WaitForShutdownAsync();\n\n[TopicSubscription(\"HelloAgents\")]\npublic class HelloAgent(\n    IAgentContext context,\n    [FromKeyedServices(\"EventTypes\")] EventTypes typeRegistry) : ConsoleAgent(\n        context,\n        typeRegistry),\n        ISayHello,\n        IHandle<NewMessageReceived>,\n        IHandle<ConversationClosed>\n{\n    public async Task Handle(NewMessageReceived item)\n    {\n        var response = await SayHello(item.Message).ConfigureAwait(false);\n        var evt = new Output\n        {\n            Message = response\n        }.ToCloudEvent(this.AgentId.Key);\n        await PublishEvent(evt).ConfigureAwait(false);\n        var goodbye = new ConversationClosed\n        {\n            UserId = this.AgentId.Key,\n            UserMessage = \"Goodbye\"\n        }.ToCloudEvent(this.AgentId.Key);\n        await PublishEvent(goodbye).ConfigureAwait(false);\n    }\n    public async Task Handle(ConversationClosed item)\n    {\n        var goodbye = $\"*********************  {item.UserId} said {item.UserMessage}  ************************\";\n        var evt = new Output\n        {\n            Message = goodbye\n        }.ToCloudEvent(this.AgentId.Key);\n        await PublishEvent(evt).ConfigureAwait(false);\n        await Task.Delay(60000);\n        await App.ShutdownAsync();\n    }\n    public async Task<string> SayHello(string ask)\n    {\n        var response = $\"\\n\\n\\n\\n***************Hello {ask}**********************\\n\\n\\n\\n\";\n        return response;\n    }\n}\npublic interface ISayHello\n{\n    public Task<string> SayHello(string ask);\n}\n```\n\n```bash\ndotnet run\n```\n\n<p align=\"right\" style=\"font-size: 14px; color: #555; margin-top: 20px;\">\n  <a href=\"#readme-top\" style=\"text-decoration: none; color: blue; font-weight: bold;\">\n    ↑ Back to Top ↑\n  </a>\n</p>\n\n## Roadmap\n\n- AutoGen 0.2 - This is the current stable release of AutoGen. We will continue to accept bug fixes and minor enhancements to this version.\n- AutoGen 0.4 - This is the first release of the new architecture. This release is still in *preview*. We will be focusing on the stability of the interfaces, documentation, tutorials, samples, and a collection of built-in agents which you can use. We are excited to work with our community to define the future of AutoGen. We are looking for feedback and contributions to help shape the future of this project. Here are some major planned items:\n  - More programming languages (e.g., TypeScript)\n  - More built-in agents and multi-agent workflows\n  - Deployment of distributed agents\n  - Re-implementation/migration of AutoGen Studio\n  - Integration with other agent frameworks and data sources\n  - Advanced RAG techniques and memory services\n\n<p align=\"right\" style=\"font-size: 14px; color: #555; margin-top: 20px;\">\n  <a href=\"#readme-top\" style=\"text-decoration: none; color: blue; font-weight: bold;\">\n    ↑ Back to Top ↑\n  </a>\n</p>\n\n## FAQs\n\n### What is AutoGen 0.4?\n\nAutoGen v0.4 is a rewrite of AutoGen from the ground up to create a more robust,\nscalable, easier to use, cross-language library for building AI Agents.\nSome key features include asynchronous messaging, support for scalable distributed agents,\nmodular extensible design (bring your own agents, implement behaviors however you like),\ncross-language support, improved observability, and full typing integration.\nIt is a breaking change.\n\n### Why these changes?\n\nWe listened to our AutoGen users, learned from what was working, and adapted to fix what wasn't.\nWe brought together wide-ranging teams working on many different types of AI Agents\nand collaborated to design an improved framework with a more flexible\nprogramming model and better scalability.\n\n### Is this project still maintained?\n\nWe want to reaffirm our commitment to supporting both the original version of AutoGen (0.2) and the redesign (0.4) . AutoGen 0.4 is still work-in-progress, and we shared the code now to build with the community. There are no plans to deprecate the original AutoGen anytime soon, and both versions will be actively maintained.\n\n### Who should use it 0.4?\n\nThis code is still experimental, so expect changes and bugs while we work towards a stable 0.4 release. We encourage early adopters to\ntry it out, give us feedback, and contribute.\nFor those looking for a stable version we recommend to continue using 0.2\n\n### I'm using AutoGen 0.2, should I upgrade?\n\nIf you consider yourself an early adopter, you are comfortable making some\nchanges to your code, and are willing to try it out, then yes.\n\n### How do I still use AutoGen 0.2?\n\nAutoGen 0.2 can be installed with:\n\n```sh\npip install autogen-agentchat~=0.2\n```\n\n### Will AutoGen Studio be supported in 0.4?\n\nYes, this is on the [roadmap](#roadmap).\nOur current plan is to enable an implementation of AutoGen Studio\non the AgentChat high level API which implements a set of agent functionalities\n(agents, teams, etc).\n\n### How do I migrate?\n\nFor users familiar with AutoGen, the AgentChat library in 0.4 provides similar concepts.\nWe are working on a migration guide.\n\n### Is 0.4 done?\n\nWe are still actively developing AutoGen 0.4. One exciting new feature is the emergence of new SDKs for .NET. The python SDKs are further ahead at this time but our goal is to achieve parity. We aim to add additional languages in future releases.\n\n### What is happening next? When will this release be ready?\n\nWe are still working on improving the documentation, samples, and enhancing the code. We are hoping to release before the end of the year when things are ready.\n\n### What is the history of this project?\n\nThe rearchitecture of the framework started with multiple Microsoft teams coming together\nto address the gaps and learnings from AutoGen 0.2 - merging ideas from several predecessor projects.\nThe team worked on this internally for some time to ensure alignment before moving work back to the open in October 2024.\n\n### What is the official channel for support?\n\nUse GitHub [Issues](https://github.com/microsoft/autogen/issues) for bug reports and feature requests.\nUse GitHub [Discussions](https://github.com/microsoft/autogen/discussions) for general questions and discussions.\n\n### Do you use Discord for communications?\n\nWe are unable to use Discord for project discussions. Therefore, we request that all discussions take place on <https://github.com/microsoft/autogen/discussions/> going forward.\n\n### What about forks?\n\n<https://github.com/microsoft/autogen/> remains the only official repo for development and support of AutoGen.\nWe are aware that there are thousands of forks of AutoGen, including many for personal development and startups building with or on top of the library. We are not involved with any of these forks and are not aware of any plans related to them.\n\n### What is the status of the license and open source?\n\nOur project remains fully open-source and accessible to everyone. We understand that some forks use different licenses to align with different interests. We will continue to use the most permissive license (MIT) for the project.\n\n### Can you clarify the current state of the packages?\n\nCurrently, we are unable to make releases to the `pyautogen` package via Pypi due to a change to package ownership that was done without our involvement. Additionally, we are moving to using multiple packages to align with the new design. Please see details [here](https://microsoft.github.io/autogen/dev/packages/index.html).\n\n### Can I still be involved?\n\nWe are grateful to all the contributors to AutoGen 0.2 and we look forward to continuing to collaborate with everyone in the AutoGen community.\n\n<p align=\"right\" style=\"font-size: 14px; color: #555; margin-top: 20px;\">\n  <a href=\"#readme-top\" style=\"text-decoration: none; color: blue; font-weight: bold;\">\n    ↑ Back to Top ↑\n  </a>\n</p>\n\n## Legal Notices\n\nMicrosoft and any contributors grant you a license to the Microsoft documentation and other content\nin this repository under the [Creative Commons Attribution 4.0 International Public License](https://creativecommons.org/licenses/by/4.0/legalcode),\nsee the [LICENSE](LICENSE) file, and grant you a license to any code in the repository under the [MIT License](https://opensource.org/licenses/MIT), see the\n[LICENSE-CODE](LICENSE-CODE) file.\n\nMicrosoft, Windows, Microsoft Azure, and/or other Microsoft products and services referenced in the documentation\nmay be either trademarks or registered trademarks of Microsoft in the United States and/or other countries.\nThe licenses for this project do not grant you rights to use any Microsoft names, logos, or trademarks.\nMicrosoft's general trademark guidelines can be found at <http://go.microsoft.com/fwlink/?LinkID=254653>.\n\nPrivacy information can be found at <https://go.microsoft.com/fwlink/?LinkId=521839>\n\nMicrosoft and any contributors reserve all other rights, whether under their respective copyrights, patents,\nor trademarks, whether by implication, estoppel, or otherwise.\n\n<p align=\"right\" style=\"font-size: 14px; color: #555; margin-top: 20px;\">\n  <a href=\"#readme-top\" style=\"text-decoration: none; color: blue; font-weight: bold;\">\n    ↑ Back to Top ↑\n  </a>\n</p>\n"
    },
    {
      "name": "dtHvinh/QA.WebAPI",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/115357244?s=40&v=4",
      "owner": "dtHvinh",
      "repo_name": "QA.WebAPI",
      "description": "Backend for QA Platform",
      "homepage": null,
      "language": "C#",
      "created_at": "2025-01-08T10:44:59Z",
      "updated_at": "2025-04-22T03:00:30Z",
      "topics": [],
      "readme": "﻿# QA Platform\nThis repository contains a comprehensive Question and Answer platform built with ASP.NET Core and React. The platform allows users to ask questions, provide answers, create collections, bookmark content, and participate in communities.\n\n## Architecture Overview\nThe application follows a clean architecture pattern with:\n\n- Minimal API Endpoints : Using ASP.NET Core's minimal API approach\n- CQRS Pattern : Command Query Responsibility Segregation for better separation of concerns\n- Repository Pattern : For data access abstraction\n- Fluent Validation : For request validation\n- JWT Authentication : For secure user authentication\n\n## Project Structure\n```plaintext\nqa_platform/\n├── back-end/\n│   ├── WebAPI/                  # Main application\n│   │   ├── CommandQuery/        # CQRS implementation\n│   │   ├── Data/                # Database context and migrations\n│   │   ├── Dto/                 # Data transfer objects\n│   │   ├── Endpoints/           # API endpoints organized by feature\n│   │   ├── Filters/             # Request filters and validations\n│   │   ├── Model/               # Domain models\n│   │   ├── Repositories/        # Data access repositories\n│   │   └── Utilities/           # Helper classes and extensions\n│   ├── WebAPI.DocumentDb/       # MongoDB integration\n│   ├── WebAPI.Libraries/        # Shared libraries\n│   └── WebAPI.Storage/          # File storage implementation\n└── front-end/                   # React frontend application\n ```\n\n## Key Features\n### Authentication\n- User registration and login\n- JWT token-based authentication\n- Token refresh mechanism\n### Questions and Answers\n- Create, read, update, delete questions\n- Answer questions\n- Accept answers\n- Upvote/downvote questions and answers\n- Question history tracking\n- Similar question suggestions\n### Comments\n- Add comments to questions and answers\n- Edit and delete comments\n### Bookmarks\n- Bookmark questions for later reference\n- Manage bookmarks\n### Collections\n- Create personal collections of questions\n- Add/remove questions from collections\n- Like collections\n### Communities\n- Create and join communities\n- Public and private communities\n- Community-specific content\n### User Reputation\n- Reputation-based privileges\n- Activity tracking\n## API Endpoints\nThe API is organized into modules, each handling a specific feature:\n\n- Auth : User authentication and token management\n- User : User profile and permissions\n- Question : Question CRUD and related operations\n- Answer : Answer management\n- Comment : Comment operations\n- Bookmark : Bookmark management\n- Collection : Collection operations\n- Community : Community management\n## Technology Stack\n### Backend\n- ASP.NET Core 9.0\n- Entity Framework Core\n- MongoDB (for document storage)\n- Elasticsearch (for search functionality)\n- Serilog (for logging)\n- FluentValidation\n- MediatR (for CQRS implementation)\n### Frontend\n- React\n- TypeScript\n- Axios (for API communication)\n- React Router\n- Form handling with FormData\n## Getting Started\n### Prerequisites\n- .NET 9.0 SDK\n- SQL Server\n- MongoDB (optional)\n- Elasticsearch (optional)\n- Node.js and npm\n### Setup and Configuration\n1. Clone the repository\n2. Configure the database connection in appsettings.json\n3. Run database migrations:\n4. Start the backend:\n5. Navigate to the frontend directory and install dependencies:\n6. Start the frontend:\n## Development Guidelines\n### Adding New Features\n1. Create appropriate models in the Model directory\n2. Add DTOs in the Dto directory\n3. Implement repositories in the Repositories directory\n4. Create commands/queries in the CommandQuery directory\n5. Add endpoints in the appropriate module under Endpoints\n### Validation\nAll DTOs should have corresponding validators in the Filters/Validation/Validators directory.\n\n### Authentication\nEndpoints requiring authentication should use the .RequireAuthorization() extension method.\n\n## File Upload\nThe application supports file uploads (like community icons) using IFormFile and FormData:\n```csharp\n// DTO\npublic record CreateCommunityDto(string Name, string? Description, IFormFile? IconImage, bool IsPrivate);\n\n// Endpoint\n[FromForm] CreateCommunityDto dto\n ```\n\nFrontend implementation:\n\n```javascript\nconst formData = new FormData();\nformData.append('name', name);\nformData.append('description', description);\nformData.append('isPrivate', isPrivate.toString());\nif (imageFile) {\n    formData.append('iconImage', imageFile);\n}\n ```\n\n```csharp\npublic class CommunityModule : IModule\n{\n    public void RegisterEndpoints(IEndpointRouteBuilder endpoints)\n    {\n        var group = endpoints.MapGroup(EG.Community)\n            .WithTags(nameof(CommunityModule))\n            .WithOpenApi();\n\n        group.MapPost(\"/\", CreateCommunity)\n            .WithName(\"CreateCommunity\")\n            .WithSummary(\"Create new community\")\n            .WithDescription(\"Creates a new community with optional icon image\")\n            .DisableAntiforgery()  // Required for FormData\n            .RequireAuthorization()\n            .AddEndpointFilter<FluentValidation<CreateCommunityDto>>();\n\n        // ... other endpoints ...\n    }\n\n    private static async Task<Results<Ok<CreateCommunityResponse>, ProblemHttpResult>> CreateCommunity(\n        [FromForm] CreateCommunityDto dto,  // Changed from [FromBody] to [FromForm]\n        [FromServices] IMediator mediator,\n        CancellationToken cancellationToken)\n    {\n        var command = new CreateCommunityCommand(dto);\n        var result = await mediator.Send(command, cancellationToken);\n        if (!result.IsSuccess)\n        {\n            return ProblemResultExtensions.BadRequest(result.Message);\n        }\n        return TypedResults.Ok(result.Value);\n    }\n\n    // ... other handlers ...\n}\n ```\n\nKey changes:\n\n1. Changed [FromBody] to [FromForm] to handle FormData\n2. Added .DisableAntiforgery() to allow FormData submission\n3. Added endpoint metadata for better documentation\nThe existing CreateCommunityDto record already supports the form fields correctly:\n\n```csharp\npublic record CreateCommunityDto(string Name, string? Description, IFormFile? IconImage, bool IsPrivate);\n ```\n\nNow the endpoint will properly receive:\n\n- name (string)\n- description (string)\n- isPrivate (bool)\n- iconImage (file) from the FormData sent by your React frontend.\n\n# QA Platform\nThis repository contains a comprehensive Question and Answer platform built with ASP.NET Core and React. The platform allows users to ask questions, provide answers, create collections, bookmark content, and participate in communities.\n\n## Architecture Overview\nThe application follows a clean architecture pattern with:\n\n- Minimal API Endpoints : Using ASP.NET Core's minimal API approach\n- CQRS Pattern : Command Query Responsibility Segregation for better separation of concerns\n- Repository Pattern : For data access abstraction\n- Fluent Validation : For request validation\n- JWT Authentication : For secure user authentication\n## Project Structure\n## Key Features\n### Authentication\n- User registration and login\n- JWT token-based authentication\n- Token refresh mechanism\n### Questions and Answers\n- Create, read, update, delete questions\n- Answer questions\n- Accept answers\n- Upvote/downvote questions and answers\n- Question history tracking\n- Similar question suggestions\n### Comments\n- Add comments to questions and answers\n- Edit and delete comments\n### Bookmarks\n- Bookmark questions for later reference\n- Manage bookmarks\n### Collections\n- Create personal collections of questions\n- Add/remove questions from collections\n- Like collections\n### Communities\n- Create and join communities\n- Public and private communities\n- Community-specific content\n### User Reputation\n- Reputation-based privileges\n- Activity tracking\n## API Endpoints\nThe API is organized into modules, each handling a specific feature:\n\n- Auth : User authentication and token management\n- User : User profile and permissions\n- Question : Question CRUD and related operations\n- Answer : Answer management\n- Comment : Comment operations\n- Bookmark : Bookmark management\n- Collection : Collection operations\n- Community : Community management\n## Technology Stack\n### Backend\n- ASP.NET Core 9.0\n- Entity Framework Core\n- MongoDB (for document storage)\n- Elasticsearch (for search functionality)\n- Serilog (for logging)\n- FluentValidation\n- MediatR (for CQRS implementation)\n### Frontend\n- React\n- TypeScript\n- Axios (for API communication)\n- React Router\n- Form handling with FormData\n## Getting Started\n### Prerequisites\n- .NET 9.0 SDK\n- SQL Server\n- MongoDB (optional)\n- Elasticsearch (optional)\n- Node.js and npm\n### Setup and Configuration\n1. Clone the repository\n2. Configure the database connection in appsettings.json\n3. Run database migrations:\n4. Start the backend:\n5. Navigate to the frontend directory and install dependencies:\n6. Start the frontend:\n## Development Guidelines\n### Adding New Features\n1. Create appropriate models in the Model directory\n2. Add DTOs in the Dto directory\n3. Implement repositories in the Repositories directory\n4. Create commands/queries in the CommandQuery directory\n5. Add endpoints in the appropriate module under Endpoints\n### Validation\nAll DTOs should have corresponding validators in the Filters/Validation/Validators directory.\n\n### Authentication\nEndpoints requiring authentication should use the .RequireAuthorization() extension method.\n\n## File Upload\nThe application supports file uploads (like community icons) using IFormFile and FormData:\n\nFrontend implementation:\n\n```javascript\nconst formData = new FormData();\nformData.append('name', name);\nformData.append('description', description);\nformData.append('isPrivate', isPrivate.toString());\nif (imageFile) {\n    formData.append('iconImage', imageFile);\n}\n ```\n\n## License\n[Your License Information]\n\n## Contributors\n[List of Contributors]"
    },
    {
      "name": "PioneerMNDR/MousyHub",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/130105724?s=40&v=4",
      "owner": "PioneerMNDR",
      "repo_name": "MousyHub",
      "description": "Web application for roleplaying with AI-powered characters",
      "homepage": "",
      "language": "C#",
      "created_at": "2024-09-12T08:29:21Z",
      "updated_at": "2025-04-18T19:22:40Z",
      "topics": [],
      "readme": "![Banner](Assets/MousyBanner.png)\n\n**MousyHub** is a web application for roleplaying with AI-powered characters. It offers a user-friendly interface, easy setup, support for various API providers, and unique features to enhance immersion in conversations.  \n\n\n<div align=\"center\">\n  <img src=\"Assets/MousyHubTrailer.gif\" alt=\"trailer\" />\n</div>\n\n---  \n## 🌟 Key Features  \n\n### 🚀 Multi-API Provider Support  \n- **Local Models**: Native execution via `LLamaSharp` (no additional backends required)  \n- **KoboldCPP**: An optimal backend for RP models  \n- **Cloud Solutions**: OpenRouter, LM Studio, and others (testing required)  \n\n### 🧠 Context Optimization  \n- **RAG (Retrieval-Augmented Generation)**: Passive fact-recall system from conversations  \n- **Summarization**: Shortens lengthy dialogues to save context  \n- **Support for ultra-long conversations**  \n\n### 🌍 Multilingual Support  \n- Built-in translator for non-English speakers  \n\n### 🔊 Voice Interaction (Beta)  \n- **Dynamic voice synthesis** via `KokoroTTS` (separate voices for characters and narrators)  \n- **Voice Mode**: Audio-only mode (currently English-only output)  \n\n---  \n\n## 🐭 Why MousyHub?  \n\n> *\"I wanted to create a convenient alternative to SillyTavern, focused solely on roleplaying.\"*  \n\n- **Simplified Interface**: Minimal complexity, maximum functionality  \n- **Everything You Need for Roleplay** without unnecessary extensions  \n- **Focus on Local Models**:  \n  - Privacy  \n  - Easy setup (especially for beginners)  \n  - Support for small RP-fine-tuned models  \n\n---  \n\n## 📋 Character Cards  \n- **Import** from Chub AI (support for other providers planned)  \n\n---  \n\n## 🛠️ Technical Details  \n\n### 🔧 Tech Stack  \n- **Backend**: C# Blazor Server (.NET 8)  \n- **Core Libraries**:  \n  - MudBlazor (UI)  \n  - LLamaSharp (native model execution)  \n  - KokoroTTS (voice synthesis)  \n\n### 🐞 Known Issues  \n- The app is under active development  \n- Bugs and areas needing refactoring may exist  \n- Not all cloud APIs have been fully tested  \n\n---  \n\n## 🤝 How to Contribute  \n\nWe welcome:  \n- Bug reports  \n- Improvement ideas  \n- Pull requests  \n- Testing assistance  \n\nHelp from experienced developers is especially appreciated!  \n\n---  \n\n## 🚀 Future Plans  \n- [ ] Adding World Info (similar to SillyTavern)  \n- [ ] Support for more character card providers  \n- [ ] Voice Mode improvements:  \n  - Multilingual output  \n  - Reduced latency  \n- [ ] Extended cloud API testing  \n\n---  \n\n*MousyHub is made with love for the RP community and local AI models!* 🐭❤️\n"
    }
  ],
  "total_dependents_number": 601,
  "public_dependents_number": 601,
  "private_dependents_number": -601,
  "public_dependents_stars": 108036,
  "badges": {
    "total_doc_url": "[![Generated by github-dependents-info](https://img.shields.io/static/v1?label=Used%20by&message=601&color=informational&logo=slickpic)](https://github.com/nvuillam/github-dependents-info)",
    "total": "[![Generated by github-dependents-info](https://img.shields.io/static/v1?label=Used%20by&message=601&color=informational&logo=slickpic)](https://github.com/microsoft/autogen/network/dependents)",
    "public": "[![Generated by github-dependents-info](https://img.shields.io/static/v1?label=Used%20by%20(public)&message=601&color=informational&logo=slickpic)](https://github.com/microsoft/autogen/network/dependents)",
    "private": "[![Generated by github-dependents-info](https://img.shields.io/static/v1?label=Used%20by%20(private)&message=-601&color=informational&logo=slickpic)](https://github.com/microsoft/autogen/network/dependents)",
    "stars": "[![Generated by github-dependents-info](https://img.shields.io/static/v1?label=Used%20by%20(stars)&message=108036&color=informational&logo=slickpic)](https://github.com/microsoft/autogen/network/dependents)"
  },
  "packages": [
    [
      {
        "id": "UGFja2FnZS01MzIzODkwMTQ4",
        "name": "autogen-ext",
        "url": "https://github.com/microsoft/autogen/network/dependents?package_id=UGFja2FnZS01MzIzODkwMTQ4",
        "public_dependent_stars": 44683,
        "public_dependents": [
          {
            "name": "microsoft/autogen",
            "stars": 43573,
            "img": "https://avatars.githubusercontent.com/u/6154722?s=40&v=4",
            "owner": "microsoft",
            "repo_name": "autogen"
          },
          {
            "name": "microsoft/ai-agents-for-beginners",
            "stars": 16610,
            "img": "https://avatars.githubusercontent.com/u/6154722?s=40&v=4",
            "owner": "microsoft",
            "repo_name": "ai-agents-for-beginners"
          },
          {
            "name": "panaversity/learn-agentic-ai",
            "stars": 1330,
            "img": "https://avatars.githubusercontent.com/u/98460328?s=40&v=4",
            "owner": "panaversity",
            "repo_name": "learn-agentic-ai"
          },
          {
            "name": "victordibia/autogen-ui",
            "stars": 905,
            "img": "https://avatars.githubusercontent.com/u/1547007?s=40&v=4",
            "owner": "victordibia",
            "repo_name": "autogen-ui"
          },
          {
            "name": "kagent-dev/kagent",
            "stars": 588,
            "img": "https://avatars.githubusercontent.com/u/195942545?s=40&v=4",
            "owner": "kagent-dev",
            "repo_name": "kagent"
          },
          {
            "name": "i-am-bee/beeai-platform",
            "stars": 440,
            "img": "https://avatars.githubusercontent.com/u/178592583?s=40&v=4",
            "owner": "i-am-bee",
            "repo_name": "beeai-platform"
          },
          {
            "name": "sambanova/ai-starter-kit",
            "stars": 220,
            "img": "https://avatars.githubusercontent.com/u/80118584?s=40&v=4",
            "owner": "sambanova",
            "repo_name": "ai-starter-kit"
          },
          {
            "name": "Azure-Samples/dream-team",
            "stars": 178,
            "img": "https://avatars.githubusercontent.com/u/1844662?s=40&v=4",
            "owner": "Azure-Samples",
            "repo_name": "dream-team"
          },
          {
            "name": "Azure-Samples/python-ai-agent-frameworks-demos",
            "stars": 125,
            "img": "https://avatars.githubusercontent.com/u/1844662?s=40&v=4",
            "owner": "Azure-Samples",
            "repo_name": "python-ai-agent-frameworks-demos"
          },
          {
            "name": "victordibia/multiagent-systems-with-autogen",
            "stars": 118,
            "img": "https://avatars.githubusercontent.com/u/1547007?s=40&v=4",
            "owner": "victordibia",
            "repo_name": "multiagent-systems-with-autogen"
          },
          {
            "name": "cyberark/agent-guard",
            "stars": 69,
            "img": "https://avatars.githubusercontent.com/u/30869256?s=40&v=4",
            "owner": "cyberark",
            "repo_name": "agent-guard"
          },
          {
            "name": "microsoft/dstoolkit-text2sql-and-imageprocessing",
            "stars": 67,
            "img": "https://avatars.githubusercontent.com/u/6154722?s=40&v=4",
            "owner": "microsoft",
            "repo_name": "dstoolkit-text2sql-and-imageprocessing"
          },
          {
            "name": "wanaku-ai/wanaku",
            "stars": 62,
            "img": "https://avatars.githubusercontent.com/u/200620763?s=40&v=4",
            "owner": "wanaku-ai",
            "repo_name": "wanaku"
          },
          {
            "name": "moxin-org/mofa",
            "stars": 57,
            "img": "https://avatars.githubusercontent.com/u/167464495?s=40&v=4",
            "owner": "moxin-org",
            "repo_name": "mofa"
          },
          {
            "name": "robertZaufall/mindmanager_ai",
            "stars": 52,
            "img": "https://avatars.githubusercontent.com/u/18199228?s=40&v=4",
            "owner": "robertZaufall",
            "repo_name": "mindmanager_ai"
          },
          {
            "name": "gnosis/prediction-market-agent",
            "stars": 50,
            "img": "https://avatars.githubusercontent.com/u/24954468?s=40&v=4",
            "owner": "gnosis",
            "repo_name": "prediction-market-agent"
          },
          {
            "name": "zhattention/ticktick-ai",
            "stars": 47,
            "img": "https://avatars.githubusercontent.com/u/7688914?s=40&v=4",
            "owner": "zhattention",
            "repo_name": "ticktick-ai"
          },
          {
            "name": "agntcy/csit",
            "stars": 47,
            "img": "https://avatars.githubusercontent.com/u/197140426?s=40&v=4",
            "owner": "agntcy",
            "repo_name": "csit"
          },
          {
            "name": "Azure/ai-foundry-workshop",
            "stars": 45,
            "img": "https://avatars.githubusercontent.com/u/6844498?s=40&v=4",
            "owner": "Azure",
            "repo_name": "ai-foundry-workshop"
          },
          {
            "name": "microsoft/agdebugger",
            "stars": 38,
            "img": "https://avatars.githubusercontent.com/u/6154722?s=40&v=4",
            "owner": "microsoft",
            "repo_name": "agdebugger"
          },
          {
            "name": "skyflo-ai/skyflo",
            "stars": 27,
            "img": "https://avatars.githubusercontent.com/u/198904896?s=40&v=4",
            "owner": "skyflo-ai",
            "repo_name": "skyflo"
          },
          {
            "name": "pablosalvador10/gbbai-agent-architecture-lab",
            "stars": 26,
            "img": "https://avatars.githubusercontent.com/u/31255154?s=40&v=4",
            "owner": "pablosalvador10",
            "repo_name": "gbbai-agent-architecture-lab"
          },
          {
            "name": "Wannabeasmartguy/RAGENT",
            "stars": 25,
            "img": "https://avatars.githubusercontent.com/u/107250451?s=40&v=4",
            "owner": "Wannabeasmartguy",
            "repo_name": "RAGENT"
          },
          {
            "name": "EnkrateiaLucca/oreilly_live_training_autogen",
            "stars": 25,
            "img": "https://avatars.githubusercontent.com/u/32417736?s=40&v=4",
            "owner": "EnkrateiaLucca",
            "repo_name": "oreilly_live_training_autogen"
          },
          {
            "name": "IBM/watsonx-developer-hub",
            "stars": 24,
            "img": "https://avatars.githubusercontent.com/u/1459110?s=40&v=4",
            "owner": "IBM",
            "repo_name": "watsonx-developer-hub"
          },
          {
            "name": "denniszielke/agentic-playground",
            "stars": 23,
            "img": "https://avatars.githubusercontent.com/u/11569044?s=40&v=4",
            "owner": "denniszielke",
            "repo_name": "agentic-playground"
          },
          {
            "name": "Azure/agent-innovator-lab",
            "stars": 23,
            "img": "https://avatars.githubusercontent.com/u/6844498?s=40&v=4",
            "owner": "Azure",
            "repo_name": "agent-innovator-lab"
          },
          {
            "name": "zhattention/autogen-toolsmith",
            "stars": 22,
            "img": "https://avatars.githubusercontent.com/u/7688914?s=40&v=4",
            "owner": "zhattention",
            "repo_name": "autogen-toolsmith"
          },
          {
            "name": "amaziahyahalom/aider-model-train",
            "stars": 20,
            "img": "https://avatars.githubusercontent.com/u/192240462?s=40&v=4",
            "owner": "amaziahyahalom",
            "repo_name": "aider-model-train"
          },
          {
            "name": "lordlinus/travel-chatbot",
            "stars": 20,
            "img": "https://avatars.githubusercontent.com/u/2416507?s=40&v=4",
            "owner": "lordlinus",
            "repo_name": "travel-chatbot"
          },
          {
            "name": "LxYuan0420/nlp",
            "stars": 19,
            "img": "https://avatars.githubusercontent.com/u/25739658?s=40&v=4",
            "owner": "LxYuan0420",
            "repo_name": "nlp"
          },
          {
            "name": "gxjorge/AI-Agents-Lab",
            "stars": 17,
            "img": "https://avatars.githubusercontent.com/u/3693420?s=40&v=4",
            "owner": "gxjorge",
            "repo_name": "AI-Agents-Lab"
          },
          {
            "name": "modular/max-recipes",
            "stars": 16,
            "img": "https://avatars.githubusercontent.com/u/150859076?s=40&v=4",
            "owner": "modular",
            "repo_name": "max-recipes"
          },
          {
            "name": "vballoli/autogen-openaiext-client",
            "stars": 16,
            "img": "https://avatars.githubusercontent.com/u/26684166?s=40&v=4",
            "owner": "vballoli",
            "repo_name": "autogen-openaiext-client"
          },
          {
            "name": "Azure-Samples/multi-agent-workshop",
            "stars": 15,
            "img": "https://avatars.githubusercontent.com/u/1844662?s=40&v=4",
            "owner": "Azure-Samples",
            "repo_name": "multi-agent-workshop"
          },
          {
            "name": "SongChiYoung/autogen-oaiapi",
            "stars": 14,
            "img": "https://avatars.githubusercontent.com/u/7540984?s=40&v=4",
            "owner": "SongChiYoung",
            "repo_name": "autogen-oaiapi"
          },
          {
            "name": "msalemor/llm-use-cases",
            "stars": 14,
            "img": "https://avatars.githubusercontent.com/u/44649358?s=40&v=4",
            "owner": "msalemor",
            "repo_name": "llm-use-cases"
          },
          {
            "name": "akshata29/finagent",
            "stars": 13,
            "img": "https://avatars.githubusercontent.com/u/18509807?s=40&v=4",
            "owner": "akshata29",
            "repo_name": "finagent"
          },
          {
            "name": "fbellame/agent-comparison",
            "stars": 11,
            "img": "https://avatars.githubusercontent.com/u/29183055?s=40&v=4",
            "owner": "fbellame",
            "repo_name": "agent-comparison"
          },
          {
            "name": "thirdweb-dev/ai",
            "stars": 10,
            "img": "https://avatars.githubusercontent.com/u/79496167?s=40&v=4",
            "owner": "thirdweb-dev",
            "repo_name": "ai"
          },
          {
            "name": "tezansahu/ai-garage",
            "stars": 9,
            "img": "https://avatars.githubusercontent.com/u/31898274?s=40&v=4",
            "owner": "tezansahu",
            "repo_name": "ai-garage"
          },
          {
            "name": "GridfireAI/reddit-mcp",
            "stars": 8,
            "img": "https://avatars.githubusercontent.com/u/204763572?s=40&v=4",
            "owner": "GridfireAI",
            "repo_name": "reddit-mcp"
          },
          {
            "name": "DonnC/pywce",
            "stars": 8,
            "img": "https://avatars.githubusercontent.com/u/47761288?s=40&v=4",
            "owner": "DonnC",
            "repo_name": "pywce"
          },
          {
            "name": "detoxio-ai/ai-red-teaming-training",
            "stars": 8,
            "img": "https://avatars.githubusercontent.com/u/165997948?s=40&v=4",
            "owner": "detoxio-ai",
            "repo_name": "ai-red-teaming-training"
          },
          {
            "name": "Shailender-Youtube/Stock-Analysis-AutoGen-Multi-Agent",
            "stars": 8,
            "img": "https://avatars.githubusercontent.com/u/174622601?s=40&v=4",
            "owner": "Shailender-Youtube",
            "repo_name": "Stock-Analysis-AutoGen-Multi-Agent"
          },
          {
            "name": "MSFT-Innovation-Hub-India/Multi-Agent-autogen-Report-Gen",
            "stars": 8,
            "img": "https://avatars.githubusercontent.com/u/63641889?s=40&v=4",
            "owner": "MSFT-Innovation-Hub-India",
            "repo_name": "Multi-Agent-autogen-Report-Gen"
          },
          {
            "name": "tsinggggg/autogen-watsonx-client",
            "stars": 8,
            "img": "https://avatars.githubusercontent.com/u/20978879?s=40&v=4",
            "owner": "tsinggggg",
            "repo_name": "autogen-watsonx-client"
          },
          {
            "name": "foundation-models/MediNoteAI",
            "stars": 7,
            "img": "https://avatars.githubusercontent.com/u/1612939?s=40&v=4",
            "owner": "foundation-models",
            "repo_name": "MediNoteAI"
          },
          {
            "name": "dasiths/llm-plan-and-execute-knowledge-provider-mesh",
            "stars": 7,
            "img": "https://avatars.githubusercontent.com/u/16128850?s=40&v=4",
            "owner": "dasiths",
            "repo_name": "llm-plan-and-execute-knowledge-provider-mesh"
          },
          {
            "name": "jennifermarsman/MultiagentResearch",
            "stars": 7,
            "img": "https://avatars.githubusercontent.com/u/10062762?s=40&v=4",
            "owner": "jennifermarsman",
            "repo_name": "MultiagentResearch"
          },
          {
            "name": "KnoWhiz/DeepTutor",
            "stars": 7,
            "img": "https://avatars.githubusercontent.com/u/112668778?s=40&v=4",
            "owner": "KnoWhiz",
            "repo_name": "DeepTutor"
          },
          {
            "name": "guangying94/magentic-one-autogen-sample",
            "stars": 7,
            "img": "https://avatars.githubusercontent.com/u/12764889?s=40&v=4",
            "owner": "guangying94",
            "repo_name": "magentic-one-autogen-sample"
          },
          {
            "name": "kimtth/azure-openai-llm-cookbook",
            "stars": 6,
            "img": "https://avatars.githubusercontent.com/u/13846660?s=40&v=4",
            "owner": "kimtth",
            "repo_name": "azure-openai-llm-cookbook"
          },
          {
            "name": "gswithjeff/autogen-multi-agent-workflow",
            "stars": 6,
            "img": "https://avatars.githubusercontent.com/u/135394694?s=40&v=4",
            "owner": "gswithjeff",
            "repo_name": "autogen-multi-agent-workflow"
          },
          {
            "name": "ziaukhan/shopping-agent",
            "stars": 6,
            "img": "https://avatars.githubusercontent.com/u/545458?s=40&v=4",
            "owner": "ziaukhan",
            "repo_name": "shopping-agent"
          },
          {
            "name": "CantCode023/evoke",
            "stars": 6,
            "img": "https://avatars.githubusercontent.com/u/95113225?s=40&v=4",
            "owner": "CantCode023",
            "repo_name": "evoke"
          },
          {
            "name": "NVISOsecurity/sans-webinar-robocop",
            "stars": 5,
            "img": "https://avatars.githubusercontent.com/u/5310436?s=40&v=4",
            "owner": "NVISOsecurity",
            "repo_name": "sans-webinar-robocop"
          },
          {
            "name": "jplck/from-single-to-multi-agent",
            "stars": 5,
            "img": "https://avatars.githubusercontent.com/u/8427174?s=40&v=4",
            "owner": "jplck",
            "repo_name": "from-single-to-multi-agent"
          },
          {
            "name": "lukaskellerstein/ai",
            "stars": 5,
            "img": "https://avatars.githubusercontent.com/u/12882519?s=40&v=4",
            "owner": "lukaskellerstein",
            "repo_name": "ai"
          },
          {
            "name": "Shailender-Youtube/Autogen-0.4-Mini-Project",
            "stars": 5,
            "img": "https://avatars.githubusercontent.com/u/174622601?s=40&v=4",
            "owner": "Shailender-Youtube",
            "repo_name": "Autogen-0.4-Mini-Project"
          },
          {
            "name": "Squidgy-AI/BoilerPlateV1",
            "stars": 4,
            "img": "https://avatars.githubusercontent.com/u/188782334?s=40&v=4",
            "owner": "Squidgy-AI",
            "repo_name": "BoilerPlateV1"
          },
          {
            "name": "minyang-chen/seat-booking-agent",
            "stars": 4,
            "img": "https://avatars.githubusercontent.com/u/3905747?s=40&v=4",
            "owner": "minyang-chen",
            "repo_name": "seat-booking-agent"
          },
          {
            "name": "aniket-work/Lets-Build-Enterprise-Cybersecurity-Risk-Assessment-Using-AI-Agents",
            "stars": 4,
            "img": "https://avatars.githubusercontent.com/u/59799105?s=40&v=4",
            "owner": "aniket-work",
            "repo_name": "Lets-Build-Enterprise-Cybersecurity-Risk-Assessment-Using-AI-Agents"
          },
          {
            "name": "michalmar/dream-team-v2",
            "stars": 4,
            "img": "https://avatars.githubusercontent.com/u/16072588?s=40&v=4",
            "owner": "michalmar",
            "repo_name": "dream-team-v2"
          },
          {
            "name": "santicalavia/TabooGPT",
            "stars": 4,
            "img": "https://avatars.githubusercontent.com/u/82812961?s=40&v=4",
            "owner": "santicalavia",
            "repo_name": "TabooGPT"
          },
          {
            "name": "guyq1997/Jupyter-Assistant",
            "stars": 4,
            "img": "https://avatars.githubusercontent.com/u/111009031?s=40&v=4",
            "owner": "guyq1997",
            "repo_name": "Jupyter-Assistant"
          },
          {
            "name": "realtmxi/mag-review",
            "stars": 3,
            "img": "https://avatars.githubusercontent.com/u/42812404?s=40&v=4",
            "owner": "realtmxi",
            "repo_name": "mag-review"
          },
          {
            "name": "Azure-Samples/AzureAIFoundryWorkshop",
            "stars": 3,
            "img": "https://avatars.githubusercontent.com/u/1844662?s=40&v=4",
            "owner": "Azure-Samples",
            "repo_name": "AzureAIFoundryWorkshop"
          },
          {
            "name": "epoikos-project/simulation",
            "stars": 3,
            "img": "https://avatars.githubusercontent.com/u/200770314?s=40&v=4",
            "owner": "epoikos-project",
            "repo_name": "simulation"
          },
          {
            "name": "mdsiprojects/agenticai",
            "stars": 3,
            "img": "https://avatars.githubusercontent.com/u/73212683?s=40&v=4",
            "owner": "mdsiprojects",
            "repo_name": "agenticai"
          },
          {
            "name": "DoctorKhan/quantum-resonance-ledger",
            "stars": 3,
            "img": "https://avatars.githubusercontent.com/u/7505793?s=40&v=4",
            "owner": "DoctorKhan",
            "repo_name": "quantum-resonance-ledger"
          },
          {
            "name": "yusufk/jarvis-azure",
            "stars": 3,
            "img": "https://avatars.githubusercontent.com/u/404139?s=40&v=4",
            "owner": "yusufk",
            "repo_name": "jarvis-azure"
          },
          {
            "name": "sambanova/integrations",
            "stars": 3,
            "img": "https://avatars.githubusercontent.com/u/80118584?s=40&v=4",
            "owner": "sambanova",
            "repo_name": "integrations"
          },
          {
            "name": "Insight-Services-APAC/Insight_Ingenious",
            "stars": 3,
            "img": "https://avatars.githubusercontent.com/u/75322600?s=40&v=4",
            "owner": "Insight-Services-APAC",
            "repo_name": "Insight_Ingenious"
          },
          {
            "name": "Pykoncio/MentorAI",
            "stars": 3,
            "img": "https://avatars.githubusercontent.com/u/83032163?s=40&v=4",
            "owner": "Pykoncio",
            "repo_name": "MentorAI"
          },
          {
            "name": "MichalRogowski/CVAgent",
            "stars": 3,
            "img": "https://avatars.githubusercontent.com/u/47182107?s=40&v=4",
            "owner": "MichalRogowski",
            "repo_name": "CVAgent"
          },
          {
            "name": "alejbormeg/AutoGen_Swarm",
            "stars": 3,
            "img": "https://avatars.githubusercontent.com/u/61965913?s=40&v=4",
            "owner": "alejbormeg",
            "repo_name": "AutoGen_Swarm"
          },
          {
            "name": "denniszielke/ai-agents-workshop",
            "stars": 3,
            "img": "https://avatars.githubusercontent.com/u/11569044?s=40&v=4",
            "owner": "denniszielke",
            "repo_name": "ai-agents-workshop"
          },
          {
            "name": "huqianghui/azure-ai-agent-workshop",
            "stars": 3,
            "img": "https://avatars.githubusercontent.com/u/7360524?s=40&v=4",
            "owner": "huqianghui",
            "repo_name": "azure-ai-agent-workshop"
          },
          {
            "name": "strand1/ComfyUI-Autogen",
            "stars": 3,
            "img": "https://avatars.githubusercontent.com/u/1580273?s=40&v=4",
            "owner": "strand1",
            "repo_name": "ComfyUI-Autogen"
          },
          {
            "name": "Azure/auto-kyc",
            "stars": 3,
            "img": "https://avatars.githubusercontent.com/u/6844498?s=40&v=4",
            "owner": "Azure",
            "repo_name": "auto-kyc"
          },
          {
            "name": "alphavector/all",
            "stars": 3,
            "img": "https://avatars.githubusercontent.com/u/11805788?s=40&v=4",
            "owner": "alphavector",
            "repo_name": "all"
          },
          {
            "name": "DeepSpringAI/search_mcp_server",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/200253356?s=40&v=4",
            "owner": "DeepSpringAI",
            "repo_name": "search_mcp_server"
          },
          {
            "name": "A-Ravioli/einstein",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/68564296?s=40&v=4",
            "owner": "A-Ravioli",
            "repo_name": "einstein"
          },
          {
            "name": "pasangimhana/sql_agent",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/85233319?s=40&v=4",
            "owner": "pasangimhana",
            "repo_name": "sql_agent"
          },
          {
            "name": "arthur-ai/arthur-autogen-agentic-demo",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/62756214?s=40&v=4",
            "owner": "arthur-ai",
            "repo_name": "arthur-autogen-agentic-demo"
          },
          {
            "name": "shigeyf/azure-ai-foundry-workshop",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/6917167?s=40&v=4",
            "owner": "shigeyf",
            "repo_name": "azure-ai-foundry-workshop"
          },
          {
            "name": "HosseinZahed/multi-agent-cloud-architect",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/19933353?s=40&v=4",
            "owner": "HosseinZahed",
            "repo_name": "multi-agent-cloud-architect"
          },
          {
            "name": "jjgriff93/autowolf",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/25235950?s=40&v=4",
            "owner": "jjgriff93",
            "repo_name": "autowolf"
          },
          {
            "name": "FrancoisChastel/OpenCustomsGPT",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/10299319?s=40&v=4",
            "owner": "FrancoisChastel",
            "repo_name": "OpenCustomsGPT"
          },
          {
            "name": "Lailiangken/collm04",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/52484757?s=40&v=4",
            "owner": "Lailiangken",
            "repo_name": "collm04"
          },
          {
            "name": "jplck/generic-ai-samples",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/8427174?s=40&v=4",
            "owner": "jplck",
            "repo_name": "generic-ai-samples"
          },
          {
            "name": "trentkim-ms-ats/multi-agent-workshop",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/197431125?s=40&v=4",
            "owner": "trentkim-ms-ats",
            "repo_name": "multi-agent-workshop"
          },
          {
            "name": "marcelmedina/ai-autogen-agent-patterns",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/17131300?s=40&v=4",
            "owner": "marcelmedina",
            "repo_name": "ai-autogen-agent-patterns"
          },
          {
            "name": "amitlals/AI-Financial-Engineering-Agentic-System",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/37605691?s=40&v=4",
            "owner": "amitlals",
            "repo_name": "AI-Financial-Engineering-Agentic-System"
          },
          {
            "name": "kantariyaraj/AI_Agent_Examples",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/6614811?s=40&v=4",
            "owner": "kantariyaraj",
            "repo_name": "AI_Agent_Examples"
          },
          {
            "name": "trojrobert/web_agent",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/28733978?s=40&v=4",
            "owner": "trojrobert",
            "repo_name": "web_agent"
          },
          {
            "name": "weinaike/agentflow",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/13814354?s=40&v=4",
            "owner": "weinaike",
            "repo_name": "agentflow"
          },
          {
            "name": "dhanaabhirajk/logical_agent",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/81916146?s=40&v=4",
            "owner": "dhanaabhirajk",
            "repo_name": "logical_agent"
          },
          {
            "name": "CellCS/AI-Agents-RnD",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/16886624?s=40&v=4",
            "owner": "CellCS",
            "repo_name": "AI-Agents-RnD"
          },
          {
            "name": "jspv/mchat",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/6540751?s=40&v=4",
            "owner": "jspv",
            "repo_name": "mchat"
          },
          {
            "name": "StefanDevstar/awesome-llm-apps",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/202806902?s=40&v=4",
            "owner": "StefanDevstar",
            "repo_name": "awesome-llm-apps"
          },
          {
            "name": "fajarmh-mf/ai-agents-frameworks",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/186990157?s=40&v=4",
            "owner": "fajarmh-mf",
            "repo_name": "ai-agents-frameworks"
          },
          {
            "name": "saytyarnorngloreia/ai-agents-for-beginners",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/205020882?s=40&v=4",
            "owner": "saytyarnorngloreia",
            "repo_name": "ai-agents-for-beginners"
          },
          {
            "name": "santhoshrao95/mulit_agent_it_support_system",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/27264958?s=40&v=4",
            "owner": "santhoshrao95",
            "repo_name": "mulit_agent_it_support_system"
          },
          {
            "name": "IamMikeHelsel/sneks",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/51098787?s=40&v=4",
            "owner": "IamMikeHelsel",
            "repo_name": "sneks"
          },
          {
            "name": "NortonHuang-SUFE/FortuneTell",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/54016596?s=40&v=4",
            "owner": "NortonHuang-SUFE",
            "repo_name": "FortuneTell"
          },
          {
            "name": "CyberCRI/WeLearn-api",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/3897500?s=40&v=4",
            "owner": "CyberCRI",
            "repo_name": "WeLearn-api"
          },
          {
            "name": "msalemor/agentic-intro-autogen",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/44649358?s=40&v=4",
            "owner": "msalemor",
            "repo_name": "agentic-intro-autogen"
          },
          {
            "name": "micchyboy237/JetScripts",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/42490977?s=40&v=4",
            "owner": "micchyboy237",
            "repo_name": "JetScripts"
          },
          {
            "name": "AskTheApi/asktheapi-team-builder",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/206834780?s=40&v=4",
            "owner": "AskTheApi",
            "repo_name": "asktheapi-team-builder"
          },
          {
            "name": "DellDi/autogen-fastapi-postgres",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/40460351?s=40&v=4",
            "owner": "DellDi",
            "repo_name": "autogen-fastapi-postgres"
          },
          {
            "name": "hinthornw/mabench",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/13333726?s=40&v=4",
            "owner": "hinthornw",
            "repo_name": "mabench"
          },
          {
            "name": "chepyle/multiagent-demo",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/6445835?s=40&v=4",
            "owner": "chepyle",
            "repo_name": "multiagent-demo"
          },
          {
            "name": "sitloboi2012/AMS",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/33688887?s=40&v=4",
            "owner": "sitloboi2012",
            "repo_name": "AMS"
          },
          {
            "name": "black-da-bull/autogen",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/110015660?s=40&v=4",
            "owner": "black-da-bull",
            "repo_name": "autogen"
          },
          {
            "name": "isaccanedo/awesome-llm-apps",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/32867124?s=40&v=4",
            "owner": "isaccanedo",
            "repo_name": "awesome-llm-apps"
          },
          {
            "name": "kohei3110/ai-tour-2025-agent-demo",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/38392404?s=40&v=4",
            "owner": "kohei3110",
            "repo_name": "ai-tour-2025-agent-demo"
          },
          {
            "name": "verwec/03_2025_ai_workshop",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/6060259?s=40&v=4",
            "owner": "verwec",
            "repo_name": "03_2025_ai_workshop"
          },
          {
            "name": "vortezwohl/experiment-03-22-2025",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/117743023?s=40&v=4",
            "owner": "vortezwohl",
            "repo_name": "experiment-03-22-2025"
          },
          {
            "name": "KamikazEr101/GoodByeSqlBoy",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/154207461?s=40&v=4",
            "owner": "KamikazEr101",
            "repo_name": "GoodByeSqlBoy"
          },
          {
            "name": "memenow/persona-agent",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/142292623?s=40&v=4",
            "owner": "memenow",
            "repo_name": "persona-agent"
          },
          {
            "name": "dheerapat/autogen-agent-demo",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/61280196?s=40&v=4",
            "owner": "dheerapat",
            "repo_name": "autogen-agent-demo"
          },
          {
            "name": "Azure/multiagent-frameworks",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/6844498?s=40&v=4",
            "owner": "Azure",
            "repo_name": "multiagent-frameworks"
          },
          {
            "name": "FranklineMisango/Quant_Research_Assistant",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/95913228?s=40&v=4",
            "owner": "FranklineMisango",
            "repo_name": "Quant_Research_Assistant"
          },
          {
            "name": "flosrv/Awesome-LLM-Apps-Shubham-Saboo",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/139999976?s=40&v=4",
            "owner": "flosrv",
            "repo_name": "Awesome-LLM-Apps-Shubham-Saboo"
          },
          {
            "name": "knd3dayo/AIChatExplorer",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/157713558?s=40&v=4",
            "owner": "knd3dayo",
            "repo_name": "AIChatExplorer"
          },
          {
            "name": "RostKoff/montage-horde",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/119884204?s=40&v=4",
            "owner": "RostKoff",
            "repo_name": "montage-horde"
          },
          {
            "name": "Delahayecarry/MAAI",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/163613131?s=40&v=4",
            "owner": "Delahayecarry",
            "repo_name": "MAAI"
          },
          {
            "name": "rajesh-nitc/autogen-backend",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/44093301?s=40&v=4",
            "owner": "rajesh-nitc",
            "repo_name": "autogen-backend"
          },
          {
            "name": "marcelmedina/ai-autogen-agents",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/17131300?s=40&v=4",
            "owner": "marcelmedina",
            "repo_name": "ai-autogen-agents"
          },
          {
            "name": "XinmiaoYan/ai-agents-for-beginners",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/83053884?s=40&v=4",
            "owner": "XinmiaoYan",
            "repo_name": "ai-agents-for-beginners"
          },
          {
            "name": "arindamhazramsft/AZ-AI-Agent-Service-Demos",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/123211674?s=40&v=4",
            "owner": "arindamhazramsft",
            "repo_name": "AZ-AI-Agent-Service-Demos"
          },
          {
            "name": "ArthurVerrez/autogen-streamlit-hackernews-demo",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/33450143?s=40&v=4",
            "owner": "ArthurVerrez",
            "repo_name": "autogen-streamlit-hackernews-demo"
          },
          {
            "name": "pmajor74/demo.autogen_meal_blogwriter",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/52909235?s=40&v=4",
            "owner": "pmajor74",
            "repo_name": "demo.autogen_meal_blogwriter"
          },
          {
            "name": "luiselopezlopez/globalai2025",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/97617267?s=40&v=4",
            "owner": "luiselopezlopez",
            "repo_name": "globalai2025"
          },
          {
            "name": "fuxin123z/autogen_mcp",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/60996067?s=40&v=4",
            "owner": "fuxin123z",
            "repo_name": "autogen_mcp"
          },
          {
            "name": "CantCode023/bloggit",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/95113225?s=40&v=4",
            "owner": "CantCode023",
            "repo_name": "bloggit"
          },
          {
            "name": "prabdeb/autogen-m1-security-sample",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/37529789?s=40&v=4",
            "owner": "prabdeb",
            "repo_name": "autogen-m1-security-sample"
          },
          {
            "name": "cwijayasundara/multi_agentic_research",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/3097200?s=40&v=4",
            "owner": "cwijayasundara",
            "repo_name": "multi_agentic_research"
          },
          {
            "name": "wwwsapir/SoftwareAgents",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/16436675?s=40&v=4",
            "owner": "wwwsapir",
            "repo_name": "SoftwareAgents"
          },
          {
            "name": "tejajuttu/Multi-Agent-Conversations",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/25482892?s=40&v=4",
            "owner": "tejajuttu",
            "repo_name": "Multi-Agent-Conversations"
          },
          {
            "name": "AliciaSJF/autogen",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/132434651?s=40&v=4",
            "owner": "AliciaSJF",
            "repo_name": "autogen"
          },
          {
            "name": "mark-watson/OllamaExamples",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/33912?s=40&v=4",
            "owner": "mark-watson",
            "repo_name": "OllamaExamples"
          },
          {
            "name": "kenilp7/Meal-Planning-AI-Assistants",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/108230926?s=40&v=4",
            "owner": "kenilp7",
            "repo_name": "Meal-Planning-AI-Assistants"
          },
          {
            "name": "yikedu7/poc",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/25717773?s=40&v=4",
            "owner": "yikedu7",
            "repo_name": "poc"
          },
          {
            "name": "rustatian/airope",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/8040338?s=40&v=4",
            "owner": "rustatian",
            "repo_name": "airope"
          },
          {
            "name": "codeh007/mtmai",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/95907738?s=40&v=4",
            "owner": "codeh007",
            "repo_name": "mtmai"
          },
          {
            "name": "lgcorzo/llmops-python-package",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/46710567?s=40&v=4",
            "owner": "lgcorzo",
            "repo_name": "llmops-python-package"
          },
          {
            "name": "thedigitalworkplace/Autogen",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/133930737?s=40&v=4",
            "owner": "thedigitalworkplace",
            "repo_name": "Autogen"
          },
          {
            "name": "DimohaZadira/ai-agent-start",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/33756112?s=40&v=4",
            "owner": "DimohaZadira",
            "repo_name": "ai-agent-start"
          },
          {
            "name": "halilural/multi-model-ai-agents",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/12976755?s=40&v=4",
            "owner": "halilural",
            "repo_name": "multi-model-ai-agents"
          },
          {
            "name": "rinormaloku/autogen-langchain-mcp-mix",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/9026451?s=40&v=4",
            "owner": "rinormaloku",
            "repo_name": "autogen-langchain-mcp-mix"
          },
          {
            "name": "vvalac/ai-experiments",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/48455003?s=40&v=4",
            "owner": "vvalac",
            "repo_name": "ai-experiments"
          },
          {
            "name": "ghchen99/agentic-conversations",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/56446026?s=40&v=4",
            "owner": "ghchen99",
            "repo_name": "agentic-conversations"
          },
          {
            "name": "vccjlc/time_machine",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/140386750?s=40&v=4",
            "owner": "vccjlc",
            "repo_name": "time_machine"
          },
          {
            "name": "r-bit-rry/magentic-two",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/34023431?s=40&v=4",
            "owner": "r-bit-rry",
            "repo_name": "magentic-two"
          },
          {
            "name": "1012638836/ai_qiniu_chatbot",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/29818043?s=40&v=4",
            "owner": "1012638836",
            "repo_name": "ai_qiniu_chatbot"
          },
          {
            "name": "shreyanavelkar/automated-ai-assistant",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/28059200?s=40&v=4",
            "owner": "shreyanavelkar",
            "repo_name": "automated-ai-assistant"
          },
          {
            "name": "richard-gyiko/aura",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/36221820?s=40&v=4",
            "owner": "richard-gyiko",
            "repo_name": "aura"
          },
          {
            "name": "mtliou/BioPharma451",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/164873773?s=40&v=4",
            "owner": "mtliou",
            "repo_name": "BioPharma451"
          },
          {
            "name": "selafmai/Magenitc-one-AGENTIC-InsurTech",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/157835927?s=40&v=4",
            "owner": "selafmai",
            "repo_name": "Magenitc-one-AGENTIC-InsurTech"
          },
          {
            "name": "denniszielke/autogen-playground",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/11569044?s=40&v=4",
            "owner": "denniszielke",
            "repo_name": "autogen-playground"
          },
          {
            "name": "eugeneVzaidun/autogen",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/72197489?s=40&v=4",
            "owner": "eugeneVzaidun",
            "repo_name": "autogen"
          },
          {
            "name": "CodeDevKyle/autogen_DevKyle",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/167522880?s=40&v=4",
            "owner": "CodeDevKyle",
            "repo_name": "autogen_DevKyle"
          }
        ],
        "public_dependents_number": 165,
        "private_dependents_number": -165,
        "total_dependents_number": 165,
        "badges": {
          "total": "[![Generated by github-dependents-info](https://img.shields.io/static/v1?label=Used%20by&message=165&color=informational&logo=slickpic)](https://github.com/microsoft/autogen/network/dependents?package_id=UGFja2FnZS01MzIzODkwMTQ4)",
          "public": "[![Generated by github-dependents-info](https://img.shields.io/static/v1?label=Used%20by%20(public)&message=165&color=informational&logo=slickpic)](https://github.com/microsoft/autogen/network/dependents?package_id=UGFja2FnZS01MzIzODkwMTQ4)",
          "private": "[![Generated by github-dependents-info](https://img.shields.io/static/v1?label=Used%20by%20(private)&message=-165&color=informational&logo=slickpic)](https://github.com/microsoft/autogen/network/dependents?package_id=UGFja2FnZS01MzIzODkwMTQ4)",
          "stars": "[![Generated by github-dependents-info](https://img.shields.io/static/v1?label=Used%20by%20(stars)&message=44683&color=informational&logo=slickpic)](https://github.com/microsoft/autogen/network/dependents?package_id=UGFja2FnZS01MzIzODkwMTQ4)"
        }
      },
      {
        "id": "UGFja2FnZS01MzAwMTAxNDU5",
        "name": "autogen-core",
        "url": "https://github.com/microsoft/autogen/network/dependents?package_id=UGFja2FnZS01MzAwMTAxNDU5",
        "public_dependent_stars": 43611,
        "public_dependents": [
          {
            "name": "microsoft/autogen",
            "stars": 43573,
            "img": "https://avatars.githubusercontent.com/u/6154722?s=40&v=4",
            "owner": "microsoft",
            "repo_name": "autogen"
          },
          {
            "name": "microsoft/ai-agents-for-beginners",
            "stars": 16610,
            "img": "https://avatars.githubusercontent.com/u/6154722?s=40&v=4",
            "owner": "microsoft",
            "repo_name": "ai-agents-for-beginners"
          },
          {
            "name": "unitycatalog/unitycatalog",
            "stars": 2806,
            "img": "https://avatars.githubusercontent.com/u/171874451?s=40&v=4",
            "owner": "unitycatalog",
            "repo_name": "unitycatalog"
          },
          {
            "name": "kagent-dev/kagent",
            "stars": 588,
            "img": "https://avatars.githubusercontent.com/u/195942545?s=40&v=4",
            "owner": "kagent-dev",
            "repo_name": "kagent"
          },
          {
            "name": "sambanova/ai-starter-kit",
            "stars": 220,
            "img": "https://avatars.githubusercontent.com/u/80118584?s=40&v=4",
            "owner": "sambanova",
            "repo_name": "ai-starter-kit"
          },
          {
            "name": "Azure-Samples/dream-team",
            "stars": 178,
            "img": "https://avatars.githubusercontent.com/u/1844662?s=40&v=4",
            "owner": "Azure-Samples",
            "repo_name": "dream-team"
          },
          {
            "name": "victordibia/multiagent-systems-with-autogen",
            "stars": 118,
            "img": "https://avatars.githubusercontent.com/u/1547007?s=40&v=4",
            "owner": "victordibia",
            "repo_name": "multiagent-systems-with-autogen"
          },
          {
            "name": "cyberark/agent-guard",
            "stars": 69,
            "img": "https://avatars.githubusercontent.com/u/30869256?s=40&v=4",
            "owner": "cyberark",
            "repo_name": "agent-guard"
          },
          {
            "name": "microsoft/dstoolkit-text2sql-and-imageprocessing",
            "stars": 67,
            "img": "https://avatars.githubusercontent.com/u/6154722?s=40&v=4",
            "owner": "microsoft",
            "repo_name": "dstoolkit-text2sql-and-imageprocessing"
          },
          {
            "name": "wanaku-ai/wanaku",
            "stars": 62,
            "img": "https://avatars.githubusercontent.com/u/200620763?s=40&v=4",
            "owner": "wanaku-ai",
            "repo_name": "wanaku"
          },
          {
            "name": "robertZaufall/mindmanager_ai",
            "stars": 52,
            "img": "https://avatars.githubusercontent.com/u/18199228?s=40&v=4",
            "owner": "robertZaufall",
            "repo_name": "mindmanager_ai"
          },
          {
            "name": "gnosis/prediction-market-agent",
            "stars": 50,
            "img": "https://avatars.githubusercontent.com/u/24954468?s=40&v=4",
            "owner": "gnosis",
            "repo_name": "prediction-market-agent"
          },
          {
            "name": "zhattention/ticktick-ai",
            "stars": 47,
            "img": "https://avatars.githubusercontent.com/u/7688914?s=40&v=4",
            "owner": "zhattention",
            "repo_name": "ticktick-ai"
          },
          {
            "name": "agntcy/csit",
            "stars": 47,
            "img": "https://avatars.githubusercontent.com/u/197140426?s=40&v=4",
            "owner": "agntcy",
            "repo_name": "csit"
          },
          {
            "name": "skyflo-ai/skyflo",
            "stars": 27,
            "img": "https://avatars.githubusercontent.com/u/198904896?s=40&v=4",
            "owner": "skyflo-ai",
            "repo_name": "skyflo"
          },
          {
            "name": "pablosalvador10/gbbai-agent-architecture-lab",
            "stars": 26,
            "img": "https://avatars.githubusercontent.com/u/31255154?s=40&v=4",
            "owner": "pablosalvador10",
            "repo_name": "gbbai-agent-architecture-lab"
          },
          {
            "name": "EnkrateiaLucca/oreilly_live_training_autogen",
            "stars": 25,
            "img": "https://avatars.githubusercontent.com/u/32417736?s=40&v=4",
            "owner": "EnkrateiaLucca",
            "repo_name": "oreilly_live_training_autogen"
          },
          {
            "name": "IBM/watsonx-developer-hub",
            "stars": 24,
            "img": "https://avatars.githubusercontent.com/u/1459110?s=40&v=4",
            "owner": "IBM",
            "repo_name": "watsonx-developer-hub"
          },
          {
            "name": "Azure/agent-innovator-lab",
            "stars": 23,
            "img": "https://avatars.githubusercontent.com/u/6844498?s=40&v=4",
            "owner": "Azure",
            "repo_name": "agent-innovator-lab"
          },
          {
            "name": "zhattention/autogen-toolsmith",
            "stars": 22,
            "img": "https://avatars.githubusercontent.com/u/7688914?s=40&v=4",
            "owner": "zhattention",
            "repo_name": "autogen-toolsmith"
          },
          {
            "name": "amaziahyahalom/aider-model-train",
            "stars": 20,
            "img": "https://avatars.githubusercontent.com/u/192240462?s=40&v=4",
            "owner": "amaziahyahalom",
            "repo_name": "aider-model-train"
          },
          {
            "name": "lordlinus/travel-chatbot",
            "stars": 20,
            "img": "https://avatars.githubusercontent.com/u/2416507?s=40&v=4",
            "owner": "lordlinus",
            "repo_name": "travel-chatbot"
          },
          {
            "name": "gxjorge/AI-Agents-Lab",
            "stars": 17,
            "img": "https://avatars.githubusercontent.com/u/3693420?s=40&v=4",
            "owner": "gxjorge",
            "repo_name": "AI-Agents-Lab"
          },
          {
            "name": "vballoli/autogen-openaiext-client",
            "stars": 16,
            "img": "https://avatars.githubusercontent.com/u/26684166?s=40&v=4",
            "owner": "vballoli",
            "repo_name": "autogen-openaiext-client"
          },
          {
            "name": "SongChiYoung/autogen-oaiapi",
            "stars": 14,
            "img": "https://avatars.githubusercontent.com/u/7540984?s=40&v=4",
            "owner": "SongChiYoung",
            "repo_name": "autogen-oaiapi"
          },
          {
            "name": "fbellame/agent-comparison",
            "stars": 11,
            "img": "https://avatars.githubusercontent.com/u/29183055?s=40&v=4",
            "owner": "fbellame",
            "repo_name": "agent-comparison"
          },
          {
            "name": "thirdweb-dev/ai",
            "stars": 10,
            "img": "https://avatars.githubusercontent.com/u/79496167?s=40&v=4",
            "owner": "thirdweb-dev",
            "repo_name": "ai"
          },
          {
            "name": "detoxio-ai/ai-red-teaming-training",
            "stars": 8,
            "img": "https://avatars.githubusercontent.com/u/165997948?s=40&v=4",
            "owner": "detoxio-ai",
            "repo_name": "ai-red-teaming-training"
          },
          {
            "name": "tsinggggg/autogen-watsonx-client",
            "stars": 8,
            "img": "https://avatars.githubusercontent.com/u/20978879?s=40&v=4",
            "owner": "tsinggggg",
            "repo_name": "autogen-watsonx-client"
          },
          {
            "name": "foundation-models/MediNoteAI",
            "stars": 7,
            "img": "https://avatars.githubusercontent.com/u/1612939?s=40&v=4",
            "owner": "foundation-models",
            "repo_name": "MediNoteAI"
          },
          {
            "name": "questcollector/autogen-kubernetes",
            "stars": 7,
            "img": "https://avatars.githubusercontent.com/u/28866236?s=40&v=4",
            "owner": "questcollector",
            "repo_name": "autogen-kubernetes"
          },
          {
            "name": "kimtth/azure-openai-llm-cookbook",
            "stars": 6,
            "img": "https://avatars.githubusercontent.com/u/13846660?s=40&v=4",
            "owner": "kimtth",
            "repo_name": "azure-openai-llm-cookbook"
          },
          {
            "name": "CantCode023/evoke",
            "stars": 6,
            "img": "https://avatars.githubusercontent.com/u/95113225?s=40&v=4",
            "owner": "CantCode023",
            "repo_name": "evoke"
          },
          {
            "name": "lukaskellerstein/ai",
            "stars": 5,
            "img": "https://avatars.githubusercontent.com/u/12882519?s=40&v=4",
            "owner": "lukaskellerstein",
            "repo_name": "ai"
          },
          {
            "name": "Squidgy-AI/BoilerPlateV1",
            "stars": 4,
            "img": "https://avatars.githubusercontent.com/u/188782334?s=40&v=4",
            "owner": "Squidgy-AI",
            "repo_name": "BoilerPlateV1"
          },
          {
            "name": "lukewu8023/agent-core",
            "stars": 4,
            "img": "https://avatars.githubusercontent.com/u/132513928?s=40&v=4",
            "owner": "lukewu8023",
            "repo_name": "agent-core"
          },
          {
            "name": "michalmar/dream-team-v2",
            "stars": 4,
            "img": "https://avatars.githubusercontent.com/u/16072588?s=40&v=4",
            "owner": "michalmar",
            "repo_name": "dream-team-v2"
          },
          {
            "name": "guyq1997/Jupyter-Assistant",
            "stars": 4,
            "img": "https://avatars.githubusercontent.com/u/111009031?s=40&v=4",
            "owner": "guyq1997",
            "repo_name": "Jupyter-Assistant"
          },
          {
            "name": "SongChiYoung/autogen-contextplus",
            "stars": 3,
            "img": "https://avatars.githubusercontent.com/u/7540984?s=40&v=4",
            "owner": "SongChiYoung",
            "repo_name": "autogen-contextplus"
          },
          {
            "name": "realtmxi/mag-review",
            "stars": 3,
            "img": "https://avatars.githubusercontent.com/u/42812404?s=40&v=4",
            "owner": "realtmxi",
            "repo_name": "mag-review"
          },
          {
            "name": "mdsiprojects/agenticai",
            "stars": 3,
            "img": "https://avatars.githubusercontent.com/u/73212683?s=40&v=4",
            "owner": "mdsiprojects",
            "repo_name": "agenticai"
          },
          {
            "name": "DoctorKhan/quantum-resonance-ledger",
            "stars": 3,
            "img": "https://avatars.githubusercontent.com/u/7505793?s=40&v=4",
            "owner": "DoctorKhan",
            "repo_name": "quantum-resonance-ledger"
          },
          {
            "name": "yusufk/jarvis-azure",
            "stars": 3,
            "img": "https://avatars.githubusercontent.com/u/404139?s=40&v=4",
            "owner": "yusufk",
            "repo_name": "jarvis-azure"
          },
          {
            "name": "hackertron/LucidLoanMachine",
            "stars": 3,
            "img": "https://avatars.githubusercontent.com/u/7667514?s=40&v=4",
            "owner": "hackertron",
            "repo_name": "LucidLoanMachine"
          },
          {
            "name": "unibaseio/aip-agent",
            "stars": 3,
            "img": "https://avatars.githubusercontent.com/u/149583566?s=40&v=4",
            "owner": "unibaseio",
            "repo_name": "aip-agent"
          },
          {
            "name": "sambanova/integrations",
            "stars": 3,
            "img": "https://avatars.githubusercontent.com/u/80118584?s=40&v=4",
            "owner": "sambanova",
            "repo_name": "integrations"
          },
          {
            "name": "playbooks-ai/playbooks",
            "stars": 3,
            "img": "https://avatars.githubusercontent.com/u/182034399?s=40&v=4",
            "owner": "playbooks-ai",
            "repo_name": "playbooks"
          },
          {
            "name": "MichalRogowski/CVAgent",
            "stars": 3,
            "img": "https://avatars.githubusercontent.com/u/47182107?s=40&v=4",
            "owner": "MichalRogowski",
            "repo_name": "CVAgent"
          },
          {
            "name": "denniszielke/ai-agents-workshop",
            "stars": 3,
            "img": "https://avatars.githubusercontent.com/u/11569044?s=40&v=4",
            "owner": "denniszielke",
            "repo_name": "ai-agents-workshop"
          },
          {
            "name": "strand1/ComfyUI-Autogen",
            "stars": 3,
            "img": "https://avatars.githubusercontent.com/u/1580273?s=40&v=4",
            "owner": "strand1",
            "repo_name": "ComfyUI-Autogen"
          },
          {
            "name": "richard-gyiko/json-schema-to-pydantic",
            "stars": 3,
            "img": "https://avatars.githubusercontent.com/u/36221820?s=40&v=4",
            "owner": "richard-gyiko",
            "repo_name": "json-schema-to-pydantic"
          },
          {
            "name": "Azure/auto-kyc",
            "stars": 3,
            "img": "https://avatars.githubusercontent.com/u/6844498?s=40&v=4",
            "owner": "Azure",
            "repo_name": "auto-kyc"
          },
          {
            "name": "alphavector/all",
            "stars": 3,
            "img": "https://avatars.githubusercontent.com/u/11805788?s=40&v=4",
            "owner": "alphavector",
            "repo_name": "all"
          },
          {
            "name": "DeepSpringAI/search_mcp_server",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/200253356?s=40&v=4",
            "owner": "DeepSpringAI",
            "repo_name": "search_mcp_server"
          },
          {
            "name": "pasangimhana/sql_agent",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/85233319?s=40&v=4",
            "owner": "pasangimhana",
            "repo_name": "sql_agent"
          },
          {
            "name": "shigeyf/azure-ai-foundry-workshop",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/6917167?s=40&v=4",
            "owner": "shigeyf",
            "repo_name": "azure-ai-foundry-workshop"
          },
          {
            "name": "HosseinZahed/multi-agent-cloud-architect",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/19933353?s=40&v=4",
            "owner": "HosseinZahed",
            "repo_name": "multi-agent-cloud-architect"
          },
          {
            "name": "jjgriff93/autowolf",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/25235950?s=40&v=4",
            "owner": "jjgriff93",
            "repo_name": "autowolf"
          },
          {
            "name": "FrancoisChastel/OpenCustomsGPT",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/10299319?s=40&v=4",
            "owner": "FrancoisChastel",
            "repo_name": "OpenCustomsGPT"
          },
          {
            "name": "Lailiangken/collm04",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/52484757?s=40&v=4",
            "owner": "Lailiangken",
            "repo_name": "collm04"
          },
          {
            "name": "trentkim-ms-ats/multi-agent-workshop",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/197431125?s=40&v=4",
            "owner": "trentkim-ms-ats",
            "repo_name": "multi-agent-workshop"
          },
          {
            "name": "marcelmedina/ai-autogen-agent-patterns",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/17131300?s=40&v=4",
            "owner": "marcelmedina",
            "repo_name": "ai-autogen-agent-patterns"
          },
          {
            "name": "kantariyaraj/AI_Agent_Examples",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/6614811?s=40&v=4",
            "owner": "kantariyaraj",
            "repo_name": "AI_Agent_Examples"
          },
          {
            "name": "trojrobert/web_agent",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/28733978?s=40&v=4",
            "owner": "trojrobert",
            "repo_name": "web_agent"
          },
          {
            "name": "dhanaabhirajk/logical_agent",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/81916146?s=40&v=4",
            "owner": "dhanaabhirajk",
            "repo_name": "logical_agent"
          },
          {
            "name": "CellCS/AI-Agents-RnD",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/16886624?s=40&v=4",
            "owner": "CellCS",
            "repo_name": "AI-Agents-RnD"
          },
          {
            "name": "jspv/mchat",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/6540751?s=40&v=4",
            "owner": "jspv",
            "repo_name": "mchat"
          },
          {
            "name": "Haervwe/Illustrated_Novel_Gen",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/31465508?s=40&v=4",
            "owner": "Haervwe",
            "repo_name": "Illustrated_Novel_Gen"
          },
          {
            "name": "UD1sto/Livepeer-Autogen-Integration",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/76160388?s=40&v=4",
            "owner": "UD1sto",
            "repo_name": "Livepeer-Autogen-Integration"
          },
          {
            "name": "fajarmh-mf/ai-agents-frameworks",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/186990157?s=40&v=4",
            "owner": "fajarmh-mf",
            "repo_name": "ai-agents-frameworks"
          },
          {
            "name": "saytyarnorngloreia/ai-agents-for-beginners",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/205020882?s=40&v=4",
            "owner": "saytyarnorngloreia",
            "repo_name": "ai-agents-for-beginners"
          },
          {
            "name": "santhoshrao95/mulit_agent_it_support_system",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/27264958?s=40&v=4",
            "owner": "santhoshrao95",
            "repo_name": "mulit_agent_it_support_system"
          },
          {
            "name": "IamMikeHelsel/sneks",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/51098787?s=40&v=4",
            "owner": "IamMikeHelsel",
            "repo_name": "sneks"
          },
          {
            "name": "NortonHuang-SUFE/FortuneTell",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/54016596?s=40&v=4",
            "owner": "NortonHuang-SUFE",
            "repo_name": "FortuneTell"
          },
          {
            "name": "CyberCRI/WeLearn-api",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/3897500?s=40&v=4",
            "owner": "CyberCRI",
            "repo_name": "WeLearn-api"
          },
          {
            "name": "DellDi/autogen-fastapi-postgres",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/40460351?s=40&v=4",
            "owner": "DellDi",
            "repo_name": "autogen-fastapi-postgres"
          },
          {
            "name": "sitloboi2012/AMS",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/33688887?s=40&v=4",
            "owner": "sitloboi2012",
            "repo_name": "AMS"
          },
          {
            "name": "black-da-bull/autogen",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/110015660?s=40&v=4",
            "owner": "black-da-bull",
            "repo_name": "autogen"
          },
          {
            "name": "kohei3110/ai-tour-2025-agent-demo",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/38392404?s=40&v=4",
            "owner": "kohei3110",
            "repo_name": "ai-tour-2025-agent-demo"
          },
          {
            "name": "verwec/03_2025_ai_workshop",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/6060259?s=40&v=4",
            "owner": "verwec",
            "repo_name": "03_2025_ai_workshop"
          },
          {
            "name": "KamikazEr101/GoodByeSqlBoy",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/154207461?s=40&v=4",
            "owner": "KamikazEr101",
            "repo_name": "GoodByeSqlBoy"
          },
          {
            "name": "memenow/persona-agent",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/142292623?s=40&v=4",
            "owner": "memenow",
            "repo_name": "persona-agent"
          },
          {
            "name": "dheerapat/autogen-agent-demo",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/61280196?s=40&v=4",
            "owner": "dheerapat",
            "repo_name": "autogen-agent-demo"
          },
          {
            "name": "Azure/multiagent-frameworks",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/6844498?s=40&v=4",
            "owner": "Azure",
            "repo_name": "multiagent-frameworks"
          },
          {
            "name": "FranklineMisango/Quant_Research_Assistant",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/95913228?s=40&v=4",
            "owner": "FranklineMisango",
            "repo_name": "Quant_Research_Assistant"
          },
          {
            "name": "RostKoff/montage-horde",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/119884204?s=40&v=4",
            "owner": "RostKoff",
            "repo_name": "montage-horde"
          },
          {
            "name": "Delahayecarry/MAAI",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/163613131?s=40&v=4",
            "owner": "Delahayecarry",
            "repo_name": "MAAI"
          },
          {
            "name": "marcelmedina/ai-autogen-agents",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/17131300?s=40&v=4",
            "owner": "marcelmedina",
            "repo_name": "ai-autogen-agents"
          },
          {
            "name": "XinmiaoYan/ai-agents-for-beginners",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/83053884?s=40&v=4",
            "owner": "XinmiaoYan",
            "repo_name": "ai-agents-for-beginners"
          },
          {
            "name": "luiselopezlopez/globalai2025",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/97617267?s=40&v=4",
            "owner": "luiselopezlopez",
            "repo_name": "globalai2025"
          },
          {
            "name": "ghchen99/agentic-conversations",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/56446026?s=40&v=4",
            "owner": "ghchen99",
            "repo_name": "agentic-conversations"
          },
          {
            "name": "CantCode023/bloggit",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/95113225?s=40&v=4",
            "owner": "CantCode023",
            "repo_name": "bloggit"
          },
          {
            "name": "prabdeb/autogen-m1-security-sample",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/37529789?s=40&v=4",
            "owner": "prabdeb",
            "repo_name": "autogen-m1-security-sample"
          },
          {
            "name": "tejajuttu/Multi-Agent-Conversations",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/25482892?s=40&v=4",
            "owner": "tejajuttu",
            "repo_name": "Multi-Agent-Conversations"
          },
          {
            "name": "jakeatmsft/fsi-ai-agent-hackathon",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/47987698?s=40&v=4",
            "owner": "jakeatmsft",
            "repo_name": "fsi-ai-agent-hackathon"
          },
          {
            "name": "AliciaSJF/autogen",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/132434651?s=40&v=4",
            "owner": "AliciaSJF",
            "repo_name": "autogen"
          },
          {
            "name": "aribornstein/autogen-llamacpp-extension",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/3045407?s=40&v=4",
            "owner": "aribornstein",
            "repo_name": "autogen-llamacpp-extension"
          },
          {
            "name": "yikedu7/poc",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/25717773?s=40&v=4",
            "owner": "yikedu7",
            "repo_name": "poc"
          },
          {
            "name": "codeh007/mtmai",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/95907738?s=40&v=4",
            "owner": "codeh007",
            "repo_name": "mtmai"
          },
          {
            "name": "thedigitalworkplace/Autogen",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/133930737?s=40&v=4",
            "owner": "thedigitalworkplace",
            "repo_name": "Autogen"
          },
          {
            "name": "DimohaZadira/ai-agent-start",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/33756112?s=40&v=4",
            "owner": "DimohaZadira",
            "repo_name": "ai-agent-start"
          },
          {
            "name": "rinormaloku/autogen-langchain-mcp-mix",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/9026451?s=40&v=4",
            "owner": "rinormaloku",
            "repo_name": "autogen-langchain-mcp-mix"
          },
          {
            "name": "lgcorzo/llmops-python-package",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/46710567?s=40&v=4",
            "owner": "lgcorzo",
            "repo_name": "llmops-python-package"
          },
          {
            "name": "r-bit-rry/magentic-two",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/34023431?s=40&v=4",
            "owner": "r-bit-rry",
            "repo_name": "magentic-two"
          },
          {
            "name": "1012638836/ai_qiniu_chatbot",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/29818043?s=40&v=4",
            "owner": "1012638836",
            "repo_name": "ai_qiniu_chatbot"
          },
          {
            "name": "shreyanavelkar/automated-ai-assistant",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/28059200?s=40&v=4",
            "owner": "shreyanavelkar",
            "repo_name": "automated-ai-assistant"
          },
          {
            "name": "parth-verma7/agentic-ai-task",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/96720577?s=40&v=4",
            "owner": "parth-verma7",
            "repo_name": "agentic-ai-task"
          },
          {
            "name": "denniszielke/autogen-playground",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/11569044?s=40&v=4",
            "owner": "denniszielke",
            "repo_name": "autogen-playground"
          },
          {
            "name": "mtliou/BioPharma451",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/164873773?s=40&v=4",
            "owner": "mtliou",
            "repo_name": "BioPharma451"
          },
          {
            "name": "selafmai/Magenitc-one-AGENTIC-InsurTech",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/157835927?s=40&v=4",
            "owner": "selafmai",
            "repo_name": "Magenitc-one-AGENTIC-InsurTech"
          },
          {
            "name": "tikendraw/jobber",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/68785366?s=40&v=4",
            "owner": "tikendraw",
            "repo_name": "jobber"
          },
          {
            "name": "eugeneVzaidun/autogen",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/72197489?s=40&v=4",
            "owner": "eugeneVzaidun",
            "repo_name": "autogen"
          },
          {
            "name": "CodeDevKyle/autogen_DevKyle",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/167522880?s=40&v=4",
            "owner": "CodeDevKyle",
            "repo_name": "autogen_DevKyle"
          },
          {
            "name": "LittleLittleCloud/Develop_Agent_With_AutoGen",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/16876986?s=40&v=4",
            "owner": "LittleLittleCloud",
            "repo_name": "Develop_Agent_With_AutoGen"
          }
        ],
        "public_dependents_number": 114,
        "private_dependents_number": -114,
        "total_dependents_number": 114,
        "badges": {
          "total": "[![Generated by github-dependents-info](https://img.shields.io/static/v1?label=Used%20by&message=114&color=informational&logo=slickpic)](https://github.com/microsoft/autogen/network/dependents?package_id=UGFja2FnZS01MzAwMTAxNDU5)",
          "public": "[![Generated by github-dependents-info](https://img.shields.io/static/v1?label=Used%20by%20(public)&message=114&color=informational&logo=slickpic)](https://github.com/microsoft/autogen/network/dependents?package_id=UGFja2FnZS01MzAwMTAxNDU5)",
          "private": "[![Generated by github-dependents-info](https://img.shields.io/static/v1?label=Used%20by%20(private)&message=-114&color=informational&logo=slickpic)](https://github.com/microsoft/autogen/network/dependents?package_id=UGFja2FnZS01MzAwMTAxNDU5)",
          "stars": "[![Generated by github-dependents-info](https://img.shields.io/static/v1?label=Used%20by%20(stars)&message=43611&color=informational&logo=slickpic)](https://github.com/microsoft/autogen/network/dependents?package_id=UGFja2FnZS01MzAwMTAxNDU5)"
        }
      },
      {
        "id": "UGFja2FnZS00Njg5OTYzMjI2",
        "name": "AutoGen.SourceGenerator",
        "url": "https://github.com/microsoft/autogen/network/dependents?package_id=UGFja2FnZS00Njg5OTYzMjI2",
        "public_dependent_stars": 9478,
        "public_dependents": [
          {
            "name": "dotnet/machinelearning",
            "stars": 9161,
            "img": "https://avatars.githubusercontent.com/u/9141961?s=40&v=4",
            "owner": "dotnet",
            "repo_name": "machinelearning"
          },
          {
            "name": "LittleLittleCloud/StepWise",
            "stars": 113,
            "img": "https://avatars.githubusercontent.com/u/16876986?s=40&v=4",
            "owner": "LittleLittleCloud",
            "repo_name": "StepWise"
          },
          {
            "name": "xuzeyu91/EasyAgent",
            "stars": 93,
            "img": "https://avatars.githubusercontent.com/u/26290929?s=40&v=4",
            "owner": "xuzeyu91",
            "repo_name": "EasyAgent"
          },
          {
            "name": "LittleLittleCloud/Agent-ChatRoom",
            "stars": 48,
            "img": "https://avatars.githubusercontent.com/u/16876986?s=40&v=4",
            "owner": "LittleLittleCloud",
            "repo_name": "Agent-ChatRoom"
          },
          {
            "name": "ZhaoYis/Berry.Spider",
            "stars": 13,
            "img": "https://avatars.githubusercontent.com/u/22954818?s=40&v=4",
            "owner": "ZhaoYis",
            "repo_name": "Berry.Spider"
          },
          {
            "name": "LittleLittleCloud/AI-Agentic-Design-Patterns-with-AutoGen.Net",
            "stars": 11,
            "img": "https://avatars.githubusercontent.com/u/16876986?s=40&v=4",
            "owner": "LittleLittleCloud",
            "repo_name": "AI-Agentic-Design-Patterns-with-AutoGen.Net"
          },
          {
            "name": "joslat/AgenticAIAutoGen",
            "stars": 9,
            "img": "https://avatars.githubusercontent.com/u/9831011?s=40&v=4",
            "owner": "joslat",
            "repo_name": "AgenticAIAutoGen"
          },
          {
            "name": "LittleLittleCloud/PowershellGPT",
            "stars": 8,
            "img": "https://avatars.githubusercontent.com/u/16876986?s=40&v=4",
            "owner": "LittleLittleCloud",
            "repo_name": "PowershellGPT"
          },
          {
            "name": "LittleLittleCloud/WebSurferAgent",
            "stars": 6,
            "img": "https://avatars.githubusercontent.com/u/16876986?s=40&v=4",
            "owner": "LittleLittleCloud",
            "repo_name": "WebSurferAgent"
          },
          {
            "name": "LittleLittleCloud/code-interpreter-workflow",
            "stars": 5,
            "img": "https://avatars.githubusercontent.com/u/16876986?s=40&v=4",
            "owner": "LittleLittleCloud",
            "repo_name": "code-interpreter-workflow"
          },
          {
            "name": "filipw/strathweb-samples-azureopenai",
            "stars": 4,
            "img": "https://avatars.githubusercontent.com/u/1710369?s=40&v=4",
            "owner": "filipw",
            "repo_name": "strathweb-samples-azureopenai"
          },
          {
            "name": "scalalang2/crypto-ai-hedge-fund",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/2603286?s=40&v=4",
            "owner": "scalalang2",
            "repo_name": "crypto-ai-hedge-fund"
          },
          {
            "name": "LittleLittleCloud/weather-report-agent",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/16876986?s=40&v=4",
            "owner": "LittleLittleCloud",
            "repo_name": "weather-report-agent"
          },
          {
            "name": "feynmanloo/AutoGen.BasicSamples",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/20107191?s=40&v=4",
            "owner": "feynmanloo",
            "repo_name": "AutoGen.BasicSamples"
          },
          {
            "name": "LittleLittleCloud/Assistant.Net",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/16876986?s=40&v=4",
            "owner": "LittleLittleCloud",
            "repo_name": "Assistant.Net"
          }
        ],
        "public_dependents_number": 15,
        "private_dependents_number": -15,
        "total_dependents_number": 15,
        "badges": {
          "total": "[![Generated by github-dependents-info](https://img.shields.io/static/v1?label=Used%20by&message=15&color=informational&logo=slickpic)](https://github.com/microsoft/autogen/network/dependents?package_id=UGFja2FnZS00Njg5OTYzMjI2)",
          "public": "[![Generated by github-dependents-info](https://img.shields.io/static/v1?label=Used%20by%20(public)&message=15&color=informational&logo=slickpic)](https://github.com/microsoft/autogen/network/dependents?package_id=UGFja2FnZS00Njg5OTYzMjI2)",
          "private": "[![Generated by github-dependents-info](https://img.shields.io/static/v1?label=Used%20by%20(private)&message=-15&color=informational&logo=slickpic)](https://github.com/microsoft/autogen/network/dependents?package_id=UGFja2FnZS00Njg5OTYzMjI2)",
          "stars": "[![Generated by github-dependents-info](https://img.shields.io/static/v1?label=Used%20by%20(stars)&message=9478&color=informational&logo=slickpic)](https://github.com/microsoft/autogen/network/dependents?package_id=UGFja2FnZS00Njg5OTYzMjI2)"
        }
      },
      {
        "id": "UGFja2FnZS00Njg5OTYzMjQw",
        "name": "AutoGen.Core",
        "url": "https://github.com/microsoft/autogen/network/dependents?package_id=UGFja2FnZS00Njg5OTYzMjQw",
        "public_dependent_stars": 9242,
        "public_dependents": [
          {
            "name": "dotnet/machinelearning",
            "stars": 9161,
            "img": "https://avatars.githubusercontent.com/u/9141961?s=40&v=4",
            "owner": "dotnet",
            "repo_name": "machinelearning"
          },
          {
            "name": "LittleLittleCloud/Agent-ChatRoom",
            "stars": 48,
            "img": "https://avatars.githubusercontent.com/u/16876986?s=40&v=4",
            "owner": "LittleLittleCloud",
            "repo_name": "Agent-ChatRoom"
          },
          {
            "name": "aevatarAI/aevatar-station",
            "stars": 23,
            "img": "https://avatars.githubusercontent.com/u/190360083?s=40&v=4",
            "owner": "aevatarAI",
            "repo_name": "aevatar-station"
          },
          {
            "name": "MaxAkbar/Local.NET.LLM",
            "stars": 3,
            "img": "https://avatars.githubusercontent.com/u/13856299?s=40&v=4",
            "owner": "MaxAkbar",
            "repo_name": "Local.NET.LLM"
          },
          {
            "name": "aevatarAI/AISmart",
            "stars": 3,
            "img": "https://avatars.githubusercontent.com/u/190360083?s=40&v=4",
            "owner": "aevatarAI",
            "repo_name": "AISmart"
          },
          {
            "name": "AndreRatzenberger/DurableFunctions.SemanticKernel",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/44863088?s=40&v=4",
            "owner": "AndreRatzenberger",
            "repo_name": "DurableFunctions.SemanticKernel"
          },
          {
            "name": "opazenha/csharp-autogen",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/172253505?s=40&v=4",
            "owner": "opazenha",
            "repo_name": "csharp-autogen"
          },
          {
            "name": "skulidropek/AutoGenTestTask-master",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/66840575?s=40&v=4",
            "owner": "skulidropek",
            "repo_name": "AutoGenTestTask-master"
          }
        ],
        "public_dependents_number": 8,
        "private_dependents_number": -8,
        "total_dependents_number": 8,
        "badges": {
          "total": "[![Generated by github-dependents-info](https://img.shields.io/static/v1?label=Used%20by&message=8&color=informational&logo=slickpic)](https://github.com/microsoft/autogen/network/dependents?package_id=UGFja2FnZS00Njg5OTYzMjQw)",
          "public": "[![Generated by github-dependents-info](https://img.shields.io/static/v1?label=Used%20by%20(public)&message=8&color=informational&logo=slickpic)](https://github.com/microsoft/autogen/network/dependents?package_id=UGFja2FnZS00Njg5OTYzMjQw)",
          "private": "[![Generated by github-dependents-info](https://img.shields.io/static/v1?label=Used%20by%20(private)&message=-8&color=informational&logo=slickpic)](https://github.com/microsoft/autogen/network/dependents?package_id=UGFja2FnZS00Njg5OTYzMjQw)",
          "stars": "[![Generated by github-dependents-info](https://img.shields.io/static/v1?label=Used%20by%20(stars)&message=9242&color=informational&logo=slickpic)](https://github.com/microsoft/autogen/network/dependents?package_id=UGFja2FnZS00Njg5OTYzMjQw)"
        }
      },
      {
        "id": "UGFja2FnZS00NjgyNzA4Njkz",
        "name": "AutoGen",
        "url": "https://github.com/microsoft/autogen/network/dependents?package_id=UGFja2FnZS00NjgyNzA4Njkz",
        "public_dependent_stars": 377,
        "public_dependents": [
          {
            "name": "axzxs2001/Asp.NetCoreExperiment",
            "stars": 357,
            "img": "https://avatars.githubusercontent.com/u/522232?s=40&v=4",
            "owner": "axzxs2001",
            "repo_name": "Asp.NetCoreExperiment"
          },
          {
            "name": "LittleLittleCloud/StepWise",
            "stars": 113,
            "img": "https://avatars.githubusercontent.com/u/16876986?s=40&v=4",
            "owner": "LittleLittleCloud",
            "repo_name": "StepWise"
          },
          {
            "name": "tylike/AI.Labs",
            "stars": 94,
            "img": "https://avatars.githubusercontent.com/u/920370?s=40&v=4",
            "owner": "tylike",
            "repo_name": "AI.Labs"
          },
          {
            "name": "xuzeyu91/EasyAgent",
            "stars": 93,
            "img": "https://avatars.githubusercontent.com/u/26290929?s=40&v=4",
            "owner": "xuzeyu91",
            "repo_name": "EasyAgent"
          },
          {
            "name": "Senparc/Senparc.AI",
            "stars": 85,
            "img": "https://avatars.githubusercontent.com/u/6435496?s=40&v=4",
            "owner": "Senparc",
            "repo_name": "Senparc.AI"
          },
          {
            "name": "LittleLittleCloud/Agent-ChatRoom",
            "stars": 48,
            "img": "https://avatars.githubusercontent.com/u/16876986?s=40&v=4",
            "owner": "LittleLittleCloud",
            "repo_name": "Agent-ChatRoom"
          },
          {
            "name": "Cloud-Jas/DurableMultiAgents",
            "stars": 39,
            "img": "https://avatars.githubusercontent.com/u/96443739?s=40&v=4",
            "owner": "Cloud-Jas",
            "repo_name": "DurableMultiAgents"
          },
          {
            "name": "aevatarAI/aevatar-station",
            "stars": 23,
            "img": "https://avatars.githubusercontent.com/u/190360083?s=40&v=4",
            "owner": "aevatarAI",
            "repo_name": "aevatar-station"
          },
          {
            "name": "filipw/strathweb-phi-engine",
            "stars": 20,
            "img": "https://avatars.githubusercontent.com/u/1710369?s=40&v=4",
            "owner": "filipw",
            "repo_name": "strathweb-phi-engine"
          },
          {
            "name": "yanxiaodi/MyCodeSamples",
            "stars": 19,
            "img": "https://avatars.githubusercontent.com/u/3395915?s=40&v=4",
            "owner": "yanxiaodi",
            "repo_name": "MyCodeSamples"
          },
          {
            "name": "joslat/SemanticKernelAgenticChatWorkflowsExtension",
            "stars": 16,
            "img": "https://avatars.githubusercontent.com/u/9831011?s=40&v=4",
            "owner": "joslat",
            "repo_name": "SemanticKernelAgenticChatWorkflowsExtension"
          },
          {
            "name": "ZhaoYis/Berry.Spider",
            "stars": 13,
            "img": "https://avatars.githubusercontent.com/u/22954818?s=40&v=4",
            "owner": "ZhaoYis",
            "repo_name": "Berry.Spider"
          },
          {
            "name": "LittleLittleCloud/AI-Agentic-Design-Patterns-with-AutoGen.Net",
            "stars": 11,
            "img": "https://avatars.githubusercontent.com/u/16876986?s=40&v=4",
            "owner": "LittleLittleCloud",
            "repo_name": "AI-Agentic-Design-Patterns-with-AutoGen.Net"
          },
          {
            "name": "joslat/AgenticAIAutoGen",
            "stars": 9,
            "img": "https://avatars.githubusercontent.com/u/9831011?s=40&v=4",
            "owner": "joslat",
            "repo_name": "AgenticAIAutoGen"
          },
          {
            "name": "LittleLittleCloud/PowershellGPT",
            "stars": 8,
            "img": "https://avatars.githubusercontent.com/u/16876986?s=40&v=4",
            "owner": "LittleLittleCloud",
            "repo_name": "PowershellGPT"
          },
          {
            "name": "LittleLittleCloud/WebSurferAgent",
            "stars": 6,
            "img": "https://avatars.githubusercontent.com/u/16876986?s=40&v=4",
            "owner": "LittleLittleCloud",
            "repo_name": "WebSurferAgent"
          },
          {
            "name": "LittleLittleCloud/code-interpreter-workflow",
            "stars": 5,
            "img": "https://avatars.githubusercontent.com/u/16876986?s=40&v=4",
            "owner": "LittleLittleCloud",
            "repo_name": "code-interpreter-workflow"
          },
          {
            "name": "LittleLittleCloud/AIAgentTeam",
            "stars": 5,
            "img": "https://avatars.githubusercontent.com/u/16876986?s=40&v=4",
            "owner": "LittleLittleCloud",
            "repo_name": "AIAgentTeam"
          },
          {
            "name": "filipw/strathweb-samples-azureopenai",
            "stars": 4,
            "img": "https://avatars.githubusercontent.com/u/1710369?s=40&v=4",
            "owner": "filipw",
            "repo_name": "strathweb-samples-azureopenai"
          },
          {
            "name": "StormHub/TinyAgent",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/16412512?s=40&v=4",
            "owner": "StormHub",
            "repo_name": "TinyAgent"
          },
          {
            "name": "LittleLittleCloud/weather-report-agent",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/16876986?s=40&v=4",
            "owner": "LittleLittleCloud",
            "repo_name": "weather-report-agent"
          },
          {
            "name": "jsboige/CoursIA",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/3159389?s=40&v=4",
            "owner": "jsboige",
            "repo_name": "CoursIA"
          },
          {
            "name": "feynmanloo/AutoGen.BasicSamples",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/20107191?s=40&v=4",
            "owner": "feynmanloo",
            "repo_name": "AutoGen.BasicSamples"
          },
          {
            "name": "AndreRatzenberger/DurableFunctions.SemanticKernel",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/44863088?s=40&v=4",
            "owner": "AndreRatzenberger",
            "repo_name": "DurableFunctions.SemanticKernel"
          },
          {
            "name": "V0v1kkk/EnglishFlashcardGenerator",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/17791114?s=40&v=4",
            "owner": "V0v1kkk",
            "repo_name": "EnglishFlashcardGenerator"
          },
          {
            "name": "Dharma-DX/AutoGenSample",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/64537546?s=40&v=4",
            "owner": "Dharma-DX",
            "repo_name": "AutoGenSample"
          },
          {
            "name": "opazenha/csharp-autogen",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/172253505?s=40&v=4",
            "owner": "opazenha",
            "repo_name": "csharp-autogen"
          },
          {
            "name": "totosan/AIAgentsTest",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/7992807?s=40&v=4",
            "owner": "totosan",
            "repo_name": "AIAgentsTest"
          },
          {
            "name": "chrnthnkmutt/slm_dotnet_experiment",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/114327550?s=40&v=4",
            "owner": "chrnthnkmutt",
            "repo_name": "slm_dotnet_experiment"
          },
          {
            "name": "ImIsaacTbh/ObscureBot",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/81532363?s=40&v=4",
            "owner": "ImIsaacTbh",
            "repo_name": "ObscureBot"
          },
          {
            "name": "montraydavis/MDLabs-ResearchTeam",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/12994571?s=40&v=4",
            "owner": "montraydavis",
            "repo_name": "MDLabs-ResearchTeam"
          },
          {
            "name": "skulidropek/AutoGenTestTask-master",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/66840575?s=40&v=4",
            "owner": "skulidropek",
            "repo_name": "AutoGenTestTask-master"
          },
          {
            "name": "LittleLittleCloud/Assistant.Net",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/16876986?s=40&v=4",
            "owner": "LittleLittleCloud",
            "repo_name": "Assistant.Net"
          }
        ],
        "public_dependents_number": 33,
        "private_dependents_number": -33,
        "total_dependents_number": 33,
        "badges": {
          "total": "[![Generated by github-dependents-info](https://img.shields.io/static/v1?label=Used%20by&message=33&color=informational&logo=slickpic)](https://github.com/microsoft/autogen/network/dependents?package_id=UGFja2FnZS00NjgyNzA4Njkz)",
          "public": "[![Generated by github-dependents-info](https://img.shields.io/static/v1?label=Used%20by%20(public)&message=33&color=informational&logo=slickpic)](https://github.com/microsoft/autogen/network/dependents?package_id=UGFja2FnZS00NjgyNzA4Njkz)",
          "private": "[![Generated by github-dependents-info](https://img.shields.io/static/v1?label=Used%20by%20(private)&message=-33&color=informational&logo=slickpic)](https://github.com/microsoft/autogen/network/dependents?package_id=UGFja2FnZS00NjgyNzA4Njkz)",
          "stars": "[![Generated by github-dependents-info](https://img.shields.io/static/v1?label=Used%20by%20(stars)&message=377&color=informational&logo=slickpic)](https://github.com/microsoft/autogen/network/dependents?package_id=UGFja2FnZS00NjgyNzA4Njkz)"
        }
      },
      {
        "id": "UGFja2FnZS00Njg5OTY0MjM3",
        "name": "AutoGen.DotnetInteractive",
        "url": "https://github.com/microsoft/autogen/network/dependents?package_id=UGFja2FnZS00Njg5OTY0MjM3",
        "public_dependent_stars": 330,
        "public_dependents": [
          {
            "name": "LittleLittleCloud/StepWise",
            "stars": 113,
            "img": "https://avatars.githubusercontent.com/u/16876986?s=40&v=4",
            "owner": "LittleLittleCloud",
            "repo_name": "StepWise"
          },
          {
            "name": "tylike/AI.Labs",
            "stars": 94,
            "img": "https://avatars.githubusercontent.com/u/920370?s=40&v=4",
            "owner": "tylike",
            "repo_name": "AI.Labs"
          },
          {
            "name": "xuzeyu91/EasyAgent",
            "stars": 93,
            "img": "https://avatars.githubusercontent.com/u/26290929?s=40&v=4",
            "owner": "xuzeyu91",
            "repo_name": "EasyAgent"
          },
          {
            "name": "LittleLittleCloud/AI-Agentic-Design-Patterns-with-AutoGen.Net",
            "stars": 11,
            "img": "https://avatars.githubusercontent.com/u/16876986?s=40&v=4",
            "owner": "LittleLittleCloud",
            "repo_name": "AI-Agentic-Design-Patterns-with-AutoGen.Net"
          },
          {
            "name": "joslat/AgenticAIAutoGen",
            "stars": 9,
            "img": "https://avatars.githubusercontent.com/u/9831011?s=40&v=4",
            "owner": "joslat",
            "repo_name": "AgenticAIAutoGen"
          },
          {
            "name": "LittleLittleCloud/code-interpreter-workflow",
            "stars": 5,
            "img": "https://avatars.githubusercontent.com/u/16876986?s=40&v=4",
            "owner": "LittleLittleCloud",
            "repo_name": "code-interpreter-workflow"
          },
          {
            "name": "feynmanloo/AutoGen.BasicSamples",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/20107191?s=40&v=4",
            "owner": "feynmanloo",
            "repo_name": "AutoGen.BasicSamples"
          },
          {
            "name": "totosan/AIAgentsTest",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/7992807?s=40&v=4",
            "owner": "totosan",
            "repo_name": "AIAgentsTest"
          },
          {
            "name": "skulidropek/AutoGenTestTask-master",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/66840575?s=40&v=4",
            "owner": "skulidropek",
            "repo_name": "AutoGenTestTask-master"
          },
          {
            "name": "LittleLittleCloud/Assistant.Net",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/16876986?s=40&v=4",
            "owner": "LittleLittleCloud",
            "repo_name": "Assistant.Net"
          }
        ],
        "public_dependents_number": 10,
        "private_dependents_number": -10,
        "total_dependents_number": 10,
        "badges": {
          "total": "[![Generated by github-dependents-info](https://img.shields.io/static/v1?label=Used%20by&message=10&color=informational&logo=slickpic)](https://github.com/microsoft/autogen/network/dependents?package_id=UGFja2FnZS00Njg5OTY0MjM3)",
          "public": "[![Generated by github-dependents-info](https://img.shields.io/static/v1?label=Used%20by%20(public)&message=10&color=informational&logo=slickpic)](https://github.com/microsoft/autogen/network/dependents?package_id=UGFja2FnZS00Njg5OTY0MjM3)",
          "private": "[![Generated by github-dependents-info](https://img.shields.io/static/v1?label=Used%20by%20(private)&message=-10&color=informational&logo=slickpic)](https://github.com/microsoft/autogen/network/dependents?package_id=UGFja2FnZS00Njg5OTY0MjM3)",
          "stars": "[![Generated by github-dependents-info](https://img.shields.io/static/v1?label=Used%20by%20(stars)&message=330&color=informational&logo=slickpic)](https://github.com/microsoft/autogen/network/dependents?package_id=UGFja2FnZS00Njg5OTY0MjM3)"
        }
      },
      {
        "id": "UGFja2FnZS00Njg5OTY0NzU3",
        "name": "AutoGen.OpenAI",
        "url": "https://github.com/microsoft/autogen/network/dependents?package_id=UGFja2FnZS00Njg5OTY0NzU3",
        "public_dependent_stars": 158,
        "public_dependents": [
          {
            "name": "tylike/AI.Labs",
            "stars": 94,
            "img": "https://avatars.githubusercontent.com/u/920370?s=40&v=4",
            "owner": "tylike",
            "repo_name": "AI.Labs"
          },
          {
            "name": "LittleLittleCloud/Agent-ChatRoom",
            "stars": 48,
            "img": "https://avatars.githubusercontent.com/u/16876986?s=40&v=4",
            "owner": "LittleLittleCloud",
            "repo_name": "Agent-ChatRoom"
          },
          {
            "name": "joslat/AgenticAIAutoGen",
            "stars": 9,
            "img": "https://avatars.githubusercontent.com/u/9831011?s=40&v=4",
            "owner": "joslat",
            "repo_name": "AgenticAIAutoGen"
          },
          {
            "name": "scalalang2/crypto-ai-hedge-fund",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/2603286?s=40&v=4",
            "owner": "scalalang2",
            "repo_name": "crypto-ai-hedge-fund"
          },
          {
            "name": "V0v1kkk/EnglishFlashcardGenerator",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/17791114?s=40&v=4",
            "owner": "V0v1kkk",
            "repo_name": "EnglishFlashcardGenerator"
          },
          {
            "name": "chrnthnkmutt/slm_dotnet_experiment",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/114327550?s=40&v=4",
            "owner": "chrnthnkmutt",
            "repo_name": "slm_dotnet_experiment"
          },
          {
            "name": "dtHvinh/QA.WebAPI",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/115357244?s=40&v=4",
            "owner": "dtHvinh",
            "repo_name": "QA.WebAPI"
          },
          {
            "name": "Dharma-DX/AutoGenSample",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/64537546?s=40&v=4",
            "owner": "Dharma-DX",
            "repo_name": "AutoGenSample"
          },
          {
            "name": "skulidropek/AutoGenTestTask-master",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/66840575?s=40&v=4",
            "owner": "skulidropek",
            "repo_name": "AutoGenTestTask-master"
          }
        ],
        "public_dependents_number": 9,
        "private_dependents_number": -9,
        "total_dependents_number": 9,
        "badges": {
          "total": "[![Generated by github-dependents-info](https://img.shields.io/static/v1?label=Used%20by&message=9&color=informational&logo=slickpic)](https://github.com/microsoft/autogen/network/dependents?package_id=UGFja2FnZS00Njg5OTY0NzU3)",
          "public": "[![Generated by github-dependents-info](https://img.shields.io/static/v1?label=Used%20by%20(public)&message=9&color=informational&logo=slickpic)](https://github.com/microsoft/autogen/network/dependents?package_id=UGFja2FnZS00Njg5OTY0NzU3)",
          "private": "[![Generated by github-dependents-info](https://img.shields.io/static/v1?label=Used%20by%20(private)&message=-9&color=informational&logo=slickpic)](https://github.com/microsoft/autogen/network/dependents?package_id=UGFja2FnZS00Njg5OTY0NzU3)",
          "stars": "[![Generated by github-dependents-info](https://img.shields.io/static/v1?label=Used%20by%20(stars)&message=158&color=informational&logo=slickpic)](https://github.com/microsoft/autogen/network/dependents?package_id=UGFja2FnZS00Njg5OTY0NzU3)"
        }
      },
      {
        "id": "UGFja2FnZS00Njg5OTYzMzIy",
        "name": "AutoGen.SemanticKernel",
        "url": "https://github.com/microsoft/autogen/network/dependents?package_id=UGFja2FnZS00Njg5OTYzMzIy",
        "public_dependent_stars": 142,
        "public_dependents": [
          {
            "name": "LittleLittleCloud/Agent-ChatRoom",
            "stars": 48,
            "img": "https://avatars.githubusercontent.com/u/16876986?s=40&v=4",
            "owner": "LittleLittleCloud",
            "repo_name": "Agent-ChatRoom"
          },
          {
            "name": "rvinothrajendran/MicrosoftSemanticKernelSamples",
            "stars": 40,
            "img": "https://avatars.githubusercontent.com/u/16264167?s=40&v=4",
            "owner": "rvinothrajendran",
            "repo_name": "MicrosoftSemanticKernelSamples"
          },
          {
            "name": "Cloud-Jas/DurableMultiAgents",
            "stars": 39,
            "img": "https://avatars.githubusercontent.com/u/96443739?s=40&v=4",
            "owner": "Cloud-Jas",
            "repo_name": "DurableMultiAgents"
          },
          {
            "name": "LittleLittleCloud/AI-Agentic-Design-Patterns-with-AutoGen.Net",
            "stars": 11,
            "img": "https://avatars.githubusercontent.com/u/16876986?s=40&v=4",
            "owner": "LittleLittleCloud",
            "repo_name": "AI-Agentic-Design-Patterns-with-AutoGen.Net"
          },
          {
            "name": "StormHub/TinyAgent",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/16412512?s=40&v=4",
            "owner": "StormHub",
            "repo_name": "TinyAgent"
          },
          {
            "name": "feynmanloo/AutoGen.BasicSamples",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/20107191?s=40&v=4",
            "owner": "feynmanloo",
            "repo_name": "AutoGen.BasicSamples"
          }
        ],
        "public_dependents_number": 6,
        "private_dependents_number": -6,
        "total_dependents_number": 6,
        "badges": {
          "total": "[![Generated by github-dependents-info](https://img.shields.io/static/v1?label=Used%20by&message=6&color=informational&logo=slickpic)](https://github.com/microsoft/autogen/network/dependents?package_id=UGFja2FnZS00Njg5OTYzMzIy)",
          "public": "[![Generated by github-dependents-info](https://img.shields.io/static/v1?label=Used%20by%20(public)&message=6&color=informational&logo=slickpic)](https://github.com/microsoft/autogen/network/dependents?package_id=UGFja2FnZS00Njg5OTYzMzIy)",
          "private": "[![Generated by github-dependents-info](https://img.shields.io/static/v1?label=Used%20by%20(private)&message=-6&color=informational&logo=slickpic)](https://github.com/microsoft/autogen/network/dependents?package_id=UGFja2FnZS00Njg5OTYzMzIy)",
          "stars": "[![Generated by github-dependents-info](https://img.shields.io/static/v1?label=Used%20by%20(stars)&message=142&color=informational&logo=slickpic)](https://github.com/microsoft/autogen/network/dependents?package_id=UGFja2FnZS00Njg5OTYzMzIy)"
        }
      },
      {
        "id": "UGFja2FnZS00Njg5OTY0NjE1",
        "name": "AutoGen.LMStudio",
        "url": "https://github.com/microsoft/autogen/network/dependents?package_id=UGFja2FnZS00Njg5OTY0NjE1",
        "public_dependent_stars": 7,
        "public_dependents": [
          {
            "name": "devcodemonkey/de.devcodemonkey.AIChecker",
            "stars": 3,
            "img": "https://avatars.githubusercontent.com/u/77974465?s=40&v=4",
            "owner": "devcodemonkey",
            "repo_name": "de.devcodemonkey.AIChecker"
          },
          {
            "name": "chrnthnkmutt/slm_dotnet_experiment",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/114327550?s=40&v=4",
            "owner": "chrnthnkmutt",
            "repo_name": "slm_dotnet_experiment"
          },
          {
            "name": "dtHvinh/QA.WebAPI",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/115357244?s=40&v=4",
            "owner": "dtHvinh",
            "repo_name": "QA.WebAPI"
          },
          {
            "name": "PioneerMNDR/MousyHub",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/130105724?s=40&v=4",
            "owner": "PioneerMNDR",
            "repo_name": "MousyHub"
          },
          {
            "name": "ImIsaacTbh/ObscureBot",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/81532363?s=40&v=4",
            "owner": "ImIsaacTbh",
            "repo_name": "ObscureBot"
          }
        ],
        "public_dependents_number": 5,
        "private_dependents_number": -5,
        "total_dependents_number": 5,
        "badges": {
          "total": "[![Generated by github-dependents-info](https://img.shields.io/static/v1?label=Used%20by&message=5&color=informational&logo=slickpic)](https://github.com/microsoft/autogen/network/dependents?package_id=UGFja2FnZS00Njg5OTY0NjE1)",
          "public": "[![Generated by github-dependents-info](https://img.shields.io/static/v1?label=Used%20by%20(public)&message=5&color=informational&logo=slickpic)](https://github.com/microsoft/autogen/network/dependents?package_id=UGFja2FnZS00Njg5OTY0NjE1)",
          "private": "[![Generated by github-dependents-info](https://img.shields.io/static/v1?label=Used%20by%20(private)&message=-5&color=informational&logo=slickpic)](https://github.com/microsoft/autogen/network/dependents?package_id=UGFja2FnZS00Njg5OTY0NjE1)",
          "stars": "[![Generated by github-dependents-info](https://img.shields.io/static/v1?label=Used%20by%20(stars)&message=7&color=informational&logo=slickpic)](https://github.com/microsoft/autogen/network/dependents?package_id=UGFja2FnZS00Njg5OTY0NjE1)"
        }
      },
      {
        "id": "UGFja2FnZS00ODA3ODE0MzY4",
        "name": "AutoGen.Ollama",
        "url": "https://github.com/microsoft/autogen/network/dependents?package_id=UGFja2FnZS00ODA3ODE0MzY4",
        "public_dependent_stars": 4,
        "public_dependents": [
          {
            "name": "devcodemonkey/de.devcodemonkey.AIChecker",
            "stars": 3,
            "img": "https://avatars.githubusercontent.com/u/77974465?s=40&v=4",
            "owner": "devcodemonkey",
            "repo_name": "de.devcodemonkey.AIChecker"
          },
          {
            "name": "chrnthnkmutt/slm_dotnet_experiment",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/114327550?s=40&v=4",
            "owner": "chrnthnkmutt",
            "repo_name": "slm_dotnet_experiment"
          }
        ],
        "public_dependents_number": 2,
        "private_dependents_number": -2,
        "total_dependents_number": 2,
        "badges": {
          "total": "[![Generated by github-dependents-info](https://img.shields.io/static/v1?label=Used%20by&message=2&color=informational&logo=slickpic)](https://github.com/microsoft/autogen/network/dependents?package_id=UGFja2FnZS00ODA3ODE0MzY4)",
          "public": "[![Generated by github-dependents-info](https://img.shields.io/static/v1?label=Used%20by%20(public)&message=2&color=informational&logo=slickpic)](https://github.com/microsoft/autogen/network/dependents?package_id=UGFja2FnZS00ODA3ODE0MzY4)",
          "private": "[![Generated by github-dependents-info](https://img.shields.io/static/v1?label=Used%20by%20(private)&message=-2&color=informational&logo=slickpic)](https://github.com/microsoft/autogen/network/dependents?package_id=UGFja2FnZS00ODA3ODE0MzY4)",
          "stars": "[![Generated by github-dependents-info](https://img.shields.io/static/v1?label=Used%20by%20(stars)&message=4&color=informational&logo=slickpic)](https://github.com/microsoft/autogen/network/dependents?package_id=UGFja2FnZS00ODA3ODE0MzY4)"
        }
      },
      {
        "id": "UGFja2FnZS01MzE3NTMxMDY4",
        "name": "AutoGen.AzureAIInference",
        "url": "https://github.com/microsoft/autogen/network/dependents?package_id=UGFja2FnZS01MzE3NTMxMDY4",
        "public_dependent_stars": 2,
        "public_dependents": [
          {
            "name": "NuevoFoundation/WorkshopsAutoTranslation",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/40712485?s=40&v=4",
            "owner": "NuevoFoundation",
            "repo_name": "WorkshopsAutoTranslation"
          }
        ],
        "public_dependents_number": 1,
        "private_dependents_number": -1,
        "total_dependents_number": 1,
        "badges": {
          "total": "[![Generated by github-dependents-info](https://img.shields.io/static/v1?label=Used%20by&message=1&color=informational&logo=slickpic)](https://github.com/microsoft/autogen/network/dependents?package_id=UGFja2FnZS01MzE3NTMxMDY4)",
          "public": "[![Generated by github-dependents-info](https://img.shields.io/static/v1?label=Used%20by%20(public)&message=1&color=informational&logo=slickpic)](https://github.com/microsoft/autogen/network/dependents?package_id=UGFja2FnZS01MzE3NTMxMDY4)",
          "private": "[![Generated by github-dependents-info](https://img.shields.io/static/v1?label=Used%20by%20(private)&message=-1&color=informational&logo=slickpic)](https://github.com/microsoft/autogen/network/dependents?package_id=UGFja2FnZS01MzE3NTMxMDY4)",
          "stars": "[![Generated by github-dependents-info](https://img.shields.io/static/v1?label=Used%20by%20(stars)&message=2&color=informational&logo=slickpic)](https://github.com/microsoft/autogen/network/dependents?package_id=UGFja2FnZS01MzE3NTMxMDY4)"
        }
      },
      {
        "id": "UGFja2FnZS01MzAwMTAxMzE5",
        "name": "autogen-agentchat",
        "url": "https://github.com/microsoft/autogen/network/dependents?package_id=UGFja2FnZS01MzAwMTAxMzE5",
        "public_dependent_stars": 1,
        "public_dependents": [
          {
            "name": "microsoft/autogen",
            "stars": 43573,
            "img": "https://avatars.githubusercontent.com/u/6154722?s=40&v=4",
            "owner": "microsoft",
            "repo_name": "autogen"
          },
          {
            "name": "microsoft/ai-agents-for-beginners",
            "stars": 16610,
            "img": "https://avatars.githubusercontent.com/u/6154722?s=40&v=4",
            "owner": "microsoft",
            "repo_name": "ai-agents-for-beginners"
          },
          {
            "name": "unitycatalog/unitycatalog",
            "stars": 2806,
            "img": "https://avatars.githubusercontent.com/u/171874451?s=40&v=4",
            "owner": "unitycatalog",
            "repo_name": "unitycatalog"
          },
          {
            "name": "microsoft/msticpy",
            "stars": 1844,
            "img": "https://avatars.githubusercontent.com/u/6154722?s=40&v=4",
            "owner": "microsoft",
            "repo_name": "msticpy"
          },
          {
            "name": "panaversity/learn-agentic-ai",
            "stars": 1330,
            "img": "https://avatars.githubusercontent.com/u/98460328?s=40&v=4",
            "owner": "panaversity",
            "repo_name": "learn-agentic-ai"
          },
          {
            "name": "EmergenceAI/Agent-E",
            "stars": 1093,
            "img": "https://avatars.githubusercontent.com/u/162732084?s=40&v=4",
            "owner": "EmergenceAI",
            "repo_name": "Agent-E"
          },
          {
            "name": "victordibia/autogen-ui",
            "stars": 905,
            "img": "https://avatars.githubusercontent.com/u/1547007?s=40&v=4",
            "owner": "victordibia",
            "repo_name": "autogen-ui"
          },
          {
            "name": "UKGovernmentBEIS/inspect_ai",
            "stars": 901,
            "img": "https://avatars.githubusercontent.com/u/19221939?s=40&v=4",
            "owner": "UKGovernmentBEIS",
            "repo_name": "inspect_ai"
          },
          {
            "name": "OWASP/www-project-top-10-for-large-language-model-applications",
            "stars": 701,
            "img": "https://avatars.githubusercontent.com/u/155815?s=40&v=4",
            "owner": "OWASP",
            "repo_name": "www-project-top-10-for-large-language-model-applications"
          },
          {
            "name": "kagent-dev/kagent",
            "stars": 588,
            "img": "https://avatars.githubusercontent.com/u/195942545?s=40&v=4",
            "owner": "kagent-dev",
            "repo_name": "kagent"
          },
          {
            "name": "microsoft/AIOpsLab",
            "stars": 582,
            "img": "https://avatars.githubusercontent.com/u/6154722?s=40&v=4",
            "owner": "microsoft",
            "repo_name": "AIOpsLab"
          },
          {
            "name": "microsoft/Trace",
            "stars": 551,
            "img": "https://avatars.githubusercontent.com/u/6154722?s=40&v=4",
            "owner": "microsoft",
            "repo_name": "Trace"
          },
          {
            "name": "microsoft/teams-ai",
            "stars": 524,
            "img": "https://avatars.githubusercontent.com/u/6154722?s=40&v=4",
            "owner": "microsoft",
            "repo_name": "teams-ai"
          },
          {
            "name": "test-zeus-ai/testzeus-hercules",
            "stars": 509,
            "img": "https://avatars.githubusercontent.com/u/178647201?s=40&v=4",
            "owner": "test-zeus-ai",
            "repo_name": "testzeus-hercules"
          },
          {
            "name": "i-am-bee/beeai-platform",
            "stars": 440,
            "img": "https://avatars.githubusercontent.com/u/178592583?s=40&v=4",
            "owner": "i-am-bee",
            "repo_name": "beeai-platform"
          },
          {
            "name": "sambanova/ai-starter-kit",
            "stars": 220,
            "img": "https://avatars.githubusercontent.com/u/80118584?s=40&v=4",
            "owner": "sambanova",
            "repo_name": "ai-starter-kit"
          },
          {
            "name": "microsoft/Multi-Agent-Custom-Automation-Engine-Solution-Accelerator",
            "stars": 180,
            "img": "https://avatars.githubusercontent.com/u/6154722?s=40&v=4",
            "owner": "microsoft",
            "repo_name": "Multi-Agent-Custom-Automation-Engine-Solution-Accelerator"
          },
          {
            "name": "Azure-Samples/dream-team",
            "stars": 178,
            "img": "https://avatars.githubusercontent.com/u/1844662?s=40&v=4",
            "owner": "Azure-Samples",
            "repo_name": "dream-team"
          },
          {
            "name": "Azure-Samples/python-ai-agent-frameworks-demos",
            "stars": 125,
            "img": "https://avatars.githubusercontent.com/u/1844662?s=40&v=4",
            "owner": "Azure-Samples",
            "repo_name": "python-ai-agent-frameworks-demos"
          },
          {
            "name": "victordibia/multiagent-systems-with-autogen",
            "stars": 118,
            "img": "https://avatars.githubusercontent.com/u/1547007?s=40&v=4",
            "owner": "victordibia",
            "repo_name": "multiagent-systems-with-autogen"
          },
          {
            "name": "Azure/gpt-rag-agentic",
            "stars": 79,
            "img": "https://avatars.githubusercontent.com/u/6844498?s=40&v=4",
            "owner": "Azure",
            "repo_name": "gpt-rag-agentic"
          },
          {
            "name": "microsoft/dstoolkit-text2sql-and-imageprocessing",
            "stars": 67,
            "img": "https://avatars.githubusercontent.com/u/6154722?s=40&v=4",
            "owner": "microsoft",
            "repo_name": "dstoolkit-text2sql-and-imageprocessing"
          },
          {
            "name": "wanaku-ai/wanaku",
            "stars": 62,
            "img": "https://avatars.githubusercontent.com/u/200620763?s=40&v=4",
            "owner": "wanaku-ai",
            "repo_name": "wanaku"
          },
          {
            "name": "moxin-org/mofa",
            "stars": 57,
            "img": "https://avatars.githubusercontent.com/u/167464495?s=40&v=4",
            "owner": "moxin-org",
            "repo_name": "mofa"
          },
          {
            "name": "robertZaufall/mindmanager_ai",
            "stars": 52,
            "img": "https://avatars.githubusercontent.com/u/18199228?s=40&v=4",
            "owner": "robertZaufall",
            "repo_name": "mindmanager_ai"
          },
          {
            "name": "agiresearch/Cerebrum",
            "stars": 50,
            "img": "https://avatars.githubusercontent.com/u/130198651?s=40&v=4",
            "owner": "agiresearch",
            "repo_name": "Cerebrum"
          },
          {
            "name": "gnosis/prediction-market-agent",
            "stars": 50,
            "img": "https://avatars.githubusercontent.com/u/24954468?s=40&v=4",
            "owner": "gnosis",
            "repo_name": "prediction-market-agent"
          },
          {
            "name": "zhattention/ticktick-ai",
            "stars": 47,
            "img": "https://avatars.githubusercontent.com/u/7688914?s=40&v=4",
            "owner": "zhattention",
            "repo_name": "ticktick-ai"
          },
          {
            "name": "agntcy/csit",
            "stars": 47,
            "img": "https://avatars.githubusercontent.com/u/197140426?s=40&v=4",
            "owner": "agntcy",
            "repo_name": "csit"
          },
          {
            "name": "Azure/ai-foundry-workshop",
            "stars": 45,
            "img": "https://avatars.githubusercontent.com/u/6844498?s=40&v=4",
            "owner": "Azure",
            "repo_name": "ai-foundry-workshop"
          },
          {
            "name": "microsoft/agdebugger",
            "stars": 38,
            "img": "https://avatars.githubusercontent.com/u/6154722?s=40&v=4",
            "owner": "microsoft",
            "repo_name": "agdebugger"
          },
          {
            "name": "gagb/autogen-yt-agent",
            "stars": 28,
            "img": "https://avatars.githubusercontent.com/u/13227607?s=40&v=4",
            "owner": "gagb",
            "repo_name": "autogen-yt-agent"
          },
          {
            "name": "skyflo-ai/skyflo",
            "stars": 27,
            "img": "https://avatars.githubusercontent.com/u/198904896?s=40&v=4",
            "owner": "skyflo-ai",
            "repo_name": "skyflo"
          },
          {
            "name": "pablosalvador10/gbbai-agent-architecture-lab",
            "stars": 26,
            "img": "https://avatars.githubusercontent.com/u/31255154?s=40&v=4",
            "owner": "pablosalvador10",
            "repo_name": "gbbai-agent-architecture-lab"
          },
          {
            "name": "Wannabeasmartguy/RAGENT",
            "stars": 25,
            "img": "https://avatars.githubusercontent.com/u/107250451?s=40&v=4",
            "owner": "Wannabeasmartguy",
            "repo_name": "RAGENT"
          },
          {
            "name": "EnkrateiaLucca/oreilly_live_training_autogen",
            "stars": 25,
            "img": "https://avatars.githubusercontent.com/u/32417736?s=40&v=4",
            "owner": "EnkrateiaLucca",
            "repo_name": "oreilly_live_training_autogen"
          },
          {
            "name": "IBM/watsonx-developer-hub",
            "stars": 24,
            "img": "https://avatars.githubusercontent.com/u/1459110?s=40&v=4",
            "owner": "IBM",
            "repo_name": "watsonx-developer-hub"
          },
          {
            "name": "Azure-Samples/azure-postgresql-openai-langchain-autogen-demo",
            "stars": 24,
            "img": "https://avatars.githubusercontent.com/u/1844662?s=40&v=4",
            "owner": "Azure-Samples",
            "repo_name": "azure-postgresql-openai-langchain-autogen-demo"
          },
          {
            "name": "denniszielke/agentic-playground",
            "stars": 23,
            "img": "https://avatars.githubusercontent.com/u/11569044?s=40&v=4",
            "owner": "denniszielke",
            "repo_name": "agentic-playground"
          },
          {
            "name": "Azure/agent-innovator-lab",
            "stars": 23,
            "img": "https://avatars.githubusercontent.com/u/6844498?s=40&v=4",
            "owner": "Azure",
            "repo_name": "agent-innovator-lab"
          },
          {
            "name": "cognitivecomputations/agenticworker",
            "stars": 23,
            "img": "https://avatars.githubusercontent.com/u/155425657?s=40&v=4",
            "owner": "cognitivecomputations",
            "repo_name": "agenticworker"
          },
          {
            "name": "zhattention/autogen-toolsmith",
            "stars": 22,
            "img": "https://avatars.githubusercontent.com/u/7688914?s=40&v=4",
            "owner": "zhattention",
            "repo_name": "autogen-toolsmith"
          },
          {
            "name": "amaziahyahalom/aider-model-train",
            "stars": 20,
            "img": "https://avatars.githubusercontent.com/u/192240462?s=40&v=4",
            "owner": "amaziahyahalom",
            "repo_name": "aider-model-train"
          },
          {
            "name": "LxYuan0420/nlp",
            "stars": 19,
            "img": "https://avatars.githubusercontent.com/u/25739658?s=40&v=4",
            "owner": "LxYuan0420",
            "repo_name": "nlp"
          },
          {
            "name": "ababdotai/awesome-agent-quickstart",
            "stars": 18,
            "img": "https://avatars.githubusercontent.com/u/196296006?s=40&v=4",
            "owner": "ababdotai",
            "repo_name": "awesome-agent-quickstart"
          },
          {
            "name": "gxjorge/AI-Agents-Lab",
            "stars": 17,
            "img": "https://avatars.githubusercontent.com/u/3693420?s=40&v=4",
            "owner": "gxjorge",
            "repo_name": "AI-Agents-Lab"
          },
          {
            "name": "modular/max-recipes",
            "stars": 16,
            "img": "https://avatars.githubusercontent.com/u/150859076?s=40&v=4",
            "owner": "modular",
            "repo_name": "max-recipes"
          },
          {
            "name": "wronkiew/autogen-agent-server",
            "stars": 15,
            "img": "https://avatars.githubusercontent.com/u/6463853?s=40&v=4",
            "owner": "wronkiew",
            "repo_name": "autogen-agent-server"
          },
          {
            "name": "Azure-Samples/multi-agent-workshop",
            "stars": 15,
            "img": "https://avatars.githubusercontent.com/u/1844662?s=40&v=4",
            "owner": "Azure-Samples",
            "repo_name": "multi-agent-workshop"
          },
          {
            "name": "SongChiYoung/autogen-oaiapi",
            "stars": 14,
            "img": "https://avatars.githubusercontent.com/u/7540984?s=40&v=4",
            "owner": "SongChiYoung",
            "repo_name": "autogen-oaiapi"
          },
          {
            "name": "msalemor/llm-use-cases",
            "stars": 14,
            "img": "https://avatars.githubusercontent.com/u/44649358?s=40&v=4",
            "owner": "msalemor",
            "repo_name": "llm-use-cases"
          },
          {
            "name": "akshata29/finagent",
            "stars": 13,
            "img": "https://avatars.githubusercontent.com/u/18509807?s=40&v=4",
            "owner": "akshata29",
            "repo_name": "finagent"
          },
          {
            "name": "rh-aiservices-bu/agentic-workshop",
            "stars": 12,
            "img": "https://avatars.githubusercontent.com/u/73964756?s=40&v=4",
            "owner": "rh-aiservices-bu",
            "repo_name": "agentic-workshop"
          },
          {
            "name": "treeleaves30760/verilog-data-generator",
            "stars": 11,
            "img": "https://avatars.githubusercontent.com/u/43369966?s=40&v=4",
            "owner": "treeleaves30760",
            "repo_name": "verilog-data-generator"
          },
          {
            "name": "fbellame/agent-comparison",
            "stars": 11,
            "img": "https://avatars.githubusercontent.com/u/29183055?s=40&v=4",
            "owner": "fbellame",
            "repo_name": "agent-comparison"
          },
          {
            "name": "dragosMC91/AutoGen-Experiments",
            "stars": 11,
            "img": "https://avatars.githubusercontent.com/u/11348201?s=40&v=4",
            "owner": "dragosMC91",
            "repo_name": "AutoGen-Experiments"
          },
          {
            "name": "thirdweb-dev/ai",
            "stars": 10,
            "img": "https://avatars.githubusercontent.com/u/79496167?s=40&v=4",
            "owner": "thirdweb-dev",
            "repo_name": "ai"
          },
          {
            "name": "dspencej/UnscriptedAdventures",
            "stars": 10,
            "img": "https://avatars.githubusercontent.com/u/110250829?s=40&v=4",
            "owner": "dspencej",
            "repo_name": "UnscriptedAdventures"
          },
          {
            "name": "tezansahu/ai-garage",
            "stars": 9,
            "img": "https://avatars.githubusercontent.com/u/31898274?s=40&v=4",
            "owner": "tezansahu",
            "repo_name": "ai-garage"
          },
          {
            "name": "up1/workshop-ai-with-technical-team",
            "stars": 9,
            "img": "https://avatars.githubusercontent.com/u/25113?s=40&v=4",
            "owner": "up1",
            "repo_name": "workshop-ai-with-technical-team"
          },
          {
            "name": "GridfireAI/reddit-mcp",
            "stars": 8,
            "img": "https://avatars.githubusercontent.com/u/204763572?s=40&v=4",
            "owner": "GridfireAI",
            "repo_name": "reddit-mcp"
          },
          {
            "name": "DonnC/pywce",
            "stars": 8,
            "img": "https://avatars.githubusercontent.com/u/47761288?s=40&v=4",
            "owner": "DonnC",
            "repo_name": "pywce"
          },
          {
            "name": "neondatabase-labs/multi-agent-ai-azure-neon-openai",
            "stars": 8,
            "img": "https://avatars.githubusercontent.com/u/183852044?s=40&v=4",
            "owner": "neondatabase-labs",
            "repo_name": "multi-agent-ai-azure-neon-openai"
          },
          {
            "name": "detoxio-ai/ai-red-teaming-training",
            "stars": 8,
            "img": "https://avatars.githubusercontent.com/u/165997948?s=40&v=4",
            "owner": "detoxio-ai",
            "repo_name": "ai-red-teaming-training"
          },
          {
            "name": "Shailender-Youtube/Stock-Analysis-AutoGen-Multi-Agent",
            "stars": 8,
            "img": "https://avatars.githubusercontent.com/u/174622601?s=40&v=4",
            "owner": "Shailender-Youtube",
            "repo_name": "Stock-Analysis-AutoGen-Multi-Agent"
          },
          {
            "name": "MSFT-Innovation-Hub-India/Multi-Agent-autogen-Report-Gen",
            "stars": 8,
            "img": "https://avatars.githubusercontent.com/u/63641889?s=40&v=4",
            "owner": "MSFT-Innovation-Hub-India",
            "repo_name": "Multi-Agent-autogen-Report-Gen"
          },
          {
            "name": "hyssh/azure-openai-quickstart",
            "stars": 8,
            "img": "https://avatars.githubusercontent.com/u/42417311?s=40&v=4",
            "owner": "hyssh",
            "repo_name": "azure-openai-quickstart"
          },
          {
            "name": "foundation-models/MediNoteAI",
            "stars": 7,
            "img": "https://avatars.githubusercontent.com/u/1612939?s=40&v=4",
            "owner": "foundation-models",
            "repo_name": "MediNoteAI"
          },
          {
            "name": "dasiths/llm-plan-and-execute-knowledge-provider-mesh",
            "stars": 7,
            "img": "https://avatars.githubusercontent.com/u/16128850?s=40&v=4",
            "owner": "dasiths",
            "repo_name": "llm-plan-and-execute-knowledge-provider-mesh"
          },
          {
            "name": "jennifermarsman/MultiagentResearch",
            "stars": 7,
            "img": "https://avatars.githubusercontent.com/u/10062762?s=40&v=4",
            "owner": "jennifermarsman",
            "repo_name": "MultiagentResearch"
          },
          {
            "name": "KnoWhiz/DeepTutor",
            "stars": 7,
            "img": "https://avatars.githubusercontent.com/u/112668778?s=40&v=4",
            "owner": "KnoWhiz",
            "repo_name": "DeepTutor"
          },
          {
            "name": "guangying94/magentic-one-autogen-sample",
            "stars": 7,
            "img": "https://avatars.githubusercontent.com/u/12764889?s=40&v=4",
            "owner": "guangying94",
            "repo_name": "magentic-one-autogen-sample"
          },
          {
            "name": "mgfrantz/CTME-llm-lecture-resources",
            "stars": 7,
            "img": "https://avatars.githubusercontent.com/u/18216423?s=40&v=4",
            "owner": "mgfrantz",
            "repo_name": "CTME-llm-lecture-resources"
          },
          {
            "name": "waldiez/jupyter",
            "stars": 7,
            "img": "https://avatars.githubusercontent.com/u/178387055?s=40&v=4",
            "owner": "waldiez",
            "repo_name": "jupyter"
          },
          {
            "name": "kimtth/azure-openai-llm-cookbook",
            "stars": 6,
            "img": "https://avatars.githubusercontent.com/u/13846660?s=40&v=4",
            "owner": "kimtth",
            "repo_name": "azure-openai-llm-cookbook"
          },
          {
            "name": "gswithjeff/autogen-multi-agent-workflow",
            "stars": 6,
            "img": "https://avatars.githubusercontent.com/u/135394694?s=40&v=4",
            "owner": "gswithjeff",
            "repo_name": "autogen-multi-agent-workflow"
          },
          {
            "name": "ziaukhan/shopping-agent",
            "stars": 6,
            "img": "https://avatars.githubusercontent.com/u/545458?s=40&v=4",
            "owner": "ziaukhan",
            "repo_name": "shopping-agent"
          },
          {
            "name": "CantCode023/evoke",
            "stars": 6,
            "img": "https://avatars.githubusercontent.com/u/95113225?s=40&v=4",
            "owner": "CantCode023",
            "repo_name": "evoke"
          },
          {
            "name": "krishsub/MultiagentHackathon",
            "stars": 6,
            "img": "https://avatars.githubusercontent.com/u/17887541?s=40&v=4",
            "owner": "krishsub",
            "repo_name": "MultiagentHackathon"
          },
          {
            "name": "denniszielke/discover-ai",
            "stars": 6,
            "img": "https://avatars.githubusercontent.com/u/11569044?s=40&v=4",
            "owner": "denniszielke",
            "repo_name": "discover-ai"
          },
          {
            "name": "okareo-ai/okareo-python-sdk",
            "stars": 6,
            "img": "https://avatars.githubusercontent.com/u/139079097?s=40&v=4",
            "owner": "okareo-ai",
            "repo_name": "okareo-python-sdk"
          },
          {
            "name": "rominirani/autogen-experiments",
            "stars": 6,
            "img": "https://avatars.githubusercontent.com/u/1614870?s=40&v=4",
            "owner": "rominirani",
            "repo_name": "autogen-experiments"
          },
          {
            "name": "NVISOsecurity/sans-webinar-robocop",
            "stars": 5,
            "img": "https://avatars.githubusercontent.com/u/5310436?s=40&v=4",
            "owner": "NVISOsecurity",
            "repo_name": "sans-webinar-robocop"
          },
          {
            "name": "jplck/from-single-to-multi-agent",
            "stars": 5,
            "img": "https://avatars.githubusercontent.com/u/8427174?s=40&v=4",
            "owner": "jplck",
            "repo_name": "from-single-to-multi-agent"
          },
          {
            "name": "lukaskellerstein/ai",
            "stars": 5,
            "img": "https://avatars.githubusercontent.com/u/12882519?s=40&v=4",
            "owner": "lukaskellerstein",
            "repo_name": "ai"
          },
          {
            "name": "shubhamchau222/AIAgentsjourney",
            "stars": 5,
            "img": "https://avatars.githubusercontent.com/u/61900242?s=40&v=4",
            "owner": "shubhamchau222",
            "repo_name": "AIAgentsjourney"
          },
          {
            "name": "Shailender-Youtube/Autogen-0.4-Mini-Project",
            "stars": 5,
            "img": "https://avatars.githubusercontent.com/u/174622601?s=40&v=4",
            "owner": "Shailender-Youtube",
            "repo_name": "Autogen-0.4-Mini-Project"
          },
          {
            "name": "Squidgy-AI/BoilerPlateV1",
            "stars": 4,
            "img": "https://avatars.githubusercontent.com/u/188782334?s=40&v=4",
            "owner": "Squidgy-AI",
            "repo_name": "BoilerPlateV1"
          },
          {
            "name": "minyang-chen/seat-booking-agent",
            "stars": 4,
            "img": "https://avatars.githubusercontent.com/u/3905747?s=40&v=4",
            "owner": "minyang-chen",
            "repo_name": "seat-booking-agent"
          },
          {
            "name": "knifeyspoony/cogentic",
            "stars": 4,
            "img": "https://avatars.githubusercontent.com/u/7167021?s=40&v=4",
            "owner": "knifeyspoony",
            "repo_name": "cogentic"
          },
          {
            "name": "lukewu8023/agent-core",
            "stars": 4,
            "img": "https://avatars.githubusercontent.com/u/132513928?s=40&v=4",
            "owner": "lukewu8023",
            "repo_name": "agent-core"
          },
          {
            "name": "aniket-work/Lets-Build-Enterprise-Cybersecurity-Risk-Assessment-Using-AI-Agents",
            "stars": 4,
            "img": "https://avatars.githubusercontent.com/u/59799105?s=40&v=4",
            "owner": "aniket-work",
            "repo_name": "Lets-Build-Enterprise-Cybersecurity-Risk-Assessment-Using-AI-Agents"
          },
          {
            "name": "michalmar/dream-team-v2",
            "stars": 4,
            "img": "https://avatars.githubusercontent.com/u/16072588?s=40&v=4",
            "owner": "michalmar",
            "repo_name": "dream-team-v2"
          },
          {
            "name": "santicalavia/TabooGPT",
            "stars": 4,
            "img": "https://avatars.githubusercontent.com/u/82812961?s=40&v=4",
            "owner": "santicalavia",
            "repo_name": "TabooGPT"
          },
          {
            "name": "guyq1997/Jupyter-Assistant",
            "stars": 4,
            "img": "https://avatars.githubusercontent.com/u/111009031?s=40&v=4",
            "owner": "guyq1997",
            "repo_name": "Jupyter-Assistant"
          },
          {
            "name": "kingjulio8238/Oceans7",
            "stars": 4,
            "img": "https://avatars.githubusercontent.com/u/120517860?s=40&v=4",
            "owner": "kingjulio8238",
            "repo_name": "Oceans7"
          },
          {
            "name": "tituslhy/literate-octo-tribble",
            "stars": 4,
            "img": "https://avatars.githubusercontent.com/u/7207877?s=40&v=4",
            "owner": "tituslhy",
            "repo_name": "literate-octo-tribble"
          },
          {
            "name": "realtmxi/mag-review",
            "stars": 3,
            "img": "https://avatars.githubusercontent.com/u/42812404?s=40&v=4",
            "owner": "realtmxi",
            "repo_name": "mag-review"
          },
          {
            "name": "Azure-Samples/AzureAIFoundryWorkshop",
            "stars": 3,
            "img": "https://avatars.githubusercontent.com/u/1844662?s=40&v=4",
            "owner": "Azure-Samples",
            "repo_name": "AzureAIFoundryWorkshop"
          },
          {
            "name": "epoikos-project/simulation",
            "stars": 3,
            "img": "https://avatars.githubusercontent.com/u/200770314?s=40&v=4",
            "owner": "epoikos-project",
            "repo_name": "simulation"
          },
          {
            "name": "mdsiprojects/agenticai",
            "stars": 3,
            "img": "https://avatars.githubusercontent.com/u/73212683?s=40&v=4",
            "owner": "mdsiprojects",
            "repo_name": "agenticai"
          },
          {
            "name": "DoctorKhan/quantum-resonance-ledger",
            "stars": 3,
            "img": "https://avatars.githubusercontent.com/u/7505793?s=40&v=4",
            "owner": "DoctorKhan",
            "repo_name": "quantum-resonance-ledger"
          },
          {
            "name": "yusufk/jarvis-azure",
            "stars": 3,
            "img": "https://avatars.githubusercontent.com/u/404139?s=40&v=4",
            "owner": "yusufk",
            "repo_name": "jarvis-azure"
          },
          {
            "name": "sambanova/integrations",
            "stars": 3,
            "img": "https://avatars.githubusercontent.com/u/80118584?s=40&v=4",
            "owner": "sambanova",
            "repo_name": "integrations"
          },
          {
            "name": "Insight-Services-APAC/Insight_Ingenious",
            "stars": 3,
            "img": "https://avatars.githubusercontent.com/u/75322600?s=40&v=4",
            "owner": "Insight-Services-APAC",
            "repo_name": "Insight_Ingenious"
          },
          {
            "name": "Pykoncio/MentorAI",
            "stars": 3,
            "img": "https://avatars.githubusercontent.com/u/83032163?s=40&v=4",
            "owner": "Pykoncio",
            "repo_name": "MentorAI"
          },
          {
            "name": "MichalRogowski/CVAgent",
            "stars": 3,
            "img": "https://avatars.githubusercontent.com/u/47182107?s=40&v=4",
            "owner": "MichalRogowski",
            "repo_name": "CVAgent"
          },
          {
            "name": "alejbormeg/AutoGen_Swarm",
            "stars": 3,
            "img": "https://avatars.githubusercontent.com/u/61965913?s=40&v=4",
            "owner": "alejbormeg",
            "repo_name": "AutoGen_Swarm"
          },
          {
            "name": "denniszielke/ai-agents-workshop",
            "stars": 3,
            "img": "https://avatars.githubusercontent.com/u/11569044?s=40&v=4",
            "owner": "denniszielke",
            "repo_name": "ai-agents-workshop"
          },
          {
            "name": "huqianghui/azure-ai-agent-workshop",
            "stars": 3,
            "img": "https://avatars.githubusercontent.com/u/7360524?s=40&v=4",
            "owner": "huqianghui",
            "repo_name": "azure-ai-agent-workshop"
          },
          {
            "name": "strand1/ComfyUI-Autogen",
            "stars": 3,
            "img": "https://avatars.githubusercontent.com/u/1580273?s=40&v=4",
            "owner": "strand1",
            "repo_name": "ComfyUI-Autogen"
          },
          {
            "name": "whiteduck-training/wd-ai-hackathon",
            "stars": 3,
            "img": "https://avatars.githubusercontent.com/u/185513507?s=40&v=4",
            "owner": "whiteduck-training",
            "repo_name": "wd-ai-hackathon"
          },
          {
            "name": "rejuve-bio/AI-Assistant",
            "stars": 3,
            "img": "https://avatars.githubusercontent.com/u/164211405?s=40&v=4",
            "owner": "rejuve-bio",
            "repo_name": "AI-Assistant"
          },
          {
            "name": "Azure/auto-kyc",
            "stars": 3,
            "img": "https://avatars.githubusercontent.com/u/6844498?s=40&v=4",
            "owner": "Azure",
            "repo_name": "auto-kyc"
          },
          {
            "name": "alphavector/all",
            "stars": 3,
            "img": "https://avatars.githubusercontent.com/u/11805788?s=40&v=4",
            "owner": "alphavector",
            "repo_name": "all"
          },
          {
            "name": "A-Ravioli/einstein",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/68564296?s=40&v=4",
            "owner": "A-Ravioli",
            "repo_name": "einstein"
          },
          {
            "name": "pasangimhana/sql_agent",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/85233319?s=40&v=4",
            "owner": "pasangimhana",
            "repo_name": "sql_agent"
          },
          {
            "name": "ckane/ctigor",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/3454769?s=40&v=4",
            "owner": "ckane",
            "repo_name": "ctigor"
          },
          {
            "name": "shigeyf/azure-ai-foundry-workshop",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/6917167?s=40&v=4",
            "owner": "shigeyf",
            "repo_name": "azure-ai-foundry-workshop"
          },
          {
            "name": "HosseinZahed/multi-agent-cloud-architect",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/19933353?s=40&v=4",
            "owner": "HosseinZahed",
            "repo_name": "multi-agent-cloud-architect"
          },
          {
            "name": "jjgriff93/autowolf",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/25235950?s=40&v=4",
            "owner": "jjgriff93",
            "repo_name": "autowolf"
          },
          {
            "name": "geteff1/ClinDiag",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/148701415?s=40&v=4",
            "owner": "geteff1",
            "repo_name": "ClinDiag"
          },
          {
            "name": "FrancoisChastel/OpenCustomsGPT",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/10299319?s=40&v=4",
            "owner": "FrancoisChastel",
            "repo_name": "OpenCustomsGPT"
          },
          {
            "name": "Lailiangken/collm04",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/52484757?s=40&v=4",
            "owner": "Lailiangken",
            "repo_name": "collm04"
          },
          {
            "name": "jplck/generic-ai-samples",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/8427174?s=40&v=4",
            "owner": "jplck",
            "repo_name": "generic-ai-samples"
          },
          {
            "name": "marcelmedina/ai-autogen-agent-patterns",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/17131300?s=40&v=4",
            "owner": "marcelmedina",
            "repo_name": "ai-autogen-agent-patterns"
          },
          {
            "name": "amitlals/AI-Financial-Engineering-Agentic-System",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/37605691?s=40&v=4",
            "owner": "amitlals",
            "repo_name": "AI-Financial-Engineering-Agentic-System"
          },
          {
            "name": "kantariyaraj/AI_Agent_Examples",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/6614811?s=40&v=4",
            "owner": "kantariyaraj",
            "repo_name": "AI_Agent_Examples"
          },
          {
            "name": "trojrobert/web_agent",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/28733978?s=40&v=4",
            "owner": "trojrobert",
            "repo_name": "web_agent"
          },
          {
            "name": "weinaike/agentflow",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/13814354?s=40&v=4",
            "owner": "weinaike",
            "repo_name": "agentflow"
          },
          {
            "name": "jspv/mchat",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/6540751?s=40&v=4",
            "owner": "jspv",
            "repo_name": "mchat"
          },
          {
            "name": "kinshu0/trading-zoo",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/67475292?s=40&v=4",
            "owner": "kinshu0",
            "repo_name": "trading-zoo"
          },
          {
            "name": "morietschel/agentic-3d",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/42514380?s=40&v=4",
            "owner": "morietschel",
            "repo_name": "agentic-3d"
          },
          {
            "name": "StefanDevstar/awesome-llm-apps",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/202806902?s=40&v=4",
            "owner": "StefanDevstar",
            "repo_name": "awesome-llm-apps"
          },
          {
            "name": "fajarmh-mf/ai-agents-frameworks",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/186990157?s=40&v=4",
            "owner": "fajarmh-mf",
            "repo_name": "ai-agents-frameworks"
          },
          {
            "name": "saytyarnorngloreia/ai-agents-for-beginners",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/205020882?s=40&v=4",
            "owner": "saytyarnorngloreia",
            "repo_name": "ai-agents-for-beginners"
          },
          {
            "name": "santhoshrao95/mulit_agent_it_support_system",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/27264958?s=40&v=4",
            "owner": "santhoshrao95",
            "repo_name": "mulit_agent_it_support_system"
          },
          {
            "name": "IamMikeHelsel/sneks",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/51098787?s=40&v=4",
            "owner": "IamMikeHelsel",
            "repo_name": "sneks"
          },
          {
            "name": "NortonHuang-SUFE/FortuneTell",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/54016596?s=40&v=4",
            "owner": "NortonHuang-SUFE",
            "repo_name": "FortuneTell"
          },
          {
            "name": "CyberCRI/WeLearn-api",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/3897500?s=40&v=4",
            "owner": "CyberCRI",
            "repo_name": "WeLearn-api"
          },
          {
            "name": "msalemor/agentic-intro-autogen",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/44649358?s=40&v=4",
            "owner": "msalemor",
            "repo_name": "agentic-intro-autogen"
          },
          {
            "name": "micchyboy237/JetScripts",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/42490977?s=40&v=4",
            "owner": "micchyboy237",
            "repo_name": "JetScripts"
          },
          {
            "name": "AskTheApi/asktheapi-team-builder",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/206834780?s=40&v=4",
            "owner": "AskTheApi",
            "repo_name": "asktheapi-team-builder"
          },
          {
            "name": "DellDi/autogen-fastapi-postgres",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/40460351?s=40&v=4",
            "owner": "DellDi",
            "repo_name": "autogen-fastapi-postgres"
          },
          {
            "name": "arashaga/agents-hackathon",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/1166344?s=40&v=4",
            "owner": "arashaga",
            "repo_name": "agents-hackathon"
          },
          {
            "name": "hinthornw/mabench",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/13333726?s=40&v=4",
            "owner": "hinthornw",
            "repo_name": "mabench"
          },
          {
            "name": "chepyle/multiagent-demo",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/6445835?s=40&v=4",
            "owner": "chepyle",
            "repo_name": "multiagent-demo"
          },
          {
            "name": "digital-duck/st_tools",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/145862907?s=40&v=4",
            "owner": "digital-duck",
            "repo_name": "st_tools"
          },
          {
            "name": "sitloboi2012/AMS",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/33688887?s=40&v=4",
            "owner": "sitloboi2012",
            "repo_name": "AMS"
          },
          {
            "name": "black-da-bull/autogen",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/110015660?s=40&v=4",
            "owner": "black-da-bull",
            "repo_name": "autogen"
          },
          {
            "name": "isaccanedo/awesome-llm-apps",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/32867124?s=40&v=4",
            "owner": "isaccanedo",
            "repo_name": "awesome-llm-apps"
          },
          {
            "name": "kohei3110/ai-tour-2025-agent-demo",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/38392404?s=40&v=4",
            "owner": "kohei3110",
            "repo_name": "ai-tour-2025-agent-demo"
          },
          {
            "name": "verwec/03_2025_ai_workshop",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/6060259?s=40&v=4",
            "owner": "verwec",
            "repo_name": "03_2025_ai_workshop"
          },
          {
            "name": "skillrepos/ai-agents",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/82792046?s=40&v=4",
            "owner": "skillrepos",
            "repo_name": "ai-agents"
          },
          {
            "name": "vortezwohl/experiment-03-22-2025",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/117743023?s=40&v=4",
            "owner": "vortezwohl",
            "repo_name": "experiment-03-22-2025"
          },
          {
            "name": "dheerapat/autogen-agent-demo",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/61280196?s=40&v=4",
            "owner": "dheerapat",
            "repo_name": "autogen-agent-demo"
          },
          {
            "name": "Azure/multiagent-frameworks",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/6844498?s=40&v=4",
            "owner": "Azure",
            "repo_name": "multiagent-frameworks"
          },
          {
            "name": "FranklineMisango/Quant_Research_Assistant",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/95913228?s=40&v=4",
            "owner": "FranklineMisango",
            "repo_name": "Quant_Research_Assistant"
          },
          {
            "name": "flosrv/Awesome-LLM-Apps-Shubham-Saboo",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/139999976?s=40&v=4",
            "owner": "flosrv",
            "repo_name": "Awesome-LLM-Apps-Shubham-Saboo"
          },
          {
            "name": "Zoyaaaaaaa/Market-Research-AI-Agents",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/126981793?s=40&v=4",
            "owner": "Zoyaaaaaaa",
            "repo_name": "Market-Research-AI-Agents"
          },
          {
            "name": "RostKoff/montage-horde",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/119884204?s=40&v=4",
            "owner": "RostKoff",
            "repo_name": "montage-horde"
          },
          {
            "name": "Delahayecarry/MAAI",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/163613131?s=40&v=4",
            "owner": "Delahayecarry",
            "repo_name": "MAAI"
          },
          {
            "name": "rajesh-nitc/autogen-backend",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/44093301?s=40&v=4",
            "owner": "rajesh-nitc",
            "repo_name": "autogen-backend"
          },
          {
            "name": "ciesko/aether",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/9701202?s=40&v=4",
            "owner": "ciesko",
            "repo_name": "aether"
          },
          {
            "name": "memenow/persona-agent",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/142292623?s=40&v=4",
            "owner": "memenow",
            "repo_name": "persona-agent"
          },
          {
            "name": "marcelmedina/ai-autogen-agents",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/17131300?s=40&v=4",
            "owner": "marcelmedina",
            "repo_name": "ai-autogen-agents"
          },
          {
            "name": "yogeshkc/ai-lets-go",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/12978461?s=40&v=4",
            "owner": "yogeshkc",
            "repo_name": "ai-lets-go"
          },
          {
            "name": "XinmiaoYan/ai-agents-for-beginners",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/83053884?s=40&v=4",
            "owner": "XinmiaoYan",
            "repo_name": "ai-agents-for-beginners"
          },
          {
            "name": "arindamhazramsft/AZ-AI-Agent-Service-Demos",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/123211674?s=40&v=4",
            "owner": "arindamhazramsft",
            "repo_name": "AZ-AI-Agent-Service-Demos"
          },
          {
            "name": "ArthurVerrez/autogen-streamlit-hackernews-demo",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/33450143?s=40&v=4",
            "owner": "ArthurVerrez",
            "repo_name": "autogen-streamlit-hackernews-demo"
          },
          {
            "name": "anitha2017/agenticai-autogen-gemini-demo",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/26123698?s=40&v=4",
            "owner": "anitha2017",
            "repo_name": "agenticai-autogen-gemini-demo"
          },
          {
            "name": "pmajor74/demo.autogen_meal_blogwriter",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/52909235?s=40&v=4",
            "owner": "pmajor74",
            "repo_name": "demo.autogen_meal_blogwriter"
          },
          {
            "name": "luiselopezlopez/globalai2025",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/97617267?s=40&v=4",
            "owner": "luiselopezlopez",
            "repo_name": "globalai2025"
          },
          {
            "name": "fuxin123z/autogen_mcp",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/60996067?s=40&v=4",
            "owner": "fuxin123z",
            "repo_name": "autogen_mcp"
          },
          {
            "name": "CantCode023/bloggit",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/95113225?s=40&v=4",
            "owner": "CantCode023",
            "repo_name": "bloggit"
          },
          {
            "name": "prabdeb/autogen-m1-security-sample",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/37529789?s=40&v=4",
            "owner": "prabdeb",
            "repo_name": "autogen-m1-security-sample"
          },
          {
            "name": "cwijayasundara/multi_agentic_research",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/3097200?s=40&v=4",
            "owner": "cwijayasundara",
            "repo_name": "multi_agentic_research"
          },
          {
            "name": "wwwsapir/SoftwareAgents",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/16436675?s=40&v=4",
            "owner": "wwwsapir",
            "repo_name": "SoftwareAgents"
          },
          {
            "name": "anshumankmr/sporky",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/24219264?s=40&v=4",
            "owner": "anshumankmr",
            "repo_name": "sporky"
          },
          {
            "name": "tejajuttu/Multi-Agent-Conversations",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/25482892?s=40&v=4",
            "owner": "tejajuttu",
            "repo_name": "Multi-Agent-Conversations"
          },
          {
            "name": "jakeatmsft/fsi-ai-agent-hackathon",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/47987698?s=40&v=4",
            "owner": "jakeatmsft",
            "repo_name": "fsi-ai-agent-hackathon"
          },
          {
            "name": "rohanmuz2/autogen-blog-post-creator",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/28555930?s=40&v=4",
            "owner": "rohanmuz2",
            "repo_name": "autogen-blog-post-creator"
          },
          {
            "name": "AliciaSJF/autogen",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/132434651?s=40&v=4",
            "owner": "AliciaSJF",
            "repo_name": "autogen"
          },
          {
            "name": "kaljuvee/agentic",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/539240?s=40&v=4",
            "owner": "kaljuvee",
            "repo_name": "agentic"
          },
          {
            "name": "benediktstroebl/agent-zoo",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/50178209?s=40&v=4",
            "owner": "benediktstroebl",
            "repo_name": "agent-zoo"
          },
          {
            "name": "mark-watson/OllamaExamples",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/33912?s=40&v=4",
            "owner": "mark-watson",
            "repo_name": "OllamaExamples"
          },
          {
            "name": "kenilp7/Meal-Planning-AI-Assistants",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/108230926?s=40&v=4",
            "owner": "kenilp7",
            "repo_name": "Meal-Planning-AI-Assistants"
          },
          {
            "name": "rustatian/airope",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/8040338?s=40&v=4",
            "owner": "rustatian",
            "repo_name": "airope"
          },
          {
            "name": "codeh007/mtmai",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/95907738?s=40&v=4",
            "owner": "codeh007",
            "repo_name": "mtmai"
          },
          {
            "name": "thedigitalworkplace/Autogen",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/133930737?s=40&v=4",
            "owner": "thedigitalworkplace",
            "repo_name": "Autogen"
          },
          {
            "name": "DanielPuentee/autogen-groq-multiagent-analyser",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/64250734?s=40&v=4",
            "owner": "DanielPuentee",
            "repo_name": "autogen-groq-multiagent-analyser"
          },
          {
            "name": "DimohaZadira/ai-agent-start",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/33756112?s=40&v=4",
            "owner": "DimohaZadira",
            "repo_name": "ai-agent-start"
          },
          {
            "name": "halilural/multi-model-ai-agents",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/12976755?s=40&v=4",
            "owner": "halilural",
            "repo_name": "multi-model-ai-agents"
          },
          {
            "name": "rinormaloku/autogen-langchain-mcp-mix",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/9026451?s=40&v=4",
            "owner": "rinormaloku",
            "repo_name": "autogen-langchain-mcp-mix"
          },
          {
            "name": "lgcorzo/llmops-python-package",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/46710567?s=40&v=4",
            "owner": "lgcorzo",
            "repo_name": "llmops-python-package"
          },
          {
            "name": "TeamADAPT/AgentStack",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/181675239?s=40&v=4",
            "owner": "TeamADAPT",
            "repo_name": "AgentStack"
          },
          {
            "name": "niyathnair/NcluxhAI",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/102501152?s=40&v=4",
            "owner": "niyathnair",
            "repo_name": "NcluxhAI"
          },
          {
            "name": "vvalac/ai-experiments",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/48455003?s=40&v=4",
            "owner": "vvalac",
            "repo_name": "ai-experiments"
          },
          {
            "name": "sillysavant/e-boyfriend",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/64178919?s=40&v=4",
            "owner": "sillysavant",
            "repo_name": "e-boyfriend"
          },
          {
            "name": "ghchen99/agentic-conversations",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/56446026?s=40&v=4",
            "owner": "ghchen99",
            "repo_name": "agentic-conversations"
          },
          {
            "name": "graphlit/AgentStack",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/130105661?s=40&v=4",
            "owner": "graphlit",
            "repo_name": "AgentStack"
          },
          {
            "name": "vccjlc/time_machine",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/140386750?s=40&v=4",
            "owner": "vccjlc",
            "repo_name": "time_machine"
          },
          {
            "name": "knd3dayo/AIChatExplorer",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/157713558?s=40&v=4",
            "owner": "knd3dayo",
            "repo_name": "AIChatExplorer"
          },
          {
            "name": "r-bit-rry/magentic-two",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/34023431?s=40&v=4",
            "owner": "r-bit-rry",
            "repo_name": "magentic-two"
          },
          {
            "name": "1012638836/ai_qiniu_chatbot",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/29818043?s=40&v=4",
            "owner": "1012638836",
            "repo_name": "ai_qiniu_chatbot"
          },
          {
            "name": "shreyanavelkar/automated-ai-assistant",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/28059200?s=40&v=4",
            "owner": "shreyanavelkar",
            "repo_name": "automated-ai-assistant"
          },
          {
            "name": "missingpages/AI-Teacher",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/65895277?s=40&v=4",
            "owner": "missingpages",
            "repo_name": "AI-Teacher"
          },
          {
            "name": "parth-verma7/agentic-ai-task",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/96720577?s=40&v=4",
            "owner": "parth-verma7",
            "repo_name": "agentic-ai-task"
          },
          {
            "name": "richard-gyiko/aura",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/36221820?s=40&v=4",
            "owner": "richard-gyiko",
            "repo_name": "aura"
          },
          {
            "name": "CalebJKim/DataPilot",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/40844938?s=40&v=4",
            "owner": "CalebJKim",
            "repo_name": "DataPilot"
          },
          {
            "name": "DhruvvArora/pulseai",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/98960322?s=40&v=4",
            "owner": "DhruvvArora",
            "repo_name": "pulseai"
          },
          {
            "name": "denniszielke/autogen-playground",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/11569044?s=40&v=4",
            "owner": "denniszielke",
            "repo_name": "autogen-playground"
          },
          {
            "name": "sanggi-wjg/ai-demos",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/24692394?s=40&v=4",
            "owner": "sanggi-wjg",
            "repo_name": "ai-demos"
          },
          {
            "name": "tikendraw/jobber",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/68785366?s=40&v=4",
            "owner": "tikendraw",
            "repo_name": "jobber"
          },
          {
            "name": "eugeneVzaidun/autogen",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/72197489?s=40&v=4",
            "owner": "eugeneVzaidun",
            "repo_name": "autogen"
          },
          {
            "name": "earzamastsev/llm-intro-course",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/5910423?s=40&v=4",
            "owner": "earzamastsev",
            "repo_name": "llm-intro-course"
          },
          {
            "name": "INTEND-Project/inSwitch",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/169916319?s=40&v=4",
            "owner": "INTEND-Project",
            "repo_name": "inSwitch"
          },
          {
            "name": "zefang-liu/AutoBnB",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/30639033?s=40&v=4",
            "owner": "zefang-liu",
            "repo_name": "AutoBnB"
          },
          {
            "name": "erinjerri/foodfinder_rag_app",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/3184381?s=40&v=4",
            "owner": "erinjerri",
            "repo_name": "foodfinder_rag_app"
          },
          {
            "name": "imankamyabi/autogen-fargate-executor",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/2341221?s=40&v=4",
            "owner": "imankamyabi",
            "repo_name": "autogen-fargate-executor"
          },
          {
            "name": "nowucca/aitools-autogen",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/1035271?s=40&v=4",
            "owner": "nowucca",
            "repo_name": "aitools-autogen"
          },
          {
            "name": "scenaristeur/smag-studio",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/4020744?s=40&v=4",
            "owner": "scenaristeur",
            "repo_name": "smag-studio"
          },
          {
            "name": "didw/RAG_daou",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/7973071?s=40&v=4",
            "owner": "didw",
            "repo_name": "RAG_daou"
          },
          {
            "name": "Bgon01/wolfLM",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/86642839?s=40&v=4",
            "owner": "Bgon01",
            "repo_name": "wolfLM"
          },
          {
            "name": "cenrak/ml-epiphany",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/762378?s=40&v=4",
            "owner": "cenrak",
            "repo_name": "ml-epiphany"
          },
          {
            "name": "tuanvo-ai/lab01-llm",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/185315429?s=40&v=4",
            "owner": "tuanvo-ai",
            "repo_name": "lab01-llm"
          },
          {
            "name": "LittleLittleCloud/Develop_Agent_With_AutoGen",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/16876986?s=40&v=4",
            "owner": "LittleLittleCloud",
            "repo_name": "Develop_Agent_With_AutoGen"
          },
          {
            "name": "sundai-club/automated_data_scientist",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/156603435?s=40&v=4",
            "owner": "sundai-club",
            "repo_name": "automated_data_scientist"
          },
          {
            "name": "Adithyan777/code-catalyst",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/63772248?s=40&v=4",
            "owner": "Adithyan777",
            "repo_name": "code-catalyst"
          },
          {
            "name": "atiq-zangoh/Welcome-Agent",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/184043077?s=40&v=4",
            "owner": "atiq-zangoh",
            "repo_name": "Welcome-Agent"
          },
          {
            "name": "atiq-zangoh/Visitors-AI-Assistant",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/184043077?s=40&v=4",
            "owner": "atiq-zangoh",
            "repo_name": "Visitors-AI-Assistant"
          },
          {
            "name": "patooworld/autogen",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/114192133?s=40&v=4",
            "owner": "patooworld",
            "repo_name": "autogen"
          }
        ],
        "public_dependents_number": 232,
        "private_dependents_number": -232,
        "total_dependents_number": 232,
        "badges": {
          "total": "[![Generated by github-dependents-info](https://img.shields.io/static/v1?label=Used%20by&message=232&color=informational&logo=slickpic)](https://github.com/microsoft/autogen/network/dependents?package_id=UGFja2FnZS01MzAwMTAxMzE5)",
          "public": "[![Generated by github-dependents-info](https://img.shields.io/static/v1?label=Used%20by%20(public)&message=232&color=informational&logo=slickpic)](https://github.com/microsoft/autogen/network/dependents?package_id=UGFja2FnZS01MzAwMTAxMzE5)",
          "private": "[![Generated by github-dependents-info](https://img.shields.io/static/v1?label=Used%20by%20(private)&message=-232&color=informational&logo=slickpic)](https://github.com/microsoft/autogen/network/dependents?package_id=UGFja2FnZS01MzAwMTAxMzE5)",
          "stars": "[![Generated by github-dependents-info](https://img.shields.io/static/v1?label=Used%20by%20(stars)&message=1&color=informational&logo=slickpic)](https://github.com/microsoft/autogen/network/dependents?package_id=UGFja2FnZS01MzAwMTAxMzE5)"
        }
      },
      {
        "id": "UGFja2FnZS00OTM5Njg2NzU4",
        "name": "AutoGen.Gemini",
        "url": "https://github.com/microsoft/autogen/network/dependents?package_id=UGFja2FnZS00OTM5Njg2NzU4",
        "public_dependent_stars": 1,
        "public_dependents": [
          {
            "name": "opazenha/csharp-autogen",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/172253505?s=40&v=4",
            "owner": "opazenha",
            "repo_name": "csharp-autogen"
          }
        ],
        "public_dependents_number": 1,
        "private_dependents_number": -1,
        "total_dependents_number": 1,
        "badges": {
          "total": "[![Generated by github-dependents-info](https://img.shields.io/static/v1?label=Used%20by&message=1&color=informational&logo=slickpic)](https://github.com/microsoft/autogen/network/dependents?package_id=UGFja2FnZS00OTM5Njg2NzU4)",
          "public": "[![Generated by github-dependents-info](https://img.shields.io/static/v1?label=Used%20by%20(public)&message=1&color=informational&logo=slickpic)](https://github.com/microsoft/autogen/network/dependents?package_id=UGFja2FnZS00OTM5Njg2NzU4)",
          "private": "[![Generated by github-dependents-info](https://img.shields.io/static/v1?label=Used%20by%20(private)&message=-1&color=informational&logo=slickpic)](https://github.com/microsoft/autogen/network/dependents?package_id=UGFja2FnZS00OTM5Njg2NzU4)",
          "stars": "[![Generated by github-dependents-info](https://img.shields.io/static/v1?label=Used%20by%20(stars)&message=1&color=informational&logo=slickpic)](https://github.com/microsoft/autogen/network/dependents?package_id=UGFja2FnZS00OTM5Njg2NzU4)"
        }
      },
      {
        "id": "UGFja2FnZS00ODA3ODEyMjA2",
        "name": "AutoGen.Anthropic",
        "url": "https://github.com/microsoft/autogen/network/dependents?package_id=UGFja2FnZS00ODA3ODEyMjA2",
        "public_dependent_stars": 0,
        "public_dependents": [],
        "public_dependents_number": 0,
        "private_dependents_number": 0,
        "total_dependents_number": 0,
        "badges": {
          "total": "[![Generated by github-dependents-info](https://img.shields.io/static/v1?label=Used%20by&message=0&color=informational&logo=slickpic)](https://github.com/microsoft/autogen/network/dependents?package_id=UGFja2FnZS00ODA3ODEyMjA2)",
          "public": "[![Generated by github-dependents-info](https://img.shields.io/static/v1?label=Used%20by%20(public)&message=0&color=informational&logo=slickpic)](https://github.com/microsoft/autogen/network/dependents?package_id=UGFja2FnZS00ODA3ODEyMjA2)",
          "private": "[![Generated by github-dependents-info](https://img.shields.io/static/v1?label=Used%20by%20(private)&message=0&color=informational&logo=slickpic)](https://github.com/microsoft/autogen/network/dependents?package_id=UGFja2FnZS00ODA3ODEyMjA2)",
          "stars": "[![Generated by github-dependents-info](https://img.shields.io/static/v1?label=Used%20by%20(stars)&message=0&color=informational&logo=slickpic)](https://github.com/microsoft/autogen/network/dependents?package_id=UGFja2FnZS00ODA3ODEyMjA2)"
        }
      },
      {
        "id": "UGFja2FnZS00NzUwMjA5NDQ5",
        "name": "AutoGen.Mistral",
        "url": "https://github.com/microsoft/autogen/network/dependents?package_id=UGFja2FnZS00NzUwMjA5NDQ5",
        "public_dependent_stars": 0,
        "public_dependents": [],
        "public_dependents_number": 0,
        "private_dependents_number": 0,
        "total_dependents_number": 0,
        "badges": {
          "total": "[![Generated by github-dependents-info](https://img.shields.io/static/v1?label=Used%20by&message=0&color=informational&logo=slickpic)](https://github.com/microsoft/autogen/network/dependents?package_id=UGFja2FnZS00NzUwMjA5NDQ5)",
          "public": "[![Generated by github-dependents-info](https://img.shields.io/static/v1?label=Used%20by%20(public)&message=0&color=informational&logo=slickpic)](https://github.com/microsoft/autogen/network/dependents?package_id=UGFja2FnZS00NzUwMjA5NDQ5)",
          "private": "[![Generated by github-dependents-info](https://img.shields.io/static/v1?label=Used%20by%20(private)&message=0&color=informational&logo=slickpic)](https://github.com/microsoft/autogen/network/dependents?package_id=UGFja2FnZS00NzUwMjA5NDQ5)",
          "stars": "[![Generated by github-dependents-info](https://img.shields.io/static/v1?label=Used%20by%20(stars)&message=0&color=informational&logo=slickpic)](https://github.com/microsoft/autogen/network/dependents?package_id=UGFja2FnZS00NzUwMjA5NDQ5)"
        }
      },
      {
        "id": "UGFja2FnZS01MzUyOTMzNTI1",
        "name": "AutoGen.OpenAI.V1",
        "url": "https://github.com/microsoft/autogen/network/dependents?package_id=UGFja2FnZS01MzUyOTMzNTI1",
        "public_dependent_stars": 0,
        "public_dependents": [],
        "public_dependents_number": 0,
        "private_dependents_number": 0,
        "total_dependents_number": 0,
        "badges": {
          "total": "[![Generated by github-dependents-info](https://img.shields.io/static/v1?label=Used%20by&message=0&color=informational&logo=slickpic)](https://github.com/microsoft/autogen/network/dependents?package_id=UGFja2FnZS01MzUyOTMzNTI1)",
          "public": "[![Generated by github-dependents-info](https://img.shields.io/static/v1?label=Used%20by%20(public)&message=0&color=informational&logo=slickpic)](https://github.com/microsoft/autogen/network/dependents?package_id=UGFja2FnZS01MzUyOTMzNTI1)",
          "private": "[![Generated by github-dependents-info](https://img.shields.io/static/v1?label=Used%20by%20(private)&message=0&color=informational&logo=slickpic)](https://github.com/microsoft/autogen/network/dependents?package_id=UGFja2FnZS01MzUyOTMzNTI1)",
          "stars": "[![Generated by github-dependents-info](https://img.shields.io/static/v1?label=Used%20by%20(stars)&message=0&color=informational&logo=slickpic)](https://github.com/microsoft/autogen/network/dependents?package_id=UGFja2FnZS01MzUyOTMzNTI1)"
        }
      },
      {
        "id": "UGFja2FnZS01MTgwOTAzMDMw",
        "name": "AutoGen.WebAPI",
        "url": "https://github.com/microsoft/autogen/network/dependents?package_id=UGFja2FnZS01MTgwOTAzMDMw",
        "public_dependent_stars": 0,
        "public_dependents": [],
        "public_dependents_number": 0,
        "private_dependents_number": 0,
        "total_dependents_number": 0,
        "badges": {
          "total": "[![Generated by github-dependents-info](https://img.shields.io/static/v1?label=Used%20by&message=0&color=informational&logo=slickpic)](https://github.com/microsoft/autogen/network/dependents?package_id=UGFja2FnZS01MTgwOTAzMDMw)",
          "public": "[![Generated by github-dependents-info](https://img.shields.io/static/v1?label=Used%20by%20(public)&message=0&color=informational&logo=slickpic)](https://github.com/microsoft/autogen/network/dependents?package_id=UGFja2FnZS01MTgwOTAzMDMw)",
          "private": "[![Generated by github-dependents-info](https://img.shields.io/static/v1?label=Used%20by%20(private)&message=0&color=informational&logo=slickpic)](https://github.com/microsoft/autogen/network/dependents?package_id=UGFja2FnZS01MTgwOTAzMDMw)",
          "stars": "[![Generated by github-dependents-info](https://img.shields.io/static/v1?label=Used%20by%20(stars)&message=0&color=informational&logo=slickpic)](https://github.com/microsoft/autogen/network/dependents?package_id=UGFja2FnZS01MTgwOTAzMDMw)"
        }
      },
      {
        "id": "UGFja2FnZS00MTAxNTA1MzEw",
        "name": "github.com/microsoft/autogen",
        "url": "https://github.com/microsoft/autogen/network/dependents?package_id=UGFja2FnZS00MTAxNTA1MzEw",
        "public_dependent_stars": 0,
        "public_dependents": [],
        "public_dependents_number": 0,
        "private_dependents_number": 0,
        "total_dependents_number": 0,
        "badges": {
          "total": "[![Generated by github-dependents-info](https://img.shields.io/static/v1?label=Used%20by&message=0&color=informational&logo=slickpic)](https://github.com/microsoft/autogen/network/dependents?package_id=UGFja2FnZS00MTAxNTA1MzEw)",
          "public": "[![Generated by github-dependents-info](https://img.shields.io/static/v1?label=Used%20by%20(public)&message=0&color=informational&logo=slickpic)](https://github.com/microsoft/autogen/network/dependents?package_id=UGFja2FnZS00MTAxNTA1MzEw)",
          "private": "[![Generated by github-dependents-info](https://img.shields.io/static/v1?label=Used%20by%20(private)&message=0&color=informational&logo=slickpic)](https://github.com/microsoft/autogen/network/dependents?package_id=UGFja2FnZS00MTAxNTA1MzEw)",
          "stars": "[![Generated by github-dependents-info](https://img.shields.io/static/v1?label=Used%20by%20(stars)&message=0&color=informational&logo=slickpic)](https://github.com/microsoft/autogen/network/dependents?package_id=UGFja2FnZS00MTAxNTA1MzEw)"
        }
      }
    ]
  ]
}