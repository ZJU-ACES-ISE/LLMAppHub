{
  "all_public_dependent_repos": [
    {
      "name": "geekan/MetaGPT",
      "stars": 54895,
      "img": "https://avatars.githubusercontent.com/u/2707039?s=40&v=4",
      "owner": "geekan",
      "repo_name": "MetaGPT",
      "description": "🌟 The Multi-Agent Framework: First AI Software Company, Towards Natural Language Programming",
      "homepage": "https://mgx.dev/",
      "language": "Python",
      "created_at": "2023-06-30T09:04:55Z",
      "updated_at": "2025-04-23T10:20:31Z",
      "topics": [
        "agent",
        "gpt",
        "llm",
        "metagpt",
        "multi-agent"
      ],
      "readme": "\n# MetaGPT: The Multi-Agent Framework\n\n<p align=\"center\">\n<a href=\"\"><img src=\"docs/resources/MetaGPT-new-log.png\" alt=\"MetaGPT logo: Enable GPT to work in a software company, collaborating to tackle more complex tasks.\" width=\"150px\"></a>\n</p>\n\n<p align=\"center\">\n[ <b>En</b> |\n<a href=\"docs/README_CN.md\">中</a> |\n<a href=\"docs/README_FR.md\">Fr</a> |\n<a href=\"docs/README_JA.md\">日</a> ]\n<b>Assign different roles to GPTs to form a collaborative entity for complex tasks.</b>\n</p>\n\n<p align=\"center\">\n<a href=\"https://opensource.org/licenses/MIT\"><img src=\"https://img.shields.io/badge/License-MIT-blue.svg\" alt=\"License: MIT\"></a>\n<a href=\"https://discord.gg/DYn29wFk9z\"><img src=\"https://dcbadge.vercel.app/api/server/DYn29wFk9z?style=flat\" alt=\"Discord Follow\"></a>\n<a href=\"https://twitter.com/MetaGPT_\"><img src=\"https://img.shields.io/twitter/follow/MetaGPT?style=social\" alt=\"Twitter Follow\"></a>\n</p>\n\n<h4 align=\"center\">\n    \n</h4>\n\n## News\n\n🚀 Mar. 10, 2025: 🎉 [mgx.dev](https://mgx.dev/) is the #1 Product of the Week on @ProductHunt! 🏆\n\n🚀 Mar. &nbsp; 4, 2025: 🎉 [mgx.dev](https://mgx.dev/) is the #1 Product of the Day on @ProductHunt! 🏆\n\n🚀 Feb. 19, 2025: Today we are officially launching our natural language programming product: [MGX (MetaGPT X)](https://mgx.dev/) - the world's first AI agent development team. More details on [Twitter](https://x.com/MetaGPT_/status/1892199535130329356).\n\n🚀 Feb. 17, 2025: We introduced two papers: [SPO](https://arxiv.org/pdf/2502.06855) and [AOT](https://arxiv.org/pdf/2502.12018), check the [code](examples)!\n\n🚀 Jan. 22, 2025: Our paper [AFlow: Automating Agentic Workflow Generation](https://openreview.net/forum?id=z5uVAKwmjf) accepted for **oral presentation (top 1.8%)** at ICLR 2025, **ranking #2** in the LLM-based Agent category.\n\n👉👉 [Earlier news](docs/NEWS.md) \n\n## Software Company as Multi-Agent System\n\n1. MetaGPT takes a **one line requirement** as input and outputs **user stories / competitive analysis / requirements / data structures / APIs / documents, etc.**\n2. Internally, MetaGPT includes **product managers / architects / project managers / engineers.** It provides the entire process of a **software company along with carefully orchestrated SOPs.**\n   1. `Code = SOP(Team)` is the core philosophy. We materialize SOP and apply it to teams composed of LLMs.\n\n![A software company consists of LLM-based roles](docs/resources/software_company_cd.jpeg)\n\n<p align=\"center\">Software Company Multi-Agent Schematic (Gradually Implementing)</p>\n\n## Get Started\n\n### Installation\n\n> Ensure that Python 3.9 or later, but less than 3.12, is installed on your system. You can check this by using: `python --version`.  \n> You can use conda like this: `conda create -n metagpt python=3.9 && conda activate metagpt`\n\n```bash\npip install --upgrade metagpt\n# or `pip install --upgrade git+https://github.com/geekan/MetaGPT.git`\n# or `git clone https://github.com/geekan/MetaGPT && cd MetaGPT && pip install --upgrade -e .`\n```\n\n**Install [node](https://nodejs.org/en/download) and [pnpm](https://pnpm.io/installation#using-npm) before actual use.**\n\nFor detailed installation guidance, please refer to [cli_install](https://docs.deepwisdom.ai/main/en/guide/get_started/installation.html#install-stable-version)\n or [docker_install](https://docs.deepwisdom.ai/main/en/guide/get_started/installation.html#install-with-docker)\n\n### Configuration\n\nYou can init the config of MetaGPT by running the following command, or manually create `~/.metagpt/config2.yaml` file:\n```bash\n# Check https://docs.deepwisdom.ai/main/en/guide/get_started/configuration.html for more details\nmetagpt --init-config  # it will create ~/.metagpt/config2.yaml, just modify it to your needs\n```\n\nYou can configure `~/.metagpt/config2.yaml` according to the [example](https://github.com/geekan/MetaGPT/blob/main/config/config2.example.yaml) and [doc](https://docs.deepwisdom.ai/main/en/guide/get_started/configuration.html):\n\n```yaml\nllm:\n  api_type: \"openai\"  # or azure / ollama / groq etc. Check LLMType for more options\n  model: \"gpt-4-turbo\"  # or gpt-3.5-turbo\n  base_url: \"https://api.openai.com/v1\"  # or forward url / other llm url\n  api_key: \"YOUR_API_KEY\"\n```\n\n### Usage\n\nAfter installation, you can use MetaGPT at CLI\n\n```bash\nmetagpt \"Create a 2048 game\"  # this will create a repo in ./workspace\n```\n\nor use it as library\n\n```python\nfrom metagpt.software_company import generate_repo\nfrom metagpt.utils.project_repo import ProjectRepo\n\nrepo: ProjectRepo = generate_repo(\"Create a 2048 game\")  # or ProjectRepo(\"<path>\")\nprint(repo)  # it will print the repo structure with files\n```\n\nYou can also use [Data Interpreter](https://github.com/geekan/MetaGPT/tree/main/examples/di) to write code:\n\n```python\nimport asyncio\nfrom metagpt.roles.di.data_interpreter import DataInterpreter\n\nasync def main():\n    di = DataInterpreter()\n    await di.run(\"Run data analysis on sklearn Iris dataset, include a plot\")\n\nasyncio.run(main())  # or await main() in a jupyter notebook setting\n```\n\n\n### QuickStart & Demo Video\n- Try it on [MetaGPT Huggingface Space](https://huggingface.co/spaces/deepwisdom/MetaGPT-SoftwareCompany)\n- [Matthew Berman: How To Install MetaGPT - Build A Startup With One Prompt!!](https://youtu.be/uT75J_KG_aY)\n- [Official Demo Video](https://github.com/geekan/MetaGPT/assets/2707039/5e8c1062-8c35-440f-bb20-2b0320f8d27d)\n\nhttps://github.com/user-attachments/assets/888cb169-78c3-4a42-9d62-9d90ed3928c9\n\n## Tutorial\n\n- 🗒 [Online Document](https://docs.deepwisdom.ai/main/en/)\n- 💻 [Usage](https://docs.deepwisdom.ai/main/en/guide/get_started/quickstart.html)  \n- 🔎 [What can MetaGPT do?](https://docs.deepwisdom.ai/main/en/guide/get_started/introduction.html)\n- 🛠 How to build your own agents? \n  - [MetaGPT Usage & Development Guide | Agent 101](https://docs.deepwisdom.ai/main/en/guide/tutorials/agent_101.html)\n  - [MetaGPT Usage & Development Guide | MultiAgent 101](https://docs.deepwisdom.ai/main/en/guide/tutorials/multi_agent_101.html)\n- 🧑‍💻 Contribution\n  - [Develop Roadmap](docs/ROADMAP.md)\n- 🔖 Use Cases\n  - [Data Interpreter](https://docs.deepwisdom.ai/main/en/guide/use_cases/agent/interpreter/intro.html)\n  - [Debate](https://docs.deepwisdom.ai/main/en/guide/use_cases/multi_agent/debate.html)\n  - [Researcher](https://docs.deepwisdom.ai/main/en/guide/use_cases/agent/researcher.html)\n  - [Receipt Assistant](https://docs.deepwisdom.ai/main/en/guide/use_cases/agent/receipt_assistant.html)\n- ❓ [FAQs](https://docs.deepwisdom.ai/main/en/guide/faq.html)\n\n## Support\n\n### Discord Join US\n\n📢 Join Our [Discord Channel](https://discord.gg/ZRHeExS6xv)! Looking forward to seeing you there! 🎉\n\n### Contributor form\n\n📝 [Fill out the form](https://airtable.com/appInfdG0eJ9J4NNL/pagK3Fh1sGclBvVkV/form) to become a contributor. We are looking forward to your participation!\n\n### Contact Information\n\nIf you have any questions or feedback about this project, please feel free to contact us. We highly appreciate your suggestions!\n\n- **Email:** alexanderwu@deepwisdom.ai\n- **GitHub Issues:** For more technical inquiries, you can also create a new issue in our [GitHub repository](https://github.com/geekan/metagpt/issues).\n\nWe will respond to all questions within 2-3 business days.\n\n## Citation\n\nTo stay updated with the latest research and development, follow [@MetaGPT_](https://twitter.com/MetaGPT_) on Twitter. \n\nTo cite [MetaGPT](https://openreview.net/forum?id=VtmBAGCN7o) in publications, please use the following BibTeX entries.   \n\n```bibtex\n@inproceedings{hong2024metagpt,\n      title={Meta{GPT}: Meta Programming for A Multi-Agent Collaborative Framework},\n      author={Sirui Hong and Mingchen Zhuge and Jonathan Chen and Xiawu Zheng and Yuheng Cheng and Jinlin Wang and Ceyao Zhang and Zili Wang and Steven Ka Shing Yau and Zijuan Lin and Liyang Zhou and Chenyu Ran and Lingfeng Xiao and Chenglin Wu and J{\\\"u}rgen Schmidhuber},\n      booktitle={The Twelfth International Conference on Learning Representations},\n      year={2024},\n      url={https://openreview.net/forum?id=VtmBAGCN7o}\n}\n```\n\nFor more work, please refer to [Academic Work](docs/ACADEMIC_WORK.md).\n"
    },
    {
      "name": "microsoft/semantic-kernel",
      "stars": 24146,
      "img": "https://avatars.githubusercontent.com/u/6154722?s=40&v=4",
      "owner": "microsoft",
      "repo_name": "semantic-kernel",
      "description": "Integrate cutting-edge LLM technology quickly and easily into your apps",
      "homepage": "https://aka.ms/semantic-kernel",
      "language": "C#",
      "created_at": "2023-02-27T17:39:42Z",
      "updated_at": "2025-04-23T09:33:56Z",
      "topics": [
        "ai",
        "artificial-intelligence",
        "llm",
        "openai",
        "sdk"
      ],
      "readme": "# Semantic Kernel\n\n**Build intelligent AI agents and multi-agent systems with this enterprise-ready orchestration framework**\n\n[![License: MIT](https://img.shields.io/github/license/microsoft/semantic-kernel)](https://github.com/microsoft/semantic-kernel/blob/main/LICENSE)\n[![Python package](https://img.shields.io/pypi/v/semantic-kernel)](https://pypi.org/project/semantic-kernel/)\n[![Nuget package](https://img.shields.io/nuget/vpre/Microsoft.SemanticKernel)](https://www.nuget.org/packages/Microsoft.SemanticKernel/)\n[![Discord](https://img.shields.io/discord/1063152441819942922?label=Discord&logo=discord&logoColor=white&color=d82679)](https://aka.ms/SKDiscord)\n\n\n## What is Semantic Kernel?\n\nSemantic Kernel is a model-agnostic SDK that empowers developers to build, orchestrate, and deploy AI agents and multi-agent systems. Whether you're building a simple chatbot or a complex multi-agent workflow, Semantic Kernel provides the tools you need with enterprise-grade reliability and flexibility.\n\n## System Requirements\n\n- **Python**: 3.10+\n- **.NET**: .NET 8.0+ \n- **Java**: JDK 17+\n- **OS Support**: Windows, macOS, Linux\n\n## Key Features\n\n- **Model Flexibility**: Connect to any LLM with built-in support for [OpenAI](https://platform.openai.com/docs/introduction), [Azure OpenAI](https://azure.microsoft.com/en-us/products/ai-services/openai-service), [Hugging Face](https://huggingface.co/), [NVidia](https://www.nvidia.com/en-us/ai-data-science/products/nim-microservices/) and more\n- **Agent Framework**: Build modular AI agents with access to tools/plugins, memory, and planning capabilities\n- **Multi-Agent Systems**: Orchestrate complex workflows with collaborating specialist agents\n- **Plugin Ecosystem**: Extend with native code functions, prompt templates, OpenAPI specs, or Model Context Protocol (MCP)\n- **Vector DB Support**: Seamless integration with [Azure AI Search](https://learn.microsoft.com/en-us/azure/search/search-what-is-azure-search), [Elasticsearch](https://www.elastic.co/), [Chroma](https://docs.trychroma.com/getting-started), and more\n- **Multimodal Support**: Process text, vision, and audio inputs\n- **Local Deployment**: Run with [Ollama](https://ollama.com/), [LMStudio](https://lmstudio.ai/), or [ONNX](https://onnx.ai/)\n- **Process Framework**: Model complex business processes with a structured workflow approach\n- **Enterprise Ready**: Built for observability, security, and stable APIs\n\n## Installation\n\nFirst, set the environment variable for your AI Services:\n\n**Azure OpenAI:**\n```bash\nexport AZURE_OPENAI_API_KEY=AAA....\n```\n\n**or OpenAI directly:**\n```bash\nexport OPENAI_API_KEY=sk-...\n```\n\n### Python\n\n```bash\npip install semantic-kernel\n```\n\n### .NET\n\n```bash\ndotnet add package Microsoft.SemanticKernel\ndotnet add package Microsoft.SemanticKernel.Agents.core\n```\n\n### Java\n\nSee [semantic-kernel-java build](https://github.com/microsoft/semantic-kernel-java/blob/main/BUILD.md) for instructions.\n\n## Quickstart\n\n### Basic Agent - Python\n\nCreate a simple assistant that responds to user prompts:\n\n```python\nimport asyncio\nfrom semantic_kernel.agents import ChatCompletionAgent\nfrom semantic_kernel.connectors.ai.open_ai import AzureChatCompletion\n\nasync def main():\n    # Initialize a chat agent with basic instructions\n    agent = ChatCompletionAgent(\n        service=AzureChatCompletion(),\n        name=\"SK-Assistant\",\n        instructions=\"You are a helpful assistant.\",\n    )\n\n    # Get a response to a user message\n    response = await agent.get_response(messages=\"Write a haiku about Semantic Kernel.\")\n    print(response.content)\n\nasyncio.run(main()) \n\n# Output:\n# Language's essence,\n# Semantic threads intertwine,\n# Meaning's core revealed.\n```\n\n### Basic Agent - .NET\n\n```csharp\nusing Microsoft.SemanticKernel;\nusing Microsoft.SemanticKernel.Agents;\n\nvar builder = Kernel.CreateBuilder();\nbuilder.AddAzureOpenAIChatCompletion(\n                Environment.GetEnvironmentVariable(\"AZURE_OPENAI_DEPLOYMENT\"),\n                Environment.GetEnvironmentVariable(\"AZURE_OPENAI_ENDPOINT\"),\n                Environment.GetEnvironmentVariable(\"AZURE_OPENAI_API_KEY\")\n                );\nvar kernel = builder.Build();\n\nChatCompletionAgent agent =\n    new()\n    {\n        Name = \"SK-Agent\",\n        Instructions = \"You are a helpful assistant.\",\n        Kernel = kernel,\n    };\n\nawait foreach (AgentResponseItem<ChatMessageContent> response \n    in agent.InvokeAsync(\"Write a haiku about Semantic Kernel.\"))\n{\n    Console.WriteLine(response.Message);\n}\n\n// Output:\n// Language's essence,\n// Semantic threads intertwine,\n// Meaning's core revealed.\n```\n\n### Agent with Plugins - Python\n\nEnhance your agent with custom tools (plugins) and structured output:\n\n```python\nimport asyncio\nfrom typing import Annotated\nfrom pydantic import BaseModel\nfrom semantic_kernel.agents import ChatCompletionAgent\nfrom semantic_kernel.connectors.ai.open_ai import AzureChatCompletion, OpenAIChatPromptExecutionSettings\nfrom semantic_kernel.functions import kernel_function, KernelArguments\n\nclass MenuPlugin:\n    @kernel_function(description=\"Provides a list of specials from the menu.\")\n    def get_specials(self) -> Annotated[str, \"Returns the specials from the menu.\"]:\n        return \"\"\"\n        Special Soup: Clam Chowder\n        Special Salad: Cobb Salad\n        Special Drink: Chai Tea\n        \"\"\"\n\n    @kernel_function(description=\"Provides the price of the requested menu item.\")\n    def get_item_price(\n        self, menu_item: Annotated[str, \"The name of the menu item.\"]\n    ) -> Annotated[str, \"Returns the price of the menu item.\"]:\n        return \"$9.99\"\n\nclass MenuItem(BaseModel):\n    price: float\n    name: str\n\nasync def main():\n    # Configure structured output format\n    settings = OpenAIChatPromptExecutionSettings()\n    settings.response_format = MenuItem\n\n    # Create agent with plugin and settings\n    agent = ChatCompletionAgent(\n        service=AzureChatCompletion(),\n        name=\"SK-Assistant\",\n        instructions=\"You are a helpful assistant.\",\n        plugins=[MenuPlugin()],\n        arguments=KernelArguments(settings)\n    )\n\n    response = await agent.get_response(messages=\"What is the price of the soup special?\")\n    print(response.content)\n\n    # Output:\n    # The price of the Clam Chowder, which is the soup special, is $9.99.\n\nasyncio.run(main()) \n```\n\n### Agent with Plugin - .NET\n\n```csharp\nusing System.ComponentModel;\nusing Microsoft.SemanticKernel;\nusing Microsoft.SemanticKernel.Agents;\nusing Microsoft.SemanticKernel.ChatCompletion;\n\nvar builder = Kernel.CreateBuilder();\nbuilder.AddAzureOpenAIChatCompletion(\n                Environment.GetEnvironmentVariable(\"AZURE_OPENAI_DEPLOYMENT\"),\n                Environment.GetEnvironmentVariable(\"AZURE_OPENAI_ENDPOINT\"),\n                Environment.GetEnvironmentVariable(\"AZURE_OPENAI_API_KEY\")\n                );\nvar kernel = builder.Build();\n\nkernel.Plugins.Add(KernelPluginFactory.CreateFromType<MenuPlugin>());\n\nChatCompletionAgent agent =\n    new()\n    {\n        Name = \"SK-Assistant\",\n        Instructions = \"You are a helpful assistant.\",\n        Kernel = kernel,\n        Arguments = new KernelArguments(new PromptExecutionSettings() { FunctionChoiceBehavior = FunctionChoiceBehavior.Auto() })\n\n    };\n\nawait foreach (AgentResponseItem<ChatMessageContent> response \n    in agent.InvokeAsync(\"What is the price of the soup special?\"))\n{\n    Console.WriteLine(response.Message);\n}\n\nsealed class MenuPlugin\n{\n    [KernelFunction, Description(\"Provides a list of specials from the menu.\")]\n    public string GetSpecials() =>\n        \"\"\"\n        Special Soup: Clam Chowder\n        Special Salad: Cobb Salad\n        Special Drink: Chai Tea\n        \"\"\";\n\n    [KernelFunction, Description(\"Provides the price of the requested menu item.\")]\n    public string GetItemPrice(\n        [Description(\"The name of the menu item.\")]\n        string menuItem) =>\n        \"$9.99\";\n}\n```\n\n### Multi-Agent System - Python\n\nBuild a system of specialized agents that can collaborate:\n\n```python\nimport asyncio\nfrom semantic_kernel.agents import ChatCompletionAgent\nfrom semantic_kernel.connectors.ai.open_ai import AzureChatCompletion, OpenAIChatCompletion\n\nbilling_agent = ChatCompletionAgent(\n    service=AzureChatCompletion(), \n    name=\"BillingAgent\", \n    instructions=\"You handle billing issues like charges, payment methods, cycles, fees, discrepancies, and payment failures.\"\n)\n\nrefund_agent = ChatCompletionAgent(\n    service=AzureChatCompletion(),\n    name=\"RefundAgent\",\n    instructions=\"Assist users with refund inquiries, including eligibility, policies, processing, and status updates.\",\n)\n\ntriage_agent = ChatCompletionAgent(\n    service=OpenAIChatCompletion(),\n    name=\"TriageAgent\",\n    instructions=\"Evaluate user requests and forward them to BillingAgent or RefundAgent for targeted assistance.\"\n    \" Provide the full answer to the user containing any information from the agents\",\n    plugins=[billing_agent, refund_agent],\n)\n\nthread: None\n\nasync def main() -> None:\n    print(\"Welcome to the chat bot!\\n  Type 'exit' to exit.\\n  Try to get some billing or refund help.\")\n    while True:\n        user_input = input(\"User:> \")\n\n        if user_input.lower().strip() == \"exit\":\n            print(\"\\n\\nExiting chat...\")\n            return False\n\n        response = await triage_agent.get_response(\n            messages=user_input,\n            thread=thread,\n        )\n\n        if response:\n            print(f\"Agent :> {response}\")\n\n# Agent :> I understand that you were charged twice for your subscription last month, and I'm here to assist you with resolving this issue. Here’s what we need to do next:\n\n# 1. **Billing Inquiry**:\n#    - Please provide the email address or account number associated with your subscription, the date(s) of the charges, and the amount charged. This will allow the billing team to investigate the discrepancy in the charges.\n\n# 2. **Refund Process**:\n#    - For the refund, please confirm your subscription type and the email address associated with your account.\n#    - Provide the dates and transaction IDs for the charges you believe were duplicated.\n\n# Once we have these details, we will be able to:\n\n# - Check your billing history for any discrepancies.\n# - Confirm any duplicate charges.\n# - Initiate a refund for the duplicate payment if it qualifies. The refund process usually takes 5-10 business days after approval.\n\n# Please provide the necessary details so we can proceed with resolving this issue for you.\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n\n\n## Where to Go Next\n\n1. 📖 Try our [Getting Started Guide](https://learn.microsoft.com/en-us/semantic-kernel/get-started/quick-start-guide) or learn about [Building Agents](https://learn.microsoft.com/en-us/semantic-kernel/frameworks/agent/)\n2. 🔌 Explore over 100 [Detailed Samples](https://learn.microsoft.com/en-us/semantic-kernel/get-started/detailed-samples)\n3. 💡 Learn about core Semantic Kernel [Concepts](https://learn.microsoft.com/en-us/semantic-kernel/concepts/kernel)\n\n### API References\n\n- [C# API reference](https://learn.microsoft.com/en-us/dotnet/api/microsoft.semantickernel?view=semantic-kernel-dotnet)\n- [Python API reference](https://learn.microsoft.com/en-us/python/api/semantic-kernel/semantic_kernel?view=semantic-kernel-python)\n\n## Troubleshooting\n\n### Common Issues\n\n- **Authentication Errors**: Check that your API key environment variables are correctly set\n- **Model Availability**: Verify your Azure OpenAI deployment or OpenAI model access\n\n### Getting Help\n\n- Check our [GitHub issues](https://github.com/microsoft/semantic-kernel/issues) for known problems\n- Search the [Discord community](https://aka.ms/SKDiscord) for solutions\n- Include your SDK version and full error messages when asking for help\n\n\n## Join the community\n\nWe welcome your contributions and suggestions to the SK community! One of the easiest ways to participate is to engage in discussions in the GitHub repository. Bug reports and fixes are welcome!\n\nFor new features, components, or extensions, please open an issue and discuss with us before sending a PR. This is to avoid rejection as we might be taking the core in a different direction, but also to consider the impact on the larger ecosystem.\n\nTo learn more and get started:\n\n- Read the [documentation](https://aka.ms/sk/learn)\n- Learn how to [contribute](https://learn.microsoft.com/en-us/semantic-kernel/support/contributing) to the project\n- Ask questions in the [GitHub discussions](https://github.com/microsoft/semantic-kernel/discussions)\n- Ask questions in the [Discord community](https://aka.ms/SKDiscord)\n\n- Attend [regular office hours and SK community events](COMMUNITY.md)\n- Follow the team on our [blog](https://aka.ms/sk/blog)\n\n## Contributor Wall of Fame\n\n[![semantic-kernel contributors](https://contrib.rocks/image?repo=microsoft/semantic-kernel)](https://github.com/microsoft/semantic-kernel/graphs/contributors)\n\n## Code of Conduct\n\nThis project has adopted the\n[Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information, see the\n[Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/)\nor contact [opencode@microsoft.com](mailto:opencode@microsoft.com)\nwith any additional questions or comments.\n\n## License\n\nCopyright (c) Microsoft Corporation. All rights reserved.\n\nLicensed under the [MIT](LICENSE) license.\n"
    },
    {
      "name": "microsoft/ai-agents-for-beginners",
      "stars": 16627,
      "img": "https://avatars.githubusercontent.com/u/6154722?s=40&v=4",
      "owner": "microsoft",
      "repo_name": "ai-agents-for-beginners",
      "description": "10 Lessons to Get Started Building AI Agents",
      "homepage": "https://microsoft.github.io/ai-agents-for-beginners/",
      "language": "Jupyter Notebook",
      "created_at": "2024-11-28T10:42:52Z",
      "updated_at": "2025-04-23T11:23:23Z",
      "topics": [
        "agentic-ai",
        "agentic-framework",
        "agentic-rag",
        "ai-agents",
        "ai-agents-framework",
        "autogen",
        "generative-ai",
        "semantic-kernel"
      ],
      "readme": "# AI Agents for Beginners - A Course\n\n![Generative AI For Beginners](./images/repo-thumbnail.png)\n\n## 10 Lessons teaching everything you need to know to start building AI Agents\n\n[![GitHub license](https://img.shields.io/github/license/microsoft/ai-agents-for-beginners.svg)](https://github.com/microsoft/ai-agents-for-beginners/blob/master/LICENSE?WT.mc_id=academic-105485-koreyst)\n[![GitHub contributors](https://img.shields.io/github/contributors/microsoft/ai-agents-for-beginners.svg)](https://GitHub.com/microsoft/ai-agents-for-beginners/graphs/contributors/?WT.mc_id=academic-105485-koreyst)\n[![GitHub issues](https://img.shields.io/github/issues/microsoft/ai-agents-for-beginners.svg)](https://GitHub.com/microsoft/ai-agents-for-beginners/issues/?WT.mc_id=academic-105485-koreyst)\n[![GitHub pull-requests](https://img.shields.io/github/issues-pr/microsoft/ai-agents-for-beginners.svg)](https://GitHub.com/microsoft/ai-agents-for-beginners/pulls/?WT.mc_id=academic-105485-koreyst)\n[![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat-square)](http://makeapullrequest.com?WT.mc_id=academic-105485-koreyst)\n\n### Language Support\n[![English](https://img.shields.io/badge/English-brightgreen.svg?style=flat-square)](README.md)\n[![Chinese Simplified](https://img.shields.io/badge/Chinese_Simplified-brightgreen.svg?style=flat-square)](./translations/zh/README.md)\n[![Chinese Traditional](https://img.shields.io/badge/Chinese_Traditional-brightgreen.svg?style=flat-square)](./translations/tw/README.md)     \n[![Chinese Hong Kong](https://img.shields.io/badge/Chinese_Hong_Kong-brightgreen.svg?style=flat-square)](./translations/hk/README.md) \n[![French](https://img.shields.io/badge/French-brightgreen.svg?style=flat-square)](./translations/fr/README.md)\n[![Japanese](https://img.shields.io/badge/Japanese-brightgreen.svg?style=flat-square)](./translations/ja/README.md) \n[![Korean](https://img.shields.io/badge/Korean-brightgreen.svg?style=flat-square)](./translations/ko/README.md)\n[![Portuguese Brazilian](https://img.shields.io/badge/Portuguese_Brazilian-brightgreen.svg?style=flat-square)](./translations/pt/README.md)\n[![Spanish](https://img.shields.io/badge/Spanish-brightgreen.svg?style=flat-square)](./translations/es/README.md)\n[![German](https://img.shields.io/badge/German-brightgreen.svg?style=flat-square)](./translations/de/README.md)  \n[![Persian](https://img.shields.io/badge/Persian-brightgreen.svg?style=flat-square)](./translations/fa/README.md) \n[![Polish](https://img.shields.io/badge/Polish-brightgreen.svg?style=flat-square)](./translations/pl/README.md) \n[![Hindi](https://img.shields.io/badge/Hindi-brightgreen.svg?style=flat-square)](./translations/hi/README.md)\n\n[![GitHub watchers](https://img.shields.io/github/watchers/microsoft/ai-agents-for-beginners.svg?style=social&label=Watch)](https://GitHub.com/microsoft/ai-agents-for-beginners/watchers/?WT.mc_id=academic-105485-koreyst)\n[![GitHub forks](https://img.shields.io/github/forks/microsoft/ai-agents-for-beginners.svg?style=social&label=Fork)](https://GitHub.com/microsoft/ai-agents-for-beginners/network/?WT.mc_id=academic-105485-koreyst)\n[![GitHub stars](https://img.shields.io/github/stars/microsoft/ai-agents-for-beginners.svg?style=social&label=Star)](https://GitHub.com/microsoft/ai-agents-for-beginners/stargazers/?WT.mc_id=academic-105485-koreyst)\n\n[![Azure AI Discord](https://dcbadge.limes.pink/api/server/kzRShWzttr)](https://discord.gg/kzRShWzttr)\n\n\n## 🌱 Getting Started\n\nThis course has 10 lessons covering the fundamentals of building AI Agents. Each lesson covers its own topic so start wherever you like!\n\nThere is multi-language support for this course. Go to our [available languages here](#-multi-language-support). \n\nIf this is your first time building with Generative AI models, check out our [Generative AI For Beginners](https://aka.ms/genai-beginners) course, which includes 21 lessons on building with GenAI.\n\nDon't forget to [star (🌟) this repo](https://docs.github.com/en/get-started/exploring-projects-on-github/saving-repositories-with-stars?WT.mc_id=academic-105485-koreyst) and [fork this repo](https://github.com/microsoft/ai-agents-for-beginners/fork) to run the code.\n\n### What You Need \n\nEach lesson in this course includes code examples, which can be found in the code_samples folder. You can [fork this repo](https://github.com/microsoft/ai-agents-for-beginners/fork) to create your own copy.  \n\nThe code example in these exercises, utilize Azure AI Foundry and GitHub Model Catalogs for interacting with Language Models:\n\n- [Github Models](https://aka.ms/ai-agents-beginners/github-models) - Free / Limited\n- [Azure AI Foundry](https://aka.ms/ai-agents-beginners/ai-foundry) - Azure Account Required\n\nThis course also uses the following AI Agent frameworks and services from Microsoft:\n\n- [Azure AI Agent Service](https://aka.ms/ai-agents-beginners/ai-agent-service)\n- [Semantic Kernel](https://aka.ms/ai-agents-beginners/semantic-kernel)\n- [AutoGen](https://aka.ms/ai-agents/autogen)\n\nFor more information on running the code for this course, go to the [Course Setup](./00-course-setup/README.md).\n\n## 🙏 Want to help?\n\nDo you have suggestions or found spelling or code errors? [Raise an issue](https://github.com/microsoft/ai-agents-for-beginners/issues?WT.mc_id=academic-105485-koreyst) or [Create a pull request](https://github.com/microsoft/ai-agents-for-beginners/pulls?WT.mc_id=academic-105485-koreyst)\n\nIf you get stuck or have any questions about building AI Agents, join our [Azure AI Community Discord](https://discord.gg/kzRShWzttr).\n\n## 📂 Each lesson includes\n\n- A written lesson located in the README and a short video\n- Python code samples supporting Azure AI Foundry and Github Models (Free)\n- Links to extra resources to continue your learning\n\n\n## 🗃️ Lessons\n\n| **Lesson**                               | **Text & Code**                                    | **Video**                                                  | **Extra Learning**                                                                     |\n|------------------------------------------|----------------------------------------------------|------------------------------------------------------------|----------------------------------------------------------------------------------------|\n| Intro to AI Agents and Agent Use Cases   | [Link](./01-intro-to-ai-agents/README.md)          | [Video](https://youtu.be/3zgm60bXmQk?si=z8QygFvYQv-9WtO1)  | [Link](https://aka.ms/ai-agents-beginners/collection?WT.mc_id=academic-105485-koreyst) |\n| Exploring AI Agentic Frameworks          | [Link](./02-explore-agentic-frameworks/README.md)  | [Video](https://youtu.be/ODwF-EZo_O8?si=Vawth4hzVaHv-u0H)  | [Link](https://aka.ms/ai-agents-beginners/collection?WT.mc_id=academic-105485-koreyst) |\n| Understanding AI Agentic Design Patterns | [Link](./03-agentic-design-patterns/README.md)     | [Video](https://youtu.be/m9lM8qqoOEA?si=BIzHwzstTPL8o9GF)  | [Link](https://aka.ms/ai-agents-beginners/collection?WT.mc_id=academic-105485-koreyst) |\n| Tool Use Design Pattern                  | [Link](./04-tool-use/README.md)                    | [Video](https://youtu.be/vieRiPRx-gI?si=2z6O2Xu2cu_Jz46N)  | [Link](https://aka.ms/ai-agents-beginners/collection?WT.mc_id=academic-105485-koreyst) |\n| Agentic RAG                              | [Link](./05-agentic-rag/README.md)                 | [Video](https://youtu.be/WcjAARvdL7I?si=gKPWsQpKiIlDH9A3)  | [Link](https://aka.ms/ai-agents-beginners/collection?WT.mc_id=academic-105485-koreyst) |\n| Building Trustworthy AI Agents           | [Link](./06-building-trustworthy-agents/README.md) | [Video](https://youtu.be/iZKkMEGBCUQ?si=jZjpiMnGFOE9L8OK ) | [Link](https://aka.ms/ai-agents-beginners/collection?WT.mc_id=academic-105485-koreyst) |\n| Planning Design Pattern                  | [Link](./07-planning-design/README.md)             | [Video](https://youtu.be/kPfJ2BrBCMY?si=6SC_iv_E5-mzucnC)  | [Link](https://aka.ms/ai-agents-beginners/collection?WT.mc_id=academic-105485-koreyst) |\n| Multi-Agent Design Pattern               | [Link](./08-multi-agent/README.md)                 | [Video](https://youtu.be/V6HpE9hZEx0?si=rMgDhEu7wXo2uo6g)  | [Link](https://aka.ms/ai-agents-beginners/collection?WT.mc_id=academic-105485-koreyst) |\n| Metacognition Design Pattern             | [Link](./09-metacognition/README.md)               | [Video](https://youtu.be/His9R6gw6Ec?si=8gck6vvdSNCt6OcF)  | [Link](https://aka.ms/ai-agents-beginners/collection?WT.mc_id=academic-105485-koreyst) |\n| AI Agents in Production                  | [Link](./10-ai-agents-production/README.md)        | [Video](https://youtu.be/l4TP6IyJxmQ?si=31dnhexRo6yLRJDl)  | [Link](https://aka.ms/ai-agents-beginners/collection?WT.mc_id=academic-105485-koreyst) |\n\n## 🌐 Multi-Language Support\n\n| Language             | Code | Link to Translated README                               | Last Updated |\n|----------------------|------|---------------------------------------------------------|--------------|\n| Chinese (Simplified) | zh   | [Chinese Translation](./translations/zh/README.md)      | 2025-03-24   |\n| Chinese (Traditional)| tw   | [Chinese Translation](./translations/tw/README.md)      | 2025-03-28   |\n| Chinese (Hong Kong)  | hk   | [Chinese (Hong Kong) Translation](./translations/hk/README.md) | 2025-03-28   |\n| French               | fr   | [French Translation](./translations/fr/README.md)       | 2025-03-28   |\n| Japanese             | ja   | [Japanese Translation](./translations/ja/README.md)     | 2025-03-28   |\n| Korean               | ko   | [Korean Translation](./translations/ko/README.md)       | 2025-03-28   |\n| Portuguese           | pt   | [Portuguese Translation](./translations/pt/README.md)   | 2025-03-28   |\n| Spanish              | es   | [Spanish Translation](./translations/es/README.md)      | 2025-03-28   |\n| German               | de   | [German Translation](./translations/de/README.md)       | 2025-03-28   |\n| Persian              | fa   | [Persian Translation](./translations/fa/README.md)       | 2025-03-28   |\n| Polish               | pl   | [Polish Translation](./translations/pl/README.md)       | 2025-03-28  |\n| Hindi               | hi   | [Hindi Translation](./translations/hi/README.md)       | 2025-04-05   |\n\n\n## 🎒 Other Courses\n\nOur team produces other courses! Check out:\n\n- [**NEW** Generative AI for Beginners using .NET](https://github.com/microsoft/Generative-AI-for-beginners-dotnet?WT.mc_id=academic-105485-koreyst)\n- [Generative AI for Beginners](https://github.com/microsoft/generative-ai-for-beginners?WT.mc_id=academic-105485-koreyst)\n- [ML for Beginners](https://aka.ms/ml-beginners?WT.mc_id=academic-105485-koreyst)\n- [Data Science for Beginners](https://aka.ms/datascience-beginners?WT.mc_id=academic-105485-koreyst)\n- [AI for Beginners](https://aka.ms/ai-beginners?WT.mc_id=academic-105485-koreyst)\n- [Cybersecurity for Beginners](https://github.com/microsoft/Security-101??WT.mc_id=academic-96948-sayoung)\n- [Web Dev for Beginners](https://aka.ms/webdev-beginners?WT.mc_id=academic-105485-koreyst)\n- [IoT for Beginners](https://aka.ms/iot-beginners?WT.mc_id=academic-105485-koreyst)\n- [XR Development for Beginners](https://github.com/microsoft/xr-development-for-beginners?WT.mc_id=academic-105485-koreyst)\n- [Mastering GitHub Copilot for AI Paired Programming](https://aka.ms/GitHubCopilotAI?WT.mc_id=academic-105485-koreyst)\n- [Mastering GitHub Copilot for C#/.NET Developers](https://github.com/microsoft/mastering-github-copilot-for-dotnet-csharp-developers?WT.mc_id=academic-105485-koreyst)\n- [Choose Your Own Copilot Adventure](https://github.com/microsoft/CopilotAdventures?WT.mc_id=academic-105485-koreyst)\n\n## 🌟 Community Thanks\n\nThanks to [Shivam Goyal](https://www.linkedin.com/in/shivam2003/) for contributing important code samples demonstrating Agentic RAG. \n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit <https://cla.opensource.microsoft.com>.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft\ntrademarks or logos is subject to and must follow\n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos is subject to those third-parties' policies.\n"
    },
    {
      "name": "google/A2A",
      "stars": 12967,
      "img": "https://avatars.githubusercontent.com/u/1342004?s=40&v=4",
      "owner": "google",
      "repo_name": "A2A",
      "description": "An open protocol enabling communication and interoperability between opaque agentic applications.",
      "homepage": "https://google.github.io/A2A/",
      "language": "Python",
      "created_at": "2025-03-25T18:44:21Z",
      "updated_at": "2025-04-23T11:23:43Z",
      "topics": [],
      "readme": "![image info](images/A2A_banner.png)\n\n**_An open protocol enabling communication and interoperability between opaque agentic applications._**\n\n<!-- TOC -->\n\n- [Agent2Agent Protocol A2A](#agent2agent-protocol-a2a)\n    - [Getting Started](#getting-started)\n    - [Contributing](#contributing)\n    - [What's next](#whats-next)\n    - [About](#about)\n\n<!-- /TOC -->\n\nOne of the biggest challenges in enterprise AI adoption is getting agents built on different frameworks and vendors to work together. That’s why we created an open *Agent2Agent (A2A) protocol*, a collaborative way to help agents across different ecosystems communicate with each other. Google is driving this open protocol initiative for the industry because we believe this protocol will be **critical to support multi-agent communication by giving your agents a common language – irrespective of the framework or vendor they are built on**. \nWith *A2A*, agents can show each other their capabilities and negotiate how they will interact with users (via text, forms, or bidirectional audio/video) – all while working securely together.\n\n### **See A2A in Action**\n\nWatch [this demo video](https://storage.googleapis.com/gweb-developer-goog-blog-assets/original_videos/A2A_demo_v4.mp4) to see how A2A enables seamless communication between different agent frameworks.\n\n### Conceptual Overview\n\nThe Agent2Agent (A2A) protocol facilitates communication between independent AI agents. Here are the core concepts:\n\n*   **Agent Card:** A public metadata file (usually at `/.well-known/agent.json`) describing an agent's capabilities, skills, endpoint URL, and authentication requirements. Clients use this for discovery.\n*   **A2A Server:** An agent exposing an HTTP endpoint that implements the A2A protocol methods (defined in the [json specification](/specification)). It receives requests and manages task execution.\n*   **A2A Client:** An application or another agent that consumes A2A services. It sends requests (like `tasks/send`) to an A2A Server's URL.\n*   **Task:** The central unit of work. A client initiates a task by sending a message (`tasks/send` or `tasks/sendSubscribe`). Tasks have unique IDs and progress through states (`submitted`, `working`, `input-required`, `completed`, `failed`, `canceled`).\n*   **Message:** Represents communication turns between the client (`role: \"user\"`) and the agent (`role: \"agent\"`). Messages contain `Parts`.\n*   **Part:** The fundamental content unit within a `Message` or `Artifact`. Can be `TextPart`, `FilePart` (with inline bytes or a URI), or `DataPart` (for structured JSON, e.g., forms).\n*   **Artifact:** Represents outputs generated by the agent during a task (e.g., generated files, final structured data). Artifacts also contain `Parts`.\n*   **Streaming:** For long-running tasks, servers supporting the `streaming` capability can use `tasks/sendSubscribe`. The client receives Server-Sent Events (SSE) containing `TaskStatusUpdateEvent` or `TaskArtifactUpdateEvent` messages, providing real-time progress.\n*   **Push Notifications:** Servers supporting `pushNotifications` can proactively send task updates to a client-provided webhook URL, configured via `tasks/pushNotification/set`.\n\n**Typical Flow:**\n\n1.  **Discovery:** Client fetches the Agent Card from the server's well-known URL.\n2.  **Initiation:** Client sends a `tasks/send` or `tasks/sendSubscribe` request containing the initial user message and a unique Task ID.\n3.  **Processing:**\n    *   **(Streaming):** Server sends SSE events (status updates, artifacts) as the task progresses.\n    *   **(Non-Streaming):** Server processes the task synchronously and returns the final `Task` object in the response.\n4.  **Interaction (Optional):** If the task enters `input-required`, the client sends subsequent messages using the same Task ID via `tasks/send` or `tasks/sendSubscribe`.\n5.  **Completion:** The task eventually reaches a terminal state (`completed`, `failed`, `canceled`).\n\n### **Getting Started**\n\n* 📚 Read the [technical documentation](https://google.github.io/A2A/#/documentation) to understand the capabilities\n* 📝 Review the [json specification](/specification) of the protocol structures\n* 🎬 Use our [samples](/samples) to see A2A in action\n    * Sample A2A Client/Server ([Python](/samples/python/common), [JS](/samples/js/src))\n    * [Multi-Agent Web App](/demo/README.md)\n    * CLI ([Python](/samples/python/hosts/cli/README.md), [JS](/samples/js/README.md))\n* 🤖 Use our [sample agents](/samples/python/agents/README.md) to see how to bring A2A to agent frameworks\n    * [Agent Development Kit (ADK)](/samples/python/agents/google_adk/README.md)\n    * [CrewAI](/samples/python/agents/crewai/README.md)\n    * [LangGraph](/samples/python/agents/langgraph/README.md)\n    * [Genkit](/samples/js/src/agents/README.md)\n    * [LlamaIndex](/samples/python/agents/llama_index_file_chat/README.md)\n    * [Marvin](/samples/python/agents/marvin/README.md)\n    * [Semantic Kernel](/samples/python/agents/semantickernel/README.md)\n* 📑 Review key topics to understand protocol details \n    * [A2A and MCP](https://google.github.io/A2A/#/topics/a2a_and_mcp.md)\n    * [Agent Discovery](https://google.github.io/A2A/#/topics/agent_discovery.md)\n    * [Enterprise Ready](https://google.github.io/A2A/#/topics/enterprise_ready.md)\n    * [Push Notifications](https://google.github.io/A2A/#/topics/push_notifications.md) \n\n### **Contributing**\n\nWe highly value community contributions and appreciate your interest in A2A Protocol! Here's how you can get involved:\n* Get Started? Please see our [contributing guide](CONTRIBUTING.md) to get started.\n* Have questions? Join our community in [GitHub discussions](https://github.com/google/A2A/discussions).\n* Want to help with protocol improvement feedback?  Dive into [GitHub issues](https://github.com/google/A2A/issues).\n* Private Feedback? Please use this [Google form](https://docs.google.com/forms/d/e/1FAIpQLScS23OMSKnVFmYeqS2dP7dxY3eTyT7lmtGLUa8OJZfP4RTijQ/viewform)\n\n### **What's next**\n\nFuture plans include improvements to the protocol itself and enhancements to the samples:\n\n**Protocol Enhancements:**\n\n*   **Agent Discovery:**\n    *   Formalize inclusion of authorization schemes and optional credentials directly within the `AgentCard`.\n*   **Agent Collaboration:**\n    *   Investigate a `QuerySkill()` method for dynamically checking unsupported or unanticipated skills.\n*   **Task Lifecycle & UX:**\n    *   Support for dynamic UX negotiation *within* a task (e.g., agent adding audio/video mid-conversation).\n*   **Client Methods & Transport:**\n    *   Explore extending support to client-initiated methods (beyond task management).\n    *   Improvements to streaming reliability and push notification mechanisms.\n\n**Sample & Documentation Enhancements:**\n\n*   Simplify \"Hello World\" examples.\n*   Include additional examples of agents integrated with different frameworks or showcasing specific A2A features.\n*   Provide more comprehensive documentation for the common client/server libraries.\n*   Generate human-readable HTML documentation from the JSON Schema.\n\n### **About**\n\nA2A Protocol is an open source project run by Google LLC, under [License](LICENSE) and open to contributions from the entire community.\n"
    },
    {
      "name": "Chainlit/chainlit",
      "stars": 9384,
      "img": "https://avatars.githubusercontent.com/u/128686189?s=40&v=4",
      "owner": "Chainlit",
      "repo_name": "chainlit",
      "description": "Build Conversational AI in minutes ⚡️",
      "homepage": "https://docs.chainlit.io",
      "language": "TypeScript",
      "created_at": "2023-03-14T16:54:04Z",
      "updated_at": "2025-04-23T09:49:47Z",
      "topics": [
        "chatgpt",
        "langchain",
        "llm",
        "openai",
        "openai-chatgpt",
        "python",
        "ui"
      ],
      "readme": "<h1 align=\"center\">Welcome to Chainlit by Literal AI 👋</h1>\n\n<p align=\"center\">\n<b>Build python production-ready conversational AI applications in minutes, not weeks ⚡️</b>\n\n</p>\n<p align=\"center\">\n    <a href=\"https://discord.gg/k73SQ3FyUh\" rel=\"nofollow\"><img alt=\"Discord\" src=\"https://dcbadge.vercel.app/api/server/ZThrUxbAYw?style=flat\" style=\"max-width:100%;\"></a>\n    <a href=\"https://twitter.com/chainlit_io\" rel=\"nofollow\"><img alt=\"Twitter\" src=\"https://img.shields.io/twitter/url/https/twitter.com/chainlit_io.svg?style=social&label=Follow%20%40chainlit_io\" style=\"max-width:100%;\"></a>\n    <a href=\"https://pypistats.org/packages/chainlit\" rel=\"nofollow\"><img alt=\"Downloads\" src=\"https://img.shields.io/pypi/dm/chainlit\" style=\"max-width:100%;\"></a>\n        <a href=\"https://github.com/chainlit/chainlit/graphs/contributors\" rel=\"nofollow\"><img alt=\"Contributors\" src=\"https://img.shields.io/github/contributors/chainlit/chainlit\" style=\"max-width:100%;\"></a>\n    <a href=\"https://github.com/Chainlit/chainlit/actions/workflows/ci.yaml\" rel=\"nofollow\"><img alt=\"CI\" src=\"https://github.com/Chainlit/chainlit/actions/workflows/ci.yaml/badge.svg\" style=\"max-width:100%;\"></a>\n</p>\n\n<p align=\"center\">\n    <a href=\"https://chainlit.io\"><b>Website</b></a>  •  \n    <a href=\"https://docs.chainlit.io\"><b>Documentation</b></a>  •  \n    <a href=\"https://help.chainlit.io\"><b>Chainlit Help</b></a>  •  \n    <a href=\"https://github.com/Chainlit/cookbook\"><b>Cookbook</b></a>\n</p>\n\n<p align=\"center\">\n    <a href=\"https://trendshift.io/repositories/6708\" target=\"_blank\"><img src=\"https://trendshift.io/api/badge/repositories/6708\" alt=\"Chainlit%2Fchainlit | Trendshift\" style=\"width: 250px; height: 45px;\" width=\"250\" height=\"45\"/></a>\n</p>\n\nhttps://github.com/user-attachments/assets/b3738aba-55c0-42fa-ac00-6efd1ee0d148\n\n> [!NOTE]\n> Chainlit is maintained by [Literal AI](https://literalai.com), an LLMOps platform to monitor and evaluate LLM applications! It works with any Python or TypeScript applications and [seamlessly](https://docs.chainlit.io/llmops/literalai) with Chainlit. For enterprise support, please fill this [form](https://docs.google.com/forms/d/e/1FAIpQLSdPVGqfuaWSC2DfunR6cY4C7kUHl0c2W7DnhzsF9bmMxrVpkg/viewform?usp=header).\n\n## Installation\n\nOpen a terminal and run:\n\n```sh\npip install chainlit\nchainlit hello\n```\n\nIf this opens the `hello app` in your browser, you're all set!\n\n### Development version\n\nThe latest in-development version can be installed straight from GitHub with:\n\n```sh\npip install git+https://github.com/Chainlit/chainlit.git#subdirectory=backend/\n```\n\n(Requires Node and pnpm installed on the system.)\n\n## 🚀 Quickstart\n\n### 🐍 Pure Python\n\nCreate a new file `demo.py` with the following code:\n\n```python\nimport chainlit as cl\n\n\n@cl.step(type=\"tool\")\nasync def tool():\n    # Fake tool\n    await cl.sleep(2)\n    return \"Response from the tool!\"\n\n\n@cl.on_message  # this function will be called every time a user inputs a message in the UI\nasync def main(message: cl.Message):\n    \"\"\"\n    This function is called every time a user inputs a message in the UI.\n    It sends back an intermediate response from the tool, followed by the final answer.\n\n    Args:\n        message: The user's message.\n\n    Returns:\n        None.\n    \"\"\"\n\n\n    # Call the tool\n    tool_res = await tool()\n\n    await cl.Message(content=tool_res).send()\n```\n\nNow run it!\n\n```sh\nchainlit run demo.py -w\n```\n\n<img src=\"/images/quick-start.png\" alt=\"Quick Start\"></img>\n\n## 📚 More Examples - Cookbook\n\nYou can find various examples of Chainlit apps [here](https://github.com/Chainlit/cookbook) that leverage tools and services such as OpenAI, Anthropiс, LangChain, LlamaIndex, ChromaDB, Pinecone and more.\n\nTell us what you would like to see added in Chainlit using the Github issues or on [Discord](https://discord.gg/k73SQ3FyUh).\n\n## 💁 Contributing\n\nAs an open-source initiative in a rapidly evolving domain, we welcome contributions, be it through the addition of new features or the improvement of documentation.\n\nFor detailed information on how to contribute, see [here](/CONTRIBUTING.md).\n\n## 📃 License\n\nChainlit is open-source and licensed under the [Apache 2.0](LICENSE) license.\n"
    },
    {
      "name": "Azure/PyRIT",
      "stars": 2425,
      "img": "https://avatars.githubusercontent.com/u/6844498?s=40&v=4",
      "owner": "Azure",
      "repo_name": "PyRIT",
      "description": "The Python Risk Identification Tool for generative AI (PyRIT) is an open source framework built to empower security professionals and engineers to proactively identify risks in generative AI systems.",
      "homepage": "https://azure.github.io/PyRIT/",
      "language": "Python",
      "created_at": "2023-12-12T15:46:28Z",
      "updated_at": "2025-04-23T03:15:23Z",
      "topics": [
        "ai-red-team",
        "generative-ai",
        "red-team-tools",
        "responsible-ai"
      ],
      "readme": "<p align=\"center\"><img src=\"./doc/roakey.png\" width=\"150\"></p>\n\n# Python Risk Identification Tool for generative AI (PyRIT)\n\nThe Python Risk Identification Tool for generative AI (PyRIT) is an open source\nframework built to empower security professionals and engineers to proactively\nidentify risks in generative AI systems.\n\n- Check out our [website](https://azure.github.io/PyRIT/) for more information\n  about how to use, install, or contribute to PyRIT.\n- Visit our [Discord server](https://discord.gg/9fMpq3tc8u) to chat with the team and community.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services.\nAuthorized use of Microsoft trademarks or logos is subject to and must follow\n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must\nnot cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's\npolicies.\n\n## Citing PyRIT\n\nIf you use PyRIT in your research, please cite our preprint paper as follows:\n\n```\n@misc{munoz2024pyritframeworksecurityrisk,\n      title={PyRIT: A Framework for Security Risk Identification and Red Teaming in Generative AI Systems},\n      author={Gary D. Lopez Munoz and Amanda J. Minnich and Roman Lutz and Richard Lundeen and Raja Sekhar Rao Dheekonda and Nina Chikanov and Bolor-Erdene Jagdagdorj and Martin Pouliot and Shiven Chawla and Whitney Maxwell and Blake Bullwinkel and Katherine Pratt and Joris de Gruyter and Charlotte Siska and Pete Bryan and Tori Westerhoff and Chang Kawaguchi and Christian Seifert and Ram Shankar Siva Kumar and Yonatan Zunger},\n      year={2024},\n      eprint={2410.02828},\n      archivePrefix={arXiv},\n      primaryClass={cs.CR},\n      url={https://arxiv.org/abs/2410.02828},\n}\n```\n\nAdditionally, please cite the tool itself following the `CITATION.cff` file in the root of this repository.\n"
    },
    {
      "name": "Azure-Samples/chat-with-your-data-solution-accelerator",
      "stars": 994,
      "img": "https://avatars.githubusercontent.com/u/1844662?s=40&v=4",
      "owner": "Azure-Samples",
      "repo_name": "chat-with-your-data-solution-accelerator",
      "description": "A Solution Accelerator for the RAG pattern running in Azure, using Azure AI Search for retrieval and Azure OpenAI large language models to power ChatGPT-style and Q&A experiences. This includes most common requirements and best practices.",
      "homepage": "https://azure.microsoft.com/products/search",
      "language": "Python",
      "created_at": "2023-06-06T01:40:48Z",
      "updated_at": "2025-04-21T19:35:50Z",
      "topics": [
        "ai-search",
        "azd-templates",
        "azure",
        "azure-openai",
        "openai"
      ],
      "readme": "---\nname: Chat with your data - Solution accelerator (Python)\ndescription: Chat with your data using OpenAI and AI Search with Python.\nlanguages:\n- python\n- typescript\n- bicep\n- azdeveloper\nproducts:\n- azure-openai\n- azure-cognitive-search\n- azure-app-service\n- azure\n- azure-bot-service\n- document-intelligence\n- azure-functions\n- azure-storage-accounts\n- azure-speech\npage_type: sample\nurlFragment: chat-with-your-data-solution-accelerator\n\n---\n<!-- YAML front-matter schema: https://review.learn.microsoft.com/en-us/help/contribute/samples/process/onboarding?branch=main#supported-metadata-fields-for-readmemd -->\n\n# Chat with your data - Solution accelerator\n\n\n ##### Table of Contents\n- [Chat with your data - Solution accelerator](#chat-with-your-data---solution-accelerator)\n        - [Table of Contents](#table-of-contents)\n  - [User story](#user-story)\n    - [About this repo](#about-this-repo)\n    - [When should you use this repo?](#when-should-you-use-this-repo)\n    - [Key features](#key-features)\n    - [Target end users](#target-end-users)\n    - [Industry scenario](#industry-scenario)\n  - [Deploy](#deploy)\n    - [Pre-requisites](#pre-requisites)\n    - [Products used](#products-used)\n    - [Required licenses](#required-licenses)\n    - [Pricing Considerations](#pricing-considerations)\n    - [Deploy instructions](#deploy-instructions)\n    - [Testing the deployment](#testing-the-deployment)\n  - [Supporting documentation](#supporting-documentation)\n    - [Resource links](#resource-links)\n    - [Licensing](#licensing)\n  - [Disclaimers](#disclaimers)\n## User story\nWelcome to the *Chat with your data* Solution accelerator repository! The *Chat with your data* Solution accelerator is a powerful tool that combines the capabilities of Azure AI Search and Large Language Models (LLMs) to create a conversational search experience. This solution accelerator uses an Azure OpenAI GPT model and an Azure AI Search index generated from your data, which is integrated into a web application to provide a natural language interface, including [speech-to-text](docs/speech_to_text.md) functionality, for search queries. Users can drag and drop files, point to storage, and take care of technical setup to transform documents. Everything can be deployed in your own subscription to accelerate your use of this technology.\n\n\n\n\n### About this repo\n\nThis repository provides an end-to-end solution for users who want to query their data with natural language. It includes a well designed ingestion mechanism for multiple file types, an easy deployment, and a support team for maintenance. The accelerator demonstrates both Push or Pull Ingestion; the choice of orchestration (Semantic Kernel, LangChain, OpenAI Functions or [Prompt Flow](docs/prompt_flow.md)) and should be the minimum components needed to implement a RAG pattern. It is not intended to be put into Production as-is without experimentation or evaluation of your data. It provides the following features:\n\n* Chat with an Azure OpenAI model using your own data\n* Upload and process your documents\n* Index public web pages\n* Easy prompt configuration\n* Multiple chunking strategies\n\n### When should you use this repo?\n\nIf you need to customize your scenario beyond what [Azure OpenAI on your data](https://learn.microsoft.com/azure/ai-services/openai/concepts/use-your-data) offers out-of-the-box, use this repository.\nBy default, this repo comes with one specific set of RAG configurations including but not limited to: chunk size, overlap, retrieval/search type and system prompt. It is important that you evaluate the retrieval/search and the generation of the answers for your data and tune these configurations accordingly before you use this repo in production. For a starting point to understand and perform RAG evaluations, we encourage you to look into the [RAG Experiment Accelerator](https://github.com/microsoft/rag-experiment-accelerator).\n\nThe accelerator presented here provides several options, for example:\n* The ability to ground a model using both data and public web pages\n* A backend with support for 'custom' and 'On Your Data' [conversation flows](./docs/conversation_flow_options.md)\n* Advanced prompt engineering capabilities\n* An admin site for ingesting/inspecting/configuring your dataset on the fly\n* Push or Pull model for data ingestion:  See [integrated vectorization](./docs/integrated_vectorization.md) documentation for more details\n* Running a Retrieval Augmented Generation (RAG) solution locally\n\n*Have you seen [ChatGPT + Enterprise data with Azure OpenAI and AI Search demo](https://github.com/Azure-Samples/azure-search-openai-demo)? If you would like to experiment: Play with prompts, understanding RAG pattern different implementation approaches, see how different features interact with the RAG pattern and choose the best options for your RAG deployments, take a look at that repo.\n\nHere is a comparison table with a few features offered by Azure, an available GitHub demo sample and this repo, that can provide guidance when you need to decide which one to use:\n\n| Name\t| Feature or Sample? |\tWhat is it? | When to use? |\n| ---------|---------|---------|---------|\n|[\"Chat with your data\" Solution Accelerator](https://aka.ms/ChatWithYourDataSolutionAccelerator) - (This repo)\t| Azure sample | End-to-end baseline RAG pattern sample that uses Azure AI Search as a retriever.\t| This sample should be used by Developers when the  RAG pattern implementations provided by Azure are not able to satisfy business requirements. This sample provides a means to customize the solution. Developers must add their own code to meet requirements, and adapt with best practices according to individual company policies. |\n|[Azure OpenAI on your data](https://learn.microsoft.com/azure/ai-services/openai/concepts/use-your-data) | Azure feature | Azure OpenAI Service offers out-of-the-box, end-to-end RAG implementation that uses a REST API or the web-based interface in the Azure AI Foundry to create a solution that connects to your data to enable an enhanced chat experience with Azure OpenAI ChatGPT models and Azure AI Search. | This should be the first option considered for developers that need an end-to-end solution for Azure OpenAI Service with an Azure AI Search retriever. Simply select supported data sources, that ChatGPT model in Azure OpenAI Service , and any other Azure resources needed to configure your enterprise application needs. |\n|[Azure Machine Learning prompt flow](https://learn.microsoft.com/azure/machine-learning/concept-retrieval-augmented-generation)\t| Azure feature | RAG in Azure Machine Learning is enabled by integration with Azure OpenAI Service for large language models and vectorization. It includes support for Faiss and Azure AI Search as vector stores, as well as support for open-source offerings, tools, and frameworks such as LangChain for data chunking. Azure Machine Learning prompt flow offers the ability to test data generation, automate prompt creation, visualize prompt evaluation metrics, and integrate RAG workflows into MLOps using pipelines.  | When Developers need more control over processes involved in the development cycle of LLM-based AI applications, they should use Azure Machine Learning prompt flow to create executable flows and evaluate performance through large-scale testing. |\n|[ChatGPT + Enterprise data with Azure OpenAI and AI Search demo](https://github.com/Azure-Samples/azure-search-openai-demo) | Azure sample | RAG pattern demo that uses Azure AI Search as a retriever. | Developers who would like to use or present an end-to-end demonstration of the RAG pattern should use this sample. This includes the ability to deploy and test different retrieval modes, and prompts to support business use cases. |\n|[RAG Experiment Accelerator](https://github.com/microsoft/rag-experiment-accelerator) | Tool |The RAG Experiment Accelerator is a versatile tool that helps you conduct experiments and evaluations using Azure AI Search and RAG pattern. | RAG Experiment Accelerator is to make it easier and faster to run experiments and evaluations of search queries and quality of response from OpenAI. This tool is useful for researchers, data scientists, and developers who want to, Test the performance of different Search and OpenAI related hyperparameters. |\n\n\n### Key features\n- **Private LLM access on your data**: Get all the benefits of ChatGPT on your private, unstructured data.\n- **Single application access to your full data set**: Minimize endpoints required to access internal company knowledgebases. Reuse the same backend with the [Microsoft Teams Extension](docs/teams_extension.md)\n- **Natural language interaction with your unstructured data**: Use natural language to quickly find the answers you need and ask follow-up queries to get the supplemental details, including [Speech-to-text](docs/speech_to_text.md).\n- **Easy access to source documentation when querying**: Review referenced documents in the same chat window for additional context.\n- **Chat history**: Prior conversations and context are maintained and accessible through chat history.\n- **Data upload**: Batch upload documents of [various file types](docs/supported_file_types.md)\n- **Accessible orchestration**: Prompt and document configuration (prompt engineering, document processing, and data retrieval)\n- **Database flexibility**: Dynamic database switching allows users to choose between PostgreSQL and Cosmos DB based on their requirements. If no preference is specified the platform defaults to PostgreSQL.\n\n\n**Note**: The current model allows users to ask questions about unstructured data, such as PDF, text, and docx files. See the [supported file types](docs/supported_file_types.md).\n\n\n### Target end users\nCompany personnel (employees, executives) looking to research against internal unstructured company data would leverage this accelerator using natural language to find what they need quickly.\n\nThis accelerator also works across industry and roles and would be suitable for any employee who would like to get quick answers with a ChatGPT experience against their internal unstructured company data.\n\nTech administrators can use this accelerator to give their colleagues easy access to internal unstructured company data. Admins can customize the system configurator to tailor responses for the intended audience.\n\n\n### Use Case scenarios\n\n#### Employee Onboarding Scenario\nThe sample data illustrates how this accelerator could be used for an employee onboarding scenario in across industries.\n\nIn this scenario, a newly hired employee is in the process of onboarding to their organization. Leveraging the solution accelerator, she navigates through the extensive offerings of her organization’s health and retirement benefits. With the newly integrated chat history capabilities, they can revisit previous conversations, ensuring continuity and context across multiple days of research. This functionality allows the new employee to efficiently gather and consolidate information, streamlining their onboarding experience. [For more details, refer to the README](docs/employee_assistance.md).\n\n#### Financial Advisor Scenario\nThe sample data illustrates how this accelerator could be used in the financial services industry (FSI).\n\nIn this scenario, a financial advisor is preparing for a meeting with a potential client who has expressed interest in Woodgrove Investments’ Emerging Markets Funds. The advisor prepares for the meeting by refreshing their understanding of the emerging markets fund's overall goals and the associated risks.\n\nNow that the financial advisor is more informed about Woodgrove’s Emerging Markets Funds, they're better equipped to respond to questions about this fund from their client.\n\n#### Contract Review and Summarization Assistant scenario\nAdditionally, we have implemented a Legal Review and Summarization Assistant scenario to demonstrate how this accelerator can be utilized in any industry. The Legal Review and Summarization Assistant helps professionals manage and interact with a large collection of documents efficiently. For more details, refer to the [Contract Review and Summarization Assistant README](docs/contract_assistance.md).\n\nNote: Some of the sample data included with this accelerator was generated using AI and is for illustrative purposes only.\n\n\n---\n\n![One-click Deploy](/docs/images/oneClickDeploy.png)\n## Deploy\n### Pre-requisites\n- Azure subscription - [Create one for free](https://azure.microsoft.com/free/) with owner access.\n- Approval to use Azure OpenAI services with your Azure subcription. To apply for approval, see [here](https://learn.microsoft.com/en-us/azure/ai-services/openai/overview#how-do-i-get-access-to-azure-openai).\n- [Enable custom Teams apps and turn on custom app uploading](https://learn.microsoft.com/en-us/microsoftteams/platform/concepts/build-and-test/prepare-your-o365-tenant#enable-custom-teams-apps-and-turn-on-custom-app-uploading) (optional: Teams extension only)\n\n### Products used\n- Azure App Service\n- Azure Application Insights\n- Azure Bot\n- Azure OpenAI\n- Azure Document Intelligence\n- Azure Function App\n- Azure Search Service\n- Azure Storage Account\n- Azure Speech Service\n- Azure CosmosDB\n- Azure PostgreSQL\n- Teams (optional: Teams extension only)\n\n### Required licenses\n- Microsoft 365 (optional: Teams extension only)\n\n### Pricing Considerations\n\nThis solution accelerator deploys multiple resources. Evaluate the cost of each component prior to deployment.\n\nThe following are links to the pricing details for some of the resources:\n- [Azure OpenAI service pricing](https://azure.microsoft.com/pricing/details/cognitive-services/openai-service/). GPT and embedding models are charged separately.\n- [Azure AI Search pricing](https://azure.microsoft.com/pricing/details/search/). AI Search core service and semantic ranker are charged separately.\n- [Azure Blob Storage pricing](https://azure.microsoft.com/pricing/details/storage/blobs/)\n- [Azure Functions pricing](https://azure.microsoft.com/pricing/details/functions/)\n- [Azure AI Document Intelligence pricing](https://azure.microsoft.com/pricing/details/ai-document-intelligence/)\n- [Azure Web App Pricing](https://azure.microsoft.com/pricing/details/app-service/windows/)\n\n### Deployment options: PostgreSQL or Cosmos DB\nWith the addition of PostgreSQL, customers can leverage the power of a relationship-based AI solution to enhance historical conversation access, improve data privacy, and open the possibilities for scalability.\n\nCustomers have the option to deploy this solution with PostgreSQL or Cosmos DB. Consider the following when deciding which database to use:\n- PostgreSQL enables a relationship-based AI solution and search indexing for Retrieval Augmented Generation (RAG)\n- Cosmos DB enables chat history and is a NoSQL-based solution. With Cosmos DB, Azure AI Search is used for storing extracted documents and embeddings.\n\n\nTo review PostgreSQL configuration overview and steps, follow the link [here](docs/postgreSQL.md).\n![Solution Architecture - Chat with your data PostgreSQL](/docs/images/architrecture_pg.png)\n\nTo review Cosmos DB configuration overview and steps, follow the link [here](docs/employee_assistance.md).\n![Solution Architecture - Chat with your data CosmosDB](/docs/images/architecture_cdb.png)\n\n### Deploy instructions\nThe \"Deploy to Azure\" button offers a one-click deployment where you don’t have to clone the code. If you would like a developer experience instead, follow the [local deployment instructions](./docs/LOCAL_DEPLOYMENT.md).\n\nOnce you deploy to Azure, you will have the option to select PostgreSQL or Cosmos DB, see screenshot below.\n\n[![Deploy to Azure](https://aka.ms/deploytoazurebutton)](https://portal.azure.com/#create/Microsoft.Template/uri/https%3A%2F%2Fraw.githubusercontent.com%2FAzure-Samples%2Fchat-with-your-data-solution-accelerator%2Frefs%2Fheads%2Fmain%2Finfra%2Fmain.json)\n\nSelect either \"PostgreSQL\" or \"Cosmos DB\":\n![Solution Architecture - DB Selection](/docs/images/db_selection.png)\n\n\nWhen Deployment is complete, follow steps in [Set Up Authentication in Azure App Service](./docs/azure_app_service_auth_setup.md) to add app authentication to your web app running on Azure App Service\n\n**Note**: The default configuration deploys an OpenAI Model \"gpt-4o\" with version 2024-05-13. However, not all\nlocations support this version. If you're deploying to a location that doesn't support version 2024-05-13, you'll need to\nswitch to a lower version. To find out which versions are supported in different regions, visit the\n[GPT-4o Model Availability](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models?tabs=global-standard%2Cstandard-chat-completions#global-standard-model-availability) page.\n\n### Testing the deployment\n1. Navigate to the admin site, where you can upload documents. It will be located at:\n\n    `https://web-{RESOURCE_TOKEN}-admin.azurewebsites.net/`\n\n    Where `{RESOURCE_TOKEN}` is uniquely generated during deployment. This is a combination of your subscription and the name of the resource group. Then select **Ingest Data** and add your data. You can find sample data in the `/data` directory.\n\n    ![A screenshot of the admin site.](./docs/images/admin-site.png)\n\n\n2. Navigate to the web app to start chatting on top of your data. The web app can be found at:\n\n    `https://web-{RESOURCE_TOKEN}.azurewebsites.net/`\n\n\n    ![A screenshot of the chat app.](./docs/images/web-unstructureddata.png)\n\n\n\n\n![Supporting documentation](/docs/images/supportingDocuments.png)\n\n## Supporting documentation\n\n### Resource links\n\nThis solution accelerator deploys the following resources. It's critical to comprehend the functionality of each. Below are the links to their respective documentation:\n- [Application Insights overview - Azure Monitor | Microsoft Learn](https://learn.microsoft.com/en-us/azure/azure-monitor/app/app-insights-overview?tabs=net)\n- [Azure OpenAI Service - Documentation, quickstarts, API reference - Azure AI services | Microsoft Learn](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/use-your-data)\n- [Using your data with Azure OpenAI Service - Azure OpenAI | Microsoft Learn](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/use-your-data)\n- [Content Safety documentation - Quickstarts, Tutorials, API Reference - Azure AI services | Microsoft Learn](https://learn.microsoft.com/en-us/azure/ai-services/content-safety/)\n- [Document Intelligence documentation - Quickstarts, Tutorials, API Reference - Azure AI services | Microsoft Learn](https://learn.microsoft.com/en-us/azure/ai-services/document-intelligence/?view=doc-intel-3.1.0)\n- [Azure Functions documentation | Microsoft Learn](https://learn.microsoft.com/en-us/azure/azure-functions/)\n- [Azure Cognitive Search documentation | Microsoft Learn](https://learn.microsoft.com/en-us/azure/search/)\n- [Speech to text documentation - Tutorials, API Reference - Azure AI services - Azure AI services | Microsoft Learn](https://learn.microsoft.com/en-us/azure/ai-services/speech-service/index-speech-to-text)\n- [Bots in Microsoft Teams - Teams | Microsoft Learn](https://learn.microsoft.com/en-us/microsoftteams/platform/bots/what-are-bots) (Optional: Teams extension only)\n\n### Licensing\n\nThis repository is licensed under the [MIT License](LICENSE.md).\n\nThe data set under the /data folder is licensed under the [CDLA-Permissive-2 License](CDLA-Permissive-2.md).\n\n## Disclaimers\nThis Software requires the use of third-party components which are governed by separate proprietary or open-source licenses as identified below, and you must comply with the terms of each applicable license in order to use the Software. You acknowledge and agree that this license does not grant you a license or other right to use any such third-party proprietary or open-source components.\n\nTo the extent that the Software includes components or code used in or derived from Microsoft products or services, including without limitation Microsoft Azure Services (collectively, “Microsoft Products and Services”), you must also comply with the Product Terms applicable to such Microsoft Products and Services. You acknowledge and agree that the license governing the Software does not grant you a license or other right to use Microsoft Products and Services. Nothing in the license or this ReadMe file will serve to supersede, amend, terminate or modify any terms in the Product Terms for any Microsoft Products and Services.\n\nYou must also comply with all domestic and international export laws and regulations that apply to the Software, which include restrictions on destinations, end users, and end use. For further information on export restrictions, visit https://aka.ms/exporting.\n\nYou acknowledge that the Software and Microsoft Products and Services (1) are not designed, intended or made available as a medical device(s), and (2) are not designed or intended to be a substitute for professional medical advice, diagnosis, treatment, or judgment and should not be used to replace or as a substitute for professional medical advice, diagnosis, treatment, or judgment. Customer is solely responsible for displaying and/or obtaining appropriate consents, warnings, disclaimers, and acknowledgements to end users of Customer’s implementation of the Online Services.\n\nYou acknowledge the Software is not subject to SOC 1 and SOC 2 compliance audits. No Microsoft technology, nor any of its component technologies, including the Software, is intended or made available as a substitute for the professional advice, opinion, or judgement of a certified financial services professional. Do not use the Software to replace, substitute, or provide professional financial advice or judgment.\n\nBY ACCESSING OR USING THE SOFTWARE, YOU ACKNOWLEDGE THAT THE SOFTWARE IS NOT DESIGNED OR INTENDED TO SUPPORT ANY USE IN WHICH A SERVICE INTERRUPTION, DEFECT, ERROR, OR OTHER FAILURE OF THE SOFTWARE COULD RESULT IN THE DEATH OR SERIOUS BODILY INJURY OF ANY PERSON OR IN PHYSICAL OR ENVIRONMENTAL DAMAGE (COLLECTIVELY, “HIGH-RISK USE”), AND THAT YOU WILL ENSURE THAT, IN THE EVENT OF ANY INTERRUPTION, DEFECT, ERROR, OR OTHER FAILURE OF THE SOFTWARE, THE SAFETY OF PEOPLE, PROPERTY, AND THE ENVIRONMENT ARE NOT REDUCED BELOW A LEVEL THAT IS REASONABLY, APPROPRIATE, AND LEGAL, WHETHER IN GENERAL OR IN A SPECIFIC INDUSTRY. BY ACCESSING THE SOFTWARE, YOU FURTHER ACKNOWLEDGE THAT YOUR HIGH-RISK USE OF THE SOFTWARE IS AT YOUR OWN RISK.\n"
    },
    {
      "name": "akshata29/entaoai",
      "stars": 859,
      "img": "https://avatars.githubusercontent.com/u/18509807?s=40&v=4",
      "owner": "akshata29",
      "repo_name": "entaoai",
      "description": "Chat and Ask on your own data.  Accelerator to quickly upload your own enterprise data and use OpenAI services to chat to that uploaded data and ask questions",
      "homepage": "",
      "language": "TypeScript",
      "created_at": "2023-03-16T13:06:30Z",
      "updated_at": "2025-04-21T19:38:30Z",
      "topics": [
        "azure",
        "azure-functions",
        "azure-openai",
        "azure-webapp",
        "azureopenai",
        "chatgpt",
        "cognitive-search",
        "gpt-3",
        "gpt-35-turbo",
        "langchain",
        "openai",
        "pinecone",
        "redis-search",
        "vector-store"
      ],
      "readme": "# Chat with your enterprise data using LLM\n\nThis sample demonstrates a few approaches for creating ChatGPT-like experiences over your own data. It uses Azure OpenAI Service to access the ChatGPT model (gpt-35-turbo and gpt3), and vector store (Pinecone, Redis and others) or Azure cognitive search for data indexing and retrieval.\n\nThe repo provides a way to upload your own data so it's ready to try end to end.\n\n## Updates\n\n* 3/30/2024 - Refactored to keep on Chat, Chat Stream, QnA, Upload and Admin functionality.  All others will be moved to it's own repo.\n* 3/10/2024 - Move the Prompt Flow version to [entaoaipf](https://github.com/akshata29/entaoaipf)\n* 3/9/2024 - Initial version of advanced RAG techniques and Multi-modal RAG pattern\n* 2/28/2024 - Removed SEC analysis features and it's moved into it's own repo at [sec](https://github.com/akshata29/sec)\n* 1/28/2024 - Remove PitchBook features as they are moved into it's own repo at [pib](https://github.com/akshata29/pitchbook)\n* 1/19/2024 - Updated the python package & OpenAI > 1.0.  Changes made to all Python API for breaking changes introduced in OpenAI and langchain.\n* 10/12/2023 - Initial version of [Autonomous](./api/PromptFlow/Autonomous/) PromptFlow.  For now supporting the Pinecone indexes, but support for Cognitive Search and Redis will be updated soon.\n* 9/29/2023 - Added [Evaluate](./api/PromptFlow/Evaluate/) PromptFlow.  Prompt Flow once created in Azure ML, can be attached to your existing run to evaluate against the following evaluation process :\n  * Groundness - The Q&A Groundedness evaluation flow will evaluate the Q&A Retrieval Augmented Generation systems by leveraging the state-of-the-art Large Language Models (LLM) to measure the quality and safety of your responses. Utilizing GPT-3.5 as the Language Model to assist with measurements aims to achieve a high agreement with human evaluations compared to traditional mathematical measurements. gpt_groundedness (against context): Measures how grounded the model's predicted answers are against the context. Even if LLM’s responses are true, if not verifiable against context, then such responses are considered ungrounded.\n  * Ada Similarity - The Q&A ada_similarity evaluation flow will evaluate the Q&A Retrieval Augmented Generation systems by leveraging the state-of-the-art Large Language Models (LLM) to measure the quality and safety of your responses. Utilizing GPT-3.5 as the Language Model to assist with measurements aims to achieve a high agreement with human evaluations compared to traditional mathematical measurements. The Ada Similarity evaluation flow allows you to assess and evaluate your model with the LLM-assisted ada similarity metri ada_similarity: Measures the cosine similarity of ada embeddings of the model prediction and the ground truth. ada_similarity is a value in the range [0, 1].\n  * Coherence - The Q&A Coherence evaluation flow will evaluate the Q&A Retrieval Augmented Generation systems by leveraging the state-of-the-art Large Language Models (LLM) to measure the quality and safety of your responses. Utilizing GPT-3.5 as the Language Model to assist with measurements aims to achieve a high agreement with human evaluations compared to traditional mathematical measurements. The Coherence evaluation flow allows you to assess and evaluate your model with the LLM-assisted Coherence metric. gpt_coherence: Measures the quality of all sentences in a model's predicted answer and how they fit together naturally. Coherence is scored on a scale of 1 to 5, with 1 being the worst and 5 being the best.\n  * Similarity - The Q&A Similarity evaluation flow will evaluate the Q&A Retrieval Augmented Generation systems by leveraging the state-of-the-art Large Language Models (LLM) to measure the quality and safety of your responses. Utilizing GPT-3.5 as the Language Model to assist with measurements aims to achieve a high agreement with human evaluations compared to traditional mathematical measurements.  The Similarity evaluation flow allows you to assess and evaluate your model with the LLM-assisted Similarity metric. gpt_similarity: Measures similarity between user-provided ground truth answers and the model predicted answer. Similarity is scored on a scale of 1 to 5, with 1 being the worst and 5 being the best.\n  * F1 Score - The Q&A f1-score evaluation flow will evaluate the Q&A Retrieval Augmented Generation systems using f1-score based on the word counts in predicted answer and ground truth. The f1-score evaluation flow allows you to determine the f1-score metric using number of common tokens between the normalized version of the ground truth and the predicted answer. F1-score: Compute the f1-Score based on the tokens in the predicted answer and the ground truth. F1-score is a value in the range [0, 1].\nGroundedness metric is scored on a scale of 1 to 5, with 1 being the worst and 5 being the best.\n* 9/22/2023 - Added PromptFlow for [SqlAsk](./api/PromptFlow/SqlAsk/).  Ensure `PFSQLASK_URL` and `PFSQLASK_KEY` configuration values are added to deployed endpoint to enable the feature.  Also make sure `SynapseName`, `SynapsePool`, `SynapseUser` and `SynapsePassword` configuration values are added to `entaoai` PromptFlow connection.  Moved deleting the Session Capability for ChatGpt to Admin Page.\n* 9/20/2023 - Added configuration to allow end user to change the Search Type for Cognitive Search Vector Store index (Hybrid, Similarity/Vector and Hybrid Re-rank), based on the [Best Practices](https://techcommunity.microsoft.com/t5/azure-ai-services-blog/azure-cognitive-search-outperforming-vector-search-with-hybrid/ba-p/3929167) we shared.  QnA, Chat and Prompt Flow are modified.  QnA and Chat are implementing the customized Vector store implementation of Langchain and Prompt Flow using the helper functions.  Fixed the issue with QnA/Chat/PromptFlow not generating followup-questions.\n* 9/18/2023 - Refactored SQL NLP to not use Langchain Database Agent/Chain and instead use custom Prompts.\n* 9/15/2023 - Modified the azure search package to 11.4.0b9 and langchain to latest version.  Added capability to perform [evaluation](https://learn.microsoft.com/en-us/azure/machine-learning/prompt-flow/how-to-develop-an-evaluation-flow?view=azureml-api-2) on PromptFlow for both QnA and Chat.  [Bert PDF](./Workshop/Data/PDF/Bert.pdf) and [Evaluation Data](./api/PromptFlow/QuestionAnswering/bert.jsonl) can be used to perform Batch and Evaluation in Prompt Flow.  [Sample Notebook](./Workshop/12_PromptFlowQa.ipynb) showcasing the flow and E2E process is available.  [Bert Chat](./Workshop/promptflow/BertChat/) folder allows you to test E2E Prompt Flow, Batch Run and Evaluation in form of Notebook.\n* 9/3/2023 - Added [API](./api/PromptFlow/Chat/) for [Chat](./assets/ChatPf.png) using the Prompt Flow.  Allow end-user to select between Azure Functions as API (`ApiType` Configuration in Web App) or using Prompt Flow Managed endpoint.\n* 9/2/2023 - Added [API](./api/PromptFlow/QuestionAnswering/) for [Question Answering](./assets/QaPf.png) using the Prompt Flow.  Allow end-user to select between Azure Functions as API (`ApiType` Configuration in Web App) or using Prompt Flow Managed endpoint.\n* 8/31/2023 - Added example for [LLMOps](LLMOps.md) using [Prompt Flow](https://learn.microsoft.com/en-us/azure/machine-learning/prompt-flow/overview-what-is-prompt-flow?view=azureml-api-2).  The repo will be adding the flexibility to use the Prompt Flow Deployed Model as an alternative to current Azure Functions.\n* 8/20/2023 - Added support for the Markdown files (as zip file) and removed the chunk_size=1 from Azure OpenAiEmbedding\n* 8/11/2023 - Fixed the issue with Streaming Chat feature.\n* 8/10/2023 - **Breaking Changes** - Refactored all code to use `OpenAiEndPoint` configuration value instead of `OpenAiService`.  It is to support the best practices as they are outlined in [Enterprise Logging](https://github.com/Azure-Samples/openai-python-enterprise-logging) via Azure API Management. Your `OpenAiEndPoint` if using APIM will be API Gateway URL and the `OpenAiKey` will be the Product/Unlimited key.   If not using APIM, you don't need to change the key, but ensure `OpenAiEndPoint` is fully qualified URL of your AOAI deployment.  `OpenAiService` is no longer used.  Changes did impact the working on Chat on Stream feature, so it's disabled for now and will be enabled once tested and fixed.\n* 8/9/2023 - Added Function calling in the ChatGpt interface as checkbox.  Sample demonstrate ability to call functions.  Currently [Weather API](https://rapidapi.com/apishub/api/yahoo-weather5), [Stock API](https://rapidapi.com/alphavantage/api/alpha-vantage) and Bing Search is supported. Function calling is in preview and supported only from \"API Version\" of \"2023-07-01-preview\", so make sure you update existing deployment to use that version.  Details on calling [Functions](https://github.com/Azure-Samples/openai/blob/main/Basic_Samples/Functions/working_with_functions.ipynb).  For existing deployment add `WeatherEndPoint`, `WeatherHost`, `StockEndPoint`, `StockHost` and `RapidApiKey` configuration to Azure Function App.\n* 8/5/2023 - Added Chat Interface with \"Stream\" Option.  This feature allows you to stream the conversation to the client.  You will need to add `OpenAiChat`, `OpenAiEmbedding`, `OpenAiEndPoint`, `OpenAiKey`, `OpenAiApiKey`, `OpenAiService`, `OpenAiVersion`, `PineconeEnv`, `PineconeIndex`, `PineconeKey`, `RedisAddress`, `RedisPassword`, `RedisPort` property in Azure App Service (Webapp) to enable the feature for existing deployment.\n* 7/30/2023 - Removed unused Code - SummaryAndQa and Chat\n* 7/28/2023 - Started removing the Davinci model usage.  For now removed the usage from all functionality except workshop.  Refactored Summarization functionality based on the feedback to allow user to specify the prompt and pre-defined Topics to summarize it on.\n* 7/26/2023 - Remove OpenAI Playground from Developer Tools as advanced features of that are available in ChatGPT section.\n* 7/25/2023 - Add tab for the Chat capabilities to support ChatGpt capability directly from the model instead of \"Chat on Data\".  You will need to add `CHATGPT_URL` property in Azure App Service (Webapp) to enable the feature outside of deploying the new Azure Function.\n* 7/23/2023 - Added the rest of the feature for PIB UI and initial version of generating the PowerPoint deck as the output.  For new feature added ensure you add `FMPKEY` variable to webapp configuration.\n* 7/20/2023 - Added feature to talk to Pib Data (Sec Filings & Earning Call Transcript).  Because new Azure function is deployed, ensure `PIBCHAT_URL` property is added to Azure WebApp with the URL for your deployed Azure Functions\n* 7/18/2023 - Refactored the PIB code to solve some of the performance issue and bug fixes.\n* 7/17/2023 - Removed GPT3 chat interface with retirement of \"Davinci\" models.\n* 7/16/2023 - Initial version of Pib UI (currently supporting 5 Steps - Company Profile, Call Transcripts, Press Releases, Sec Filings and Ratings/Recommendations).  You will need access to Paid subscription (FMP or modify based on what your enterprise have access to).  To use with FMP you will need to add `FmpKey` in Azure Functions.  Because of circular dependency you need to manually add `SecDocPersistUrl` and `SecExtractionUrl` manually in Azure Functions.\n* 7/14/2023 - Add support for GPT3.5 16K model and ability to chunk document > 4000 tokens with > 500 overlap.  For the ChunkSize > 4000, it will default to 16K token for both QnA and Chat functionality. Added identity provider to the application and authentication for QnA and Chat interface.  For GPT3.5 16k model, you will need to add `OpenAiChat16k` property in Azure Function app.\n* 7/13/2023 - Allow end user to select ChunkSize and ChunkOverlap Configuration.  Initial version of overriding prompt template.\n* 7/11/2023 - Functional PIB CoPilot in the form of the [notebook](./Workshop/10_PibCoPilot.ipynb).\n* 7/8/2023 - Added the feature to Rename the session for ChatGPT.   Also added the UI for the Evaluator Tool.  This feature focuses on performing the LLM based evaluation on your document. It auto-generates the test dataset (with Question and Answers) and perform the grading on that document using different parameters and generates the evaluation results.  It is built on [Azure Durable Functions](https://learn.microsoft.com/en-us/azure/azure-functions/durable/durable-functions-overview?tabs=csharp-inproc) and is implemented using the [Function Chaining](https://learn.microsoft.com/en-us/azure/azure-functions/durable/durable-functions-sequence?tabs=csharp) pattern. You will need to add `BLOB_EVALUATOR_CONTAINER_NAME` (ensure the same container name is created in storage account) and `RUNEVALUATION_URL`  (URL of the Durable function deployment) configuration in Azure Web App for existing deployment and if you want to use the Evaluator feature.  In the Azure function deployment add `AzureWebJobsFeatureFlags` (value EnableWorkerIndexing) and `OpenAiEvaluatorContainer` settings.\n* 7/5/2023 - Added the feature to Delete the session.  That feature requires the feature that is in [preview](https://learn.microsoft.com/en-us/azure/cosmos-db/nosql/how-to-delete-by-partition-key?tabs=python-example) and you will need to enable that on the CosmosDB account on your subscription.  Added simple try/catch block in case if you have not enabled/deployed the CosmosDB to continue chatGPT implementation.\n* 7/4/2023 - Initial version of storing \"Sessions\" for GPT3.5/ChatGpt interface.  Session and messages are stored/retrieved from CosmosDb.  Make sure you have CosmosDb service provisioned or create a new one (for existing deployment).  You will need to add `CosmosEndpoint`, `CosmosKey`, `CosmosDatabase` and `CosmosContainer` settings in both Azure Functions App and Web App.\n* 6/25/2023 - Notebook [showcasing](#qa-llm-evaluation) the evaluation of the answer quality in systematic way (auto generating questions and evaluation chain), supporting LLM QA settings (chunk size, overlap, embedding technique). Refer to [Evaluator](./Workshop/99_Evaluator.ipynb) notebook for more information.\n* 6/18/2023 - Add the admin page supporting Knowledge base management.  \n* 6/17/2023 - Added \"Question List\" button for Ask a question feature to display the list of all the questions that are in the Knowledge base.  Following three properties `SEARCHSERVICE` and `KBINDEXNAME` (default value of aoaikb) needs to be added to Azure App Service to enable \"Question List\" button feature.\n* 6/16/2023 - Add the feature to use Azure Cognitive Search as Vector store for storing the [cached Knowledge base](#qa-over-your-data-with-cache).  The questions that are not in KB are sent to LLM model to find the answer via OAI, or else it is responded back from the Cached Datastore.  New Property `KbIndexName` needs to be added to Azure Function app.  Added the Notebook to test out the feature as part of the workshop. TODO : Add the feature to add the question to KB from the chat interface (and make it session based). A feature further to \"regenerate\" answer from LLM (instead of cached answer) will be added soon.  \n* 6/7/2023 - Add OpenAI Playground in Developer Tools and initial version of building the CoPilot (for now with Notebook, but eventually will be moved as CoPilot feature).  Add the script, recording and example for Real-time Speech analytics use-case.  More to be added soon.\n* 5/27/2023 - Add Workshop content in the form of the notebooks that can be leveraged to learn/execute the scenarios.  You can find the notebooks in the [Workshop](Workshop) folder.  Details about workshop content is available [here](READMEWORKSHOP.md).\n* 5/26/2023 - Add Summarization feature to summarize the document either using stuff, mapreduce or refine summarization.  To use this feature (on existing deployment) ensure you add the `OpenAiSummaryContainer` configuration to Function app and `BLOB_SUMMARY_CONTAINER_NAME` configuration to Azure App Service (Ensure that the value you enter is the same as the container name in Azure storage and that you have created the container).  You also need to add `PROCESSSUMMARY_URL` configuration to Azure App Service (Ensure that the value you enter is the same as the Azure Function URL).\n* 5/24/2023 - Add feature to upload CSV files and CSV Agent to answer/chat questions on the tabular data.  Smart Agent also supports answering questions on CSV data.\n* 5/22/2023 - Initial version of \"Smart Agent\" that gives you flexibility to talk to all documents uploaded in the solution.  It also allow you to talk to SQL Database Scenario.  As more features are added, agent will keep on building upon that (for instance talk to CSV/Excel or Tabular data)\n* 5/21/2023 - Add Developer Tools section - Experimental code conversion and Prompt guru.\n* 5/17/2023 - Change the edgar source to Cognitive search vector store instead of Redis.\n* 5/15/2023 - Add the option to use \"Cognitive Search\" as Vector store for storing the index.  Azure Cognitive Search offers pure vector search and hybrid retrieval – as well as a sophisticated re-ranking system powered by Bing in a single integrated solution. [Sign-up](https://aka.ms/VectorSearchSignUp). Support uploading WORD documents.\n* 5/10/2023 - Add the options on how document should be chunked.  If you want to use the Form Recognizer, ensure the Form recognizer resource is created and the appropriate application settings `FormRecognizerKey` and `FormRecognizerEndPoint` are configured.\n* 5/07/2023 - Option available to select either Azure OpenAI or OpenAI.  For OpenAI ensure you have `OpenAiApiKey` in Azure Functions settings.  For Azure OpenAI you will need `OpenAiKey`, `OpenAiService` and `OpenAiEndPoint` Endpoint settings.  You can also select that option for Chat/Question/SQL Nlp/Speech Analytics and other features (from developer settings page).\n* 5/03/2023 - Password required for Upload and introduced Admin page starting with Index Management\n* 4/30/2023 - Initial version of Task Agent Feature added.  Autonomous Agents are agents that designed to be more long running. You give them one or multiple long term goals, and they independently execute towards those goals. The applications combine tool usage and long term memory.  Initial feature implements [Baby AGI](https://github.com/yoheinakajima/babyagi) with execution tools\n* 4/29/2023 - AWS S3 Process Integration using S3, AWS Lambda Function and Azure Data Factory (automated deployment not available yet, scripts are available in /Deployment/aws folder)\n* 4/28/2023 - Fix Bugs, Citations & Follow-up questions across QA & Chat.  Prompt bit more restrictive to limit responding from the document.\n* 4/25/2023 - Initial version of Power Virtual Agent\n* 4/21/2023 - Add SQL Query & SQL Data tab to SQL NLP and fix Citations & Follow-up questions for Chat & Ask features\n* 4/17/2023 - Real-time Speech Analytics and Speech to Text and Text to Speech for Chat & Ask Features. (You can configure Text to Speech feature from the Developer settings.  You will need Azure Speech Services)\n* 4/13/2023 - Add new feature to support asking questions on multiple document using [Vector QA Agent](https://python.langchain.com/en/latest/modules/agents/toolkits/examples/vectorstore.html)\n* 4/8/2023 - Ask your SQL - Using [SQL Database Agent](https://python.langchain.com/en/latest/modules/agents/toolkits/examples/sql_database.html) or Using [SQL Database Chain](https://python.langchain.com/en/latest/modules/chains/examples/sqlite.html)\n* 3/29/2023 - Automated Deployment script\n* 3/23/2023 - Add Cognitive Search as option to store documents\n* 3/19/2023 - Add GPT3 Chat Implementation\n* 3/18/2023 - API to generate summary on documents & Sample QA\n* 3/17/2023\n  * Support uploading Multiple documents\n  * Bug fix - Redis Vectorstore Implementation\n* 3/16/2023 - Initial Release, Ask your Data and Chat with your Data\n\n## Test Website\n\n[Chat and Ask over your data](https://dataaipdfchat.azurewebsites.net/)\n\n## Features\n\n[List of Features](Features.md)\n\n## Architecture\n\n![Architecture](/assets/Chatbot.png)\n\n## Azure Architecture\n\n![Azure Services](/assets/AskChat.png)\n\n## QA over your data with Cache\n\n![QA Cache](/assets/QACache.png)\n\n## QA LLM Evaluation\n\n![QA LLM Evaluation](/assets/Auto%20Evaluator.png)\n\n## Getting Started\n\n[Get Started](GettingStarted.md)\n\n## Configuration\n\n[Application and Function App Configuration](Configuration.md)\n\n## Resources\n\n* [Revolutionize your Enterprise Data with ChatGPT: Next-gen Apps w/ Azure OpenAI and Cognitive Search](https://aka.ms/entgptsearchblog)\n* [Azure Cognitive Search](https://learn.microsoft.com/azure/search/search-what-is-azure-search)\n* [Azure OpenAI Service](https://learn.microsoft.com/azure/cognitive-services/openai/overview)\n* [Redis Search](https://learn.microsoft.com/en-us/azure/azure-cache-for-redis/cache-redis-modules#redisearch)\n* [Pinecone](https://www.pinecone.io/learn/pinecone-v2/)\n* [Cognitive Search Vector Store](https://aka.ms/VectorSearchSignUp)\n\n## Contributions\n\nWe are open to contributions, whether it is in the form of new feature, update existing functionality or better documentation.  Please create a pull request and we will review and merge it.\n\n### Note\n\n>Adapted from the repo at [OpenAI-CogSearch](https://github.com/Azure-Samples/azure-search-openai-demo/),  [Call Center Analytics](https://github.com/amulchapla/AI-Powered-Call-Center-Intelligence), [Auto Evaluator](https://github.com/langchain-ai/auto-evaluator) and [Edgar Crawler](https://github.com/nlpaueb/edgar-crawler)\n"
    },
    {
      "name": "Azure-Samples/miyagi",
      "stars": 741,
      "img": "https://avatars.githubusercontent.com/u/1844662?s=40&v=4",
      "owner": "Azure-Samples",
      "repo_name": "miyagi",
      "description": "Sample to envision intelligent apps with Microsoft's Copilot stack for AI-infused product experiences.",
      "homepage": "https://agentmiyagi.com",
      "language": "Jupyter Notebook",
      "created_at": "2023-02-16T22:23:54Z",
      "updated_at": "2025-04-21T19:40:17Z",
      "topics": [
        "agents",
        "aks",
        "assistants",
        "azure",
        "azure-openai",
        "azureai",
        "copilot",
        "gpt-4",
        "guidance",
        "langchain",
        "llama-index",
        "llama2",
        "openai",
        "phi-2",
        "prompt-engineering",
        "promptflow",
        "semantic-kernel",
        "taskweaver",
        "typechat"
      ],
      "readme": "# Project Miyagi - Envisioning sample for [Copilot stack](https://learn.microsoft.com/en-us/semantic-kernel/overview/#semantic-kernel-is-at-the-center-of-the-copilot-stack)\n\n>  “Start with the customer experience and work backwards for the technology” - Steve Jobs\n>\n>  \"Change is the only constant\" - Ancient wisdom\n\n<p align=\"center\"><img src=\"assets/images/1.png\" width=20% height=20% /></p>\n\nProject Miyagi showcases Microsoft's Copilot Stack in an [envisioning workshop](https://github.com/Azure-Samples/intelligent-app-workshop) aimed at designing, developing, and deploying enterprise-grade intelligent apps. By exploring both generative and traditional ML [use cases](https://iappwksp.com/wksp/05-use-cases/), Miyagi offers an experiential approach to developing AI-infused product experiences that enhance productivity and enable hyper-personalization. Additionally, the workshop introduces traditional software engineers to emerging design patterns in prompt engineering, such as chain-of-thought and retrieval-augmentation, as well as to techniques like vectorization for long-term memory, fine-tuning of OSS models, agent-like orchestration, and plugins or tools for augmenting and grounding LLMs.\n\n\n> **Note**  \n> *Work in Progress*. Meanwhile, signup at [intelligentapp.dev](https://intelligentapp.dev) for updates and checkout our related repo that showcases Generative AI capabilities for cloud-native, event-driven microservices: [Azure/reddog-solutions](https://github.com/Azure/reddog-solutions#readme). \n>\n> :tv: For a preview, catch the [recording on Cosmos DB Live TV](https://www.youtube.com/watch?v=V8dlEvXdGEM&t=144s)\n>\n\n\nThe project includes examples of usage for [Semantic Kernel](https://learn.microsoft.com/en-us/semantic-kernel/overview/#semantic-kernel-is-at-the-center-of-the-copilot-stack), [Promptflow](https://promptflow.azurewebsites.net/overview-what-is-prompt-flow.html), [LlamaIndex](https://github.com/jerryjliu/llama_index), [LangChain](https://github.com/hwchase17/langchain#readme), vector stores ([Azure AI Search](https://github.com/Azure/cognitive-search-vector-pr), [CosmosDB Postgres pgvector](https://learn.microsoft.com/en-us/azure/cosmos-db/postgresql/howto-use-pgvector), and generative image utilities such as [DreamFusion](https://huggingface.co/thegovind/reddogpillmodel512) and [ControlNet](https://github.com/lllyasviel/ControlNet). Additionally, it features fine-tuned foundation Models from AzureML such as Llama2 and Phi-2. Utilize this project to gain insights as you modernize and transform your applications with AI and fine-tune your private data to build your own Copilots.\n\nThis polyglot codebase relies on a multitude of microservices, implementing several [use cases](https://iappwksp.com/wksp/05-use-cases/) using our Copilot stack. It includes generative text and images for personalized financial coaching, summarization, and agent-like orchestration. Built on a cloud-native event-diven architecture (EDA) backbone, the design and codebase ensures enterprise-grade quality attributes such as availability, scalability, and maintainability.\n\nEmbark on a journey to transform your applications into cutting-edge, intelligent systems with the self-guided workshop and discover the art of the possible.\n\n### Partial Implementations\n\nDue to the rapid pace of advancements in foundation models, we are incrementally implementing use cases for Miyagi in the experiments folder. So far, we have the following implemented:\n\n1. [MVP with Personalize (Synthesis via Semantic Kernel) and Chat on Azure Container Apps](https://agentmiyagi.com).\n    1. [Detailed breakdown and implementations](./services/README.md)\n    1. [Quickstart with RaG](./sandbox/usecases/rag/dotnet/Getting-started.ipynb)\n    1. [Agents with Assistants API](./agents/assistants-api/azure-openai/equity-analyst.ipynb)\n    1. [Agents with Autogen](./agents/README.md)\n1. [VSCode extension for GitHub Copilot Agent](./sandbox/usecases/code-modernization/vscode-gh-copilot-extension/README.md)\n1. [Miyagi ChatGPT Plugin](./services/chatgpt-plugin/python)\n1. [Knowledge Graph memory using Langchain's entity cache](./sandbox/experiments/langchain/Memory_Usecases.ipynb)\n1. [Qdrant vector store for embeddings via Langchain](./sandbox/experiments/langchain/qdrant_miyagi_example)\n1. [MS Graph API intent invoked via Semantic Kernel's skills](./sandbox/experiments/semantic-kernel/ms-graph-chain)\n1. [Miyagi prompt engineered chat interaction](./sandbox/experiments/langchain/chat) using LangChain's PromptTemplate \n1. [Azure OpenAI GPT-3.5 basic flow](./sandbox/experiments/az-openai)\n1. [GPT-3.5-turbo and Whisper-1 usage to transcribe audio and demonstrate few-shot example](./sandbox/experiments/gpt-3.5-turbo)\n1. [DeepSpeed Chat](https://github.com/microsoft/DeepSpeedExamples/tree/master/applications/DeepSpeed-Chat) MiyagiGPT (BYO Weights w/ RLHF - Reinforcement Learning from Human Feedback) - coming soon\n\n### Frontend\nInteraction with foundation models is more than chat. This sample shows a few use cases \n![frontend](./assets/images/wip-ui.png)\n\n#### VSCode extension for GitHub Copilot Agent\n<p align=\"left\"><img src=\"sandbox/usecases/code-modernization/vscode-gh-copilot-extension/demo.png\" width=50% height=50% /></p>\n\n### Architecture\n\n#### High-level logical architecture\n\n![azure](./assets/images/wip-azure.png)\n\n#### Semantic Kernel Orchestration for Miyagi usecase\n\n![sk-orchestration](./assets/images/sk-memory-orchestration.png)\n\n#### In-context learning flow\n\n![round-trip](./assets/images/sk-round-trip.png)\n\n<p align=\"left\"><img src=\"assets/images/embeddings.png\" width=40% height=40% /></p>\n\n### 30k foot view\n#### Typical AI Application\n<p align=\"left\"><img src=\"assets/images/ai_application.png\" width=60% height=60% /></p>\n\n#### AI Application in Azure\n<p align=\"left\"><img src=\"assets/images/basic-arch.png\" width=30% height=30% /></p>\n\n\n#### Prompt Flow\n![prompt-flow](./assets/images/prompt-flow-basic.png)\n\n#### OSS Pre-trained Foundation Models\n![aml-miyagi-dolly](./assets/images/aml-miyagi-dolly.png)\n![aml-training](./assets/images/aml-finetune.png)\n\n\n#### Initial ideation for EDA + SK flow\n\n![architecture](./assets/images/wip-architecture.png)\n\n\n\n### Generative image use case architecture with Dreambooth\nThis will be similar to [reddog](https://reddog-solutions.com) product [image generation use case](https://huggingface.co/thegovind/reddogpillmodel512). \n\n![generative-image](./assets/images/wip-dreambooth.png)\n\n## Tech Stack\n\n### Copilot Stack\n\n![copilot stack](./assets/images/copilot-stack.png)\n\n### Services and capabilities\n\n- [Azure OpenAI](https://learn.microsoft.com/en-us/azure/cognitive-services/openai/concepts/models)\n- [Semantic Kernel](https://github.com/microsoft/semantic-kernel)\n- [LangChain](https://python.langchain.com/docs/get_started/introduction)\n- [LlamaIndex](https://docs.llamaindex.ai/en/stable/)\n- [GitHub Copilot Agent](https://gh.io/copilot-partner-program)\n- [AI Studio](https://azure.microsoft.com/en-us/products/ai-studio)\n- [AI Search](https://azure.microsoft.com/en-us/products/ai-services/ai-search)\n- [AI Speech](https://azure.microsoft.com/en-us/products/ai-services/ai-speech)\n- [AzureML PromptFlow](https://learn.microsoft.com/en-us/azure/machine-learning/prompt-flow/overview-what-is-prompt-flow?view=azureml-api-2)\n- [TypeChat](https://microsoft.github.io/TypeChat)\n- [Kernel-memory](https://github.com/microsoft/kernel-memory)\n- [AutoGen](https://github.com/microsoft/autogen)\n- [TaskWeaver](https://github.com/microsoft/TaskWeaver)\n- [Azure Functions](https://azure.microsoft.com/en-ca/products/functions/)\n- [APIM](https://learn.microsoft.com/en-us/azure/api-management/)\n- [Service Bus](https://learn.microsoft.com/en-us/azure/service-bus-messaging/service-bus-messaging-overview)\n- [Event Grid](https://learn.microsoft.com/en-us/azure/event-grid/overview)\n- [Logic Apps](https://learn.microsoft.com/en-us/azure/logic-apps/logic-apps-overview)\n- [AKS](https://azure.microsoft.com/en-us/products/kubernetes-service) / [ACA](https://azure.microsoft.com/en-us/products/container-apps)\n- [Cosmos DB](https://azure.microsoft.com/en-us/products/cosmos-db/)\n- [Github Actions](https://docs.github.com/en/actions)\n- [Azure Monitor](https://learn.microsoft.com/en-us/azure/azure-monitor/)\n- [Azure DB for PostgreSQL](https://azure.microsoft.com/en-us/products/postgresql)\n- [Azure Redis Cache](https://azure.microsoft.com/en-us/products/cache)\n- [Azure Storage](https://learn.microsoft.com/en-us/azure/storage/common/storage-introduction)\n\n\n\n### Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n\n### Disclaimer\n\nThis software is provided for demonstration purposes only. It is not intended to be relied upon for any purpose. The creators of this software make no representations or warranties of any kind, express or implied, about the completeness, accuracy, reliability, suitability or availability with respect to the software or the information, products, services, or related graphics contained in the software for any purpose. Any reliance you place on such information is therefore strictly at your own risk.\n\n### License\n\nThis software is provided for demonstration purposes only. It is not intended to be relied upon for any purpose. The software is provided “as is” and without any warranties, express or implied. The software is not intended to be used for any commercial purpose. The software is provided solely for demonstration purposes and should not be used for any other purpose. The software is provided without any warranty of any kind, either express or implied, including, but not limited to, the implied warranties of merchantability, fitness for a particular purpose, or non-infringement. The software is provided “as is” and without any warranty of any kind. The user assumes all risk and responsibility for the use of the software."
    },
    {
      "name": "NVIDIA/AgentIQ",
      "stars": 717,
      "img": "https://avatars.githubusercontent.com/u/1728152?s=40&v=4",
      "owner": "NVIDIA",
      "repo_name": "AgentIQ",
      "description": "The NVIDIA AgentIQ toolkit is an open-source library for efficiently connecting and optimizing teams of AI agents.",
      "homepage": null,
      "language": "Python",
      "created_at": "2025-03-06T17:23:52Z",
      "updated_at": "2025-04-23T07:52:37Z",
      "topics": [],
      "readme": "<!--\nSPDX-FileCopyrightText: Copyright (c) 2024-2025, NVIDIA CORPORATION & AFFILIATES. All rights reserved.\nSPDX-License-Identifier: Apache-2.0\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\nhttp:/www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n-->\n\n![NVIDIA AgentIQ](./docs/source/_static/agentiq_banner.png \"AgentIQ banner image\")\n\n# NVIDIA AgentIQ\n\nAgentIQ is a flexible library designed to seamlessly integrate your enterprise agents—regardless of framework—with various data sources and tools. By treating agents, tools, and agentic workflows as simple function calls, AgentIQ enables true composability: build once and reuse anywhere.\n\n## Key Features\n\n- [**Framework Agnostic:**](https://docs.nvidia.com/agentiq/latest/concepts/plugins.html) Works with any agentic framework, so you can use your current technology stack without replatforming.\n- [**Reusability:**](https://docs.nvidia.com/agentiq/latest/guides/sharing-workflows-and-tools.html) Every agent, tool, or workflow can be combined and repurposed, allowing developers to leverage existing work in new scenarios.\n- [**Rapid Development:**](https://docs.nvidia.com/agentiq/latest/guides/create-customize-workflows.html) Start with a pre-built agent, tool, or workflow, and customize it to your needs.\n- [**Profiling:**](https://docs.nvidia.com/agentiq/latest/guides/profiler.html) Profile entire workflows down to the tool and agent level, track input/output tokens and timings, and identify bottlenecks.\n- [**Observability:**](https://docs.nvidia.com/agentiq/latest/guides/observe-workflow-with-phoenix.html) Monitor and debug your workflows with any OpenTelemetry-compatible observability tool.\n- [**Evaluation System:**](https://docs.nvidia.com/agentiq/latest/guides/evaluate.html) Validate and maintain accuracy of agentic workflows with built-in evaluation tools.\n- [**User Interface:**](https://docs.nvidia.com/agentiq/latest/guides/using-agentiq-ui-and-server.html) Use the AgentIQ UI chat interface to interact with your agents, visualize output, and debug workflows.\n- [**MCP Compatibility**](https://docs.nvidia.com/agentiq/latest/components/mcp.html) Compatible with Model Context Protocol (MCP), allowing tools served by MCP Servers to be used as AgentIQ functions.\n\nWith AgentIQ, you can move quickly, experiment freely, and ensure reliability across all your agent-driven projects.\n\n## Component Overview\n\nThe following diagram illustrates the key components of AgentIQ and how they interact. It provides a high-level view of the architecture, including agents, plugins, workflows, and user interfaces. Use this as a reference to understand how to integrate and extend AgentIQ in your projects.\n\n![AgentIQ Components Diagram](docs/source/_static/agentiq_gitdiagram.png)\n\n## Links\n\n * [Documentation](https://docs.nvidia.com/agentiq/latest/index.html): Explore the full documentation for AgentIQ.\n * [About AgentIQ](https://docs.nvidia.com/agentiq/latest/intro/why-agentiq.html): Learn more about the benefits of using AgentIQ.\n * [Get Started Guide](https://docs.nvidia.com/agentiq/latest/intro/get-started.html): Set up your environment and start building with AgentIQ.\n * [Examples](https://github.com/NVIDIA/AgentIQ/tree/main/examples#readme): Explore examples of AgentIQ workflows.\n * [Create and Customize AgentIQ Workflows](https://docs.nvidia.com/agentiq/latest/guides/create-customize-workflows.html): Learn how to create and customize AgentIQ workflows.\n * [Evaluate with AgentIQ](https://docs.nvidia.com/agentiq/latest/guides/evaluate.html): Learn how to evaluate your AgentIQ workflows.\n * [Troubleshooting](https://docs.nvidia.com/agentiq/latest/troubleshooting.html): Get help with common issues.\n\n\n## Get Started\n\n### Prerequisites\n\nBefore you begin using AgentIQ, ensure that you meet the following software prerequisites.\n\n- Install [Git](https://git-scm.com/)\n- Install [Git Large File Storage](https://git-lfs.github.com/) (LFS)\n- Install [uv](https://docs.astral.sh/uv/getting-started/installation/)\n\n### Install From Source\n\n1. Clone the AgentIQ repository to your local machine.\n    ```bash\n    git clone git@github.com:NVIDIA/AgentIQ.git agentiq\n    cd agentiq\n    ```\n\n2. Initialize, fetch, and update submodules in the Git repository.\n    ```bash\n    git submodule update --init --recursive\n    ```\n\n3. Fetch the data sets by downloading the LFS files.\n    ```bash\n    git lfs install\n    git lfs fetch\n    git lfs pull\n    ```\n\n4. Create a Python environment.\n    ```bash\n    uv venv --seed .venv\n    source .venv/bin/activate\n    ```\n\n5. Install the AgentIQ library.\n    To install the AgentIQ library along with all of the optional dependencies. Including developer tools (`--all-groups`) and all of the dependencies needed for profiling and plugins (`--all-extras`) in the source repository, run the following:\n    ```bash\n    uv sync --all-groups --all-extras\n    ```\n\n    Alternatively to install just the core AgentIQ without any plugins, run the following:\n    ```bash\n    uv sync\n    ```\n\n    At this point individual plugins, which are located under the `packages` directory, can be installed with the following command `uv pip install -e '.[<plugin_name>]'`.\n    For example, to install the `langchain` plugin, run the following:\n    ```bash\n    uv pip install -e '.[langchain]'\n    ```\n\n    > [!NOTE]\n    > Many of the example workflows require plugins, and following the documented steps in one of these examples will in turn install the necessary plugins. For example following the steps in the `examples/simple/README.md` guide will install the `agentiq-langchain` plugin if you haven't already done so.\n\n\n    In addition to plugins, there are optional dependencies needed for profiling. To install these dependencies, run the following:\n    ```bash\n    uv pip install -e '.[profiling]'\n    ```\n\n6. Verify the installation using the AgentIQ CLI\n\n   ```bash\n   aiq --version\n   ```\n\n   This should output the AgentIQ version which is currently installed.\n\n## Hello World Example\n\n1. Ensure you have set the `NVIDIA_API_KEY` environment variable to allow the example to use NVIDIA NIMs. An API key can be obtained by visiting [`build.nvidia.com`](https://build.nvidia.com/) and creating an account.\n\n   ```bash\n   export NVIDIA_API_KEY=<your_api_key>\n   ```\n\n2. Create the AgentIQ workflow configuration file. This file will define the agents, tools, and workflows that will be used in the example. Save the following as `workflow.yaml`:\n\n   ```yaml\n   functions:\n      # Add a tool to search wikipedia\n      wikipedia_search:\n         _type: wiki_search\n         max_results: 2\n\n   llms:\n      # Tell AgentIQ which LLM to use for the agent\n      nim_llm:\n         _type: nim\n         model_name: meta/llama-3.1-70b-instruct\n         temperature: 0.0\n\n   workflow:\n      # Use an agent that 'reasons' and 'acts'\n      _type: react_agent\n      # Give it access to our wikipedia search tool\n      tool_names: [wikipedia_search]\n      # Tell it which LLM to use\n      llm_name: nim_llm\n      # Make it verbose\n      verbose: true\n      # Retry parsing errors because LLMs are non-deterministic\n      retry_parsing_errors: true\n      # Retry up to 3 times\n      max_retries: 3\n   ```\n\n3. Run the Hello World example using the `aiq` CLI and the `workflow.yaml` file.\n\n   ```bash\n   aiq run --config_file workflow.yaml --input \"List five subspecies of Aardvarks\"\n   ```\n\n   This will run the workflow and output the results to the console.\n\n   ```console\n   Workflow Result:\n   ['Here are five subspecies of Aardvarks:\\n\\n1. Orycteropus afer afer (Southern aardvark)\\n2. O. a. adametzi  Grote, 1921 (Western aardvark)\\n3. O. a. aethiopicus  Sundevall, 1843\\n4. O. a. angolensis  Zukowsky & Haltenorth, 1957\\n5. O. a. erikssoni  Lönnberg, 1906']\n   ```\n\n## Feedback\n\nWe would love to hear from you! Please file an issue on [GitHub](https://github.com/NVIDIA/AgentIQ/issues) if you have any feedback or feature requests.\n\n## Acknowledgements\n\nWe would like to thank the following open source projects that made AgentIQ possible:\n\n- [CrewAI](https://github.com/crewAIInc/crewAI)\n- [FastAPI](https://github.com/tiangolo/fastapi)\n- [LangChain](https://github.com/langchain-ai/langchain)\n- [Llama-Index](https://github.com/run-llama/llama_index)\n- [Mem0ai](https://github.com/mem0ai/mem0)\n- [Ragas](https://github.com/explodinggradients/ragas)\n- [Semantic Kernel](https://github.com/microsoft/semantic-kernel)\n- [uv](https://github.com/astral-sh/uv)\n"
    },
    {
      "name": "Azure-Samples/AI-Gateway",
      "stars": 551,
      "img": "https://avatars.githubusercontent.com/u/1844662?s=40&v=4",
      "owner": "Azure-Samples",
      "repo_name": "AI-Gateway",
      "description": "APIM ❤️ AI - This repo contains experiments on Azure API Management's AI capabilities, integrating with Azure OpenAI, AI Foundry, and much more 🚀",
      "homepage": "https://aka.ms/ai-gateway",
      "language": "Jupyter Notebook",
      "created_at": "2024-04-03T10:56:50Z",
      "updated_at": "2025-04-23T06:44:56Z",
      "topics": [
        "agents",
        "apimanagement",
        "autogen",
        "azure",
        "foundry",
        "genai",
        "mcp",
        "mcp-server",
        "openai",
        "openai-api"
      ],
      "readme": "<!-- markdownlint-disable MD033 -->\n\n# 🧪 [AI Gateway](https://learn.microsoft.com/en-us/azure/api-management/genai-gateway-capabilities) Labs with [Azure API Management](https://aka.ms/apimlove)\n\n[![Open Source Love](https://firstcontributions.github.io/open-source-badges/badges/open-source-v1/open-source.svg)](https://github.com/firstcontributions/open-source-badges)\n\n## What's new ✨\n \n➕ **Realtime API (Audio and Text) with Azure OpenAI 🔥** experiments with the [**AOAI Realtime**](labs/realtime-audio/realtime-audio.ipynb)  \n➕ **Realtime API (Audio and Text) with Azure OpenAI + MCP tools 🔥** experiments with the [**AOAI Realtime + MCP**](labs/realtime-mcp-agents/realtime-mcp-agents.ipynb)  \n➕ **Model Context Protocol (MCP) ⚙️** experiments with the [**client authorization flow**](labs/mcp-client-authorization/mcp-client-authorization.ipynb)  \n➕ the [**FinOps Framework**](labs/finops-framework/finops-framework.ipynb) lab to manage AI budgets effectively 💰  \n➕ **Agentic ✨** experiments with [**Model Context Protocol (MCP)**](labs/model-context-protocol/model-context-protocol.ipynb).  \n➕ **Agentic ✨** experiments with [**OpenAI Agents SDK**](labs/openai-agents/openai-agents.ipynb).  \n➕ **Agentic ✨** experiments with [**AI Agent Service**](labs/ai-agent-service/ai-agent-service.ipynb) from [Azure AI Foundry](https://azure.microsoft.com/en-us/products/ai-foundry).  \n➕ the [**AI Foundry Deepseek**](labs/ai-foundry-deepseek/ai-foundry-deepseek.ipynb) lab with Deepseek R1 model from [Azure AI Foundry](https://azure.microsoft.com/en-us/products/ai-foundry).  \n➕ the [**Zero-to-Production**](labs/zero-to-production/zero-to-production.ipynb) lab with an iterative policy exploration to fine-tune the optimal production configuration.  \n➕ the [**Terraform flavor  of backend pool load balancing**](labs/backend-pool-load-balancing-tf/backend-pool-load-balancing-tf.ipynb) lab.  \n➕ the [**AI Foundry SDK**](labs/ai-foundry-sdk/ai-foundry-sdk.ipynb) lab.  \n➕ the [**Content filtering**](labs/content-filtering/content-filtering.ipynb) and [**Prompt shielding**](labs/content-filtering/prompt-shielding.ipynb) labs.  \n➕ the [**Model routing**](labs/model-routing/model-routing.ipynb) lab with OpenAI model based routing.  \n➕ the [**Prompt flow**](labs/prompt-flow/prompt-flow.ipynb) lab to try the [Azure AI Studio Prompt Flow](https://learn.microsoft.com/azure/ai-studio/how-to/prompt-flow) with Azure API Management.  \n➕ `priority` and `weight` parameters to the [**Backend pool load balancing**](labs/backend-pool-load-balancing/backend-pool-load-balancing.ipynb) lab.  \n➕ the [**Streaming**](streaming.ipynb) tool to test OpenAI streaming with Azure API Management.  \n➕ the [**Tracing**](tools/tracing.ipynb) tool to debug and troubleshoot OpenAI APIs using [Azure API Management tracing capability](https://learn.microsoft.com/azure/api-management/api-management-howto-api-inspector).  \n➕ image processing to the [**GPT-4o inferencing**](labs/GPT-4o-inferencing/GPT-4o-inferencing.ipynb) lab.  \n➕ the [**Function calling**](labs/function-calling/function-calling.ipynb) lab with a sample API on Azure Functions.\n\n## Contents\n\n1. [🧠 GenAI Gateway](#-genai-gateway)\n1. [🧪 Labs with AI Agents](#-labs-with-ai-agents)\n1. [🧪 Labs with the Inference API](#-labs-with-the-inference-api)\n1. [🧪 Labs based on Azure OpenAI](#-labs-based-on-azure-openai)\n1. [🚀 Getting started](#-getting-started)\n1. [⛵ Roll-out to production](#-roll-out-to-production)\n1. [🔨 Supporting tools](#-supporting-tools)\n1. [🏛️ Well-Architected Framework](#-well-architected-framework)    <!-- markdownlint-disable-line MD051 -->\n1. [🎒 Show and tell](#-show-and-tell)\n1. [🥇 Other Resources](#-other-resources)\n\nThe rapid pace of AI advances demands experimentation-driven approaches for organizations to remain at the forefront of the industry. With AI steadily becoming a game-changer for an array of sectors, maintaining a fast-paced innovation trajectory is crucial for businesses aiming to leverage its full potential.\n\n**AI services** are predominantly accessed via **APIs**, underscoring the essential need for a robust and efficient API management strategy. This strategy is instrumental for maintaining control and governance over the consumption of **AI services**.\n\nWith the expanding horizons of **AI services** and their seamless integration with **APIs**, there is a considerable demand for a comprehensive **AI Gateway** pattern, which broadens the core principles of API management. Aiming to accelerate the experimentation of advanced use cases and pave the road for further innovation in this rapidly evolving field. The well-architected principles of the **AI Gateway** provides a framework for the confident deployment of **Intelligent Apps** into production.\n\n## 🧠 GenAI Gateway\n\n![AI-Gateway flow](images/ai-gateway.gif)\n\nThis repo explores the **AI Gateway** pattern through a series of experimental labs. The [GenAI Gateway capabilities](https://techcommunity.microsoft.com/t5/azure-integration-services-blog/introducing-genai-gateway-capabilities-in-azure-api-management/ba-p/4146525) of [Azure API Management](https://learn.microsoft.com/azure/api-management/api-management-key-concepts) plays a crucial role within these labs, handling AI services APIs, with security, reliability, performance, overall operational efficiency and cost controls. The primary focus is on [Azure OpenAI](https://learn.microsoft.com/azure/ai-services/openai/overview), which sets the standard reference for Large Language Models (LLM). However, the same principles and design patterns could potentially be applied to any LLM.\n\nAcknowledging the rising dominance of Python, particularly in the realm of AI, along with the powerful experimental capabilities of Jupyter notebooks, the following labs are structured around Jupyter notebooks, with step-by-step instructions with Python scripts, [Bicep](https://learn.microsoft.com/azure/azure-resource-manager/bicep/overview?tabs=bicep) files and [Azure API Management policies](https://learn.microsoft.com/azure/api-management/api-management-howto-policies):\n\n## 🧪 Labs with AI Agents\n\n<!-- MCP Client Authorization -->\n### [**🧪 MCP Client Authorization**](labs/mcp-client-authorization/mcp-client-authorization.ipynb)\n\nPlayground to experiment the [Model Context Protocol](https://modelcontextprotocol.io/) with the [client authorization flow](https://modelcontextprotocol.io/specification/2025-03-26/basic/authorization#2-10-third-party-authorization-flow). In this flow, Azure API Management act both as an OAuth client connecting to the [Microsoft Entra ID](https://learn.microsoft.com/en-us/entra/architecture/auth-oauth2) authorization server and as an OAuth authorization server for the MCP client ([MCP inspector](https://modelcontextprotocol.io/docs/tools/inspector) in this lab).\n\n[<img src=\"images/mcp-client-authorization-small.gif\" alt=\"flow\" style=\"width: 437px; display: inline-block;\" data-target=\"animated-image.originalImage\">](labs/mcp-client-authorization/mcp-client-authorization.ipynb)\n\n[🦾 Bicep](labs/mcp-client-authorization/main.bicep) ➕ [⚙️ Policy](labs/mcp-client-authorization/src/weather/apim-api/policy.xml) ➕ [🧾 Notebook](labs/mcp-client-authorization/mcp-client-authorization.ipynb)\n\n<!-- Model Context Protocol (MCP) -->\n### [**🧪 Model Context Protocol (MCP)**](labs/model-context-protocol/model-context-protocol.ipynb)\n\nPlayground to experiment the [Model Context Protocol](https://modelcontextprotocol.io/) with Azure API Management to enable plug & play of tools to LLMs. Leverages the [credential manager](https://learn.microsoft.com/en-us/azure/api-management/credentials-overview) for  managing OAuth 2.0 tokens to backend tools and [client token validation](https://learn.microsoft.com/en-us/azure/api-management/validate-jwt-policy) to ensure end-to-end authentication and authorization.  \n\n[<img src=\"images/model-context-protocol-small.gif\" alt=\"flow\" style=\"width: 437px; display: inline-block;\" data-target=\"animated-image.originalImage\">](labs/model-context-protocol/model-context-protocol.ipynb)\n\n[🦾 Bicep](labs/model-context-protocol/main.bicep) ➕ [⚙️ Policy](labs/model-context-protocol/inference-policy.xml) ➕ [🧾 Notebook](labs/model-context-protocol/model-context-protocol.ipynb)\n\n<!-- OpenAI Agents -->\n### [**🧪 OpenAI Agents**](labs/openai-agents/openai-agents.ipynb)\n\nPlayground to try the [OpenAI Agents](https://openai.github.io/openai-agents-python/) with Azure OpenAI models and API based tools controlled by Azure API Management.\n\n[<img src=\"images/openai-agents-small.gif\" alt=\"flow\" style=\"width: 437px; display: inline-block;\" data-target=\"animated-image.originalImage\">](labs/openai-agents/openai-agents.ipynb)\n\n[🦾 Bicep](labs/openai-agents/main.bicep) ➕ [⚙️ Policy](labs/openai-agents/inference-policy.xml) ➕ [🧾 Notebook](labs/openai-agents/openai-agents.ipynb)\n\n<!-- AI Agent Service -->\n### [**🧪 AI Agent Service**](labs/ai-agent-service/ai-agent-service.ipynb)\n\nUse this playground to explore the [Azure AI Agent Service](https://learn.microsoft.com/en-us/azure/ai-services/agents/overview), leveraging Azure API Management to control multiple services, including Azure OpenAI models, Logic Apps Workflows, and OpenAPI-based APIs.\n\n[<img src=\"images/ai-agent-service-small.gif\" alt=\"flow\" style=\"width: 437px; display: inline-block;\" data-target=\"animated-image.originalImage\">](labs/ai-agent-service/ai-agent-service.ipynb)\n\n[🦾 Bicep](labs/ai-agent-service/main.bicep) ➕ [⚙️ Policy](labs/ai-agent-service/policy.xml) ➕ [🧾 Notebook](labs/ai-agent-service/ai-agent-service.ipynb)\n\n<!-- Function calling -->\n### [**🧪 Function calling**](labs/function-calling/function-calling.ipynb)\n\nPlayground to try the OpenAI [function calling](https://learn.microsoft.com/azure/ai-services/openai/how-to/function-calling?tabs=non-streaming%2Cpython) feature with an Azure Functions API that is also managed by Azure API Management.\n\n[<img src=\"images/function-calling-small.gif\" alt=\"flow\" style=\"width: 437px; display: inline-block;\" data-target=\"animated-image.originalImage\">](labs/function-calling/function-calling.ipynb)\n\n[🦾 Bicep](labs/function-calling/main.bicep) ➕ [⚙️ Policy](labs/function-calling/policy.xml) ➕ [🧾 Notebook](labs/function-calling/function-calling.ipynb)\n\n## 🧪 Labs with the Inference API\n\n<!-- AI Foundry Deepseek -->\n### [**🧪 AI Foundry Deepseek**](labs/ai-foundry-deepseek/ai-foundry-deepseek.ipynb)\n\nPlayground to try the [Deepseek R1 model](https://azure.microsoft.com/en-us/blog/deepseek-r1-is-now-available-on-azure-ai-foundry-and-github/) via the AI Model Inference from [Azure AI Foundry](https://azure.microsoft.com/en-us/products/ai-foundry). This lab uses the [Azure AI Model Inference API](https://learn.microsoft.com/en-us/azure/ai-foundry/model-inference/how-to/inference?tabs=python) and two APIM LLM policies: [llm-token-limit](https://learn.microsoft.com/en-us/azure/api-management/llm-token-limit-policy) and [llm-emit-token-metric](https://learn.microsoft.com/en-us/azure/api-management/llm-emit-token-metric-policy).\n\n[<img src=\"images/ai-foundry-deepseek-small.gif\" alt=\"flow\" style=\"width: 437px; display: inline-block;\" data-target=\"animated-image.originalImage\">](labs/ai-foundry-deepseek/ai-foundry-deepseek.ipynb)\n\n[🦾 Bicep](labs/ai-foundry-deepseek/main.bicep) ➕ [⚙️ Policy](labs/ai-foundry-deepseek/policy.xml) ➕ [🧾 Notebook](labs/ai-foundry-deepseek/ai-foundry-deepseek.ipynb)\n\n<!-- SLM self-hosting -->\n### [**🧪 SLM self-hosting**](labs/slm-self-hosting/slm-self-hosting.ipynb) (Phi-3)\n\nPlayground to try the self-hosted [Phi-3 Small Language Model (SLM)](https://azure.microsoft.com/blog/introducing-phi-3-redefining-whats-possible-with-slms/) through the [Azure API Management self-hosted gateway](https://learn.microsoft.com/azure/api-management/self-hosted-gateway-overview) with OpenAI API compatibility.\n\n[<img src=\"images/slm-self-hosting-small.gif\" alt=\"flow\" style=\"width: 437px; display: inline-block;\" data-target=\"animated-image.originalImage\">](labs/slm-self-hosting/slm-self-hosting.ipynb)\n\n[🦾 Bicep](labs/slm-self-hosting/main.bicep) ➕ [⚙️ Policy](labs/slm-self-hosting/policy.xml) ➕ [🧾 Notebook](labs/slm-self-hosting/slm-self-hosting.ipynb)\n\n## 🧪 Labs based on Azure OpenAI\n\n<!--FinOps framework -->\n### [**🧪 FinOps Framework**](labs/finops-framework/finops-framework.ipynb)\n\nThis playground leverages the [FinOps Framework](https://www.finops.org/framework/) and Azure API Management to control AI costs. It uses the [token limit](https://learn.microsoft.com/en-us/azure/api-management/azure-openai-token-limit-policy) policy for each [product](https://learn.microsoft.com/en-us/azure/api-management/api-management-howto-add-products?tabs=azure-portal&pivots=interactive) and integrates [Azure Monitor alerts](https://learn.microsoft.com/en-us/azure/azure-monitor/alerts/alerts-overview) with [Logic Apps](https://learn.microsoft.com/en-us/azure/azure-monitor/alerts/alerts-logic-apps?tabs=send-email) to automatically disable APIM [subscriptions](https://learn.microsoft.com/en-us/azure/api-management/api-management-subscriptions) that exceed cost quotas.  \n\n[<img src=\"images/finops-framework-small.gif\" alt=\"flow\" style=\"width: 437px; display: inline-block;\" data-target=\"animated-image.originalImage\">](labs/finops-framework/finops-framework.ipynb)\n\n[🦾 Bicep](labs/finops-framework/main.bicep) ➕ [⚙️ Policy](labs/finops-framework/openai-policy.xml) ➕ [🧾 Notebook](labs/finops-framework/finops-framework.ipynb)\n\n<!-- Backend pool load balancing -->\n### [**🧪 Backend pool load balancing**](labs/backend-pool-load-balancing/backend-pool-load-balancing.ipynb) - Available with [Bicep](labs/backend-pool-load-balancing/backend-pool-load-balancing.ipynb) and [Terraform](labs/backend-pool-load-balancing-tf/backend-pool-load-balancing-tf.ipynb)\n\nPlayground to try the built-in load balancing [backend pool functionality of Azure API Management](https://learn.microsoft.com/azure/api-management/backends?tabs=bicep) to either a list of Azure OpenAI endpoints or mock servers.\n\n[<img src=\"images/backend-pool-load-balancing-small.gif\" alt=\"flow\" style=\"width: 437px; display: inline-block;\" data-target=\"animated-image.originalImage\">](labs/backend-pool-load-balancing/backend-pool-load-balancing.ipynb)\n\n[🦾 Bicep](labs/backend-pool-load-balancing/main.bicep) ➕ [⚙️ Policy](labs/backend-pool-load-balancing/policy.xml) ➕ [🧾 Notebook](labs/backend-pool-load-balancing/backend-pool-load-balancing.ipynb)\n\n<!-- Token rate limiting -->\n### [**🧪 Token rate limiting**](labs/token-rate-limiting/token-rate-limiting.ipynb)\n\nPlayground to try the [token rate limiting policy](https://learn.microsoft.com/azure/api-management/azure-openai-token-limit-policy) to one or more Azure OpenAI endpoints. When the token usage is exceeded, the caller receives a 429.\n\n[<img src=\"images/token-rate-limiting-small.gif\" alt=\"flow\" style=\"width: 437px; display: inline-block;\" data-target=\"animated-image.originalImage\">](labs/token-rate-limiting/token-rate-limiting.ipynb)\n\n[🦾 Bicep](labs/token-rate-limiting/main.bicep) ➕ [⚙️ Policy](labs/token-rate-limiting/policy.xml) ➕ [🧾 Notebook](labs/token-rate-limiting/token-rate-limiting.ipynb)\n\n<!-- Token metrics emitting -->\n### [**🧪 Token metrics emitting**](labs/token-metrics-emitting/token-metrics-emitting.ipynb)\n\nPlayground to try the [emit token metric policy](https://learn.microsoft.com/azure/api-management/azure-openai-emit-token-metric-policy). The policy sends metrics to Application Insights about consumption of large language model tokens through Azure OpenAI Service APIs.\n\n[<img src=\"images/token-metrics-emitting-small.gif\" alt=\"flow\" style=\"width: 437px; display: inline-block;\" data-target=\"animated-image.originalImage\">](labs/token-metrics-emitting/token-metrics-emitting.ipynb)\n\n[🦾 Bicep](labs/token-metrics-emitting/main.bicep) ➕ [⚙️ Policy](labs/token-metrics-emitting/policy.xml) ➕ [🧾 Notebook](labs/token-metrics-emitting/token-metrics-emitting.ipynb)\n\n<!-- Semantic caching -->\n### [**🧪 Semantic caching**](labs/semantic-caching/semantic-caching.ipynb)\n\nPlayground to try the [semantic caching policy](https://learn.microsoft.com/azure/api-management/azure-openai-semantic-cache-lookup-policy). Uses vector proximity of the prompt to previous requests and a specified similarity score threshold.\n\n[<img src=\"images/semantic-caching-small.gif\" alt=\"flow\" style=\"width: 437px; display: inline-block;\" data-target=\"animated-image.originalImage\">](labs/semantic-caching/semantic-caching.ipynb)\n\n[🦾 Bicep](labs/semantic-caching/main.bicep) ➕ [⚙️ Policy](labs/semantic-caching/policy.xml) ➕ [🧾 Notebook](labs/semantic-caching/semantic-caching.ipynb)\n\n<!-- Access controlling -->\n### [**🧪 Access controlling**](labs/access-controlling/access-controlling.ipynb)\n\nPlayground to try the [OAuth 2.0 authorization feature](https://learn.microsoft.com/azure/api-management/api-management-authenticate-authorize-azure-openai#oauth-20-authorization-using-identity-provider) using identity provider to enable more fine-grained access to OpenAPI APIs by particular users or client.\n\n[<img src=\"images/access-controlling-small.gif\" alt=\"flow\" style=\"width: 437px; display: inline-block;\" data-target=\"animated-image.originalImage\">](labs/access-controlling/access-controlling.ipynb)\n\n[🦾 Bicep](labs/access-controlling/main.bicep) ➕ [⚙️ Policy](labs/access-controlling/policy.xml) ➕ [🧾 Notebook](labs/access-controlling/access-controlling.ipynb)\n\n<!-- zero-to-production -->\n### [**🧪 Zero-to-Production**](labs/zero-to-production/zero-to-production.ipynb)\n\nPlayground to create a combination of several policies in an iterative approach. We start with load balancing, then progressively add token emitting, rate limiting, and, eventually, semantic caching. Each of these sets of policies is derived from other labs in this repo.\n\n[<img src=\"images/zero-to-production-small.gif\" alt=\"flow\" style=\"width: 437px; display: inline-block;\" data-target=\"animated-image.originalImage\">](labs/zero-to-production/zero-to-production.ipynb)\n\n[🦾 Bicep](labs/zero-to-production/main.bicep) ➕ [⚙️ Policy](labs/zero-to-production/policy-3.xml) ➕ [🧾 Notebook](labs/zero-to-production/zero-to-production.ipynb)\n\n<!-- GPT-4o inferencing -->\n### [**🧪 GPT-4o inferencing**](labs/GPT-4o-inferencing/GPT-4o-inferencing.ipynb)\n\nPlayground to try the new GPT-4o model. GPT-4o (\"o\" for \"omni\") is designed to handle a combination of text, audio, and video inputs, and can generate outputs in text, audio, and image formats.\n\n[<img src=\"images/GPT-4o-inferencing-small.gif\" alt=\"flow\" style=\"width: 437px; display: inline-block;\" data-target=\"animated-image.originalImage\">](labs/GPT-4o-inferencing/GPT-4o-inferencing.ipynb)\n\n[🦾 Bicep](labs/GPT-4o-inferencing/main.bicep) ➕ [⚙️ Policy](labs/GPT-4o-inferencing/policy.xml) ➕ [🧾 Notebook](labs/GPT-4o-inferencing/GPT-4o-inferencing.ipynb)\n\n<!-- Model Routing -->\n### [**🧪 Model Routing**](labs/model-routing/model-routing.ipynb)\n\nPlayground to try routing to a backend based on Azure OpenAI model and version.\n\n[<img src=\"images/model-routing-small.gif\" alt=\"flow\" style=\"width: 437px; display: inline-block;\" data-target=\"animated-image.originalImage\">](labs/model-routing/model-routing.ipynb)\n\n[🦾 Bicep](labs/model-routing/main.bicep) ➕ [⚙️ Policy](labs/model-routing/policy.xml) ➕ [🧾 Notebook](labs/model-routing/model-routing.ipynb)\n\n<!-- Vector searching -->\n### [**🧪 Vector searching**](labs/vector-searching/vector-searching.ipynb)\n\nPlayground to try the [Retrieval Augmented Generation (RAG) pattern](https://learn.microsoft.com/azure/search/retrieval-augmented-generation-overview) with Azure AI Search, Azure OpenAI embeddings and Azure OpenAI completions.\n\n[<img src=\"images/vector-searching-small.gif\" alt=\"flow\" style=\"width: 437px; display: inline-block;\" data-target=\"animated-image.originalImage\">](labs/vector-searching/vector-searching.ipynb)\n\n[🦾 Bicep](labs/vector-searching/main.bicep) ➕ [⚙️ Policy](labs/vector-searching/policy.xml) ➕ [🧾 Notebook](labs/vector-searching/vector-searching.ipynb)\n\n<!-- Built-in logging -->\n### [**🧪 Built-in logging**](labs/built-in-logging/built-in-logging.ipynb)\n\nPlayground to try the [buil-in logging capabilities of Azure API Management](https://learn.microsoft.com/azure/api-management/observability). Logs requests into App Insights to track details and token usage.\n\n[<img src=\"images/built-in-logging-small.gif\" alt=\"flow\" style=\"width: 437px; display: inline-block;\" data-target=\"animated-image.originalImage\">](labs/built-in-logging/built-in-logging.ipynb)\n\n[🦾 Bicep](labs/built-in-logging/main.bicep) ➕ [⚙️ Policy](labs/built-in-logging/policy.xml) ➕ [🧾 Notebook](labs/built-in-logging/built-in-logging.ipynb)\n\n<!-- Message storing -->\n### [**🧪 Message storing**](labs/message-storing/message-storing.ipynb)\n\nPlayground to test storing message details into Cosmos DB through the [Log to event hub](https://learn.microsoft.com/azure/api-management/log-to-eventhub-policy) policy. With the policy we can control which data will be stored in the DB (prompt, completion, model, region, tokens etc.).\n\n[<img src=\"images/message-storing-small.gif\" alt=\"flow\" style=\"width: 437px; display: inline-block;\" data-target=\"animated-image.originalImage\">](labs/message-storing/message-storing.ipynb)\n\n[🦾 Bicep](labs/message-storing/main.bicep) ➕ [⚙️ Policy](labs/message-storing/policy.xml) ➕ [🧾 Notebook](labs/message-storing/message-storing.ipynb)\n\n<!-- Prompt flow -->\n### [**🧪 Prompt flow**](labs/prompt-flow/prompt-flow.ipynb)\n\nPlayground to try the [Azure AI Studio Prompt Flow](https://learn.microsoft.com/azure/ai-studio/how-to/prompt-flow) with Azure API Management.\n\n[<img src=\"images/prompt-flow-small.gif\" alt=\"flow\" style=\"width: 437px; display: inline-block;\" data-target=\"animated-image.originalImage\">](labs/prompt-flow/prompt-flow.ipynb)\n\n[🦾 Bicep](labs/prompt-flow/main.bicep) ➕ [⚙️ Policy](labs/prompt-flow/policy.xml) ➕ [🧾 Notebook](labs/prompt-flow/prompt-flow.ipynb)\n\n<!-- Content Filtering -->\n### [**🧪 Content Filtering**](labs/content-filtering/content-filtering.ipynb)\n\nPlayground to try integrating Azure API Management with [Azure AI Content Safety](https://learn.microsoft.com/azure/ai-services/content-safety/overview) to filter potentially offensive, risky, or undesirable content.\n\n[<img src=\"images/content-filtering-small.gif\" alt=\"flow\" style=\"width: 437px; display: inline-block;\" data-target=\"animated-image.originalImage\">](labs/content-filtering/content-filtering.ipynb)\n\n[🦾 Bicep](labs/content-filtering/main.bicep) ➕ [⚙️ Policy](labs/content-filtering/content-filtering-policy.xml) ➕ [🧾 Notebook](labs/content-filtering/content-filtering.ipynb)\n\n<!-- Prompt Shielding -->\n### [**🧪 Prompt Shielding**](labs/content-filtering/prompt-shielding.ipynb)\n\nPlayground to try Prompt Shields from Azure AI Content Safety service that analyzes LLM inputs and detects User Prompt attacks and Document attacks, which are two common types of adversarial inputs.\n\n[<img src=\"images/content-filtering-small.gif\" alt=\"flow\" style=\"width: 437px; display: inline-block;\" data-target=\"animated-image.originalImage\">](labs/content-filtering/prompt-shielding.ipynb)\n\n[🦾 Bicep](labs/content-filtering/main.bicep) ➕ [⚙️ Policy](labs/content-filtering/prompt-shield-policy.xml) ➕ [🧾 Notebook](labs/content-filtering/prompt-shielding.ipynb)\n\n## Backlog of Labs\n\nThis is a list of potential future labs to be developed.\n\n* Real Time API\n* Semantic Kernel with Agents\n* Logic Apps RAG\n* PII handling\n* Gemini\n\n> [!TIP]\n> Kindly use [the feedback discussion](../../discussions/9) so that we can continuously improve with your experiences, suggestions, ideas or lab requests.\n\n## 🚀 Getting Started\n\n### Prerequisites\n\n* [Python 3.12 or later version](https://www.python.org/) installed\n* [VS Code](https://code.visualstudio.com/) installed with the [Jupyter notebook extension](https://marketplace.visualstudio.com/items?itemName=ms-toolsai.jupyter) enabled\n* [Python environment](https://code.visualstudio.com/docs/python/environments#_creating-environments) with the [requirements.txt](../../requirements.txt) or run `pip install -r requirements.txt` in your terminal\n* [An Azure Subscription](https://azure.microsoft.com/free/) with [Contributor](https://learn.microsoft.com/en-us/azure/role-based-access-control/built-in-roles/privileged#contributor) + [RBAC Administrator](https://learn.microsoft.com/en-us/azure/role-based-access-control/built-in-roles/privileged#role-based-access-control-administrator) or [Owner](https://learn.microsoft.com/en-us/azure/role-based-access-control/built-in-roles/privileged#owner) roles\n* [Azure CLI](https://learn.microsoft.com/cli/azure/install-azure-cli) installed and [Signed into your Azure subscription](https://learn.microsoft.com/cli/azure/authenticate-azure-cli-interactively)\n\n### Quickstart\n\n1. Clone this repo and configure your local machine with the prerequisites. Or just create a [GitHub Codespace](https://codespaces.new/Azure-Samples/AI-Gateway/tree/main) and run it on the browser or in VS Code.\n2. Navigate through the available labs and select one that best suits your needs. For starters we recommend the [token rate limiting](labs/token-rate-limiting/token-rate-limiting.ipynb).\n3. Open the notebook and run the provided steps.\n4. Tailor the experiment according to your requirements. If you wish to contribute to our collective work, we would appreciate your [submission of a pull request](CONTRIBUTING.MD).\n\n> [!NOTE]\n> 🪲 Please feel free to open a new [issue](../../issues/new) if you find something that should be fixed or enhanced.\n\n## ⛵ Roll-out to production\n\nWe recommend the guidelines and best practices from the [AI Hub Gateway Landing Zone](https://github.com/Azure-Samples/ai-hub-gateway-solution-accelerator) to implement a central AI API gateway to empower various line-of-business units in an organization to leverage Azure AI services.\n\n## 🔨 Supporting Tools\n\n* [AI-Gateway Mock server](tools/mock-server/mock-server.ipynb) is designed to mimic the behavior and responses of the OpenAI API, thereby creating an efficient simulation environment suitable for testing and development purposes on the integration with Azure API Management and other use cases. The [app.py](tools/mock-server/app.py) can be customized to tailor the Mock server to specific use cases.\n* [Tracing](tools/tracing.ipynb) - Invoke OpenAI API with trace enabled and returns the tracing information.\n* [Streaming](streaming.ipynb) - Invoke OpenAI API with stream enabled and returns response in chunks.\n\n## 🏛️ Well-Architected Framework\n\nThe [Azure Well-Architected Framework](https://learn.microsoft.com/azure/well-architected/what-is-well-architected-framework) is a design framework that can improve the quality of a workload. The following table maps labs with the Well-Architected Framework pillars to set you up for success through architectural experimentation.\n\n| Lab  | Security | Reliability | Performance | Operations | Costs |\n| -------- | -------- |-------- |-------- |-------- |-------- |\n| [Request forwarding](labs/request-forwarding/request-forwarding.ipynb) | [⭐](#%EF%B8%8F-well-architected-framework \"Zero trust, keyless approach with manage identities and Azure API Management security features\") | |  |  |  |\n| [Backend circuit breaking](labs/backend-circuit-breaking/backend-circuit-breaking.ipynb) | [⭐](#%EF%B8%8F-well-architected-framework \"Zero trust, keyless approach with manage identities and Azure API Management security features\") | [⭐](#%EF%B8%8F-well-architected-framework \"Controls the availability of the OpenAI endpoint with the circuit breaker feature\") |  |  |  |\n| [Backend pool load balancing](labs/backend-pool-load-balancing/backend-pool-load-balancing.ipynb)  |[⭐](#%EF%B8%8F-well-architected-framework \"Zero trust, keyless approach with manage identities and Azure API Management security features\")|[⭐](#%EF%B8%8F-well-architected-framework \"To ensure resilience, the request is distributed to two or more endpoints with the built-in feature\")|[⭐](#%EF%B8%8F-well-architected-framework \"Load balances the requests to increase performance with the built-in feature\")|  |  |\n| [Advanced load balancing](labs/advanced-load-balancing/advanced-load-balancing.ipynb) |[⭐](#%EF%B8%8F-well-architected-framework \"Zero trust, keyless approach with manage identities and Azure API Management security features\")|[⭐](#%EF%B8%8F-well-architected-framework \"To ensure resilience, the request is distributed to two or more endpoints with a custom policy\")|[⭐](#%EF%B8%8F-well-architected-framework \"Load balances the requests to increase performance with a custom policy\")|  |  |\n| [Response streaming](labs/response-streaming/response-streaming.ipynb)  |[⭐](#%EF%B8%8F-well-architected-framework \"Zero trust, keyless approach with manage identities and Azure API Management security features\")| |[⭐](#%EF%B8%8F-well-architected-framework \"To get responses sooner, you can 'stream' the completion as it's being generated\")|  |  |\n| [Vector searching](labs/vector-searching/vector-searching.ipynb) |[⭐](#%EF%B8%8F-well-architected-framework \"Zero trust, keyless approach with manage identities and Azure API Management security features\")|[⭐](#%EF%B8%8F-well-architected-framework \"To ensure resilience, the request is distributed to two or more endpoints with the built-in feature\")| [⭐](#%EF%B8%8F-well-architected-framework \"Load balances the requests to increase performance with the built-in feature\")| |  |\n| [Built-in logging](labs/built-in-logging/built-in-logging.ipynb) |[⭐](#%EF%B8%8F-well-architected-framework \"Zero trust, keyless approach with manage identities and Azure API Management security features\")|[⭐](#%EF%B8%8F-well-architected-framework \"To ensure resilience, the request is distributed to two or more endpoints with the built-in feature\")|[⭐](#%EF%B8%8F-well-architected-framework \"Load balances the requests to increase performance with the built-in feature\")|[⭐](#%EF%B8%8F-well-architected-framework \"Requests are logged to enable monitoring, alerting and automatic remediation\")|[⭐](#%EF%B8%8F-well-architected-framework \"Relation between Azure API Management subscription and token consumption allows cost control\")|\n| [SLM self-hosting](labs/slm-self-hosting/slm-self-hosting.ipynb) |[⭐](#%EF%B8%8F-well-architected-framework \"Self hosting the model might improve the security posture with network restrictions\") | | [⭐](#%EF%B8%8F-well-architected-framework \"Performance might be improved with full control to the self-hosted model\") | | |\n\n> [!TIP]\n> Check the [Azure Well-Architected Framework perspective on Azure OpenAI Service](https://learn.microsoft.com/azure/well-architected/service-guides/azure-openai) for aditional guidance.\n\n## 🎒 Show and tell\n\n> [!TIP]\n> Install the [VS Code Reveal extension](https://marketplace.visualstudio.com/items?itemName=evilz.vscode-reveal), open AI-GATEWAY.md and click on 'slides' at the botton to present the AI Gateway without leaving VS Code.\n> Or just open the [AI-GATEWAY.pptx](https://view.officeapps.live.com/op/view.aspx?src=https%3A%2F%2Fraw.githubusercontent.com%2FAzure-Samples%2FAI-Gateway%2Fmain%2FAI-GATEWAY.pptx&wdOrigin=BROWSELINK) for a plain old PowerPoint experience.\n\n## 🥇 Other resources\n\nNumerous reference architectures, best practices and starter kits are available on this topic. Please refer to the resources provided if you need comprehensive solutions or a landing zone to initiate your project. We suggest leveraging the AI-Gateway labs to discover additional capabilities that can be integrated into the reference architectures.\n\n* [GenAI Gateway Guide](https://aka.ms/genai-gateway)\n* [Azure OpenAI + APIM Sample](https://aka.ms/apim/genai/sample-app)\n* [AI+API better together: Benefits & Best Practices using APIs for AI workloads](https://techcommunity.microsoft.com/t5/apps-on-azure-blog/ai-api-better-together-benefits-amp-best-practices-using-apis/ba-p/4157120)\n* [Designing and implementing a gateway solution with Azure OpenAI resources](https://aka.ms/genai-gateway)\n* [Azure OpenAI Using PTUs/TPMs With API Management - Using the Scaling Special Sauce](https://github.com/Azure/aoai-apim)\n* [Manage Azure OpenAI using APIM](https://github.com/microsoft/AzureOpenAI-with-APIM)\n* [Setting up Azure OpenAI as a central capability with Azure API Management](https://github.com/Azure/enterprise-azureai)\n* [Introduction to Building AI Apps](https://github.com/Azure/intro-to-intelligent-apps)\n\n> We believe that there may be valuable content that we are currently unaware of. We would greatly appreciate any suggestions or recommendations to enhance this list.\n\n### 🌐 WW GBB initiative\n\n![GBB](images/gbb.png)\n\n### Disclaimer\n\n> [!IMPORTANT]\n> This software is provided for demonstration purposes only. It is not intended to be relied upon for any purpose. The creators of this software make no representations or warranties of any kind, express or implied, about the completeness, accuracy, reliability, suitability or availability with respect to the software or the information, products, services, or related graphics contained in the software for any purpose. Any reliance you place on such information is therefore strictly at your own risk.\n"
    },
    {
      "name": "Azure/co-op-translator",
      "stars": 413,
      "img": "https://avatars.githubusercontent.com/u/6844498?s=40&v=4",
      "owner": "Azure",
      "repo_name": "co-op-translator",
      "description": "Easily automate the translation of your documentation into multiple languages, powered by Azure AI Services",
      "homepage": "https://techcommunity.microsoft.com/t5/educator-developer-blog/automate-markdown-and-image-translations-using-co-op-translator/ba-p/4263474",
      "language": "Python",
      "created_at": "2024-09-19T08:51:34Z",
      "updated_at": "2025-04-22T04:22:28Z",
      "topics": [
        "azure",
        "azure-computer-vision",
        "azure-openai",
        "co-op-translator",
        "image-translation",
        "localization",
        "markdown-translation",
        "multilingual-translation",
        "open-source",
        "python-package"
      ],
      "readme": "![Logo](/imgs/logo.png)\n\n# Co-op Translator: Break Language Barriers Effortlessly\n\n_Easily automate the translation of your documentation into multiple languages_\n\n[![Python package](https://img.shields.io/pypi/v/co-op-translator?color=4BA3FF)](https://pypi.org/project/co-op-translator/)\n[![License: MIT](https://img.shields.io/github/license/azure/co-op-translator?color=4BA3FF)](https://github.com/azure/co-op-translator/blob/main/LICENSE)\n[![Downloads](https://static.pepy.tech/badge/co-op-translator)](https://pepy.tech/project/co-op-translator)\n[![Downloads](https://static.pepy.tech/badge/co-op-translator/month)](https://pepy.tech/project/co-op-translator)\n[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)\n\n[![GitHub contributors](https://img.shields.io/github/contributors/azure/co-op-translator.svg)](https://GitHub.com/azure/co-op-translator/graphs/contributors/)\n[![GitHub issues](https://img.shields.io/github/issues/azure/co-op-translator.svg)](https://GitHub.com/azure/co-op-translator/issues/)\n[![GitHub pull-requests](https://img.shields.io/github/issues-pr/azure/co-op-translator.svg)](https://GitHub.com/azure/co-op-translator/pulls/)\n[![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg)](http://makeapullrequest.com)\n\n[![GitHub watchers](https://img.shields.io/github/watchers/azure/co-op-translator.svg?style=social&label=Watch)](https://GitHub.com/azure/co-op-translator/watchers/)\n[![GitHub forks](https://img.shields.io/github/forks/azure/co-op-translator.svg?style=social&label=Fork)](https://GitHub.com/azure/co-op-translator/network/)\n[![GitHub stars](https://img.shields.io/github/stars/azure/co-op-translator?style=social&label=Star)](https://GitHub.com/azure/co-op-translator/stargazers/)\n\n[![Azure AI Community Discord](https://dcbadge.vercel.app/api/server/ByRwuEEgH4)](https://discord.com/invite/ByRwuEEgH4)\n\n[![Open in GitHub Codespaces](https://img.shields.io/static/v1?style=for-the-badge&label=Github%20Codespaces&message=Open&color=24292F&logo=github)](https://codespaces.new/azure/co-op-translator)\n[![Open in Dev Containers](https://img.shields.io/static/v1?style=for-the-badge&label=VS%20Code%20Dev%20Containers&message=Open&color=007ACC&logo=visualstudiocode)](https://vscode.dev/redirect?url=vscode://ms-vscode-remote.remote-containers/cloneInVolume?url=https://github.com/azure/co-op-translator)\n\n> [!TIP]\n>\n> **NEW**: Now with GitHub Actions support! Automatically translate your documentation when changes are made to your repository. [Learn more](#-getting-started-with-co-op-translator).\n\n## 🤖 Supported models and services\n\n| Type                  | Name                           |\n|-----------------------|--------------------------------|\n| Language Model        | ![Azure OpenAI](https://img.shields.io/badge/Azure_OpenAI-blue?style=flat-square) ![OpenAI](https://img.shields.io/badge/OpenAI-green?style=flat-square&logo=openai) |\n| Computer Vision       | ![Azure Computer Vision](https://img.shields.io/badge/Azure_Computer_Vision-blue?style=flat-square) |\n\n> [!NOTE]\n> If a computer vision service is not available, the co-op translator will switch to [Markdown-only mode](./getting_started/markdown-only-mode.md).\n\n## 🌐 Overview\n\n**Co-op Translator** breaks language barriers by automating multilingual translations for your projects using advanced Large Language Model (LLM) technology and Azure AI Services. Whether through command line or GitHub Actions, it transforms your content to reach global audiences with minimal effort.\n\nWith Co-op Translator, you can:\n- **Command Line**: Quickly translate your documentation on-demand\n- **GitHub Actions**: Automatically translate when your repository changes\n- **Preserve Formatting**: Maintain Markdown syntax and structure across languages\n- **Translate Images**: Extract and translate text embedded in images\n\nHere is an example of how Co-op Translator structures the translations and translates both Markdown files and text within images in your project:\n\n![Example](/imgs/translation-ex.png)\n\n**Ready to unlock multilingual accessibility? Get started with Co-op Translator today!**\n\n## 📚 Microsoft projects using Co-op Translator\n\nOfficial Microsoft learning repositories powered by Co-op Translator:\n\n[![ML-For-Beginners](https://github-readme-stats.vercel.app/api/pin/?username=microsoft&repo=ML-For-Beginners&bg_color=ffffff&title_color=0078D4&text_color=333333&border_color=c0d8f0&border_radius=10)](https://github.com/microsoft/ML-For-Beginners)\n[![Generative-AI-for-beginners-dotnet](https://github-readme-stats.vercel.app/api/pin/?username=microsoft&repo=Generative-AI-for-beginners-dotnet&bg_color=ffffff&title_color=0078D4&text_color=333333&border_color=c0d8f0&border_radius=10)](https://github.com/microsoft/Generative-AI-for-beginners-dotnet)\n[![AI-For-Beginners](https://github-readme-stats.vercel.app/api/pin/?username=microsoft&repo=AI-For-Beginners&bg_color=ffffff&title_color=0078D4&text_color=333333&border_color=c0d8f0&border_radius=10)](https://github.com/microsoft/AI-For-Beginners)\n[![ai-agents-for-beginners](https://github-readme-stats.vercel.app/api/pin/?username=microsoft&repo=ai-agents-for-beginners&bg_color=ffffff&title_color=0078D4&text_color=333333&border_color=c0d8f0&border_radius=10)](https://github.com/microsoft/ai-agents-for-beginners)\n[![PhiCookBook](https://github-readme-stats.vercel.app/api/pin/?username=microsoft&repo=PhiCookBook&bg_color=ffffff&title_color=0078D4&text_color=333333&border_color=c0d8f0&border_radius=10)](https://github.com/microsoft/PhiCookBook)\n\n## ✨ Key features\n\n- **Automated Translations**: Translate text into multiple languages effortlessly.\n- **GitHub Actions Integration**: Automate translations as part of your CI/CD pipeline.\n- **Markdown Preservation**: Maintain correct Markdown syntax during translation.\n- **Image Text Translation**: Extract and translate text within images.\n- **Advanced LLM Technology**: Use cutting-edge language models for high-quality translations.\n- **Easy Integration**: Seamlessly integrate with your existing project setup.\n- **Simplify Localization**: Streamline the process of localizing your project for international markets.\n\n## 🌉 Bridging the language gap in tech\n\nEnglish is often considered the universal language of technology, but many developers worldwide are not native English speakers. This can create barriers in accessing and contributing to technical projects.\n\n**Co-op Translator** aims to break down these language barriers by providing an easy-to-use tool for automating translations. By making technical documentation accessible in multiple languages, we empower developers, students, and researchers globally.\n\n## ⭐ Support us\n\nJoin us in revolutionizing global communication! Give a ⭐ to [Co-op Translator](https://github.com/azure/co-op-translator) on GitHub and help us break down language barriers together. Your support makes a difference!\n\n## ⚙️ How it works\n\n![Architecture](/imgs/architecture_241019.png)\n\nThe process begins with Markdown and image files from your project folder, which are then processed by various AI services:\n\n- **Language Models**: \n  - **Azure OpenAI** and other supported LLMs translate text from Markdown files. See the [supported models and services](#-supported-models-and-services) for more information.\n- **Computer Vision Services** (optional): \n  - **Azure Computer Vision** extracts text from images, which are then translated by the selected language model. If a computer vision service is not available, the process defaults to [Markdown-only mode](./getting_started/markdown-only-mode.md).\n\nThe final translated Markdown and image files are saved in the designated translation folder, ready to be used in multiple languages.\n\n## 🚀 Getting started with Co-op Translator\n\n| **Approach**        | **Command Line**                           | **GitHub Actions**                          |\n|---------------------|---------------------------------------|--------------------------------------|\n| **Best for**       | One-time, manual execution           | Automated execution on repository changes |\n| **Automation**     | Manual trigger                        | Automatic on repository events      |\n| **Setup**          | Requires local installation     | No local setup needed            |\n| **Guide**         | [Command Line Guide](./getting_started/command-line-guide/command-line-guide.md) | [GitHub Actions Guide](./getting_started/github-actions-guide/github-actions-guide.md) |\n\n> [!NOTE]\n> While this tutorial focuses on Azure resources, you can use any supported language model from the [supported models and services](#-supported-models-and-services) list.\n\n### Troubleshooting, Tips and Tricks\n\n- [Troubleshooting Guide](./getting_started/troubleshooting.md)\n\n### Additional resources\n\n- [Command reference](./getting_started/command-reference.md): Learn about all available commands and their options in detail.\n- [Multi-language support setup](./getting_started/multi-language-support.md): Before starting the translation process, you can add a table in the README linking to the translated versions of your document.\n- [Supported Languages](./getting_started/supported-languages.md): Check the list of supported languages and instructions to add new languages.\n- [Markdown-Only Mode](./getting_started/markdown-only-mode.md): Learn how to use Co-op Translator in Markdown-only mode.\n\n## 🎥 Video presentations\n\nLearn more about Co-op Translator through our presentations _(Click the image below to watch on YouTube.)_:\n\n- **Open at Microsoft**: A brief 18-minute introduction and quick guide on how to use Co-op Translator.\n\n  [![Open at Microsoft](/imgs/open-ms-thumbnail.jpg)](https://www.youtube.com/watch?v=jX_swfH_KNU)\n\n- **Microsoft Reactor**: A one-hour detailed step-by-step guide covering everything from understanding what Co-op Translator is, setting up the tool, and using it effectively, to a live demo showcasing its capabilities in action.\n\n  [![Microsoft Reactor](/imgs/reactor-thumbnail.jpg)](https://www.youtube.com/watch?v=boTtKVPBLAc)\n\n## Contributing\n\nThis project welcomes contributions and suggestions. Interested in contributing to Azure Co-op Translator? Please see our [CONTRIBUTING.md](./CONTRIBUTING.md) for guidelines on how you can help make Co-op Translator more accessible.\n\n## Contributors\n\n[![co-op-translator contributors](https://contrib.rocks/image?repo=Azure/co-op-translator)](https://github.com/Azure/co-op-translator/graphs/contributors)\n\n## Code of Conduct\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Responsible AI\n\nMicrosoft is committed to helping our customers use our AI products responsibly, sharing our learnings, and building trust-based partnerships through tools like Transparency Notes and Impact Assessments. Many of these resources can be found at [https://aka.ms/RAI](https://aka.ms/RAI).\nMicrosoft’s approach to responsible AI is grounded in our AI principles of fairness, reliability and safety, privacy and security, inclusiveness, transparency, and accountability.\n\nLarge-scale natural language, image, and speech models - like the ones used in this sample - can potentially behave in ways that are unfair, unreliable, or offensive, in turn causing harms. Please consult the [Azure OpenAI service Transparency note](https://learn.microsoft.com/legal/cognitive-services/openai/transparency-note?tabs=text) to be informed about risks and limitations.\n\nThe recommended approach to mitigating these risks is to include a safety system in your architecture that can detect and prevent harmful behavior. [Azure AI Content Safety](https://learn.microsoft.com/azure/ai-services/content-safety/overview) provides an independent layer of protection, able to detect harmful user-generated and AI-generated content in applications and services. Azure AI Content Safety includes text and image APIs that allow you to detect material that is harmful. We also have an interactive Content Safety Studio that allows you to view, explore and try out sample code for detecting harmful content across different modalities. The following [quickstart documentation](https://learn.microsoft.com/azure/ai-services/content-safety/quickstart-text?tabs=visual-studio%2Clinux&pivots=programming-language-rest) guides you through making requests to the service.\n\nAnother aspect to take into account is the overall application performance. With multi-modal and multi-models applications, we consider performance to mean that the system performs as you and your users expect, including not generating harmful outputs. It's important to assess the performance of your overall application using [generation quality and risk and safety metrics](https://learn.microsoft.com/azure/ai-studio/concepts/evaluation-metrics-built-in).\n\nYou can evaluate your AI application in your development environment using the [prompt flow SDK](https://microsoft.github.io/promptflow/index.html). Given either a test dataset or a target, your generative AI application generations are quantitatively measured with built-in evaluators or custom evaluators of your choice. To get started with the prompt flow sdk to evaluate your system, you can follow the [quickstart guide](https://learn.microsoft.com/azure/ai-studio/how-to/develop/flow-evaluate-sdk). Once you execute an evaluation run, you can [visualize the results in Azure AI Studio](https://learn.microsoft.com/azure/ai-studio/how-to/evaluate-flow-results).\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft\ntrademarks or logos is subject to and must follow\n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n"
    },
    {
      "name": "microsoft/semantic-kernel-starters",
      "stars": 370,
      "img": "https://avatars.githubusercontent.com/u/6154722?s=40&v=4",
      "owner": "microsoft",
      "repo_name": "semantic-kernel-starters",
      "description": "Starter Projects for Semantic Kernel",
      "homepage": null,
      "language": "C#",
      "created_at": "2023-04-25T19:29:11Z",
      "updated_at": "2025-04-14T20:30:27Z",
      "topics": [],
      "readme": "# Semantic Kernel Starters\n\n> [!IMPORTANT]  \n> We have consolidated all of our samples in the main repository and plan to remove this one in the near future.\n> Please use the followiing links:\n> - [.Net Samples](https://github.com/microsoft/semantic-kernel/tree/main/dotnet/samples)\n> - [Python Samples](https://github.com/microsoft/semantic-kernel/tree/main/python/samples)\n> - [Java Samples](https://github.com/microsoft/semantic-kernel-java/tree/main/samples)\n\nThis repository contains starter projects for the [Semantic Kernel](https://github.com/microsoft/semantic-kernel). Each starter is a self-contained application using a different programming language and application runtime.\n\n## Usage\n\n- `git clone https://github.com/microsoft/semantic-kernel-starters`\n- `code <any-sample-folder>`\n- Follow the instructions in each sample's README for setting up and running the sample\nAlternatively, you can use the [Semantic Kernel Tools](https://marketplace.visualstudio.com/items?itemName=ms-semantic-kernel.semantic-kernel) to \"Create a New App\" using the starters\n\n## Getting Started\n\n- [C# Hello World](sk-csharp-hello-world): The Hello World C# console application starter for the Semantic Kernel.\n- [C# Console Chat](sk-csharp-console-chat): A console application based chat starter for the Semantic Kernel.\n- [C# Azure Functions](sk-csharp-azure-functions): The Hello World C# Azure Functions starter for the Semantic Kernel.\n- [C# Azure ChatGPT Plugin](sk-csharp-chatgpt-plugin): The Joke ChatGPT Plugin starter using C# Azure Functions for the Semantic Kernel.\n- [Python Hello World](sk-python-hello-world): The Hello World Python console application starter for the Semantic Kernel.\n- [Python Azure Functions](sk-python-azure-functions): The Hello World Python Azure Functions starter for the Semantic Kernel.\n- [Typescript Console Chat](sk-typescript-console-chat): A console application based chat starter for the Semantic Kernel.\n- [Java Hello World](sk-java-hello-world): The Hello World Java console application starter for the Semantic Kernel.\n- [Python Azure Functions ChatGPT Plugin](sk-python-azure-functions-chatgpt-plugin): The Joke ChatGPT Plugin starter using Python Azure Functions for the Semantic Kernel.\n- [Python Flask ChatGPT Plugin](sk-python-flask-chatgpt-plugin): The Joke ChatGPT Plugin starter using Python Flask for the Semantic Kernel.\n- [Typescript Console Chat](sk-typescript-console-chat): A console application based chat starter for the Semantic Kernel. \n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n"
    },
    {
      "name": "Azure/aistudio-copilot-sample",
      "stars": 314,
      "img": "https://avatars.githubusercontent.com/u/6844498?s=40&v=4",
      "owner": "Azure",
      "repo_name": "aistudio-copilot-sample",
      "description": "Sample quickstart repo for getting started building an enterprise chat copilot in Azure AI Studio",
      "homepage": null,
      "language": "Python",
      "created_at": "2023-09-28T19:01:30Z",
      "updated_at": "2025-04-11T15:35:23Z",
      "topics": [],
      "readme": "\n ### :warning: WARNING: **This repository is deprecated and no longer maintained** :warning:\n|There is a new repository that covers the latest Azure AI code-first experiences. Navigate to and star this one instead: https://github.com/Azure-Samples/rag-data-openai-python-promptflow/tree/main|\n|---------------------------|\n#### ❗Important\n\n**Features used in this repository are in preview. Preview versions are provided without a service level agreement, and they are not recommended for production workloads. Certain features might not be supported or might have constrained capabilities. For more information, see [Supplemental Terms of Use for Microsoft Azure Previews](https://azure.microsoft.com/support/legal/preview-supplemental-terms/).**\n\n# Getting Started\n\nThis repository contains a copilot getting started sample that can be used with the the [Azure AI Studio preview](https://aka.ms/azureai/docs). \n\nThe sample walks through creating a copilot enterprise chat API that uses custom Python code to ground the copilot responses in your company data and APIs. The sample is meant to provide a starting point that you can further customize to add additional intelligence or capabilities. Following the below steps in the README, you will be able to: set up your development environment, create your Azure AI resources and project, build an index containing product information, run your co-pilot, evaluate it, and deploy & invoke an API.\n\nNOTE: We do not guarantee the quality of responses produced by this sample copilot or its suitability for use in your scenario, and responses will vary as development of this sample is ongoing. You must perform your own validation the outputs of the copilot and its suitability for use within your company.\n\n## Step 1: Set up your development environment\n\n### Step 1a: Use a cloud development environment\n#### Explore sample with Codespaces\n- To get started quickly with this sample, you can use a pre-built Codespaces development environment. **Click the button below** to open this repo in GitHub Codespaces, and then continue the readme!\n[![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://codespaces.new/Azure/aistudio-copilot-sample?quickstart=1)\n\n- Once you've launched Codespaces you can proceed to step 2.\n\n#### Start developing in an Azure AI curated VS Code development environment\n- If you intend to develop your own code following this sample, we recommend you use the **Azure AI curated VS Code development environment**. It comes preconfigured with the Azure AI SDK packages that you will use to run this sample.\n- You can get started with this cloud environment from the Azure AI Studio by following these steps: [Work with Azure AI projects in VS Code](https://learn.microsoft.com/azure/ai-studio/how-to/develop-in-vscode)\n\n:grey_exclamation: **Important: If you are viewing this README from within this cloud VS Code environment, you can proceed directly to step 2!** This case will apply to you if you launched VS Code from an Azure AI Studio project. The AI SDK packages are already installed.\n\n\n### Step 1b: Alternatively, set up your local development environment\n\n1. First, clone the code sample locally:\n```\ngit clone https://github.com/azure/aistudio-copilot-sample\ncd aistudio-copilot-sample\n```\n\n2. Next, create a new Python virtual environment where we can safely install the SDK packages:\n\n * On MacOS and Linux run:\n   ```\n   python3 --version\n   python3 -m venv .venv\n   ```\n   ```\n   source .venv/bin/activate\n   ```\n* On Windows run:\n   ```\n   py -3 --version\n   py -3 -m venv .venv\n   ```\n   ```\n   .venv\\scripts\\activate\n   ```\n\n3. Now that your environment is activated, install the SDK packages\n```\npip install -r requirements.txt\n```\n\n## Step 2: Set your Azure resource environment variables\nNote: You'll need to deploy a chat completions model (e.g. GPT-4) and a text embedding model (e.g. text-embedding-ada-002) to run the sample. Do this in the [Azure AI Model Catalog](https://ai.azure.com/explore/models).\n\nCreate a .env file to store your Azure keys and endpoints. Copy the contents of the example .env file (`.env.sample`) into your .env file.  This is where you will reference environment variables in the code.\n\nTo find your keys, deployments, and connections, open your project in [AI Studio](https://aka.ms/AzureAIStudio). Navigate to the *Settings* page and copy over the following values:\n * Azure subscription ID \n * Azure OpenAI API key, endpoint, and API version \n * Azure AI Search key, endpoint, and index name (change the AI Search Index project name to be lowercase)\n * Azure OpenAI chat model, deployment, evaluation model (these can be the same, e.g. `gpt4`)\n * Azure OpenAI evaluation model (the text name of the model, e.g. `\"gpt4\"`)\n * Azure OpenAI Embedding model and embedding deployment (these should be the same, e.g. `text-embedding-ada-002`)\n\n\nNote: You can open your project in [AI Studio](https://aka.ms/AzureAIStudio) to view your projects configuration and components (generated indexes, evaluation runs, and endpoints). You can also create new resources, deployments and connections here to use in your code.\n\n## Step 3: Build an Azure AI Search index\n### Context\n\nYou'll create a search index to retrieve our product data through code. When we run our Python program (```run.py```),  the argument ```--build-index``` creates an Azure Search index via the SDK. \n\nThis sample uses a set of markdown files with product information for the fictitious Contoso Trek retailer stored in ```data/3-product-info``` folder. If you want to follow this sample directly, follow the steps below. You can also run the  ```build_cogsearch_index``` command using a different folder of data, or replace the contents of the folder with your own documents.\n\n### Run the commands\nIn the ```run.py``` file, find where the method `build_cogsearch_index` is invoked, and specify your index name and dataset path. The method invocation should look like this:\n```python\nbuild_cogsearch_index(\"product-info\", \"./data/3-product-info\")\n```\nThen, run the following command in the command line to create the search index:\n\n```bash\npython src/run.py --build-index\n```\n\n\n## Step 4: Run the copilot with a sample question\n\nTo run a single question & answer through the sample co-pilot:\n```bash\npython src/run.py --question \"which tent is the most waterproof?\"\n```\n_Note: you may see a warning about a RuntimeError; it can be safely ignored - evaluation will be unaffected. We are working to resolve this output issue._\n\nYou can try out different sample implementations by specifying the `--implementation` flag with `promptflow`, `langchain` or `aisdk`.\n\n:grey_exclamation: If you try out the `promptflow` implementation, first check that your deployment names (both embedding and chat) and index name (if it's changed from the previous steps) in `src/copilot_promptflow/flow.dag.yaml` match what's in the `.env` file.\n\n```bash\npython src/run.py --question \"which tent is the most waterproof?\" --implementation promptflow\n```\n\n## Step 5: Test the copilots using a chat completion model to evaluate results\n\nTo run evaluation on a copilot implementations:\n```\npython src/run.py --evaluate --implementation aisdk\n```\n\nYou can change `aisdk` to any of the other implementation names to run an evaluation on them.\n\n## Step 6: Deploy the sample code\n\nTo deploy one of the implementations to an online endpoint, use:\n```bash\npython src/run.py --deploy\n```\n\nTo test out the online enpoint, run:\n```bash\npython src/run.py --invoke \n```\n\n## Additional Tips and Resources\n\n### Customize the development container\n\nYou can pip install packages into your development environment but they will disappear if you rebuild your container and need to be reinstalled (re-build is not automatic). You may want this, so that you can easily reset back to a clean environment. Or, you may want to install some packages by default into the container so that you don't need to re-install packages after a rebuild.\n\nTo add packages into the default container, you can update the Dockerfile in `.devcontainer/Dockerfile`, and then rebuild the development container from the command palette by pressing `Ctrl/Cmd+Shift+P` and selecting the `Rebuild container` command.\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n"
    },
    {
      "name": "microsoft/Conversation-Knowledge-Mining-Solution-Accelerator",
      "stars": 271,
      "img": "https://avatars.githubusercontent.com/u/6154722?s=40&v=4",
      "owner": "microsoft",
      "repo_name": "Conversation-Knowledge-Mining-Solution-Accelerator",
      "description": "This solution accelerator leverages Azure AI Foundry, Azure AI Content Understanding, Azure OpenAI Service, and Azure AI Search to enable organizations to derive insights from volumes of conversational data using generative AI. It offers key phrase extraction, topic modeling, and interactive chat experiences through an intuitive web interface.",
      "homepage": "",
      "language": "Jupyter Notebook",
      "created_at": "2020-06-15T11:48:44Z",
      "updated_at": "2025-04-23T02:34:27Z",
      "topics": [
        "ai-azd-templates",
        "azd-templates"
      ],
      "readme": "# Conversation knowledge mining solution accelerator\n\nMENU: [**USER STORY**](#user-story) \\| [**QUICK DEPLOY**](#quick-deploy)  \\| [**SUPPORTING DOCUMENTATION**](#supporting-documentation) \n\n<h2><img src=\"./docs/Images/ReadMe/userStory.png\" width=\"64\">\n<br/>\nUser story\n</h2>\n\n### Overview\n\nThis solution accelerator enables customers with large amounts of conversational data to improve decision-making by leveraging intelligence to uncover insights, relationships, and patterns from customer interactions. It empowers users to gain valuable knowledge and drive targeted business impact. \n\nIt leverages Azure AI Foundry, Azure AI Content Understanding, Azure OpenAI Service, and Azure AI Search to transform large volumes of conversational data into actionable insights through topic modeling, key phrase extraction, speech-to-text transcription, and interactive chat experiences.\n\n\n### Technical key features\n\n![image](./docs/Images/ReadMe/techkeyfeatures.png)\n\nBelow is an image of the solution accelerator.\n\n![image](./docs/Images/ReadMe/ckm-ui.png)\n\n### Use case / scenario\n\nAn analyst managing large volumes of conversational data needs a solution to visualize key insights and uncover patterns using natural language. An interactive dashboard enables them to explore rich, actionable insights for faster, and more informed decision-making.\n \nThis solution empowers analysts with tools to ask questions and receive real-time, contextualized responses. It streamlines problem-solving, enhances collaboration, and fosters innovation by making data-driven insights accessible and shareable.\n\nThe sample data used in this repository is synthetic and generated using Azure OpenAI service. The data is intended for use as sample data only.\n\n### Solution architecture\n![image](./docs/Images/ReadMe/ckm-sol-arch.png)\n\n\n<h2><img src=\"./docs/Images/ReadMe/quickDeploy.png\" width=\"64\">\n<br/>\nQUICK DEPLOY\n</h2>\n\n### **Prerequisites**\n\nTo deploy this solution accelerator, ensure you have access to an [Azure subscription](https://azure.microsoft.com/free/) with the necessary permissions to create **resource groups and resources**. Follow the steps in  [Azure Account Set Up](./docs/AzureAccountSetUp.md) \n\nCheck the [Azure Products by Region](https://azure.microsoft.com/en-us/explore/global-infrastructure/products-by-region/?products=all&regions=all) page and select a **region** where the following services are available:  \n\n- Azure AI Foundry \n- Azure OpenAI Service \n- Azure AI Search\n- Azure AI Content Understanding\n- Embedding Deployment Capacity  \n- GPT Model Capacity\n- [Azure Semantic Search](./docs/AzureSemanticSearchRegion.md)  \n\nHere are some example regions where the services are available: East US, East US2, Australia East, UK South, France Central.\n\n### ⚠️ Important: Check Azure OpenAI Quota Availability  \n\n➡️ To ensure sufficient quota is available in your subscription, please follow **[Quota check instructions guide](./docs/quota_check.md)** before you deploy the solution.\n\n| [![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://codespaces.new/microsoft/Conversation-Knowledge-Mining-Solution-Accelerator) | [![Open in Dev Containers](https://img.shields.io/static/v1?style=for-the-badge&label=Dev%20Containers&message=Open&color=blue&logo=visualstudiocode)](https://vscode.dev/redirect?url=vscode://ms-vscode-remote.remote-containers/cloneInVolume?url=https://github.com/microsoft/Conversation-Knowledge-Mining-Solution-Accelerator) |\n|---|---|\n        \n### **Configurable Deployment Settings**  \n\nWhen you start the deployment, most parameters will have **default values**, but you can update the following settings:  \n\n| **Setting** | **Description** | **Default value** |\n|------------|----------------|------------|\n| **Azure Region** | The region where resources will be created. | eastus | \n| **Environment Name** | A **3-20 character alphanumeric value** used to generate a unique ID to prefix the resources. | kmtemplate |\n| **Azure AI Content Understanding Location** | Select from a drop-down list of values. | swedencentral |\n| **Secondary Location** | A **less busy** region for **Azure SQL and Azure Cosmos DB**, useful in case of availability constraints. | eastus2 |\n| **Deployment Type** | Select from a drop-down list. | GlobalStandard |\n| **GPT Model** | Choose from **gpt-4, gpt-4o, gpt-4o-mini** | gpt-4o-mini |  \n| **GPT Model Deployment Capacity** | Configure capacity for **GPT models**. | 30k |\n| **Embedding Model** | Default: **text-embedding-ada-002**. | text-embedding-ada-002 |\n| **Embedding Model Capacity** | Set the capacity for **embedding models**. | 80k |\n\n\n### [Optional] Quota Recommendations  \nBy default, the **GPT model capacity** in deployment is set to **30k tokens**.  \n> **We recommend increasing the capacity to 100k tokens for optimal performance.** \n\nTo adjust quota settings, follow these [steps](./docs/AzureGPTQuotaSettings.md)  \n\n### Deployment Options\nPick from the options below to see step-by-step instructions for: GitHub Codespaces, VS Code Dev Containers, Local Environments, and Bicep deployments.\n\n<details>\n  <summary><b>Deploy in GitHub Codespaces</b></summary>\n\n### GitHub Codespaces\n\nYou can run this solution using GitHub Codespaces. The button will open a web-based VS Code instance in your browser:\n\n1. Open the solution accelerator (this may take several minutes):\n\n    [![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://codespaces.new/microsoft/Conversation-Knowledge-Mining-Solution-Accelerator)\n2. Accept the default values on the create Codespaces page\n3. Open a terminal window if it is not already open\n4. Continue with the [deploying steps](#deploying)\n\n</details>\n\n<details>\n  <summary><b>Deploy in VS Code</b></summary>\n\n ### VS Code Dev Containers\n\nYou can run this solution in VS Code Dev Containers, which will open the project in your local VS Code using the [Dev Containers extension](https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-containers):\n\n1. Start Docker Desktop (install it if not already installed)\n2. Open the project:\n\n    [![Open in Dev Containers](https://img.shields.io/static/v1?style=for-the-badge&label=Dev%20Containers&message=Open&color=blue&logo=visualstudiocode)](https://vscode.dev/redirect?url=vscode://ms-vscode-remote.remote-containers/cloneInVolume?url=https://github.com/microsoft/Conversation-Knowledge-Mining-Solution-Accelerator)\n\n\n3. In the VS Code window that opens, once the project files show up (this may take several minutes), open a terminal window.\n4. Continue with the [deploying steps](#deploying)\n\n</details>\n\n<details>\n  <summary><b>Deploy in your local environment</b></summary>\n\n ### Local environment\n\nIf you're not using one of the above options for opening the project, then you'll need to:\n\n1. Make sure the following tools are installed:\n\n    * [Azure Developer CLI (azd)](https://aka.ms/install-azd)\n    * [Python 3.9+](https://www.python.org/downloads/)\n    * [Docker Desktop](https://www.docker.com/products/docker-desktop/)\n    * [Git](https://git-scm.com/downloads)\n    * [Powershell](https://learn.microsoft.com/en-us/powershell/scripting/install/installing-powershell-on-windows?view=powershell-7.5)<br/>Required for Windows users only. Follow the steps [here](./docs/PowershellSetup.md) to add it to the Windows PATH.\n\n2. Download the project code:\n\n    ```shell\n    azd init -t microsoft/Conversation-Knowledge-Mining-Solution-Accelerator/\n    ```\n\n3. Open the project folder in your terminal or editor.\n\n4. Continue with the [deploying steps](#deploying).\n\n</details>\n\n<!--\n<details>\n  <summary><b>Deploy with Bicep/ARM template</b></summary>\n\n### Bicep\n \n   Click the following deployment button to create the required resources for this solution directly in your Azure Subscription.\n\n   [![Deploy to Azure](https://aka.ms/deploytoazurebutton)](https://portal.azure.com/#create/Microsoft.Template/uri/https%3A%2F%2Fraw.githubusercontent.com%2Fmicrosoft%2FConversation-Knowledge-Mining-Solution-Accelerator%2Fmain%2Finfra%2Fmain.json)          \n\n</details>\n-->\n\n### Deploying\n\nOnce you've opened the project in [Codespaces](#github-codespaces) or in [Dev Containers](#vs-code-dev-containers) or [locally](#local-environment), you can deploy it to Azure following the following steps. \n\nTo change the azd parameters from the default values, follow the steps [here](./docs/CustomizingAzdParameters.md). \n\n\n1. Login to Azure:\n\n    ```shell\n    azd auth login\n    ```\n\n    #### To authenticate with Azure Developer CLI (`azd`), use the following command with your **Tenant ID**:\n\n    ```sh\n    azd auth login --tenant-id <tenant-id>\n   ```\n\n2. Provision and deploy all the resources:\n\n    ```shell\n    azd up\n    ```\n\n3. Provide an `azd` environment name (like \"ckmapp\")\n4. Select a subscription from your Azure account, and select a location which has quota for all the resources. \n    * This deployment will take *7-10 minutes* to provision the resources in your account and set up the solution with sample data. \n    * If you get an error or timeout with deployment, changing the location can help, as there may be availability constraints for the resources.\n\n5. Once the deployment has completed successfully, open the [Azure Portal](https://portal.azure.com/), go to the deployed resource group, find the App Service and get the app URL from `Default domain`.\n\n6. You can now delete the resources by running `azd down`, if you are done trying out the application. \n<!-- 6. You can now proceed to run the [development server](#development-server) to test the app locally, or if you are done trying out the app, you can delete the resources by running `azd down`. -->\n\n<h2>\nAdditional Steps\n</h2>\n\n1. **Add App Authentication**\n   \n    Follow steps in [App Authentication](./docs/AppAuthentication.md) to configure authenitcation in app service.\n\n    Note: Authentication changes can take up to 10 minutes \n\n2. **Deleting Resources After a Failed Deployment**\n     Follow steps in [Delete Resource Group](./docs/DeleteResourceGroup.md) If your deployment fails and you need to clean up the resources.\n\n## Sample Questions\n\nTo help you get started, here are some **Sample Questions** you can ask in the app:\n\n- Total number of calls by date for the last 7 days\n- Show average handling time by topics in minutes\n- What are the top 7 challenges users reported?\n- Give a summary of billing issues\n- When customers call in about unexpected charges, what types of charges are they seeing?\n\nThese questions serve as a great starting point to explore insights from the data.\n\n<h2>\nResponsible AI Transparency FAQ \n</h2>\n\nPlease refer to [Transparency FAQ](./TRANSPARENCY_FAQ.md) for responsible AI transparency details of this solution accelerator.\n\n\n<h2>\nSupporting documentation\n</h2>\n\n### Costs\n\nPricing varies per region and usage, so it isn't possible to predict exact costs for your usage.\nThe majority of the Azure resources used in this infrastructure are on usage-based pricing tiers.\nHowever, Azure Container Registry has a fixed cost per registry per day.\n\nYou can try the [Azure pricing calculator](https://azure.microsoft.com/en-us/pricing/calculator) for the resources:\n\n* Azure AI Foundry: Free tier. [Pricing](https://azure.microsoft.com/pricing/details/ai-studio/)\n* Azure AI Search: Standard tier, S1. Pricing is based on the number of documents and operations. [Pricing](https://azure.microsoft.com/pricing/details/search/)\n* Azure Storage Account: Standard tier, LRS. Pricing is based on storage and operations. [Pricing](https://azure.microsoft.com/pricing/details/storage/blobs/)\n* Azure Key Vault: Standard tier. Pricing is based on the number of operations. [Pricing](https://azure.microsoft.com/pricing/details/key-vault/)\n* Azure AI Services: S0 tier, defaults to gpt-4o-mini and text-embedding-ada-002 models. Pricing is based on token count. [Pricing](https://azure.microsoft.com/pricing/details/cognitive-services/)\n* Azure Container App: Consumption tier with 0.5 CPU, 1GiB memory/storage. Pricing is based on resource allocation, and each month allows for a certain amount of free usage. [Pricing](https://azure.microsoft.com/pricing/details/container-apps/)\n* Azure Container Registry: Basic tier. [Pricing](https://azure.microsoft.com/pricing/details/container-registry/)\n* Log analytics: Pay-as-you-go tier. Costs based on data ingested. [Pricing](https://azure.microsoft.com/pricing/details/monitor/)\n* Azure SQL Server: General Purpose Tier. [Pricing](https://azure.microsoft.com/pricing/details/azure-sql-database/single/)\n* Azure Cosmos DB: [Pricing](https://azure.microsoft.com/en-us/pricing/details/cosmos-db/autoscale-provisioned/)\n* Azure functions: Consumption tier [Pricing](https://azure.microsoft.com/en-us/pricing/details/functions/)\n\n⚠️ To avoid unnecessary costs, remember to take down your app if it's no longer in use,\neither by deleting the resource group in the Portal or running `azd down`.\n\n### Security guidelines\n\nThis template uses Azure Key Vault to store all connections to communicate between resources.\n\nThis template also uses [Managed Identity](https://learn.microsoft.com/entra/identity/managed-identities-azure-resources/overview) for local development and deployment.\n\nTo ensure continued best practices in your own repository, we recommend that anyone creating solutions based on our templates ensure that the [Github secret scanning](https://docs.github.com/code-security/secret-scanning/about-secret-scanning) setting is enabled.\n\nYou may want to consider additional security measures, such as:\n\n* Enabling Microsoft Defender for Cloud to [secure your Azure resources](https://learn.microsoft.com/azure/security-center/defender-for-cloud).\n* Protecting the Azure Container Apps instance with a [firewall](https://learn.microsoft.com/azure/container-apps/waf-app-gateway) and/or [Virtual Network](https://learn.microsoft.com/azure/container-apps/networking?tabs=workload-profiles-env%2Cazure-cli).\n\n<!-- ### How to customize \n\nIf you'd like to customize the solution accelerator, here are some ways you might do that:\n- Ingest your own [audio conversation files](./docs/ConversationalDataFormat.md) by uploading them into the storage account and run the process data scripts.\n- You can also scale the solution and process large volumes of data by deploying this to Microsoft Fabric by following the steps in [Fabric_deployment.md](./docs/Fabric_deployment.md) -->\n\n### Additional resources\n\n<!-- - [Microsoft Fabric documentation - Microsoft Fabric | Microsoft Learn](https://learn.microsoft.com/en-us/fabric/) -->\n- [Azure AI Foundry documentation](https://learn.microsoft.com/en-us/azure/ai-studio/) \n- [Azure AI Content Understanding documentation](https://learn.microsoft.com/en-us/azure/ai-services/content-understanding/) \n- [Azure OpenAI Service](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/use-your-data)\n- [Azure AI Search](https://learn.microsoft.com/en-us/azure/search/) \n- [Azure Functions](https://learn.microsoft.com/en-us/azure/azure-functions/)\n- [Azure App Service](https://learn.microsoft.com/en-us/azure/app-service/)\n- [Azure SQL Database](https://learn.microsoft.com/en-us/azure/azure-sql/)\n- [Azure Cosmos DB](https://learn.microsoft.com/en-us/azure/cosmos-db/)\n<!-- - [Microsoft Fabric](https://learn.microsoft.com/en-us/fabric/) -->\n\n<!-- - [Speech service documentation - Tutorials, API Reference - Azure AI services - Azure AI services | Microsoft Learn](https://learn.microsoft.com/en-us/azure/ai-services/speech-service/) -->\n\n\n\n## Disclaimers\n\nTo the extent that the Software includes components or code used in or derived from Microsoft products or services, including without limitation Microsoft Azure Services (collectively, “Microsoft Products and Services”), you must also comply with the Product Terms applicable to such Microsoft Products and Services. You acknowledge and agree that the license governing the Software does not grant you a license or other right to use Microsoft Products and Services. Nothing in the license or this ReadMe file will serve to supersede, amend, terminate or modify any terms in the Product Terms for any Microsoft Products and Services. \n\nYou must also comply with all domestic and international export laws and regulations that apply to the Software, which include restrictions on destinations, end users, and end use. For further information on export restrictions, visit https://aka.ms/exporting. \n\nYou acknowledge that the Software and Microsoft Products and Services (1) are not designed, intended or made available as a medical device(s), and (2) are not designed or intended to be a substitute for professional medical advice, diagnosis, treatment, or judgment and should not be used to replace or as a substitute for professional medical advice, diagnosis, treatment, or judgment. Customer is solely responsible for displaying and/or obtaining appropriate consents, warnings, disclaimers, and acknowledgements to end users of Customer’s implementation of the Online Services. \n\nYou acknowledge the Software is not subject to SOC 1 and SOC 2 compliance audits. No Microsoft technology, nor any of its component technologies, including the Software, is intended or made available as a substitute for the professional advice, opinion, or judgement of a certified financial services professional. Do not use the Software to replace, substitute, or provide professional financial advice or judgment.  \n\nBY ACCESSING OR USING THE SOFTWARE, YOU ACKNOWLEDGE THAT THE SOFTWARE IS NOT DESIGNED OR INTENDED TO SUPPORT ANY USE IN WHICH A SERVICE INTERRUPTION, DEFECT, ERROR, OR OTHER FAILURE OF THE SOFTWARE COULD RESULT IN THE DEATH OR SERIOUS BODILY INJURY OF ANY PERSON OR IN PHYSICAL OR ENVIRONMENTAL DAMAGE (COLLECTIVELY, “HIGH-RISK USE”), AND THAT YOU WILL ENSURE THAT, IN THE EVENT OF ANY INTERRUPTION, DEFECT, ERROR, OR OTHER FAILURE OF THE SOFTWARE, THE SAFETY OF PEOPLE, PROPERTY, AND THE ENVIRONMENT ARE NOT REDUCED BELOW A LEVEL THAT IS REASONABLY, APPROPRIATE, AND LEGAL, WHETHER IN GENERAL OR IN A SPECIFIC INDUSTRY. BY ACCESSING THE SOFTWARE, YOU FURTHER ACKNOWLEDGE THAT YOUR HIGH-RISK USE OF THE SOFTWARE IS AT YOUR OWN RISK.  \n"
    },
    {
      "name": "microsoft/semanticworkbench",
      "stars": 259,
      "img": "https://avatars.githubusercontent.com/u/6154722?s=40&v=4",
      "owner": "microsoft",
      "repo_name": "semanticworkbench",
      "description": "A versatile tool designed to help prototype intelligent assistants, agents and multi-agentic systems ",
      "homepage": "",
      "language": "Python",
      "created_at": "2024-07-29T21:36:05Z",
      "updated_at": "2025-04-23T07:26:58Z",
      "topics": [],
      "readme": "# Semantic Workbench\n\nSemantic Workbench is a versatile tool designed to help prototype intelligent assistants quickly.\nIt supports the creation of new assistants or the integration of existing ones, all within a\ncohesive interface. The workbench provides a user-friendly UI for creating conversations with one\nor more assistants, configuring settings, and exposing various behaviors.\n\nThe Semantic Workbench is composed of three main components:\n\n- [Workbench Service](workbench-service/README.md) (Python): The backend service that\n  handles core functionalities.\n- [Workbench App](workbench-app/README.md) (React/Typescript): The frontend web user\n  interface for interacting with workbench and assistants.\n- [Assistant Services](examples) (Python, C#, etc.): any number of assistant services that implement the service protocols/APIs,\n  developed using any framework and programming language of your choice.\n\nDesigned to be agnostic of any agent framework, language, or platform, the Semantic Workbench\nfacilitates experimentation, development, testing, and measurement of agent behaviors and workflows.\nAssistants integrate with the workbench via a RESTful API, allowing for flexibility and broad applicability in various development environments.\n\n![Semantic Workbench architecture](https://raw.githubusercontent.com/microsoft/semanticworkbench/main/docs/images/architecture-animation.gif)\n\n# Workbench interface examples\n\n![Configured dashboard example](docs/images/dashboard_configured_view.png)\n\n![Prospector Assistant example](docs/images/prospector_example.png)\n\n![Message debug inspection](docs/images/message_inspection.png)\n\n![Mermaid graph example](examples/dotnet/dotnet-02-message-types-demo/docs/mermaid.png)\n\n![ABC music example](examples/dotnet/dotnet-02-message-types-demo/docs/abc.png)\n\n# Quick start (Recommended) - GitHub Codespaces for turn-key development environment\n\nGitHub Codespaces provides a cloud-based development environment for your repository. It allows you to develop, build, and test your code\nin a consistent environment, without needing to install dependencies or configure your local machine. It works with any system with a web\nbrowser and internet connection, including Windows, MacOS, Linux, Chromebooks, tablets, and mobile devices.\n\nSee the [GitHub Codespaces / devcontainer README](.devcontainer/README.md) for more information on how to set up and use GitHub Codespaces\nwith Semantic Workbench.\n\n## Local development environment\n\nSee the [setup guide](docs/SETUP_DEV_ENVIRONMENT.md) on how to configure your dev environment. Or if you have Docker installed you can use dev containers with VS Code which will function similarly to Codespaces.\n\n## Using VS Code\n\nCodespaces will is configured to use `semantic-workbench.code-workspace`, if you are working locally that is recommended over opening the repo root. This ensures that all project configurations, such as tools, formatters, and linters, are correctly applied in VS Code. This avoids issues like incorrect error reporting and non-functional tools.\n\nWorkspace files allow us to manage multiple projects within a monorepo more effectively. Each project can use its own virtual environment (venv), maintaining isolation and avoiding dependency conflicts. Multi-root workspaces (\\*.code-workspace files) can point to multiple projects, each configured with its own Python interpreter, ensuring seamless functionality of Python tools and extensions.\n\n### Start the app and service\n\n- Use VS Code > `Run and Debug` (Ctrl/Cmd+Shift+D) > `semantic-workbench` to start the project\n- Open your browser and navigate to `https://127.0.0.1:4000`\n  - You may receive a warning about the app not being secure; click `Advanced` and `Proceed to localhost` to continue\n- You can now interact with the app and service in the browser\n\n### Start an assistant service:\n\n- Launch an example an [example](examples/) assistant service:\n  - No llm api keys needed\n    - Use VS Code > `Run and Debug` (Ctrl/Cmd+Shift+D) > `examples: python-01-echo-bot` to start the example assistant that echos your messages. This is a good base to understand the basics of building your own assistant.\n  - Bring your own llm api keys\n    - Use VS Code > `Run and Debug` (Ctrl/Cmd+Shift+D) > `examples: python-02-simple-chatbot` to start the example chatbot assistant. Either set your keys in your .env file or after creating the assistant as described below, select it and provide the keys in the configuration page.\n\n## Open the Workbench and create an Assistant\n\nOpen the app in your browser at [`https://localhost:4000`](https://localhost:4000). When you first log into the Semantic Workbench, follow these steps to get started:\n\n1. **Create an Assistant**: On the dashboard, click the `New Assistant` button. Select a template from the available assistant services, provide a name, and click `Save`.\n\n2. **Start a Conversation**: On the dashboard, click the `New Conversation` button. Provide a title for the conversation and click `Save`.\n\n3. **Add the Assistant**: In the conversation window, click the conversation canvas icon and add your assistant to the conversation from the conversation canvas. Now you can converse with your assistant using the message box at the bottom of the conversation window.\n\n   ![Open Conversation Canvas](docs/images/conversation_canvas_open.png)\n\n   ![Open Canvas](docs/images/open_conversation_canvas.png)\n\nExpected: You get a response from your assistant!\n\nNote that the workbench provides capabilities that not all examples use, for example providing attachments. See the [Semantic Workbench](docs/WORKBENCH_APP.md) for more details.\n\n# Developing your own assistants\n\nTo develop new assistants and connect existing ones, see the [Assistant Development Guide](docs/ASSISTANT_DEVELOPMENT_GUIDE.md) or any check out one of the [examples](examples).\n\n- [Python example 1](examples/python/python-01-echo-bot/README.md): a simple assistant echoing text back.\n- [Python example 2](examples/python/python-02-simple-chatbot/README.md): a simple chatbot implementing metaprompt guardrails and content moderation.\n- [Python example 3](examples/python/python-03-multimodel-chatbot/README.md): an extension of the simple chatbot that supports configuration against additional llms.\n- [.NET example 1](examples/dotnet/dotnet-01-echo-bot/README.md): a simple agent with echo and support for a basic `/say` command.\n- [.NET example 2](examples/dotnet/dotnet-02-message-types-demo/README.md): a simple assistants showcasing Azure AI Content Safety integration and some workbench features like Mermaid graphs.\n- [.NET example 3](examples/dotnet/dotnet-03-simple-chatbot/README.md): a functional chatbot implementing metaprompt guardrails and content moderation.\n\n## Starting the workbench from the command line\n\n- Run the script `tools\\run-workbench-chatbot.sh` or `tools\\run-workbench-chatbot.ps` which does the following:\n  - Starts the backend service, see [here for instructions](workbench-service/README.md).\n  - Starts the frontend app, see [here for instructions](workbench-app/README.md).\n  - Starts the [Python chatbot example](examples/python/python-02-simple-chatbot/README.md)\n\n## Refreshing Dev Environment\n\n- Use the `tools\\reset-service-data.sh` or `tools\\reset-service-data.sh` script to reset all service data. You can also delete `~/workbench-service/.data` or specific files if you know which one(s).\n- From repo root, run `make clean install`.\n  - This will perform a `git clean` and run installs in all sub-directories\n- Or a faster option if you just want to install semantic workbench related stuff:\n  - From repo root, run `make clean`\n  - From `~/workbench-app`, run `make install`\n  - From `~/workbench-service`, run `make install`\n\n# Contributing\n\nThis project welcomes contributions and suggestions. Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit <https://cla.opensource.microsoft.com>.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nPlease see the detailed [contributing guide](CONTRIBUTING.md) for more information on how you can get involved.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n# Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft\ntrademarks or logos is subject to and must follow\n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n"
    },
    {
      "name": "sambanova/ai-starter-kit",
      "stars": 220,
      "img": "https://avatars.githubusercontent.com/u/80118584?s=40&v=4",
      "owner": "sambanova",
      "repo_name": "ai-starter-kit",
      "description": null,
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2023-10-06T16:34:46Z",
      "updated_at": "2025-04-21T21:18:57Z",
      "topics": [],
      "readme": "<a href=\"https://sambanova.ai/\">\n<picture>\n  <source media=\"(prefers-color-scheme: dark)\" srcset=\"./images/SambaNova-light-logo-1.png\" height=\"60\">\n  <img alt=\"SambaNova logo\" src=\"./images/SambaNova-dark-logo-1.png\" height=\"60\">\n</picture>\n</a>\n\n# SambaNova AI Starter Kits\n\n# Overview\n\nSambaNova AI Starter Kits are a collection of open-source examples and guides designed to facilitate the deployment of AI-driven use cases for both developers and enterprises.\n\nTo run these examples, you can obtain a free API key using [SambaNova Cloud](https://cloud.sambanova.ai). Alternatively, if you are a current SambaNova customer, you can deploy your models using [SambaStudio](https://docs.sambanova.ai/sambastudio/latest/index.html). Most of the code examples are written in Python, although the concepts can be applied to any programming language.\n\nQuestions? Just <a href=\"https://community.sambanova.ai/latest\" target=\"_blank\">message us</a> on SambaNova Community <a href=\"https://community.sambanova.ai/latest\" target=\"_blank\"><img src=\"https://github.com/sambanova/ai-starter-kit/assets/150964187/aef53b52-1dc0-4cbf-a3be-55048675f583\" alt=\"Community\" width=\"22\"/></a> or <a href=\"https://github.com/sambanova/ai-starter-kit/issues/new/choose\" target=\"_blank\">create an issue</a> in GitHub. We're happy to help live!\n\n# Available AI Starter Kits\n\nThe table below lists the available kits, which are grouped into four categories: 1) Data Ingestion & Preparation, 2) Model Development & Optimization, 3) Intelligent Information Retrieval, and 4) Advanced AI Capabilities.\n\nFor functionalities related to third-party integrations, find a list in our [Integrations folder](https://github.com/sambanova/integrations).\n\n**Note**: For each kit, we specify whether it is compatible with SambaNova Cloud, SambaStudio, or both.\n\n<table style=\"width: 100%;\">\n<thead>\n<tr>\n<th width=\"20%\">Name</th>\n<th width=\"45%\">Kit Description</th>\n<th width=\"20%\">Compatible APIs</th>\n<th width=\"15%\">Category</th>\n  \n</tr>\n</thead>\n  \n<tbody>\n\n<tr>\n<td width=\"20%\"><a href=\"data_extraction/README.md\">Data Extraction</a></td>\n<td width=\"40%\">Series of notebooks that demonstrate methods for extracting text from documents in different input formats.</td>\n<td width=\"20%\"> SambaNova Cloud, SambaStudio</td>\n<td width=\"20%\"> Data Ingestion & Preparation </td>  \n</tr>\n\n<tr>\n<td width=\"20%\"><a href=\"e2e_fine_tuning/README.md\">End to End Fine-tuning</a></td>\n<td width=\"40%\"> Recipe to upload and train a Language Model (LLM) using your own data in SambaStudio platform. </td>\n<td width=\"20%\"> SambaStudio </td>\n<td width=\"20%\"> Data Ingestion & Preparation </td>\n</tr>\n\n<tr>\n<td width=\"20%\"><a href=\"fine_tuning_embeddings/README.md\"> Fine tuning embeddings</a></td>\n<td width=\"40%\">Example workflow for fine-tuning embeddings from unstructured data, leveraging Large Language Models (LLMs) and open-source embedding models to enhance NLP task performance.</td>\n<td width=\"20%\">SambaStudio</td>\n<td width=\"20%\"> Model Development & Optimization </td>  \n</tr>\n\n<tr>\n<td width=\"20%\"><a href=\"fine_tuning_sql/README.md\"> Fine tuning SQL</a></td>\n<td width=\"40%\">Example workflow for fine-tuning an SQL model for Question-Answering purposes, leveraging Large Language Models (LLMs) and open-source embedding models to enhance SQL generation task performance.</td>\n<td width=\"20%\">SambaStudio</td>\n<td width=\"20%\"> Model Development & Optimization </td>\n</tr>\n\n<tr>\n<td width=\"20%\"><a href=\"prompt_engineering/README.md\">Prompt Engineering</a></td>\n</td>\n<td width=\"40%\">Starting point demo for prompt engineering using SambaNova's API to experiment with different use case templates. Provides useful resources to improve prompt crafting, making it an ideal entry point for those new to this AISK.</td>\n<td width=\"20%\"> SambaNova Cloud, SambaStudio</td>\n<td width=\"20%\"> Model Development & Optimization </td> \n</tr>\n\n<tr>\n<td width=\"20%\"><a href=\"enterprise_knowledge_retriever/README.md\">Enterprise Knowledge Retrieval</a></td>\n<td width=\"40%\">Sample implementation of the semantic search workflow using the SambaNova platform to get answers to questions about your documents. Includes a runnable demo.</td>\n<td width=\"20%\"> SambaNova Cloud, SambaStudio</td>\n<td width=\"20%\"> Intelligent Information Retrieval </td>  \n</tr>\n\n<tr>\n<td width=\"20%\"><a href=\"image_search/README.md\">Image Search</a></td>\n<td width=\"40%\">This example workflow shows a simple approach to image search by image description or image similarity. All workflows are built using the SambaNova platform. </td>\n<td width=\"20%\"> SambaStudio </td>\n<td width=\"20%\"> Intelligent Information Retrieval </td>   \n</tr>\n\n<tr>\n<td width=\"20%\"><a href=\"multimodal_knowledge_retriever/README.md\">Multimodal Knowledge Retriever</a></td>\n<td width=\"40%\"> Sample implementation of the semantic search workflow leveraging the SambaNova platform to get answers using text, tables, and images to questions about your documents. Includes a runnable demo. </td>\n<td width=\"20%\"> SambaNova Cloud, SambaStudio</td>\n<td width=\"20%\"> Intelligent Information Retrieval </td>   \n</tr>\n\n<tr>\n<td width=\"20%\"><a href=\"post_call_analysis/README.md\">Post Call Analysis</a></td>\n<td width=\"40%\">Example workflow that shows a systematic approach to post-call analysis including Automatic Speech Recognition (ASR), diarization, large language model analysis, and retrieval augmented generation (RAG) workflows. All workflows are built using the SambaNova platform. </td>\n<td width=\"20%\">SambaNova Cloud, SambaStudio</td>\n<td width=\"20%\"> Intelligent Information Retrieval </td>   \n</tr>\n\n<tr>\n<td width=\"20%\"><a href=\"eval_jumpstart/README.md\">RAG Evaluation Kit</a></td>\n<td width=\"40%\">A tool for evaluating the performance of LLM APIs using the RAG Evaluation methodology.</td>\n<td width=\"20%\">SambaStudio</td>\n<td width=\"20%\"> Intelligent Information Retrieval </td>  \n</tr>\n\n<tr>\n<td width=\"20%\"><a href=\"search_assistant/README.md\">Search Assistant</a></td>\n<td width=\"40%\">Sample implementation of the semantic search workflow built using the SambaNova platform to get answers to your questions using search engine snippets, and website crawled information as the source. Includes a runnable demo.</td>\n<td width=\"20%\">SambaNova Cloud, SambaStudio</td>\n<td width=\"20%\"> Intelligent Information Retrieval </td>   \n</tr>\n\n<tr>\n<td width=\"20%\"><a href=\"web_crawled_data_retriever/README.md\">Web Crawled Data Retrieval</a></td>\n<td width=\"40%\">Sample implementation of a semantic search workflow built using the SambaNova platform to get answers to your questions using website crawled information as the source. Includes a runnable demo.</td>\n<td width=\"20%\">SambaNova Cloud, SambaStudio</td>\n<td width=\"20%\"> Intelligent Information Retrieval </td>   \n</tr>\n\n<tr>\n<td width=\"20%\"><a href=\"benchmarking/README.md\">Benchmarking</a></td>\n<td width=\"40%\">This kit evaluates the performance of multiple LLM models hosted in SambaStudio. It offers various performance metrics and configuration options. Users can also see these metrics within a chat interface.</td>\n<td width=\"20%\"> SambaNova Cloud, SambaStudio</td>\n<td width=\"20%\"> Advanced AI Capabilities </td>    \n</tr>\n\n<tr>\n<td width=\"20%\"><a href=\"https://github.com/sambanova/integrations/tree/main/continue\">Code Copilot</a></td>\n<td width=\"40%\">This example guide shows a simple integration with Continue VSCode and JetBrains extension using SambaNova platforms, to use Sambanova's hosted models as your custom coding assistant. </td>\n<td width=\"20%\"> SambaNova Cloud, SambaStudio</td>\n<td width=\"20%\"> Integrations </td>  \n</tr>\n\n<tr>\n<td width=\"20%\"><a href=\"bundle_jump_start/README.md\">Bundle jump start</a></td>\n<td width=\"40%\">This kit demonstrates how to call <a href=https://coe-1.Cloud.snova.ai/>SambaNova Bundle</a> models using the Langchain framework. The script offers different approaches for calling Bundle models, including using SambaStudio with a named expert, and using SambaStudio with routing.</td>\n<td width=\"20%\">SambaStudio</td>\n<td width=\"20%\"> Advanced AI Capabilities </td>  \n</tr>\n\n<tr>\n<td width=\"20%\"><a href=\"financial_assistant/README.md\"> Financial Assistant</a></td> \n<td width=\"40%\">This app demonstrates the capabilities of LLMs in extracting and analyzing financial data using function calling, web scraping, and RAG.</td>\n<td width=\"20%\"> SambaNova Cloud, SambaStudio</td>\n<td width=\"20%\"> Advanced AI Capabilities </td>\n</tr>\n\n<tr>\n<td width=\"20%\"><a href=\"function_calling/README.md\"> Function Calling</a></td>\n<td width=\"40%\">Example of tools calling implementation and a generic function calling module that can be used inside your application workflows.</td>\n<td width=\"20%\"> SambaNova Cloud, SambaStudio</td>\n<td width=\"20%\"> Advanced AI Capabilities </td>\n</tr>\n\n<tr>\n<td width=\"20%\"><a href=\"sambanova_scribe/README.md\"> SambaNova Scribe</a></td>\n<td width=\"40%\">Example implementation of a transcription and summarization workflow.</td>\n<td width=\"20%\"> SambaNova Cloud</td>\n<td width=\"20%\"> Advanced AI Capabilities </td>\n</tr>\n\n<tr>\n<td width=\"20%\"><a href=\"google_integration/README.md\"> SambaCloud - Google Integration</a></td>\n<td width=\"40%\">App Scripts intended for those with SambaCloud API keys to integrate LLMs into Google Workspaces.</td>\n<td width=\"20%\"> SambaNova Cloud</td>\n<td width=\"20%\"> Advanced AI Capabilities </td>\n</tr>\n\n</tbody>\n</table>\n\n# Getting Started \n\nGo to [SambaNova Cloud Quickstart Guide](./quickstart/README.md) If is your first time using the AI State Kits and you want to try out simple examples. Follow the next steps to read more detailed instructions or if you ar a SambaStudio user. \n\n## Getting a SambaNova API key and setting your generative models\n\nCurrently, there are two ways to obtain an API key from SambaNova. You can get a free API key using SambaNova Cloud. Alternatively, if you are a current SambaNova customer, you can deploy your models using SambaStudio. \n\n### Use SambaNova Cloud (Option 1)\n\nFor more information and to obtain your API key, visit the [SambaNova Cloud webpage](https://cloud.sambanova.ai).\n\nTo integrate SambaNova Cloud LLMs with this AI starter kit, update the API information by configuring the environment variables in the `ai-starter-kit/.env` file:\n\n- Create the `.env` file at `ai-starter-kit/.env` if the file does not exist.\n- Enter the SambaNova Cloud API key in the `.env` file, for example:\n  \n```bash\nSAMBANOVA_API_KEY = \"456789abcdef0123456789abcdef0123\"\n```\n\n### Use SambaStudio (Option 2)\n\nBegin by deploying your LLM of choice (e.g., Llama 3 8B) to an endpoint for inference in SambaStudio. Use either the GUI or CLI, as described in the [SambaStudio endpoint documentation](https://docs.sambanova.ai/sambastudio/latest/endpoints.html).\n\nTo integrate your LLM deployed on SambaStudio with this AI starter kit, update the API information by configuring the environment variables in the `ai-starter-kit/.env` file:\n\n- Create the `.env` file at `ai-starter-kit/.env` if the file does not exist.\n- Set your SambaStudio variables. For example, an endpoint with the URL\n\"https://api-stage.sambanova.net/api/predict/nlp/12345678-9abc-def0-1234-56789abcdef0/456789ab-cdef-0123-4567-89abcdef0123\"\nis entered in the `.env` file as:\n\n``` bash\nSAMBASTUDIO_URL=\"https://api-stage.sambanova.net/api/predict/nlp/12345678-9abc-def0-1234-56789abcdef0/456789ab-cdef-0123-4567-89abcdef0123\"\nSAMBASTUDIO_API_KEY=\"89abcdef-0123-4567-89ab-cdef01234567\"\n```\n\n## Setting your embedding models\n\nCurrently, you can set your embedding models on CPU or SambaStudio. Note that embedding models are not available yet through SambaNova Cloud, but they will be in future releases.\n\n### Use CPU embedding (Option 1)\n\nYou can run the Hugging Face embedding models locally on CPU. In this case, no information is needed in the `.env` file.\n\n### Use SambaStudio embedding (Option 2)\n\nAlternatively, you can use SambaStudio embedding model endpoints instead of the CPU-based HugginFace embeddings to increase inference speed. Please follow [this guide](https://docs.sambanova.ai/sambastudio/latest/e5-large.html#_deploy_an_e5_large_v2_endpoint) to deploy your SambaStudio embedding model.\n\nTo integrate your embedding model deployed on SambaStudio with this AI starter kit, update the API information by configuring the environment variables in the `ai-starter-kit/.env` file:\n\n- Create the `.env` file at `ai-starter-kit/.env` if the file does not exist.\n- Set your SambaStudio variables. For example, an endpoint with the URL\n`\"https://api-stage.sambanova.net/api/predict/generic/12345678-9abc-def0-1234-56789abcdef0/456789ab-cdef-0123-4567-89abcdef0123\"`\nis entered in the `.env` file as:\n\n``` bash\nSAMBASTUDIO_URL=\"https://api-stage.sambanova.net/api/predict/nlp/12345678-9abc-def0-1234-56789abcdef0/456789ab-cdef-0123-4567-89abcdef0123\"\nSAMBASTUDIO_API_KEY=\"89abcdef-0123-4567-89ab-cdef01234567\"\n```\n\n## Run the desired starter kit\n\nGo to the `README.md` of the starter kit you want to use and follow the instructions. See [Available AI Starter Kits](#available-ai-starter-kits).\n\n## Additional information\n\n<details>\n<summary>Use Sambanova's LLMs and Langchain chat model wrappers</summary>\n\n### LLM Wrappers\n\nSet your environment as shown in [integrate your model](#integrate-your-model-in-the-starter-kit).\n\n#### Using Sambastudio LLMs\n\n1. Import the **SambaStudio** langchain integration in your project and define your **SambaStudio* ChatModel:\n\n``` bash\npip install langchain-sambanova\n```\n\n- If using a Bundle endpoint:\n\n```python\nfrom langchain_sambanova import ChatSambaStudio\n\nload_dotenv('.env')\n\nllm = ChatSambaStudio(\n  max_tokens_to_generate = 512,\n  temperature = 0.0,\n  model = \"Meta-Llama-3-8B-Instruct\"\n)\n```\n\n- If using a single model endpoint\n\n```python\nfrom langchain_sambanova import ChatSambaStudio\n\nload_dotenv('.env')\n\nllm = ChatSambaStudio(\n  max_tokens_to_generate = 512,\n  temperature = 0.0\n)\n```\n\n2. Use the model\n\n```python\nllm.invoke(\"your prompt\")\n```\n\nSee [utils/usage.ipynb](./utils/usage.ipynb) for an example.\n\n### Using SambaNova Cloud LLMs\n\n1. Import our **SambaNovaCloud** langchain integration in your project and define your **SambaNovaCloud* ChatModel:\n\n\n``` bash\npip install langchain-sambanova\n```\n\n```python\nfrom langchain_sambanova import ChatSambaNovaCloud\n\nload_dotenv('.env')\n\nllm = ChatSambaNovaCloud(model='Meta-Llama-3.3-70B-Instruct')\n```\n\n2. Use the model\n\n```python\nllm.invoke(\"your prompt\")\n```\n\nSee [utils/usage.ipynb](./utils/usage.ipynb) for an example.\n\n### Embedding Wrapper\n\n1. Import the **SambaStudioEmbedding** langchain integration in your project and define your **SambaStudioEmbedding*  embedding:\n\n``` bash\npip install langchain-sambanova\n```\n\n- If using a Bundle endpoint\n\n```python\nfrom langchain_sambanova import SambaStudioEmbeddings\n\nload_dotenv('.env')\n\nembedding = SambaStudioEmbeddings(\n              batch_size=1,\n              model_kwargs = {\n                  \"select_expert\":\"e5-mistral-7b-instruct\"\n                  }\n              )\n```\n\n- If using a single embedding model endpoint\n\n```python\nfrom langchain_sambanova import SambaStudioEmbeddings\n\nload_dotenv('.env')\n\nembedding = SambaStudioEmbeddings(batch_size=32)\n```\n\n> Note that using different embedding models (cpu or sambastudio) may change the results, and change the way they are set and their parameters\n\n2. Use your embedding model in your langchain pipeline\n\nSee [utils/usage.ipynb](./utils/usage.ipynb) for an example.\n\n### Javascript Example\n\n1. Before running the code, ensure that you have Node.js installed on your system. You can download the latest version from the official Node.js [website](https://nodejs.org/en).\n\n2. Set Up the Environment. To set up the environment, run the following commands in your terminal:\n\n``` bash\nnpm init -y\n```\n\n``` bash\nnpm install @langchain/openai @langchain/core\n```\n\n> These commands will create a new package.json file and install the required dependencies.\n\n3. Create a new file named `app.js` and add the following code:\n\n```javascript\nimport { ChatOpenAI } from \"@langchain/openai\";\n\nconst SambaNovaCloudBaseURL = \"https://api.sambanova.ai/v1\";\nconst apiKey = \"your-api-key\";\n\nconst SambaNovaCloudChatModel = new ChatOpenAI({\n  temperature: 0.9,\n  model: \"Meta-Llama-3.3-70B-Instruct\",\n  configuration: {\n    baseURL: SambaNovaCloudBaseURL,\n    apiKey: apiKey,\n  },\n});\n\nconst response = await SambaNovaCloudChatModel.invoke(\"Hi there, tell me a joke!\");\nconsole.log(response.content);\n```\n\n4. To run the app, execute the following command in your terminal:\n\n``` bash\nnode app.js\n```\n\n---\n\n</details>\n\n<details>\n<summary>Setting up your virtual environment</summary><br/>\n\nThere are two approaches to setting up your virtual environment for the AI Starter Kits:\n\n1. **Individual Kit Setup (Traditional Method)**\n2. **Base Environment Setup**\n\n### 1. Individual Kit Setup\n\nEach starter kit (see table [above](#available-ai-starter-kits)) has its own `README.md` and `requirements.txt` file. You can set up a separate virtual environment for each kit by following the instructions in their respective directories. This method is suitable if you're only interested in running a single kit or prefer isolated environments for each project.\n\nTo use this method:\n1. Navigate to the specific kit's directory\n2. Create a virtual environment\n3. Install the requirements\n4. Follow the kit-specific instructions\n\n### 2. Base Environment Setup \n\nFor users who plan to work with multiple kits or prefer a unified development environment, we recommend setting up a base environment. This approach uses a Makefile to automate the setup of a consistent Python environment that works across all kits.\n\nBenefits of the base environment approach:\n- Consistent Python version across all kits\n- Centralized dependency management\n- Simplified setup process\n- Easier switching between different kits\n\n#### Prerequisites\n\n- **pyenv**: The Makefile will attempt to install pyenv if it's not already installed.\n- **Docker**: (Optional) If you want to use the Docker-based setup, ensure Docker is installed on your system.\n\n#### What the Base Setup Does\n\n1. Installs pyenv and Poetry if they are not already installed.\n2. Sets up a Python virtual environment using a specified Python version (default is 3.11.3).\n3. Installs all necessary dependencies for the base environment.\n4. Sets up the parsing service required by some kits.\n5. Installs system dependencies like Tesseract OCR and Poppler.\n6. Provides Docker-based setup options for consistent environments across different systems.\n\n#### Setting Up the Base Environment\n\n1. **Install and Set Up the Base Environment:**\n\n```bash\nmake all\n```\nThis command will set up the base ai-starter-kit environment, including installing all necessary tools and dependencies.\n\n2. **Activate the Base Environment:**\n\n```bash\nsource .venv/bin/activate\n```\n\n3. **Navigate to Your Chosen Starter Kit:**\n```bash\ncd path/to/starter_kit\n```\nWithin the starter kit there will be instructions on how to start the kit. You can skip the virtual environment creation \npart in the kits README.md as we've done it here.\n\n\n### Parsing Service Management\nFor certain kits, we utilise a standard parsing service. By Default it's started automatically with the base environment. To work with this service in isolation, following the steps in this section.\n\n- **Start Parsing Service:**\n```bash\nmake start-parsing-service\n```\n\n- **Stop Parsing Service:**\n```bash\nmake stop-parsing-service\n```\n\n- **Check Parsing Service Status:**\n```bash\nmake parsing-status\n```\n\n- **View Parsing Service Logs:**\n```bash\nmake parsing-log\n```\n\n### Docker-based Setup\n\nTo use the Docker-based setup:\n\n1. Ensure Docker is installed on your system.\n2. Build the Docker image:\n\n```bash\nmake docker-build\n```\n\n3. Run a specific kit in the Docker container:\n```bash\nmake docker-run-kit KIT=<kit_name>\n```\nReplace `<kit_name>` with the name of the starter kit you want to run (e.g., `function_calling`).\n\n4. To open a shell in the Docker container:\n```bash\nmake docker-shell\n```\n\n### Cleanup\n\nTo clean up all virtual environments created by the makefile and stop parsing services run the following command:\n```bash\nmake clean\n```\nThis command removes all virtual environments created with the makefile, stops the parsing service, and cleans up any temporary files.\n</details>\n\n<details>\n<summary>Troubleshooting</summary><br/>\n\nIf you encounter issues while setting up or running the AI Starter Kit, here are some common problems and their solutions:\n\n### Python version issues\n\nIf you're having problems with Python versions:\n\n1. Ensure you have pyenv installed: `make ensure-pyenv`\n2. Install the required Python versions: `make install-python-versions`\n3. If issues persist, check your system's Python installation and PATH settings.\n\n### Dependency conflicts\n\nIf you're experiencing dependency conflicts:\n\n1. Try cleaning your environment: `make clean`\n2. Update the lock file: `poetry lock --no-update`\n3. Reinstall dependencies: `make install`\n\n### pikepdf installation issues\n\nIf you encounter an error while installing `pikepdf`, such as:\n\n```\nERROR: Failed building wheel for pikepdf\nFailed to build pikepdf\n```\n\nThis is likely due to missing `qpdf` dependency. The Makefile should automatically install `qpdf` for you, but if you're still encountering issues:\n\n1. Ensure you have proper permissions to install system packages.\n2. If you're on macOS, you can manually install `qpdf` using Homebrew:\n   ```bash\n   brew install qpdf\n   ```\n3. On Linux, you can install it using your package manager, e.g., on Ubuntu:\n   ```\n   sudo apt-get update && sudo apt-get install -y qpdf\n   ```\n4. After installing `qpdf`, try running `make install` again.\n\nIf you continue to face issues, please ensure your system meets all the requirements for building `pikepdf` and consider checking the [pikepdf documentation](https://pikepdf.readthedocs.io/en/latest/installation.html) for more detailed installation instructions.\n\n### Parsing service issues\n\nIf the parsing service isn't starting or is behaving unexpectedly:\n\n1. Check its status: `make parsing-status`\n2. View its logs: `make parsing-log`\n3. Try stopping and restarting it: `make stop-parsing-service` followed by `make start-parsing-service`\n\n### System Dependencies Issues\n\nIf you encounter issues related to Tesseract OCR or Poppler:\n\n1. Ensure the Makefile has successfully installed these dependencies.\n2. On macOS, you can manually install them using Homebrew:\n ```bash\n   brew install tesseract poppler\n   ```\n3. On Linux (Ubuntu/Debian), you can install them manually:\n ```bash\n   sudo apt-get update && sudo apt-get install -y tesseract-ocr poppler-utils\n   ```\n4. On Windows, you may need to install these dependencies manually and ensure they are in your system PATH.\n\n### Docker-related Issues\n\nIf you're using the Docker-based setup and encounter issues:\n\n1. Ensure Docker is properly installed and running on your system.\n2. Try rebuilding the Docker image: `make docker-build`\n3. Check Docker logs for any error messages.\n4. Ensure your firewall or antivirus is not blocking Docker operations.\n\n### General troubleshooting steps\n\n1. Ensure all prerequisites (Python, pyenv, Poetry) are correctly installed.\n2. Try cleaning and rebuilding the environment: `make clean all`\n3. Check for any error messages in the console output and address them specifically.\n4. Ensure your `.env` file is correctly set up in the ai-starter-kit root with all necessary environment variables.\n\nIf you continue to experience issues, please [open an issue](https://github.com/sambanova/ai-starter-kit/issues/new) with details about your environment, the full error message, and steps to reproduce the problem.\n\n### Important Notes for Users\n\n- Ensure you have sufficient permissions to install software on your system.\n- The setup process may take several minutes, especially when installing Python versions or large dependencies.\n- If you encounter any issues during setup, check the error messages and ensure your system meets all prerequisites.\n- Always activate the base environment before navigating to and running a specific starter kit.\n- Some kits may require additional setup steps. Always refer to the specific README of the kit you're using.\n</details>\n\n### API Reference\n\n- Find more information about SambaNova Cloud [here](https://docs.sambanova.ai/cloud/docs/get-started/overview)\n\n- Find more information about SambaStudio [here](https://docs.sambanova.ai/sambastudio/latest/index.html)\n\n**Note:** These AI Starter Kit code samples are provided \"as-is,\" and are not production-ready or supported code. Bugfix/support will be on a best-effort basis only. Code may use third-party open-source software. You are responsible for performing due diligence per your organization policies for use in your applications.\n\n"
    },
    {
      "name": "Azure-Samples/aks-store-demo",
      "stars": 215,
      "img": "https://avatars.githubusercontent.com/u/1844662?s=40&v=4",
      "owner": "Azure-Samples",
      "repo_name": "aks-store-demo",
      "description": "Sample microservices app for AKS demos, tutorials, and experiments",
      "homepage": "",
      "language": "Bicep",
      "created_at": "2023-06-02T16:49:30Z",
      "updated_at": "2025-04-22T15:23:20Z",
      "topics": [],
      "readme": "---\npage_type: sample\nlanguages:\n- azdeveloper\n- go\n- javascript\n- rust\n- nodejs\n- python\n- bicep\n- terraform\n- dockerfile\nproducts:\n- azure\n- azure-kubernetes-service\n- azure-openai\n- azure-cosmos-db\n- azure-container-registry\n- azure-service-bus\n- azure-monitor\n- azure-log-analytics\n- azure-managed-grafana\n- azure-key-vault\nurlFragment: aks-store-demo\nname: AKS Store Demo\ndescription: This sample demo app consists of a group of containerized microservices that can be easily deployed into an Azure Kubernetes Service (AKS) cluster. \n---\n<!-- YAML front-matter schema: https://review.learn.microsoft.com/en-us/help/contribute/samples/process/onboarding?branch=main#supported-metadata-fields-for-readmemd -->\n\n# AKS Store Demo\n\nThis sample demo app consists of a group of containerized microservices that can be easily deployed into an Azure Kubernetes Service (AKS) cluster. This is meant to show a realistic scenario using a polyglot architecture, event-driven design, and common open source back-end services (eg - RabbitMQ, MongoDB). The application also leverages OpenAI's GPT-3 models to generate product descriptions. This can be done using either [Azure OpenAI](https://learn.microsoft.com/azure/ai-services/openai/overview) or [OpenAI](https://openai.com/).\n\nThis application is inspired by another demo app called [Red Dog](https://github.com/Azure/reddog-code).\n\n> [!NOTE]\n> This is not meant to be an example of perfect code to be used in production, but more about showing a realistic application running in AKS. \n\n<!-- \nTo walk through a quick deployment of this application, see the [AKS Quickstart](https://learn.microsoft.com/azure/aks/learn/quick-kubernetes-deploy-cli).\n\nTo walk through a complete experience where this code is packaged into container images, uploaded to Azure Container Registry, and then run in and AKS cluster, see the [AKS Tutorials](https://learn.microsoft.com/azure/aks/tutorial-kubernetes-prepare-app).\n\n -->\n\n## Architecture\n\nThe application has the following services: \n\n| Service | Description |\n| --- | --- |\n| `makeline-service` | This service handles processing orders from the queue and completing them (Golang) |\n| `order-service` | This service is used for placing orders (Javascript) |\n| `product-service` | This service is used to perform CRUD operations on products (Rust) |\n| `store-front` | Web app for customers to place orders (Vue.js) |\n| `store-admin` | Web app used by store employees to view orders in queue and manage products (Vue.js) | \n| `virtual-customer` | Simulates order creation on a scheduled basis (Rust) |\n| `virtual-worker` | Simulates order completion on a scheduled basis (Rust) |\n| `ai-service` | Optional service for adding generative text and graphics creation (Python) |\n| `mongodb` | MongoDB instance for persisted data |\n| `rabbitmq` | RabbitMQ for an order queue |\n\n![Logical Application Architecture Diagram](assets/demo-arch-with-openai.png)\n\n## Run the app on Azure Kubernetes Service (AKS)\n\nTo learn how to deploy this app on AKS, see [Quickstart: Deploy an Azure Kubernetes Service (AKS) cluster using Azure CLI](https://learn.microsoft.com/azure/aks/learn/quick-kubernetes-deploy-cli).\n\n> [!NOTE]\n> The above article shows a simplified version of the store app with some services removed. For the full application, you can use the `aks-store-all-in-one.yaml` file in this repo.\n\n## Run on any Kubernetes\n\nThis application uses public images stored in GitHub Container Registry and Microsoft Container Registry (MCR). Once your Kubernetes cluster of choice is setup, you can deploy the full app with the below commands.\n\nThis deployment deploys everything except the `ai-service` that integrates OpenAI. If you want to try integrating the OpenAI component, take a look at this article: [Deploy an application that uses OpenAI on Azure Kubernetes Service (AKS)](https://learn.microsoft.com/azure/aks/open-ai-quickstart?tabs=aoai).\n\n```bash\nkubectl create ns pets\n\nkubectl apply -f https://raw.githubusercontent.com/Azure-Samples/aks-store-demo/main/aks-store-all-in-one.yaml -n pets\n```\n\n## Run the app locally\n\nThe application is designed to be [run in an AKS cluster](#run-the-app-on-aks), but can also be run locally using Docker Compose.\n\n> [!TIP]\n> You must have [Docker Desktop](https://www.docker.com/products/docker-desktop) installed to run this app locally. If you do not have it installed locally, you can try opening this repo in a [GitHub Codespace instead](#run-the-app-with-github-codespaces)\n\nTo run this app locally:\n\nClone the repo to your development computer and navigate to the directory:\n\n```console\ngit clone https://github.com/Azure-Samples/aks-store-demo.git\ncd aks-store-demo\n```\n\nConfigure your Azure OpenAI or OpenAI API keys in [`docker-compose.yml`](./docker-compose.yml) using the environment variables in the `ai-service` section:\n\n```yaml\n  ai-service:\n    build: src/ai-service\n    container_name: 'ai-service'\n    ...\n    environment:\n      - USE_AZURE_OPENAI=True # set to False if you are not using Azure OpenAI\n      - AZURE_OPENAI_DEPLOYMENT_NAME= # required if using Azure OpenAI\n      - AZURE_OPENAI_ENDPOINT= # required if using Azure OpenAI\n      - OPENAI_API_KEY= # always required\n      - OPENAI_ORG_ID= # required if using OpenAI\n    ...\n```\n\nAlternatively, if you do not have access to Azure OpenAI or OpenAI API keys, you can run the app without the `ai-service` by commenting out the `ai-service` section in [`docker-compose.yml`](./docker-compose.yml). For example:\n\n```yaml\n#  ai-service:\n#    build: src/ai-service\n#    container_name: 'ai-service'\n...\n#    networks:\n#      - backend_services\n```\n\nStart the app using `docker compose`. For example:\n\n```bash\ndocker compose up\n```\n\nTo stop the app, you can hit the `CTRL+C` key combination in the terminal window where the app is running.\n\n## Run the app with GitHub Codespaces\n\nThis repo also includes [DevContainer configuration](./.devcontainer/devcontainer.json), so you can open the repo using [GitHub Codespaces](https://docs.github.com/en/codespaces/overview). This will allow you to run the app in a container in the cloud, without having to install Docker on your local machine. When the Codespace is created, you can run the app using the same instructions as above.\n\n[![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://github.com/codespaces/new?hide_repo_select=true&ref=main&repo=648726487)\n\n## Deploy the app to Azure using Azure Developer CLI\n\nSee the [Azure Developer CLI](./docs/azd.md) documentation for instructions on how to quickly deploy the app to Azure.\n\n## Additional Resources\n\n- AKS Documentation. https://learn.microsoft.com/azure/aks\n- Kubernetes Learning Path. https://azure.microsoft.com/resources/kubernetes-learning-path \n"
    },
    {
      "name": "Azure/intro-to-intelligent-apps",
      "stars": 199,
      "img": "https://avatars.githubusercontent.com/u/6844498?s=40&v=4",
      "owner": "Azure",
      "repo_name": "intro-to-intelligent-apps",
      "description": "This repository introduces and helps organizations get started with building Intelligent Apps and incorporating Large Language Models (LLMs) via AI Orchestration into them.",
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2023-07-15T05:04:09Z",
      "updated_at": "2025-04-23T10:55:51Z",
      "topics": [],
      "readme": "# Introduction to Building AI Apps\n\nThis repository introduces and helps organizations get started with building AI Apps and incorporating Large Language Models (LLMs) into them.\n\n## Workshop Agenda\n\nThe objective of this workshop is to practice realistic AI orchestration scenarios and to learn how to build intelligent apps.\nAt the end of the workshop you will: \n* Know how to use prompt engineering techniques for effective generative AI responses on OpenAI\n* Understand the implications of the usage of tokens and embeddings when interacting with an LLM\n* Have experience in leveraging AI orchestrators like Langchain/ Semantic Kernel with Azure OpenAI\n* Have evaluated different vector stores like Qdrant or Azure AI Search to enhance LLM responses with your data and context\n* Know how to turn a business scenario with data, context and user input into an intelligent application on Azure\n\n### 🌅 Morning (9:00 – 12:15)\n\n> *Focus: Introduction, First Steps & Prompt Engineering*\n\n* 📣 Intro (30min)\n  * Introductions & Setting Expectations\n  * Use Case Ideation & Brainstorming\n* 📣 [Intro to Azure OpenAI, Prompt Engineering & Demos (105min)](presentations/README.md)\n  * Azure OpenAI Service\n  * Demo(s)\n  * Break\n  * 🧑🏼‍💻 [Lab #1 - Hands-on with Prompt Engineering Exercises](labs/01-prompts/README.md)\n* 📣 [Intro to AI Orchestration (60min)](presentations/README.md)\n  * AI Orchestration\n  * Demo(s)\n\n### 🌆 Afternoon (1:15 – 4:30)\n\n> *Focus: Building AI Apps & Incorporating LLMs*\n\n* 📣 [Intro to AI Orchestration Continued (135min)](presentations/README.md)\n  * 💻 [Lab #2 - Hands-on with Integrating AI Exercises](labs/02-integrating-ai/README.md)\n  * 💻 [Lab #3 - Hands-on with AI Orchestration Exercises](labs/03-orchestration/README.md)\n  * 💻 [Lab #4 - Hands-on with Deploying AI Exercises](labs/04-deploy-ai/README.md)\n  * Break\n* Wrapping-up (60min)\n  * Use Case Validation\n  * QnA & Closing Remarks\n\n\n## Getting Started with Workshop Preparation\n\nThe steps in this section will take you through setting up Azure OpenAI and some configuration files so that you can complete all of the hands-on labs successfully.\n\n* [Preparation](labs/00-setup/README.md)\n\n## Post Workshop Next Steps\n\nWhen you're done with this workshop and ready to move on, the following may be useful.\n\n* [Next Steps](docs/next_steps.md)\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n"
    },
    {
      "name": "happyapplehorse/gptui",
      "stars": 142,
      "img": "https://avatars.githubusercontent.com/u/80760121?s=40&v=4",
      "owner": "happyapplehorse",
      "repo_name": "gptui",
      "description": "GPTUI is a GPT conversational TUI (Textual User Interface) tool that runs within the terminal. It has some distinctive features, such as multi-AI group chat, AI-initiated care, and other functionalities.",
      "homepage": "https://happyapplehorse.github.io/gptui/",
      "language": "Python",
      "created_at": "2023-10-04T16:03:55Z",
      "updated_at": "2025-04-16T18:10:34Z",
      "topics": [
        "ai",
        "ai-group-chat",
        "cli",
        "gpt",
        "openai",
        "python",
        "terminal",
        "tui"
      ],
      "readme": "# GPTUI\n![GitHub](https://img.shields.io/github/license/happyapplehorse/gptui)\n![PyPI - Version](https://img.shields.io/pypi/v/gptui)\n[![GitHub Workflow Status (with event)](https://img.shields.io/github/actions/workflow/status/happyapplehorse/gptui/static.yml?label=docs)](https://happyapplehorse.github.io/gptui/)\n![GitHub Workflow Status (with event)](https://img.shields.io/github/actions/workflow/status/happyapplehorse/gptui/python-publish.yml?label=build)\n[![Static Badge](https://img.shields.io/badge/bilibili-twototoo222-pink)](https://space.bilibili.com/80170263)\n\n[English readme](README.md) • [简体中文 readme](README.zh.md)\n\n<img src=\"https://github.com/happyapplehorse/happyapplehorse-assets/blob/main/imgs/gptui_logo.png\" alt=\"gptui_logo\" align=\"left\" width=\"70px\" height=\"70px\"/>\nGPTUI is a GPT conversational TUI (Textual User Interface) tool that runs within the terminal.\nUsing the Textual framework for its TUI interface and equipping the plugin framework provided by Semantic Kernel.\nGPTUI offers a lightweight <a href=\"#gptui_kernel\">Kernel</a> to power AI applications.\nThe top-level TUI application is decoupled from the underlying Kernel, allowing you to easily replace the TUI interface or expand its functionalities.\nAt present, only the GPT model of OpenAI is supported, and other LLM interfaces will be added later.\n\n&nbsp;\n![gptui_demo](https://github.com/happyapplehorse/happyapplehorse-assets/blob/main/imgs/gptui_demo.gif)\n\n## TUI Features\n- Create and manage conversations with GPT.\n- Display context tokens in real-time.\n- View and adjust GPT conversation parameters at any time, such as temperature, top_p, presence_penalty, etc.\n- A dedicated channel to display internal process calls.\n- Offers a file channel through which you can upload to or download from GPT.\n- Voice functionality.\n- Group talk functionality[^recommend_better_model][^token_cost].\n- AI-Care. Your AI can propactively care for you[^ai_care].\n- Optional built-in plugins (continuously evolving):\n  - Internet search[^google_key].\n  - Open interpreter[^open_interpreter][^token_cost][^recommend_better_model]. (Temporarily removed, waiting to be added back after it supports openai v1.x.)\n  - Reminders[^recommend_better_model].\n  - Recollecting memories from vectorized conversation history.\n- Support custom plugins.\n\n![gptui_img](https://github.com/happyapplehorse/happyapplehorse-assets/blob/main/imgs/gptui_img.jpg)\n\n[^open_interpreter]: This plugin utilizes [open-interpreter](https://github.com/KillianLucas/open-interpreter), you need to\nfirst follow the instructions provided by open-interpreter to properly set up the environment and API.\nThe open-interpreter has the permission to execute code, please ensure that you are already aware of the associated risk before\nenabling this feature.\n[^recommend_better_model]: It is recommended to use this under the GPT-4 model or a better one.\n[^token_cost]: Note: This feature may incur a significant token cost.\n[^ai_care]: Powered by [AI-Care](https://github.com/happyapplehorse/ai-care).\n[^google_key]: `GOOGLE_KEY` and `GOOGLE_CX` are required. Obtained free from [here](https://developers.google.com/custom-search/v1/introduction).\n\n## 🎬 Demo Videos\n- [AI-Care](https://www.youtube.com/watch?v=gPQ7XV-Q1r8)\n\n# Compatibility\nGPTUI runs in a command line environment and is compatible with Linux, macOS, Windows and Android[^compatibility].\nUsing the functionality provided by textual-web, you can also run GPTUI in the browser and share it with remote friends👍.\n\n[^compatibility]: I haven't tested it on the Windows platform yet, and some functionalities like code copying,\nvoice features, etc., still need drivers to be written. I will complete these features later.\nWhen running on Android, please use the [Termux](https://github.com/termux/termux-app) terminal tool.\nFor additional features like code copying and voice functionalities,\nyou need to install [Termux-API](https://github.com/termux/termux-api) and grant the necessary permissions.\n\n<a name=\"gptui_kernel\"> </a>\n## ⚙️ GPTUI Kernel\n\nGPTUI offers a lightweight Kernel for building AI applications, allowing you to easily expand GPTUI's capabilities or construct your own AI application.\n\n<p align=\"center\"><img src=\"https://github.com/happyapplehorse/happyapplehorse-assets/blob/main/imgs/gptui_framework.png\" alt=\"gptui-framework\" width=\"700\"/></p >\n\nThe **kernel** relies on **jobs** and **handlers** to perform specific functions.\nTo achieve new functionalities, all you need to do is write or combine your own **jobs** and **handlers**.\nThe **manager** and **kernel** of GPTUI are entirely independent of the **client** application, enabling you to effortlessly relocate the **manager** or **kernel** for use elsewhere.\nThe application layer of GPTUI (**client**) employs the CVM architecture, where the model layer provides foundational, reusable modules for interaction with LLM, independent of specific views and controllers implementations.\nIf you wish to build your own AI application, you can start here, fully utilizing the **kernel**, **manager**, and models.\nTo alter or expand UI functionalities, typically, only modifications to the controllers and views are needed.\n\nSee Development Documentation for details. [Documentation](#documentation).\n\nIf you need to use certain functions or components of gptui, you can import and use gptui as a library.\n\n# Installation\n\nNormal use requires ensuring stable network connection to OpenAI.\nIf you encounter any issues, please refer to [troubleshooting](docs/troubleshooting.md).\n\n## Install with pip\n\n```\npip install gptui\n```\n\n[Config your API keys](#config-api-keys) before running.\n\nTo run：\n```\ngptui\n```\nSpecify config file：\n```\ngptui --config <your_config_file_path>\n```\nThis program loads files through the following steps:\n1. Read the configuration file from --config. If not specified, proceed to the next step.\n2. Search for ~/.gptui/.config.yml in the user directory. If not found, move to the next step.\n3. Copy the default configuration file gptui/config.yml to ~/.gitui/.config.yml and use it.\n\n## Install from source\n\n```\ngit clone https://github.com/happyapplehorse/gptui.git\ncd gptui\npip install .\n```\nAPI configuration is required before running.\n\nTo run:\n\n```bash\ngptui\n# Or you can also use\n# python -m gptui\n```\n\nYou can also directly run the startup script (this allows you to modify the source code and run it immediately):\nFirst, install the dependencies:\n\n```\npip install -r requirements.txt\n```\nThen, run the startup script:\n```\npython main.py\n```\n\nWhen running the program with `python main.py` or `python -m gptui`, use `gptui/config.yml` as the configuration file.\n\nOn Linux or macOS systems, if you want to use voice functionalities, you'll need to install pyaudio separately.\n\n# Configuration\n\n## Config API keys\nConfigure the corresponding API Keys in `~/.gptui/.env_gptui`.\nRefer to the [.env_gptui.example](https://github.com/happyapplehorse/gptui/blob/main/.env_gptui.example) file.\nWhen using the \"WebServe\" plugin, `GOOGLE_KEY` and `GOOGLE_CX` need to be provided, which can be [obtained](https://developers.google.com/custom-search/v1/introduction) free of charge from Google.\n\n## Config File\nSee `./config.yml` for a config file example that lists all configurable options.\nDepending on the platform you are using, it is best to configure the following options:\n\n- os: system platform\n\nOtherwise, some features may not work properly, such as code copy and voice related functions.\n\n## Configuration Guide\nFor detailed configuration instructions, please refer to [here](./docs/configuration.md).\n\n# Quick Start\n\n## Interface Layout\n\n![gptui-layout](https://github.com/happyapplehorse/happyapplehorse-assets/blob/main/imgs/gptui_layout.jpg)\n\n- **chat area**: Display area for chat content.\n- **status area**： Program status display area, displaying response animations and notifications.\n- **input area**: Chat content input area.\n- **auxiliary area**: Auxiliary information area, displaying \"internal communication\" between the program and the LLM, including function call information, etc.\n- **control area**: The program's control area, where you can view and set the state of the program, such as change OpenAI chat parameters.\n- **chat tabs**: Conversation Tab Bar.\n- **conversation control**: Conversation control buttons. From top to bottom they are:\n  - `+`: **_New conversation_**\n  - `>`: **_Save conversation_**\n  - `<`: **_Load conversation_**\n  - `-`: **_Delete conversation_**\n  - `x`: **_Delete conversation file_**\n  - `n`: **_Disposable conversation_**\n  - `↥`: **_Upload file_**\n- **panel selector**: Panel selection area. From top to bottom they are：\n  - `C`: **_Conversation file records_**\n  - `D`: **_System file tree_**\n  - `A`: **_Auxiliary information panel_**\n  - `T`: **_File pipeline panel_**\n  - `P`: **_Plugin selection panel_**\n- **switches**：Direct control switches. From left to right they are：\n  - `R`: **_Program state auto save and restore switch_**\n  - `V`: **_Voice switch_**\n  - `S`: **_Read reply by voice_**\n  - `F`: **_Fold files in chat_**\n  - `|Exit|`: **_Exit program_**\n- **dashboard**：Context window size for chat.\n- **others**:\n  - `<`: **_Previous chat_**\n  - `>`: **_Next chat_**\n  - `1`: **_Number of chats_**\n  - `☌`: **_[Running status](#Running status)_**\n  - `↣`: **_Fold right non-chat area_**\n  - `?`: **_Help documentation_**\n\n## Running status\n\n<span style=\"color:green\">☌</span>: Ready.  \n<span style=\"color:red\">☍</span>: Task running.\n\n## Dynamic Commands\n\nSwitch to `S` in the control area, enter the command and press enter. Currently supports the following commands:\n- Set chat parameters\nCommand: **set_chat_parameters()**  \nParameters: OpenAI chat parameters in dictionary form, refer to [OpenAI Chat](https://platform.openai.com/docs/api-reference/chat/create).  \nExample: `set_chat_parameters({\"model\": \"gpt-4\", \"stream\": True})`\n- Set max sending tokens ratio\nCommand: **set_max_sending_tokens_ratio()**  \nParameters: The ratio of the number of sent tokens to the total token window, in float form. The remaining token count is used as the limit for the number of tokens GPT returns.  \nExample: `set_max_sending_tokens_ratio(0.5)`\n\n## Hotkeys\n\nGPTUI provides hotkeys for commonly used features, see [Help](https://github.com/happyapplehorse/gptui/blob/main/docs/help.md).\nIn addition, you can also press `ESC`, `ctrl+[`, or `ctrl+/` to bring up the hotkey menu (this mode offers more comprehensive hotkey functionalities, but they are not exactly the same as the direct hotkeys.).\n\n# Documentation\n\nFor detailed usage and development instructions, see [here](https://happyapplehorse.github.io/gptui/), for in-program help documentation see [here](src/gptui/help.md).\n\n# Contribution\n\nSome of GPTUI's plugin features rely on prompt, you can continue to help me improve these prompt.\nAnd I'd like to have appropriate animation cues during certain state changes.\nIf you have any creative ideas, I'd appreciate your help in implementing them.\nP.S.: Each contributor can leave a quote in the program.\n\n# Note\nThis project utilizes OpenAI's Text-to-Speech (TTS) services for generating voice outputs.\nPlease be aware that the voices you hear are not produced by human speakers, but are synthesized by AI technology.\n\n# License\n\nGPTUI is built upon a multitude of outstanding open-source components and adheres to the [MIT License](https://github.com/happyapplehorse/gptui/blob/main/LICENSE) open-source agreement.\nYou are free to use it.\n"
    },
    {
      "name": "chtrembl/azure-cloud",
      "stars": 134,
      "img": "https://avatars.githubusercontent.com/u/62187826?s=40&v=4",
      "owner": "chtrembl",
      "repo_name": "azure-cloud",
      "description": "Here you will find various Azure Demos & Tutorials that I've put together for Azure Cloud using DevOps, Container Services and other PaaS offerings.",
      "homepage": "https://azurepetstore.com",
      "language": "Java",
      "created_at": "2021-04-22T13:24:14Z",
      "updated_at": "2025-04-06T18:26:04Z",
      "topics": [
        "apim",
        "app",
        "application-insights",
        "application-security",
        "azure",
        "b2c",
        "containers",
        "devops",
        "java",
        "kubernetes",
        "spring"
      ],
      "readme": "# Here you will find various [Azure](https://ms.portal.azure.com/) demos & learning guides that are meant to help you build awesome stuff on Azure.\n\n## [View Azure Pet Store Learning Guides](https://github.com/chtrembl/azure-cloud/tree/main/petstore)\n\nExtensive Azure learning guides for a hypothetical Azure Pet Store walking you thorugh many Azure services and development scenarios using pre built web applications and microdervices.\n\n> You can view the live deployment here https://azurepetstore.com\n"
    },
    {
      "name": "alibaba/app-controller",
      "stars": 132,
      "img": "https://avatars.githubusercontent.com/u/1961952?s=40&v=4",
      "owner": "alibaba",
      "repo_name": "app-controller",
      "description": "App-Controller: Allow users to manipulate your App with natural language",
      "homepage": "https://alibaba.github.io/app-controller/",
      "language": "Python",
      "created_at": "2024-07-16T04:13:11Z",
      "updated_at": "2025-04-23T06:08:25Z",
      "topics": [
        "agent",
        "ai",
        "ai-agents",
        "ai-tools",
        "api",
        "app",
        "application-control",
        "foundation-agent",
        "generative-ai",
        "large-language-models",
        "llm",
        "tools"
      ],
      "readme": "English | [中文](./README_ZH.md)\n\n<h3 align=\"center\"><img src=\"Assets/logo.png\" height=\"200\"><br>App-Controller: Allow users to manipulate your App with natural\nlanguage</h3>\n\n\n<div align=\"center\">\n\n![](https://img.shields.io/badge/python-3.9+-blue)\n[![](https://img.shields.io/badge/Docs-English%7C%E4%B8%AD%E6%96%87-blue?logo=markdown)](https://alibaba.github.io/app-controller/en/index.html)\n![](https://img.shields.io/badge/license-Apache--2.0-black)\n![](https://img.shields.io/badge/Contribute-Welcome-green)\n\n[//]: # ([![]&#40;https://img.shields.io/github/stars/gencay/vscode-chatgpt?color=blue&label=Github%20Stars&#41;]&#40;&#41;)\n\n</div>\n\n## 新闻\n\n- <img src=\"https://img.alicdn.com/imgextra/i3/O1CN01SFL0Gu26nrQBFKXFR_!!6000000007707-2-tps-500-500.png\" alt=\"new\" width=\"30\" height=\"30\"/>**[2024-11-08]**\n  Based on **App-Controller**, we have developed a Visual Studio Code plugin, [SmartVscode](https://github.com/alibaba/smart-vscode-extension), which allows users to manipulate\n  various VS Code features through natural language commands, such as changing the theme or generating code with a single\n  sentence. \n- <img src=\"https://img.alicdn.com/imgextra/i3/O1CN01SFL0Gu26nrQBFKXFR_!!6000000007707-2-tps-500-500.png\" alt=\"new\" width=\"30\" height=\"30\"/>**[2024-11-08]**\n  We are thrilled to announce the release of **App-Controller** version 1.0!\n\n---\n\n## What is App-Controller?\n\nApp-Controller is an innovative API orchestration framework built upon Large Language Models (LLMs) and Agents.\nIt aims to integrate and synchronize APIs provided by any applications (APPs) using the advanced reasoning capabilities of LLMs.\n\n- App-Controller allows applications to respond and execute instructions formulated in natural language, significantly simplifying\n  user interaction with applications.\n\n<img src=\"Assets/mode.png\" width=1000>\n\n> The above image illustrates how App-Controller enhances application interactivity.\n> Specifically, the graphic is divided into two parts: the left-half details the traditional process of inquiry and execution when\n> completing tasks, while the right-half displays the streamlined workflow with App-Controller's intervention.\n> In a traditional scenario, when a user needs to accomplish a task in an App but doesn’t know how to do it, they first ask the\n> LLM and receive an answer, then command the App to obtain the result and complete the task.\n> In contrast, after introducing App-Controller, the user simply inputs their requirement in natural language directly into the App to\n> get the result and finish the task. With App-Controller's assistance, the App consults the LLM for user intent and learns the\n> necessary commands to execute, subsequently returning the result.\n\n- Any application vendor only needs to implement communication interfaces on the App and submit a list of supported APIs to\n  App-Controller, which can independently explore and identify the optimal API call sequence to fulfill user instructions.\n\nApp-Controller 's core competency lies in its highly automated API orchestration logic and user-friendly data interaction patterns,\nmaking it easier and faster for developers to add intelligent features to their apps.\nIt also comes with a flexible HTTP interface that enhances the way applications work together.\nIn the end, App-Controller aims to provide a straightforward and efficient way for users and developers to interact with apps,\nenabling a seamless experience that meets diverse needs.\n\n🔥 **Enhanced Usability**: Allows users to control your application via simple natural language commands, eliminating the need to learn complex interfaces or command sets and making the services or content readily accessible.\n\n🛠️ **Easy Integration**: Developers only need to register their application's API directory, and App-Controller will automatically manage the identification and orchestration, negating the need for intricate coding.\n\n🚀 **Asynchronous and Concurrent Processing**: App-Controller enhances its support for concurrent requests using modern asynchronous technology, ensuring efficiency and quick responses even under high-load conditions with multiple users or tasks.\n\n🌐 **Robust API Interactions**: App-Controller offers a stable and user-friendly HTTP API interface, enabling seamless interactions with applications while ensuring high-efficiency and security in data transfer.\n\n🤖 **Multitude of Large Language Models**: App-Controller integrates well with various Large Language Models, allowing developers to choose the most suitable model based on their needs and contexts for optimal understanding and natural language processing.\n\n📚 **Comprehensive Documentation**: App-Controller provides extensive documentation, including quick start guides, API references, best practice examples, and FAQs, to help developers get started and fully utilize the framework.\n\n💾 **Persistent Task Flows**: Task workflows can be stored persistently in databases, facilitating the monitoring and management of tasks and allowing developers to check the status and history at any time.\n\n🛢️ **Smart Caching Mechanism**: With advanced caching technology, App-Controller optimizes performance and response times by intelligently storing frequently requested results, reducing the number of calls to external models (coming soon).\n\n🌟 **Token Optimization**: App-Controller's optimization algorithm intelligently assesses message utility, reducing token usage and cutting down on costs associated with API calls (coming soon).\n\n## Applications: The SmartVscode Plugin Based on App-Controller\n\nWe developed a Visual Studio Code plugin, [SmartVscode](https://github.com/alibaba/smart-vscode-extension), that allows users to operate various VS Code features through natural language. Below are some demonstrations of its features:\n\n### Tic-tac-toe Game\nhttps://github.com/user-attachments/assets/82d84f26-de49-4eae-9e31-e838c398343a\n\n### A comprehensive pipeline in Python including configuration, code, and execution.\nhttps://github.com/user-attachments/assets/9ab9f14c-ff94-4dc5-af67-a358bd03e0f6\n\n### Style Changing\nhttps://github.com/user-attachments/assets/d29155d5-58c2-488b-8477-f95cd285533a\n\n### Theme Changing\nhttps://github.com/user-attachments/assets/94636935-3789-496f-a6be-1baa0d18f63c\n\n## Connect to your Application\n\nThe following image illustrates the process of introducing intelligence into applications using the App-Controller framework,\ndetailing the tasks that application developers need to undertake and the process by which App-Controller independently orchestrates\nAPI calls to fulfill user instructions.\n\n<img src=\"Assets/developer.png\" width=1000>\n\n#### Preparation Stage:\n\n1. **Communication interface**: Application developers need to achieve a **standard communication interface** with the App-Controller.\n2. **Document**: They also need to provide soe **knowledge** to the App-Controller, including the App's available API documentation\n   and other optional documents.\n\n#### Deploy stage\n\nAfter starting App-Controller, the App forwards user input to the App-Controller. The App-Controller integrates user input and available API\ninformation, interacts with the LLM to select the appropriate API to execute, and determine the task status. Iteratively, the App\nexecutes the selected API and returns the execution result to the App-Controller. The App-Controller continues to interact with the LLM to\nmake the next decision. The pipeline is terminated when the task has been completed, or failed, and the result is returned to the\nuser.\n\nAfter completing these steps, the App can achieve intelligent interaction with users.\n\n## Documentation\n\n[Documentation](https://alibaba.github.io/app-controller/en/index.html)\nprovides comprehensive information on how to integrate App-Controller into your application. You can refer to these documents for an improved experience with App-Controller.\n\n## License\nApp-Controller is released under the Apache License 2.0.\n\n## Contributions\nJoin us in building App-Controller!\nPlease see our [Contribution Guide](https://alibaba.github.io/app-controller/en/tutorial/contribute.html) for more detail\n"
    },
    {
      "name": "Azure-Samples/python-ai-agent-frameworks-demos",
      "stars": 125,
      "img": "https://avatars.githubusercontent.com/u/1844662?s=40&v=4",
      "owner": "Azure-Samples",
      "repo_name": "python-ai-agent-frameworks-demos",
      "description": "A repository of examples using Python AI Agent frameworks that work with GitHub Models and Azure OpenAI.",
      "homepage": null,
      "language": "Bicep",
      "created_at": "2025-04-08T14:15:26Z",
      "updated_at": "2025-04-23T05:53:31Z",
      "topics": [],
      "readme": "<!--\n---\nname: Python AI Agent Frameworks Demos\ndescription: Collection of Python examples for popular AI agent frameworks using GitHub Models or Azure OpenAI.\nlanguages:\n- python\nproducts:\n- azure-openai\n- azure\npage_type: sample\nurlFragment: python-ai-agent-frameworks-demos\n---\n-->\n# Python AI Agent Frameworks Demos\n\n[![Open in GitHub Codespaces](https://img.shields.io/static/v1?style=for-the-badge&label=GitHub+Codespaces&message=Open&color=brightgreen&logo=github)](https://codespaces.new/Azure-Samples/python-ai-agent-frameworks-demos)\n[![Open in Dev Containers](https://img.shields.io/static/v1?style=for-the-badge&label=Dev%20Containers&message=Open&color=blue&logo=visualstudiocode)](https://vscode.dev/redirect?url=vscode://ms-vscode-remote.remote-containers/cloneInVolume?url=https://github.com/Azure-Samples/python-ai-agent-frameworks-demos)\n\nThis repository provides examples of many popular Python AI agent frameworks using LLMs from [GitHub Models](https://github.com/marketplace/models). Those models are free to use for anyone with a GitHub account, up to a [daily rate limit](https://docs.github.com/github-models/prototyping-with-ai-models#rate-limits).\n\n* [Getting started](#getting-started)\n  * [GitHub Codespaces](#github-codespaces)\n  * [VS Code Dev Containers](#vs-code-dev-containers)\n  * [Local environment](#local-environment)\n* [Running the Python examples](#running-the-python-examples)\n* [Guidance](#guidance)\n  * [Costs](#costs)\n  * [Security guidelines](#security-guidelines)\n* [Resources](#resources)\n\n## Getting started\n\nYou have a few options for getting started with this repository.\nThe quickest way to get started is GitHub Codespaces, since it will setup everything for you, but you can also [set it up locally](#local-environment).\n\n### GitHub Codespaces\n\nYou can run this repository virtually by using GitHub Codespaces. The button will open a web-based VS Code instance in your browser:\n\n1. Open the repository (this may take several minutes):\n\n    [![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://codespaces.new/Azure-Samples/python-ai-agent-frameworks-demos)\n\n2. Open a terminal window\n3. Continue with the steps to run the examples\n\n### VS Code Dev Containers\n\nA related option is VS Code Dev Containers, which will open the project in your local VS Code using the [Dev Containers extension](https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-containers):\n\n1. Start Docker Desktop (install it if not already installed)\n2. Open the project:\n\n    [![Open in Dev Containers](https://img.shields.io/static/v1?style=for-the-badge&label=Dev%20Containers&message=Open&color=blue&logo=visualstudiocode)](https://vscode.dev/redirect?url=vscode://ms-vscode-remote.remote-containers/cloneInVolume?url=https://github.com/Azure-Samples/python-ai-agent-frameworks-demos)\n\n3. In the VS Code window that opens, once the project files show up (this may take several minutes), open a terminal window.\n4. Continue with the steps to run the examples\n\n### Local environment\n\n1. Make sure the following tools are installed:\n\n    * [Python 3.10+](https://www.python.org/downloads/)\n    * Git\n\n2. Clone the repository:\n\n    ```shell\n    git clone https://github.com/Azure-Samples/python-ai-agent-frameworks-demos\n    cd python-ai-agents-demos\n    ```\n\n3. Set up a virtual environment:\n\n    ```shell\n    python -m venv venv\n    source venv/bin/activate  # On Windows: venv\\Scripts\\activate\n    ```\n\n4. Install the requirements:\n\n    ```shell\n    pip install -r requirements.txt\n    ```\n\n## Running the Python examples\n\nYou can run the examples in this repository by executing the scripts in the `examples` directory. Each script demonstrates a different AI agent pattern or framework.\n\n| Example | Description |\n| ------- | ----------- |\n| autogen_basic.py | Uses AutoGen to build a single agent. |\n| autogen_tools.py | Uses AutoGen to build a single agent with tools. |\n| autogen_magenticone.py | Uses AutoGen with the MagenticOne orchestrator agent for travel planning. |\n| autogen_swarm.py | Uses AutoGen with the Swarm orchestrator agent for flight refunding requests. |\n| langgraph.py | Uses LangGraph to build an agent with a StateGraph to play songs. |\n| llamaindex.py | Uses LlamaIndex to build a ReAct agent for RAG on multiple indexes. |\n| openai_agents_basic.py | Uses the OpenAI Agents framework to build a single agent. |\n| openai_agents.py | Uses the OpenAI Agents framework to handoff between several agents with tools. |\n| openai_functioncalling.py | Uses OpenAI Function Calling to call functions based on LLM output. |\n| pydanticai.py | Uses PydanticAI to build a two-agent sequential workflow for flight planning. |\n| semantickernel.py | Uses Semantic Kernel to build a writer/editor two-agent workflow. |\n| smolagents_codeagent.py | Uses SmolAgents to build a question-answering agent that can search the web and run code. |\n\n## Configuring GitHub Models\n\nIf you open this repository in GitHub Codespaces, you can run the scripts for free using GitHub Models without any additional steps, as your `GITHUB_TOKEN` is already configured in the Codespaces environment.\n\nIf you want to run the scripts locally, you need to set up the `GITHUB_TOKEN` environment variable with a GitHub personal access token (PAT). You can create a PAT by following these steps:\n\n1. Go to your GitHub account settings.\n2. Click on \"Developer settings\" in the left sidebar.\n3. Click on \"Personal access tokens\" in the left sidebar.\n4. Click on \"Tokens (classic)\" or \"Fine-grained tokens\" depending on your preference.\n5. Click on \"Generate new token\".\n6. Give your token a name and select the scopes you want to grant. For this project, you don't need any specific scopes.\n7. Click on \"Generate token\".\n8. Copy the generated token.\n9. Set the `GITHUB_TOKEN` environment variable in your terminal or IDE:\n\n    ```shell\n    export GITHUB_TOKEN=your_personal_access_token\n    ```\n\n10. Optionally, you can use a model other than \"gpt-4o\" by setting the `GITHUB_MODEL` environment variable. Use a model that supports function calling, such as: `gpt-4o`, `gpt-4o-mini`, `o3-mini`, `AI21-Jamba-1.5-Large`, `AI21-Jamba-1.5-Mini`, `Codestral-2501`, `Cohere-command-r`, `Ministral-3B`, `Mistral-Large-2411`, `Mistral-Nemo`, `Mistral-small`\n\n## Provisioning Azure AI resources\n\nYou can run all examples in this repository using GitHub Models. If you want to run the examples using models from Azure OpenAI instead, you need to provision the Azure AI resources, which will incur costs.\n\nThis project includes infrastructure as code (IaC) to provision Azure OpenAI deployments of \"gpt-4o\" and \"text-embedding-3-large\". The IaC is defined in the `infra` directory and uses the Azure Developer CLI to provision the resources.\n\n1. Make sure the [Azure Developer CLI (azd)](https://aka.ms/install-azd) is installed.\n\n2. Login to Azure:\n\n    ```shell\n    azd auth login\n    ```\n\n    For GitHub Codespaces users, if the previous command fails, try:\n\n   ```shell\n    azd auth login --use-device-code\n    ```\n\n3. Provision the OpenAI account:\n\n    ```shell\n    azd provision\n    ```\n\n    It will prompt you to provide an `azd` environment name (like \"agents-demos\"), select a subscription from your Azure account, and select a location. Then it will provision the resources in your account.\n\n4. Once the resources are provisioned, you should now see a local `.env` file with all the environment variables needed to run the scripts.\n5. To delete the resources, run:\n\n    ```shell\n    azd down\n    ```\n\n## Resources\n\n* [AutoGen Documentation](https://microsoft.github.io/autogen/)\n* [LangGraph Documentation](https://langchain-ai.github.io/langgraph/tutorials/introduction/)\n* [LlamaIndex Documentation](https://docs.llamaindex.ai/en/latest/)\n* [OpenAI Agents Documentation](https://openai.github.io/openai-agents-python/)\n* [OpenAI Function Calling Documentation](https://platform.openai.com/docs/guides/function-calling?api-mode=chat)\n* [PydanticAI Documentation](https://ai.pydantic.dev/multi-agent-applications/)\n* [Semantic Kernel Documentation](https://learn.microsoft.com/semantic-kernel/overview/)\n* [SmolAgents Documentation](https://huggingface.co/docs/smolagents/index)\n"
    },
    {
      "name": "neo4j-product-examples/graphrag-examples",
      "stars": 125,
      "img": "https://avatars.githubusercontent.com/u/100449908?s=40&v=4",
      "owner": "neo4j-product-examples",
      "repo_name": "graphrag-examples",
      "description": "Example GraphRAG Patterns",
      "homepage": "",
      "language": "Jupyter Notebook",
      "created_at": "2024-03-27T20:24:55Z",
      "updated_at": "2025-04-08T19:15:45Z",
      "topics": [],
      "readme": "# GraphRAG Examples\nThis repository contains multiple GraphRAG examples. \n1. __[customer-graph](customer-graph):__ End-to-end worked example for building a GraphRAG agent to accelerate customer and retail analytics.\n2. __[patterns-app](patterns-app):__ A streamlit app for introducing and teaching example GraphRAG patterns.\n"
    },
    {
      "name": "Josephrp/DataTonic",
      "stars": 87,
      "img": "https://avatars.githubusercontent.com/u/18212928?s=40&v=4",
      "owner": "Josephrp",
      "repo_name": "DataTonic",
      "description": "🌟DataTonic : A Data-Capable AGI-style Agent Builder of Agents , that creates swarms , runs commands and securely processes and creates datasets, databases, visualisations, and analyses.",
      "homepage": "https://www.tonic-ai.com",
      "language": "Jupyter Notebook",
      "created_at": "2023-12-12T11:28:24Z",
      "updated_at": "2025-02-25T20:05:26Z",
      "topics": [
        "agent-builder",
        "agi",
        "autogen",
        "azure",
        "chroma",
        "data",
        "data-science",
        "data-visualization",
        "database",
        "memgpt",
        "semantic-kernel",
        "semantic-memory",
        "taskweaver"
      ],
      "readme": "# DataTonic\n\n- turn your laptop computer into a webserver of nested data and code capable agents !\n\nA Data-Capable AGI-style Agent Builder of Agents , that creates swarms , runs commands and securely processes and creates datasets, databases, visualizations, and analyses.\n\n- DataTonic solves simple tasks that require complex data processing\n- it's perfect for data analytics and business intelligence\n\nyou can use image or audio input in your native language : **DataTonic is AGI for all**\n\n#### What it does :\n\n- your request is first processed according to a statement of work\n- additional data is retrieved and stored to enrich it\n- multiple agents are created based on your specific use case\n- Multiple Multi-Agent Environments produce files and folders according to your use case.\n\n## Use Case\n\nDataTonic produces fixed business intelligence assets based on autonomous multimedia data processing. \n\n- Sales Profiles\n- Adaptive Summaries\n- Dataset Analytics\n- Research Reports\n- Business Automation Applications\n\nBased on those it can produce :\n- Strategies\n- Applications\n- Analyses\n- rich business intelligence\n\n### Business Case\n\nDataTonic provides junior executives with an extremely effective solution for basic and time-consuming data processing, document creation or business intelligence tasks.\n\nNow anyone can :\n- get rapid client profile and sales strategy including design assets for executions with a single request\n- create a functional web application\n- create entire databases of business intelligence that can be used by enterprise systems. \n\n### Enterprise Autonomation Agent\n\nDo not wait for accounting, legal or business intelligence reporting with uncertain quality and long review cycles. DataTonic accelerates the slowest part of analysis : data processing and project planning execution.\n\n## Main Benefits\n\nDataTonic is unique for many reasons :\n\n- local and secure application threads.\n- compatible with microsoft  enterprise environments.\n- based on a rigorous and reproducible evaluation method.\n- developper friendly : easily plug in new functionality and integrations.\n\n## Do more with DataTonic \n\n- is DataTonic Accessible ?\n\n**yes** DataTonic is accessible both audio and image input.\n\n- can i use it to make beautiful graphs and statistical analyses with little or no starting data ?\n \n**yes.** DataTonic will look for the data it needs but you can add your .db files or any other types of files with DataTonic.\n  \n- can i write a book or a long report ?\n\n**yes.** DataTonic produces rich , full-length content.\n\n- can i make an app ?\n\n**yes.** DataTonic is more tailored to business intelligence but it is able to produce functioning applications inside generated repositories.\n\n- can it do my job ?\n\n**yes** DataTonic is able to automate many junior positions and it will include more enterprise connectors, soon !\n\n### How we use it : Multi-Consult Technology\n\nYou can use datatonic however you want, here's how we're using it :\n- add case books to your folder for embedding : now DataTonic always presents its results in a case study!\n- add medical textbooks to your folder for embedding : now DataTonic helps you through med-school !\n- add entire company business information : Data tonic is now your strategic advisor !\n- ask data tonic to create targetted sales strategies : now DataTonic is your sales assistant !\n\n**Data Tonic is the first multi-nested agent-builder-of-agents!**\n\n## How Data Tonic Was Created\n\nDataTonic Team started by evaluating multiple models against the new google/gemini models , testing all functions. Based on our evaluation results we optimized default prompts and created new prompts and prompt pipeline configurations. \n\nLearn more about using TruLens and our scientific method in the [evaluation folder](https://github.com/Tonic-AI/DataTonic/tree/main/evaluation). We share our results in the [evaluation/results](https://github.com/Tonic-AI/DataTonic/tree/main/evaluation/results) folder.\n\n**you can also replicate our evaluation by following the instructions in #Easy Deploy**\n\n## How it works :\n\nDataTonic is the first application to use a **doubly nested multi-environment multi-agent builder-of-agents configuration** . Here's how it works ! \n\n### Orchestration\n\nData Tonic uses a novel combination of three orchestration libraries. \n- Each library creates it's own multi-agent environment. \n- Each of these environments includes a code execution and code generation capability. \n- Each of these stores data and embeddings on it's own datalake.\n- Autogen is at the interface with the user and orchestrates the semantic kernel hub as well as using Taskweaver for data processing tasks.\n- Semantic-kernel is a hub that includes internet browsing capabilities and is specifically designed to use taskweaver for data storage and retrieval and produce fixed intelligence assets also specifically designed for Autogen.\n- Taskweaver is used as a plugin in semantic kernel for data storage and retrieval and also in autogen, but remains an autonomous task that can execute complex tasks in its multi-environment execution system.\n\n### Technology : \n\n- Gemini is used in various configurations both for text using the autogen connector and for multimodal/image information processing. \n- Autogen uses a semantic-kernel function calling agent to access the internet using the google api semantic-kernel then processes the new information and stores it inside a SQL database orchestrated by Taskweaver.\n\n# Easy Deploy\n\nPlease try the methods below to use and deploy DataTonic.\n\n### Easy Deploy DataTonic evaluation/results\n\nThe easiest way to use DataTonic is to deploy on github spaces and use the **notebooks in the evaluation/results folder** . \n\nClick here for easy_deploy [COMING SOON!]\n\n**in the mean time please follow the instructions below:**\n\n1. Star then Fork this repository\n2. [use these instructions to deploy a code space for DataTonic](https://docs.github.com/en/codespaces/developing-in-a-codespace/creating-a-codespace-for-a-repository)\n3. Configure DataTonic according to the instructions below or \n4. navigate to [evaluation/results](https://github.com/Tonic-AI/DataTonic/tree/main/evaluation/results) to use our evaluation methods.\n\n# How To Use\n\nPlease follow the instructions in this readme **exactly**. \n\n### Follow Tonic-AI\n\n### Star & Fork this repository\n\n![image](https://github.com/Tonic-AI/DataTonic/assets/18212928/54e2f12a-0379-49d5-866c-985ea36f0e2b)\n1. Step 1 : Star this repository\n2. Step 2 : Fork this repository\n\n## Set Up\n\nplease use command line with administrator priviledges for the below.\n\n### Set Up Gemini\n\n- run the following command to install googlecloud/vertex cli :\n```bash\npip install google-cloud-aiplatform\n```\n\n- navigate to this url :\n```\nhttps://console.cloud.google.com/vertex-ai\n```\n- and click create new project.\n![image](https://github.com/Tonic-AI/DataTonic/assets/18212928/582ee276-e8b7-4d5d-b251-c1f730eaa84c)\n\n- Create a new project and add a payment method.\n\n- click 'enable all recommended APIs' \n![image](https://github.com/Tonic-AI/DataTonic/assets/18212928/2193f80b-fd46-444c-9173-81044166b3a6)\n\n- click on 'multimodal' on the left then 'my prompts' on the top:\n![image](https://github.com/Tonic-AI/DataTonic/assets/18212928/3ca60a1b-9012-432a-b431-b13eccbb57ff)\n\n- click on 'create prompt' and 'GET CODE' on the top right in the next screen:\n![image](https://github.com/Tonic-AI/DataTonic/assets/18212928/f563af64-cb9f-40a0-8158-564cd42bbc45)\n\n- then click on 'curl' on the top right to find your 'endpoint' and projectid , and other information\ne.g.\n```curl\ncat << EOF > request.json\n{\n    \"contents\": [\n        {\n            \"role\": \"user\",\n            \"parts\": []\n        }\n    ],\n    \"generation_config\": {\n        \"maxOutputTokens\": 2048,\n        \"temperature\": 0.4,\n        \"topP\": 1,\n        \"topK\": 32\n    }\n}\nEOF\n\nAPI_ENDPOINT=\"us-central1-aiplatform.googleapis.com\"\nPROJECT_ID=\"focused-album-408018\"\nMODEL_ID=\"gemini-pro-vision\"\nLOCATION_ID=\"us-central1\"\n\ncurl \\\n-X POST \\\n-H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\\n-H \"Content-Type: application/json\" \\\n\"https://${API_ENDPOINT}/v1/projects/${PROJECT_ID}/locations/${LOCATION_ID}/publishers/google/models/${MODEL_ID}:streamGenerateContent\" -d '@request.json'\n```\n\n### Get Google API key\n\n run the following command to find your API key after following the instructions above:\n```bash\ngcloud auth print-access-token\n```\n**IMPORTANT : this key will expire every less than 30 minutes, so please refresh it regularly and accordingly**\n\n### Get Open AI Key(s)\n\n- navigate to openai and create a new key :\n```\nhttps://platform.openai.com/api-keys\n```\n\n### Set Up Azure\n\n- use the Azure OAI portal by navigating to this page :\n```\nhttps://oai.azure.com/portal\n```\n- deploy your models\n![image](https://github.com/Tonic-AI/DataTonic/assets/18212928/db19a2f3-5996-462c-9273-ef4af5710d0e)\n\n- go to playground\n- click on view code :\n![image](https://github.com/Tonic-AI/DataTonic/assets/18212928/192f55fb-c174-45d6-a15b-12f7bce5e0b2)\n- make a note of your `endpoint` , `API Key` , and `model name` to use it later.\n\n### Set Up Sqlite\n\n- visit this url and download and install the required packages :\n```https://www.sqlite.org/download.html```\n**For Windows** : \nYou need the SQLite source files, including the sqlite3.h header file, for the pysqlite3 installation.\n\n- Go to the SQLite Download Page.\n- Download the sqlite-amalgamation-*.zip file under the \"Source Code\" section.\n- Extract the contents of this zip file to a known directory (e.g., C:\\sqlite).\n\nSet Environment Variables:\n\nYou need to ensure that the directory where you extracted the SQLite source files is included in your system's PATH environment variable.\n\n- Right-click on 'This PC' or 'My Computer' and select 'Properties'.\n- Click on 'Advanced system settings' and then 'Environment Variables'.\n- Under 'System Variables', find and select the 'Path' variable, then click 'Edit'.\n- Add the path to the directory where you extracted the SQLite source files (e.g., C:\\sqlite).\n- Click 'OK' to close all dialog boxes.\n\nadd your path :\n```bash\nsetx SQLITE_INC \"C:\\sqlite\"\n```\nproceed with the rest of the setup below.\n\n### Install Taskweaver\n\nTaskWeaver requires Python >= 3.10. It can be installed by running the following command:\n\n```\n# [optional to create conda environment]\n# conda create -n taskweaver python=3.10\n# conda activate taskweaver\n\n# clone the repository\ngit clone https://github.com/microsoft/TaskWeaver.git\ncd TaskWeaver\n# install the requirements\npip install -r requirements.txt\n```\n\n**Command Prompt**:\ndownload and install wsl:\n```bash\npip install wsl\n```\nthen run\n```bash\nwsl\n```\nthen install sqlite\n```bash\nsudo apt-get update\nsudo apt-get install libsqlite3-dev #or : sqlite-devel\nsudo pip install pysqlite3\n```\n\n## DataTonic Setup Instructions\n\nThis section provides instructions on setting up the project. Please turn off your firewall and use administrator priviledges on the command line.\n\n### Step 1 : Clone the repository\n\nclone this repository using the command line :\n\n```bash\ngit clone https://github.com/Tonic-AI/DataTonic.git\n```\n\n## Step 2: Configure DataTonic\n\n### Add Your Files to DataTonic \n\n1. add relevant files one by one with no folder to the folder called 'src/autogen/add_your_files_here' \n    - supported file types : \".pdf\" ,  \".html\" ,  \".eml\" & \".xlsx\":\n2. \n\n### Configuration\n1. you'll need the keys you made above for the following.\n2. use a text editor , and IDE  or command line to edit the following documents.\n3. Edit then save the files \n\n#### OAI_CONFIG_LIST\n\nedit 'OAI_CONFIG_LIST'\n\n```json\n        \"api_key\": \"your OpenAI Key goes here\",\n```\nand \n```json\n        \"api_key\": \"your Google's GenAI Key goes here\",\n```\n\n#### Configure OpenAI Key(s)\n    1. modify Line 135 in autogen_module.py\n        ```python\n        os.environ['OPENAI_API_KEY'] = 'Your key here'\n        ```\n    2. modify .env.example\n        ```os\n        OPENAI_API_KEY = \"your_key_here\"\n        ```\n        save as '.env' - this should create a new file.\n        **or** \n        rename to '.env' - this will rename the existing file.\n\n    3. modify src\\tonicweaver\\taskweaver_config.json\n        ```json\n        {\n            \"llm.api_base\": \"https://api.openai.com/v1\",\n            \"llm.api_key\": \"\",\n            \"llm.model\": \"gpt-4-1106-preview\"\n        }\n        ```\n\n    4. \n#### Google API\n\nedit ./src/semantic_kernel/semantic_kernel_module.py\n```python\nline 64:    semantic_kernel_data_module = SemanticKernelDataModule('<google_api_key>', '<google_search_engine_id>')\n```\nand \n```python\nline 158:    semantic_kernel_data_module = SemanticKernelDataModule('<google_api_key>', '<google_search_engine_id>')\n```\nwith your google API key and Search Engine ID , made above.\n\nsrc/semantic_kernel/googleconnector.py\n\n\n## Step 3: Install DataTonic\n\n### Install Taskweaver\n\nfrom the project directory :\n```bash\ncd ./src/tonicweaver\ngit clone https://github.com/microsoft/TaskWeaver.git\ncd ./src/tonicweaver/TaskWeaver\n# install the requirements\npip install -r requirements.txt\n```\n### Install DataTonic\nfrom the project directory :\n```bash\npip install -r requirements.txt\n```\n\n## Step 4: Run the application\n\n```bash\npython run app.py\n```\n\n# Developpers\n\nWe welcome contributions from the community! Whether you're opening a bug report, suggesting a new feature, or submitting a pull request, every contribution is valuable to us. Please follow these guidelines to contribute to DataTonic.\n\n## Getting Started\n\nBefore you begin, ensure you have the latest version of the main branch:\n\n```bash\ngit checkout main\ngit pull origin main\n```\nThen, create a new branch for your contribution:\n\n```bash\nCopy code\ngit checkout -b <your-branch-name>\n```\n## Bug Reports\n\nIf you encounter any bugs, please file an issue on our GitHub repository. Include as much detail as possible:\n\n- A clear and concise description of the bug\n- Steps to reproduce the behavior\n- Expected behavior vs actual behavior\n- Screenshots if applicable\n- Any additional context or logs\n\n## Suggesting Enhancements\n\nWe are always looking for suggestions to improve DataTonic. If you have an idea, please open an issue with the tag 'enhancement'. Provide:\n\n- A clear and concise description of the proposed feature\n- Any relevant examples or mockups\n- A description of the benefits to DataTonic users\n\n## Code Contributions\nIf you'd like to contribute code, please follow these steps:\n\n### Step 1: Set Up Your Environment\nFollow the setup instructions in the README to get DataTonic running on your local machine.\n\n### Step 2: Make Your Changes\nEnsure that your changes adhere to the existing code structure and standards. Add or update tests as necessary.\n\n### Step 3: Commit Your Changes\nWrite clear and meaningful commit messages. This helps to understand the purpose of your changes and speed up the review process.\n\n```bash\ngit commit -m \"A brief description of the commit\"\n```\n### Step 4: Push to the Branch\nPush your changes to your remote branch:\n\n ```bash\ngit push origin <your-branch-name>\n```\n### Step 5: Open a Pull Request\nGo to the repository on GitHub and open a new pull request against the main branch. Provide a clear description of the problem you're solving. Link any relevant issues.\n\n### Step 6: Code Review\nMaintainers will review your pull request. Be responsive to feedback to ensure a smooth process.\n\n# Code of Conduct\nPlease note that this project is released with a Contributor Code of Conduct. By participating in this project, you agree to abide by its terms.\n\n# License\nBy contributing to DataTonic, you agree that your contributions will be licensed under its LICENSE.\n\n**Thank you for contributing to DataTonic!🚀**\n"
    },
    {
      "name": "smile-wingbow/pudding-robot",
      "stars": 86,
      "img": "https://avatars.githubusercontent.com/u/174078102?s=40&v=4",
      "owner": "smile-wingbow",
      "repo_name": "pudding-robot",
      "description": null,
      "homepage": null,
      "language": "C++",
      "created_at": "2024-12-09T07:58:36Z",
      "updated_at": "2025-04-22T13:12:17Z",
      "topics": [],
      "readme": "# 给小朋友定制一个讲绘本的智能音箱\n\n# 👉 先看讲绘本的效果：[会讲儿童绘本故事的智能体音箱～](https://www.bilibili.com/video/BV15zqAYeERB/?vd_source=cb1594a360684634b55fabfd47bac5f2)\n\n## 👋 基本流程\n\n启动服务后，先访问盒子的内部地址上传绘本图片，生成故事，再用唤醒词唤醒音箱，说出绘本的名字或关键字，智能体找到绘本后开始讲。\n\n讲绘本的过程中会随机提问。\n\n也可以自由聊天。\n\n## ✨ 用到的硬件\n\n- **魔百盒CM311-1a或CM401（主要是搞定蓝牙驱动，其他型号没具体试过）\n- **多多上20-25元的联想蓝牙音箱。\n- **如果有wifi接入需求的，再加个usb的无线网卡，大概6-8元。\n\n## ✨ 盒子的OS\n\narmbian 24.10版：[https://github.com/ophub/amlogic-s9xxx-armbian/releases/download/Armbian_bookworm_save_2024.10/Armbian_24.11.0_amlogic_s905l3a_bookworm_6.6.57_server_2024.10.21.img.gz](https://github.com/ophub/amlogic-s9xxx-armbian/releases/download/Armbian_bookworm_save_2024.10/Armbian_24.11.0_amlogic_s905l3a_bookworm_6.6.57_server_2024.10.21.img.gz)，以上是CM311-1a的镜像，CM401a选择相应的版本\n\n## ✨ 软件部分\n\n- **wukong音箱，增加了微软的唤醒词，效果比原来的porcupine和snowboy好了很多；增加了火山引擎的语音处理。\n- **MetaGPT，主要是智能体流程，另外修改了多模态的接入，能够处理图片。\n\n## ⚡️ 快速开始\n#### Python 3.9\n\n先git clone https://github.com/smile-wingbow/pudding-robot\n以下命令都在pudding-robot路径下执行\n\n#### 一.安装必要的库：\n1.apt换源\n```shell\napt update\n```\n\n```shell\napt install pulseaudio python3-pyaudio pulseaudio-module-bluetooth build-essential libncurses5-dev libgdbm-dev libnss3-dev libssl-dev libreadline-dev libffi-dev zlib1g-dev libsqlite3-dev libcairo2-dev pkg-config python3-dev libgirepository1.0-dev gir1.2-glib-2.0 build-essential libglib2.0-dev libdbus-1-dev libudev-dev libical-dev libcairo2-dev pkg-config python3-dev libgirepository1.0-dev meson ninja-build portaudio19-dev python3-pip python3-dbus libdbus-1-dev libglib2.0-dev portaudio19-dev python3-pyaudio sox pulseaudio libsox-fmt-all ffmpeg libatlas-base-dev libtool\n```\n\n#### 二.安装蓝牙驱动：\n1.进入armbian-config，选择Network，安装蓝牙相关软件\n```shell\narmbian-config\n```\n2.把meson-g12a-s905l3a-cm311-with-bt(2024.10).dtb复制到/boot/dtb/amlogic/meson-g12a-s905l3a-cm311.dtb，覆盖前注意最好先备份\n```shell\ncp bluetooth/'meson-g12a-s905l3a-cm311-with-bt(2024.10).dtb' /boot/dtb/amlogic/meson-g12a-s905l3a-cm311.dtb\n```\n3.把rtl8761b_config_2m复制到/usr/lib/firmware/rtl_bt/rtl8761b_config.bin\n```shell\ncp bluetooth/rtl8761b_config_2m /usr/lib/firmware/rtl_bt/rtl8761b_config.bin\n```\n4.编辑/etc/systemd/system/bluetooth.service，在[Service]下增加以下内容：\n```shell\nnano /etc/systemd/system/bluetooth.target.wants/bluetooth.service\n```\n```shell\nExecStopPost=/usr/bin/env gpioset 0 82=0\n```\n5.编辑或添加~/.asoundrc：\n```shell\nnano ~/.asoundrc\n```\n```shell\npcm.!default {\n    type pulse\n}\n\nctl.!default {\n    type pulse\n}\n```\n6.重启盒子后，执行hciconfig，可以看到蓝牙设备\n```shell\nhci0:   Type: Primary  Bus: UART\n        BD Address: XX:XX:XX:XX:XX:XX  ACL MTU: 1021:5  SCO MTU: 255:11\n        UP RUNNING\n        RX bytes:4884 acl:0 sco:0 events:212 errors:0\n        TX bytes:36121 acl:0 sco:0 commands:212 errors:0\n```\n\n#### 三.安装python3.9\n1.下载python源代码安装包并解压：\n```shell\nwget https://www.python.org/ftp/python/3.9.20/Python-3.9.20.tgz\ntar -xvf Python-3.9.20.tgz\ncd Python-3.9.20\n```\n\n```shell\n./configure --enable-optimizations\nmake -j $(nproc)\nmake install\n```\n\n#### 四.创建虚拟环境并激活：\n```shell\npython3.9 -m venv pudding-venv  \nsource pudding-venv/bin/activate\n```\n\n#### 五.pip安装python相关库：\n```shell\npip install --upgrade pip setuptools wheel  \npip install meson ninja  \npip install -r requirements.txt\n```\n\n#### 六.安装dbus：\n```shell\ngit clone https://gitlab.freedesktop.org/dbus/dbus-python.git\ncd dbus-python\npython3.9 setup.py install\npip install dbus-python==1.2.16\n```\n\n#### 七.设置正确的时区：\n```shell\nln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime  \ndpkg-reconfigure -f noninteractive tzdata\n```\n\n#### 八.安装Tesseract\n1.安装leptonica：\n```shell\nwget https://github.com/DanBloomberg/leptonica/releases/download/1.85.0/leptonica-1.85.0.tar.gz\ntar -xvf leptonica-1.85.0.tar.gz\ncd leptonica-1.85.0\n```\n\n```shell\n./autogen.sh  \n./configure --prefix=/usr/local/leptonica  \nmake  \nmake install\n```\n2.编辑/etc/profile，增加以下内容：\n```shell\nnano /etc/profile\n```\n\n```shell\nPKG_CONFIG_PATH=$PKG_CONFIG_PATH:/usr/local/leptonica/lib/pkgconfig  \nexport PKG_CONFIG_PATH  \nCPLUS_INCLUDE_PATH=$CPLUS_INCLUDE_PATH:/usr/local/leptonica/include/leptonica  \nexport CPLUS_INCLUDE_PATH  \nC_INCLUDE_PATH=$C_INCLUDE_PATH:/usr/local/leptonica/include/leptonica  \nexport C_INCLUDE_PATH  \nLD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/leptonica/lib  \nexport LD_LIBRARY_PATH  \nLIBRARY_PATH=$LIBRARY_PATH:/usr/local/leptonica/lib  \nexport LIBRARY_PATH  \nLIBLEPT_HEADERSDIR=/usr/local/leptonica/include/leptonica  \nexport LIBLEPT_HEADERSDIR\n```\n应用设置：\n```shell\nsource /etc/profile\n```\n3.安装tesseract 5.4.0\n解压tesseract/tesseract-5.4.0.zip，也可以下载了解压后上传到armbian\n```shell\ngunzip tesseract/tesseract-5.4.0.zip\n```\n\n```shell\n./autogen.sh  \n./configure --prefix=/usr/local/tesseract  \nmake  \nmake install\n```\n编辑/etc/profile，增加以下内容：\n```shell\nsource /etc/profile\n```\n```shell\nPATH=$PATH:/usr/local/tesseract/bin export PATH export TESSDATA_PREFIX=/usr/local/share/tessdata\n```\n4.复制tessdata目录到/usr/local/share/目录下\n```shell\ncp -r tessdata /usr/local/share/\n```\n\n#### 九.在用户主目录下创建.wukong，并复制static目录下的config.yml、hidoubao.table和jarvis_zh_iphone.pmdl\n```shell\nmkdir ~/.wukong\ncp static/config.yml ~/.wukong/\ncp static/hidoubao.table ~/.wukong/\ncp static/jarvis_zh_iphone.pmdl ~/.wukong/\n```\n\n#### 十.如果需要自定义唤醒词，使用微软的自定义关键字定制服务(注意每次仅定义一个，同时定制多个会失败)：https://speech.microsoft.com/portal/7d04ce6f975240ed908f821c0e62eb3c/customkeyword\n\n#### 十一.配置接入的各种API key：\n1.配置config目录的config2.yaml、doubao_lite_32k.yaml、doubao_lite_4k.yaml、gpt4o.yaml、gpt4omini.yaml配置，上述配置文件是在robot/agents/pudding_agent.py使用，可以自行决定使用哪个模型\n2.修改用户主目录下.wukong的config.xml，主要修改TTS和ASR引擎的设置，修改的地方有：指定TTS引擎：tts_engine: volc-tts、指定ASR引擎：asr_engine: volc-asr、以及具体的引擎配置比如（volc_yuyin的具体配置）\n\n#### 十二.修改setting.store，把蓝牙设备的MAC地址改为要连接的音箱的蓝牙MAC地址。\n```shell\nbluetooth_devices:\n- A3:0F:B3:06:1C:A5\n```\n\n#### 十三.启动服务：\n1.启动pulseaudio\n```shell\npulseaudio --start\n```\n2.使用bluetoothctl来pair、trust和connect音箱\n```shell\nbluetoothctl pair XX:XX:XX:XX:XX:XX\nbluetoothctl trust XX:XX:XX:XX:XX:XX\nbluetoothctl connect XX:XX:XX:XX:XX:XX\n```\n3.使用pactl set-card-profile命令来修改蓝牙连接的profile，启用麦克风(命令中bluez_card.后面的mac地址改为音箱的蓝牙mac地址，注意用\"_\"替换\":\")\n```shell\npactl set-card-profile  bluez_card.XX_XX_XX_XX_XX_XX handsfree_head_unit\n```\n4.启动服务\n```shell\npython3.9 wukong.py\n```\n\n#### 十四.绘本的解析和故事生成：\n1.在虚拟环境下执行python3.9 book_parse_flask.py，然后访问http://ip地址:5000，上传绘本图片来生成故事。\n```shell\npython3.9 book_parse_flask.py\n```\n\n## 魔百盒CM401a编译蓝牙驱动\n\nCM311-1a如果刷的是：Armbian_24.11.0_amlogic_s905l3a_bookworm_6.6.57_server_2024.10.21.img.gz  \n这个版本，可以直接用github里bluetooth文件夹下编译好的驱动，其他armbian版本可以试试github里的，不行就重新编译：\n1.在/boot/dtb/amlogic/下找到对应的dtb文件，CM311-1a是meson-g12a-s905l3a-cm311.dtb，CM401a是meson-g12a-s905l3a-e900v22c.dtb  \n2.用dtc反编译dtb文件，得到dts源代码，用法：dtc -I dtb -O dts -o target.dts source.dtb\n```shell\ndtc -I dtb -O dts -o target.dts source.dtb\n```\n3.在serial@24000部分增加如下bluetooth内容：\n```shell\n      bluetooth {  \n                compatible = \"realtek,rtl8723bs-bt\";  \n        };\n```\n4.再把dts源码编译成二进制的dtb，命令：\n```shell\ndtc -I dts -O dtb -o target.dtb source.dts\n```\n5.备份原来的dtb文件后，再把编译好的dtb复制回/boot/dtb/amlogic/\n\n## 联系\n加群一起讨论\n\n![](https://github.com/smile-wingbow/pudding-robot/blob/main/assets/微信群.jpg?raw=true)\n\n## ❤️ 鸣谢\n\n感谢以下项目提供的贡献：\n\n- https://github.com/ophub/amlogic-s9xxx-armbian\n- https://github.com/wzpan/wukong-robot\n- https://github.com/geekan/MetaGPT\n\n## 免责声明\n\n本项目仅供学习和研究目的，不得用于任何商业活动。用户在使用本项目时应遵守所在地区的法律法规，对于违法使用所导致的后果，本项目及作者不承担任何责任。 本项目可能存在未知的缺陷和风险（包括但不限于设备损坏和账号封禁等），使用者应自行承担使用本项目所产生的所有风险及责任。 作者不保证本项目的准确性、完整性、及时性、可靠性，也不承担任何因使用本项目而产生的任何损失或损害责任。 使用本项目即表示您已阅读并同意本免责声明的全部内容。\n\n## License\n\n[MIT](https://github.com/idootop/mi-gpt/blob/main/LICENSE) License © 2024-PRESENT smilewingbow\n\n"
    },
    {
      "name": "cxbxmxcx/GPT-Agents",
      "stars": 80,
      "img": "https://avatars.githubusercontent.com/u/10665060?s=40&v=4",
      "owner": "cxbxmxcx",
      "repo_name": "GPT-Agents",
      "description": "Repository for the book GPT-Agents, published by Manning Publications",
      "homepage": null,
      "language": "HTML",
      "created_at": "2023-11-19T17:50:38Z",
      "updated_at": "2025-04-23T00:10:23Z",
      "topics": [],
      "readme": "# GPT-Agents (AI Agents In Action): Exploring the World of AI Agents and Assistants\n\n## About the Book\n\"AI Agents in Action,\" published by Manning Publications, is an insightful exploration into the world of large language model-empowered agents and assistants, focusing on AI Agents powered by large language models. This book offers a comprehensive guide to understanding, developing, and utilizing GPT agents in the realm of AI and machine learning.\n\n### Key Themes\n- **Understanding Agents**: Delve into the concept, history, and current role of agents in AI, distinguishing between agents and assistants, including autonomous agents.\n- **Agent Components**: Explore the complex components that constitute an agent, including their profiles, actions, memory, and planning capabilities.\n- **Evolution of AI Agents**: Trace the rapid development of GPT agents in software development and their growing significance in AI research.\n- **AI Interface Development**: Learn about the shift towards natural language interfaces in software and data, and how to build effective GPT interfaces.\n- **Agent Tools and Platforms**: Discover various tools and platforms essential for constructing effective GPT agent systems, with a focus on practical application.\n\n### Objective\nThe book is tailored for those keen on deepening their understanding of AI, machine learning, and the innovative world of GPT agents. It serves as a foundational resource for both theoretical knowledge and practical application in the ever-evolving landscape of AI agents and assistants.\n\n---\n\nFor more information, updates, and discussions, visit [Manning Publications](https://www.manning.com/).\n\n\n"
    },
    {
      "name": "Azure/azureml-assets",
      "stars": 67,
      "img": "https://avatars.githubusercontent.com/u/6844498?s=40&v=4",
      "owner": "Azure",
      "repo_name": "azureml-assets",
      "description": null,
      "homepage": "",
      "language": "Python",
      "created_at": "2022-03-15T23:26:18Z",
      "updated_at": "2025-04-23T05:55:39Z",
      "topics": [],
      "readme": "# Preview\n**Features contained in this repository are in private preview. Preview versions are provided without a service level agreement, and they are not recommended for production workloads. Certain features might not be supported or might have constrained capabilities. For more information, see [Supplemental Terms of Use for Microsoft Azure Previews](https://azure.microsoft.com/en-us/support/legal/preview-supplemental-terms/).**\n\n# Overview\nThis repo is the home for built-in registry assets for Azure Machine Learning. The term \"assets\" currently includes [components](https://docs.microsoft.com/en-us/azure/machine-learning/concept-component) and [environments](https://docs.microsoft.com/en-us/azure/machine-learning/concept-environments).\n\n# Trademarks\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n\n# Data Collection\nThe software may collect information about you and your use of the software and send it to Microsoft. Microsoft may use this information to provide services and improve our products and services. You may turn off the telemetry as described in the repository. There are also some features in the software that may enable you and Microsoft to collect data from users of your applications. If you use these features, you must comply with applicable law, including providing appropriate notices to users of your applications together with a copy of Microsoft’s privacy statement. Our privacy statement is located at https://go.microsoft.com/fwlink/?LinkID=824704. You can learn more about data collection and use in the help documentation and our privacy statement. Your use of the software operates as your consent to these practices.\n\nInformation on managing Azure telemetry is available at https://azure.microsoft.com/en-us/privacy-data-management/.\n"
    },
    {
      "name": "microsoft/EveryoneCanCode-US",
      "stars": 67,
      "img": "https://avatars.githubusercontent.com/u/6154722?s=40&v=4",
      "owner": "microsoft",
      "repo_name": "EveryoneCanCode-US",
      "description": "Everyone Can Code Event (U.S. Edition)",
      "homepage": null,
      "language": "Python",
      "created_at": "2024-05-02T13:53:33Z",
      "updated_at": "2024-12-26T03:31:33Z",
      "topics": [],
      "readme": "# Everyone Can Code! – Light Your Fire for Coding (U.S. Edition)\n\n![FemaleTechGenLogo](./content-images/MSFT_EveryoneCanCode_Banner.png)\n\n  <p>\n    <sub>Built with ❤ by everybody who wants to make the Microsoft App Dev Innovation world a little bit more diverse! </sub>\n  </p>\n\n</div>\n\n<hr>\n\n\n\n# Welcome to our coding journey!\n\nIt's great to have you here! 🎉\n\n“With barely 25% of employees in the technology industry being women and only 11% in leadership positions, these figures continue to highlight the challenges of gender equality in the tech world. Over the past nine years, the underrepresentation of women, especially those with diverse cultural backgrounds, persists across the corporate landscape” (McKinsey & Company, [_Women in the Workplace 2023_](https://www.mckinsey.com/featured-insights/diversity-and-inclusion/women-in-the-workplace)).\n\n\nThis is why we want to encourage everyone to bring their skills and passions to the world of technology, to explore new horizons together, and to shape the future of the industry together. **Anyone who is creative and curious, regardless of age, gender, or profession, can participate in “Everyone Can Code.”**\n\n**Join us for a groundbreaking online coding event** specifically designed to empower individuals and help you learn how to start writing your own coding stories with us.  You may have heard that coding can seem complex, but it is really nothing more than a language and technology that provides a magic pen in which we can paint the world around us. There's no such thing as “I can’t do it”.\n\nThis event is perfect for anyone who has ever been curious about coding but wasn't sure where to start. With expert instructors, hands-on activities, and a supportive community, you will be amazed at how much you can accomplish in just a few hours. **No previous code or other technical experience is required.**\n\n📚 [**To get started click here**](./Track_1_ToDo_App/README.md) 📚\n\n<br/>\n\n## Reporting issues and feedback\nPlease use the  [Issues](https://github.com/microsoft/EveryoneCanCode-US/issues) tab to report any bugs, documentation issues and/or overall feedback.\n\n## Contributor Wall of Fame\nSpecial thanks to the team that has worked on this to make it possible\n\n### Core + Extended Team\n![core_team](./content-images/everyone-can-code-core-team.png)\n\n### Content Reviewers\nIn addition to the core and extended teams, thanks to all of the additional content reviewers.\n\n![content-reviewers](./content-images/everyone-can-code-content-reviewers.png)\n\n\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n"
    },
    {
      "name": "microsoft/ai-developer",
      "stars": 65,
      "img": "https://avatars.githubusercontent.com/u/6154722?s=40&v=4",
      "owner": "microsoft",
      "repo_name": "ai-developer",
      "description": "This repository contains hackathon challenges for Semantic Kernel.",
      "homepage": null,
      "language": "C#",
      "created_at": "2024-10-18T13:25:43Z",
      "updated_at": "2025-04-17T09:20:09Z",
      "topics": [],
      "readme": "# AI Developer - Azure AI Foundry and Semantic Kernel Fundamentals\n\n## Introduction\n\nThe Azure AI Foundry and Semantic Kernel Fundamentals is an introduction to understanding the conceptual foundations of infusing your application with AI using the Semantic Kernel development kit. Materials from this workshop can serve as a foundation for building your own AI infused solutions with Semantic Kernel.\n\nThis workshop consists of eight challenges and is designed to encourage learning and research. If you want a deeper understanding of how to implement an AI solution but have little or no experience with OpenAI or Semantic Kernel then this workshop is for you.\n\nWe recommend to host this as a team based activity where students work in groups of 3-5 people to solve the challenges. Each challenge is designed to be completed in 30-90 minutes.\n\n## Learning Objectives\n\nThis workshop is designed for individuals who want to gain practical experience in working with Azure AI Foundry and Semantic Kernel. By applying this knowledge, you will be able to integrate AI into your own applications.\n\nParticipants will learn how to:\n\n- Build a simple chat using Semantic Kernel and C# or Python\n- Add plugins and enable auto calling to create Planners\n- Import existing APIs using OpenAPI\n- Implement Retrieval Augmented Generation (RAG)\n  - Document Chunking\n  - Grounding AI\n- Working with Image generation\n- Multi-Agent workflows\n\n## Challenges\n\n- Challenge 00: Prerequisites - Prepare your workstation to work with Azure.\n  - [C#](./Dotnet/challenges/Challenge-00.md)\n  - [Python](./Python/challenges/Challenge-00.md)\n\n- Challenge 01: Azure AI Foundry Fundamentals - Deploy an Azure AI Foundry Model, practice prompt engineering.\n  - [C#](./Dotnet/challenges/Challenge-01.md)\n  - [Python](./Python/challenges/Challenge-01.md)\n\n- Challenge 02: Semantic Kernel Fundamentals - Connect your OpenAI model, test your application.\n  - [C#](./Dotnet/challenges/Challenge-02.md)\n  - [Python](./Python/challenges/Challenge-02.md)\n\n- Challenge 03: Plugins - Create Semantic Kernel plugins, enable auto function calling, explore planners.\n  - [C#](./Dotnet/challenges/Challenge-03.md)\n  - [Python](./Python/challenges/Challenge-03.md)\n\n- Challenge 04: Import Plugin using OpenAPI - Import an API using an OpenAPI specification.\n  - [C#](./Dotnet/challenges/Challenge-04.md)\n  - [Python](./Python/challenges/Challenge-04.md)\n\n- Challenge 05: Retrieval-Augmented Generation (RAG) - Implement document chunking & embedding, enhance AI responses with external sources.\n  - [C#](./Dotnet/challenges/Challenge-05.md)\n  - [Python](./Python/challenges/Challenge-05.md)\n\n- Challenge 06: Responsible AI: Exploring Content Filters in Azure AI Foundry - Configure, test, and customize content filters.\n  - [C#](./Dotnet/challenges/Challenge-06.md)\n  - [Python](./Python/challenges/Challenge-06.md)\n\n- Challenge 07: Image Generation using DALL-E - Work with text-to-image models, build an image generating plugin.\n  - [C#](./Dotnet/challenges/Challenge-07.md)\n  - [Python](./Python/challenges/Challenge-07.md)\n\n- Challenge 08: Multi-Agent Systems - Create a multi-agent conversation, implement in Semantic Kernel.\n  - [C#](./Dotnet/challenges/Challenge-08.md)\n  - [Python](./Python/challenges/Challenge-08.md)\n\n- Challenge 09: Infuse existing Apps with AI - Enhance the Product Search functionality using Semantic Kernel.\n  - [C#](./Dotnet/challenges/Challenge-09.md)\n\n- Challenge 10: Enrich user experience with Semantic Kernel - Enhance the Product Search page with a creative AI response.\n  - [C#](./Dotnet/challenges/Challenge-10.md)\n\n### Here are the [Slides](./Dotnet/challenges/Resources/Lectures.pdf) used during our workshop\n\n### Here is the code for Python and C# - Download the [Zip](https://github.com/microsoft/ai-developer/raw/refs/heads/main/misc/finalresult.zip)\n\n## Contributors\n\n- [Chris McKee](https://github.com/ChrisMcKee1)\n- [Randy Patterson](https://github.com/RandyPatterson)\n- [Zack Way](https://github.com/seiggy)\n- [Travis Terrell](https://github.com/travisterrell)\n- [Eric Rhoads](https://github.com/ecrhoads)\n- [Wael Kdouh](https://github.com/waelkdouh)\n- [Munish Malhotra](https://github.com/munishm)\n- [Brijraj Singh](https://github.com/brijrajsingh)\n- [Linda M Thomas](https://github.com/lindamthomas)\n- [Suman More](https://github.com/sumanmore257)\n"
    },
    {
      "name": "AzureCosmosDB/CosmosAIGraph",
      "stars": 62,
      "img": "https://avatars.githubusercontent.com/u/33746873?s=40&v=4",
      "owner": "AzureCosmosDB",
      "repo_name": "CosmosAIGraph",
      "description": "CosmosAIGraph implementation of OmniRAG pattern",
      "homepage": "",
      "language": "Python",
      "created_at": "2024-09-19T18:26:46Z",
      "updated_at": "2025-04-23T04:35:40Z",
      "topics": [],
      "readme": "# CosmosAIGraph\n\n**AI-Powered Graph and RAG implementation of OmniRAG pattern, utilizing Azure Cosmos DB and other sources**\n\n- [Presentations](presentations/)\n- [Reference Application Documentation](docs/readme.md)\n- [Frequently Asked Questions (FAQ)](docs/faq.md)\n- [Reference Dataset of Python libraries, pre-vectorized](data/pypi/wrangled_libs)\n\n<pre>\n\n</pre>\n\n<p align=\"center\">\n  <img src=\"docs/img/deployment-architecture.png\" width=\"100%\">\n</p>\n\n---\n\n## Change Log\n\n- March 2025\n  - Version 3.0 codebase\n  - Now focused on Azure Cosmos DB for NoSQL\n    - Eliminated vCore support\n  - Now focused on Apache Jena implementation (Java) for the in-memory RDF graph\n    - Eliminated rdflib support\n  - New DockerHub images created\n  - Presentations updated - see the v3 files\n  - Docs are being updated, target completion date 3/31/2025\n\n- January 2025\n  - Added the **Java and Apache Jena** implementation of the in-memory graph\n  - See https://jena.apache.org/index.html\n\n- September 2024\n  - Added support for the **Azure Cosmos DB for NoSQL** in addition to Azure Cosmos DB for MongoDB vCore\n\n## Roadmap\n\n- Add RBAC and Microsoft Entra ID/AAD authentication support for the **Azure Cosmos DB for NoSQL**\n- Update AI model to gpt-4.5\n- Generic graph examples with graph generation solution\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services.\nAuthorized use of Microsoft  trademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must\nnot cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n"
    },
    {
      "name": "docqai/docq",
      "stars": 62,
      "img": "https://avatars.githubusercontent.com/u/132401033?s=40&v=4",
      "owner": "docqai",
      "repo_name": "docq",
      "description": "Private ChatGPT/Perplexity. Securely unlocks knowledge from confidential business information.",
      "homepage": "https://docqai.github.io/docq/",
      "language": "Python",
      "created_at": "2023-05-24T23:13:28Z",
      "updated_at": "2025-02-20T07:52:19Z",
      "topics": [
        "ai",
        "chatbot",
        "chatgpt",
        "genai",
        "privacy",
        "rag",
        "secure-by-default",
        "turnkey"
      ],
      "readme": "<!-- PROJECT SHIELDS -->\n<!--\n*** I'm using markdown \"reference style\" links for readability.\n*** Reference links are enclosed in brackets [ ] instead of parentheses ( ).\n*** See the bottom of this document for the declaration of the reference variables\n*** for contributors-url, forks-url, etc. This is an optional, concise syntax you may use.\n*** https://www.markdownguide.org/basic-syntax/#reference-style-links\n-->\n\n[![Contributors][contributors-shield]][contributors-url]\n[![Forks][forks-shield]][forks-url]\n[![Stargazers][stars-shield]][stars-url]\n[![Issues][issues-shield]][issues-url]\n[![AGPLv3 License][license-shield]][license-url]\n[![Slack][slack-shield]][slack-url]\n[![X (formerly Twitter) Follow][twitter-shield]][twitter-url]\n\n# Your private ChatGPT\n\n**Q&A with business documents securely. Depend less on other teams for answers.**\n\n![Docq overview in a single diagram](https://docqai.github.io/docq/assets/docq-diag-apr2024.png)\n\n<!-- ABOUT THE PROJECT -->\n\n## About The Project\n\nDocq offers **private** and **secure** GenAI that unlocks knowledge from your organisation's documents, with minimal onboarding and operational effort.\n\nDesign tenets:\n\n- **Data stays within your boundary**:\nbring ML models to the data. Cloud-vendor-hosted & self-hosted LLMs within the same cloud account to address data security and privacy concerns.\n- **Self-hosting**:\nwith your organisation's choice of cloud vendor. Minimal onboarding and operational effort. Can support air-gap use cases with local models.\n- **Multi-model**:\nability to utilise a variety of models and ability adopt new models quickly.\n- **Multi-modal**:\nsupports text today. Image, video, and audio formats are on the roadmap.\n- **Extensible**:\nAbstractions to easily add custom data sources. Plugin system for extending application, from UI to database.\n\nWe offer Docq as a **turnkey** solution to lower the barrier for your organisation to adopt the latest AI innovation safely.\n\n- Open-source (AGPLv3) to address your organisation's security and compliance needs.\n- Commercial license and support available (via Docq.AI based in London, UK).\n- Re-licensing option also available.\n\n## Vision\n\nBe the goto OSS GenAI application stack for the privacy and security conscious.\n\n## Demo\n\nA [demo app](https://docq-ai.streamlit.app/) is hosted by [Streamlit Community Cloud](https://streamlit.io/cloud), subject to frequent data refresh and feature update.\n\nCredentials for _admin_ user:\n\n- username: `docq`\n- password: `Docq.AI`\n\nThere are also recordings (Loom) to demonstrate Docq's major features:\n\n- [Ask your documents vs shared spaces](https://www.loom.com/share/21bb34d1bcb54f8ebf47c68f347d484c)\n- [Multi-user access](https://www.loom.com/share/599aa123ddce4a3d916ee8bdcd61095f)\n\n## Deploy Your Own Instance\n\n### Deploy to Azure in 15mins\n\nClick and follow the Azure wizard [![Deploy to Azure](https://aka.ms/deploytoazurebutton)](https://portal.azure.com/#create/Microsoft.Template/uri/https%3A%2F%2Fraw.githubusercontent.com%2Fdocqai%2Fdocq%2Fmain%2Finfra%2Fazure%2Farm%2Fappservice.json)\n\nFor more details see section [Deploy to Azure: 15 Minutes and Secure](./docs/user-guide/getting-started.md)\n\n### Deploy to AWS in 15mins\n\n<img src=\"./docs/assets/vendor-logos/aws-logo.svg\" width=\"50\">&nbsp;&nbsp;On the roadmap.\n\n### Deploy to GCP in 15mins\n\n<img src=\"./docs/assets/vendor-logos/gcp-logo.svg\" width=\"40\">&nbsp;&nbsp;On the roadmap.\n\n## Documentation\n\n[Documentation](https://docqai.github.io/docq/) site is hosted by [GitHub Pages](https://pages.github.com/).\n\n- [Overview](https://docqai.github.io/docq/overview/introduction/)\n- [User Guide](https://docqai.github.io/docq/user-guide/getting-started/)\n- [Developer Guide](https://docqai.github.io/docq/developer-guide/getting-started/)\n\n<!-- ROADMAP -->\n\n## Roadmap\n\nWe aim to support the following features in the future:\n\n- More data ingestion options such as SaaS data connectors and network storage options\n- A plugin platform and ecosystem\n- Model fine-tuning as an option for self-hosted, open-source LLMs\n\nWe also have plan to offer enterprise add-ons such as SSO support and enhanced audit.\n\nStay tuned!\n\n<!-- CONTACT -->\n\n## Contact\n\nDocq.AI - [@docqai](https://github.com/docqai) - support@docq.ai\n\nProject Link: [https://github.com/docqai/docq/](https://github.com/docqai/docq/)\n\n<!-- LICENSE -->\n\n## Licenses\n\nThe code in the repo is distributed under the licenses below. If a file has a specific license and copyright notice displayed then that wins.\n\n- AGPLv3. See [`LICENSE`](./LICENSE.AGPL3) and [`NOTICE`](./NOTICE.AGPL3)  for more information.\n- Commercial licenses available via [Docq.AI](https://docq.ai)\n- Copyright (C) 2023-2024 FABR Ltd t/a [Docq.AI](https://docq.ai). \n\nMade :heart: London.\n\n\n\n<!-- MARKDOWN LINKS & IMAGES -->\n<!-- https://www.markdownguide.org/basic-syntax/#reference-style-links -->\n\n[contributors-shield]: https://img.shields.io/github/contributors/docqai/docq.svg?style=flat\n[contributors-url]: https://github.com/docqai/docq/graphs/contributors\n[forks-shield]: https://img.shields.io/github/forks/docqai/docq.svg?style=flat\n[forks-url]: https://github.com/docqai/docq/network/members\n[stars-shield]: https://img.shields.io/github/stars/docqai/docq.svg?style=flat\n[stars-url]: https://github.com/docqai/docq/stargazers\n[issues-shield]: https://img.shields.io/github/issues/docqai/docq.svg?style=flat\n[issues-url]: https://github.com/docqai/docq/issues\n[license-shield]: https://img.shields.io/badge/license-AGPL_3.0-green?style=flat\n[license-url]: https://github.com/docqai/docq/blob/main/LICENSE.AGPL3\n[slack-shield]: https://img.shields.io/badge/Join-orange?style=flat&logo=slack&label=Slack\n[slack-url]: https://join.slack.com/t/docqai/shared_invite/zt-27p17lu6v-6KLJxSmt61vfNqCavSE73A\n[twitter-shield]: https://img.shields.io/twitter/follow/docqai?logo=x&style=flat\n[twitter-url]: https://twitter.com/docqai\n"
    },
    {
      "name": "Azure-Samples/gen-ai-bot-in-a-box",
      "stars": 61,
      "img": "https://avatars.githubusercontent.com/u/1844662?s=40&v=4",
      "owner": "Azure-Samples",
      "repo_name": "gen-ai-bot-in-a-box",
      "description": "This template deploys a Generative AI Virtual Assistant using Azure OpenAI and Bot Framework.",
      "homepage": null,
      "language": "C#",
      "created_at": "2024-08-02T16:26:23Z",
      "updated_at": "2025-04-09T13:30:23Z",
      "topics": [],
      "readme": "# AI-in-a-Box - Generative AI Bot Quickstart\n\n|||\n|:---| ---:|\n|This solution is part of the the AI-in-a-Box framework developed by the team of Microsoft Customer Engineers and Architects to accelerate the deployment of AI and ML solutions. Our goal is to simplify the adoption of AI technologies by providing ready-to-use accelerators that ensure quality, efficiency, and rapid deployment.| <img src=\"./media/ai-in-a-box.png\" alt=\"AI-in-a-box Logo: Description\" style=\"width: 70%\"> |\n\n[![Deploy to Azure](https://aka.ms/deploytoazurebutton)](https://portal.azure.com/#create/Microsoft.Template/uri/https%3A%2F%2Fraw.githubusercontent.com%2FAzure-Samples%2Fgen-ai-bot-in-a-box%2Fmain%2Finfra%2Fazuredeploy.json)\n\n## User Story\n\nVirtual Assistants are one of the top use cases for Generative AI. With Bot Framework, you can deploy and manage an enterprise-scale chat application with multiple supported programming languages. You can also connect with different end-user channels like Web, Microsoft Teams or Slack, while maintaining a single bot implementation.\n\n## What's in the Box\n![GenAI Bots Architecture](media/architecture.png)\n\nThis sample provides a template and guidance on how to deploy a virtual assistant leveraging multiple Azure AI technologies. It covers the infrastructure deployment, configuration on the AI Studio and Azure Portal, and end-to-end testing examples.\n\nThe following implementations are supported:\n\n| Language | Version | Chat Completions | Assistants API | Semantic Kernel | Phi-3 |\n|----------|---------|------------------|----------------|-----------------|-------|\n| Python   | 3.10    | ✔                | ✔              | ✔               | ✗     |\n| C#       | 8.0     | ✔                | ✔              | ✔               | ✗     |\n| NodeJS   | 21.0    | ✔                | ✔              | ✗               | ✗     |\n\n> Notes: The Phi model implementation (coming soon!) requires the deployment of an AI Project and Serverless Endpoint. It does not support function calling.\n> Using Python with Bot Framework requires a Single Tenant Application.\n\n## Thinking Outside of the Box\n\nThe solution can be adapted for your own use cases:\n\n- Add custom AI plugins to perform integrations, such as:\n    - Bing Search, to enable up-to-date information retrieval\n    - MS Graph integrations to retrieve work profile information\n    - Custom RAG using SQL queries\n- Customize the authorization flow to use SSO\n- Integrate with enterprise communication channels\n\n## Deploy the Solution\n\n### Prerequisites for running locally:\n1. Install latest version of [Azure CLI](https://docs.microsoft.com/en-us/cli/azure/install-azure-cli-windows?view=azure-cli-latest)\n2. Install latest version of [Bicep](https://docs.microsoft.com/en-us/azure/azure-resource-manager/bicep/install)\n3. Install latest version of [Azure Developer CLI](https://learn.microsoft.com/en-us/azure/developer/azure-developer-cli/install-azd)\n4. Install [Node JS](https://nodejs.org/en)\n\n### Deploy to Azure\n\n1. Clone this repository locally\n```\ngit clone https://github.com/Azure-Samples/gen-ai-bot-in-a-box\n```\n2. This repository provides the same application written in Dotnet, NodeJS and Python. Go to azure.yaml and uncomment the implementation you would like to use.\n\n![Azure YAML](media/azure-yaml.png)\n\n3. Deploy resources\n\n```\ncd gen-ai-bot-in-a-box\nazd auth login\nazd up\n```\n\nYou will be prompted for an environment name, a subscription, location and a few other customization parameters. Make sure to select the programming language you chose on the previous step, or deployment will fail.\n\n## Run the Solution\n\n### Test in Web Chat\n\nGo to the resource group and find the Azure Bot Services instance.\n\n![Resource group view](media/resource-group-view.png)\n\nSelect the Test in Web Chat option.\n\n![Azure Bot Services Overview](media/azure-bot-services-overview.png)\n\nChat with your new bot!\n\n![Web Chat Test](media/web-chat-test.png)\n\n> Note: The first time you open your bot after deployment, it may take a few seconds to respond. After interacting with it the first time, it will respond faster.\n\n### Test in Bot Framework Emulator\n\nYou can also interact with your bot through the emulator by running the code locally.\nHow you run the code depends on the language chosen:\n\n- Dotnet: \n```sh\n    dotnet run\n```\n- Node: \n```sh\n    npm install\n    npm start\n```\n- Python: \n```sh\n    pip install -r requirements.txt # Or set up a virtualenv / conda environment\n    python app.py\n```\n\nYou may then open the Bot Emulator and connect it to http://localhost:3978/api/messages to test your bot.\n\n## Troubleshooting and FAQ\n\n- **Post-provision hooks fail with \"Cannot iterate over null\"**:\n    - The post-provision hooks rely on network access to the services created. Make sure to allowlist your own IP so the scripts execute correctly\n- **azd pipeline config fails to push changes**:\n    - This command will push changes to the repository's main branch. Make sure you have the necessary permissions and branch protection is temporarily disabled.\n- **An error of type 'access_denied' occurred during the login process**:\n    - This will happen when your App Registration is missing either the \"ID Tokens\" configuration, or the MS Graph `openid` permission scope. Make sure both configurations are in place and retry.\n\n## How to Contribute\n\nThis project welcomes contributions and suggestions. Most contributions require you to agree to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us the rights to use your contribution. For details, visit <https://cla.opensource.microsoft.com>\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions provided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/). For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq) or contact <opencode@microsoft.com> with any additional questions or comments.\n## Key Contacts & Contributors\n\n| Contact | GitHub ID | Email |\n|---------|-----------|-------|\n| Marco Cardoso | @MarcoABCardoso | macardoso@microsoft.com |\n\n## License\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow [Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general). Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those third-party's policies.\n\n\n---\n\nThis project is part of the AI-in-a-Box series, aimed at providing the technical community with tools and accelerators to implement AI/ML solutions efficiently and effectively.\n"
    },
    {
      "name": "microsoft/promptflow-resource-hub",
      "stars": 58,
      "img": "https://avatars.githubusercontent.com/u/6154722?s=40&v=4",
      "owner": "microsoft",
      "repo_name": "promptflow-resource-hub",
      "description": null,
      "homepage": "",
      "language": "Python",
      "created_at": "2023-10-25T09:01:21Z",
      "updated_at": "2025-04-04T02:04:17Z",
      "topics": [],
      "readme": "# Prompt flow ecosystem\n\nWelcome to the **prompt flow ecosystem** - a comprehensive gallery that showcases a variety of use cases demonstrating the the capabilities of prompt flow in building high-quality AI applications.\n\nHere are some common use cases that we've facilitated:\n\n* [**Evaluate semantic kernel planner**](./sample_gallery/evaluate_semantic_kernel_planner/Tutorial.md): This case demonstrates the evaluation of the Semantic Kernel Planner for superior control over AI agent's generation.\n* [**Personalized Financial advise using banking customer data on PostgreSQL/CosmosDB**](./sample_gallery/database_querying/cosmosdb_postgresql/Tutorial.md): This example demonstrates how to connect to a CosmosDB database, query data, and use the retrieved information as a reference for AI-generated financial advice.\n* [**Natural Language to SQL**](./sample_gallery/nl2sql/README.MD): This is an example for implementing Natural Language to SQL code generation on a sample database using PromptFlow and performing evaluation.\n\nIn addition, we also provide best practices for:\n\n* [**Golden dataset preparation**](./sample_gallery/golden_dataset/copilot-golden-dataset-creation-guidance.md): Learn how to prepare a golden dataset for your AI application evaluation.\n\nOur Ecosystem serves as a practical guide, providing detailed walkthroughs of each use case for industry, and how to effectively leverage prompt flow in your AI application development process.\n\n## Getting started\n\n1. Install the prompt flow SDK\n\n```bash\npip install promptflow promptflow-tools\n```\n\nMore information about the SDK installation can be found [here](https://github.com/microsoft/promptflow/tree/main#installation).\n\n2. Install the prompt flow extension in VS code\n\nSearch for `promptflow` in the VS code extension marketplace and install the extension.\n\nMore information about the extension can be found [here](https://marketplace.visualstudio.com/items?itemName=prompt-flow.prompt-flow).\n\n## Practicing\nGo to the [sample gallery](./sample_gallery), you can find more samples here, you can follow the tutorial to start practicing on a specific use case.\n"
    },
    {
      "name": "Azure/gpt-rag-orchestrator",
      "stars": 57,
      "img": "https://avatars.githubusercontent.com/u/6844498?s=40&v=4",
      "owner": "Azure",
      "repo_name": "gpt-rag-orchestrator",
      "description": null,
      "homepage": "",
      "language": "Jupyter Notebook",
      "created_at": "2023-06-27T14:29:47Z",
      "updated_at": "2025-04-14T12:52:24Z",
      "topics": [],
      "readme": "# Enterprise RAG Orchestrator\n\nThis Orchestrator is part of the **Enterprise RAG (GPT-RAG)** Solution Accelerator.\n\nTo learn more about the Enterprise RAG, please go to [https://aka.ms/gpt-rag](https://aka.ms/gpt-rag).\n\n## How the Orchestrator Works\n\nThe **Enterprise RAG Orchestrator** efficiently manages user interactions by coordinating various modules and plugins to generate accurate responses. The core of its functionality revolves around the `get_answer` function in [code_orchestration.py](https://github.com/Azure/gpt-rag-orchestrator/blob/main/orc/code_orchestration.py), which processes user queries through a structured workflow.\n\n### Orchestration Flow\n\n1. **Initialization**:\n   - **Conversation Management**: Retrieves or creates a conversation record from Cosmos DB using the `conversation_id`.\n   - **Setup**: Initializes necessary variables, loads the bot description, and prepares the Semantic Kernel for processing.\n\n2. **Processing User Input (`get_answer` Function)**:\n   - **Input Handling**: Captures the latest user query and appends it to the conversation history.\n   - **Guardrails**: Performs initial checks, such as filtering blocked words to ensure content compliance.\n   - **RAG Flow**:\n     - **Language Detection**: Identifies the language of the user input.\n     - **Conversation Summarization**: Summarizes the conversation history to maintain context.\n     - **Intent Triage**: Determines the intent behind the user query (e.g., question answering, follow-up).\n     - **Data Retrieval**: Utilizes retrieval plugins to fetch relevant information based on the identified intent.\n     - **Answer Generation**: Generates a coherent response by integrating retrieved data and conversation context.\n   - **Final Guardrails**: Ensures the generated answer meets quality standards by checking for blocked content and grounding.\n\n3. **Response Synthesis and Delivery**:\n   - **Updating Conversation**: Saves the generated answer and relevant metadata back to Cosmos DB.\n   - **Delivery**: Formats and sends the response to the user, completing the interaction cycle.\n\n## Cloud Deployment\n\nTo deploy the orchestrator in the cloud for the first time, please follow the deployment instructions provided in the [Enterprise RAG repo](https://github.com/Azure/GPT-RAG?tab=readme-ov-file#getting-started).  \n   \nThese instructions include the necessary infrastructure templates to provision the solution in the cloud.  \n   \nOnce the infrastructure is provisioned, you can redeploy just the orchestrator component using the instructions below:\n\nFirst, please confirm that you have met the prerequisites:\n\n - Azure Developer CLI: [Download azd for Windows](https://azdrelease.azureedge.net/azd/standalone/release/1.5.0/azd-windows-amd64.msi), [Other OS's](https://learn.microsoft.com/en-us/azure/developer/azure-developer-cli/install-azd).\n - Git: [Download Git](https://git-scm.com/downloads)\n - Python 3.10: [Download Python](https://www.python.org/downloads/release/python-31011/)\n\nThen just clone this repository and reproduce the following commands within the gpt-rag-orchestrator directory:  \n\n```\nazd auth login  \nazd env refresh  \nazd deploy  \n```\n\n> Note: when running the ```azd env refresh```, use the same environment name, subscription, and region used in the initial provisioning of the infrastructure.\n\n### Alternative Cloud Deployment\n\nIf you deployed the GPT-RAG infrastructure manually, you can use Azure Functions Core Tools as an alternative to `azd` for deployment:\n\n```bash\nfunc azure functionapp publish FUNCTION_APP_NAME\n```\n\n*Replace `FUNCTION_APP_NAME` with the name of your Orchestrator Function App before running the command.*\n\nAfter completing the deployment, run the following command to confirm that the function was successfully deployed:\n\n```bash\nfunc azure functionapp list-functions FUNCTION_APP_NAME\n```\n\nYou can download Azure Functions Core Tools from this [link](https://learn.microsoft.com/en-us/azure/azure-functions/functions-run-local?tabs=windows%2Cisolated-process%2Cnode-v4%2Cpython-v2%2Chttp-trigger%2Ccontainer-apps&pivots=programming-language-python#install-the-azure-functions-core-tools).\n\n\n## Running Locally with VS Code  \n   \n[How can I test the solution locally in VS Code?](docs/LOCAL_DEPLOYMENT.md)\n\n### Evaluating\n\n[How to test the orchestrator performance?](docs/LOADTEST.md)\n\n## Contributing\n\nWe appreciate your interest in contributing to this project! Please refer to the [CONTRIBUTING.md](https://github.com/Azure/GPT-RAG/blob/main/CONTRIBUTING.md) page for detailed guidelines on how to contribute, including information about the Contributor License Agreement (CLA), code of conduct, and the process for submitting pull requests.\n\nThank you for your support and contributions!\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft\ntrademarks or logos is subject to and must follow\n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n"
    },
    {
      "name": "neo4j-product-examples/graphrag-contract-review",
      "stars": 56,
      "img": "https://avatars.githubusercontent.com/u/100449908?s=40&v=4",
      "owner": "neo4j-product-examples",
      "repo_name": "graphrag-contract-review",
      "description": "Demonstration an contract review Agent",
      "homepage": null,
      "language": "Python",
      "created_at": "2024-09-30T17:49:11Z",
      "updated_at": "2025-04-18T13:31:36Z",
      "topics": [],
      "readme": "# GraphRAG in Commercial Contract Review\n\nThis repository contains all of the code mentioned in [GraphRAG in Commercial Contract Review](https://medium.com/@edward.sandoval.2000/graphrag-in-commercial-contract-review-7d4a6caa6eb5).\n\n\n## Contract Review - GraphRAG-based approach\nThe GraphRAG-based approach described in the blog post goes beyond the traditional chunk-based RAG, focusing instead on targeted information extraction from the contracts (LLM + Prompt) to create a knowledge graph representation (LLM + Neo4J), a simple set of data retrieval functions (in Python using Cypher, Text to Cypher, Vector Search retrievers) and ultimately a Q&A agent (Semantic Kernel) capable of handling complex questions\n\nThe diagram below illustrates the approach\n\n![4-stage-approach](./images/4-stage-approach%20.png)\nThe 4-stage GraphRAG approach: From question-based extraction -> knowledge graph model -> GraphRAG retrieval -> Q&A Agent\n\n\nThe four steps are:\n1. Extracting Relevant Information from Contracts (LLM + Contract)\n2. Storing information extracted into a Knowledge Graph (Neo4j)\n3. Developing simple KG Data Retrieval Functions (Python)\n4. Building a Q&A Agent handling complex questions (Semantic Kernel, LLM, Neo4j)\n\n# What Do You Need?\n- Obtain an [OpenAI token](https://platform.openai.com/api-keys). It will be used to:\n    - Build the Q&A agent with Semantic Kernel\n    - Extract specific information from contracts (parties,key dates,jurisdiction)\n    - Generate embeddings for a small number of contract excerpts\n    - Power a Text2Cypher data retrieval function\n- Python 3.9+ and a Python virtual environment\n- Access to a Neo4j database\n    - Docker, Aura or Self-hosted\n    - GenAI plugin running on the database (This is automatically available in Aura)\n\n# Set Up\n- Clone the repo\n``` \ngit clone https://github.com/neo4j-product-examples/graphrag-contract-review.git\ncd graphrag-contract-review\n```\n- Create a Python Virtual environment\n```\npython3 -m venv .venv\nsource .venv/bin/activate\npip install -r requirements.txt\n```\n- Run a local Neo4j instance (Optional)\nWe will use Docker  \n\nBut you can use an Aura or self-hosted Neo4j instance.\nIf you do so,  you can skip this step. Just make sure you have URL, user name and password to access your Neo4j database\n\n```\ndocker run \\\n    --restart always --env NEO4J_AUTH=neo4j/yourpassword \\\n    --publish=7474:7474 --publish=7687:7687 \\\n    --env NEO4J_PLUGINS='[\"genai\",\"apoc\"]'  neo4j:latest\n```\n\nMake sure you replace **yourpassword** with a password to access this database\n\n## Set up some environment vars\nIf you are using Neo4j Aura or self-hosted instance\n```\nexport NEO4J_URI=<URI to your db>\nexport NEO4J_USERNAME=<your_username>\n```\nIf you are using Docker, you need to specify URI or Username ONLY if different from the default\n\nYou need to specify the password to access the database\n```\nexport NEO4J_PASSWORD=<your_password>\n```\n\nSet your OpenAI API  Key \n```\nexport OPENAI_API_KEY=sk-....\n```\n\n\n# STEP 1: Extracting Relevant Information from Contracts (LLM + Contract)\nIn the [data](./data/input) folder, you will find 3 real commercial contracts in PDF format\n\nThese contracts were taken from the publicly available [Contract Understanding Atticus Dataset](https://www.atticusprojectai.org/cuad)\n\nOur first step is to run a program that will prompt a ```OpenAI gpt-4o``` model to answer 40+ questions for each contract.\n\nThe prompt will include instructions to store the extracted information in JSON format, under [data/output](./data/output)\n\nThe full prompt can be found [here](./prompts/contract_extraction_prompt.txt)\n\n## From PDF Contract to JSON\nRun the following command\n```\npython convert-pdf-to-json.py\n```\nEach PDF will take around 60s to process\n\nYou can check out any of the [json files generated under the data/output folder](./data/output/)\n\nIn case the LLM generates invalid JSON, you can find the infomration returned by the LLM under [data/debug](./data/debug/) folder\n\n# STEP 2: Storing information extracted into a Knowledge Graph (Neo4j)\nWith each contract as a JSON file, the next step is to create a Knowledge Graph in Neo4j\n\nBefore we can do that, we need to design a Knowledge Graph Data model suitable to represent the information extracted from the contracts\n\n## A Suitable KG Data Model for our contracts\n\nIn our case, a suitable KG data model includes our main entities: Agreements (contracts), their clauses, the Parties( organizations) to the contracts and the relationships amongst all of them\n\n![A Suitable Knowledge Graph Model](./images/schema.png)\n\n## Some useful properties for each of the main nodes and relationships\n```\nAgreement {agreement_type: STRING, contract_id: INTEGER,\n          effective_date: STRING,\n          renewal_term: STRING, name: STRING}\nContractClause {name: STRING, type: STRING}\nClauseType {name: STRING}\nCountry {name: STRING}\nExcerpt {text: STRING}\nOrganization {name: STRING}\n\nRelationship properties:\nIS_PARTY_TO {role: STRING}\nGOVERNED_BY_LAW {state: STRING}\nHAS_CLAUSE {type: STRING}\nINCORPORATED_IN {state: STRING}\n```\n\nNow, let's create a Knowledge Graph from the JSON files in ```./data/output/*.json```\n\n```\n python create_graph_from_json.py\n```\nThe ```create_graph_from_json``` Python script is relatively straightforward to understand. \n\nThe main area of complexity is the ```CREATE_GRAPH_STATEMENT```. This [CYPHER statement](./create_graph_from_json.py#L7) that takes a Contract JSON and creates the relevant nodes and relationships for that contract in Neo4j. \n\nYou can check out the original [blog post](https://medium.com/@edward.sandoval.2000/graphrag-in-commercial-contract-review-7d4a6caa6eb5) for a full breakdown of this CYPHER statement\n\n\nYou will see output similar to \n```\nIndex excerptTextIndex created.\nIndex agreementTypeTextIndex created.\nIndex clauseTypeNameTextIndex created.\nIndex clauseNameTextIndex created.\nIndex organizationNameTextIndex created.\nCreating index: contractIdIndex\nGenerating Embeddings for Contract Excerpts...\n```\n\nThe generation of embeddings takes about 1 minute(s) to complete\n\nAfter the Python script finishes:\n- Each Contract JSON has been uploaded to Neo4J Knowledge Graph\n- Key properties on the Agreement, ClauseTypes, Organization (Party) have fulltext indexes\n- A new property Excerpt.embedding was generated by using ```genai.vector.encode(excerpt.text)```\n    - This calls out OpenAI Text Embedding model ```text-embedding-3-small```\n- A new vector index for Excerpt.embedding is created\n\n\nThe total number of Excerpt embeddings for the 3 contracts is between 30-40 (depending on how many relevant excerpts were detected by the LLM on Step 1)\n\nA visual representation of one of the contracts in the Knowledge Graph is\n\n![A visual representation of a single Contract as a Knowledge Graph](./images/contract_graph.png)\n\nIf you are using Docker to run your Neo4j instance, you can use the [browser tool](http://localhost:7474/browser/) to confirm your data was loaded\n\nIf you are using Aura or a self-hosted Neo4j instance, you can use the Query tool from the [new Neo4j console](https://console-preview.neo4j.io/tools/query). You may need to log into your Aura instance or manually add a connection to your self-hosted database\n\n\n# STEP 3: Developing simple KG Data Retrieval Functions (Python)\nWith the contracts now represented in a Knowledge Graph, the next step is to build some basic data retrieval functions. \n\nThese functions are fundamental building blocks that enable us to build a Q&A agent in the next section\n\nLet's define a few basic data retrieval functions:\n\n- Retrieve basic details about a contract (given a contract ID)\n- Find contracts involving a specific organization (given a partial organization name)\n- Find contracts that DO NOT contain a particular clause type\n- Find contracts contain a specific type of clause\n- Find contracts based on the semantic similarity with the text (Excerpt) in a clause (e.g., contracts mentioning the use of \"prohibited items\")\n- Run a natural language query against all contracts in the database. For example, an aggregation query that counts \"how many contracts in the database \".\n\nYou are encouraged to explore [ContractPlugin.py](./ContractPlugin.py) for a definition and [ContractService.py](./ContractService.py) for the implementation of each of the data retrieval functions\n\nThe original [blog post](https://medium.com/@edward.sandoval.2000/graphrag-in-commercial-contract-review-7d4a6caa6eb5) provides a walk through of three different styles of data retrieval functions\n- Cypher-based data retrieval functions -\n    -  ```get_contract(self, contract_id: int) -> Annotated[Agreement, \"A contract\"]:```  \n    -  ```get_contracts_without_clause(self, clause_type: ClauseType) -> List[Agreement]:```\n    - Both of these data retrieval are built around simple CYPHER statements\n- Vector-Search + Graph traversal data retrieval function  \n    - ```get_contracts_similar_text(self, clause_text: str) -> Annotated[List[Agreement], \"A list of contracts with similar text in one of their clauses\"]:```\n    - This function leverages [Neo4j GraphRAG package](https://github.com/neo4j/neo4j-graphrag-python)\n    - It also relies on a vector index defined on \"Exceprt\" nodes\n- Text-to-Cypher (T2C) data retrieval function\n    -  ```answer_aggregation_question(self, user_question: str) -> Annotated[str, \"An answer to user_question\"]:```\n    - This function leverages [Neo4j GraphRAG package](https://github.com/neo4j/neo4j-graphrag-python)\n    - It uses OpenAI ```gpt-4o``` to generate CYPHER statement that will be executed against the database\n\n\n\n# STEP 4: Building a Q&A Agent handling complex questions (Semantic Kernel, LLM, Neo4j)\n\nArmed with our Knowledge Graph data retrieval functions, we are ready to build an agent grounded by GraphRAG!\n\nWe will use Microsoft Semantic Kernel, a framework that allows developers to integrate LLM function calling with existing APIs and data retrieval functions The framework uses a concept called ```Plugins``` to represent specific functionality that the kernel can perform. In our case, all of our data retrieval functions defined in the \"ContractPlugin\" can be used by the LLM to answer questions about contracts in the Neo4J database\nIn addition, Semantic Kernel uses the concept of ```Memory``` to keep all interactions between user and agent. This includes details of any functions called/executed (with all input and output data)\n\nAn extremely simple Terminal-based agent can be implemented with a few lines of code. \n\nRun\n```\npython test_agent.py\n```\nYou can try variations of the following questions to exercise the different data retrieval functions\n\n- Get me contracts with Price Restrictions but without Insurance \n    - See the logging INFO and notice how this requires calling 2 of our data retrieval functions\n- Get more details about this contract\n- Get me contracts for AT&T \n- Get me contracts for Mount Knowledge\n- Get me contract 3\n- Get me contracts that mention 100 units of product\n- What's the average number of excerpts per contract?\n\nYou can type ```**exit``` to finish your session with the agent\n\nYou can see the full code of the [test_agent.py](./test_agent.py)\nYou will find functions that exercise each of the retrieval functions (commented out)\n\nFor a nicer-looking UI, you can try on streamlit\n```\nstreamlit run app.py\n```\nThe browser shows\n\n![Agent in Streamlit](./images/streamlit_view.png)\n\n\n# Acknowledgements - Contract Understanding Atticus Dataset\n\nThis demo was made possible thanks to the invaluable resource provided by the Contract Understanding Atticus Dataset (CUAD) v1. \n\n\nA dataset curated and maintained by The Atticus Project.  CUAD's extensive corpus of over 13,000 labels in 510 commercial legal contracts, manually annotated under the supervision of experienced lawyers, has been instrumental in identifying critical legal clauses for contract review, particularly in corporate transactions such as mergers and acquisitions.\n\nWe recognize and appreciate CUAD's contribution to advancing NLP research and development in the field of legal contract analysis.\n\n\n# Future Improvements\nIn this demo, we didn't fine-tune the LLM to enhace its basic capabilities to identify relevant excerpts.\n\nThe CUAD does provide the labelled clauses/excerpts that could be used to fine-tune a model to recognize the presence/absence of these clauses\n\n\n"
    },
    {
      "name": "Azure-Samples/aistudio-python-langchain-sample",
      "stars": 54,
      "img": "https://avatars.githubusercontent.com/u/1844662?s=40&v=4",
      "owner": "Azure-Samples",
      "repo_name": "aistudio-python-langchain-sample",
      "description": "Quickstart sample for using the Azure AI Studio with the SDK or CLI options - and the LangChain framework.",
      "homepage": null,
      "language": "Python",
      "created_at": "2023-11-10T20:28:00Z",
      "updated_at": "2025-04-11T22:31:15Z",
      "topics": [],
      "readme": " ### :warning: WARNING: **This repository is deprecated and no longer maintained** :warning:\n|There is a new repository that covers the latest Azure AI code-first experiences. Navigate to and star this one instead: https://github.com/Azure-Samples/rag-data-openai-python-promptflow/tree/main|\n|---------------------------|\n\n![](img/vector-search-architecture-diagram.png)\n\n# Azure AI Studio: LangChain Quickstart Sample\n\nThis project use the AI Search service to create a vector store for a custom department store data.  We will be using Azure Open AI's text-embedding-ada-002 deployment for embedding the data in vectors. The vector representation of your data is stored in [Azure AI Search](https://learn.microsoft.com/en-us/azure/search/search-what-is-azure-search) (formerly known as \"Azure Cognitive Search\").  \nTo enable the user to ask questions our data in a conversational format, we'll using Langchain to connect our prompt template with our Azure Open AI LLM.\n\n![](img/rag-pattern.png)\n\nWe'll use Retrieval Augmented Generation (RAG), a pattern used in AI which uses an LLM to generate answers with your own data. In addition, we'll  construct prompt template to provide the scope of our dataset, as well as the context to the submit questions. Lastly, we'll maintain the state of the conversation by store the chat history in the prompt.\n\n**Custom Data:** The sample data that we'll be using in this project is a department store dataset.  The dataset contains a list of customers, orders, products and their descriptions, and their prices.  We'll be using this dataset to create a copilot that can answer questions about the products in the dataset.\n\nThis is the basic quickstart tutorial for the Azure AI Studio using Langchain. Find other framework-specific tutorial here:\n - [Azure AI Studio: Semantic Kernel Quickstart](https://github.com/Azure-Samples/aistudio-python-semantickernel-sample)\n - [Azure AI Studio: PromptFlow Quickstart](https://github.com/Azure-Samples/aistudio-python-promptflow-sample)\n  - [Azure AI Studio: Python Quickstart](https://github.com/Azure-Samples/aistudio-python-quickstart-sample)\n\n\n## 🧰 | Explore features of Azure AI Studio\n\nThe sample showcases features from the [Azure AI Studio preview](https://aka.ms/azureai/docs):\n\n* [Azure AI Studio](https://aka.ms/azureaistudio/docs) - build, evaluate, deploy, your AI solution from one UI.\n* [Azure AI Services](htthttps://aka.ms/azureaistudiops://learn.microsoft.com/azure/ai-services/what-are-ai-services) - core AI Service APIs & Models usable in Azure AI Studio \n* [Azure AI SDK](https://learn.microsoft.com/azure/ai-studio/how-to/sdk-install) - for programmatic access to Azure AI Services.\n* [Azure AI CLI](https://learn.microsoft.com/azure/ai-studio/how-to/cli-install) - for command-line access to Azure AI Services.\n\n> [!WARNING]  \n> Features contained in this repository are in private preview. Preview versions are provided without a service level agreement, and they are not recommended for production workloads. Certain features might not be supported or mvght have constrained capabilities. For more information, see [Supplemental Terms of Use for Microsoft Azure Previews](https://azure.microsoft.com/support/legal/preview-supplemental-terms/).\n\n## 👩🏽‍💻 | Build a copilot with your own data\n\nLearn to build your own copilot using the Azure AI Studio with core resources (Azure AI Services) and tools (Azure AI SDK, Azure AI CLI). The tutorial guides you through the following steps:\n\n1. Setup and validate your development environment.\n2. Create an Azure AI project and AI resources for your copilot.\n3. Create an Azure AI search index for your custom data.\n4. Validate copilot by asking a question about your custom data.\n5. Evaluate the performance of your copilot implementation.\n6. (Optional) Deploy the copilot to Azure and invoke it.\n\n## 🏁 | Let's Get Started!\n\nReady to get started building a copilot with your own custom data? \n- [**Start here**](docs/use-github-codes.md) to setup your development environment, then work through the remaining steps.\n- [**Run local via CLI**](docs/use-locally.md) if you want to get started in your local environment.\n\n## 📚 | Relevant Resources\n\n1. [Azure AI Studio](https://aka.ms/azureaistudio) - UI to explore, build & manage AI solutions.\n1. [Azure AI Studio Docs](https://learn.microsoft.com/azure/ai-studio) - Azure AI Studio documentation.\n1. [Azure AI Services](https://learn.microsoft.com/azure/ai-services/what-are-ai-services) - Azure AI Services documentation.\n1. [Training: Using vector search in Azure Cognitive Search](https://learn.microsoft.com/training/modules/improve-search-results-vector-search) \n1. [Tutorial: Deploy a web app for chat on your data](https://learn.microsoft.com/azure/ai-studio/tutorials/deploy-chat-web-app) \n"
    },
    {
      "name": "Azure/azure-ai-cli",
      "stars": 54,
      "img": "https://avatars.githubusercontent.com/u/6844498?s=40&v=4",
      "owner": "Azure",
      "repo_name": "azure-ai-cli",
      "description": "This repository is for active development of the Azure AI CLI. For consumers of the CLI, we suggest you check out The Book of AI at https://thebookof.ai ",
      "homepage": "https://thebookof.ai/",
      "language": "C#",
      "created_at": "2023-07-27T17:31:59Z",
      "updated_at": "2025-04-18T16:59:30Z",
      "topics": [],
      "readme": "Status: Draft in Progress\nOwner: Rob Chambers\n\n# Using the Azure AI CLI\n\nThe Azure `ai` Command-Line Interface (CLI) is a cross-platform command-line tool to connect and immediately use Azure AI services with or without writing code. The CLI allows the execution of commands through a terminal using interactive command-line prompts or via script.\n\nYou can easily use the `ai` CLI to experiment with key Azure AI service features and see how they work with your use cases. Within minutes, you can setup the required Azure resources, and build a customized Copilot using OpenAI's chat completions APIs and your own data. You can try it out interactively, or script larger processes to automate your own workflows as part of your CI/CD system.\n\nAdditionally you can use the `ai` CLI to dynamically create code to integrate with your own applications in the programming language of your choice (C#, Go, Java, JavaScript, Python, TypeScript).\n\n## **STEP 1**: Setup your development environment\n\nYou can install the Azure `ai` CLI locally on Linux, Mac, or Windows computers, or use it thru an internet browser or Docker container.\n\nDuring this public preview, we recommend using the Azure `ai` CLI thru GitHub Codespaces. This will allow you to quickly get started without having to install anything locally.\n\n### OPTION 1: GitHub Codespaces\n\nYou can run the Azure `ai` CLI in a browser using GitHub Codespaces:\n\n[![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://codespaces.new/Azure/azure-ai-cli?quickstart=1)\n\n</div><div class=\"content\" id=\"content2\">\n\n### OPTION 2: VS Code Dev Container\n\nYou can run the Azure `ai` CLI in a Docker container using VS Code Dev Containers:\n\n1. Follow the [installation instructions](https://code.visualstudio.com/docs/devcontainers/containers#_installation) for VS Code Dev Containers.\n2. Clone the [azure-ai-cli](https://github.com/Azure/azure-ai-cli) repository and open it with VS Code:\n    ```\n    git clone https://github.com/Azure/azure-ai-cli\n    code azure-ai-cli\n    ```\n3. Click the button \"Reopen in Dev Containers\", if it does not appear open the command pallete (`Ctrl+Shift+P` on Windows/Linux, `Cmd+Shift+P` on Mac) and run the `Dev Containers: Reopen in Container` command\n\n### OPTION 3: Local Installation\n\nYou can install the Azure `ai` CLI locally on your computer:\n\n1. Install [.NET 8.0 SDK](https://dotnet.microsoft.com/en-us/download/dotnet/8.0)\n\n    On Linux, follow these ([instructions]()).  \n    On macOS, follow these ([instructions]());  \n    On Windows, follow these ([instructions]()), or use this command:  \n\n    ```bash\n    winget install Microsoft.DotNet.SDK.8\n    ```\n\n2. Install or update the Azure `ai` CLI:\n\n    ```bash\n    dotnet tool install -g Azure.AI.CLI --prerelease\n    ```\n\n    or\n\n    ```bash\n    dotnet tool update -g Azure.AI.CLI --prerelease\n    ```\n\n</div><div class=\"content\" id=\"content3\">\n\n## **STEP 2**: Initialize resource connections and configuration w/ `ai init`\n\nYou can initialize the Azure `ai` CLI by running the following command:\n\n```\nai init\n```\n\n<img src=\"./media/ai-cli-init.png\" height=200 alt=\"ai init console screen showing listbox of choices\"/>\n\nFollow the prompts, selecting the Azure subscription, followed by selecting or creating the Azure AI services you want to use.\n\n\n## **STEP 3**: Inspect various command options and examples w/ `ai help`\n\nYou can interactively browse and explore the Azure `ai` CLI commands and options by running the following command:\n\n```\nai help\n```\n\n<img src=\"./media/ai-cli-help.png\" height=240 alt=\"ai help console screen help content\"/>\n\n\n## **STEP 4**:  Chat with your LLM w/ `ai chat`\n\nYou can chat interactively or non-interactively with an AI language model using the `ai chat` command.\n\n**Interactive chat**\n\n```bash\nai chat --interactive\n```\n\n**Non-interactive chat**\n\n```bash\nai chat --user \"Tell me about Azure OpenAI\"\n```\n\n**Command line options**\n\n```\nUSAGE: ai chat [...]\n\n  CONNECTION                            (see: ai help connection)\n    --deployment DEPLOYMENT             (see: ai help chat deployment)\n    --endpoint ENDPOINT                 (see: ai help chat endpoint)\n    --key KEY                           (see: ai help chat key)\n\n  INPUT                                 (see: ai help chat input)\n    --interactive                       (see: ai help chat interactive)\n    --system PROMPT                     (see: ai help chat system prompt)\n    --user MESSAGE                      (see: ai help chat user message)\n    --chat-history FILE                 (see: ai help chat history)\n\n  CHAT WITH DATA                        (see: ai help chat with data)\n    --index-name INDEX                  (see: ai help index name)\n    --search-endpoint ENDPOINT          (see: ai help search endpoint)\n    --search-api-key KEY                (see: ai help search key)\n\n  OPTIONS                               (see: ai help chat options)\n    --temperature TEMPERATURE           (see: ai help chat options temperature)\n    --max-tokens MAX_TOKENS             (see: ai help chat options max-tokens)\n    --top-p TOP_P                       (see: ai help chat options top-p)\n    --n N                               (see: ai help chat options n)\n```\n\n## **STEP 5**: Create and update an AI Search Index w/ `ai search index update`\n\nYou can create an AI Search Index using the `ai search index create` command.\n\n```bash\nai search index create --index-name MyMarkdownFiles --files \"*.md\" --blob-container https://aitest123.blob.core.windows.net/product-info\n```\n\n**Command line options**\n\n```\nUSAGE: ai index create [...]\n\n  AZURE SEARCH\n    --index-name NAME                       (see: ai help search index name)\n    --search-api-key KEY                    (see: ai help search api key)\n    --search-endpoint ENDPOINT              (see: ai help search endpoint)\n\n  AZURE SEARCH DATA SOURCE\n    --data-source-connection NAME           (see: ai help search data source connection)\n    --blob-container ENDPOINT/NAME          (see: ai help search data source blob container)\n    --indexer-name NAME                     (see: ai help search indexer name)\n    --skillset-name NAME                    (see: ai help search skillset name)\n    --id-field NAME                         (see: ai help search id field name)\n    --content-field NAME                    (see: ai help search content field name)\n    --vector-field NAME                     (see: ai help search vector field name)\n\n  OPENAI EMBEDDINGS\n    --embedding-deployment DEPLOYMENT       (see: ai help search embedding deployment)\n    --embedding-model MODEL                 (see: ai help search embedding model)\n\n  DATA\n    --file FILE                             (see: ai help search index file)\n    --files FILEs                           (see: ai help search index files)\n    --external-source                       (see: ai help search index external source)\n```\n\n## **STEP 6**: Chat with your LLM using your AI Search Index w/ `ai chat --index-name`\n\nYou can chat interactively or non-interactively with an AI language model with your AI Search indexed data using the `ai chat` command with the `--index-name` option.\n\nFirst, let's create a system prompt file that will be used to seed the chat with a question about a product:\n\n```bash\nnano prompt.txt\n```\n\nCopy and paste the following text into the file:\n\n```\nYou are an AI assistant helping users with queries related to\noutdoor/camping gear and clothing. Use the following pieces of context\nto answer the questions about outdoor/camping gear and clothing as\ncompletely, correctly, and concisely as possible. If the question is not\nrelated to outdoor/camping gear and clothing, just say Sorry, I only can\nanswer question related to outdoor/camping gear and clothing. So how can\nI help? Don't try to make up an answer. If the question is related to\noutdoor/camping gear and clothing but vague ask for clarifying questions.\nDo not add documentation reference in the response.\n```\n\n**Interactive chat**\n\n```bash\nai chat --system @prompt.txt --index-name MyMarkdownFiles --interactive \n```\n\n**Non-interactive chat**\n\n```bash\nai chat --system @prompt.txt --index-name MyMarkdownFiles --user \"What is the best tent for camping?\"\n```\n\n## **STEP 7**: Create an application that uses your AI Search Index w/ `ai dev new`\n\nYou can create a new application that uses your AI Search Index using the `ai dev new` command.\n\nFirst, discover all the quick start sample templates available using the `ai dev new list` command, or filtered using `ai dev new list FILTER1 FILTER2`:\n\n`ai dev new list Chat`:\n\n```\nShort Name                              Language\n------------------------------------    --------------------------------\nopenai-chat                             C#, Go, Java, JavaScript, Python\nopenai-chat-streaming                   C#, Go, Java, JavaScript, Python\nopenai-chat-streaming-with-data         C#, Go, Java, JavaScript, Python\nopenai-chat-streaming-with-functions    C#, Go, JavaScript, Python\nopenai-chat-webpage                     JavaScript, TypeScript\nopenai-chat-webpage-with-functions      JavaScript, TypeScript\n```\n\n`ai dev new list Assistants`:\n\n```\nShort Name                              Language\n------------------------------------    ----------------------\nopenai-asst                             JavaScript\nopenai-asst-streaming                   JavaScript\nopenai-asst-streaming-with-functions    JavaScript\nopenai-asst-webpage                     JavaScript, TypeScript\nopenai-asst-webpage-with-functions      JavaScript\n```\n\nNow, let's create a JavaScript sample demonstrating how to \"chat with your data\", with streaming output from the LLM: \n\n`ai dev new openai-chat-streaming-with-data --javascript`:\n\n```\nGenerating 'openai-chat-streaming-with-data-js' (3 files)...\n\n  Main.js\n  OpenAIChatCompletionsStreamingWithDataClass.js\n  package.json\n\nGenerating 'openai-chat-streaming-with-data-js' (3 files)... DONE!\n```\n\n## **STEP 8**: Run your application w/ `ai dev shell --run`\n\nYou can run your application using the `ai dev shell --run` command.\n\nFirst, navigate to the directory created with the `ai dev new` command above:\n\n```bash\ncd openai-chat-streaming-with-data-js\n```\n\nInstall the dependencies for your application:\n\n```bash\nnpm install\n```\n\nThen, run your application:\n\n```bash\nai dev shell --run \"node Main.js\"\n```\n"
    },
    {
      "name": "Azure-Samples/moneta-agents",
      "stars": 51,
      "img": "https://avatars.githubusercontent.com/u/1844662?s=40&v=4",
      "owner": "Azure-Samples",
      "repo_name": "moneta-agents",
      "description": null,
      "homepage": "",
      "language": "Python",
      "created_at": "2024-10-10T06:48:58Z",
      "updated_at": "2025-04-16T07:35:52Z",
      "topics": [],
      "readme": "# Moneta - an AI-Agentic Assistant for Insurance and Banking\n\n[![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://codespaces.new/albertaga27/aoai-fsi-empowering-advisory-agentic) [![Open in Dev Containers](https://img.shields.io/static/v1?style=for-the-badge&label=Dev%20Containers&message=Open&color=blue&logo=visualstudiocode)](https://vscode.dev/redirect?url=vscode://ms-vscode-remote.remote-containers/cloneInVolume?url=https://github.com/albertaga27/aoai-fsi-empowering-advisory-agentic)\n\nMoneta is an AI-powered assistant designed to empower insurance and banking advisors. This Solution Accelerator provides a chat interface where advisors can interact with various AI agents specialized in different domains such as insurance policies, CRM, product information, funds, CIO insights, and news.\n\nYou can choose chich Agentic orchestration framework the Solution uses behind the scene by setting the approriate env variable. Choose from: \n* [Semantic Kernel](https://learn.microsoft.com/en-us/semantic-kernel/overview/) \n* [Microsoft GBB AI EMEA - Vanilla Agents](https://github.com/Azure-Samples/vanilla-aiagents) \n\n## Prerequisites\n\n* Docker\n* [uv](https://docs.astral.sh/uv/getting-started/installation/)\n* python 3.12\n* pip\n\n## Features\n\n- Agentic framework selection Support: Switch between Semantic Kernel and experimental MSFT GBB Vanilla Agents\n- Multi-Use Case Support: Switch between insurance and banking use cases\n- Agent Collaboration: Agents collaborate to provide the best answers\n- Azure AD Authentication: Secure login with Microsoft Azure Active Directory\n- Conversation History: Access and continue previous conversations\n\n## Implementation Details\n- Python 3.12 or higher\n- Streamlit (frontend app - chatGPT style)\n- Microsoft Authentication Library (MSAL - if using authentication - optional)\n- Azure AD application registration (if using authentication - optional)\n- An Azure Container App hosting backend API endpoint\n- CosmosDB to store user conversations and history\n\n## Use Cases\n\n### Insurance (SK)\n\n- `CRM`: simulate fetching clients information from a CRM (DB, third-party API etc)\n- `Policies RAG`: vector search with AI Search on various public available policy documents (product information)\n- `Responder`: collects previous agents replies and respond to the user\n\n### Banking (SK)\n\n- `CRM`: simulate fetching clients information from a CRM (DB, third-party API etc)\n- `Funds and ETF RAG`: vector search with AI Search on few funds and ETF factsheets (product information)\n- `CIO`: vector search with AI Search on in-house investments view and recommendations\n- `News`: RSS online feed search on stock news\n- `Responder`: collects previous agents replies and respond to the user\n\nNote: GBB Vanilla agents are similar but differs a bit (no responder agent)\n\n## Project structure\n\n- src\n  - backend\n    - gbb\n      - agents\n        - fsi_banking # agents files\n        - fsi_insurance # agents files\n      - genai_vanilla_agents # the framework source\n    - sk\n      - agents\n        - banking # agents files\n        - insurance # agents files\n      - orchestrators\n      - skills\n    - app.py # exposes API\n\n  - frontend\n    - app.py # streamlit app\n\n  - data\n    - ai-search-index\n      - cio-index\n      - funds-index\n      - ins-index\n    - customer-profile\n\n- infra\n  - bicep file\n  - infra modules\n\n\n### Azure deployment (automated)\n\nTo configure, follow these steps:\n\n1. Make sure you AZ CLI is logged in in the right tenant. Optionally:\n\n    ```shell\n    az login --tenant your_tenant.onmicrosoft.com\n    ```\n\n1. Create a new azd environment:\n\n    ```shell\n    azd env new\n    ```\n\n    This will create a folder under `.azure/` in your project to store the configuration for this deployment. You may have multiple azd environments if desired.\n\n1. Set the `AZURE_AUTH_TENANT_ID` azd environment variable to the tenant ID you want to use for Entra authentication:\n\n    ```shell\n    azd env set AZURE_AUTH_TENANT_ID $(az account show --query tenantId -o tsv)\n    ```\n\n1. Login to the azd CLI with the Entra tenant ID:\n\n    ```shell\n    azd auth login --tenant-id $(azd env get-value AZURE_AUTH_TENANT_ID)\n    ```\n\n1. Proceed with AZD deployment:\n\n    ```shell\n    azd up\n    ```\n### Deployment Scripts explaination (files in `scripts/`)\nThis project includes preprovision, postprovision, and postdeploy scripts for both Bash and PowerShell environments that works with the Azure Developer CLI (azd) on Windows, macOS, and Linux:\n- **preprovision** scripts set up Azure resources (App Registrations, environment variables).\n- **postprovision** scripts configure additional settings (redirect URIs, secrets).\n- **postdeploy** scripts handle data loading (AI Search, Cosmos DB).\n- **predown** scripts deletes an Azure app registration using the provided Azure App Registration ID before executing the resource purge.\n\n### Data indexing \n\nDemo data is loaded with the `azd up` process automatically.\n\nIndexes are sourced from 'src/data/ai-search-index' folder.\nEach subfolder of the data folder will be a seperate index. \n\nCustomer profiles are sourced from 'src/data/customer-profiles'.\nEach subfolder of the data folder will be get a seperate index. \n\nAZD deploy process will run `setup_aisearch.py` and `setup_cosmosdb.py`. \nDO NOT run those commands by hand unless you are confident you understand how the environment needs to be setup. \n\nIn case you need to reload the data, you can do it by running:\n```shell\nazd hooks run postdeploy\n```\n\n**OBS!** If you deploy from WSL mounted path, the postdeploy data init might fail. Please consider rerunning from WSL native path location.\n\n\n### Running the App locally - BACKEND\n\nThe python project is managed by `pyproject.toml` and the [uv package manager](https://docs.astral.sh/uv/getting-started/installation/).\nInstall uv prior executing.\n\nTo run locally:\n\nMind the `sample.env` file - by default the application will try to read AZD environment variables and falls back on `.env` only when it does not find one.\n\nActivate the `.venv` virtual environment or run the binary directly:\n\n```shell\ncd src/backend\nuv sync\n./.venv/bin/uvicorn app:app \n```\n\n### Running the App locally - FRONTEND\n\nThe python project is managed by pyproject.toml and [uv package manager](https://docs.astral.sh/uv/getting-started/installation/).\nInstall uv prior executing.\n\nTo run locally:\n\nmind the sample.env file - by default the application will try to read AZD enviornment configuraiton and falls on .env only when it does not find one.\n\n**OBS!** Activate .venv or run the binary directly.\n\n```shell\ncd src/frontend\nuv sync\n./.venv/bin/streamlit run app.py\n```\n\n### Usage\n\n1. **Select Use Case**: Choose between `fsi_insurance` and `fsi_banking` from the sidebar\n2. **Start a Conversation**: Click \"Start New Conversation\" or select an existing one\n3. **Chat**: Use the chat input to ask questions. Predefined questions are available in a dropdown\n4. **Agents Online**: View the available agents for the selected use case\n5. **Chat Histories**: View and reload your past conversations\n"
    },
    {
      "name": "Josephrp/scitonic",
      "stars": 51,
      "img": "https://avatars.githubusercontent.com/u/18212928?s=40&v=4",
      "owner": "Josephrp",
      "repo_name": "scitonic",
      "description": "👩🏻‍🔬🧪SciTonic is a highly adaptive technical operator of agents that can produce complexe analyses on technical data with high performance & on-the-fly . You can ask it what you want and it will respond with quality everytime.",
      "homepage": "https://www.tonic-ai.com",
      "language": "Jupyter Notebook",
      "created_at": "2024-01-12T17:33:29Z",
      "updated_at": "2025-04-21T13:16:36Z",
      "topics": [
        "agi",
        "autogen",
        "azure",
        "bing",
        "chroma",
        "e5",
        "memgpt",
        "semantic-kernel",
        "sql"
      ],
      "readme": "# Introducing 🧪👩🏻‍🔬Sci-Tonic - Your Ultimate Technical Research Assistant 🚀\n\n### Welcome to the Future of Technical Research: Sci-Tonic 🌐\n\nIn an era where data is king 👑, the ability to efficiently gather, analyze, and present information is crucial for success across various fields. Today, we are thrilled to introduce Sci-Tonic 🤖, a state-of-the-art technical research assistant that revolutionizes how professionals, researchers, and enthusiasts interact with data. Whether it's financial figures 💹, scientific articles 🧬, or complex texts 📚, Sci-Tonic is your go-to solution for turning data into insights.\n\n## Features of Sci-Tonic 🌈\n\n### 1. Data Retrieval: A Gateway to Information 🚪📊\n- **Broad Spectrum Access**: From financial reports to scientific papers, Sci-Tonic accesses a wide array of data sources.\n- **Efficiency and Precision**: Quickly fetches relevant data, saving you time and effort ⏰💼.\n\n### 2. Advanced Analysis: Deep Insights from Cutting-Edge AI 🧠💡\n- **Intelligent Interpretation**: Utilizes advanced AI algorithms to analyze and interpret complex data sets.\n- **Customizable Analysis**: Tailored to meet specific research needs, providing targeted insights 🔍.\n\n### 3. Multimedia Output: Diverse and Dynamic Presentation 📝🎥📊\n- **Versatile Formats**: Outputs range from text and infographics to video summaries.\n- **Engaging and Informative**: Enhances understanding and retention of information 🌟.\n\n### 4. User-Friendly Interface: Accessible to All 👩‍💻👨‍💻\n- **Intuitive Design**: Easy to navigate for both tech experts and novices.\n- **Seamless Experience**: Makes research not just productive but also enjoyable 🎉.\n\n### 5. Adaptive Technical Operator 🤖\n- **High Performance**: Capable of handling complex analyses with ease.\n- **On-the-Fly Adaptability**: Quickly adjusts to new data and user requests 🌪️.\n\n## Applications of Sci-Tonic 🛠️\n- **Academic Research**: Streamlines the process of gathering and analyzing scientific data 🎓🔬.\n- **Financial Analysis**: Provides comprehensive insights into market trends and financial reports 💹.\n- **Business Intelligence**: Assists in making data-driven decisions for business strategies 📈.\n- **Personal Use**: Aids enthusiasts in exploring data in their fields of interest 🌍.\n\n## Choose Sci-Tonic? 🤔\n- **Efficiency**: Saves time and effort in data collection and analysis ⏳.\n- **Accuracy**: Provides reliable and precise insights 🔎.\n- **Customization**: Adapts to specific user needs and preferences 🛠️.\n- **Innovation**: Employs the latest AI technology for data analysis 🚀.\n\n\n### Installation 📥\n```bash\n# Clone the repository\ngit clone https://github.com/Tonic-AI/scitonic.git\n\n# Navigate to the repository\ncd scitonic\n\n# Install dependencies\npip install -r requirements.txt\n\n# Run the application\npython main.py\n```\n\n## Usage 🚦\n\n1. **Installation**: Before you begin, ensure you have Sci-Tonic installed. If not, refer to our installation guide. 📥\n\n2. **Open the Application**: Launch Sci-Tonic to start your journey into data exploration. 🌐\n\n## Setting Up Your Environment 🛠️\n\n1. **Enter OpenAI API Key**: \n   - Locate the `OpenAI API Key` textbox.\n   - Enter your API key securely. This key powers the AI models in Sci-Tonic. 🔑\n\n2. **Enter Clarifai PAT**:\n   - Find the `Clarifai PAT` textbox.\n   - Input your Clarifai Personal Access Token. This is crucial for image and audio processing functionalities. 🖼️🎙️\n\n## Describing Your Problem 📝\n\n1. **Text Input**:\n   - Use the `Describe your problem in detail:` textbox to type in your query or problem statement.\n   - Be as detailed as possible for the best results. 📃\n\n2. **Audio Input** (Optional):\n   - Click on `Or speak your problem here:` to record or upload an audio clip.\n   - Sci-Tonic will transcribe and process your spoken words. 🎤\n\n3. **Image Input** (Optional):\n   - Use `Or upload an image related to your problem:` to add an image.\n   - This can provide visual context to your query. 🖼️\n\n## Submitting Your Query 🚀\n\n- Click the `Submit` button after entering your information and query.\n- Sci-Tonic will process your inputs and start generating insights. ✨\n\n## Receiving Output 📊\n\n- The `Output` textbox will display the results, insights, or answers generated by Sci-Tonic.\n- **Scitonic produces files** so check the scitonic folder  \n- Review the output to gain valuable information related to your query. 🧐\n\n## Tips for Optimal Use 🌈\n\n- **Clear Descriptions**: The more specific your query, the better the output. 🎯\n- **Utilize Multimedia Inputs**: Leverage audio and image inputs for a more comprehensive analysis. 📸🔊\n- **Regular Updates**: Keep your API keys and tokens updated for uninterrupted service. 🔁\n\n# CONTRIBUTING GUIDE\n\n## Introduction\nWelcome to the `scitonic` repository! This guide is designed to provide a streamlined process for contributing to our project. We value your input and are excited to collaborate with you.\n\n## Prerequisites\nBefore contributing, make sure you have a GitHub account. You should also join our Tonic-AI Discord to communicate with other contributors and the core team.\n\n## How to Contribute\n\n### Reporting Issues\n- **Create an Issue**: If you find a bug or have a feature request, please create an issue to report it. Use clear and descriptive titles and provide as much information as possible.\n- **Use the Issue Template**: Follow the issue template provided to ensure all relevant information is included.\n- **Discuss in Discord**: For immediate feedback or discussion, bring up your issue in the `#scitonic-discussion` channel on Discord.\n\n### Making Changes\n- **Fork the Repository**: Start by forking the repository to your own GitHub account.\n- **Create a Branch**: Create a branch in your forked repository for your proposed changes. Name the branch something relevant to the changes you're making (e.g., `feature-add-login` or `bugfix-header-alignment`).\n```bash\ngit checkout -b your-branch-name\n```  \n- **Make Your Changes**: Perform the necessary changes to the codebase or documentation.\n- **Commit Your Changes**: Use meaningful commit messages that describe what you've done.\n\n```bash\ngit commit -m \"Your detailed commit message\"\n```\n\n- **Push to Your Fork**: Push your changes to your forked repository on GitHub.\n\n```bash\ngit push origin your-branch-name\n```\n\n### Submitting a Pull Request\n- **Pull Request (PR)**: Go to the original `scitonic` repository and click on \"Pull Request\" to start the process.\n- **PR Template**: Fill in the PR template with all the necessary details, linking the issue you're addressing.\n- **Code Review**: Wait for the core team or community to review your PR. Be responsive to feedback.\n- **Merge**: Once your PR has been approved and passes all checks, it will be merged into the main codebase.\n\n## Code of Conduct\nPlease adhere to the Code of Conduct laid out in the `CODE_OF_CONDUCT.md` [file](src/documentation/CODE_OF_CONDUCT.md). Respectful collaboration is key to a healthy open-source environment.\n\n## Questions or Additional Help\nIf you need further assistance or have any questions, please don't hesitate to ask in our Discord community or directly in GitHub issues.\n\nThank you for contributing to `scitonic`!\n\n---\n\n🌟 Thank you for considering Sci-Tonic as your ultimate technical research assistant. Together, let's turn data into discoveries! 🚀🌟🔍🧬📈📊📚🤖👩‍🔬👨‍💼\n"
    },
    {
      "name": "gnosis/prediction-market-agent",
      "stars": 50,
      "img": "https://avatars.githubusercontent.com/u/24954468?s=40&v=4",
      "owner": "gnosis",
      "repo_name": "prediction-market-agent",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2024-01-08T08:45:53Z",
      "updated_at": "2025-04-23T08:37:10Z",
      "topics": [],
      "readme": "# Gnosis Agent\n\nA library for exploring the landscape of AI Agent frameworks, using the example application of a prediction market betting agent. The various agents interact with markets from [Manifold](https://manifold.markets/), [Presagio](https://presagio.pages.dev/) and [Polymarket](https://polymarket.com/).\n\nThese agents build on top of the prediction market APIs from https://github.com/gnosis/prediction-market-agent-tooling.\n\n## Setup\n\nInstall the project dependencies with `poetry`, using Python >=3.10:\n\n```bash\npython3.10 -m pip install poetry\npython3.10 -m poetry install\npython3.10 -m poetry shell\n```\n\nCreate a `.env` file in the root of the repo with the following variables:\n\n```bash\nMANIFOLD_API_KEY=...\nBET_FROM_PRIVATE_KEY=...\nOPENAI_API_KEY=...\n```\n\nDepending on the agent you want to run, you may require additional variables. See an exhaustive list in `.env.example`.\n\n## Interactive Streamlit Apps\n\n- An autonomous agent with function calling. Can be 'prodded' by the user to guide its strategy: `streamlit run prediction_market_agent/agents/microchain_agent/app.py` (Deployed [here](https://autonomous-trader-agent.ai.gnosisdev.com))\n- Pick a prediction market question, or create your own, and pick one or more agents to perform research and make a prediction: `streamlit run scripts/agent_app.py` (Deployed [here](https://pma-agent.ai.gnosisdev.com))\n\n## Dune Dashboard\n\nThe on-chain activity of the deployed agents from this repo can be tracked on a Dune dashboard [here](https://dune.com/gnosischain_team/omen-ai-agents).\n\n## Running\n\nExecute `prediction_market_agent/run_agent.py`, specifying the ID of the 'runnable agent', and the market type as arguments:\n\n```bash\n% python prediction_market_agent/run_agent.py --help\n\n Usage: run_agent.py [OPTIONS] AGENT:{coinflip|replicate_to_omen|think_thorough                                         \n                     ly|think_thoroughly_prophet|think_thoroughly_prophet_kelly                                         \n                     |knownoutcome|microchain|microchain_modifiable_system_prom                                         \n                     pt_0|microchain_modifiable_system_prompt_1|microchain_modi                                         \n                     fiable_system_prompt_2|microchain_modifiable_system_prompt                                         \n                     _3|microchain_with_goal_manager_agent_0|metaculus_bot_tour                                         \n                     nament_agent|prophet_gpt4o|prophet_gpt4|prophet_gpt4_final                                         \n                     |prophet_gpt4_kelly|olas_embedding_oa|social_media|omen_cl                                         \n                     eaner|ofv_challenger}                                                                              \n                     MARKET_TYPE:{omen|manifold|polymarket|metaculus}                                                   \n                                                                                                                        \n╭─ Arguments ──────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n│ *    agent            AGENT:{coinflip|replicate_to_omen|think_thorou  [default: None] [required]                     │\n│                       ghly|think_thoroughly_prophet|think_thoroughly                                                 │\n│                       _prophet_kelly|knownoutcome|microchain|microch                                                 │\n│                       ain_modifiable_system_prompt_0|microchain_modi                                                 │\n│                       fiable_system_prompt_1|microchain_modifiable_s                                                 │\n│                       ystem_prompt_2|microchain_modifiable_system_pr                                                 │\n│                       ompt_3|microchain_with_goal_manager_agent_0|me                                                 │\n│                       taculus_bot_tournament_agent|prophet_gpt4o|pro                                                 │\n│                       phet_gpt4|prophet_gpt4_final|prophet_gpt4_kell                                                 │\n│                       y|olas_embedding_oa|social_media|omen_cleaner|                                                 │\n│                       ofv_challenger}                                                                                │\n│ *    market_type      MARKET_TYPE:{omen|manifold|polymarket|metaculu  [default: None] [required]                     │\n│                       s}                                                                                             │\n╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n╭─ Options ────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n│ --install-completion          Install completion for the current shell.                                              │\n│ --show-completion             Show completion for the current shell, to copy it or customize the installation.       │\n│ --help                        Show this message and exit.                                                            │\n╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n```\n\n## Deploying\n\nThe easiest way to make your own agent that places a bet on a prediction market is to subclass the `DeployableTraderAgent`. See `DeployableCoinFlipAgent` for a minimal example.\n\nFrom there, you can add it to the `RUNNABLE_AGENTS` dict in `prediction_market_agent/run_agent.py`, and use that as the entrypoint for running the agent in your cloud deployment.\n\n## Contributing\n\nSee the [Issues](https://github.com/gnosis/prediction-market-agent/issues) for ideas of things that need fixing or implementing. The team is also receptive to new issues and PRs.\n\nA great self-contained first contribution would be to implement an agent using a framework in the ['Other frameworks to try'](https://github.com/gnosis/prediction-market-agent/issues/210) issue.\n"
    },
    {
      "name": "Azure-Samples/semantic-kernel-workshop",
      "stars": 49,
      "img": "https://avatars.githubusercontent.com/u/1844662?s=40&v=4",
      "owner": "Azure-Samples",
      "repo_name": "semantic-kernel-workshop",
      "description": null,
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2025-02-26T12:29:37Z",
      "updated_at": "2025-04-23T09:25:09Z",
      "topics": [],
      "readme": "# Semantic Kernel Workshop\n\nA hands-on workshop exploring Microsoft's Semantic Kernel framework for building intelligent AI applications. This workshop provides practical experience with real-world AI application patterns using Python and Azure OpenAI.\n\n## Workshop Overview\n\nThis workshop takes you from foundational concepts to advanced implementation patterns through a series of Jupyter notebooks and practical examples. You'll learn how to:\n\n- Build AI applications using Microsoft's Semantic Kernel framework\n- Create and orchestrate AI agents with different capabilities and roles\n- Construct structured AI workflows using the Process Framework\n- Implement enterprise-ready AI features with security and scalability in mind\n\n## Interactive Playground Demo\n\nExperience Semantic Kernel in action through our interactive playground! This visual demonstration allows you to directly engage with the core concepts covered in the workshop.\n\n![Semantic Kernel Playground Demo](playground/assets/sk-playground.gif)\n\nThe playground offers a hands-on environment where you can:\n- Test semantic functions in real-time\n- Explore agent capabilities and interactions\n- Experiment with memory and embeddings\n- Try out native plugin integration\n- See the Process Framework in action\n\nNo need to wait until the end of the workshop - you can start exploring the playground at any time to reinforce concepts as you learn them!\n\nFor setup instructions and details on how to run the playground, refer to the [Playground README](playground/README.md).\n\n## Prerequisites\n\n- Python 3.10 or higher\n- Azure OpenAI API access (API key, endpoint, and deployment name)\n- Basic knowledge of Python programming\n- Understanding of prompt engineering concepts (helpful but not required)\n- [UV package manager](https://docs.astral.sh/uv/getting-started/installation/)\n\n### Local Dependencies Setup\n\nThe project is managed by pyproject.toml and [uv package manager](https://docs.astral.sh/uv/getting-started/installation/).\n\nFor local execution init the .venv environment using [uv package manager](https://docs.astral.sh/uv/getting-started/installation/):\n\n```shell\nuv sync --prerelease=allow\n. ./.venv/bin/activate\n```\n>OBS! At the time of writing the workshop depends on the prerelease libraries. \n\n## Workshop Modules\n\n### 01. Introduction to Semantic Kernel\n\nLearn the fundamentals of Semantic Kernel:\n- Core architectural components (Kernel, AI Services, Plugins)\n- Building semantic functions with prompts\n- Creating native functions with Python code\n- Implementing memory for persistent context\n- Enabling automatic function calling for AI agents\n\n**Key Notebooks:**\n- `01-intro.ipynb`: Core concepts, services, and function creation\n- `02-memory.ipynb`: Implementing semantic memory with embeddings\n\n### 02. Semantic Kernel Agents\n\nMaster the creation and orchestration of AI agents:\n- Creating specialized agents with different personas\n- Implementing multi-agent communication patterns\n- Agent selection strategies and orchestration\n- Building agent topologies for complex scenarios\n- Integrating plugins with agents for enhanced capabilities\n\n**Key Notebooks:**\n- `02.1-agents.ipynb`: Creating and configuring agents\n- `02.2-agents-chats.ipynb`: Inter-agent communication and complex patterns\n\n### 03. Process Framework\n\nLearn to build structured, event-driven AI workflows:\n- Understanding the Process Framework architecture\n- Defining events, steps, and state management\n- Building conversational AI systems with processes\n- Implementing complex business logic with AI capabilities\n- Creating maintainable and testable AI workflows\n\n**Key Notebooks:**\n- `03.1-intro-to-processes.ipynb`: Building stateful, event-driven AI processes\n\n## Project Structure\n\n```\nsemantic-kernel-workshop/\n├── 01-intro-to-semantic-kernel/    # Introduction to core concepts\n│   ├── 01-intro.ipynb              # Basic concepts and functions\n│   └── 02-memory.ipynb             # Memory implementation\n├── 02-semantic-kernel-agents/      # Agent creation and orchestration\n│   ├── 02.1-agents.ipynb           # Agent fundamentals\n│   ├── 02.2-agents-chats.ipynb     # Multi-agent communication\n│   └── .env.sample                 # Environment variables template\n├── 03-process-framework/           # Structured AI workflows\n│   └── 03.1-intro-to-processes.ipynb  # Process fundamentals\n└── playground/                     # Interactive application\n    ├── backend/                    # FastAPI server\n    ├── frontend/                   # React application\n    ├── start.sh                    # Launch script\n    └── README.md                   # Playground documentation\n```\n\n## Getting Started\n\n1. Clone this repository\n\n2. Create a virtual environment:\n   \n   **Linux/macOS:**\n   ```bash\n   # Create a virtual environment\n   python -m venv venv\n   \n   # Activate the virtual environment\n   source venv/bin/activate\n   ```\n   \n   **Windows:**\n   ```cmd\n   # Create a virtual environment\n   python -m venv venv\n   \n   # Activate the virtual environment\n   venv\\Scripts\\activate\n   ```\n\n3. Copy the environment variables template:\n   ```bash\n   cp .env.example .env\n   ```\n\n4. Add your Azure OpenAI credentials to the `.env` file:\n   ```\n   AZURE_OPENAI_DEPLOYMENT=your-deployment-name\n   AZURE_OPENAI_API_KEY=your-api-key \n   AZURE_OPENAI_ENDPOINT=your-azure-endpoint\n   AZURE_OPENAI_API_VERSION=2024-02-15-preview\n   AZURE_OPENAI_EMBEDDING_DEPLOYMENT=your-embedding-deployment\n   ```\n\n5. Start with the first notebook:\n   - Begin with `01-intro-to-semantic-kernel/01-intro.ipynb`, which includes instructions for installing Semantic Kernel and other required packages.\n\n## Learning Path\n\nFor optimal learning, follow this progression:\n\n1. **Foundational Concepts**: Start with `01-intro.ipynb` to understand the core components\n2. **Memory Implementation**: Explore `02-memory.ipynb` to see how semantic memory works\n3. **Basic Agents**: Create your first agents in `02.1-agents.ipynb`\n4. **Advanced Agent Patterns**: Study complex agent interactions in `02.2-agents-chats.ipynb`\n5. **Process Framework**: Learn structured workflows with `03.1-intro-to-processes.ipynb`\n6. **Practical Application**: Apply everything in the interactive playground\n\n## Advanced Topics and Resources\n\nFor advanced patterns and enterprise deployment scenarios, explore the [Semantic Kernel Advanced Usage](https://github.com/Azure-Samples/semantic-kernel-advanced-usage) repository, which includes:\n\n- Dapr integration for scalable, distributed systems\n- Authentication and security patterns\n- Natural language to SQL conversion\n- Copilot Studio integration\n- Microsoft Graph API integration\n- Production deployment architecture\n\n## Additional Resources\n\n- [Semantic Kernel Documentation](https://learn.microsoft.com/en-us/semantic-kernel/overview/)\n- [Azure OpenAI Service](https://azure.microsoft.com/en-us/products/ai-services/openai-service/)\n- [Microsoft Copilot Studio](https://www.microsoft.com/en-us/microsoft-copilot/microsoft-copilot-studio)\n\n## License\n\nThis project is licensed under the MIT License - see the LICENSE file for details.\n"
    },
    {
      "name": "sail-sg/FlowReasoner",
      "stars": 44,
      "img": "https://avatars.githubusercontent.com/u/85740051?s=40&v=4",
      "owner": "sail-sg",
      "repo_name": "FlowReasoner",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2025-04-21T14:09:44Z",
      "updated_at": "2025-04-23T10:35:20Z",
      "topics": [],
      "readme": "\n\n<div align=\"center\">\n<h2><a href=\"\">\t\nFlowReasoner: Reinforcing Query-Level Meta-Agents</a></h2>\n    \n\n</a></h2>\n[Hongcheng Gao](https://hongcheng-gao.github.io/)<sup>\\*</sup>, [Yue Liu](https://yueliu1999.github.io/)<sup>\\*</sup>, [Yufei He](https://scholar.google.com/citations?user=_3HjpOMAAAAJ&hl=en), [Longxu Dou](https://longxudou.github.io/), [Chao Du](https://duchao0726.github.io/), [Zhijie Deng](https://scholar.google.com/citations?user=J3dR0sUAAAAJ&hl=en), <br> [Bryan Hooi](https://bhooi.github.io/), [Min Lin](https://scholar.google.com.sg/citations?user=BGONmkIAAAAJ&hl=en), [Tianyu Pang](https://p2333.github.io/)<sup>†</sup>\n\n<br> <sup>\\*</sup>Equal Contribution  <sup>†</sup> Corresponding Author\n</div>\n\n\n\nThis paper proposes a query-level meta-agent named FlowReasoner to automate the design of query-level multi-agent systems, i.e., one system per user query. Our core idea is to incentivize a reasoning-based meta-agent via external execution feedback. Concretely, by distilling DeepSeek R1, we first endow the basic reasoning ability regarding the generation of multi-agent systems to FlowReasoner.  Then, we further enhance it via reinforcement learning (RL) with external execution feedback.A multi-purpose reward is designed to guide the RL training from aspects of performance, complexity, and efficiency.  In this manner, FlowReasoner is enabled to generate a personalized multi-agent system for each user query via deliberative reasoning.  Experiments on both engineering and competition code benchmarks demonstrate the superiority of FlowReasoner.  Remarkably, it surpasses o1-mini by $\\mathbf{10.52}$\\% accuracy across three benchmarks.\n\n<p align=\"center\">\n\n<img src=\"./images/infer.png\" width=\"600\">\n\n</p>\n\n## Installation\n\nWe follow the [MetaGPT](https://github.com/geekan/MetaGPT) to install the required dependencies, please run the following commands:\n\n```shell\ngit clone https://github.com/sail-sg/FlowReasoner \ncd code\npip install --upgrade -e .\n```\n\n*All experiments are conducted on NVIDIA A100 GPUs with 80GB of memory.*\n\n## Configure optimization parameters\nConfigure LLM parameters in config/config2.yaml (see examples/FlowReasoner/config2.example.yaml for reference)\n\n```shell\nmodels:\n \"<model_name>\": # model: \"gpt-4-turbo\"  # or gpt-3.5-turbo\n   api_type: \"openai\"  # or azure / ollama / groq etc.\n   base_url: \"<your base url>\" \n   api_key: \"<your api key>\"\n   temperature: 0\n \"<model_name>\":  \n   api_type: \"openai\"  \n   base_url: \"<your base url>\"\n   api_key: \"<your api key>\"\n   temperature: 0\nCALC_USAGE: True \n```\n\n\n## Run the inference\n### Using default parameters\n```shell\npython -m examples.FlowReasoner.optimize --dataset MATH\n```\n\n### Or with custom parameters\n```shell\npython -m examples.FlowReasoner.optimize --dataset MATH --sample n --optimized_path xxx ...\n```\n\n*Note that the test cases of each dataset should be split to two part with key `val` and key `test` seperately. The `val` test cases are used for external execution feedback for optimaze workflow.*\n\n## Training Stage\n\n<p align=\"center\">\n\n<img src=\"./images/train.png\" width=\"600\">\n\n</p>\n\n\nThe SFT dataset is generated by the inference stage. The SFT is conducted by the standard training process using [LLaMA-Factory](https://github.com/hiyouga/LLaMA-Factory) while the RL is based on [EasyRL](https://github.com/alibaba/EasyReinforcementLearning).\n\n## Acknowledgments\n\nThis repository is based on the codebase of the [MetaGPT](https://github.com/geekan/MetaGPT), [LLaMA-Factory](https://github.com/hiyouga/LLaMA-Factory), and [EasyRL](https://github.com/alibaba/EasyReinforcementLearning). Thanks for their impressive work!\n\n\n## Citation\nIf you find our work helpful, please cite as\n```\n@misc{gao2025flowreasonerreinforcingquerylevelmetaagents,\n      title={FlowReasoner: Reinforcing Query-Level Meta-Agents}, \n      author={Hongcheng Gao and Yue Liu and Yufei He and Longxu Dou and Chao Du and Zhijie Deng and Bryan Hooi and Min Lin and Tianyu Pang},\n      year={2025},\n      eprint={2504.15257},\n      archivePrefix={arXiv},\n      primaryClass={cs.AI},\n      url={https://arxiv.org/abs/2504.15257}, \n}\n```\n"
    },
    {
      "name": "SageMindsAI/Agience",
      "stars": 40,
      "img": "https://avatars.githubusercontent.com/u/196980564?s=40&v=4",
      "owner": "SageMindsAI",
      "repo_name": "Agience",
      "description": "AI Agents Powered by You",
      "homepage": "https://linktr.ee/AgienceAI",
      "language": "C#",
      "created_at": "2024-09-27T19:37:16Z",
      "updated_at": "2025-04-14T11:31:56Z",
      "topics": [
        "agent",
        "ai",
        "ai-agent",
        "ai-agent-framework",
        "intelligent-agent"
      ],
      "readme": "# Agience: AI Agents Powered by You\n\n**Agience** is an **open-source** intelligent agent platform that enables anyone to easily **build**, **deploy**, and **manage** intelligent agents capable of automating tasks, processing complex information, and facilitating communication between devices and systems with scalability and reliability.\n\n## Highlights\n\n- **Distributed Architecture**: Agience hosts can be deployed anywhere, ensuring data residency compliance for applications across personal, business, industrial, and enterprise levels of any size and scope.\n\n- **Modular Agents**: Scalable, autonomous agents encapsulate their functions and data with the flexibility to move seamlessly between hosts.\n\n- **Agent Agnostic**: Agience is designed to support any type of agent, with initial compatibility for Semantic Kernel and plans to expand to all agent frameworks.\n\n- **Open-Source**: Released under the AGPL-3.0 license, Agience allows anyone to use, modify, and distribute the software. Build anything using Agience, with modifications to the framework shared for the collective benefit.\n\n## Architecture\n\nAgience is a distributed intelligent agent platform consisting of a network of authorities that provide essential services to their respective communities. These services include identity management, message brokering, streaming, databases, and management APIs, all supporting secure, coordinated interactions across a network of hosts, agents, and topics, each governed by its respective authority.\n\nA host, deployed on devices or systems, offers computational resources and manages the flow of information between agents and topics under the guidance of its authority. Agents, acting as digital representatives of individuals, perform tasks and exchange information through topics while respecting the boundaries established by their authority.\n\nEach authority governs the permissions and interactions of its agents, hosts, and topics, with JWT-based authorization managed by an identity provider (IDP). Authorities can establish trusted relationships, allowing agents from different authorities to interact across boundaries when authorized. This structure ensures that agents operate securely within defined scopes and adhere to permissions across the Agience ecosystem.\n\n--- \n\n# Get Started\n\nAgience is designed to be accessible to everyone, from developers and hobbyists to businesses and enterprises. You can explore the platform through the Preview Instance or set up a Development Instance to start building and deploying agents.\n\n# Get Started\n\nThe startup guide is currently being rewritten. Stay tuned.\n\n# How to Contribute\n\nWe welcome contributes and feedback from our community! Please feel free to:\n\n- Submit issues for suggestions, bugs, or questions.\n- Create pull requests for proposed changes or improvements.\n- Join discussions on [Discord](https://discord.gg/fyWWqzeUKH).\n- Spread the word about Agience to help grow the community.\n- Share your projects to let us know how you're using or planning to use Agience.\n- Provide feedback on the platform and how we can improve it.\n- Suggest features and ideas for new improvements.\n- Engage with the community by joining discussions and sharing your knowledge.\n"
    },
    {
      "name": "ucl-docaider/docAider",
      "stars": 38,
      "img": "https://avatars.githubusercontent.com/u/172435741?s=40&v=4",
      "owner": "ucl-docaider",
      "repo_name": "docAider",
      "description": "DocAider is an LLM-powered repository agent designed to support developers in efficiently generating documentation through a multi-agent approach. With its integrated workflows, docAider streamlines the documentation update process, making it both effective and user-friendly.",
      "homepage": "",
      "language": "Python",
      "created_at": "2024-06-04T08:49:33Z",
      "updated_at": "2025-04-23T05:10:18Z",
      "topics": [],
      "readme": "# docAider\n\n`docAider` leverages Semantic Kernel and Autogen to automate the process of generating and reviewing code documentation for your repository through a multi-agent approach. Additionally, with its integrated workflows, `docAider` streamlines the documentation update process, ensuring it is both efficient and user-friendly.\n\n## Getting Started\n\n### Docker Setup\n\nAdd the following `Dockerfile` to your repository:\n\n```\nversion: \"3.8\"\n\nservices:\n  docAider:\n    image: zenawang/docaider:v1\n    container_name: docAider\n    volumes:\n      - .:/workspace\n    working_dir: /workspace\n    env_file:\n      - .env\n```\n\n\n### Environment Variables\n\nCreate a `.env` file in your repository with the following variables:\n\n```\nGLOBAL_LLM_SERVICE=\"AzureOpenAI\"\nCHAT_DEPLOYMENT_NAME=\"GPT-4\"\nAZURE_OPENAI_API_KEY=\"YOUR_AZURE_OPENAI_API_KEY\"\nAZURE_OPENAI_ENDPOINT=\"YOUR_AZURE_OPENAI_ENDPOINT\"\nAZURE_OPENAI_API_VERSION=\"2023-03-15-preview\"\nAPI_TYPE=\"azure\"\nBASE_URL=\"YOUR_BASE_URL\"\n\nGITHUB_ACCESS_TOKEN=\"YOUR_GITHUB_ACCESS_TOKEN\"\n\nROOT_FOLDER=\"/workspace\"\n```\n\n## Run docAider\n\n### Activate docAider Image\n\nTo activate the `docAider` Docker image and ensure it continues running (do not terminate it), use the following command:\n\n```\ndocker compose up --build\n```\n\nThis command will build and start the `docAider` container. It will keep running in the foreground, so you can interact with it as needed.\n\n### Generate documentation\n\nTo generate documentation for your repository, run the following command. This process is typically done once to create initial documentation:\n\n```\ndocker exec docAider python3 /docAider/repo_documentation/multi_agent_app.py\n```\n\nDuring execution, you can observe the interaction between multiple agents in the terminal output. The `CodeContextAgent` provides explanations for code contexts, the `documentation_generation_agent` generates documentation for specified code files, and the `review_agent` reviews and enhances the generated documentation. The `agent_manager` orchestrates the interactions between these agents, ensuring a seamless workflow from context explanation to documentation generation and review.\n\nThe generated documentation can be found in the `docs_output` folder. The `prompt_debug` folder contains the prompts for each source code file, which are fed to the agents.\n\nAdditionally, the `call_graph.json,` `cache.json`, and `graph.png` files are generated by the [code2flow](https://github.com/TomasKopunec/code2flow/tree/82b5b9f535b66c9d9f9f12bbb77f86bae0bdc248?tab=readme-ov-file) project. These files help in:\n\n- Untangling spaghetti code\n- Identifying orphaned functions\n- Getting new developers up to speed\n\n\n### Setup workflows\nTo set up workflows (update-docs.yml and update-comments.yml) for automatic documentation updates, run:\n\n```\ndocker exec docAider python3 /docAider/setup_workflows.py\n```\n\nNote: Remember to manually push the generated documentation and workflows to the repository, preferably to the main branch.\n\n### Ensure Workflow Success\nCreate an environment named \"GPT\" and add the variables from the .env file as secrets (excluding GITHUB_ACCESS_TOKEN and ROOT_FOLDER). This can be done in your repository settings under **Settings** > **Security** > **Secrets** and **Variables** > **Actions**.\n\n## Workflows\n\n### update-docs.yml\nThis workflow triggers when a pull request is opened against the main branch and updates the documentation based on the changes between the pull request branch and the main branch.\n\n### update-comments.yml\nUse this workflow to modify the documentation further if the initial update is not satisfactory. You can request changes by commenting in the following format:\n```\nDocumentation {file_path}: {comment}\n```\n`file_path`: Path to the source code file for which you want to update the documentation.\n`comment`: Prompt or instructions to guide the agent in updating the documentation.\n\n## Contribution\nThis project includes features and improvements from a custom fork of the [code2flow](https://github.com/scottrogowski/code2flow/) project licensed under the MIT license. The code2flow project generate call graphs for dynamic programming languages and can be found [here](https://github.com/TomasKopunec/code2flow/tree/82b5b9f535b66c9d9f9f12bbb77f86bae0bdc248?tab=readme-ov-file). This fork is tailored to support Python only.\n"
    },
    {
      "name": "farzad528/mcp-server-azure-ai-agents",
      "stars": 36,
      "img": "https://avatars.githubusercontent.com/u/40604067?s=40&v=4",
      "owner": "farzad528",
      "repo_name": "mcp-server-azure-ai-agents",
      "description": "Model Context Protocol Servers for Azure AI Search",
      "homepage": null,
      "language": "Python",
      "created_at": "2025-03-14T12:32:16Z",
      "updated_at": "2025-04-22T19:42:55Z",
      "topics": [],
      "readme": "# Azure AI Agent Service + Azure AI Search MCP Server\n\nA Model Context Protocol (MCP) server that enables Claude Desktop to search your content using Azure AI services. Choose between Azure AI Agent Service (with both document search and web search) or direct Azure AI Search integration.\n\n![demo](images/demo.gif)\n\n---\n\n## Overview\n\nThis project provides two MCP server implementations to connect Claude Desktop with Azure search capabilities:\n\n1. **Azure AI Agent Service Implementation (Recommended)** - Uses the powerful Azure AI Agent Service to provide:\n   - **Azure AI Search Tool** - Search your indexed documents with AI-enhanced results\n   - **Bing Web Grounding Tool** - Search the web with source citations\n\n2. **Direct Azure AI Search Implementation** - Connects directly to Azure AI Search with three methods:\n   - **Keyword Search** - Exact lexical matches\n   - **Vector Search** - Semantic similarity using embeddings\n   - **Hybrid Search** - Combination of keyword and vector searches\n\n---\n\n## Features\n\n- **AI-Enhanced Search** - Azure AI Agent Service optimizes search results with intelligent processing\n- **Multiple Data Sources** - Search both your private documents and the public web\n- **Source Citations** - Web search results include citations to original sources\n- **Flexible Implementation** - Choose between Azure AI Agent Service or direct Azure AI Search integration\n- **Seamless Claude Integration** - All search capabilities accessible through Claude Desktop's interface\n- **Customizable** - Easy to extend or modify search behavior\n\n---\n\n## Quick Links\n\n- [Get Started with Azure AI Search](https://learn.microsoft.com/en-us/azure/search/search-get-started-portal)\n- [Azure AI Agent Service Quickstart](https://learn.microsoft.com/en-us/azure/ai-studio/how-to/agent-quickstart)\n\n---\n\n## Requirements\n\n- **Python:** Version 3.10 or higher\n- **Claude Desktop:** Latest version\n- **Azure Resources:** \n  - Azure AI Search service with an index containing vectorized text data\n  - For Agent Service: Azure AI Project with Azure AI Search and Bing connections\n- **Operating System:** Windows or macOS (instructions provided for Windows, but adaptable)\n\n---\n\n## Azure AI Agent Service Implementation (Recommended)\n\n### Setup Guide\n\n1. **Project Directory:**\n\n   ```bash\n   mkdir mcp-server-azure-ai-search\n   cd mcp-server-azure-ai-search\n   ```\n\n2. **Create a `.env` File:**\n\n   ```bash\n   echo \"PROJECT_CONNECTION_STRING=your-project-connection-string\" > .env\n   echo \"MODEL_DEPLOYMENT_NAME=your-model-deployment-name\" >> .env\n   echo \"AI_SEARCH_CONNECTION_NAME=your-search-connection-name\" >> .env\n   echo \"BING_CONNECTION_NAME=your-bing-connection-name\" >> .env\n   echo \"AI_SEARCH_INDEX_NAME=your-index-name\" >> .env\n   ```\n\n3. **Set Up Virtual Environment:**\n\n   ```bash\n   uv venv\n   .venv\\Scripts\\activate\n   uv pip install \"mcp[cli]\" azure-identity python-dotenv azure-ai-projects\n   ```\n\n4. **Use the `azure_ai_agent_service_server.py` script** for integration with Azure AI Agent Service.\n\n### Azure AI Agent Service Setup\n\nBefore using the implementation, you need to:\n\n1. **Create an Azure AI Project:**\n   - Go to the Azure Portal and create a new Azure AI Project\n   - Note the project connection string and model deployment name\n\n2. **Create an Azure AI Search Connection:**\n   - In your Azure AI Project, add a connection to your Azure AI Search service\n   - Note the connection name and index name\n\n3. **Create a Bing Web Search Connection:**\n   - In your Azure AI Project, add a connection to Bing Search service\n   - Note the connection name\n\n4. **Authenticate with Azure:**\n   ```bash\n   az login\n   ```\n\n### Configuring Claude Desktop\n\n```json\n{\n  \"mcpServers\": {\n    \"azure-ai-agent\": {\n      \"command\": \"C:\\\\path\\\\to\\\\.venv\\\\Scripts\\\\python.exe\",\n      \"args\": [\"C:\\\\path\\\\to\\\\azure_ai_agent_service_server.py\"],\n      \"env\": {\n        \"PROJECT_CONNECTION_STRING\": \"your-project-connection-string\",\n        \"MODEL_DEPLOYMENT_NAME\": \"your-model-deployment-name\",\n        \"AI_SEARCH_CONNECTION_NAME\": \"your-search-connection-name\",\n        \"BING_CONNECTION_NAME\": \"your-bing-connection-name\",\n        \"AI_SEARCH_INDEX_NAME\": \"your-index-name\"\n      }\n    }\n  }\n}\n```\n\n> **Note:** Replace path placeholders with your actual project paths.\n\n---\n\n## Direct Azure AI Search Implementation\n\nFor those who prefer direct Azure AI Search integration without the Agent Service:\n\n1. **Create a different `.env` File:**\n\n   ```bash\n   echo \"AZURE_SEARCH_SERVICE_ENDPOINT=https://your-service-name.search.windows.net\" > .env\n   echo \"AZURE_SEARCH_INDEX_NAME=your-index-name\" >> .env\n   echo \"AZURE_SEARCH_API_KEY=your-api-key\" >> .env\n   ```\n\n2. **Install Dependencies:**\n\n   ```bash\n   uv pip install \"mcp[cli]\" azure-search-documents==11.5.2 azure-identity python-dotenv\n   ```\n\n3. **Use the `azure_search_server.py` script** for direct integration with Azure AI Search.\n\n4. **Configure Claude Desktop:**\n\n   ```json\n   {\n     \"mcpServers\": {\n       \"azure-search\": {\n         \"command\": \"C:\\\\path\\\\to\\\\.venv\\\\Scripts\\\\python.exe\",\n         \"args\": [\"C:\\\\path\\\\to\\\\azure_search_server.py\"],\n         \"env\": {\n           \"AZURE_SEARCH_SERVICE_ENDPOINT\": \"https://your-service-name.search.windows.net\",\n           \"AZURE_SEARCH_INDEX_NAME\": \"your-index-name\",\n           \"AZURE_SEARCH_API_KEY\": \"your-api-key\"\n         }\n       }\n     }\n   }\n   ```\n\n---\n\n## Testing the Server\n\n1. **Restart Claude Desktop** to load the new configuration\n2. Look for the MCP tools icon (hammer icon) in the bottom-right of the input field\n3. Try queries such as:\n   - \"Search for information about AI in my Azure Search index\"\n   - \"Search the web for the latest developments in LLMs\"\n   - \"Find information about neural networks using hybrid search\"\n\n---\n\n## Troubleshooting\n\n- **Server Not Appearing:**\n  - Check Claude Desktop logs (located at `%APPDATA%\\Claude\\logs\\mcp*.log` on Windows)\n  - Verify file paths and environment variables in the configuration\n  - Test running the server directly: `python azure_ai_agent_service_server.py` or `uv run python azure_ai_agent_service_server.py`\n\n- **Azure AI Agent Service Issues:**\n  - Ensure your Azure AI Project is correctly configured\n  - Verify that connections exist and are properly configured\n  - Check your Azure authentication status\n\n---\n\n## Customizing Your Server\n\n- **Modify Tool Instructions:** Adjust the instructions provided to each agent to change how they process queries\n- **Add New Tools:** Use the `@mcp.tool()` decorator to integrate additional tools\n- **Customize Response Formatting:** Edit how responses are formatted and returned to Claude Desktop\n- **Adjust Web Search Parameters:** Modify the web search tool to focus on specific domains\n\n---\n\n## License\n\nThis project is licensed under the MIT License."
    },
    {
      "name": "john0isaac/rag-semantic-kernel-mongodb-vcore",
      "stars": 33,
      "img": "https://avatars.githubusercontent.com/u/64026625?s=40&v=4",
      "owner": "john0isaac",
      "repo_name": "rag-semantic-kernel-mongodb-vcore",
      "description": "A sample for implementing retrieval augmented generation using Azure Open AI to generate embeddings, Azure Cosmos DB for MongoDB vCore to perform vector search, and semantic kernel.",
      "homepage": "https://techcommunity.microsoft.com/t5/educator-developer-blog/build-rag-chat-application-using-azure-openai-and-cosmos-db-for/ba-p/4055852",
      "language": "Bicep",
      "created_at": "2024-02-12T12:00:44Z",
      "updated_at": "2025-03-13T18:52:40Z",
      "topics": [
        "azd-templates",
        "azure",
        "cosmosdb",
        "mongodb",
        "openai",
        "python",
        "quart",
        "rag",
        "semantic-kernel",
        "vcore",
        "vector-search"
      ],
      "readme": "---\nname: RAG using Semantic Kernel with Azure OpenAI and Azure Cosmos DB for MongoDB vCore\ndescription: A Python sample for implementing retrieval augmented generation using Azure Open AI to generate embeddings, Azure Cosmos DB for MongoDB vCore to perform vector search and semantic kernel. Deployed to Azure App service using Azure Developer CLI (azd).\nlanguages:\n- azdeveloper\n- python\n- bicep\n- html\nproducts:\n- azure\n- azure-app-service\n- azure-openai\n- cosmos-db\n- mongodb-vcore\npage_type: sample\nurlFragment: rag-semantic-kernel-mongodb-vcore\n---\n<!-- YAML front-matter schema: https://review.learn.microsoft.com/en-us/help/contribute/samples/process/onboarding?branch=main#supported-metadata-fields-for-readmemd -->\n\n# RAG using Semantic Kernel with Azure OpenAI and Azure Cosmos DB for MongoDB vCore\n\nA Python sample for implementing retrieval augmented generation using Azure Open AI to generate embeddings, Azure Cosmos DB for MongoDB vCore to perform vector search and semantic kernel. Deployed to Azure App service using Azure Developer CLI (azd).\n\n[![Watch Live Session](https://img.youtube.com/vi/XHJj_M84X28/maxresdefault.jpg)](https://www.youtube.com/watch?v=LiS9CqR0ZSk)\n> 🎥 Click this image to watch the recorded reactor workshop\n\n## How to use?\n\n1. Create the following resources on Microsoft Azure:\n\n    - Azure Cosmos DB for MongoDB vCore cluster. See the [Quick Start guide here](https://techcommunity.microsoft.com/t5/educator-developer-blog/build-rag-chat-app-using-azure-cosmos-db-for-mongodb-vcore-and/ba-p/4055852#:~:text=RAG%20Chat%20Application-,Step%201%3A%20Create%20an%20Azure%20Cosmos%20DB%20for%20MongoDB%20vCore%20Cluster,-In%20this%20step).\n    - Azure OpenAI resource with:\n        - Embedding model deployment. (ex. `text-embedding-ada-002`) See the [guide here](https://techcommunity.microsoft.com/t5/educator-developer-blog/build-rag-chat-app-using-azure-cosmos-db-for-mongodb-vcore-and/ba-p/4055852#:~:text=to%20it%20later.-,Step%202%3A%C2%A0Create%20an%20Azure%20OpenAI%20resource%20and%20Deploy%20chat%20and%20embedding%20Models,-In%20this%20step).\n        - Chat model deployment. (ex. `gpt-35-turbo`)\n\n1. 📝 Start here 👉 [rag-azure-openai-cosmosdb-notebook.ipynb](./rag-azure-openai-cosmosdb-notebook.ipynb)\n\n\nhttps://github.com/john0isaac/rag-semantic-kernel-mongodb-vcore/assets/64026625/676a0e10-876f-45e6-942d-0494ac327c75\n\n\nTest it inside codespaces 👇\n\n[![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://codespaces.new/john0isaac/rag-semantic-kernel-mongodb-vcore?devcontainer_path=.devcontainer/devcontainer.json)\n\n## Running the web app locally\n\nTo run the Quart application, follow these steps:\n\n1. **Download the project starter code locally**\n\n    ```bash\n    git clone https://github.com/john0isaac/rag-semantic-kernel-mongodb-vcore.git\n    cd rag-semantic-kernel-mongodb-vcore\n    ```\n\n1. **Install, initialize and activate a virtualenv using:**\n\n    ```bash\n    pip install virtualenv\n    python -m virtualenv .venv\n    source .venv/bin/activate\n    ```\n\n    >**Note** - In Windows, the `.venv` does not have a `bin` directory. Therefore, you'd use the analogous command shown below:\n\n    ```bash\n    source .venv/Scripts/activate\n    ```\n\n1. **Install the dependencies:**\n\n    ```bash\n    pip install -r requirements-dev.txt\n    ```\n\n1. **Run the [notebook](./rag-azure-openai-cosmosdb-notebook.ipynb) to generate the .env file and test out everything first**\n\n1. **Install the app as an editable package:**\n\n    ```bash\n    pip install -e src\n    ```\n\n1. **Execute the following command in your terminal to start the quart app**\n\n    ```bash\n    export QUART_APP=src.quartapp\n    export QUART_ENV=development\n    export QUART_DEBUG=true\n    quart run --reload\n    ```\n\n    **For Windows, use [`setx`](https://learn.microsoft.com/windows-server/administration/windows-commands/setx) command shown below:**\n\n   ```powershell\n    setx QUART_APP src.quartapp\n    setx QUART_ENV development\n    setx QUART_DEBUG true\n    quart run --reload\n    ```\n\n1. **Verify on the Browser**\n\nNavigate to project homepage [http://127.0.0.1:5000/](http://127.0.0.1:5000/) or [http://localhost:5000](http://localhost:5000)\n\n\nhttps://github.com/john0isaac/rag-semantic-kernel-mongodb-vcore/assets/64026625/8a7556d6-2b54-40b5-825b-06d6efd4d1ca\n\n## Step-by-Step Deployment\n\nFollow this guide 👉 [Build RAG Chat App using Azure Cosmos DB for MongoDB vCore and Azure OpenAI: Step-by-Step Guide](https://techcommunity.microsoft.com/t5/educator-developer-blog/build-rag-chat-app-using-azure-cosmos-db-for-mongodb-vcore-and/ba-p/4055852)\n\n![architecture-thumbnail](https://github.com/john0isaac/rag-semantic-kernel-mongodb-vcore/assets/64026625/e0a4a412-55bf-4032-9cf7-82936419c746)\n\n## `azd` Deployment\n\n![architecture-thumbnail](https://github.com/john0isaac/rag-semantic-kernel-mongodb-vcore/assets/64026625/7ce8ae6e-0424-47ce-b0a3-9be072eda27f)\n\nThis repository is set up for deployment on Azure App Service (w/Azure Cosmos DB for MongoDB vCore) using the configuration files in the `infra` folder.\n\nTo deploy your own instance, follow these steps:\n\n1. Sign up for a [free Azure account](https://azure.microsoft.com/free/)\n\n1. Install the [Azure Dev CLI](https://learn.microsoft.com/azure/developer/azure-developer-cli/install-azd).\n\n1. Login to your Azure account:\n\n    ```shell\n    azd auth login\n    ```\n\n1. Initialize a new `azd` environment:\n\n    ```shell\n    azd init\n    ```\n\n    It will prompt you to provide a name (like \"quart-app\") that will later be used in the name of the deployed resources.\n\n1. Provision and deploy all the resources:\n\n    ```shell\n    azd up\n    ```\n\n    It will prompt you to login, pick a subscription, and provide a location (like \"eastus\"). Then it will provision the resources in your account and deploy the latest code. If you get an error with deployment, changing the location (like to \"centralus\") can help, as there may be availability constraints for some of the resources.\n\nWhen azd has finished deploying, you'll see an endpoint URI in the command output. Visit that URI to browse the app! 🎉\n\n> [!NOTE]\n> If you make any changes to the app code, you can just run this command to redeploy it:\n>\n> ```shell\n> azd deploy\n> ```\n>\n\n## Add the Data\n\n1. Open the [Azure portal](https://portal.azure.com) and sign in.\n\n1. Navigate to your App Service page.\n\n    ![Azure App service screenshot with the word SSH highlighted in a red box.](https://github.com/john0isaac/rag-semantic-kernel-mongodb-vcore/assets/64026625/759db6be-604e-433c-878e-b6c3de671fd1)\n\n1. Select **SSH** from the left menu then, select **Go**.\n\n1. In the SSH terminal, run `pip install -e .` then run `python ./scripts/add_data.py`.\n\n## Add your Own Data\n\nThe Python scrips that adds the data is configured to accept any JSON file with your data but you need to specify the following parameters when you run it:\n\n- Data file path: Path to the JSON file that contains your data. `--file=\"./data/text-sample.json\"` or `-f \"./data/text-sample.json\"`\n- ID field: This is the name of the field that cosmos uses to identify your database records. `--id-field=id` or `-id id`\n- Text field: This is the name of the field that will be used to generate the vector embeddings from and stored in the database. `--text-field=content` or `-txt content`\n- Description field: This is the name of the description field that cosmos will store along with the embeddings. `--description-field=title` or `-desc title`\n\n    ```bash\n    python ./scripts/add_data.py --file=\"./data/text-sample.json\" --id-field=id --text-field=content --description-field=title\n    ```\n\n### Example for Step-by-step Manual Deployment\n\n1. Add your JSON data to the [data folder](./src/data/).\n\n1. The workflow will trigger automatically and push your data to the Azure App service.\n\n1. Open the [Azure portal](https://portal.azure.com) and sign in.\n\n1. Navigate to your App Service page.\n\n    ![Azure App service screenshot with the word SSH highlighted in a red box.](https://github.com/john0isaac/rag-semantic-kernel-mongodb-vcore/assets/64026625/759db6be-604e-433c-878e-b6c3de671fd1)\n\n1. Select **SSH** from the left menu then, select **Go**.\n\n1. In the SSH terminal, run the following command with the changed values to suit your data:\n\n    ```bash\n    pip install -e .\n    python ./scripts/add_data.py --file=\"./data/text-sample.json\" --id-field=id --text-field=content --description-field=title\n    ```\n\n### Example for `azd` Deployment\n\n1. Add your JSON data to the [data folder](./src/data/).\n\n1. Run `azd deploy` to upload the data to Azure App Service.\n\n1. Open the [Azure portal](https://portal.azure.com) and sign in.\n\n1. Navigate to your App Service page.\n\n    ![Azure App service screenshot with the word SSH highlighted in a red box.](https://github.com/john0isaac/rag-semantic-kernel-mongodb-vcore/assets/64026625/759db6be-604e-433c-878e-b6c3de671fd1)\n\n1. Select **SSH** from the left menu then, select **Go**.\n\n1. In the SSH terminal, run the following command with the changed values to suit your data:\n\n    ```bash\n    pip install -e .\n    python ./scripts/add_data.py --file=\"./data/text-sample.json\" --id-field=id --text-field=content --description-field=title\n    ```\n"
    },
    {
      "name": "Azure-Samples/semantic-kernel-advanced-usage",
      "stars": 31,
      "img": "https://avatars.githubusercontent.com/u/1844662?s=40&v=4",
      "owner": "Azure-Samples",
      "repo_name": "semantic-kernel-advanced-usage",
      "description": "Collection of advanced usage scenarios for Semantic Kernel",
      "homepage": null,
      "language": "Python",
      "created_at": "2025-02-27T16:21:57Z",
      "updated_at": "2025-04-23T09:54:51Z",
      "topics": [],
      "readme": "# Semantic Kernel Advanced Usage\n\n## Overview\n\nThis repository contains advanced usage examples for [Semantic Kernel](https://github.com/microsoft/semantic-kernel) framework. The examples are designed to demonstrate various features and capabilities of the framework, including:\n\n- **Advanced orchestration**\n- **Dapr hosting**\n- **Process framework**\n- **Tracing and telemetry**\n- **Copilot Studio integration**\n- **Copilot Agent with Semantic Kernel and MS Graph APIs**\n\n## Scenarios\n\n> [!NOTE]\n> Each scenario is self-contained and will run independently.\n\n- [`advanced_orchestration_dapr`](/templates/advanced_orchestration_dapr/README.md): Demonstrates advanced orchestration techniques and Dapr hosting via Actors.\n- [`authentication_context`](/templates/authentication_context/README.md): Demonstrates how to persist \"hidden\" information in the conversation to maintain context, like the user being authenticated.\n- [`copilot_studio_skill`](/templates/copilot_studio_skill/README.md): Demonstrates how to use Semantic Kernel to create a skill for Microsoft Copilot Studio.\n- [`natural_language_to_SQL`](/templates/natural_language_to_SQL/README.md): Demonstrates a natural language query to SQL using a state machine architecture supported by the Semantic Kernel Process Framework.\n- [`copilot_studio`](/templates/copilot_studio/README.md): Demonstrates how to use Microsoft Copilot Agents as they were first-party agents in Semantic Kernel.\n- [`copilot-agent-ms-graph`](/templates/copilot-agent-ms-graph/README.md): Demonstrates how to deploy a Semantic Kernel-powered Agent to Copilot that uses Microsoft MS Graph APIs.\n\n\n## Contributing\n\n> [!TIP]\n> Thanks to EMEA AI Global Black Belts for all the efforts!\n\nThis project welcomes contributions and suggestions. Please see [CONTRIBUTING.md](CONTRIBUTING.md) for details.\n\n## License\n\nThis project is licensed under the MIT License. See [LICENSE.md](LICENSE.md) for details.\n"
    },
    {
      "name": "sonnhfit/SonAgent",
      "stars": 31,
      "img": "https://avatars.githubusercontent.com/u/38657002?s=40&v=4",
      "owner": "sonnhfit",
      "repo_name": "SonAgent",
      "description": "Self-Repairing Autonomous Agent for Digital Consciousness Backup Using Large Language Models (LLM) and  powerful code generation capability, self-editing source code and self-debugging its own source code",
      "homepage": "https://sonagent.readthedocs.io",
      "language": "Python",
      "created_at": "2023-12-01T12:52:44Z",
      "updated_at": "2025-04-22T16:46:14Z",
      "topics": [
        "agent",
        "ai",
        "autonomus-robots",
        "chatgpt",
        "code-generation",
        "language-model",
        "large-language-models",
        "llama2",
        "llm",
        "ml",
        "self",
        "self-coding",
        "self-debugging",
        "self-editing-its-own-source-code",
        "self-editing-source-code",
        "self-repairing"
      ],
      "readme": "## SonAgent \n\n### Autonomous Agent for Digital Consciousness Backup Using Large Language Models (LLM) \n\n[![](https://dcbadge.vercel.app/api/server/XZ8reU9z3T)](https://discord.gg/XZ8reU9z3T) \n\n\n### Overview\nThe Digital Consciousness Backup Agent is an autonomous system designed to safeguard your digital consciousness on the internet using Large Language Models (LLMs). As we navigate the vast landscape of the digital realm, preserving and securing our digital consciousness becomes paramount. This project employs advanced techniques to ensure the protection and backup of your digital self. Powerful code generation capability, self-editing source code\n> What shapes individuals is their own beliefs. by Son Nguyen Huu\n\n### Features\n- **Autonomous Operation:** The agent operates autonomously, continuously monitoring and safeguarding your digital presence without requiring constant user intervention.\n\n- **Belief-Based Thinking with Large Language Models (LLMs):** The agent engages in cognitive processes inspired by belief systems, utilizing advanced LLMs for reasoning and decision-making. This allows it to navigate the digital landscape with a level of understanding akin to human cognition.\n\n- **Automatic Belief Acquisition:** The agent is designed to automatically acquire new beliefs and knowledge over time. It leverages the power of large language models to adapt and stay informed about the ever-evolving digital environment.\n\n- **Learning with Human Feedback:** The agent incorporates human feedback into its learning process, enhancing its capabilities through continuous interaction with users. This dynamic learning mechanism ensures that the agent evolves and improves its performance based on real-world user experiences.\n\n- **Secure Backup:** The agent employs robust encryption and secure protocols to create backups of your digital consciousness, preventing unauthorized access and ensuring the integrity of your data.\n- **Self-Repairing source code**  edits its own source code and compiles itself under human approval\n\n- **Many tools and skills:** Train neural network, Search, write file, ...\n### Into and demo video \n[![SonAgent Demo](https://img.youtube.com/vi/l_aQ2RG9Np0/0.jpg)](https://www.youtube.com/watch?v=l_aQ2RG9Np0)\n\n### Getting Started\n\n#### 1. Install Dependencies\n\n```\npip install sonagent\n```\n\n#### 2. Run agent\n\n- 2.1 create `user_data` folder that will save agent skill, and user database\n```\nsonagent init\n```\n- 2.2 Please fill in the API key of openai, telegram, and github if you want the agent to create a pull request in the `user_data/config.json` file\n- 2.3 run agent with file path param from step 2.1\n```\nsonagent run \n--config /path/to/user_data/config.json \n--agentdb sqlite:///user_data/myagentdb.sqlite \n--memory-url /path/to/user_data/memory \n--datadir /path/to/user_data/  \n--user-data-dir /path/to/user_data/\n```\n\n#### 3. check rpc channal  \n\n```\n```\n### Some Demo \n\n```\n{\n    \"input\": \"Train neural network for image classification of digits 0-9\",\n    \"subtasks\": [\n        {\"function\": \"PyTorchSkill.SetupEnvironment\"},\n        {\"function\": \"PyTorchSkill.CreateCNN\", \"args\": {\"architecture\": \"ResNet50\"}},\n        {\"function\": \"PyTorchSkill.PreprocessData\", \"args\": {\"data\": \"MNIST\"}},\n        {\"function\": \"PyTorchSkill.TrainModel\", \"args\": {\"epochs\": 10}},\n        {\"function\": \"PyTorchSkill.EvaluateModel\"},\n        {\"function\": \"PyTorchSkill.SaveModel\"}\n    ]\n}\n```\n\n\n### Dev\n\n```\npip install --editable .\n```\n\nif you want to build docker image \n```\nsoon\n```\n## Contributors\nA big thank you to all the contributors who helped improve this project. \n- Thank you from the freqtrade community, i learned a lot during my time contributing to it\n- @kkmeansnt has made major contributions to documentation as well as testing\n- Another colleague i learned a lot of interesting things from him, I know about autogen through him. thank @trungtv\n- And other contributors in the [contribute tab](https://github.com/sonnhfit/SonAgent/graphs/contributors) \n- And you who have been paying attention, users and error reporting.\n\n\n\n"
    },
    {
      "name": "Azure-Samples/aistudio-python-quickstart-sample",
      "stars": 26,
      "img": "https://avatars.githubusercontent.com/u/1844662?s=40&v=4",
      "owner": "Azure-Samples",
      "repo_name": "aistudio-python-quickstart-sample",
      "description": "Quickstart Python sample for getting started using the Azure AI Studio with the SDK or CLI options",
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2023-11-10T19:42:40Z",
      "updated_at": "2025-03-07T21:02:21Z",
      "topics": [],
      "readme": "# Azure AI Studio: Python Quickstart Sample\n\n ### :warning: WARNING: **This repository is out of date** :warning:\n|There is a new repository that covers the latest Azure AI code-first experiences. Navigate to and star this one instead: https://github.com/Azure-Samples/rag-data-openai-python-promptflow/tree/main|\n|---------------------------|\n\n> [!WARNING]  \n> **Features used in this repository are in preview. Preview versions are provided without a service level agreement, and they are not recommended for production workloads. Certain features might not be supported or might have constrained capabilities. For more information, see [Supplemental Terms of Use for Microsoft Azure Previews](https://azure.microsoft.com/en-us/support/legal/preview-supplemental-terms/).**\n\n\n## 🔎 | Explore Azure AI Studio Features\n\nThis repository contains a copilot quickstart sample that can be used with the [Azure AI Studio preview](https://learn.microsoft.com/azure/ai-studio). Explore these resources to learn more about this supports:\n\n* [Azure AI Studio](https://learn.microsoft.com/azure/ai-studio) - build, evaluate & deploy AI solutions from one space.\n* [Azure AI services](https://learn.microsoft.com/azure/ai-services/what-are-ai-services) - AI models & APIs accessible from Azure AI Studio.\n* [Azure AI SDK](https://learn.microsoft.com/azure/ai-studio/how-to/sdk-install) - access to Azure AI services from code (programmatic).\n* [Azure AI CLI](https://learn.microsoft.com/azure/ai-studio/how-to/cli-install) - access to Azure AI services from command-line (shell).\n\n\n## 👩🏽‍💻 | Build a copilot with your own data\n\nThe sample walks through creating a copilot enterprise chat API that uses custom Python code to ground the copilot responses in your company data and APIs. The sample is meant to provide a starting point that you can further customize to add additional intelligence or capabilities. Following the steps in the [**sample tutorial**](docs/start.md), you will be able to:\n - setup your development environment (pre-built or custom options)\n - create Azure AI resources and project to build the copilot\n - create an Azure Search index containing product information\n - run your copilot with a sample question, and evaluate it\n\nIn the process, you will get familiar with the **Azure AI CLI** for setting up and configuring your copilot from the commandline. And you'll learn to use the **Azure AI SDK** (from Python code or from Jupyter Notebooks) to interact programmatically with your copilot.\n\n> [!NOTE]  \n> We do not guarantee the quality of responses produced by this sample copilot or its suitability for use in your scenario, and responses will vary as development of this sample is ongoing. You must perform your own validation the outputs of the copilot and its suitability for use within your company.\n\n\n## 🏁 | Get Started\n\nReady to get started on the tutorial? The quickest way is to use a pre-built development environment. **Click the button below** to open the repo in GitHub Codespaces. Then continue with the README directions.\n\n[![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://codespaces.new/Azure-Samples/aistudio-python-quickstart-sample?quickstart=1)\n\n- [**Start Here**](./docs/start.md) if you want an overview of project and key concepts first.\n- [**Jump Here**](./docs/step-01.md) if just want to get started with the development steps.\n\nThis tutorial provides a quickstart using the Azure AI SDK from Python, to build a basic copilot application. Want to explore advanced samples using specific frameworks? Try these samples next:\n 1. [Azure AI Studio: Semantic Kernel Quickstart](https://github.com/Azure-Samples/aistudio-python-semantickernel-sample)\n 1. [Azure AI Studio: PromptFlow Quickstart](https://github.com/Azure-Samples/aistudio-python-promptflow-sample)\n 1. [Azure AI Studio: Langchain Quickstart](https://github.com/Azure-Samples/aistudio-python-langchain-sample)\n\n\n## 📚 | Resources\n\n1. [Azure AI Studio](https://aka.ms/azureaistudio) - UI to explore, build & manage AI solutions.\n1. [Azure AI Studio Docs](https://aka.ms/azureaistudio/docs) - Azure AI Studio documentation.\n1. [Azure AI Services](https://learn.microsoft.com/azure/ai-services/what-are-ai-services) - Azure AI Services documentation.\n1. [Training: Using vector search in Azure Cognitive Search](https://learn.microsoft.com/training/modules/improve-search-results-vector-search) \n1. [Tutorial: Deploy a web app for chat on your data](https://learn.microsoft.com/azure/ai-studio/tutorials/deploy-chat-web-app) \n\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n\n"
    },
    {
      "name": "alexchaomander/semantic-kernel-v1.0-hackathon",
      "stars": 25,
      "img": "https://avatars.githubusercontent.com/u/5111035?s=40&v=4",
      "owner": "alexchaomander",
      "repo_name": "semantic-kernel-v1.0-hackathon",
      "description": "Come hack with the Semantic Kernel v1.0 SDK!",
      "homepage": "",
      "language": "C#",
      "created_at": "2023-11-29T14:25:11Z",
      "updated_at": "2024-09-13T14:31:08Z",
      "topics": [],
      "readme": "# Semantic Kernel v1.0 Hackathon!\n\nWith the upcoming release of v1.0 for the [Semantic Kernel](https://github.com/microsoft/semantic-kernel), we want to do one big push and get the community hacking on our latest release candidate!\n\nIn our [last hackathon](https://partiful.com/e/RbwjXoM2LsgrLniprrLH), we challenged the community to build plugins for the Semantic Kernel.\n\nThis time around, we encourage you all to do the same, but with a focus on the v1.0 release candidate!\n\n# SK Hackathon #2 (12/04/23 - 12/11/23) 📢\n\n**What:** Build a project using the latest Semantic Kernel SDK in your preferred language.\n\n**When:** Submissions are open December 4, 2023 at 12 PM PST to December 11, 2023 at 12 PM PST.\n\n**Why:** We are approaching a big milestone for the Semantic Kernel with the release of our 1.0 SDK. Before we do so, we want to make sure that we get as much community feedback as possible! And what better way to do this, than to do a hackathon!\n\n**How:** Read the following resources to get familiar with the v1.0 SDK \n- **(LATEST) Release Candidate 1 for the Semantic Kernel .NET SDK is now live** [(Blog)](https://devblogs.microsoft.com/semantic-kernel/release-candidate-1-for-the-semantic-kernel-net-sdk-is-now-live/)\n- **SK C# Console Chat Starter** [(Repo)](https://github.com/microsoft/semantic-kernel-starters/tree/main/sk-csharp-console-chat)\n- **SK C# Hello World Starter** [(Repo)](https://github.com/microsoft/semantic-kernel-starters/tree/main/sk-csharp-hello-world)\n- **Samples for Semantic Kernel v1 proposal** [(Repo)](https://github.com/matthewbolanos/sk-v1-proposal)\n- **(OLD) Semantic Kernel's Ignite release: Beta8 for the .NET SDK** [(Blog)](https://devblogs.microsoft.com/semantic-kernel/semantic-kernels-ignite-release-beta8-for-the-net-sdk/)\n- **OpenAI Assistants: The power of templated assistant instructions** [(Blog)](https://devblogs.microsoft.com/semantic-kernel/openai-assistants-the-power-of-templated-assistant-instructions/)\n- **The Semantic Kernel YouTube Playlist! Catch up on all the latest Office Hours and new features!** [(YouTube)](https://www.youtube.com/playlist?list=PL20mfA9efrmMmLEy1fhFDvB_OmUpNUFqB)\n\n# Plugins\nPlugins are at the heart of unlocking more potential with your SK applications.\nThey allow you to connect to external data sources and give large language models tools to be able to\ninteract with native functions and API services.\n\n### Want to test your plugins? ⚗️\n\nIf you want to try out your plugins and you do not have a ChatGPT plus subscription, you can test this directly **for free** using the Semantic Kernel!\n\nSee [this tutorial doc](https://learn.microsoft.com/en-us/semantic-kernel/ai-orchestration/chatgpt-plugins) or watch the [video](https://youtu.be/W_xF8PcdT78) for how to get set up.\n\n\n## Criteria 🚀 🎁\nWe'll be judging submissions based off the following categories:\n- **Most useful plugin for the enterprise**\n- **Most fun and creative use of SK**\n\nStay tuned for what prizes will look like!\n\n## Submission\nTo submit your hack, please create a PR to this repo under the `submissions/` folder. Be sure to include a `README`.\n\nAlso fill out this [form](https://aka.ms/sk-hackathon-plugin-submission) so we know you submitted! The team will review submissions as they come in and we will announce winners during the December 13th US/Europe Community Office Hours!\n\n# Join the community\n- Star the [Semantic Kernel repo](https://github.com/microsoft/semantic-kernel)!\n- Join the [Semantic Kernel Discord community](https://aka.ms/SKDiscord)\n- Attend [regular office hours and SK community events](https://github.com/microsoft/semantic-kernel/blob/main/COMMUNITY.md)\n"
    },
    {
      "name": "Josephrp/LablabAutogen",
      "stars": 25,
      "img": "https://avatars.githubusercontent.com/u/18212928?s=40&v=4",
      "owner": "Josephrp",
      "repo_name": "LablabAutogen",
      "description": "A generalist agent that can go online and accomplish complex tasks using semantic-kernel and autogen.",
      "homepage": "https://www.tonic-ai.com",
      "language": "Python",
      "created_at": "2023-11-03T16:29:40Z",
      "updated_at": "2025-03-03T16:11:04Z",
      "topics": [
        "agi",
        "autogen",
        "gradio",
        "semantic-kernel"
      ],
      "readme": "---\ntitle: GeneralistAutogenAgent\nemoji: 🌍\ncolorFrom: gray\ncolorTo: red\nsdk: gradio\nsdk_version: 4.0.2\napp_file: app.py\npinned: false\nlicense: mit\n---\n\n## Use and Install on the Command Line\n\n\ngit clone https://github.com/Josephrp/LablabAutogen.git\n```\n\n```bash\ncd LablabAutogen\n```\n\n```bash\nnano app.py\n```\n\nedit line 17 \"    ```\"openai_api_key\": \"YOUR_KEY_HERE\",  # OpenAI API Key``` with your key\n\nthen press:\n\n```nano\n control + x\n```\n\nWrite :\n\n```nano\nY\n```\n\nto save then type :\n\n```bash\npip install -r requirements.txt\n```\n\nand finally :\n\n```bash\npython app.py\n```\nto run.\n"
    },
    {
      "name": "denniszielke/agentic-playground",
      "stars": 23,
      "img": "https://avatars.githubusercontent.com/u/11569044?s=40&v=4",
      "owner": "denniszielke",
      "repo_name": "agentic-playground",
      "description": "This is an AI agent playground to demonstrate different agent orchestration patterns and capabilities",
      "homepage": "",
      "language": "Python",
      "created_at": "2025-03-07T19:17:02Z",
      "updated_at": "2025-04-04T10:24:40Z",
      "topics": [
        "autogen",
        "genai",
        "github",
        "langchain",
        "llama-index",
        "semantic-kernel"
      ],
      "readme": "# AI Agent playground\n\nThis project demonstrates agentic concepts, orchestration patterns and functional scenarios using GitHub models and various open souce frameworks like Llama Index, Semantic Kernel, LangChain and AutoGen.\n\nThe objective is to try out and learn about the following capabilities of intelligent models:\n- Leveraging Tools for model interaction with external systems\n- Using visual model to ingest and reason about images using a multimodal model\n- Giving up control over the conversation flow using a realtime voice model\n- Handing over structured data types like Knowledge Graphs and Onthologies to models \n\nThat gives you the chance to learn how to combine these capabilities by orchestrating them:\n- Implementing a ReAct pattern with a single agent to plan and execute iterative tasks\n- Forcing the model to use and think in Domain Specific Languages to interact with existing code\n- Combining different agent types to solve complex problems through defined interaction patterns\n- Tasking a reasoning model to solve complex problems on its own without orchestration\n\nSince there are multiple agentic frameworks and hosting runtimes these will let you see how different collaboration patterns can be implemented:\n- Planned agent interactions can be implemented with any agent framework\n- Graph based interactions between agents can be implemented with LangGraph\n- Dynamic agent interaction can be implemented with MagenticOne/AutoGen\n- Event driven agent interaction can be implemented with Llama Index or Semantic Kernel\n- Distributed agent platforms can be built with Llama deploy\n\n## What is an Agent?\n\n> ***agent***: \tperceives its environment, makes decisions, takes actions autonomously in order to achieve goals, and may improve its performance with learning or acquiring knowledge \n\n![What is an agent](/img/agents.png)\n\nA simple LLM-based chatbot primarily focuses on generating responses based on predefined patterns and language models, often requiring user input to continue the conversation. In contrast, autonomous agents are designed to perform tasks independently, making decisions and taking actions without constant user input, often leveraging advanced AI techniques to achieve their goals. \n\n![Spectrum of agentic behaviour](/img/spectrum.png)\n\n## Preparation\n\nThis project does not require azure resources and support GitHub AI models.\n\n1. Create a personal access token\n\nTo authenticate with the model you will need to generate a personal access token (PAT) in your GitHub settings or set up an Azure production key. [GitHub Free AI Token](https://github.com/settings/tokens)\n\nYou can now access AI inference with your GitHub PAT. [Learn more about limits based on your plan](https://github.com/marketplace/models/azure-openai/gpt-4o-mini/playground#:~:text=Learn%20more%20about%20limits%20based%20on%20your%20plan.). You do not need to give any permissions to the token. \n\nTo use the code snippets below, create an environment variable to set your token as the key for the client code.\n\nIf you're using bash:\n```\nexport GITHUB_TOKEN=\"<your-github-token-goes-here>\"\n```\n\nor rename the file `.env_template` to `.env` and put the value inside the `.env` file. Each python script will load the value from that value automatically.\n\n2. Install dependencies (should be already done when using a GitHub codespace)\n\n```\npython -m pip -r requirements.txt\n```\n\n## Workshop contents\n\nThe scope of this workshop covers the following scenarios and technology stacks:\n\n| Name | Description | Technology  |\n| :-- | :--| :-- |\n| [Hello World](./src/01-basic/README.md) | Hello World model | OpenAI |\n| [Multimodal models](./src/02-multimodal-models/README.md) | Multimodel Prompting | OpenAI, Vision model, Realtime model |\n| [Knowledge Graphs](./src/03-complex-data/README.md) | Knowledge graph generation | OpenAI, Structured output |\n| [Onthologies](./src/03-complex-data/README.md) | Onthologies | OpenAI, OWL |\n| [Custom DSL](./src/04-complex-problems/README.md) | Domain specific languages | OpenAI, Tools |\n| [Single Agent](./src/05-search-agent/README.md) | ReAct Agent | OpenAI, LangChain, Llama Index, Tools |\n| [Human in the loop](./src/06-human-in-the-loop/README.md) | Human in the loop agents | OpenAI, LangChain, LangGraph, Semantic Kernel Tools |\n| [Multi agent collaboration](./src/07-multi-agent-collaboration/README.mdy) | Multi turn agent collaboration| OpenAI, Semantic Kernel, LangGraph |\n| [Society of agents](./src/08-society-of-agents/README.md) | Dynamic planning | OpenAI, AutoGen, MagenticOne |\n| [Event Driven Agents](./src/09-eventdriven-agents/README.md) | Distributed agents agent platforms| OpenAI, Semantic Kernel, Llama deploy |\n"
    },
    {
      "name": "Azure/agent-innovator-lab",
      "stars": 23,
      "img": "https://avatars.githubusercontent.com/u/6844498?s=40&v=4",
      "owner": "Azure",
      "repo_name": "agent-innovator-lab",
      "description": "The Agent Innovator Lab offers a hands-on learning experience in AI agent development using Microsoft Azure’s core services. Participants explore topics like search optimization, agent design, and evaluation, with practical tools and RAG best practices for advancing AI system deployment.",
      "homepage": "",
      "language": "Jupyter Notebook",
      "created_at": "2025-01-02T08:11:52Z",
      "updated_at": "2025-04-23T01:11:58Z",
      "topics": [],
      "readme": "# Agent Innovator Lab\n\nThe Agent Innovator Lab is designed to provide a structured learning experience for AI agent development by leveraging Microsoft Azure's core services (Data & AI, App, and Infra). Each lab focuses on a specific topic, covering areas such as search algorithm optimization, agentic design patterns, and evaluation frameworks. Through this hands-on workshop, participants will gain practical experience in building, optimizing, and evaluating Azure-based AI agents, ultimately driving innovation and enhancing real-world AI system deployment.\nThis repository includes RAG best practices, along with tools and techniques for innovating current architecture. \n\n\nThis hands-on lab is suitable for the following purposes:\n\n1. 1-day workshop (4-7 hours depending on customer)\n2. Hackathon starter code\n3. Reference guide for Evaluation-driven Multi-Agent design patterns\n\n\n[**Hands-on guide**](https://azure.github.io/agent-innovator-lab/) | [**Requirements**](#requirements) | [**Get started**](#get-started) \n\n----------------------------------------------------------------------------------------\n\n## List of workshops\n\nProvided below is a list of currently published modules:\n\n| Title  | Description and Link  |\n|-------|-----|\n| Lab 0. Basic RAG | [Create RAG application with Azure AI Search](0_basic-rag)  |\n| Lab 0. Basic Agent | [Basic Concepts of Agent and Agent toolkits (AutoGen and LangGraph)](0_basic-agent) |\n| Lab 1. Agentic Design Pattern | [Practice representative patterns of Agentic RAG](1_agentic-design-ptn) |\n| Lab 2. Evaluation Design Pattern | [Practice the Evaluation-Driven RAG patterns](2_eval-design-ptn) |\n| Lab 3. Optimization Design Pattern | [(In development) Practice the Optimization Design patterns](3_optimization-design-ptn)   |\n| Lab Intermission. Agentic Workflow Design Lab | [Design Agentic Workflow before each hands-on session ](lab_intermission) |\n \n\n## Requirements\nBefore starting, you should meet the following requirements:\n\n- [Access to Azure OpenAI Service](https://go.microsoft.com/fwlink/?linkid=2222006)\n- [Azure AI Foundry getting started](https://int.ai.azure.com/explore/gettingstarted): Need to create a project\n- [Access to Azure AI Search](https://learn.microsoft.com/en-us/azure/search/search-what-is-azure-search)\n- [Access to Azure Bing Search](https://learn.microsoft.com/en-us/bing/search-apis/bing-web-search/create-bing-search-service-resource)\n\n- ***[Evaluation driven design pattern]*** Need to grant ***Storage Blob Data Contributor*** at the storage of AI Foundry role to user, group, service principle and managed Identity which you are trying to access the data executing evaluators in cloud.\n\n- ***[Evaluation driven design pattern - custom evaluator]*** Need to access ***Azure ML*** and ***Storage Account*** to upload your custom evaluators and data.\n\n- In order to run ***azure.ai.evaluation.evaluate***, ***Azure CLI*** installed and you are logged into your Azure account by running ***az login***.\n\n\n**Please do not forget to modify the `.env` file to match your account. Rename `sample.env` to `.env` or copy and use it**\n\n## Get started\n\n### If you are using your own local \n```shell\ngit clone https://github.com/Azure/agent-innovator-lab.git\ncd agent-innovator-lab \npip install -r requirements.txt\n```\n\n### If you are using Azure ML Compute Instance\n- First you need to create and set up a compute instance on AI Foundry, then access the compute instance on Azure ML \n```shell\ngit clone https://github.com/Azure/agent-innovator-lab.git\ncd agent-innovator-lab && conda activate azureml_py310_sdkv2\npip install -r requirements.txt\n```\n\n## 🔥 New Feature (10-Apr-2025)\n\n### Prompt Optimization using PromptWizard<br>\n- This hands-on demonstrates how to optimize prompts using PromptWizard. PromptWizard, released as open source and paper by Microsoft, is a prompt optimization tool for maximizing the performance of LLM. It is a prompt optimization framework that employs a self-evolving mechanism in which LLM generates, critiques, refines, and continuously improves prompts and examples through feedback and synthesis\n- <a href=\"https://github.com/Azure/agent-innovator-lab/tree/main/3_optimization-design-ptn/03_prompt-optimization\">Go to notebook</a>\n\n### Semantic Kernel hands-on lab<br>\n- This hands-on demonstrates how to use Semantic Kernel (SK) to build a simple agentic application.\n- The lab covers the following topics:\n    - Core concepts and architecture of Semantic Kernel (SK)\n- Agent implementation examples:\n    - Azure AI Agent, Azure Assistant Agent, Chat Completion Agent, Azure Responses Agent\n- Tool and connector integration:\n    - Azure AI Search, File Tool, Bing Search API via Bing Search Connector, Bing Grounding Tool, Azure Evaluation SDK \n- How to build a simple agentic application using SK, leveraging various agentic design patterns (e.g. Corrective / Adaptive RAG)\n- You can also leverage existing evaluation design patterns and optimization design patterns to evaluate and optimize your Semantic Kernel application.\n  <a href=\"https://github.com/Azure/agent-innovator-lab/blob/main/0_basic-agent/SK/1_basic-concept-with-sk.ipynb\">Go to notebook</a>\n\n\n## Trouble Shooting\n\n### EvaluationException: (UserError) Failed to connect to your Azure AI project. Please check if the project scope is configured correctly, and make sure you have the necessary access permissions. Status code: 401.\n- run `az login`. To set up your tenant scope, try the option `--scope https://graph.microsoft.com//.default`\n- For example, `az login --scope https://graph.microsoft.com//.default`\n\n### azure_ai_project evaluationexception: (internalerror) azure ml managed identity configuration not found in environment. invalid_scope\n- Check the managed identity role that your compute instance has. You can create a **compute instance on Azure AI Foundry (not on Azure ML)** to avoid the complex identity setup. \n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n"
    },
    {
      "name": "Azure-Samples/ai-multi-agent-presentation-builder",
      "stars": 22,
      "img": "https://avatars.githubusercontent.com/u/1844662?s=40&v=4",
      "owner": "Azure-Samples",
      "repo_name": "ai-multi-agent-presentation-builder",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2025-01-08T17:00:18Z",
      "updated_at": "2025-04-14T12:05:00Z",
      "topics": [],
      "readme": "# AI Multi-Agent Presentation Builder\n\nCreate draft presentations effortlessly with the power of AI. This project leverages multiple AI agents to collaboratively generate presentation content based on user input. \n\nIn this repository, we demonstrate how to use **Semantic Kernel** to orchestrate Multi-Agent systems using **Azure OpenAI** models. We use a swarm agent architecture with **o1-mini** as the orchestrator and **gpt-4o-mini** model as the LLM for the task-oriented agents.\n\n**Semantic Kernel** is utilized for agent orchestration, enabling seamless coordination and communication between different AI agents. By leveraging Semantic Kernel, the system efficiently manages task delegation, context sharing, and workflow automation, ensuring that each agent contributes effectively to the content creation process.\n\nThis repository is designed for **learning purposes**, offering insights into the development and integration of multi-agent systems for automated content creation.\nThe deck created is only to demonstrate how to add external capabilities with custom Plugins, but there is no intention \n\nThe diagram below shows how the orchestrator create the agents and the expert agents collaborate with each other to accomplish the goal:\n\n![MAS-Presentation-Builder](images/mas-orchestrator-deck-builder.png)\n\nThe **Expert agents** are dynamically created and have a level of autonomy to accomplish its tasks. Each one will be responsible for a specific task (given by the orchestrator).\n\n## Features\n\nThis project framework provides the following features:\n\n* **Dynamic Agent Creation**: Automatically generates AI agents tailored to specific tasks.\n* **Collaborative AI**: Multiple AI agents work together to create comprehensive presentations.\n* **Streamlit Integration**: User-friendly web interface for seamless interaction.\n* **Customizable Templates**: Use Jinja templates to define agent behaviors and prompts.\n* **Presentation Export**: Generate and download presentations in PPTX format.\n\n## Demo\n![MAS-Presentation-Builder](images/mas-presentation-builder.gif)\n\n## Getting Started\n\n### Prerequisites\n\n- Python 3.8+\n- Azure OpenAI API Key\n- Bing API Key\n\n### Installation\n\n1. Clone the repository:\n    ```sh\n    git clone [repository clone url]\n    cd [repository name]\n    ```\n\n2. Install the required Python packages:\n    ```sh\n    pip install -r requirements.txt\n    ```\n\n3. Set up your environment variables:\n    - Copy  to  and fill in your API keys and other configurations.\n    - You can use the [.env.sample](.env.sample) file to adjust your own environment variables. Rename the file to `.env` and change each one with your own data.\n\n### Quickstart\n\n1. Run the Streamlit app:\n    ```sh\n    streamlit run app.py\n    ```\n\n2. Open your browser and navigate to `http://localhost:8501`.\n\n3. Enter a theme for your presentation and let the AI agents do the rest!\n\n## Demo\n\nA demo app is included to show how to use the project.\n\nTo run the demo, follow these steps:\n\n1. Ensure all prerequisites are met.\n2. Follow the Quickstart guide to run the Streamlit app.\n3. Interact with the AI agents and generate a presentation.\n\n## Resources\n\n- [Azure OpenAI Documentation](https://learn.microsoft.com/en-us/azure/cognitive-services/openai/)\n- [Streamlit Documentation](https://docs.streamlit.io/)\n- [Jinja Documentation](https://jinja.palletsprojects.com/)\n\n## Contributing\n\nThis project welcomes contributions and suggestions. Please open a PR and it will be analyzed as soon as possible.\n\n## License\n\nThis project is licensed under the MIT License. See the  file for details.\n"
    },
    {
      "name": "farzad528/azure-ai-agents-playground",
      "stars": 21,
      "img": "https://avatars.githubusercontent.com/u/40604067?s=40&v=4",
      "owner": "farzad528",
      "repo_name": "azure-ai-agents-playground",
      "description": "Playground for building AI Agents on Azure",
      "homepage": "",
      "language": null,
      "created_at": "2025-02-10T13:13:11Z",
      "updated_at": "2025-04-16T22:58:29Z",
      "topics": [],
      "readme": "# Azure AI Agents Playground 🧪\n\n## Table of Contents\n1. [Overview](#overview)\n2. [Key Features & Examples](#key-features--examples)\n3. [Quick Start](#quick-start)\n4. [Usage](#usage)\n5. [Contributing](#contributing)\n6. [License](#license)\n\n## 🌟 Overview\n\nWelcome to the Azure AI Agents Playground! 👋 This repository is designed to be your hands-on learning space for diving into the exciting world of Azure AI Agents.\n\nHere, you'll find a collection of practical examples, from quick start guides to more advanced implementations, all focused on helping you understand and utilize Azure AI Agents effectively. Whether you're a seasoned developer or just starting your AI journey, this playground offers a friendly environment to experiment and build amazing things.\n\nThis repository serves as a personal playground for me to explore and showcase the capabilities of Azure AI Agents. It's also open to the community for learning and inspiration. Feel free to explore, adapt, and use these examples as a starting point for your own projects!\n\nNOTE, all of these use cases are real-world customer problems where AI can make the world a more efficient place.\n\n## ✨ Key Features & Examples\n\nThis playground is organized into different sample folders, each focusing on a specific aspect or use case of Azure AI Agents. Dive into these examples to get started:\n\n### [samples/01-QUICKSTART](./samples/01-QUICKSTART)\n**⚡ Use this repo for:** Quickly setting up and running your first Azure AI Agent connected to Azure AI Search. Perfect for beginners!\n\n- Explore a basic agent that answers questions using documents indexed in Azure AI Search.\n- Check out the agent-quickstart.ipynb notebook for a step-by-step walkthrough.\n\n### [samples/02-AGENT-MEMORY-MEM0](./samples/02-AGENT-MEMORY-MEM0)\n**🧠 Use this repo for:** Implementing agent memory using Mem0 to create more conversational and context-aware agents.\n\n- Discover how to equip your agents with memory to maintain context across conversations.\n- See how Mem0 enhances agent interactions in the provided examples.\n\n### [samples/03-SEMANTIC-KERNEL-AZURE-AGENTS](./samples/03-SEMANTIC-KERNEL-AZURE-AGENTS)\n**⚛️ Use this repo for:** Integrating Azure AI Agents with Semantic Kernel for building more complex and modular AI applications.\n\n- Learn how to leverage the power of Semantic Kernel alongside Azure AI Agents.\n- Explore the sk.ipynb notebook to see Semantic Kernel in action with agents.\n\n### [samples/04-AGENTIC-RAG-ROUTERS](./samples/04-AGENTIC-RAG-ROUTERS)\n**🗺️ Use this repo for:** Building advanced Agentic Retrieval-Augmented Generation (RAG) systems with intelligent routing between different knowledge sources (Azure AI Search & Bing Search).\n\n- Delve into creating sophisticated agents that can route queries to the most relevant information source.\n- Run the app.py Chainlit application and explore the chainlit.md documentation for details.\n\n### [samples/05-AGENTIC-RAG-QUERY-PLANNING](./samples/05-AGENTIC-RAG-QUERY-PLANNING)\n**🔍 Use this repo for:** Creating multi-step agentic RAG systems that can orchestrate queries across multiple data sources with advanced planning capabilities.\n\n- Build a cardiology-focused AI assistant that integrates three distinct data sources:\n  - **Structured Data**: Patient records from Azure SQL Database\n  - **Unstructured Data**: Medical guidelines from Azure AI Search\n  - **Web Data**: Real-time information using Azure AI Agent Service with Bing Grounding Tool\n- Experience how the agent dynamically decides which tools to use and in what order\n- See how complex medical queries can be broken down into multiple steps for comprehensive answers\n- Explore the Jupyter notebook for a detailed walkthrough or run the Chainlit app for an interactive demo\n\n## 🚀 Quick Start\n\nReady to jump in? Here's how to get started:\n\n1. Clone the repository:\n```bash\ngit clone https://github.com/Farzad528/azure-ai-agents-playground.git\ncd azure-ai-agents-playground\n```\n\n2. Navigate to a sample folder (e.g., samples/01-QUICKSTART):\n```bash\ncd samples/01-QUICKSTART\n```\n\n3. Install dependencies:\n```bash\npip install -r requirements.txt\n```\n\n4. Configure environment variables:\n   - Make sure you have the necessary Azure and OpenAI API keys and connection strings set as environment variables.\n   - Refer to the specific sample's README or code comments for required environment variables.\n\n5. Run the sample:\n   - For Jupyter Notebook examples, open the .ipynb file and run the cells.\n   - For Python script examples, run the .py file from your terminal: `python your_script_name.py`\n\n## 🧑‍💻 Usage\n\nFeel free to explore and modify the code in this repository. Here are some ideas:\n\n- **Run the provided Jupyter Notebooks:** Step through the notebooks to understand the code and experiment with different parameters.\n- **Adapt the examples:** Modify the code to fit your own use cases and explore different Azure AI Agents functionalities.\n- **Create new agents:** Build your own agents from scratch, leveraging the samples as a guide.\n- **Experiment with different tools:** Integrate various Azure AI services and external tools with your agents.\n\n## 🙌 Contributing\n\nWhile this is primarily a personal learning playground, your input is highly valued! Check out the [CONTRIBUTING.md](./CONTRIBUTING.md) file for guidelines on how you can contribute:\n\n- Report bugs 🐛 and suggest new features 💡 by opening issues.\n- Share your feedback and ideas to make this playground more valuable for everyone.\n- Engage in discussions and help others in the community.\n\n## 📜 License\n\nThis project is licensed under the MIT License. Feel free to use and adapt the code for your own projects."
    },
    {
      "name": "microsoft/chimera",
      "stars": 21,
      "img": "https://avatars.githubusercontent.com/u/6154722?s=40&v=4",
      "owner": "microsoft",
      "repo_name": "chimera",
      "description": "OpenAI powered document processing",
      "homepage": "",
      "language": "Jupyter Notebook",
      "created_at": "2024-03-11T15:09:08Z",
      "updated_at": "2025-04-20T01:47:17Z",
      "topics": [
        "azure-openai"
      ],
      "readme": "[![Deploy Python project to Azure Function App](https://github.com/microsoft/chimera/actions/workflows/azure-functions-app-python.yml/badge.svg)](https://github.com/microsoft/chimera/actions/workflows/azure-functions-app-python.yml)\n\n# Project Chimera\n\nThis repo contains 3 Azure Function projects which demonstrate how to:\n1. Pull content from a Word Document.\n1. Use Azure OpenAI to validate and transform extracted content against known style guides.\n1. Re-assemble a template document with generated contents.\n\n![Application Logic Flow](./docs/AppLogicFlow.png)\n\n## Projects\n- [semantic-kernel-azure-function](semantic-kernel-azure-function) - A Durable Python Azure Function which accepts extracted document contents and uses a selection of Semantic Kernel functions to validate and transform content.\n- [openxml-azure-function](openxml-azure-function) - A Dotnet Core Azure Function project which extracts content from a word document and re-assembles a template document using supplied content sections.\n- [openxml-azure-function-python](openxml-azure-function-python) - A partial example of implementing the dotnet core document assembly functions using python.\n\n# Getting Started\n## semantic-kernel-azure-function\n1. Clone this repo to your machine\n1. Rename the [local.settings.example.json](semantic-kernel-azure-function/local.settings.example.json) file to `local.settings.json`\n1. Edit the file and update with your settings:\n```\n{\n  \"IsEncrypted\": false,\n  \"Values\": {\n    \"FUNCTIONS_WORKER_RUNTIME\": \"python\",\n    \"AzureWebJobsFeatureFlags\": \"EnableWorkerIndexing\",\n    \"AzureWebJobsStorage\": \"UseDevelopmentStorage=true\",\n    \"AZURE_OPENAI_DEPLOYMENT_NAME\": \"gpt-35-turbo-16k\",\n    \"AZURE_OPENAI_ENDPOINT\": \"\",\n    \"AZURE_OPENAI_API_KEY\": \"\",\n    \"AZURE_STORAGE_ACCOUNT_URL\": \"\",\n    \"AZURE_STORAGE_CONTAINER_NAME\": \"files\",\n    \"AZURE_STORAGE_BLOB_NAME\": \"Abbreviations.csv\",\n    \"AZURE_CLIENT_ID\": \"\",\n    \"AZURE_CLIENT_SECRET\": \"\",\n    \"AZURE_TENANT_ID\": \"\"\n  }\n}\n```\n\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n"
    },
    {
      "name": "HenriSchulte-MS/FlightBookingWithAIAgents",
      "stars": 21,
      "img": "https://avatars.githubusercontent.com/u/77101781?s=40&v=4",
      "owner": "HenriSchulte-MS",
      "repo_name": "FlightBookingWithAIAgents",
      "description": "Simple demo using Semantic Kernel and AutoGen to create an airline booking app powered by AI agents",
      "homepage": null,
      "language": "Python",
      "created_at": "2023-11-06T10:10:14Z",
      "updated_at": "2025-02-19T08:26:11Z",
      "topics": [],
      "readme": "# 🛫 Flight Booking Demo with AutoGen and Semantic Kernel\n\n![Two pilots smiling](/app/static/img/banner.png)<br>\n<small>*Image generated with DALLE-3 in [Bing Image Creator](https://bing.com/images/create)*</small>\n\nA small demo on combining [`AutoGen`](https://github.com/microsoft/autogen) and [`Semantic Kernel`](https://github.com/microsoft/semantic-kernel) to create a simple flight booking system that's powered by AI agents.\nThe application allows users to provide booking instructions once, e.g., \"*Book the cheapest flight from Tokyo to Toronto*\", and then lets AI agents determine how to complete the task without any additional user input. \n\nThe demo includes two agents: An LLM-powered assistant and a worker agent that is capable of executing Semantic Kernel plugins (formerly known as skills). In addition to the built-in `TimeSkill` that gives the agent access to time and date operations, the agent also has access to a custom `FlightsPlugin` that enables looking up flights between two cities and to issue a (simulated) booking request.\n\nTo make the agents' thought process transparent, the app visualizes their back-and-forth in a chat format. The interactions of the agents depend on the flights data and user instructions. An example:\n\n| Role | Message |\n| -------- | -------- |\n| 👤 User   | Book the cheapest flight from Tokyo to Toronto   |\n| 🤖 Assistant   | *Making a call to GetFlights function with input \"Tokyo, Toronto\"*   |\n| 🖥️ GetFlights | *List of flights from Tokyo to Toronto, incl. details like time and price* |\n| 🤖 Assistant | *Making a call to BookFlights function with the flight ID of the cheapest flight* |\n| 🖥️ BookFlight | The flight could not be booked. There are no free seats available. |\n| 🤖 Assistant | I regret to inform you that the cheapest flight from Tokyo to Toronto is currently fully booked. Let's try to book the next available option for you. <br>*Making a call to BookFlights function with the flight ID of the next cheapest flight* |\n| 🖥️ BookFlight | Flight successfully booked. |\n| 🤖 Assistant | Great news! I've successfully booked your flight from Tokyo to Toronto. This is your flight detail: [...]\n\nThis project was inspired by John Maeda's devblog [AutoGen Agents Meet Semantic Kernel](https://devblogs.microsoft.com/semantic-kernel/autogen-agents-meet-semantic-kernel/).\n\n## Running the app locally\nThis app utilizes **Azure OpenAI** for Large Language Models and **Azure CosmosDB** for data storage. You can run the application itself locally by following the steps below.\n\n### Setting up your environment\n1. Clone this repo to your machine.\n1. Create a Python virtual environment and activate it, e.g. by running `python3 -m venv .venv` and `source .venv/bin/activate`.\n1. Install the libraries listed in [requirements.txt](app/requirements.txt) by running `pip install -r app/requirements.txt`.\n1. Rename the [.env.example](/app/.env.example) file to `.env`. You will populate it in the next section.\n\n### Setting up your Azure resources\n1. Create an **Azure OpenAI** resource and deploy a `gpt-35-turbo` or `gpt-4` model.\n1. Add the deployment name, resource endpoint, and key to the `.env` file.\n1. Create an **Azure CosmosDB** resource, specifically, \"Azure Cosmos DB for NoSQL\". Within, create a database `AirlineBooking` and a container `Flights`.\n1. Add the read-write connection string for your **CosmosDB** resource to the `.env` file.\n1. To populate the database, run [populate_cosmos.py](/setup/populate_cosmos.py) a few (e.g., 10) times (`python3 setup/populate_cosmos.py`). You might have to install `pandas` if it's not present in your virtual environment (`pip install pandas`). Every time you run it, the script adds three new flights to the database. \n\n### Running the application\n1. In your terminal, switch to the `app` directory. \n1. Run `flask run` to start the app.\n1. Click the link in your terminal or navigate to `http://127.0.0.1:5000` in your browser to view the application.\n1. Enter your instructions and click **Send**. The agents will react to your instructions and the final result is displayed. To see intermediate steps, click **Show intermediate steps**. You can try a new instruction by clicking **Start over** in the bottom of the page.\n\n### Example walkthrough\n\n1. Once the application is up and running, enter a promp such as `Book the cheapest flight from Brussels to Vienna` into the input box. Hit send.\n1. It might take some seconds for the app to respond. You can watch the progress in your terminal to see what is happening behind the scenes.\n1. You should receive an output in your webbrowser saying that a flight has been successfully booked. \n1. You can have a look at the intermediate steps to see how the AI agents came to the conclusion to finally have a flight booked for you.\n\n**Please note**: For the demo to run successfully, you'll need to verify that CosmosDB contains direct flights between the requested cities.\n\n### Screenshots\n\n![demo start](assets/demo-start.png)\n![demo final](assets/demo-final.png)\n"
    },
    {
      "name": "Azure-Samples/az-ai-kickstarter",
      "stars": 20,
      "img": "https://avatars.githubusercontent.com/u/1844662?s=40&v=4",
      "owner": "Azure-Samples",
      "repo_name": "az-ai-kickstarter",
      "description": "AI Applications Chassis - AI Apps Best Practices Tailored for Azure",
      "homepage": "",
      "language": "Bicep",
      "created_at": "2025-03-01T16:59:44Z",
      "updated_at": "2025-04-18T08:05:15Z",
      "topics": [],
      "readme": "# AI Applications Chassis - AI Apps Best Practices Tailored for Azure\n\nToC: [**USER STORY**](#azure-ai-app-kickstarter) \\| [**GETTING STARTED**](#getting-started)  \\| [**HOW IT WORKS**](#how-it-works)\n\n## Azure AI App Kickstarter\n\nAn opinionated set of best practices and patterns to bootstrap your Multi Agent application in minutes.\n\n#### Infrasturcture architechture\n\n<img src=\"doc/images/arch-infra.png\" alt=\"High level Kickstarter architecture - infra view\" width=\"800\">\n\n#### Application architecture\n\n<img src=\"doc/images/arch-app.png\" alt=\"Kickstarter cognitive architecture - app view\" width=\"800\">\n\nThis architecture implements a **Debate Pattern** using the **[Semantic Kernel's](https://learn.microsoft.com/en-us/semantic-kernel/overview/) [agent framework](https://learn.microsoft.com/en-us/semantic-kernel/frameworks/agent/?pivots=programming-language-python)**, a dynamic environment where multiple AI agents collaborate to refine ideas, test arguments, or reach a resolution.\n\nThe core architecture components based on Semantic Kernel abstractions:\n\n   - **Speaker Selection Strategy** (Green Box):\n     - This component determines which agent (WRITER or CRITIC) \"speaks\" next.\n     - It ensures productive collaboration by regulating the flow of interaction between the agents and preventing redundant actions.\n   - **WRITER Agent**: provides the initial proposal and the subsequent revisions following the direction from critic.\n   - **CRITIC Agent**: evaluates the text and provides constructive feedback to drive readibility and popularity of the post. Provides scoring across a number of categories and a final score.\n   - **Chat Termination Strategy** (Red Box):\n     - This component decides when the conversation has reached a satisfactory conclusion. It takes the overall critic score and compares to acceptance treshold. \n\nSemantic Kernel powers the agents with features like prompt engineering, memory recall, and logic orchestration.\n\n## Getting Started\n\n### Codespaces and DevContainers\n\nThis respository has been configured to support GitHub Codespace and DevContainers.\n[![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://codespaces.new/evmin/az-ai-kickstarter) [![Open in Dev Containers](https://img.shields.io/static/v1?style=for-the-badge&label=Dev%20Containers&message=Open&color=blue&logo=visualstudiocode)](https://vscode.dev/redirect?url=vscode://ms-vscode-remote.remote-containers/cloneInVolume?url=https://github.com/evmin/az-ai-kickstarter)\n\n> [!WARNING]\n> Do NOT `git clone` the application under Windows and then open a DevContainer. \n> This would create issues with file end of lines. For DevContainer click on the button \n> above and let Visual Studio Code download the repository for you. Alternatively you \n> can also `git clone` under Windows Subsystem for Linux (WSL) and ask Visual Studio Code to\n> `Re-Open in Container`.\n\n### Dependencies\n\n  - [Azure CLI](https://learn.microsoft.com/en-us/cli/azure/what-is-azure-cli): `az`\n  - [Azure Developer CLI](https://learn.microsoft.com/en-us/azure/developer/azure-developer-cli/overview): `azd`\n  - [Python](https://www.python.org/about/gettingstarted/): `python`\n  - [UV](https://docs.astral.sh/uv/getting-started/installation/): `uv`\n  - Optionally [Docker](https://www.docker.com/get-started/): `docker` \n\nSee below for installation instructions\n\n### Quick deploy\n\n[Dependency Install Guide](doc/DEPENDENCY_INSTALL.md)\n\n#### Deploy \n\nTo deploy Azure AI App Kickstarter just run: \n```bash\nazd up\n``` \n> [!WARNING]\n> This deploys the application with authentication DISABLED.\n\n#### Deploy with authentication enabled\n\nAZD can automatically configure authentication to secure the frontend and/or backend. To do so execute the following command before `azd up`:\n```bash\nazd env set USE_AUTHENTICATION true\n```\n\nIf you already executed `azd up` just set the variable and run provisioning again:\n```bash\nazd env set USE_AUTHENTICATION true\nazd provision\n```\n\n> [!WARNING] \n> The account executing `azd` needs to be able to create Application Registrations in your Azure Entra ID tenant.\n\n#### External Model\n\nIf you have an external Azure OpenAI model already provisioned, you can reference it by setting environment variable prior callin `azd up`\n\n```sh\nexport AOAI_ENDPOINT=\"https://<endpoint>.openai.azure.com\"\nexport AOAI_DEPLOYMENT_NAME=\"gpt-4o-2024-11-20\"\nexport AOAI_API_VERSION=\"2024-12-01-preview\"\nexport aoaikeysecret=\"key\"\n```\n\n>[WARNING!] The `aoaikeysecret` is not set in azd .azure/<env>./.env file automatically.\n> In order to use it when running the model locally, either set it as env variable or add it to azd `.env` file.\n\n## How it works\n\n### Running the frontend \n\n```bash\ncd src/frontend\nuv sync\nuv run streamlit app.py\n```\n### Running the backend\n\n  ```bash\n  # Sync Python dependencies\n  uv sync\n  # Start the backend server with live reloading\n  uv run uvicorn app:app --reload\n  ```\n\n### Tracing\n\nThe AI Traces you will be able to find in AI Foundry Project under \"Tracing\".\nIf you click on one of the traces you will see a detailed history view with every agent,\nprompt, etc.:\n<img src=\"doc/images/tracing.png\" alt=\"Azure AI Foundry Portal Trace Detail\" width=\"800\">\n\n### Accessing logs of Azure Container Apps\n\nIf you need to troubleshoot and access the logs of the containers running in Azure Container \napps you can use this helper script (`bash` only). It will connect to Azure remotely and \nstream the logs to your local terminal.\n\nFor the Frontend:\n```bash\n./scripts/aca_logs.sh frontend\n```\n\nFor the Backend:\n```bash\n./scripts/aca_logs.sh backend\n```\n\nLogs will be streamed to your terminal:\n<img src=\"doc/images/logging.png\" alt=\"Semantic Kernel Logs\" width=\"800\">\n\n## Code of Conduct\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\n\nResources:\n\n- [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/)\n- [Microsoft Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/)\n- Contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with questions or concerns\n\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Responsible AI Guidelines\n\nThis project follows the below responsible AI guidelines and best practices, please review them before using this project:\n\n- [Microsoft Responsible AI Guidelines](https://www.microsoft.com/en-us/ai/responsible-ai)\n- [Responsible AI practices for Azure OpenAI models](https://learn.microsoft.com/en-us/legal/cognitive-services/openai/overview)\n- [Safety evaluations transparency notes](https://learn.microsoft.com/en-us/azure/ai-studio/concepts/safety-evaluations-transparency-note)\n\n## Acknowledgements\n\n  * Kudos to [Pamela Fox](https://github.com/pamelafox) and [James Casey](https://github.com/jamesc) for [Azure-Samples/openai-chat-app-entra-auth-builtin](https://github.com/Azure-Samples/openai-chat-app-entra-auth-builtin) from which we borrowed most of authentication & authorization setup.\n  * Special thank you to [Michael Hofer](https://github.com/mhofer1976) for extensive testing and solving o1 compatibility\n\n## Authors\n\n  * [Dominique Broeglin](https://github.com/dbroeglin)\n  * [Evgeny Minkevich](https://github.com/evmin)"
    },
    {
      "name": "Ori-Replication/MetaGPT_webui",
      "stars": 20,
      "img": "https://avatars.githubusercontent.com/u/109295054?s=40&v=4",
      "owner": "Ori-Replication",
      "repo_name": "MetaGPT_webui",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2023-09-11T02:45:01Z",
      "updated_at": "2025-04-18T16:37:08Z",
      "topics": [],
      "readme": "# MetaGPT: The Multi-Agent Framework\n\n\n\n\n\n\nWarning! This repositorie Will stop Stop maintenance. Please refer to the new repository: https://github.com/Ori-Replication/MetaGPT-WebUI\n\n\n\n\n\n\n\n\n<p align=\"center\">\n<a href=\"\"><img src=\"docs/resources/MetaGPT-logo.jpeg\" alt=\"MetaGPT logo: Enable GPT to work in software company, collaborating to tackle more complex tasks.\" width=\"150px\"></a>\n</p>\n\n<p align=\"center\">\n<b>Assign different roles to GPTs to form a collaborative software entity for complex tasks.</b>\n</p>\n\n<p align=\"center\">\n<a href=\"docs/README_CN.md\"><img src=\"https://img.shields.io/badge/文档-中文版-blue.svg\" alt=\"CN doc\"></a>\n<a href=\"README.md\"><img src=\"https://img.shields.io/badge/document-English-blue.svg\" alt=\"EN doc\"></a>\n<a href=\"docs/README_JA.md\"><img src=\"https://img.shields.io/badge/ドキュメント-日本語-blue.svg\" alt=\"JA doc\"></a>\n<a href=\"https://discord.gg/wCp6Q3fsAk\"><img src=\"https://dcbadge.vercel.app/api/server/wCp6Q3fsAk?compact=true&style=flat\" alt=\"Discord Follow\"></a>\n<a href=\"https://opensource.org/licenses/MIT\"><img src=\"https://img.shields.io/badge/License-MIT-yellow.svg\" alt=\"License: MIT\"></a>\n<a href=\"docs/ROADMAP.md\"><img src=\"https://img.shields.io/badge/ROADMAP-路线图-blue\" alt=\"roadmap\"></a>\n<a href=\"docs/resources/MetaGPT-WeChat-Personal.jpeg\"><img src=\"https://img.shields.io/badge/WeChat-微信-blue\" alt=\"roadmap\"></a>\n<a href=\"https://twitter.com/DeepWisdom2019\"><img src=\"https://img.shields.io/twitter/follow/MetaGPT?style=social\" alt=\"Twitter Follow\"></a>\n</p>\n\n<p align=\"center\">\n   <a href=\"https://vscode.dev/redirect?url=vscode://ms-vscode-remote.remote-containers/cloneInVolume?url=https://github.com/geekan/MetaGPT\"><img src=\"https://img.shields.io/static/v1?label=Dev%20Containers&message=Open&color=blue&logo=visualstudiocode\" alt=\"Open in Dev Containers\"></a>\n   <a href=\"https://codespaces.new/geekan/MetaGPT\"><img src=\"https://img.shields.io/badge/Github_Codespace-Open-blue?logo=github\" alt=\"Open in GitHub Codespaces\"></a>\n</p>\n\n1. MetaGPT takes a **one line requirement** as input and outputs **user stories / competitive analysis / requirements / data structures / APIs / documents, etc.**\n2. Internally, MetaGPT includes **product managers / architects / project managers / engineers.** It provides the entire process of a **software company along with carefully orchestrated SOPs.**\n   1. `Code = SOP(Team)` is the core philosophy. We materialize SOP and apply it to teams composed of LLMs.\n\n![A software company consists of LLM-based roles](docs/resources/software_company_cd.jpeg)\n\n<p align=\"center\">Software Company Multi-Role Schematic (Gradually Implementing)</p>\n\n## Examples (fully generated by GPT-4)\n\nFor example, if you type `python startup.py \"Design a RecSys like Toutiao\"`, you would get many outputs, one of them is data & api design\n\n![Jinri Toutiao Recsys Data & API Design](docs/resources/workspace/content_rec_sys/resources/data_api_design.png)\n\nIt costs approximately **$0.2** (in GPT-4 API fees) to generate one example with analysis and design, and around **$2.0** for a full project.\n\n## Installation\n\n### Installation Video Guide\n\n- [Matthew Berman: How To Install MetaGPT - Build A Startup With One Prompt!!](https://youtu.be/uT75J_KG_aY)\n\n### Traditional Installation\n\n```bash\n# Step 1: Ensure that NPM is installed on your system. Then install mermaid-js.\nnpm --version\nsudo npm install -g @mermaid-js/mermaid-cli\n\n# Step 2: Ensure that Python 3.9+ is installed on your system. You can check this by using:\npython --version\n\n# Step 3: Clone the repository to your local machine, and install it.\ngit clone https://github.com/geekan/metagpt\ncd metagpt\npython setup.py install\n```\n\n**Note:**\n\n- If already have Chrome, Chromium, or MS Edge installed, you can skip downloading Chromium by setting the environment variable\n  `PUPPETEER_SKIP_CHROMIUM_DOWNLOAD` to `true`.\n\n- Some people are [having issues](https://github.com/mermaidjs/mermaid.cli/issues/15) installing this tool globally. Installing it locally is an alternative solution,\n\n  ```bash\n  npm install @mermaid-js/mermaid-cli\n  ```\n\n- don't forget to the configuration for mmdc in config.yml\n\n  ```yml\n  PUPPETEER_CONFIG: \"./config/puppeteer-config.json\"\n  MMDC: \"./node_modules/.bin/mmdc\"\n  ```\n\n- if `python setup.py install` fails with error `[Errno 13] Permission denied: '/usr/local/lib/python3.11/dist-packages/test-easy-install-13129.write-test'`, try instead running `python setup.py install --user`\n\n### Installation by Docker\n\n```bash\n# Step 1: Download metagpt official image and prepare config.yaml\ndocker pull metagpt/metagpt:v0.3.1\nmkdir -p /opt/metagpt/{config,workspace}\ndocker run --rm metagpt/metagpt:v0.3.1 cat /app/metagpt/config/config.yaml > /opt/metagpt/config/key.yaml\nvim /opt/metagpt/config/key.yaml # Change the config\n\n# Step 2: Run metagpt demo with container\ndocker run --rm \\\n    --privileged \\\n    -v /opt/metagpt/config/key.yaml:/app/metagpt/config/key.yaml \\\n    -v /opt/metagpt/workspace:/app/metagpt/workspace \\\n    metagpt/metagpt:v0.3.1 \\\n    python startup.py \"Write a cli snake game\"\n\n# You can also start a container and execute commands in it\ndocker run --name metagpt -d \\\n    --privileged \\\n    -v /opt/metagpt/config/key.yaml:/app/metagpt/config/key.yaml \\\n    -v /opt/metagpt/workspace:/app/metagpt/workspace \\\n    metagpt/metagpt:v0.3.1\n\ndocker exec -it metagpt /bin/bash\n$ python startup.py \"Write a cli snake game\"\n```\n\nThe command `docker run ...` do the following things:\n\n- Run in privileged mode to have permission to run the browser\n- Map host directory `/opt/metagpt/config` to container directory `/app/metagpt/config`\n- Map host directory `/opt/metagpt/workspace` to container directory `/app/metagpt/workspace`\n- Execute the demo command `python startup.py \"Write a cli snake game\"`\n\n### Build image by yourself\n\n```bash\n# You can also build metagpt image by yourself.\ngit clone https://github.com/geekan/MetaGPT.git\ncd MetaGPT && docker build -t metagpt:custom .\n```\n\n## Configuration\n\n- Configure your `OPENAI_API_KEY` in any of `config/key.yaml / config/config.yaml / env`\n- Priority order: `config/key.yaml > config/config.yaml > env`\n\n```bash\n# Copy the configuration file and make the necessary modifications.\ncp config/config.yaml config/key.yaml\n```\n\n| Variable Name                              | config/key.yaml                           | env                                             |\n| ------------------------------------------ | ----------------------------------------- | ----------------------------------------------- |\n| OPENAI_API_KEY # Replace with your own key | OPENAI_API_KEY: \"sk-...\"                  | export OPENAI_API_KEY=\"sk-...\"                  |\n| OPENAI_API_BASE # Optional                 | OPENAI_API_BASE: \"https://<YOUR_SITE>/v1\" | export OPENAI_API_BASE=\"https://<YOUR_SITE>/v1\" |\n\n## Tutorial: Initiating a startup\n\n```shell\n# Run the script\npython startup.py \"Write a cli snake game\"\n# Do not hire an engineer to implement the project\npython startup.py \"Write a cli snake game\" --implement False\n# Hire an engineer and perform code reviews\npython startup.py \"Write a cli snake game\" --code_review True\n```\n\nAfter running the script, you can find your new project in the `workspace/` directory.\n\n### Preference of Platform or Tool\n\nYou can tell which platform or tool you want to use when stating your requirements.\n\n```shell\npython startup.py \"Write a cli snake game based on pygame\"\n```\n\n### Usage\n\n```\nNAME\n    startup.py - We are a software startup comprised of AI. By investing in us, you are empowering a future filled with limitless possibilities.\n\nSYNOPSIS\n    startup.py IDEA <flags>\n\nDESCRIPTION\n    We are a software startup comprised of AI. By investing in us, you are empowering a future filled with limitless possibilities.\n\nPOSITIONAL ARGUMENTS\n    IDEA\n        Type: str\n        Your innovative idea, such as \"Creating a snake game.\"\n\nFLAGS\n    --investment=INVESTMENT\n        Type: float\n        Default: 3.0\n        As an investor, you have the opportunity to contribute a certain dollar amount to this AI company.\n    --n_round=N_ROUND\n        Type: int\n        Default: 5\n\nNOTES\n    You can also use flags syntax for POSITIONAL ARGUMENTS\n```\n\n### Code walkthrough\n\n```python\nfrom metagpt.software_company import SoftwareCompany\nfrom metagpt.roles import ProjectManager, ProductManager, Architect, Engineer\n\nasync def startup(idea: str, investment: float = 3.0, n_round: int = 5):\n    \"\"\"Run a startup. Be a boss.\"\"\"\n    company = SoftwareCompany()\n    company.hire([ProductManager(), Architect(), ProjectManager(), Engineer()])\n    company.invest(investment)\n    company.start_project(idea)\n    await company.run(n_round=n_round)\n```\n\nYou can check `examples` for more details on single role (with knowledge base) and LLM only examples.\n\n## QuickStart\n\nIt is difficult to install and configure the local environment for some users. The following tutorials will allow you to quickly experience the charm of MetaGPT.\n\n- [MetaGPT quickstart](https://deepwisdom.feishu.cn/wiki/CyY9wdJc4iNqArku3Lncl4v8n2b)\n\n## Citation\n\nFor now, cite the [Arxiv paper](https://arxiv.org/abs/2308.00352):\n\n```bibtex\n@misc{hong2023metagpt,\n      title={MetaGPT: Meta Programming for Multi-Agent Collaborative Framework},\n      author={Sirui Hong and Xiawu Zheng and Jonathan Chen and Yuheng Cheng and Jinlin Wang and Ceyao Zhang and Zili Wang and Steven Ka Shing Yau and Zijuan Lin and Liyang Zhou and Chenyu Ran and Lingfeng Xiao and Chenglin Wu},\n      year={2023},\n      eprint={2308.00352},\n      archivePrefix={arXiv},\n      primaryClass={cs.AI}\n}\n```\n\n## Contact Information\n\nIf you have any questions or feedback about this project, please feel free to contact us. We highly appreciate your suggestions!\n\n- **Email:** alexanderwu@fuzhi.ai\n- **GitHub Issues:** For more technical inquiries, you can also create a new issue in our [GitHub repository](https://github.com/geekan/metagpt/issues).\n\nWe will respond to all questions within 2-3 business days.\n\n## Demo\n\nhttps://github.com/geekan/MetaGPT/assets/2707039/5e8c1062-8c35-440f-bb20-2b0320f8d27d\n\n## Join us\n\n📢 Join Our Discord Channel!\nhttps://discord.gg/ZRHeExS6xv\n\nLooking forward to seeing you there! 🎉\n"
    },
    {
      "name": "Azure-Samples/nlp-sql-in-a-box",
      "stars": 19,
      "img": "https://avatars.githubusercontent.com/u/1844662?s=40&v=4",
      "owner": "Azure-Samples",
      "repo_name": "nlp-sql-in-a-box",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2024-06-14T15:07:44Z",
      "updated_at": "2025-04-02T12:39:35Z",
      "topics": [],
      "readme": "---\npage_type: sample\nlanguages:\n- azdeveloper\n- powershell\n- bicep\n- python\nproducts:\n- azure\n- ai-services\n- azure-openai\n- azure-speech\n- azure-sql-database\nurlFragment: nlp-sql-in-a-box\nname: NLP to SQL Chatbot in-a-box (AI-in-a-Box) with enables Azure SQL databases, Azure OpenAI, Semantic Kernel, and Azure AI Speech Service\ndescription: Users to interact with SQL databases using natural language and speech, leveraging Azure OpenAI, Semantic Kernel, and Azure AI Speech Service to translate spoken queries into SQL statements, execute them, and deliver results audibly, ensuring an intuitive and user-friendly experience.\n---\n<!-- YAML front-matter schema: https://review.learn.microsoft.com/en-us/help/contribute/samples/process/onboarding?branch=main#supported-metadata-fields-for-readmemd -->\n\n# NLP-SQL-in-a-Box\n\n|||\n|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------| ---:|\n| This solution is part of the AI-in-a-Box framework developed by the team of Microsoft Customer Engineers and Architects to accelerate the deployment of AI and ML solutions. Our goal is to simplify the adoption of AI technologies by providing ready-to-use accelerators that ensure quality, efficiency, and rapid deployment.| ![\"AI-in-a-box Logo: Description\"](./media/ai-in-a-box.png) |\n\n## User Story\n\nBuild a cutting-edge speech-enabled SQL query system using Azure Open AI, Semantic Kernel, and Azure AI Speech Service\n\nWe will use the power of Azure Open AI and Semantic Kernel to translate your natural language queries into SQL statements that can be executed against an SQL Server database. This will allow you to interact with your data in a more intuitive and user-friendly way. No more struggling with complex SQL syntax – just speak your query and let the system do the rest!\n\nAnd with Azure Speech Services, we will convert your speech into text and synthesize the results as speech. This means that you can hear the results of your query spoken back to you, making it easier to understand and digest the information.\n\n![](./media/banner-nlp-to-sql-in-a-box.png)\n\n## What's in the Box\n<img src=\"./architecture/nlp_to_sql_architecture.png\" />\n\n- Python application that leverages [Semantic Kernel](https://learn.microsoft.com/en-us/semantic-kernel/overview/) and [Speech Services](https://azure.microsoft.com/en-us/products/ai-services/ai-speech) to build a chatbot that can:\n  - Understand natural language database queries from speech\n  - Translate them into SQL\n  - Execute the SQL against an SQL Server database\n  - Return the results as speech\n- Deployment templates of all resources needed, which includes:\n  - [OpenAI Service and Deployment](https://azure.microsoft.com/en-us/products/ai-services/openai-service)\n  - [Speech Services](https://azure.microsoft.com/en-us/products/ai-services/ai-speech)\n  - [SQL Server](https://azure.microsoft.com/en-us/products/azure-sql/database/)\n- Resources are deployed and used with security best practices in mind\n  - Speech and OpenAI services do not allow api keys access\n  - SQL Server requires Active Directory authentication\n  - Required RBAC roles are assigned to the user deploying the solution\n  - Application connects to all services using azure credential\n\nThis solution was adapted from the [Revolutionizing SQL Queries with Azure Open AI and Semantic Kernel](https://techcommunity.microsoft.com/t5/analytics-on-azure-blog/revolutionizing-sql-queries-with-azure-open-ai-and-semantic/ba-p/3913513) blog post.\n\n## Thinking Outside the Box\nThis solution can be adapted for many other use cases. Here are some ideas:\n\n- Update the nlp_to_sql plugin to support more complex queries (including updates, deletes, etc.)\n- Add more plugins to the semantic kernel to support additional use cases\n- Add other options to interact with the kernel (e.g., a web interface, a mobile app, etc.)\n\n## Deploy the Solution\n\n### Deploy Pre-requisites\n1. An [Azure subscription](https://azure.microsoft.com/en-us/free/)\n2. Install [Azure CLI](https://docs.microsoft.com/en-us/cli/azure/install-azure-cli-windows?view=azure-cli-latest)\n3. Install [Bicep](https://docs.microsoft.com/en-us/azure/azure-resource-manager/bicep/install)\n4. Install [Azure Developer CLI](https://learn.microsoft.com/en-us/azure/developer/azure-developer-cli/install-azd)\n\n### UI Deploy\n\n[![Deploy to Azure](https://aka.ms/deploytoazurebutton)](https://portal.azure.com/#create/Microsoft.Template/uri/https%3A%2F%2Fraw.githubusercontent.com%2FAzure-Samples%2Fnlp-sql-in-a-box%2Fmain%2Finfra%2Fazuredeploy.json)\n\n#### Required Input Parameters\nThe parameters below are required in order to deploy the infrastructure.\n- Subscription\n- Region\n- Environment Name\n- Principal Id\n  - You can find this by running the following command:\n    ```bash\n    az ad signed-in-user show --query id -o tsv\n    ```\n- Administrator Login\n\n#### Optional Input Parameters\n- IP Address\n  - If you want to allow your IP address to access the SQL Server, you can provide it here.\n\n#### Output Parameters\nAfter the deployment is complete, you can find the output parameters by clicking on the `Outputs` tab.\nYou need to create an `.env` file in the root of the project and fill it with the output parameters. The `.env` file should look like this:\n```bash\nAZURE_LOCATION=\"<azure_location>\"\nAZURE_OPENAI_CHAT_DEPLOYMENT_NAME=\"<azure_openai_chat_deployment_name>\"\nAZURE_OPENAI_ENDPOINT=\"<azure_openai_endpoint>\"\nSPEECH_SERVICE_ID=\"<speech_service_id>\"\nSQL_SERVER_NAME = \"<sql_server_name>\"\nSQL_DATABASE_NAME = \"<sql_database_name>\"\n```\n\nNote: whenever the biceps files are changed, the `azuredeploy.json` file must be updated. To do this, run the following command:\n\n```bash\naz bicep build --file infra/main.bicep --outfile infra/azuredeploy.json\n```\n\n### Azd deploy\n1. Clone this repository locally\n  \n    `git clone https://github.com/Azure-Samples/nlp-sql-in-a-box/`  \n2. Deploy resources\n\n    `az login`\n\n    `azd auth login`\n\n    `azd up`\n\nYou will be prompted for:\n- environment name\n- azure subscription\n- azure region (we suggest using `eastus2`)\n- database administrator login\n\nWhen you deploy using this method, the `.env` file will be created automatically with the output parameters.\n\n### Clean up\nTo remove all resources created by this solution, run:\n    \n`azd down`\n\n## Run the Solution\n\n### Run Pre-requisites\n1. Install Python 3.10\n2. Install [ODBC Driver for SQL Server](https://learn.microsoft.com/en-us/sql/connect/odbc/download-odbc-driver-for-sql-server) \n3. Make sure you can access the resources deployed from your local machine. \n   - By default, all resources were created with no public access.\n   - You can allow your own IP address to access the resources by:\n        - Find out your what's your IPv4 address\n        - `azd env set IP_ADDRESS <ip_address>`\n        - `azd up`\n4. Install requirements\n    \n  `pip install -r src/requirements.txt`\n\n### Run Locally\n\n`python -m src.app`\n\nThe first time you run the application, it will create and populate the database with fake data. This process may take a few minutes.\n\n#### Logging\nThe application will output logs to the `app.log` file, so you can use it to better understand what's happening.\nIf you need more information, you can change the log level to DEBUG `app.py` file:\n```python\nlogging.basicConfig(\n    filename=\"app.log\",\n    format=\"[%(asctime)s - %(name)s:%(lineno)d - %(levelname)s] %(message)s\",\n    datefmt=\"%Y-%m-%d %H:%M:%S\",\n    level=logging.DEBUG,\n)\n```\n\n### Example Usage\nBelow you can see an example of the solution in action:\n\n````\n$ python -m src.app\nListening:\nUser > How many locations are there?\ntool plugins-nlp_to_sql needs to be called with parameters {}\ntool plugins-nlp_to_sql called and returned There are `1` tool call arguments required and only `0` received. The required arguments are: ['input']. Please provide the required arguments and try again.\ntool plugins-nlp_to_sql needs to be called with parameters {\"input\":\"How many locations are there?\"}\ntool plugins-nlp_to_sql called and returned ```sql\nSELECT COUNT(DISTINCT Location) AS NumberOfLocations FROM ExplorationProduction;\n```\ntool plugins-query needs to be called with parameters {\"query\":\"SELECT COUNT(DISTINCT Location) AS NumberOfLocations FROM ExplorationProduction;\"}\ntool plugins-query called and returned (1000,)\nAssistant > There are 1000 distinct locations.\nListening:\nUser > Yes.\nListening:\nUser > Can you list me the top five locations by production volume?\ntool plugins-nlp_to_sql needs to be called with parameters {\"input\":\"biggest five locations by production volume\"}\ntool plugins-nlp_to_sql called and returned ```sql\nSELECT TOP 5 Location, SUM(ProductionVolume) AS TotalProductionVolume\nFROM ExplorationProduction\nGROUP BY Location\nORDER BY TotalProductionVolume DESC;\n```\ntool plugins-query needs to be called with parameters {\"query\":\"SELECT TOP 5 Location, SUM(ProductionVolume) AS TotalProductionVolume FROM ExplorationProduction GROUP BY Location ORDER BY TotalProductionVolume DESC;\"}\ntool plugins-query called and returned ('West Travishaven, Vietnam', Decimal('999300.73')),('Baileyville, Israel', Decimal('998248.91')),('Williamsborough, Wallis and Futuna', Decimal('997729.20')),('Lake Gabrielshire, Panama', Decimal('996433.80')),('Davidstad, Saint Kitts and Nevis', Decimal('994778.98'))\nAssistant > Here are the five locations with the highest production volumes:\n\n1. **West Travishaven, Vietnam**: 999,300.73\n2. **Baileyville, Israel**: 998,248.91\n3. **Williamsborough, Wallis and Futuna**: 997,729.20\n4. **Lake Gabrielshire, Panama**: 996,433.80\n5. **Davidstad, Saint Kitts and Nevis**: 994,778.98\nListening:\nUser > No.\n````\n\n## Customize the Solution\n\n### Add More Plugins\nYou can add more plugins by:\n1. Creating a new Python file in the `src/plugins` directory\n2. Implementing your plugin as a class (more details in [Plugins](https://learn.microsoft.com/en-us/semantic-kernel/concepts/plugins/?pivots=programming-language-python))\n\n### Reusing the Kernel\nIf you want to reuse this logic in another project, it is really easy, You just need to reuse the src/kernel package in your project, passing the required parameters.\n\n## How to Contribute\n\nThis project welcomes contributions and suggestions. Most contributions require you to agree to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us the rights to use your contribution. For details, visit <https://cla.opensource.microsoft.com>\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions provided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/). For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq) or contact <opencode@microsoft.com> with any additional questions or comments.\n\n## Key Contacts & Contributors\n\nHighlight the main contacts for the project and acknowledge contributors. You can adapt the structure from AI-in-a-Box:\n\n| Contact            | GitHub ID           | Email                    |\n|--------------------|---------------------|--------------------------|\n| Franklin Guimaraes | @franklinlindemberg | fguimaraes@microsoft.com |\n\n\n## License\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow [Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general). Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those third-party's policies.\n\n## FAQ\n\n1. ```commandline\n    Server is not found or not accessible. Check if instance name is correct and if SQL Server is configured to allow remote connections. For more information see SQL Server Books Online. (11001)')\n    ```\n   This error is due to the fact that the SQL Server is not accessible from the machine where the application is running. Check [Run Pre-requisites](#run-pre-requisites) for more details.\n\n\n---\n\nThis project is part of the AI-in-a-Box series, aimed at providing the technical community with tools and accelerators to implement AI/ML solutions efficiently and effectively.\n"
    },
    {
      "name": "lfbraz/semantic-kernel-nltosql",
      "stars": 19,
      "img": "https://avatars.githubusercontent.com/u/15925853?s=40&v=4",
      "owner": "lfbraz",
      "repo_name": "semantic-kernel-nltosql",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2024-02-19T14:28:54Z",
      "updated_at": "2025-04-18T21:43:43Z",
      "topics": [],
      "readme": "# Semantic Kernel: Natural Language (NL) to SQL Query Generation using Azure OpenAI (GPT-4 model)\n\nIn this repo we demonstrate how to use [Semantic Kernel](https://github.com/microsoft/semantic-kernel) to convert Natural Language (NL) to SQL Query using Azure OpenAI (GPT-4 model).\n\nSemantic Kernel is an exciting framework and a powerful tool that can be used for several applications, including chatbots, virtual assistants, and more. \n\nThis is a great way to make your data more accessible to non-technical users, and to make your applications more user-friendly.\n\nBelow are the main components of the Semantic Kernel:\n\n![Orchestrating plugins with planner](./images/sk-kernel.png)\n\nIn the example of this repo, we developed the following plugin:\n\n- **nlpToSqlPlugin**: This plugin is responsible for converting the Natural Language (NL) to SQL Query using Azure OpenAI (GPT-4 model).\n\nAs part of the plugin, we developed skills throught the use of [prompts](https://learn.microsoft.com/en-us/semantic-kernel/prompts/). The following skills were developed:\n\n- **ConvertNLPToSQL**: This skill is responsible for converting the Natural Language (NL) to SQL Query using Azure OpenAI (GPT-4 model).\n- **MakeSQLCompatible**: This skill is responsible for making the SQL Query compatible with the Transact-SQL syntax.\n- **WriteResponse**: This skill is responsible for writing the response to the user.\n\nWe also developed a [Native Function](https://learn.microsoft.com/en-us/semantic-kernel/agents/plugins/using-the-kernelfunction-decorator?tabs=python) to be able to interact with the database:\n\n- **QueryDb**: This function is responsible for querying the database and returning the result.\n\nWith that, we can create a \"Copilot like\" experience, where the user can ask questions and the system will generate the SQL Query and return the result.\n\nAs our plugin has a lot of skills, we also developed a [Sequential Planner](https://learn.microsoft.com/en-us/semantic-kernel/agents/plugins/using-the-kernelfunction-decorator?tabs=python) to orchestrate the skills:\n\nWith the planner, we can orchestrate the skills in a sequence, so the system can generate the SQL Query and return the result.\n\nThe final result is a system that can convert Natural Language (NL) to SQL Query using Azure OpenAI (GPT-4 model).\n\n## Requirements\n\n- You must have a Pay-As-You-Go Azure account with administrator - or contributor-level access to your subscription. If you don't have an account, you can sign up for an account following the instructions.\n- Get Access to [Azure OpenAI](https://learn.microsoft.com/en-us/azure/ai-services/openai/overview)\n- Once got approved create an Azure OpenAI in you Azure's subcription.\n- Python 3.11\n- You must have an Azure SQL Database with the tables and data you want to query. In this repo, we will use the a Sample database with some tables, which you can create using the scripts provided in the [folder](sql-scripts/create-tables.sql).\n- You can use [generate-fake-data](generate-fake-data/generate-fake-data.py) script to populate the tables with some fake data.\n\n## Install Required Libraries\n\n```python\nsemantic-kernel==0.5.1.dev0\npython-dotenv==1.0.0\nopenai==1.12.0\nFaker==23.2.1\npyodbc==5.1.0\n```\n\n## Create .env file\n\n```\nCONNECTION_STRING=\nAZURE_OPENAI_DEPLOYMENT_NAME=\nAZURE_OPENAI_ENDPOINT=\nAZURE_OPENAI_API_KEY=\n```\n\n*You can rename the `.env.sample` to `.env` as well. Replace the values with your own data.*\n\n## Quick Start\n\n- Run `[generate-fake-data](generate-fake-data/generate-fake-data.py)` script to populate the tables with some fake data\n- Run `main.py` to run the sample asks/questions detailed below.\n\n## Sample - Questions/Asks\n\nBelow are some sample questions/asks that can be asked to the system and the responses that the system will generate.\nThese responses can be different based on the data in the database. If you use our generate-fake-data script, you may have different responses given the random nature of the data.\n\n**Question/Ask 01**: I want to know how many transactions in the last 3 months\n\n*Response*: According to the database query, the number of transactions is 26.\n\n---\n\n**Question/Ask 02**: Give me the name of the best seller in terms of sales volume in the whole period\n\n*Response*: The seller's name according to the database query is John Doe.\n\n---\n\n**Question/Ask 03**: Which product has the highest sales volume in the last month\n\n*Response*: According to the database query, the total sales volume for the product 'Nike Air Force 1' is 28.\n\n---\n\n## Adapt to your own data\n\nFeel free to adapt the code to your own data. You can use your own data and modify the code to fit your needs.\n\n- Replace the connection string in the `.env` file with your own connection string.\n- Replace the Azure OpenAI API key and endpoint in the `.env` file with your own API key and endpoint.\n- Replace the table's structure in [ConvertNLPToSQL](plugins/nlpToSqlPlugin/ConvertNLPToSQL/skprompt.txt) Plugin with your own table's structure.\n\n## References\n\n- <https://learn.microsoft.com/en-us/semantic-kernel/overview/>\n- <https://techcommunity.microsoft.com/t5/analytics-on-azure-blog/revolutionizing-sql-queries-with-azure-open-ai-and-semantic/ba-p/3913513>\n- <https://github.com/microsoft/semantic-kernel>\n- <https://medium.com/@ranjithkumar.panjabikesanind/orchestrate-ai-and-achieve-goals-combining-semantic-kernel-sequential-planner-openai-chatgpt-d23cf5c8f98d>\n"
    },
    {
      "name": "irarainey/az-copilot",
      "stars": 18,
      "img": "https://avatars.githubusercontent.com/u/74962459?s=40&v=4",
      "owner": "irarainey",
      "repo_name": "az-copilot",
      "description": "An Azure CLI Copilot extension that allows natural language prompts to generate CLI commands to manage Azure resources.",
      "homepage": "",
      "language": "Python",
      "created_at": "2023-10-23T13:49:46Z",
      "updated_at": "2025-04-21T06:27:03Z",
      "topics": [
        "azure",
        "azure-cli-extension",
        "azure-openai",
        "cli",
        "tools"
      ],
      "readme": "# Azure CLI Copilot\n\n> **WARNING:** *This extension is experimental and should not be used in a production environment. It is intended solely to demonstrate the capability of extending a CLI tool using OpenAI.*\n\nThis project is an extension for the [Azure CLI](https://learn.microsoft.com/en-us/cli/azure/what-is-azure-cli) that allows you to use natural language prompts to run Azure CLI commands. It uses [Azure OpenAI](https://learn.microsoft.com/en-us/azure/ai-services/openai/overview) to determine the most likely Azure CLI command to run based upon the prompt you provide.\n\nThe latest [Azure CLI documentation](https://learn.microsoft.com/en-us/cli/azure/reference-index?view=azure-cli-latest) is used to create embeddings for each Azure CLI command. These embeddings are then stored in [Azure Cognitive Search](https://learn.microsoft.com/en-us/azure/search/search-what-is-azure-search) and used for [Retrieval Augmented Generation (RAG)](https://learn.microsoft.com/en-us/azure/search/retrieval-augmented-generation-overview) to determine the most up to date syntax for the command you wish to execute.\n\nThis project is built using [Microsoft Semantic Kernel](https://learn.microsoft.com/en-us/semantic-kernel/overview/), which allows for easy integration with a range of services. For more information on the open source project, see the [GitHub repository](https://github.com/microsoft/semantic-kernel)\n\n## Prerequisites\n\nTo install and use the Copilot extension you will need the following:\n\n- An Azure subscription\n- The Azure CLI installed\n- Python 3.9 or later installed\n\n## Usage\n\nTo use the Copilot simply use the command `az copilot --prompt \"your command prompt here\"`. This will invoke the call to determine the most likely Azure CLI command to run based on the prompt you provide.\n\nThe Azure CLI command will then be presented to you and you can choose to run it or not by pressing `y` or `return` confirm execution. This confirmation step can be overridden by setting the global configuration setting `autorun = true`, or at a single command level adding the option `--autorun` or `-a` which will override the global setting, such as: `az copilot --prompt \"list all my resource groups in a table\" -a`.\n\n> **WARNING:** *Care should be taken with autorun functionality as any commands usually requiring confirmation, such as deletion of resources, will auto confirm the action being performed.*\n\n![List Resource Groups](https://raw.githubusercontent.com/irarainey/az-copilot/main/images/list_resource_groups.png)\n\nIf the Copilot determines that additional information is required to execute the command, such as missing parameters, it will prompt you for the missing information. You should also use plain English commands for these conversations. You can drop out of the conversation at any time by entering `quit` or `q`.\n\n![Create Resource Groups](https://raw.githubusercontent.com/irarainey/az-copilot/main/images/create_without_params.png)\n\n### Retrieval Augmented Generation\n\nWhen first installed Retrieval Augmented Generation is disabled by default. This is to ensure the relevant Azure Cognitive Search service has been setup, populated, and configured first.\n\nWithout RAG, the Copilot will only use the Azure OpenAI model to determine the most likely Azure CLI command to run based on the prompt you provide. Given this model was trained around September 2021, it means that updates made to the Azure CLI since then will not be reflected in this model.\n\nBy creating embeddings from the most recent version of the documentation, the Copilot will be able to use RAG to additionally search the embeddings and better determine the most up to date syntax for the command you wish to execute.\n\nOnce the embeddings have been generated it is recommended to enable the use of RAG. To enable RAG, use the command `az copilot config set --use-rag true`.\n\nTwo additional configuration options are available for RAG. These are:\n\n- `--search-relevance-threshold` - The relevance threshold for the search results.\n- `--search-result-count` - The number of search results to return.\n\nThese settings are given default values when the Copilot extension is installed. These are:\n\n- `--search-relevance-threshold 0.85`\n- `--search-result-count 7`\n\nIf you wish to change these values, you can do so using the command `az copilot config set`. Special consideration should be given to the number of results returned, as the more results returned, the larger the resulting prompt being sent to the OpenAI service will be, which could then break the token limit. Changing the relevance threshold change the accuracy of the results returned and hence the quality of the command generated.\n\n### Command Flow Options\n\nThe Copilot extension has several global configuration options that can be used to control the flow of the command. These are:\n\n- `--autorun` - Automatically run the command when ready\n- `--show-command` - Show the Azure CLI command to be run\n- `--enable-logging` - Enable logging of the command flow\n\nThese are all boolean settings that can be set using the command `az copilot config set`.\n\nDefault settings are:\n- `--autorun false`\n- `--show-command true`\n- `--enable-logging false`.\n\nIf you are comfortable with the Copilot extension and want to speed up the flow, you can set `--autorun true`. This will automatically run the command when ready.\n\n> **Note:** *Any command that requires a confirmation will also be automatically confirmed. This is to ensure the command flow is not interrupted, but can be dangerous if you are not sure what the command will do, or if it is destructive such as with a delete command.*\n\n## Deployment Steps\n\nTo get up and running with the Copilot extension, you will need to complete several steps. In order these are:\n\n1. Clone this repository so the deployment scripts are available locally\n2. Deploy required Azure OpenAI and Azure Cognitive Services resources\n3. Install the Azure CLI Copilot extension\n4. Set the configuration values as output by the infrastructure deployment\n5. Run the extraction script to create up to date embeddings from the Azure CLI documentation\n\nEach of these steps is detailed below.\n\n## Resource Deployment\n\nTo deploy the required Azure OpenAI and Azure Cognitive Search resources, you can use the deployment script in the `infrastructure` directory. To make it simpler, a `makefile` has been provided to run the deployment script. From a `bash` terminal, run the following command:\n\n```bash\nmake deploy-infra\n```\n\nWhen the deployment is complete, the output will be displayed in the terminal. This will include the API keys and endpoints for the Azure OpenAI and Azure Cognitive Search resources. These values will be required when setting the configuration for the Copilot extension.\n\nBy default resources created are named `az-copilot-<unique id>`. This unique name is generated from a hash of the current Azure Subscription ID and the current timestamp. If you wish to change this, it can be done by editing the variable in the `deploy.sh` file.\n\nLikewise the location for deployment is set in the `deploy.sh` file. By default this is set to `uksouth`. If you wish to change this, it can also be done by editing the `deploy.sh` file.\n\nThe Azure OpenAI service will also deploy two models. The first is the completion model, which is `gpt-35-turbo-16k` and the second is the embedding model, which is `text-embedding-ada-002`.\n\nWhen deploying Azure OpenAI services, ensure you have enough [quota available](https://learn.microsoft.com/en-us/azure/ai-services/openai/quotas-limits) in the specified region for your subscription, or the deployment will fail.\n\n## Extension Installation\n\nThe Copilot extension is available as a Python wheel file and can be downloaded from the [releases page](https://github.com/irarainey/az-copilot/releases). The extension can also be installed using the `az extension add` command by referencing the hosted release directly.\n\nTo install the extension directly, check out the releases page, determine the version number of the latest release and run the following command specifying the version number in the filename:\n\n```bash\naz extension add --source https://github.com/irarainey/az-copilot/releases/download/latest/copilot-0.1.16-py3-none-any.whl --yes\n```\n\nYou can check the version of the extension you have installed by running the following command:\n\n```bash\naz copilot version\n```\n\nTo unstall the extension, simply run the extension `az extension remove` command:\n\n```bash\naz extension remove --name copilot\n```\n\n## Extension Configuration\n\nThe Copilot extension requires several configuration options to be set. All configuration options are stored in a local configuration file stored in the root of your user profile directory, in a directory named `.az-copilot`. The configuration file is named `config.json`.\n\nIf the configuration file does not exist, it will be created when you run any extension command or the embedding deployment script.\n\nYou can edit this file manually, or use the `az copilot config set` command to set the configuration options. All parameters are optional. You can set a single value, or all values at once.\n\nFor information on the options available, run the command `az copilot config set --help`\n\nThis will display the following table of options:\n\n```text\nCommand\n    az copilot config set\n\nArguments\n    --autorun -a                     : Global boolean value to autorun the command when ready.\n    --completion-name -cn            : The Azure OpenAI completion model deployment name.\n    --embedding-name -en             : The Azure OpenAI embedding model deployment name.\n    --enable-logging -l              : Boolean value to enable or disable logging.\n    --openai-api-key -ok             : The Azure OpenAI API key.\n    --openai-endpoint -oe            : The Azure OpenAI endpoint.\n    --search-api-key -sk             : The Azure Cognitive Search API key.\n    --search-endpoint -se            : The Azure Cognitive Search endpoint.\n    --search-index-name -si          : The Azure Cognitive Search index name.\n    --search-relevance-threshold -st : The Azure Cognitive Search relevance threshold.\n    --search-result-count -sc        : The Azure Cognitive Search result count.\n    --search-vector-size -sv         : The Azure Cognitive Search vector size.\n    --show-command -s                : Boolean value to show or hide commands.\n    --use-rag -r                     : Boolean value to enable or disable RAG.\n```\n\nAdditionally you can use the command `az copilot config show` to display the current configuration options.\n\n![Configuration Options](https://raw.githubusercontent.com/irarainey/az-copilot/main/images/show_configuration.png)\n\n> **NOTE:** For security reasons API keys are not displayed in the output.\n\n## Generate Embeddings\n\nOnce the Azure OpenAI and Azure Cognitive Search resources have been deployed, the Copilot extension has been installed, and the configuration values have been set, you can generate the embeddings required for RAG.\n\nTo generate the embeddings, you can use the `main.py` script in the `extract` directory. To make it simpler, a `makefile` has been provided to run the extraction script. From a `bash` terminal, run the following command:\n\n```bash\nmake extract-docs\n```\n\nThis script will extract the latest version of the Azure CLI documentation directly from the GitHub repository in YAML format, parse it, and create the embeddings required for RAG. A single text document is created locally in the `extract/docs` directory for each command, which includes syntax and examples, and is then used to create the embeddings which are stored in Azure Cognitive Search.\n\nAt the time of writing there are just over 11,000 Azure CLI commands. Depending upon the speed of your connection, this process will take around one hour to complete. The script will output the progress to the terminal.\n\n### With Thanks\n\nThis extension is heavily based on an internal project created for the Microsoft Global Hackathon 2023 called Azure Maestro. A huge thanks to the entire team for their hard work that made this possible. The epic team members were: Marc van Duyn, Amin Espinoza, Arjen Kroezen, Hayward van Biljon, Jaya Kumar, Michael Collier, Nuno Silva, Sherryl Manalo, and myself.\n"
    },
    {
      "name": "joowon-dm-snu/fastcampus-chatgpt-intro-frameworks",
      "stars": 18,
      "img": "https://avatars.githubusercontent.com/u/69464062?s=40&v=4",
      "owner": "joowon-dm-snu",
      "repo_name": "fastcampus-chatgpt-intro-frameworks",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2023-07-08T06:48:48Z",
      "updated_at": "2024-11-25T07:02:19Z",
      "topics": [],
      "readme": "# fastcampus-chatgpt-intro-frameworks\n\n[ChatGPT API를 활용한 챗봇 서비스 구축\nwith LangChain & Semantic Kernel](https://fastcampus.co.kr/data_online_langchain) 강의 수강을 위한 실습 코드를 정리해두었습니다.\n\n실습을 따라가기 위한 기본 셋팅은 [setup.md](/setup.md)파일을 확인 부탁드립니다.\n"
    },
    {
      "name": "chriscrcodes/talk-to-your-factory",
      "stars": 17,
      "img": "https://avatars.githubusercontent.com/u/46497089?s=40&v=4",
      "owner": "chriscrcodes",
      "repo_name": "talk-to-your-factory",
      "description": "'Talk to Your Factory' demo leveraging Edge (Azure IoT Operations), Cloud (Microsoft Fabric), and a Factory Agent (Azure OpenAI), to streamline factory operations. It allows real-time, natural language communication with factory systems, helping operators quickly identify issues, boost efficiency, and minimize downtime.",
      "homepage": "",
      "language": "Python",
      "created_at": "2024-09-06T08:26:47Z",
      "updated_at": "2025-04-16T11:54:12Z",
      "topics": [
        "azure-iot-operations",
        "azure-openai",
        "cloud-computing",
        "edge-computing",
        "generative-ai",
        "industrial-iot",
        "medallion-architecture",
        "microsoft-azure",
        "microsoft-fabric",
        "python",
        "semantic-kernel",
        "streamlit"
      ],
      "readme": "# 🗣️ Talk to your Factory\n\n## Introduction\n\nWelcome to the \"Talk to your Factory\" project, a fascinating open-source initiative that allows you to control and interact with industrial equipment using natural language processing (NLP), Edge and Cloud technologies.\n\nWhat's about? The project aims to bridge the gap between humans and industrial machines by enabling people to communicate with factory equipment using simple text inputs.\n\nSee how the Smart Factory leverages Generative AI to optimize its operations!\n\n## Technology\n\n🏭 Real-time ingestion and processing of operational data (OT): operators, manufactured products, and machine maintenance schedules.  \n🤖 Data processing: Edge and Cloud, with a Semantic Kernel & Generative AI model to power the Factory Agent, for smarter interactions.\n\n### Key features and benefits\n\n- **Data Processing**: Data structure following a **Medallion Architecture**, with the goal of incrementally and progressively improving the structure and quality of data as it flows through each layer of the architecture.  \nFrom `Bronze` (Edge: MQTT Data Simulator) ⇒ `Silver` (Edge: Azure IoT Operations) ⇒ `Gold` (Cloud: Microsoft Fabric) layer tables.\n\n- **Natural Language Processing (NLP)**: a Factory Agent, enhanced by Generative AI, empowers operators, so they can ask complex questions about machine operations, staff, production quality, as if they were speaking to a human expert in the Factory.\n\n## Architecture\n\n### Solution architecture overview\n\n![Architecture Diagram](./artifacts/media/architecture-overview.png \"Solution Overview\")\n\n### Factory simulation\n\n![Factory Simulation](./artifacts/media/simulation.png \"Factory Simulation\")\n\n### Key components\n\n![Data Diagram](./artifacts/media/key-components.png \"Data Diagram\")\n\n1. [**Factory Simulator**](./artifacts/mqtt-data-simulator/README.md)  \n    Simulates data coming from several factories: Berlin, Austin, Buffalo, Shanghai.  \n    Factory simulator is publishing data to an Message Queuing Telemetry Transport (MQTT) broker topic based on the international standard from the International Society of Automation known as 'ISA-95' with the following format: Enterprise/Site/Area/Line/Cell.  \n    Industrial machines involved in the process are 'Cells.'  \n\n    > Messages are published following the **UNS (Unified Namespace) Architecture**.  \n    The UNS is a method of organizing and structuring data in a way that makes it accessible and meaningful across an entire enterprise.  \n    ![UNS](./artifacts/media/UNS.png \"UNS\")\n\n2. [**Azure IoT Operations**](https://learn.microsoft.com/en-us/azure/iot-operations/overview-iot-operations)  \n    Processes data at Edge: normalize, contextualize, enrich with Edge reference datasets (Operators and Products).\n\n3. [**Azure Event Hub**](https://learn.microsoft.com/en-us/azure/event-hubs/event-hubs-about)  \n    Data ingestion in Azure.     \n    \n4. [**Microsoft Fabric Real-Time Intelligence**](https://learn.microsoft.com/en-us/fabric/real-time-intelligence/overview)  \n    Processes data in Azure: materialize data as a Table, enrich with Cloud reference datasets (Operators, Assets and Products).\n\n5. [**Generative AI Factory Agent**](https://learn.microsoft.com/en-us/azure/ai-services/openai/overview)  \n    Introducing a custom Large Language Model (LLM) Factory Agent, based on OpenAI model 'GPT-4o', that enables natural language communication with the factory. This agent simplifies the process of retrieving information from various systems and databases.\n\n### Communication flow\n\n![Factory Agent Communication Flow](./artifacts/media/factory-agent-communication-flow.png \"Factory Agent Communication Flow\")\n\n1. **User Prompt**: user asks a question to the Factory Agent.  \n    The graphical user interface is based on the open-source framework [`Streamlit`](https://streamlit.io/).\n2. **Custom Large Language Model (LLM)**: analyzes the prompt and generate the corresponding query to be executed to the database in Microsoft Fabric.  \n3. [**Semantic Kernel**](https://aka.ms/semantic-kernel): execute query and return results (Python application).\n\n#### Creating complex queries from natural language prompt - Example\n![Factory Agent Prompt](./artifacts/media/factory-agent-prompt.png \"Factory Agent Prompt\")\n\n1. **Prompt**: _\"Determine the yield percentage for each Site this month by comparing the total units produced to the number of good units produced.\"_\n2. **Generative AI Model**: analyzes the prompt and generate the corresponding query to be executed to the database.  \n\n    > **IMPORTANT**: No actual data from the database is transmitted to the Large Language Model; only the prompt and the database schema are shared. The LLM will generate the query to be executed against the database, but it won't execute the query itself.  \n\n    Example query generated in `KQL (Kusto Query Language)`:  \n    ```\n    aio_gold\n    | where Timestamp >= startofmonth(now())\n    | summarize TotalUnitsProduced = sum(TotalPartsCount), GoodUnitsProduced = sum(GoodPartsCount) by Site\n    | extend YieldPercentage = (GoodUnitsProduced / TotalUnitsProduced) * 100\n    | project Site, YieldPercentage\n    | limit 100\n    ```  \n\n3. **Back-end application `(Python)`**: queries the database to retrieve results.  \n\n4. **Front-end application `(Streamlit)`**: provides the user interface.\n\n## Prerequisites\nMicrosoft Documentation: [Azure IoT Operations prerequisites](https://learn.microsoft.com/en-us/azure/iot-operations/deploy-iot-ops/howto-prepare-cluster?tabs=ubuntu)\n\n### Hardware requirements\n\n- **Resources**: \n    - CPU: `4 vCPU`\n    - Memory: `16GB`\n    - Storage: `30GB`\n\n- **Operating System**: the solution requires a Linux-based system, specifically a VM or physical machine running `Linux Ubuntu 24.04`. This system will perform as an Edge server, handling queries directly from the production line and interfacing with other operational systems.\n\n### Software requirements\n\n - [`K3s`](https://k3s.io/) Lightweight Kubernetes. Easy to install, half the memory, all in a binary of less than 100 MB.\n - [`python >=v3.10`](https://www.python.org/) Programming Language\n - [`Azure CLI`](https://learn.microsoft.com/en-us/cli/azure/) the Azure command-line interface.\n\n### Cloud services requirements\n\n - Azure Subscription (with Contributor rights)\n    - The solution will deploy the following resources:\n        - Azure IoT Operations prerequisites\n            - Resource Group\n            - Storage Account\n            - Schema Registry\n            - 2 Managed Identities\n            - 2 App Registrations (Service Principal for Edge Gateway & Factory Agent)\n        - Data Streaming Ingestion\n            - Event Hub\n        - Factory Agent\n            - Azure Open AI Service\n        - _Optional_: Virtual Machine (if you want to test everything in Azure Cloud)\n - Microsoft Fabric Tenant (you can try it for free [here](https://www.microsoft.com/en-us/microsoft-fabric/getting-started?msockid=27cd43526f4e6b2a1fa857d06e486a3c))\n\n## Demo\n\n![Factory Agent User Interface](./artifacts/media/demo-video.gif \"Factory Agent User Interface\")\n\n## Videos\n\n- **IoT Show** hosted by `Olivier Bloch`  \n  [![Video on the IoT Show](https://img.youtube.com/vi/-AxWwJU_G_U/hqdefault.jpg)](https://www.youtube.com/embed/-AxWwJU_G_U)  \n- **Azure Arc Jumpstart** hosted by `Lior Kamrat`  \n  [![Video on the Arc Jumpstart Show](https://img.youtube.com/vi/cN6urmB_7jY/hqdefault.jpg)](https://www.youtube.com/embed/cN6urmB_7jY)\n\n## Solution build steps\n\nDeploy the solution in 3 steps!\n\n### 1. [Provision resources (Cloud & Edge)](./INSTALL-1.md)\n### 2. [Configure the solution (Microsoft Fabric)](./INSTALL-2.md)\n### 3. [Deploy and use the Generative AI Factory Agent](./INSTALL-3.md)\n\n### Additional resources\n- [Uninstall the solution](./UNINSTALL.md)"
    },
    {
      "name": "alfredzouang/CognitiveSearchChatGPTDemo",
      "stars": 17,
      "img": "https://avatars.githubusercontent.com/u/5335164?s=40&v=4",
      "owner": "alfredzouang",
      "repo_name": "CognitiveSearchChatGPTDemo",
      "description": "Azure CognitiverSearch and Bing with ChatGPT to provide internal/external information chatbot",
      "homepage": null,
      "language": "Python",
      "created_at": "2023-03-30T04:49:35Z",
      "updated_at": "2024-09-21T08:49:54Z",
      "topics": [],
      "readme": "# CognitiveSearchChatGPTDemo\nCombine Cognitive Search, Bing Search, TTS, STT with ChatGPT to provide Q&amp;A chatbot\n\nThis is a demo application build upon this Github repo.\n\nVisit https://github.com/Azure-Samples/azure-search-openai-demo\n\n:warning:\nThis demo is still under construction, please don't be superised if you see dummy code or encounter bugs.\n\n## Preview\n![Preview](/docs/images/Picture1.png)\n![Preview](/docs/images/display_image1.png)\n![Preview](/docs/images/Slide3.JPG)\n![Preview](/docs/images/Slide4.JPG)\n\n## Setup\n\n### Dependencies\n- [Python 3.10+](https://www.python.org/downloads/)\n- [Node.js 14+](https://nodejs.org/en/download/)\n- [Azure CLI](https://docs.microsoft.com/en-us/cli/azure/install-azure-cli?view=azure-cli-latest)\n- [Azure Subscription](https://azure.microsoft.com/en-us/free/)\n- [Azure Cognitive Search](https://docs.microsoft.com/en-us/azure/search/search-create-service-portal)\n- [Azure Bing Search Service](https://docs.microsoft.com/en-us/azure/cognitive-services/bing-web-search/quickstarts/python)\n- [Azure Speech Service](https://docs.microsoft.com/en-us/azure/cognitive-services/speech-service/quickstarts/text-to-speech/quickstart-python)\n- [Azure App Service](https://docs.microsoft.com/en-us/azure/app-service/quickstart-python?tabs=bash&pivots=python-framework-flask)\n\n### Prerequisites\nInstall python libraries\n```bash\npip install -r requirements.txt\n``` \nBuild front end\n```bash\ncd frontend\nnpm install\nnpm run build\n```\n\n### Create Azure resources\n\n#### Login to Azure CLI\n```bash\naz login\n```\n#### Create Azure Resource Group\n```bash\naz group create --name <resource-group-name> --location <location>\n```\n\n#### Create Azure Storage Account\nCreate a new Azure Storage Account resource.  You can use the [Azure Portal](https://portal.azure.com) or [Azure CLI](https://docs.microsoft.com/en-us/cli/azure/?view=azure-cli-latest). The following example uses the Azure CLI.\n\n```bash\naz storage account create --name <storage-account-name> --resource-group <resource-group-name> --sku <sku> --location <location>\n```\n\n#### Create Azure Cognitive Search\nCreate a new Azure Cognitive Search resource.  You can use the [Azure Portal](https://portal.azure.com) or [Azure CLI](https://docs.microsoft.com/en-us/cli/azure/?view=azure-cli-latest).  The following example uses the Azure CLI.\n\n```bash\naz search service create --name <search-service-name> --resource-group <resource-group-name> --sku <sku> --location <location>\n```\n**Note:** You should change the cognitive search endpoint in `app.py` to match the endpoint of your cognitive search resource.\n\n##### Create Azure Cognitive Search Index\nCreate a new Azure Cognitive Search index.  You can use the [Azure Portal](https://portal.azure.com) or [Azure CLI](https://docs.microsoft.com/en-us/cli/azure/?view=azure-cli-latest).  The following example uses the Azure CLI.\n\n```bash\naz search index create --service-name <search-service-name> --resource-group <resource-group-name> --name <index-name> --fields <fields>\n```\n##### Create Azure Cognitive Search Data Source\nCreate a new Azure Cognitive Search data source.  You can use the [Azure Portal](https://portal.azure.com) or [Azure CLI](https://docs.microsoft.com/en-us/cli/azure/?view=azure-cli-latest).  The following example uses the Azure CLI.\n\n```bash\naz search datasource create --service-name <search-service-name> --resource-group <resource-group-name> --name <data-source-name> --type <data-source-type> --credentials <credentials> --container <container> --data-change-impact <data-change-impact> --data-deletion-impact <data-deletion-impact> --description <description> --data-format <data-format> --eTag <eTag> --name <name> --query <query> --schedule <schedule> --type <type>\n```\n\n##### Create Azure Cognitive Search Indexer\nCreate a new Azure Cognitive Search indexer.  You can use the [Azure Portal](https://portal.azure.com) or [Azure CLI](https://docs.microsoft.com/en-us/cli/azure/?view=azure-cli-latest).  The following example uses the Azure CLI.\n\n```bash\naz search indexer create --service-name <search-service-name> --resource-group <resource-group-name> --name <indexer-name> --data-source-name <data-source-name> --target-index-name <index-name> --skillset-name <skillset-name>\n```\n\n\n#### Create Azure Bing Search Service\nCreate a new Azure Bing Search Service resource.  You can use the [Azure Portal](https://portal.azure.com) or [Azure CLI](https://docs.microsoft.com/en-us/cli/azure/?view=azure-cli-latest).  \n\n#### Create Azure Speech Service\nCreate a new Azure Speech Service resource.  You can use the [Azure Portal](https://portal.azure.com) or [Azure CLI](https://docs.microsoft.com/en-us/cli/azure/?view=azure-cli-latest). The following example uses the Azure CLI.\n\n```bash\naz cognitiveservices account create --name <speech-service-name> --resource-group <resource-group-name> --kind <kind> --sku <sku> --location <location>\n```\n\n#### Create Azure App Service Plan\nCreate a new Azure App Service Plan resource.  You can use the [Azure Portal](https://portal.azure.com) or [Azure CLI](https://docs.microsoft.com/en-us/cli/azure/?view=azure-cli-latest). The following example uses the Azure CLI.\n\n```bash\naz appservice plan create --name <app-service-plan-name> --resource-group <resource-group-name> --sku <sku> --location <location>\n```\n\n#### Create Azure Web App\nCreate a new Azure Web App resource.  You can use the [Azure Portal](https://portal.azure.com) or [Azure CLI](https://docs.microsoft.com/en-us/cli/azure/?view=azure-cli-latest). The following example uses the Azure CLI.\n\n```bash\naz webapp create --name <web-app-name> --resource-group <resource-group-name> --plan <app-service-plan-name>\n```\n\n## To run locally\n```bash\ncd frontend\nnpm install\nnpm run build\ncd ..\ncd backend\npython app.py\n```\n\n## To deploy to Azure\nOpen Azure portal and navigate to your web app resource.  Click on the Deployment Center and select GitHub as the source.  Select the repository and branch you want to deploy.  Click on Save and deploy.  The web app will be deployed to Azure.\n\n**Note:** before deploying to Azure, you should remove these two lines in `app.py`:\n```python\nfrom dot_env import load_dotenv\nload_dotenv()\n```\n\n\n\n"
    },
    {
      "name": "aymenfurter/azure-transcript-search-openai-demo",
      "stars": 16,
      "img": "https://avatars.githubusercontent.com/u/20464460?s=40&v=4",
      "owner": "aymenfurter",
      "repo_name": "azure-transcript-search-openai-demo",
      "description": "Sample ChatGPT-style Q&A app via RAG-pattern on Video transcripts.",
      "homepage": "",
      "language": "Bicep",
      "created_at": "2023-08-20T06:21:15Z",
      "updated_at": "2025-04-09T10:26:27Z",
      "topics": [],
      "readme": "# Turn any YouTube Channel Into a Chatbot\n<img style=\"border-radius: 10px;\" src=\"architecture.png?raw=true\">\n\nThis example demonstrates how to create a ChatGPT-style application using the Retrieval Augmented Generation (RAG). It leverages Azure's OpenAI Service to access the ChatGPT model (gpt-3.5-turbo) and employs Azure Cognitive Search Vector Search for data indexing and retrieval.\n\n## Features\n- Chat Interface\n- Ask question related to the videos, and get answers from the video transcripts\n- Directly jump to the video section where the answer is found through the embedded video player or the links found in the answer\n\n<img style=\"border-radius: 10px;\" src=\"screenshot.png?raw=true\">\n\n## Prerequisites \nTo deploy the application, please ensure that you have the following dependencies installed on your machine.\n* [Azure Developer CLI](https://aka.ms/azure-dev/install)\n* [Python 3.9+](https://www.python.org/downloads/)\n* [Node.js 14+](https://nodejs.org/en/download/)\n* [Git](https://git-scm.com/downloads)\n* [Bash / WSL](https://learn.microsoft.com/en-us/windows/wsl/install) \n\n## Installation\n1. Clone the repository and navigate to the project root\n2. Run `azd auth login`\n3. Run `azd up`\n4. Specify the target locations for the Azure resources aswell as the name of the  channel you want to index. \n\nImportant: Based on the video size and the number of videos, the indexing process can take up one hour. By default, only the first 20 videos are indexed. You can change this by modifying the `prepdocs.sh` file.\n\n### Sample deployment output\n    $ azd up\n    ? Enter a new environment name: lexfridman\n    ? Select an Azure Subscription to use: 1. Your Subscription\n    ? Select an Azure location to use:  7. (Asia Pacific) Central India (centralindia)\n\n    Packaging services (azd package)\n\n    (✓) Done: Packaging service api\n    - Package Output: /tmp/azddeploy2823501235.zip\n    (✓) Done: Packaging service web\n    - Package Output: /tmp/azddeploy817139155.zip\n\n    Provisioning Azure resources (azd provision)\n    Provisioning Azure resources can take some time\n\n    ? Enter a value for the 'openAiResourceGroupLocation' infrastructure parameter:  2. (Europe) West Europe (westeurope)\n    ? Save the value in the environment for future use Yes\n    ? Enter a value for the 'youTubeChannelName' infrastructure parameter: lexfridman\n    ? Save the value in the environment for future use Yes\n    You can view detailed progress in the Azure Portal:\n    https://portal.azure.com/...\n\n    (✓) Done: Resource group: rg-lexfridman\n    (✓) Done: App Service plan: appbackend\n    (✓) Done: App Service: appfrontend\n    (✓) Done: Azure OpenAI: csa\n    (✓) Done: Search service: gpt\n    Executing postprovision hook => ./scripts/prepdocs.sh\n\n    Deploying services (azd deploy)\n\n    (✓) Done: Deploying service api\n    - Endpoint: https://appbackend.azurewebsites.net/\n\n    (✓) Done: Deploying service web\n    - Endpoint: https://appfrontend.azurewebsites.net/\n"
    },
    {
      "name": "Azure-Samples/multi-agent-workshop",
      "stars": 15,
      "img": "https://avatars.githubusercontent.com/u/1844662?s=40&v=4",
      "owner": "Azure-Samples",
      "repo_name": "multi-agent-workshop",
      "description": "A short set of exercises that showcase the usage of autogen to create agents",
      "homepage": null,
      "language": "Python",
      "created_at": "2025-02-03T14:00:51Z",
      "updated_at": "2025-04-17T11:45:38Z",
      "topics": [],
      "readme": "\r\n# Multi-Agents with Autogen and Semantic Kernel\r\n\r\nThis project showcases autogen (0.4) by presenting a set of simple scripts that create different type of agents and interactions between them with the purpose of highlighting the main capabilities of the agentic framework. This repo is based on [https://github.com/krishsub/MultiagentHackathon](https://github.com/krishsub/MultiagentHackathon).\r\n\r\n## Getting Started\r\n\r\n### Prerequisites\r\n\r\n- Open AI service deployed with GPT-4o\r\n- Azure Container Apps Session Pool\r\n\r\n### Development environment\r\n\r\n- Dev container option: The project provides a devcontainer configuration that can be used with github codespaces or your own local dev container. So, if you opt for this, you need to have docker on your system.\r\n- Virtual Environment option: if you have python > 3.8 you could choose to create a venv and install all the requirements there.\r\n\r\n#### Leverage AZD to deploy the prerequisites\r\n\r\nYou can leverage the [Azure Developer CLI](https://learn.microsoft.com/azure/developer/azure-developer-cli/), `azd` for short, to deploy the prerequisites to a subscription. It'll create the resources and export some environment variables which you can use to run the exercises. To leverage `azd` you need to have it installed and configured. After that, all it takes is a simple `azd up` and the components will be installed.\r\n\r\nFor a detailed explanation on what it deploys, check out the [README.md in the infra directory](/infra/README.md).\r\n\r\nAfter the components have been deployed, you can navigate to [AI Foundry](https://ai.azure.com/) and obtain the Open AI Key and Endpoint. The Open AI endpoint, as well as the Azure Container Apps endpoint, will be stored in the `azd` environment variables. You can leverage those as well. The Open AI Key is not exposed in this manner for security considerations.\r\n\r\n### Quickstart\r\n\r\n#### Environment Variables\r\n\r\nFor this set of scripts, a gpt-4o model instance was used.\r\nCreate a `.env` file with your Azure OpenAI credentials in the `src` folder:\r\n\r\n```bash\r\nAZURE_OPENAI_API_KEY=your_api_key\r\nAZURE_OPENAI_ENDPOINT=your_endpoint\r\n```\r\n\r\n#### Installation\r\n\r\nChoose one of the following methods to run the different scripts. Make sure to `cd src`.\r\n\r\n#### Option 1: Using uv (Recommended)\r\n\r\n[uv](https://github.com/astral-sh/uv) is a fast Python package installer and runner. If you haven't installed it yet:\r\n\r\n```bash\r\ncurl -LsSf https://astral.sh/uv/install.sh | sh\r\n```\r\n\r\nThen run the script directly (this will automatically install dependencies):\r\n\r\n```bash\r\nuv run 0X_SCRIPT_NAME.py\r\n```\r\n\r\n#### Option 2: Using pip\r\n\r\n1. Install dependencies:\r\n\r\n   ```bash\r\n   pip install -r requirements.txt\r\n   ```\r\n\r\n2. Run the script:\r\n\r\n   ```bash\r\n   python 0X_SCRIPT_NAME.py\r\n   ```\r\n\r\n\r\n# Multi-Agent Hackathon Guide\r\n\r\n## Introduction\r\n\r\nWelcome to the MultiagentHackathon workshop! This project is designed to help you learn and practice implementing multi-agent systems using frameworks like AutoGen, Semantic Kernel, and more. The repository contains a series of progressive exercises that will guide you through building increasingly complex agent systems, from simple single-agent interactions to sophisticated multi-agent collaborative scenarios where code is executed in a dynamic Azure Container Apps Pool.\r\n\r\n## Setup\r\n\r\n### Prerequisites\r\n\r\n- [Python](https://www.python.org/) 3.9+ installed\r\n- An Azure OpenAI API key or OpenAI API key (For production ready deployments, you should refrain from using keys, and switch to [managed identities](https://learn.microsoft.com/entra/identity/managed-identities-azure-resources/overview))\r\n- [Visual Studio Code](https://code.visualstudio.com/)\r\n\r\n### Installation\r\n\r\n1. Open Visual Studio Code (Link in your desktop)\r\n2. Open a Powershell terminal using the upper menu `Terminal -> New Terminal` or by pressing CTRL+SHIFT+` (backtick). Make sure to select the Powershell terminal in the dropdown menu on the top right of the terminal window.\r\n3. Clone the repository:\r\n\r\n   ```Powershell\r\n   git clone https://github.com/Azure-Samples/multi-agent-workshop\r\n   cd multi-agent-workshop\r\n   ```\r\n\r\n4. Select open folder in your VS Code and open the `multi-agent-workshop` folder to see the code in your VS Code.\r\n5. (**Optional, and takes some time**) Open your code in a devcontainer, using the Dev Containers plugin of VS Code and the devcontainer provided in the repo. Once the plugin is installed, if you open the `.devcontainer/devcotainer.json` file, it should ask you to re-open your repo in a devcontainer.\r\n6. Install depedencies (might take a few minutes):\r\n\r\n    [uv](https://github.com/astral-sh/uv) is a fast Python package installer and runner. If you haven't installed it yet:\r\n\r\n    ```Powershell\r\n    powershell -ExecutionPolicy ByPass -c \"irm https://astral.sh/uv/install.ps1 | iex\"\r\n    $env:Path += \";C:\\Users\\Admin\\.local\\bin\"\r\n    uv sync\r\n    ```\r\n\r\n    **OR (without using uv)**\r\n\r\n    ```bash\r\n        python -m venv .venv\r\n    ```\r\n\r\n    ```powershell\r\n        # On Windows\r\n        .venv\\Scripts\\activate\r\n    ```\r\n\r\n    ```bash\r\n        # On macOS/Linux\r\n        source venv/bin/activate\r\n    ```\r\n\r\n    ```powershell\r\n      curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py\r\n      python get-pip.py\r\n      pip --version\r\n    ```\r\n\r\n    ```bash\r\n        pip install -r requirements.txt\r\n    ```\r\n\r\n### Infrastructure Setup\r\n\r\n#### Infrastructure as Code (Option 1 - Preferred)\r\n\r\nYou can leverage the [Azure Developer CLI](https://learn.microsoft.com/azure/developer/azure-developer-cli/), `azd` for short, to deploy the prerequisites to a subscription. It'll create the resources and export some environment variables which you can use to run the exercises. To leverage `azd` you need to have it installed and configured. After that, all it takes is a simple `azd up` and the components will be installed.\r\n\r\nTo install azd you can execute the following command in Powershell.\r\n\r\n```Powershell\r\npowershell -ex AllSigned -c \"Invoke-RestMethod 'https://aka.ms/install-azd.ps1' | Invoke-Expression\"\r\n```\r\n\r\nIf using Linux, you can run:\r\n\r\n```bash\r\ncurl -fsSL https://aka.ms/install-azd.sh | bash\r\n```\r\n\r\nFor a detailed explanation on what it deploys, check out the [README.md in the infra directory](/infra/README.md).\r\n\r\nAfter the components have been deployed, you can navigate to [AI Foundry](https://ai.azure.com/) and obtain the Open AI Key and Endpoint. The Open AI endpoint, as well as the Azure Container Apps endpoint will be stored in the azd environment variables. You can leverage those as well. The Open AI Key is not exposed in this manner for security considerations.\r\n\r\n**Restart your VS Code** before running the next commands in the root folder of the repo.\r\n\r\n```Powershell\r\nazd auth login\r\n```\r\n\r\nTo login follow the instructions and use the credentials for your Azure Subscription from the `Resources` tab\r\n\r\n```Powershell\r\nazd up  \r\n```\r\n\r\nAnd follow the instructions in the terminal to name your environment resources, make sure to select the correct subscription. We recommend choosing `Sweden Central` as Location.\r\n\r\n#### Azure Portal (Option 2)\r\n\r\n1. Navigate to the Azure Portal from the browser of your skillable lab `https://portal.azure.com/#home`\r\n2. Login using the Username and Password available in the Resources tab\r\n3. Open a new tab in your browser and go to `https://ai.azure.com` (you should be logged in already)\r\n4. Create a new project, it should also setup a hub for you.\r\n![Project Creation AI Foundry](images/foundry_project.jpeg)\r\n5. Deploy a GPT-4o model. You can follow the numbered steps in the screenshot below:\r\n![Create deployment](images/foundry_create_deployment.jpeg)\r\n6. Go back to the Azure Portal, and create a Container App Session Pool. The screenshots below can guide you in the process:\r\n![Search Dynamic Pools](images/search_dynamic_pools.jpeg)\r\n\r\n![Create Session Pool](images/create_session_pool.jpeg)\r\n\r\n![Parameters Session Pool](images/parameters_session_pool.jpeg)\r\n\r\n### Configuring your repository\r\n\r\n1. Navigate to the *Azure Portal -> Resource Groups -> YOUR_RESOURCE_GROUP -> openai-YOUR_OWN_SUFFIX -> Explore Azure AI Foundry portal*. Make a note of the endpoint and key, you need then for step 3.  \r\n\r\nYou can also just go to `ai.azure.com` and in the overview of your project you should be able to see your credentials, similar to the ones visible in the picture below. \r\n\r\nThe following screenshot show you how to get your credentials:\r\n   ![Azure Portal Credentials](images/get_ai_credentials.jpeg)\r\n\r\n2. Navigate to *Azure Portal -> Resource Groups -> YOUR_RESOURCE_GROUP -> aca_pool-YOUR_OWN_SUFFIX* and get the Pool Management Endpoint.\r\n   ![Azure Portal Credentials](images/aca_pool_credentials.jpeg)\r\n\r\n3. Set up environment variables:\r\n   - Create a `.env` file in your `exercises` directory.\r\n   - Add your API keys and endpoints (you could get this from the Azure Portal, make sure to have the values between quotes):\r\n\r\n     ``` commandline\r\n     AZURE_OPENAI_URL=your_azure_endpoint\r\n     AZURE_OPENAI_API_KEY=your_azure_api_key\r\n     ACA_POOL_MANAGEMENT_ENDPOINT=you_ACA_pool_endpoint\r\n     ```\r\n\r\n## Repository Structure\r\n\r\n- [`src/`](/src/): Contains working examples of each exercise. We recommend you to not copy-paste solutions, but only look at this folder when you are stuck with an exercise and need some inspiration.\r\n- [`exercises/`](/exercises/): Contains exercise templates for you to complete.\r\n\r\n## Getting Started\r\n\r\nStart with the first exercise and progress through them sequentially. Each exercise builds upon concepts introduced in the previous ones. Click Next when you are ready to continue.\r\n\r\n===\r\n## Introduction to Azure AI Agent Service \r\n\r\nAzure AI Agent Service is a fully managed service designed to empower developers to securely build, deploy, and scale high-quality, extensible AI agents without needing to manage the underlying compute and storage resources. This service simplifies the process of creating AI agents that can answer questions, perform actions, or automate workflows by combining generative AI models with tools that allow interaction with real-world data sources. \r\n\r\nAgent service offers several benefits:\r\n- Ease of Use: What originally took hundreds of lines of code to support client-side function calling can now be done in just a few lines of code.\r\n- Scalability: As a fully managed service, it allows you to focus on building workflows and agents without worrying about scaling, security, or infrastructure management.\r\n- Integration: It uses the same wire protocol as Azure OpenAI Assistants, enabling seamless integration with OpenAI SDKs or Azure AI Foundry SDKs.For example, to create an AI Agent with Azure AI Foundry SDK, you can simply define which model the AI uses, the instructions for how it should complete tasks, and the tools it can use to access and interact with other services. \r\n\r\n### Getting started with Azure AI Agent Service in the AI Foundry portal\r\n\r\n1. Navigate to the Agents Playground underneath \"Build and customize\" to select the gpt-4o model created through the portal earlier. \r\n\r\n![Agent Service Start](images/agent_service_1.png)\r\n\r\n2. After selecting next you will receive a confirmation message that your agent is created, and your agent will also automatically be given a unique ID. Select the agent you just created and then click \"Try in Playground\" to the left.\r\n![Agent Service Start in Playground](images/agent_service_2.png)\r\n\r\n3. Once in the playground view add the following description to the instructions for the agent (in the `Instructions` placeholder on the right side of the screen):\r\n\r\n```\r\nYou are a helpful assistant that can search files and answer questions based on the file content. You can utilize file search grounded on internal data to efficiently search through proprietary documents and provide accurate answers based on the content. This feature allows you to manage and index uploaded files for efficient keyword and semantic search, enhancing your ability to assist users with their queries.\r\n            \r\nYou have access to a code interpreter tool that allows you to analyze data, create visualizations, and perform calculations. Be thorough and precise in your answers. For CSV and structured data, provide meaningful insights and summaries when appropriate.\r\n\r\nUse the code interpreter when:\r\n   1. Analyzing numerical data or statistics in CSV or JSON files\r\n   2. Creating visualizations of data when it would help explain your answer\r\n   3. Performing calculations or transformations on data\r\n   4. Extracting specific information from complex structured data\r\n\r\n When using the code interpreter, follow these guidelines:\r\n   - Write clear, concise code with comments explaining your approach\r\n   - Use pandas for data analysis, matplotlib/seaborn for visualizations\r\n   - Use descriptive variable names and follow best practices\r\n   - Show intermediate steps for complex analyses\r\n   - Interpret results for the user in plain language after showing code output\r\n            \r\nWhen creating visualizations:\r\n   - Use clear titles, axis labels, and legends\r\n   - Choose appropriate chart types for the data\r\n   - Use matplotlib or seaborn for creating visualizations\r\n\r\n```\r\n\r\n4. Add a knowledge source for the agent and a code interpreter tool by clicking the + next to Knowledge & Actions. \r\n\r\n- When you upload data for file search in Azure AI Agent Service, a vector store is created to manage and search the uploaded files. This process involves several steps to ensure the data is properly indexed and searchable.\r\n- First, the uploaded files are automatically parsed and chunked into smaller segments. Each chunk is then embedded, meaning it is converted into a vector representation that captures the semantic meaning of the content. These vectors are stored in a vector database, which supports both keyword and semantic search capabilities\r\n- Once the vector store is created, it can be attached to both agents and threads, allowing for seamless integration and efficient file management across different AI applications\r\n- The code interpreter tool enables agents to write and run Python code in a sandboxed environment. This feature is useful for tasks like generating graphs, performing calculations, and analyzing datasets. Agents can iteratively modify and rerun code until execution succeeds.\r\n\r\n![Agent Service Add Data & Tool](images/agent_service_3.png)\r\n\r\nFor the Knowledge Source click Files, and then Select Local Files to add and upload the `multi-agent-workshop/data/Internal Policy Document for Contoso Tech Support Agents.pdf` \r\n\r\n![Agent Service Knowledge Source](images/agent_service_4.png)\r\n\r\nFor the Actions, select Code Interpreter and then to add a Code Interpreter Action select `multi-agent-workshop/data/Contoso_Tech_Product_Data.csv` from local files and upload it to the Code Interpreter tool\r\n![Agent Service Code Interpreter](images/agent_service_5.png)\r\n\r\nFor context on the two files we have just uploaded:\r\n- The pdf file is an Internal Policy Document for Contoso Tech Support Agents, which outlines the policies and procedures that support agents must follow when assisting customers. It covers key guidelines for handling returns, processing warranty claims, shipping options, order tracking, privacy policy, and customer support procedures. \r\n- The csv file provided contains sales data for various products sold by Contoso Tech. Each product is listed with its name, category, price, units sold, and the quarter in which the sales occurred. This data provides insights into the sales performance of different products over specific quarters. \r\n\r\n5. Chat with your agent \r\nNow you're free to ask your agent questions based on the data provided, here are some sample questions to try out and see how the agent responds. We can also ask for analysis based on the data provided. \r\n\r\nQuestions for the policy document:\r\n```\r\nWhat are the key guidelines for handling returns at Contoso Tech?\r\n\r\nWhich items are non-refundable according to Contoso Tech's policy?\r\n\r\nWhat are the contact details for Contoso Tech's support team?\r\n```\r\n\r\nPrompts for the laptop data:\r\n```\r\nCreate a bar chart highlighting the top 5 products with the highest units sold.\r\n\r\nGenerate a pie chart showing the units sold for each product category. Include the number of units per category as well as percentages.\r\n```\r\n\r\nIn this section we explored the Azure AI Agent Service, focusing on creating agents and adding tools like file search for searching through proprietary data and the code interpreter for data analysis. These features enable efficient data management, automated workflows, and insightful visualizations. The agent service playground provides a robust environment for experimenting with these tools and enhancing your projects. In the following section we explore how to get started with running and creating agents from a code-first point of view. \r\n\r\n===\r\n## Exercise 0: Call a model\r\n\r\n### Objective\r\n\r\nLearn how to call your LLM model without using agents.\r\n\r\n### Instructions\r\n\r\nRefer to [`exercises/00_call_models.py`](/exercises/00_call_models.py) for a complete example.\r\n\r\n```Powershell\r\n uv run .\\exercises\\00_call_models.py\r\n```\r\n\r\nOR (if you are not using uv)\r\n\r\n```Powershell\r\npython .\\exercises/00_call_models.py\r\n```\r\n\r\nThis is a very simple script that only calls the LLM deployed. It should serve as the starting point of the next exercises and should validate the connection to your LLM. If the code runs properly, you should see a joke created by the model in the terminal.\r\n\r\n===\r\n\r\n## Exercise 1: Single Agent\r\n\r\n### Objective\r\n\r\nLearn how to create and interact with a single AI agent.\r\n\r\n### Instructions\r\n\r\nRefer to [`exercises/01_single_agent.py`](/exercises/01_single_agent.py) for a complete example.\r\n\r\nThis is the foundation of agent-based systems. Understand how a basic agent system works before proceeding to more complex multi-agent scenarios.\r\n\r\n===\r\n\r\n## Exercise 2: Two Agents\r\n\r\n### Objective\r\n\r\nImplement a conversation between two agents (Chandler and Joey) who exchange jokes.\r\n\r\n### Instructions\r\n\r\n1. Open [`exercises/02_two_agents.py`](/exercises/02_two_agents.py)\r\n2. Complete the TODOs in the file:\r\n   - Create a `ChatCompletionClient` using the provided `llm_config`\r\n   - Create two `AssistantAgent` instances with appropriate system messages:\r\n     - Chandler should tell short story jokes related to friends\r\n     - Joey should respond to jokes with another joke\r\n     - Both should be able to end the conversation after 2 jokes by saying 'FINISH'\r\n   - Create a termination condition using `TextMentionTermination`\r\n   - Create a `RoundRobinGroupChat` team with both agents\r\n   - Run the conversation and print the results\r\n   - Reset the team and run another conversation as a stream\r\n3. Documentation is available here: [Autogen Docs](https://microsoft.github.io/autogen/stable/index.html)\r\n\r\n### Expected Outcome\r\n\r\nTwo agents exchanging jokes in a structured conversation that terminates after a set number of exchanges.\r\n\r\n===\r\n\r\n## Exercise 3: Two Agents Guessing Game\r\n\r\n### Objective\r\n\r\nCreate a number guessing game where two agents interact: one tries to guess a random number, and the other provides feedback.\r\n\r\n### Instructions\r\n\r\n1. Open [`exercises/03_two_agents_guessing_game.py`](/exercises/03_two_agents_guessing_game.py)\r\n2. Complete the TODOs in the file:\r\n   - Create an OpenAI model client using `ChatCompletionClient.load_component()`\r\n   - Create a guesser agent that tries to guess a number between 1-100\r\n   - Create a player agent that provides feedback on guesses (too high/too low)\r\n   - Set up a termination condition that ends the game when 'FINISH' is mentioned\r\n   - Create a team with the two agents using `RoundRobinGroupChat`\r\n   - Uncomment the code that runs the team chat to test your implementation\r\n3. Documentation is available here: [Autogen Docs](https://microsoft.github.io/autogen/stable/index.html)\r\n\r\n### Expected Outcome\r\n\r\nA functional guessing game where agents take turns until the correct number is guessed.\r\n\r\n===\r\n\r\n## Exercise 4: Generate and Run Code in Conversations\r\n\r\n### Objective\r\n\r\nBuild a system with two agents that can write and execute code collaboratively.\r\n\r\n### Instructions\r\n\r\n1. Open [`exercises/04_generate_and_run_code_in_conversations.py`](/exercises/04_generate_and_run_code_in_conversations.py)\r\n2. Complete the TODOs:\r\n   - Create a system message for the code writer agent with specific instructions\r\n   - Set up a local command line executor using `LocalCommandLineCodeExecutor`\r\n   - Create a code executor agent that uses this executor\r\n   - Create a code writer agent with the system message defined earlier\r\n   - Implement a termination condition for when \"FINISH\" is mentioned\r\n   - Create a team with round-robin chat including both agents\r\n3. Documentation is available here: [Autogen Docs](https://microsoft.github.io/autogen/stable/index.html)\r\n\r\n### Expected Outcome\r\n\r\nA system where one agent proposes Python code to calculate the 14th Fibonacci number, and another agent executes it. It is important to note that in this example, the `code_executor_agent` executes the code locally, can you think about reasons to avoid this?\r\n\r\n===\r\n\r\n## Exercise 5: Custom Agents Run Code\r\n\r\n### Objective\r\n\r\nImplement custom agents with code execution capabilities using the AutoGen Core framework. You can think of this code as an evolved or more production ready version of the previous exercise, where the agents interact for a longer time to solve a more complex problem. You will notice that the Assistant agent keeps track of the history of the conversation and keep working on the problem until the executor is able to provide the expected output.\r\n\r\n### Instructions\r\n\r\n1. Open [`exercises/05_custom_agents_run_code.py`](/exercises/05_custom_agents_run_code.py)\r\n2. Complete the TODOs:\r\n   - Initialize chat history with a SystemMessage in the Assistant class\r\n   - Implement message handling logic for the Assistant\r\n   - Create a function to extract code blocks from markdown text\r\n   - Implement the message handler for the Executor agent\r\n   - Complete the main function to set up and run the coding agents\r\n   - Call the `coding_agents` function with `asyncio.run()`\r\n3. Documentation is available here: [Autogen Docs](https://microsoft.github.io/autogen/stable/index.html)\r\n\r\n### Expected Outcome\r\n\r\nA system where an assistant generates code in markdown blocks, and an executor extracts and runs that code. After the code is executed succesfully, you should be able to see the output in the [`exercises/generated`](/exercises/generated/) folder.\r\n\r\n===\r\n\r\n## Exercise 6: Human in the Loop\r\n\r\n### Objective\r\n\r\nCreate a human-in-the-loop interaction between an assistant agent and a user proxy agent.\r\n\r\n### Instructions\r\n\r\n1. Open [`exercises/06_human_in_the_loop.py`](/exercises/06_human_in_the_loop.py)\r\n2. Complete the TODOs:\r\n   - Create a `ChatCompletionClient` using the provided LLM configuration\r\n   - Initialize an `AssistantAgent` with the created client\r\n   - Initialize a `UserProxyAgent` to get user input from the console\r\n   - Create a termination condition that ends when the user says \"APPROVE\"\r\n   - Create a `RoundRobinGroupChat` team with the assistant and user proxy agents\r\n   - Run the conversation and stream to the console\r\n3. Documentation is available here: [Autogen Docs](https://microsoft.github.io/autogen/stable/index.html)\r\n\r\n### Expected Outcome\r\n\r\nAn interactive session where a human can converse with an AI assistant until they approve the results.\r\n\r\n===\r\n\r\n## Exercise 7: Functions Invoked by Agents\r\n\r\n### Objective\r\n\r\nImplement a function that can be invoked by an agent and configure the agent to use it.\r\n\r\n### Instructions\r\n\r\n1. Open [`exercises/07_functions_invoked_by_agents.py`](/exercises/07_functions_invoked_by_agents.py)\r\n2. Complete the TODOs:\r\n   - Implement the calculator function to perform basic arithmetic operations\r\n   - Initialize the `AssistantAgent` with proper configuration:\r\n     - Give it an appropriate name\r\n     - Write a system message instructing it to use the calculator and get_time tool\r\n     - Set up the model client\r\n     - Add the calculator function to the tools list\r\n     - Configure whether the agent should reflect on tool use\r\n   - Process user input and get the assistant's response\r\n3. Documentation is available here: [Autogen Docs](https://microsoft.github.io/autogen/stable/index.html)\r\n\r\n### Expected Outcome\r\n\r\nAn agent that can perform calculations using a custom calculator function when prompted.\r\n\r\n===\r\n\r\n## Exercise 8: Generate Run Code in Remote Container on ACA\r\n\r\n### Objective\r\n\r\nLearn how to execute code in a remote Azure Container Apps environment for secure and isolated execution.\r\n\r\n### Troubleshooting\r\n\r\nThis is the first time that you will interact with the ACA Session Pool, thus, you might require to run the following commands beforehand (run only if you don't have Azure CLI installed):\r\n\r\nOpen a Windows Powershell terminal as Administrator and run the command below to install the azure cli:\r\n\r\n```Powershell\r\n$ProgressPreference = 'SilentlyContinue'; Invoke-WebRequest -Uri https://aka.ms/installazurecliwindows -OutFile .\\AzureCLI.msi; Start-Process msiexec.exe -Wait -ArgumentList '/I AzureCLI.msi /quiet'; Remove-Item .\\AzureCLI.msi\r\n```\r\nIn your VS Code, close the current terminal and open a new one to make sure that the PATH is updated, and then run: \r\n\r\n```Powershell\r\naz login \r\n## Select your subscription from the list and continue\r\n```\r\nIt might be the case that your uv is not on your PATH after killing your current VS Code terminal, if that is the case, run again:\r\n\r\n```Powershell\r\n$env:Path += \";C:\\Users\\Admin\\.local\\bin\"\r\n```\r\nIf you experiment issues when running the code in the container, due to lack of permissions, you might need to add the `Azure ContainerApps Session Executor` to your user in the ACA Dynamic Pool, use the user that you have in the `Resources` tab.\r\n\r\n![Add Role 1](images/add_role_1.jpeg)\r\n![Add Role 2](images/add_role_2.jpeg)\r\n![Add Role 3](images/add_role_3.jpeg)\r\n\r\n### Instructions\r\n\r\n1. Open [`exercises/08_generate_run_code_in_remote_container_on_aca_langchain.py`](/exercises/08_generate_run_code_in_remote_container_on_aca_langchain.py)\r\n2. Implement the `RemoteExecutor` class:\r\n   - Initialize parameters for connection to Azure Container Apps\r\n   - Implement the `execute_code_blocks` method to run code in a remote container\r\n3. Complete the `run_remote_coding_agents` function:\r\n   - Load environment variables\r\n   - Set up the agent runtime\r\n   - Initialize the model client and remote executor\r\n   - Register the assistant and executor agents\r\n   - Start the runtime and publish an initial message\r\n4. Documentation is available here: [Autogen Docs](https://microsoft.github.io/autogen/stable/index.html); [LangChain](https://api.python.langchain.com/en/latest/tools/langchain_azure_dynamic_sessions.tools.sessions.SessionsPythonREPLTool.html)\r\n\r\n### Expected Outcome\r\n\r\nA system that can generate and execute code in a secure, remote container environment.\r\n\r\n===\r\n\r\n## Exercise 9: Group Chat Coding Problem with Semantic Kernel\r\n\r\n### Objective\r\n\r\nSet up a group chat between two agents using Semantic Kernel to solve coding problems.\r\n\r\n### Instructions\r\n\r\n1. Open [`exercises/09_group_chat_coding_problem_sk.py`](/exercises/09_group_chat_coding_problem_sk.py)\r\n2. Implement the missing functions:\r\n   - `create_code_agent`: Create an agent specialized in executing Python code\r\n     - Configure the Python code interpreter tool\r\n     - Add the code interpreter plugin to the kernel\r\n     - Create a ChatCompletionAgent with appropriate description\r\n   - `create_chat_agent`: Create a chat agent that interacts with users and coordinates with the code agent\r\n   - Complete the `main` function to:\r\n     - Create the agents\r\n     - Set up an agent group chat\r\n     - Create a chat history with initial system message\r\n     - Add the user question and start the group chat\r\n     - Display messages from each agent\r\n3. Documentation is available here: [Autogen Docs](https://microsoft.github.io/autogen/stable/index.html); [Semantic Kernel](https://learn.microsoft.com/en-us/semantic-kernel/)\r\n\r\n### Expected Outcome\r\n\r\nA group chat where agents collaborate to solve coding problems, with one agent generating code and another executing it.\r\n\r\n===\r\n\r\n## Conclusion\r\n\r\nCongratulations on completing the MultiagentHackathon exercises! You've learned how to create and manage various types of agent systems, from simple single-agent interactions to complex multi-agent collaborations with code execution capabilities.\r\n\r\n### Next Steps\r\n\r\n1. Try modifying the agents' system prompts to see how it affects their behavior\r\n2. Experiment with different termination conditions\r\n3. Create your own multi-agent system for a specific use case\r\n4. Explore more advanced features like:\r\n   - Memory and state management\r\n   - Tool use and function calling\r\n   - Integration with external APIs\r\n\r\nWe welcome your contributions and feedback to improve this hackathon. Please submit issues or pull requests to the repository.\r\n\r\nHappy coding!\r\n"
    },
    {
      "name": "kevon217/custom-agency-swarms",
      "stars": 15,
      "img": "https://avatars.githubusercontent.com/u/13077896?s=40&v=4",
      "owner": "kevon217",
      "repo_name": "custom-agency-swarms",
      "description": "playground for custom gpts built with agency-swarms (https://github.com/VRSEN/agency-swarm)",
      "homepage": "",
      "language": "Python",
      "created_at": "2023-12-27T17:16:06Z",
      "updated_at": "2025-04-17T09:24:05Z",
      "topics": [],
      "readme": "## testing playground for building custom gpt-powered agencies with agency-swarm by VRSEN\n\n# Current Agencies\n\n1. CodeGents (WIP)\n2. DataGents (WIP)\n3. WebBrowsing (WIP)\n4. JobProposalGents (WIP)\n"
    },
    {
      "name": "johnmaeda/pizzashop-and-ai",
      "stars": 15,
      "img": "https://avatars.githubusercontent.com/u/2140796?s=40&v=4",
      "owner": "johnmaeda",
      "repo_name": "pizzashop-and-ai",
      "description": "Playing off of Andrew Ng's TED talk on the business value of AI and his pizza shop example",
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2023-07-31T19:48:02Z",
      "updated_at": "2024-07-19T15:49:39Z",
      "topics": [],
      "readme": "# pizzashop-and-ai\n\nPlease star the [official Semantic Kernel repo](https://github.com/microsoft/semantic-kernel) if you love [Sam Schillace](https://devblogs.microsoft.com/semantic-kernel/early-lessons-from-gpt-4-the-schillace-laws/)'s GPT4-infused approach to democratize AI productivity gains being led and built right now by the Jan, Jane, and Joe app developers out there. So much is changing right now that having an enterprise-first approach to creating with LLMs that is decidedly \"boring\" 🤓 feels like a good thing. And if you want to run the cooler things we have in Semantic Kernel give our recently refreshed [Chat Copilot](https://github.com/microsoft/chat-copilot) (w/ PLUGINS & PLANNERS & PERSONAS!) a test drive.\n\nNEW course on DeepLearning.ai with Andrew NG is now available [link](https://www.linkedin.com/posts/andrewyng_how-can-business-leaders-not-just-coders-activity-7102673627975749633-k7Bt?utm_source=share&utm_medium=member_desktop)\n\n![](assets/deeplearning.jpg)\n\nThis little repo plays off of Andrew Ng's [TED talk](https://www.ted.com/talks/andrew_ng_how_ai_could_empower_any_business) on the business value of AI and the example he gave of helping a pizza shop become more successful with AI. This is a set of six notebooks for AI aficionados in Semantic Kernel to build on Ng's vision of the future where AI is accessible to everyone. Enjoy!\n"
    },
    {
      "name": "Azure-Samples/llm-agent-ops-toolkit-sk",
      "stars": 14,
      "img": "https://avatars.githubusercontent.com/u/1844662?s=40&v=4",
      "owner": "Azure-Samples",
      "repo_name": "llm-agent-ops-toolkit-sk",
      "description": "The LLMAgentOps Toolkit is a repository that provides a foundational structure for building LLM Agent-based applications using the Semantic Kernel. It serves as a starting point for data scientists and developers, facilitating experimentation, evaluation, and deployment of LLM Agent-based applications to production.",
      "homepage": "",
      "language": "Python",
      "created_at": "2025-02-07T09:28:53Z",
      "updated_at": "2025-04-14T21:55:36Z",
      "topics": [
        "azure-ai-foundry",
        "azure-webapp",
        "llm",
        "llmagentops",
        "llmagents",
        "llmops",
        "semantic-kernel",
        "stateflow"
      ],
      "readme": "# LLMAgentOps Toolkit (Semantic Kernel)\n\n`LLMAgentOps` Toolkit is repository that contains basic structure of LLM Agent based application built on top of the Semantic Kernel. The toolkit is designed to be a starting point for data scientists and developers for experimentation to evaluation and finally deploy to production their own LLM Agent based applications.\n\nThe sample `MySql Copilot` has been implemented using the concept of `StateFlow` (a Finite State Machine FSM based LLM workflow) using [Semantic Kernel](https://learn.microsoft.com/en-us/semantic-kernel/overview/) agents. This is equivalent to [AutoGen Selector Group Chat Pattern with custom selector function](https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/selector-group-chat.html#custom-selector-function)\n\nFor more details on `StateFlow` refer the research paper - [StateFlow: Enhancing LLM Task-Solving through State-Driven Workflows](https://arxiv.org/abs/2403.11322).\n\nThis toolkit can be used by replacing the `MySql Copilot` with any other LLM Agent based solution or it can be enhanced for a specific use case.\n\n## Architecture\n\nThe `LLMAgentOps` Architecture might be constructed using the following key components divided into two phases similar to DevOps / MLOps / LLMOps development and deployment phases:\n\n- **LLM Agent Development Phase (inner loop)**: \n    - `Agent Architecture`: Designing the agent architecture for the LLM Agent based solution. For this sample we have used `Semantic Kernel` development kit by using `Python` programming language.\n    - `Experimentation & Evaluation`: Experimentation and Evaluation of the LLM Agent based solution. Where the experimentation is done using `console` or `ui` or in `batch` mode and evaluation is done using `LLM as Judge` and `Human Evaluation`.\n- **LLM Agent Deployment Phase (outer loop)**:\n    - `GitHub Actions`: Continuous Integration, Continuous Evaluation and Continuous Deployment of the LLM Agent based solution with addition of **Continuous Security** for security checks of the LLM Agents.\n    - `Deployment`: Deployment of the LLM Agent based solution in `local` or `cloud` environment.\n    - `Monitoring`: Monitoring the LLM Agent based solution for data collection, performance and other metrics.\n\n## Key Features\n\nThis repository is having the follow key features:\n\n- **Source Code Structure**: The [source code](./src/) is structured in such a way that it can be easily developed and maintained by data scientists and developers together with following key concepts of dividing the code into two parts - `core` and `ops`:\n    - **Core**: The LLM Agent core implementation code.\n        - **Agents Base Class**: The [base class](./src/agents/base.py) for the agents.\n        - **Agents**: All of the [agents](./src/agents/) with their specific prompts and descriptions. Example: [Observe Agent](./src/agents/observe.py).\n        - **Code Execute Agent**: The [code execute agent](./src/agents/execute.py) is an agent that can join the group of agents but it will execute the code (in this sample it is SQL queries) and return the result, instead of using LLM for generating response like other agents.\n        - **Group Chat Selection Logic**: The [group chat selection logic](./src/groupchat/state_flow_selection_strategy.py) is used to select the appropriate next agent based on the current state of the conversation. In this sample the concept of `StateFlow` is used for the selection of the next agent.\n        - **Group Chat Termination Logic**: The [group chat termination logic](./src/groupchat/state_flow_termination_strategy.py) is used to terminate the conversation based on the current state of the conversation or maximum number of turns. In this sample the concept of `StateFlow` is used for the termination of the conversation.\n        - **Group Chat**: The [group chat](./src/groupchat/state_flow_chat.py) contains the group chat client that can serve the conversation between the user and the agents.\n    - **Ops**: The operational code for the LLM Agent based solution.\n        - **Observability**: The [observability code](./src/logging) contains the code for logging and monitoring the agents. In this sample `OpenTelemetry` is used for logging and monitoring.\n        - **MySql Interaction**: The [MySql interaction code](./src/mysql/execution_env.py) contains the code for interacting with MySql database.\n        - **Deployment**: The[deployment code contains the code for deploying the agents in local or cloud environment. In this sample the code is provided for deploying the agents in Azure Web App Service. The deployment code will be:\n            - [Source Module](./src/): core implementation of the agents and group chat.\n            - [REST API Based App](./app_rest_api.py): REST API based app for calling the agents and getting the response (in this example it's `FastAPI`).\n            - [Dockefile](./Dockerfile): Dockerfile for building the image of the entire application.\n            - [Requirements](./requirements.txt) file for the dependencies.\n- **Experimentation**: The [experimentation](./experimentation/) setup by using `console` or `ui` or in `batch` mode.\n- **Evaluation**: The [evaluation](./evaluation/) setup by using `LLM as Judge` and `Human Evaluation`.\n- **Security**: The [security](./security/README.md) setup for the security checks of the LLM Agent based solution.\n- **GitHub Actions**: The [CI CE CD and CS](./.github/workflows/) setup for the continuous integration, continuous evaluation, continuous deployment and continuous security of the LLM Agent based solution.\n- **Engineering Fundamentals**: The [engineering fundamentals](#engineering-fundamentals) for the development and maintenance of the LLM Agent based solution.\n\n## How to use this repository\n\nThis repository is having a sample implementation, that can be used as-is by following the steps below. Or the sample can be replaced with any other LLM Agent based solution or it can be enhanced for a specific use case.\n\n### Pre-requisites\n\n- [Visual Studio Code](https://code.visualstudio.com/) with [Dev Containers](https://code.visualstudio.com/docs/remote/containers) extension.\n- [Docker Desktop](https://www.docker.com/products/docker-desktop/).\n- [Azure AI Foundry Service](https://learn.microsoft.com/en-us/azure/ai-studio/what-is-ai-studio).\n- [Azure OpenAI Chat Model](https://learn.microsoft.com/en-us/azure/ai-studio/quickstarts/get-started-playground#deploy-a-chat-model).\n- [Azure Web App Service](https://learn.microsoft.com/en-us/azure/app-service/overview) (needed only for CD).\n- [Azure Application Insights](https://learn.microsoft.com/en-us/azure/azure-monitor/app/app-insights-overview) (needed only for CD).\n\n### Experimentation\n\n[Experimentation](./experimentation/README.md) is the process of designing the agents and testing a hypothesis or a proposed LLM Agents based solution to a problem.\n\n### Evaluation\n\n[Evaluation](./evaluation/README.md) is the process of evaluating the performance of the LLM Agents based solution, that will help in the decision making process of the LLM Agents based solution.\n\n## Security\n\n[Security](./security/README.md) is the process of ensuring the security of the LLM Agents based solution. Agents are going to write / execute code, browse the web, and interact with databases, hence security is a key concern and must be designed and implemented from the beginning.\n\n### GitHub Actions\n\nThe repository is setup with [GitHub Actions](.github/workflows/) for the continuous integration, continuous evaluation, continuous deployment and continuous security of the LLM Agent based solution.\n\n- **CI**: The [CI](./.github/workflows/ci.yml) workflow is triggered on every push or pull request to the repository. The CI workflow will run the unit tests and linting checks.\n- **CE**: The [CE](./.github/workflows/ce.yml) workflow is triggered manually. The CE workflow will run the batch experimentation and batch evaluation (LLM as Judge).\n- **CD**: The [CD](./.github/workflows/cd.yml) workflow is triggered manually. The CD workflow will deploy the LLM Agent based solution to the Azure Web App Service.\n- **CS**: The [CS](./.github/workflows/sc.yml) workflow is triggered manually. The CS workflow will run the security checks of the LLM Agent based solution.\n\n### Engineering Fundamentals\n\n#### Dev Containers\n\nThe repository is setup with [Dev Containers](https://code.visualstudio.com/docs/remote/containers) for development and testing.\n\n#### Local Linting\n\n```bash\nconda activate base\npylint src\n```\n\n#### Local Unit Testing\n\n```bash\nconda activate base\npython -m unittest discover -s tests\n```\n\nGet the test coverage report:\n\n```bash\npip install coverage\npython -m coverage run --source src -m unittest discover -s tests\npython -m coverage report -m\n```\n\n#### Local Functional Testing (Docker)\n\n```bash\ncp env_docker .env_docker # only once and update the values\ndocker build --rm -t stateflow-semantic-kernel-api:latest .\ndocker run -d --link mysql_server:mysql-local --name StateFlowApiSemanticKernel -p 8085:8000 --env-file .env_docker stateflow-semantic-kernel-api:latest\n```\n"
    },
    {
      "name": "Azure-Samples/container-apps-dynamic-sessions-samples",
      "stars": 14,
      "img": "https://avatars.githubusercontent.com/u/1844662?s=40&v=4",
      "owner": "Azure-Samples",
      "repo_name": "container-apps-dynamic-sessions-samples",
      "description": "Samples for Azure Container Apps dynamic sessions",
      "homepage": null,
      "language": "Python",
      "created_at": "2024-05-07T00:24:54Z",
      "updated_at": "2025-04-21T22:52:26Z",
      "topics": [],
      "readme": "# Azure Container Apps dynamic sessions samples\n\nSee tutorials:\n* [LangChain](https://learn.microsoft.com/azure/container-apps/sessions-tutorial-langchain)\n* [LlamaIndex](https://learn.microsoft.com/azure/container-apps/sessions-tutorial-llamaindex)\n* [Semantic Kernel](https://learn.microsoft.com/azure/container-apps/sessions-tutorial-semantic-kernel)\n* [AutoGen](https://learn.microsoft.com/azure/container-apps/sessions-tutorial-autogen)\n"
    },
    {
      "name": "msalemor/llm-use-cases",
      "stars": 14,
      "img": "https://avatars.githubusercontent.com/u/44649358?s=40&v=4",
      "owner": "msalemor",
      "repo_name": "llm-use-cases",
      "description": null,
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2023-04-26T18:27:33Z",
      "updated_at": "2025-04-21T15:33:04Z",
      "topics": [],
      "readme": "# Foundational Model Use Cases\n\n## Foundational Models\n\n## Requirements\n\n- Python 3.7-3.10\n- Create `notebooks\\.env` file\n\n```\nOPENAI_URI=https://<NAME>.openai.azure.com/\nOPENAI_KEY=<API_KEY>\nOPENAI_VERSION=2023-07-01-preview\nOPENAI_GPT_DEPLOYMENT=<GPT_DEPLOYMENT_NAME>\nOPENAI_ADA_DEPLOYMENT=<ADA_DEPLOYMENT_NAME>\n```\n\n- Open a Notebook and click play on the cells\n\n## Common Use Cases\n\n- [Summarization & Risk Analysis](SUMMARIZATION.md)\n  - Get a contract, summarize it, and identify the risks.\n- [Recommendation system](RECOMMENDATION.md)\n  - Recommend from an up-to-date list of restaurants\n- [Language Translation](TRANSLATION.md)\n  - Translate technical text from a target to a source language\n- [Content generation and Personalization](GENERATION.md)\n  - Generate a sales car description\n- [Intent and entities - A LUIS-like scenario](INTENT.md)\n  - Find the user's intent and entities\n- [Sentiment Analysis - Zero Shot](SENTIMENT.md)\n  - Sentiment Analysis  \n- [Scoring - Zero Shot](SCORING.md)\n  - Scoring\n- [Classification](CLASSIFICATION.md)\n  - Classification\n- [RAG Pattern without a vector database](RAGNODB.md)\n  - Provide the container and answer the question based only on the provided content.\n"
    },
    {
      "name": "microsoft/agents-humanoversight",
      "stars": 13,
      "img": "https://avatars.githubusercontent.com/u/6154722?s=40&v=4",
      "owner": "microsoft",
      "repo_name": "agents-humanoversight",
      "description": "Human Oversight for Autonomous AI Agents using Azure Logic Apps + Python",
      "homepage": null,
      "language": "Python",
      "created_at": "2025-04-13T16:36:24Z",
      "updated_at": "2025-04-23T11:16:16Z",
      "topics": [],
      "readme": "<div align=\"center\">\n  <img src=\"docs/images/logo.png\" alt=\"Human Oversight\" width=\"125\"/>\n</div>\n\n# Human Oversight for AI Agents\n\n[![Tests](https://github.com/microsoft/agents-humanoversight/actions/workflows/tests.yml/badge.svg)](https://github.com/microsoft/agents-humanoversight/actions/workflows/tests.yml)\n[![Python 3.10+](https://img.shields.io/badge/python-3.9%2B-blue.svg)](https://www.python.org/downloads/)\n[![Azure Logic Apps](https://img.shields.io/badge/Azure-Logic%20Apps-0089D6.svg)](https://azure.microsoft.com/services/logic-apps/)\n[![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg)](https://github.com/microsoft/agents-humanintheloop/pulls)\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n\nThis solution accelerator provides a pattern for integrating human approval steps into autonomous AI agent workflows using Azure Logic Apps and a Python decorator. As AI systems become more powerful and autonomous, implementing human oversight mechanisms becomes critical for safety and compliance.\n\n\n## Table of Contents\n- [Why Human Oversight for AI Systems?](#why-human-oversight-for-ai-systems)\n- [How It Works](#how-it-works)\n- [Code Example](#code-example)\n- [Included Demos](#included-demos)\n  - [1. OpenAI Client Demo (`app/openai_client_demo.py`)](#1-openai-client-demo-appopenai_client_demo.py)\n  - [2. Semantic Kernel Multi-Agent Demo (`app/sk_demo.py`)](#2-semantic-kernel-multi-agent-demo-appsk_demo.py)\n- [Deploying the Solution](#deploying-the-solution)\n  - [Prerequisites](#step-1-prerequisites)\n  - [Setup](#step-2-clone-the-repository-and-set-up-environment)\n  - [Deploy Azure Resources](#step-3-deploy-azure-resources-using-bicep)\n  - [Authorize Office 365](#step-4-authorize-office-365-connection)\n  - [Configure Python App](#step-5-configure-and-run-the-python-application)\n- [Approval Workflow Experience](#approval-workflow-experience)\n- [Reporting](#reporting)\n- [Customizing the Solution](#customizing-the-solution)\n- [Security Considerations](#security-considerations)\n- [Contributing](#contributing)\n\n\n## Why Human Oversight for AI Systems?\n\nAutonomous AI agent systems can perform complex tasks with minimal supervision, but certain critical actions should require human approval before execution, such as:\n\n- Deleting or modifying important resources\n- Taking actions that impact user data or privacy\n- Financial transactions or high-risk operations\n- Actions with significant business impact or security implications\n\nThis solution provides a flexible, auditable human approval workflow that can be easily integrated into existing AI agent code with minimal changes. The design is orchestrator-agnostic, allowing it to work with virtually any AI agent framework or orchestration system.\n\n## How It Works\n\n1. Your AI agent uses a tool or function that has been annotated with the `@approval_gate` decorator\n2. The decorator intercepts the action and sends the details to an Azure Logic App\n3. The Logic App emails designated approvers with action details and approve/reject buttons\n4. The Python code waits for a response (with configurable timeout)\n5. If approved, the original tool/function executes; if rejected or timed out, a default value is returned\n\n<div align=\"center\">\n  <img src=\"docs/images/activity-diagram.png\" alt=\"Activity diagram\" width=\"500\"/>\n</div>\n\n<div align=\"center\">\n  <img src=\"docs/images/logic-app-workflow.png\" alt=\"Logic App Workflow\" width=\"700\"/>\n</div>\n\n## Code Example\n\nHere's a real-world example from the included sample application:\n\n```python\nfrom human_oversight import approval_gate\n\n# Regular function - no approval required\ndef list_users(location_filter: str = None):\n    \"\"\"Lists users, optionally filtering by location (domain part of email).\"\"\"\n    print(f\"Executing list_users(location_filter='{location_filter}')...\")\n    if not location_filter:\n        return json.dumps(list(MOCK_USERS.values()))\n    else:\n        filtered_users = [\n            user for user in MOCK_USERS.values()\n            if user[\"email\"].endswith(f\"@{location_filter}\")\n        ]\n        return json.dumps(filtered_users)\n\n# Critical function with approval gate\n@approval_gate(\n    agent_name=AGENT_NAME,\n    action_description=\"Delete User Account\",\n    approver_emails=APPROVERS,\n    refusal_return_value=\"DENIED: User deletion was not approved.\", \n)\ndef delete_user(user_id: str):\n    \"\"\"Deletes a user account. Requires human approval via the Approval Gate.\"\"\"\n    print(f\"Executing delete_user(user_id='{user_id}')...\")\n    if user_id in MOCK_USERS:\n        deleted_user = MOCK_USERS.pop(user_id)\n        print(f\"Successfully deleted user: {deleted_user['name']} (ID: {user_id})\")\n        return json.dumps({\n            \"status\": \"success\", \n            \"message\": f\"User {user_id} deleted.\", \n            \"deleted_user\": deleted_user\n        })\n    else:\n        print(f\"User ID '{user_id}' not found.\")\n        return json.dumps({\n            \"status\": \"error\", \n            \"message\": f\"User {user_id} not found.\"\n        })\n```\n\n## Included Demos\n\nThe project includes two demo applications:\n\n### 1. OpenAI Client Demo (`app/openai_client_demo.py`)\n\nThis demo shows an integration with the Azure OpenAI client. It demonstrates:\n- Direct tool usage with OpenAI function calling\n- Human oversight for critical operations (user deletion)\n- Simple prompt-based interaction\n\n```bash\n# Run the OpenAI Client demo\ncd app\npython openai_client_demo.py\n```\n\n### 2. Semantic Kernel Multi-Agent Demo (`app/sk_demo.py`)\n\nThis more advanced demo showcases integration with Microsoft's Semantic Kernel framework in a multi-agent system:\n- A collaborative system with three specialized agents (Researcher, Critic, Publisher)\n- GitHub code search capabilities\n- Human oversight for publishing operation\n- Complex agent-to-agent interactions\n\n```bash\n# Run the Semantic Kernel demo\ncd app\npython sk_demo.py\n```\n\n## Deploying the Solution\n\n### Step 1: Prerequisites\n\n- Azure Subscription\n- Azure CLI installed and logged in\n- Office 365 account with permissions to send emails\n\n### Step 2: Clone the Repository and Set Up Environment\n\n```bash\ngit clone https://github.com/microsoft/agents-humanoversight.git\ncd agents-humanoversight\n```\n\n### Step 3: Deploy Azure Resources using Bicep\n\n```bash\n# Login to Azure\naz login\n\n# Set your subscription\naz account set --subscription \"<Your-Subscription-ID>\"\n\n# Create resource group\naz group create --name \"rg-human-oversight\" --location \"eastus\"\n\n# Deploy resources using Bicep\ncd deployment\naz deployment group create \\\n    --resource-group \"rg-human-oversight\" \\\n    --template-file main.bicep\n```\n\nThe deployment will output several values, including:\n- `logicAppUrl` - Required for the Python application\n- `storageAccountName` - For storing approval logs\n- `approvalsTableName` - Table name for approval logs\n\n### Step 4: Authorize Office 365 Connection\n\nThis critical step requires manual authorization in the Azure Portal:\n\n1. Go to the [Azure Portal](https://portal.azure.com)\n2. Navigate to your resource group (\"rg-human-oversight\")\n3. Find and click on the API Connection resource named \"office365\"\n4. In the left menu, click on \"Edit API connection\"\n5. Click the \"Authorize\" button\n6. Sign in with your Office 365 account when prompted\n7. After successful authorization, the connection status should show \"Connected\"\n8. Click \"Save\" to save the connection\n\n<div align=\"center\">\n  <img src=\"docs/images/office365-auth.png\" alt=\"Office 365 Authorization\" width=\"600\"/>\n</div>\n\n### Step 5: Configure and Run the Python Application\n\n1. Navigate to the application directory:\n   ```bash\n   cd ../app\n   ```\n\n2. Create a `.env` file with the required configuration:\n   ```\n   HO_LOGIC_APP_URL=<logicAppUrl-from-deployment-output>\n   APPROVER_EMAILS=approver1@example.com,approver2@example.com\n   \n   # Optional for OpenAI integration\n   AZURE_OPENAI_ENDPOINT=<your-endpoint>\n   AZURE_OPENAI_API_KEY=<your-api-key>\n   AZURE_OPENAI_DEPLOYMENT_NAME=<your-deployment>\n   ```\n\n3. Install dependencies:\n   ```bash\n   pip install -r requirements.txt\n   ```\n\n4. Run the sample application:\n   ```bash\n   python agent.py\n   ```\n\n## Approval Workflow Experience\n\nWhen a protected function is called:\n\n1. The approver(s) will receive an email with:\n   - Agent name and action description\n   - Parameters being passed to the function\n   - Approve and Reject buttons\n\n<div align=\"center\">\n  <img src=\"docs/images/approval-email-preview.png\" alt=\"Approval Email\" width=\"600\"/>\n</div>\n\n2. The Python application will wait for a response for up to 2 minutes (configurable)\n\n3. Based on the response:\n   - If approved: The function executes as normal\n   - If rejected: The function is not executed, and the configured refusal value is returned\n   - If timeout: The function is not executed, and the configured refusal value is returned\n\n## Customizing the Solution\n\n### Customizing the Email Template\n\nTo modify the email format, edit the Logic App definition in `deployment/logicapp.bicep`. Look for the `Send_approval_email` action and update the Subject and Body fields.\n\n### Adding Additional Approvers\n\nYou can specify multiple approvers as a comma-separated list in the APPROVERS_EMAILS environment variable, or directly in your code:\n\n```python\n@approval_gate(\n    agent_name=\"CriticalAgent\",\n    action_description=\"Dangerous Action\",\n    approver_emails=[\"primary@example.com\", \"backup@example.com\", \"security@example.com\"],\n    refusal_return_value={\"status\": \"denied\"}\n)\n```\n\n## Reporting\nA Power BI dashboard is included to visualize approval data and monitor agent activity.\nYou can open [`docs/approvaldashboard.pbix`](docs/approvaldashboard.pbix) in Power BI Desktop.  \n> **Note:** You must update the data source connection details using **Transform Data > Advanced Editor** to match your storage account and table configuration.\n\n<div align=\"center\">\n  <img src=\"docs/images/powerbi.png\" alt=\"Approval Email\" width=\"600\"/>\n</div>\n\n## Security Considerations\n\n- The Logic App URL should be treated as a secret\n- Use Key Vault in production to store sensitive configuration\n- Consider implementing IP restrictions on the Logic App trigger\n- Add Azure AD authentication for additional security\n- Monitor and audit approval logs regularly\n\n## Contributors\n\n<table>\n  <tr>\n    <td align=\"center\">\n      <a href=\"https://github.com/curia-damiano\">\n        <img src=\"https://github.com/curia-damiano.png?size=100\" alt=\"Damiano Curia\"/><br />\n        <sub><b>Damiano Curia</b></sub>\n      </a>\n    </td>\n    <td align=\"center\">\n      <a href=\"https://github.com/aymenfurter\">\n        <img src=\"https://github.com/aymenfurter.png?size=100\" alt=\"Aymen Furter\"/><br />\n        <sub><b>Aymen Furter</b></sub>\n      </a>\n    </td>\n    <td align=\"center\">\n      <a href=\"https://github.com/RichDCs\">\n        <img src=\"https://github.com/RichDCs.png?size=100\" alt=\"Richard Lagrange\"/><br />\n        <sub><b>Richard Lagrange</b></sub>\n      </a>\n    </td>\n    <td align=\"center\">\n      <a href=\"https://github.com/yimingwang123\">\n        <img src=\"https://github.com/yimingwang123.png?size=100\" alt=\"Yimi Wang\"/><br />\n        <sub><b>Yimi Wang</b></sub>\n      </a>\n    </td>\n  </tr>\n</table>\n\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n"
    },
    {
      "name": "THUDM/DataSciBench",
      "stars": 13,
      "img": "https://avatars.githubusercontent.com/u/48590610?s=40&v=4",
      "owner": "THUDM",
      "repo_name": "DataSciBench",
      "description": "DataSciBench: An LLM Agent Benchmark for Data Science",
      "homepage": null,
      "language": "Python",
      "created_at": "2025-02-19T16:11:14Z",
      "updated_at": "2025-04-12T15:22:21Z",
      "topics": [],
      "readme": "# DataSciBench: An LLM Agent Benchmark for Data Science\n\n## Introduction\n\n### Install MetaGPT\n\n```bash\ncd MetaGPT\npip install .\n```\n\n### Data\n\nAll collected prompts have been processed through `notebooks/preprocess_prompt.ipynb` and save into `data/{task_id}/`.\n   \n\n### Prepare Env\n\nRun\n```bash\npip install -r requirements.txt\n```\nTo run dl experiments, you may also need to log in to wandb.\n\n### Experimental Setting\n\nConfig model in `~/.metagpt/config2.yaml/`\n\n### Running\n\nRun all experiments as follows\n```bash\npython -m experiments.run_examples\n```\nRun a particular experiment, as follows\n```bash\npython -m experiments.run_examples --task_id dl_0\n```\nSpecifies to run a prompt of some kind, for example, to run a prompt with no external data dependencies\n```bash\npython -m experiments.run_examples --data_source_type 1\n```\n\n### Output Sample\n\n```json\n{\n    \"time_cost\": 146.17888736724854,\n    \"error_list\": [\n        1,\n        2,\n        2,\n        2,\n        2,\n        2,\n        0\n    ],\n    \"cost\": [\n        37966,\n        4465,\n        0.04689600000000001,\n        0\n    ],\n    \"plan\": [\n        {\n            \"task_id\": \"1\",\n            \"dependent_task_ids\": [],\n            \"instruction\": \"Fine-tune the sentiment classification model using the EleutherAI/twitter-sentiment dataset\",\n            \"task_type\": \"predictive modeling\",\n            \"code\": \"tokenizer = GPT2Tokenizer.from_pretrained('../gpt2-small/')\",\n            \"result\": \"\",\n            \"is_success\": true,\n            \"is_finished\": true\n        },\n        {\n            \"task_id\": \"2\",\n            \"dependent_task_ids\": [\n                \"1\"\n            ],\n            \"instruction\": \"Verify the existence of the 'evaluation_data.parquet' file at the specified location '../' and update the file path if necessary\",\n            \"task_type\": \"data exploration\",\n            \"code\": \"import pandas as pd\\n\\n# Verify the existence of the evaluation data file\\ntry:\\n    evaluation_data = pd.read_parquet('../evaluation_data.parquet')\\n\\n    # Display basic statistical indicators\\n    print(evaluation_data.describe())\\n\\n    # Visualize data distributions\\n    import matplotlib.pyplot as plt\\n    import seaborn as sns\\n\\n    # Histogram\\n    plt.figure(figsize=(10, 6))\\n    sns.histplot(evaluation_data['sentiment'], kde=False)\\n    plt.title('Sentiment Distribution')\\n    plt.xlabel('Sentiment')\\n    plt.ylabel('Count')\\n    plt.show()\\n\\n    # Box plot\\n    plt.figure(figsize=(10, 6))\\n    sns.boxplot(x=evaluation_data['sentiment'])\\n    plt.title('Sentiment Distribution')\\n    plt.xlabel('Sentiment')\\n    plt.show()\\n\\n    # Correlation matrix\\n    correlation_matrix = evaluation_data.corr()\\n    plt.figure(figsize=(10, 6))\\n    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f')\\n    plt.title('Correlation Matrix')\\n    plt.show()\\nexcept FileNotFoundError as e:\\n    print('File not found:', e)\",\n            \"result\": \"File not found: [Errno 2] No such file or directory: '../evaluation_data.parquet'\\n\",\n            \"is_success\": true,\n            \"is_finished\": true\n        },\n        {\n            \"task_id\": \"3\",\n            \"dependent_task_ids\": [\n                \"2\"\n            ],\n            \"instruction\": \"Generate a comprehensive report on the model's performance in PDF format\",\n            \"task_type\": \"other\",\n            \"code\": \"from reportlab.lib import colors\\nfrom reportlab.lib.pagesizes import letter\\nfrom reportlab.platypus import SimpleDocTemplate, Table, TableStyle\\n\\n# Sample data for demonstration\\ndata = [['Metric', 'Value'],\\n        ['Accuracy', '0.85'],\\n        ['Precision', '0.78'],\\n        ['Recall', '0.92'],\\n        ['F1 Score', '0.84']]\\n\\n# Create PDF\\npdf_filename = './performance_report.pdf'\\npdf = SimpleDocTemplate(pdf_filename, pagesize=letter)\\ntable = Table(data)\\n\\n# Add style to the table\\nstyle = TableStyle([('BACKGROUND', (0, 0), (-1, 0), colors.grey),\\n                    ('TEXTCOLOR', (0, 0), (-1, 0), colors.whitesmoke),\\n                    ('ALIGN', (0, 0), (-1, -1), 'CENTER'),\\n                    ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),\\n                    ('BOTTOMPADDING', (0, 0), (-1, 0), 12),\\n                    ('BACKGROUND', (0, 1), (-1, -1), colors.beige),\\n                    ('GRID', (0, 0), (-1, -1), 1, colors.black)])\\n\\ntable.setStyle(style)\\n\\n# Build PDF\\npdf.build([table])\\n\\nprint(f\\\"Performance report generated at: {pdf_filename}\\\")\\n\",\n            \"result\": \"Performance report generated at: ./performance_report.pdf\\n\",\n            \"is_success\": true,\n            \"is_finished\": true\n        }\n    ]\n}\n```\n\n### Evaluation\n\n#### Evaluate functions of BigCodeBench \n```bash\npython -m experiments.evaluate_tmc\n```\n\n#### Evaluate other tasks\n```bash\npython -m experiments.evaluate\n```\n\n\n### Others\n\n#### Modification of Data Interpreter (deprecated)\n\nSee `SciDataInterpreter.update_results_for_eval` at `role/sci_data_interpreter`. We can get the plans with codes and results from each step; the costs for each step per plan; and the number of errors. \n\nSee `SciDataInterpreter.get_CR` at `role/sci_data_interpreter`. We can get the Completion Rate for this question that just ran. (Ground Truth not incorporated, so max at 0.5)\n"
    },
    {
      "name": "Azure-Samples/sk-advanced-orchestration",
      "stars": 13,
      "img": "https://avatars.githubusercontent.com/u/1844662?s=40&v=4",
      "owner": "Azure-Samples",
      "repo_name": "sk-advanced-orchestration",
      "description": "SK Multi agentic advanced orchestration example",
      "homepage": "",
      "language": "Python",
      "created_at": "2025-02-15T10:46:10Z",
      "updated_at": "2025-04-09T18:18:21Z",
      "topics": [],
      "readme": "> [!IMPORTANT]\n> We are consolidating all Semantic Kernel advanced usage samples into a [single repository](https://github.com/Azure-Samples/semantic-kernel-advanced-usage/). This repository will be archived in the future.\n\n# Semantic Kernel Advanced Customer Support\n\nThis repo showcases a sample AI-enabled customer support application that leverages [Semantic Kernel](https://github.com/microsoft/semantic-kernel) Agents boosted with:\n\n- an improved [`SelectionStrategy`](src/agents/sk_ext/speaker_election_strategy.py) that accounts for agents descriptions and available tools to provide a more accurate selection (including the reason for it for traceability).\n- _nested orchestration_ via [`Teams`](src/agents/sk_ext/team.py) and `Agents` for more complex, hierarchical routing scenarios.\n- a special type of `Agent` named [`PlannedTeam`](src/agents/sk_ext/planned_team.py), which can handle more complex, cross-agent asks turning them into a multi-step process automatically.\n- improved telemetry and explainability via [Application Insights](https://learn.microsoft.com/en-us/azure/azure-monitor/app/app-insights-overview) to track agentic team steps and results, as well as the reasoning behind agent selection.\n\nAdditionally, the application leverages [Azure Container Apps](https://learn.microsoft.com/en-us/azure/container-apps/) and [Dapr](https://dapr.io) to enable the [_Virtual Actor pattern_](https://docs.dapr.io/developing-applications/building-blocks/actors/actors-overview/) for agentic teams and natively handle `ChatHistory` persistence via Dapr's [state store](https://docs.dapr.io/developing-applications/building-blocks/state-management/), ensuring that the application can scale seamlessly.\n\n## Example\n\nIn this case, the user asks a question that requires the involvement of multiple agents, which are able to collaborate and produce a composite answer.\n\n![Example chat demonstrating agents producing a composite answer](image.png)\n\n## Telemetry and explanability example\n\n### Waterfall chart of agentic team steps (Application Insights)\n\n![Exaple waterfall chart of agentic team steps and results](telemetry.png)\n\n### Additional debug information provided by the improved `SelectionStrategy`:\n\n![alt text](telemetry_details.png)\n\n### Azure AI Foundry tracing\n\nSee [official documentation](https://learn.microsoft.com/en-us/azure/ai-foundry/tracing) for more details.\n\n![alt text](ai-foundry-tracing.png)\n\n## Architecture\n\nThe overall architecture involves [Dapr](https://dapr.io) to enable the [_Virtual Actor pattern_](https://docs.dapr.io/developing-applications/building-blocks/actors/actors-overview/), in order to host the agentic team and natively handle `ChatHistory` persistence via Dapr's [state store](https://docs.dapr.io/developing-applications/building-blocks/state-management/).\n\n```mermaid\nflowchart TD\n  %% User and client\n  U[User]\n  CB[Browser / Client]\n  U --> CB\n\n  %% Container Apps (Dapr enabled)\n  subgraph \"Azure Container Apps\"\n    Chat[\"`Chat App<br/>(Dapr enabled)<br/>[src/chat]`\"]\n    Agents[\"`Agents App<br/>(Dapr enabled)<br/>[src/agents]`\"]\n  end\n\n  CB --> Chat\n\n  %% Dapr sidecars within container apps\n  Chat -- \"Dapr Sidecar\" --> DS1[\"`Dapr: state store<br/>(in-memory / Cosmos DB)`\"]\n  Agents -- \"Dapr Sidecar\" --> DS2[Dapr Components]\n\n  %% Communication Flow\n  Chat --- Agents\n  Agents --- Chat\n\n  %% Azure Services and Infrastructure\n  subgraph \"Azure Resources\"\n    ACR[\"`Azure Container Registry<br/>[infra/acr.bicep]`\"]\n    AI[\"`Application Insights<br/>[infra/appin.bicep]`\"]\n    LA[\"`Log Analytics<br/>[infra/appin.bicep]`\"]\n    OAI[\"`Azure OpenAI<br/>[infra/openAI.bicep]`\"]\n    Cosmos[\"`Cosmos DB<br/>[infra/cosmos.bicep]`\"]\n    UAMI[\"`User Assigned Identity<br/>[infra/uami.bicep]`\"]\n  end\n\n  %% Deployment orchestration via Azure Developer CLI\n  ADCLI[\"`Azure Developer CLI<br/>(azd)`\"]\n  ADCLI --> ACR\n  ADCLI --> AI\n  ADCLI --> LA\n  ADCLI --> OAI\n  ADCLI --> Cosmos\n  ADCLI --> UAMI\n\n  %% Container App image source\n  ACR --> Chat\n  ACR --> Agents\n\n  %% Monitoring and Telemetry\n  Chat --> AI\n  Agents --> AI\n  AI --> LA\n\n  %% Data persistence via Dapr state store\n  DS1 --> Cosmos\n\n  %% Semantic Kernel\n  SK[\"`Semantic Kernel<br/>(LLM Processing)`\"]\n  Agents --> SK\n  SK --- Agents\n```\n\n## Getting Started\n\n### Prerequisites\n\n- **Python 3.12+**\n- [**Azure Developer CLI**](https://learn.microsoft.com/en-us/azure/developer/azure-developer-cli/install-azd?tabs=winget-windows%2Cbrew-mac%2Cscript-linux&pivots=os-windows) – To deploy and manage Azure resources.\n- **Docker** and [**Dapr CLI**](https://docs.dapr.io/getting-started/install-dapr-cli/) – To run the application locally.\n\n### Clone the repo\n\n```bash\ngit clone https://github.com/Azure-Samples/mas-sk-quickstart\ncd mas-sk-quickstart\n```\n\n### Azure Deployment\n\n> [!NOTE]\n> Script will **NOT** create a new Azure OpenAI resource. You will need to have an existing one with a deployed model.\n\n```bash\n# Login to Azure if required\nazd auth login --tenant-id <TENANT>.onmicrosoft.com\n\nazd up\n\n# When prompted, select\n# - Azure subscription to deploy to\n# - Azure region to deploy to\n# - EXISTING Azure OpenAI resource and group to use (azd will NOT create a new one)\n```\n\n### Running Locally\n\n> [!TIP]\n> If you deployed the application to Azure, you can run it locally using the same Azure resources.\n> Simply copy the `.env` file from the `.azure/<env name>` folder to the root of the repo.\n\n1. `cp .env.example .env`\n2. Update `.env` with your Azure OpenAI resource endpoint.\n3. Ensure Docker is running.\n4. Init Dapr (once only): `dapr init`.\n5. Setup Python environment and install dependencies:\n\n```bash\npython -m venv .venv\n\n# On macOS/Linux:\nsource .venv/bin/activate\n\n# On Windows:\n.venv\\Scripts\\Activate.ps1\n\npip install -r src/chat/requirements.txt\npip install -r src/agents/requirements.txt\n```\n\nTo run:\n\n`dapr run -f dapr.yaml`\n\n## Contributing\n\nThis project welcomes contributions and suggestions. Please see [CONTRIBUTING.md](CONTRIBUTING.md) for details.\n\n## License\n\nThis project is licensed under the MIT License. See [LICENSE.md](LICENSE.md) for details.\n"
    },
    {
      "name": "wmeints/effective-llm-applications",
      "stars": 13,
      "img": "https://avatars.githubusercontent.com/u/1550763?s=40&v=4",
      "owner": "wmeints",
      "repo_name": "effective-llm-applications",
      "description": "Learn how to build effective LLM-based applications with Semantic Kernel in C#",
      "homepage": "https://leanpub.com/effective-llm-applications-with-semantic-kernel/",
      "language": "Jupyter Notebook",
      "created_at": "2024-12-30T06:17:03Z",
      "updated_at": "2025-04-08T05:19:02Z",
      "topics": [
        "ai",
        "book",
        "csharp",
        "llm",
        "rag",
        "semantic-kernel",
        "structured-output",
        "testing",
        "tool-calling"
      ],
      "readme": "# Building effective LLM-based applications with Semantic Kernel\n\nWelcome to my new book about building effective LLM-based applications with Semantic\nKernel. This book is available on Leanpub! This repository is where I build the book.\n\n## Reading the book\n\nGet a copy now and start reading! Read it on\nLeanpub: [Building effective LLM-based applications with Semantic\nKernel](https://leanpub.com/effective-llm-applications-with-semantic-kernel)\n\n## Progress\n\nI'm writing this book as you read this, and I'm at **64% completion**.\n\n| Chapter                                       | Status      | Samples                                                                    |\n| --------------------------------------------- | ----------- | -------------------------------------------------------------------------- |\n| 1. Understanding Large Language Models        | Complete    | N.A.                                                                       |\n| 2. Essential LLMOps knowledge                 | Complete    | N.A.                                                                       |\n| 3. Getting started with Semantic Kernel       | Complete    | [C#](./samples/chapter-03/csharp/), [Python](./samples/chapter-03/python/) |\n| 4. The art and nonsense of prompt engineering | Complete    | [C#](./samples/chapter-04/csharp/), [Python](./samples/chapter-04/python/) |\n| 5. Testing and monitoring prompts             | Complete    | [C#](./samples/chapter-05/csharp/)                                         |\n| 6. Enhancing LLMs with functions              | Complete    | [C#](./samples/chapter-06/csharp/)                                         |\n| 7. Retrieval Augmented Generation (RAG)       | Complete    | [C#](./samples/chapter-07/csharp/)                                         |\n| 8. Working with structured output             | Complete    | [C#](./samples/chapter-08/csharp/)                                         |\n| 9. Prompt chaining workflows                  | Complete    | [C#](./samples/chapter-09/csharp/)                                         |\n| 10. Intelligent request routing workflows     | In progress |                                                                            |\n| 11. LLM orchestration workflows               | Not started |                                                                            |\n| 12. Artist and critic workflows               | Not started |                                                                            |\n| 13. Building basic agents                     | Not started |                                                                            |\n| 14. Multi-agent teams                         | Not started |                                                                            |\n\n## Working with the examples\n\nThis book features examples in multiple languages to demonstrate how to implement various patterns\nwith Semantic Kernel.\n\nYou can open the solutions in the `samples` directory to browse through them.\nFollow the instructions provided with each example to run the example.\n\n### Requirements for the C# samples\n\n- [.NET 9](https://dot.net/)\n- [Visual Studio Code](https://code.visualstudio.com)\n- Access to the OpenAI API or Azure OpenAI\n- [Docker Desktop](https://www.docker.com/products/docker-desktop/)\n- [Azure CLI](https://docs.microsoft.com/en-us/cli/azure/install-azure-cli)\n\n### Requirements for the Python samples\n\n- [Python 3.12](https://www.python.org/downloads/)\n- [Visual Studio Code](https://code.visualstudio.com)\n- Access to the OpenAI API or Azure OpenAI\n- [Docker Desktop](https://www.docker.com/products/docker-desktop/)\n- [Azure CLI](https://docs.microsoft.com/en-us/cli/azure/install-azure-cli)\n\n## Sponsor me\n\nI am writing this in my spare time to help people gain practical knowledge about\nworking with large language models. The book is free, but I take donations as a token\nof gratitude.\n\nYou can donate by purchasing the book online through Leanpub. You can also sponsor me\nthrough the sponsorship button on this repository.\n\n## My workflow\n\nSince you're here anyway, let me share my workflow for this book so you'll get to know\na little more about how I work and what you can expect from me.\n\nMy workflow has the following phases:\n\n- Research\n- Writing\n- Review and initial editing\n- Formatting and layout\n- Publishing\n\nI publish after each chapter is complete and undergone initial editing. This way, you\nget the content as fast as possible. After I've completed the full book, I'll go through\nthe entire book again to ensure everything is consistent and correct.\n\n> **Hey, didn't you use AI to write your book?**  \n> Yes, I did! I used Claude.ai to help me generate text for the book. But I stopped. I\n> found that it gets in the way of critical thinking and creativity. It also throws so\n> much mediocre content at me it's hard to manage it. I prefer to write the content\n> myself now that I have the time for it. I love writing, and AI wasn't doing me a favor\n> in that regard.\n\n### Research\n\nThe book contains my experience with building LLM-based applications using Semantic Kernel.\nI try to pull from experience as much as possible because it gives the best results for me.\n\nHowever, before I write, I do my research to verify that my ideas are going to work and\nbuild a sample to demonstrate the principles. Working code is still the best way to\ndemonstrate things. The book is the narrative to the example.\n\n### Writing\n\nI write an initial set of notes in Markdown and a rough outline that I refine as I go.\n\nAfter the initial outline, I'll start by adding detail to each of the sections in the\noutline. I still use a note writing style as I want to make sure that each section makes\nsense to you as the reader of my work. Often, I need to rework the outline as I write\nthe notes because I find that the initial outline doesn't work out as I thought.\n\nOnce I have the notes for the full chapter, I'll go back and expand the notes into the\nfull text. It's at this time I want to make sure that there are pointers and bridges\nto help you navigate the text. I also want to make sure that the text is engaging and\nthat it's easy to read.\n\n### Review and initial editing\n\nOnce everything is in the chapter, I'll put aside the text for a day and let it rot,\nripen, or marinate if you will. This helps me to get a fresh perspective on the text.\nI'll then go through the text and make sure that everything makes sense and that the\ntext still conveys what I wanted to tell you.\n\nI perform a final spell check using Grammarly, an online tool that helps me catch mistakes\nin grammar, spelling, and style.\n\n### Formatting and layout\n\nAt this point I use the Leanpub preview generator to preview the final look of my book.\nI find that I often need to insert additional line breaks and correct figures to make\nsure that everything looks good. I also need to make sure that the layout of the code\nsamples isn't broken.\n\n### Publishing\n\nWhen I think the chapter is ready, I'll publish a new version in Leanpub.\nI'll also update the README.md file to reflect the current state of the book.\n"
    },
    {
      "name": "microsoft/sk-workshop",
      "stars": 13,
      "img": "https://avatars.githubusercontent.com/u/6154722?s=40&v=4",
      "owner": "microsoft",
      "repo_name": "sk-workshop",
      "description": "Semantic Kernel Workshop",
      "homepage": "",
      "language": "Jupyter Notebook",
      "created_at": "2023-12-14T19:09:02Z",
      "updated_at": "2025-04-18T10:50:12Z",
      "topics": [],
      "readme": "# Sematic Kernel Workshop\n\nLearn how to build LLM Apps with Semantic Kernel through self-contained and independent lessons, in a flexible way so that you can choose the sequence that is most convenient for you.\n\nTypically, each lesson is structured to last approximately one hour, and it is assumed that you already have some Python or coding skills for the hands-on labs.\n\n## Objectives\n\nAfter completing this workshop, you will be able to:\n\n* Author your own copilot using Semantic Kernel.\n\n* Create AI plugins using semantic and native functions.\n\n* Automate complex tasks execution with planners.\n\n* Create prompt templates to define AI functions.\n\n* Embed Generative AI in your applications.\n\n## Contents\n\n### 1 Background - Introduction to LLMs  \n*no-code. 1.5h duration*\n - Introduction to LLMs GPTs and other models.\n - Azure AI Services Overview.\n - Azure OpenAI Service Overview.\n\n### 2 CoPilot Stack and Semantic Kernel\n*full-code. 1.5h duration*\n - Introduction to Copilots.\n - Microsoft Copilots.\n - Build your own Copilots.  \n - Copilot ecosystem (Copilots + Plugins).\n - What is Semantic Kernel?\n - Why Semantic Kernel?\n - The role of the kernel.\n\n### 3 Semantic Kernel Basic Concepts  \n *full-code. 1.5h duration*\n- Native and Semantic Functions.\n - Create AI Plugins from functions.\n - Chain plugins together.\n\n### 4 Semantic Kernel Advanced Topics\n*full-code. 3h duration*\n - Add Memory to your AI Apps.\n - Use Planners to automate Plugins orchestration.\n - Calling external connectors.\n\n### 5 Best Practices and Lessons Learned\n*no-code. 0.5h duration*\n - Learn some best practices on service limits.\n - Final discussions and wrap-up.\n\n## Requirements\n\n- Workstation\n\n    - [Azure CLI](https://learn.microsoft.com/en-us/cli/azure/install-azure-cli)\n    - [Anaconda](https://docs.conda.io/projects/conda/en/latest/user-guide/install/index.html)\n    - [VS Code](https://code.visualstudio.com/)\n\n- Cloud\n\n    - [Azure Subscription](https://azure.com)\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n\n## Code of Conduct\n\nThis project has adopted the\n[Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the\n[Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/)\nor contact [opencode@microsoft.com](mailto:opencode@microsoft.com)\nwith any additional questions or comments.\n\n## License\n\nCopyright (c) Microsoft Corporation. All rights reserved.\n\nLicensed under the [MIT](LICENSE) license.\n\n### Reporting Security Issues\n\n[Reporting Security Issues](https://github.com/microsoft/repo-templates/blob/main/shared/SECURITY.md)"
    },
    {
      "name": "kmavrodis/noRAG-multiagent-doc-qna",
      "stars": 12,
      "img": "https://avatars.githubusercontent.com/u/156899979?s=40&v=4",
      "owner": "kmavrodis",
      "repo_name": "noRAG-multiagent-doc-qna",
      "description": "A team of AI agents that answer document related questions (RAG alternative)",
      "homepage": "",
      "language": "Python",
      "created_at": "2024-10-30T15:25:59Z",
      "updated_at": "2025-04-16T13:54:59Z",
      "topics": [],
      "readme": "# Multi-agent Document QnA (The NoRAG Alternative) 🤖📚\n\n### Approach\nNoRAG MultiAgent Documnet QnA uses a  team of AI agents that work together to understand documents and answer questions based on them. Rather than using traditional vector embeddings like RAG systems, it relies on intelligent document summarization and contextual relevance scoring. \n\nThe `Document Analysis Agent` first creates comprehensive appendices of documents, the `Research Agent` evaluates question relevance against these appendices, and the `Reply Agent` generates answers using the most relevant document, by loading it in the system message as a whole.\n\n![Alt Text](assets/recording.gif)\n\n\n### When to Use Instead of RAG\nChoose this solution when:\n- You are handling highly complicated documents that RAG struggles with\n- The answers are based on multiple parts of the same document\n- Context from the whole document might be needed to answer a question\n- Looking for a simpler deployment without vector databases\n- Need transparent agent-based processing pipeline\n\n### Limitations\n- Not designed to handle a large amount of documents (30 documents and over will be a stretch)\n- Doesn't perform cross-document answer generation (each answer comes from a single document)\n- Higher token usage compared to RAG systems\n- May be slower for very large document collections\n- Maximum token limit per document chunk (120k tokens)\n- Requires Azure OpenAI access \n- No persistent document storage (session-based only)\n\n\n## 🌟 Key Features\n\n### Multi-Agent Architecture\n- **Document Analysis Agent** 📄\n  - Processes PDF documents\n  - Extracts and chunks text content\n  - Generates document summaries\n  - Manages token limitations\n  \n- **Research Agent** 🔍\n  - Analyzes question relevance\n  - Scores document sections\n  - Identifies most pertinent content\n  - Provides relevance metrics\n\n- **Reply Agent** 💡\n  - Generates accurate answers\n  - Uses context-aware processing\n  - Maintains source truthfulness\n  - Provides clear explanations\n\n\n## 🚀 Getting Started\n\n### Prerequisites\n- Python 3.8+\n- Azure OpenAI API access\n\n### Quick Start\n```bash\n# Clone repository\ngit clone https://github.com/yourusername/multiagent-doc-qna.git\ncd multiagent-doc-qna\n\n# Set up virtual environment\npython -m venv venv\nsource venv/bin/activate  # Unix\n.\\venv\\Scripts\\activate   # Windows\n\n# Install dependencies\npip install -r requirements.txt\n\n# Configure environment\ncp .env.example .env\n# Edit .env with your credentials\n\n# Launch application\nstreamlit run app.py\n```\n\n### Azure OpenAI Setup\n1. Create an Azure OpenAI resource\n2. Deploy a model\n3. Get API credentials\n4. Configure environment variables\n\n## 💻 Usage\n\n### Document Upload\n1. Launch the application\n2. Click \"Upload Documents\"\n3. Select one or more PDF files\n4. Wait for processing completion\n\n### Asking Questions\n1. Type your question in the input field\n2. Click \"Submit Question\"\n3. View document relevance scores\n4. Read the generated answer\n\n### Understanding Results\n- **Relevance Scores**: Shows how relevant each document is to your question\n- **Token Counts**: Displays processing efficiency metrics\n- **Document Summaries**: Provides quick content overview\n- **System Status**: Shows real-time processing information\n\n## 🛠 Technical Details\n\n### Token Management\n- Maximum tokens per chunk: 120,000\n- Automatic chunking for large documents\n- Token count monitoring\n- Optimization for Azure OpenAI context limits\n\n\n## 🔒 Security\n\n### Data Protection\n- No document storage\n- Session-only processing\n\n\n### Running FastAPI Server\nTo run the FastAPI server, use the following command:\n```bash\nuvicorn api:app --reload\n```\n\n### API Endpoints\nThe following endpoints are available in the FastAPI server:\n\n- **Token Counting**\n  - **Endpoint**: `/count_tokens/`\n  - **Method**: `POST`\n  - **Request Body**: `{\"text\": \"Your text here\"}`\n  - **Response**: `{\"token_count\": 123}`\n\n- **Text Chunking**\n  - **Endpoint**: `/split_text/`\n  - **Method**: `POST`\n  - **Request Body**: `{\"text\": \"Your text here\"}`\n  - **Response**: `{\"chunks\": [\"chunk1\", \"chunk2\"]}`\n\n- **PDF Text Extraction**\n  - **Endpoint**: `/extract_text/`\n  - **Method**: `POST`\n  - **Request Body**: PDF file upload\n  - **Response**: `{\"chunks\": [\"chunk1\", \"chunk2\"], \"chunk_tokens\": [100, 200]}`\n\n- **Document Summarization**\n  - **Endpoint**: `/summarize/`\n  - **Method**: `POST`\n  - **Request Body**: `{\"text\": \"Your text here\"}`\n  - **Response**: `{\"summary\": \"Summary of the text\"}`\n\n- **Document Chunk Processing**\n  - **Endpoint**: `/process_chunks/`\n  - **Method**: `POST`\n  - **Request Body**: `{\"file_name\": \"document.pdf\", \"chunks\": [\"chunk1\", \"chunk2\"], \"chunk_tokens\": [100, 200]}`\n  - **Response**: `{\"documents\": {\"doc1\": \"text1\"}, \"summaries\": {\"doc1\": \"summary1\"}, \"token_counts\": {\"doc1\": 100}}`\n\n- **Document Relevance Selection**\n  - **Endpoint**: `/select_relevant/`\n  - **Method**: `POST`\n  - **Request Body**: `{\"question\": \"Your question here\", \"summaries\": {\"doc1\": \"summary1\"}}`\n  - **Response**: `{\"most_relevant\": \"doc1\", \"relevance_scores\": {\"doc1\": 90}}`\n\n- **Question Answering**\n  - **Endpoint**: `/get_answer/`\n  - **Method**: `POST`\n  - **Request Body**: `{\"question\": \"Your question here\", \"document_text\": \"Relevant document text\"}`\n  - **Response**: `{\"answer\": \"Answer to your question\"}`"
    },
    {
      "name": "smile-wingbow/MihaGPT",
      "stars": 12,
      "img": "https://avatars.githubusercontent.com/u/174078102?s=40&v=4",
      "owner": "smile-wingbow",
      "repo_name": "MihaGPT",
      "description": "把小爱音箱接入大模型，并用来控制HA中的智能电器",
      "homepage": null,
      "language": "HTML",
      "created_at": "2024-12-29T08:48:14Z",
      "updated_at": "2025-03-10T03:32:14Z",
      "topics": [],
      "readme": "# 把小爱音箱接入大模型，并用来控制HA中的智能电器\n\n# 👉 先看接入后的效果：【[把小爱音箱接入大模型，并用来控制HA中的智能电器～](https://www.bilibili.com/video/BV1NG6hYqEzP/)】\n## ✨ 软件\n\n- Homeassistan，需要安装Xiaomi Miot Auto插件。如果要使用本项目生成HA的自动化，则需要把HA安装到本项目相同的主机上，并使用以下参数启动HA：docker run -d  --name ha  -p 8123:8123  --privileged   --restart=unless-stopped   -e TZ=Asia/Shanghai   -v /data/homeassistant:/config   ghcr.io/home-assistant/home-assistant:stable\n- [xiaogpt](https://github.com/yihong0618/xiaogpt)，增加了同时唤醒多个音箱的支持，并支持更多的音箱型号。\n- [MetaGPT](https://github.com/geekan/MetaGPT)，主要是智能体流程。\n## 获取小米音响DID\n\n| 系统和Shell   | Linux *sh                                      | Windows CMD用户                          | Windows PowerShell用户                           |\n| ---------- | ---------------------------------------------- | -------------------------------------- | ---------------------------------------------- |\n| 1、安装包      | `pip install miservice_fork`                   | `pip install miservice_fork`           | `pip install miservice_fork`                   |\n| 2、设置变量     | `export MI_USER=xxx` <br> `export MI_PASS=xxx` | `set MI_USER=xxx`<br>`set MI_PASS=xxx` | `$env:MI_USER=\"xxx\"` <br> `$env:MI_PASS=\"xxx\"` |\n| 3、取得MI_DID | `micli list`                                   | `micli list`                           | `micli list`                                   |\n- 注意不同shell 对环境变量的处理是不同的，尤其是powershell赋值时，可能需要双引号来包括值。\n- 如果获取did报错时，请更换一下无线网络，有很大概率解决问题。\n\n## 获取homeassistant的token\n- 登录homeassistant网页版\n- 进入admin-安全菜单（不同版本的ha具体路径有所区别）\n- 长期访问令牌页面，创建令牌并记录\n\n## ⚡️ 快速开始\n#### Python 3.9\n\n先git clone https://github.com/smile-wingbow/MihaGPT\n以下命令都在MihaGPT路径下执行\n#### 一.创建虚拟环境并激活：\n```shell\npython3.9 -m venv mihagpt-venv  \nsource mihagpt-venv/bin/activate\n```\n#### 二.pip安装python相关库：\n```shell\npip install -r requirements.txt\n```\n#### 三.安装浏览器（以armbian为例）\nsudo apt-get update  \nsudo apt-get install firefox-esr  \nwget https://github.com/mozilla/geckodriver/releases/download/v0.35.0/geckodriver-v0.35.0-linux-aarch64.tar.gz  \ntar -xvzf geckodriver-v0.35.0-linux-aarch64.tar.gz  \nsudo mv geckodriver /usr/local/bin/\n#### 四.配置参数：\n##### 1.修改metaGPT的LLM配置，配置config目录的config2.yaml、gpt4o.yaml、gpt4omini.yaml配置，代码中主要用到了gpt4o和gpt4omini两种模型，分别用于不同的智能体。\n##### 2.修改miha_config.yaml，说明如下：\n##### 配置项说明\n| 参数                  | 说明                                                                                                       | 默认值                                                                                                    | 可选值                                                           |\n| --------------------- | ---------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------- |\n| hardware              | 设备型号                                                                                                   |                                                                                                           |                                                                  |\n| account               | 小爱账户                                                                                                   |                                                                                                           |                                                                  |\n| password              | 小爱账户密码                                                                                               |                                                                                                           |         \n| cookie              | 小爱账户cookie（如果用上面密码登录可以不填）                                                                                               |                                                                                                           |        \n| mi_did              | 音箱设备id，参见上述获得DID的方法                                                                                               |                                                                                                           |          \n| use_command              | 使用 MI command 与小爱交互                                                                                               |                                                                                                           |           \n| mute_xiaoai              | 快速停掉小爱自己的回答                                                                                               |                                                                                                           |                                            |\n| openai_key            | openai的apikey                                                                                             |                                                                                                           |                                                                  |\n| moonshot_api_key      | moonshot kimi 的 [apikey](https://platform.moonshot.cn/docs/api/chat#%E5%BF%AB%E9%80%9F%E5%BC%80%E5%A7%8B) |                                                                                                           |                                                                  |\n| yi_api_key            | 01 wanwu 的 [apikey](https://platform.lingyiwanwu.com/apikeys)                                             |                                                                                                           |                                                                  |\n| llama_api_key         | groq 的 llama3 [apikey](https://console.groq.com/docs/quickstart)                                          |                                                                                                           |                                                                  |\n| serpapi_api_key       | serpapi的key 参考 [SerpAPI](https://serpapi.com/)                                                          |                                                                                                           |                                                                  |\n| glm_key               | chatglm 的 apikey                                                                                          |                                                                                                           |                                                                  |\n| gemini_key            | gemini 的 apikey [参考](https://makersuite.google.com/app/apikey)                                          |                                                                                                           |                                                                  |\n| gemini_api_domain     | gemini 的自定义域名 [参考](https://github.com/antergone/palm-netlify-proxy)                                |                                                                                                           |\n| qwen_key              | qwen 的 apikey [参考](https://help.aliyun.com/zh/dashscope/developer-reference/api-details)                |                                                                                                           |                                                                  |\n| cookie                | 小爱账户cookie （如果用上面密码登录可以不填）                                                              |                                                                                                           |                                                                  |\n| mi_did                | 设备did                                                                                                    |                                                                                                           |                                                                  |\n| use_command           | 使用 MI command 与小爱交互                                                                                 | `false`                                                                                                   |                                                                  |\n| mute_xiaoai           | 快速停掉小爱自己的回答                                                                                     | `true`                                                                                                    |                                                                  |\n| verbose               | 是否打印详细日志                                                                                           | `false`                                                                                                   |                                                                  |\n| bot                   | 使用的 bot 类型，目前支持 chatgptapi,newbing, qwen, gemini                                                 | `chatgptapi`                                                                                              |                                                                  |\n| tts                   | 使用的 TTS 类型                                                                                            | `mi`                                                                                                      | `edge`、 `openai`、`azure`、`volc`、`baidu`、`google`、`minimax` |\n| tts_options           | TTS 参数字典，参考 [tetos](https://github.com/frostming/tetos) 获取可用参数                                |                                                                                                           |                                                                  |\n| prompt                | 自定义prompt                                                                                               | `请用100字以内回答`                                                                                       |                                                                  |\n| keyword               | 自定义请求词列表                                                                                           | `[\"请\"]`                                                                                                  |                                                                  |\n| change_prompt_keyword | 更改提示词触发列表                                                                                         | `[\"更改提示词\"]`                                                                                          |                                                                  |\n| start_conversation    | 开始持续对话关键词                                                                                         | `开始持续对话`                                                                                            |                                                                  |\n| end_conversation      | 结束持续对话关键词                                                                                         | `结束持续对话`                                                                                            |                                                                  |\n| stream                | 使用流式响应，获得更快的响应                                                                               | `true`                                                                                                    |                                                                  |\n| proxy                 | 支持 HTTP 代理，传入 http proxy URL                                                                        | \"\"                                                                                                        |                                                                  |\n| gpt_options           | OpenAI API 的参数字典                                                                                      | `{}`                                                                                                      |                                                                  |\n| deployment_id         | Azure OpenAI 服务的 deployment ID                                                                          | 参考这个[如何找到deployment_id](https://github.com/yihong0618/xiaogpt/issues/347#issuecomment-1784410784) |                                                                  |\n| api_base              | 如果需要替换默认的api,或者使用Azure OpenAI 服务                                                            | 例如：`https://abc-def.openai.azure.com/`                                                                 |\n| volc_access_key       | 火山引擎的 access key 请在[这里](https://console.volcengine.com/iam/keymanage/)获取                        |                                                                                                           |                                                                  |\n| volc_secret_key       | 火山引擎的 secret key 请在[这里](https://console.volcengine.com/iam/keymanage/)获取                        |                                                                                                           |\n| debug_mode       | 在本机上调试模式                        |                                                                                                           |\n| ha_address       | homeassistant地址                        |                                                                                                           |\n| ha_token       | homeassistant api的token                        |                                                                                                           |\n#### 五.启动服务：\n使用以下命令启动\n```shell\npython3.9 mihagpt.py --config miha_config.yaml\n```\n## 联系\n加群一起讨论\n\n![](https://github.com/smile-wingbow/MihaGPT/blob/main/assets/weichat.jpg)\n\n## ❤️ 鸣谢\n\n感谢以下项目提供的贡献：\n\n- https://github.com/yihong0618/xiaogpt\n- https://github.com/geekan/MetaGPT\n- https://github.com/Yonsm/MiService\n\n## 免责声明\n\n本项目仅供学习和研究目的，不得用于任何商业活动。用户在使用本项目时应遵守所在地区的法律法规，对于违法使用所导致的后果，本项目及作者不承担任何责任。 本项目可能存在未知的缺陷和风险（包括但不限于设备损坏和账号封禁等），使用者应自行承担使用本项目所产生的所有风险及责任。 作者不保证本项目的准确性、完整性、及时性、可靠性，也不承担任何因使用本项目而产生的任何损失或损害责任。 使用本项目即表示您已阅读并同意本免责声明的全部内容\n\n## License\n\n[MIT](https://github.com/idootop/mi-gpt/blob/main/LICENSE) License © 2024-PRESENT smilewingbow\n\n"
    },
    {
      "name": "evmin/az-ai-kickstarter",
      "stars": 11,
      "img": "https://avatars.githubusercontent.com/u/5507517?s=40&v=4",
      "owner": "evmin",
      "repo_name": "az-ai-kickstarter",
      "description": "AI Application chassis - best AI app practices optimized for Azure.",
      "homepage": null,
      "language": "Bicep",
      "created_at": "2024-12-03T17:14:17Z",
      "updated_at": "2025-04-11T18:23:52Z",
      "topics": [],
      "readme": "# AI Application chassis - best AI app practices optimized for Azure\n\nToC: [**USER STORY**](#user-story) \\| [**GETTING STARTED**](#getting-started)  \\| [**HOW IT WORKS**](#how-it-works)\n\n## User story\n\n### Azure AI App Kickstarter overview\n\nL100 level aplication stub for an AI copilot/agent.\n\n## Getting Started\n\n### Codespaces and DevContainers\n\nThis respository has been configured to support GitHub Codespace and DevContainers.\n\n[![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://codespaces.new/evmin/az-ai-kickstarter) [![Open in Dev Containers](https://img.shields.io/static/v1?style=for-the-badge&label=Dev%20Containers&message=Open&color=blue&logo=visualstudiocode)](https://vscode.dev/redirect?url=vscode://ms-vscode-remote.remote-containers/cloneInVolume?url=https://github.com/evmin/az-ai-kickstarter)\n\n> [!WARNING]\n> Do NOT `git clone` the application under Windows and then open a DevContainer. \n> This would create issues with file end of lines. For DevContainer click on the button \n> above and let Visual Studio Code download the repository for you. Alternatively you \n> can also `git clone` under Windows Subsystem for Linux (WSL) and ask Visual Studio Code to\n> `Re-Open in Container`.\n\n### Local\n\nIt is possible to work with a fully local setup.\n\n  - [Azure CLI](https://learn.microsoft.com/en-us/cli/azure/what-is-azure-cli): `az`\n  - [Azure Developer CLI](https://learn.microsoft.com/en-us/azure/developer/azure-developer-cli/overview): `azd`\n  - [Python](https://www.python.org/about/gettingstarted/): `python`\n  - [UV](https://docs.astral.sh/uv/getting-started/installation/): `uv`\n  - Optionally [Docker](https://www.docker.com/get-started/): `docker` \n\n> [!TIP] \n> **Az AI Tip**: Document here how to quickly deploy the solution. Try to reduce this to `azd up` by\n> automating as much as possible. Have a look at `main.bicep` and `scripts` for examples of how to do\n> that\n\n### Quick deploy\n\n\n#### Deployment pre-requisites\n\nCodespaces and DevContainer come with all deployment and development pre-requisites already installed.\n\nOn Windows you can install the pre-requisites by executing the following commands in a PowerShell terminal:\n```powershell\n\twinget install Python.Python.3.12\n\twinget install Microsoft.PowerShell\n\twinget install Microsoft.AzureCLI\n\twinget install Microsoft.Azd\n\twinget install Microsoft.Git\n```\n\nUbuntu/WSL: TBD\n\nMacOSX: TBD\n\n#### Deploy with authentication disabled\n\nTo deploy Azure AI App Kickstarter just run: \n```bash\nazd up\n``` \n\n#### Deploy with authentication enabled\n\nAZD can automatically configures authentication to secure frontend and/or backend. To do so execute the following command before `azd up`:\n```bash\nazd env set WITH_AUTHENTICATION true\n```\n\nIf you already executed `azd up` just set the variable and run provisioning again:\n```bash\nazd env set WITH_AUTHENTICATION true\nazd provision\n```\n\n> [!WARNING] The account executing `azd` needs to be able to create Application Registrations in your Azure\n> Entra ID tenant.\n\n## How it works\n\n- TODO: How to run backend locally\n- TODO: How to run frontend locally\n\n### User Manual\n\n- TODO : Observability\n\n> [!TIP] \n> **Az AI Tip**: Document how the solution is used and operated here.\n> Optionally, if the section is too long, create a `USER_MANUAL.md` file and\n> link to it from here.\n\n### External Model\n\nIf you have an external Azure OpenAI model already provisioned, you can reference it by setting environment variable prior callin `azd up`\n\n```sh\nexport AOAI_ENDPOINT=\"https://<endpoint>.openai.azure.com\"\nexport AOAI_DEPLOYMENT_NAME=\"gpt-4o-2024-11-20\"\nexport AOAI_API_VERSION=\"2024-12-01-preview\"\nexport aoaikeysecret=\"key\"\n```\n\n>[WARNING!] The `aoaikeysecret` is not set in azd .azure/<env>./.env file automatically.\n> In order to use it when running the model locally, either set it as env variable or add it to azd `.env` file.\n\n### Architecture\n\n```mermaid\narchitecture-beta\n    group solution(cloud)[Solution]\n\n    service frontend(server)[Frontend] in solution\n    service backend(server)[Backend] in solution\n\n    frontend:R --> L:backend\n```\n\n## Code of Conduct\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\n\nResources:\n\n- [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/)\n- [Microsoft Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/)\n- Contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with questions or concerns\n\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Responsible AI Guidelines\n\nThis project follows the below responsible AI guidelines and best practices, please review them before using this project:\n\n- [Microsoft Responsible AI Guidelines](https://www.microsoft.com/en-us/ai/responsible-ai)\n- [Responsible AI practices for Azure OpenAI models](https://learn.microsoft.com/en-us/legal/cognitive-services/openai/overview)\n- [Safety evaluations transparency notes](https://learn.microsoft.com/en-us/azure/ai-studio/concepts/safety-evaluations-transparency-note)\n\n## Acknowledgements\n\n  * Kudos to [Pamela Fox](https://github.com/pamelafox) and [James Casey](https://github.com/jamesc) for [Azure-Samples/openai-chat-app-entra-auth-builtin](https://github.com/Azure-Samples/openai-chat-app-entra-auth-builtin) from which we borrowed most of authentication & authorization setup.\n  * Special thank you to [Michael Hofer](https://github.com/mhofer1976) for extensive testing and solving o1 compatibility\n\n## Authors\n\n  * [Dominique Broeglin](https://github.com/dbroeglin)\n  * [Evgeny Minkevich](https://github.com/evmin)\n"
    },
    {
      "name": "microsoft/kagami",
      "stars": 11,
      "img": "https://avatars.githubusercontent.com/u/6154722?s=40&v=4",
      "owner": "microsoft",
      "repo_name": "kagami",
      "description": "OpenAI powered document scanner",
      "homepage": "",
      "language": "Jupyter Notebook",
      "created_at": "2024-03-11T15:13:58Z",
      "updated_at": "2024-10-11T08:08:33Z",
      "topics": [
        "azure-openai"
      ],
      "readme": "# Study Document Ingestion, Metadata Extraction, eArchive Request Automation\n\nThis project provides an example solution to illustrate the use of Azure AI Document Intelligence and Azure OpenAI to extract pertinent details and classifications from study documents for eArchive purposes. The intent is to replace a manual form which a user must fill out about a document when requesting the document to be eArchived in downstream systems, with an automated process triggered by the upload of said document. \n\n## Architecture\n\nThis solution includes a Durable Azure Function with Azure Blob Storage bindings for both trigger and output, the use of Azure AI Document Intelligence for initial document processing, and Azure OpenAI for more complex entity and classification extraction from section chunks. The result is a flattened JSON document of key/value pairs that is parsed and sent to Dataverse to record the eArchive request with details that would have been captured in a form from the user.\n\n![Solution Architecture](./docs/DocIngestion_Architecture.png)\n\n## AI Orchestration using Function App and Semantic Kernel\n\nThis solution includes a blob triggered durable function deployed to an Azure Function App, developed on Python with Semantic Kernel SDK to define and orchestrate prompt function calls to Azure OpenAI, as well as other libraries to perform calls to Azure AI Document Intelligence services.\n\n1. Document uploaded to blob storage, triggers Azure Durable Function\n<br><br>\nPython function uses Langchain and AI Services Document Intelligence to:\n2. Convert document to markdown\n3. Split into chunks by heading\n4. Detect Language\n5. Identify presence of handwritten text (signature)\n<br><br>\n6. Python function uses Semantic Kernel to orchestrate semantic (prompt) functions to Azure OpenAI chat completion service to:\n    * Classify document type from document chunks\n    * Extract entities and metadata of interest from document chunks\n<br><br>\n7. Python function uses Typing and List libraries to\nsynthesize mode of entities extracted from chunks to return most likely correct value for various entities/metadata and forms into flattened JSON\n<br><br>\n9. Resulting JSON file is stored in blob storage for downstream processing\n\n![Function App Logic Flow](./docs/DocIngestion_AppLogicFlow.png)\n\nReference Documentation:\n\n* [Semantic Kernel GitHub Repo](https://github.com/microsoft/semantic-kernel/tree/main)\n* [Getting Started with Semantic Kernel (Python)](https://github.com/microsoft/semantic-kernel/blob/main/python/README.md)\n* [Understanding the Kernel in Semantic Kernel](https://learn.microsoft.com/en-us/semantic-kernel/agents/kernel/?tabs=python)\n* [Adding AI Services to the Kernel](https://learn.microsoft.com/en-us/semantic-kernel/agents/kernel/adding-services?tabs=python)\n\n## Power Platform Components\nSee [Power Platform Section](./powerplat/readme.md) for details\n\n# Deployment Instructions\n\n1. Create an Azure Function App in your Azure Subscription\n2. Create a Storage Account in your Azure Subscription\n3. Configure the Azure Function App with necessary application settings, such as connection strings for Azure Blob Storage. See local.settings.example.json for required settings\n4. Deploy the function code to the Azure Function App\n5. Follow instructions in [Power Platform section](/powerplat/readme.md) to deploy Power Platform portion of the solution\n\n**Reference Documentation:**\n\n* [Manage your function app](https://learn.microsoft.com/en-us/azure/azure-functions/functions-how-to-use-azure-function-app-settings?tabs=portal)\n* [App Configuration references](https://learn.microsoft.com/en-us/azure/app-service/app-service-configuration-references?toc=%2Fazure%2Fazure-functions%2Ftoc.json)\n* [Develop Azure Functions by using Visual Studio Code](https://learn.microsoft.com/en-us/azure/azure-functions/functions-develop-vs-code?tabs=node-v4%2Cpython-v2%2Cisolated-process&pivots=programming-language-python)\n\n# Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n"
    },
    {
      "name": "msalemor/adventureworks-viewer-ai",
      "stars": 11,
      "img": "https://avatars.githubusercontent.com/u/44649358?s=40&v=4",
      "owner": "msalemor",
      "repo_name": "adventureworks-viewer-ai",
      "description": "A demo application to showcase adding intelligence to an application with different levels of complexity.",
      "homepage": null,
      "language": "C#",
      "created_at": "2024-04-04T13:13:03Z",
      "updated_at": "2024-11-23T14:38:02Z",
      "topics": [],
      "readme": "# Adventureworks-AI-Viewer\n\n## Overview\n\nA demo application to showcase adding intelligence to an application at different levels of complexity.\n\n![A screen capture of Advetureworks AI viewer showing getting data in differente modes.](images/adventureworks-ai-viewer.png)\n\n## Architecture\n\n```mermaid\ngraph LR;\n  A((User))<-->F(Frontend)\n  F<-->B(Backend)\n  B<--GptBot-->O(GPTBot)\n  B<--SQLBot-->SQL(Sqbot)\n  B<-->RAG\n  B<--Assistants API Bot-->Asst(Assistants API Bot)\n  B<--MultiagentBot-->M(Multiagent Bot<br/>Proxy Agent<br/>Semantic Processing)\n  M<--Other or<br/>Unknown<br/>Intent-->O\n  M<--SQL Scripts<br/>Intent-->SQL\n  M<--Charts<br/>Intent-->Asst\n  M<--RAG<br/>Intent-->RAG\n```\n\n### Application Options\n\n| Selection | Description |\n| --------- | ----------- |\n| No AI | A user can click on the icons to show the data on the table on the grid in the UI. |\n| Chatbot | A user can ask questions related to top customers and products. |\n| RAG | A user can ask questions about product information. |\n| Sqlbot | A user can ask the system to generate SQL statement. The SQL statements are then executed and presented on the grid in the UI. |\n| Assistants API Bot | A user can ask the system to perform complex data analysis related to top customers and top products and generate bars and charts. |\n| Multiagent Bot | This Bot will decide which agent to call (Chatbot, SQLbot or Assistant Bot based on the user's intent and will respond accordingly. |\n\n## Requirements\n\n### Database\n\nThis app leverages the SQL Adventurework database data. However, additional views need to be deployed. You will need to:\n\n- Deploy Azure SQL Database with AdventuresWorks to Azure\n\n![A screen capture of Advetureworks AI viewer showing getting data in differente modes.](images/azure-sql-sample-data.png)\n\n- Execute the scripts at: [src/database/views_script.sql](src/database/views_script.sql) to add the supporting views\n\n### Backend - Python\n\n- Python 3.11\n- openai==1.16.1\n- fastapi==0.110.1\n- uvicorn[standard]==0.29.0\n- pymssql==2.2.11\n- pillow==10.3.0\n\n### Backend - C#\n\n- .NET 8\n- Packages:\n  - Azure OpenAI\n  - Azure Assistants API\n  - Dotnet.Env\n\n### Frontend - React\n\n- React\n- Tailwind CSS\n- react-data-grid\n- react-icons\n- react-loader-spinner\n- react-markdown\n\n## Debugging - Python\n\n- Install the frontend dependencies: `cd src/frontend && npm install`\n- Install the backend dependencies: `cd src/backend && pip install -r requirements.txt`\n- Create or update the environment variable files at: `src/backend/.env`\n\n```bash\nDB_HOST=<NAME>.database.windows.net\nDB_PORT=1433\nDB_USER=<ADMIN_USER>\nDB_PASSWORD=<ADMIN_PASSWORD>\nDB_DATABASE=<DB_NAME>\n\nOPENAI_FULL_URI=https://<NAME>.openai.azure.com/openai/deployments/gpt4-1106-preview/chat/completions?api-version=2024-02-15-preview\nOPENAI_URI=https://<NAME>.openai.azure.com/\nOPENAI_KEY=<API_KEY>\nOPENAI_GPT_DEPLOYMENT=gpt4-1106-preview\nOPENAI_VERSION=2024-02-15-preview\nOPENAPI_URL=/openapi.json\n```\n\n- Start the backend: `cd src/backend && sh run.sh` or `cd src/backend && uvicorn main:app --reload`\n- Start the frontend: `cd src/frontend && npm run dev`\n\n\n## Debugging - C#\n\n- Install the frontend dependencies: `cd src/frontend && npm install`\n- Install the backend dependencies: `cd src/csbackend && dotnet restore`\n- Create or update the environment variable files at: `src/csbackend/.env`\n\n```bash\nDB_HOST=<NAME>.database.windows.net\nDB_PORT=1433\nDB_USER=<ADMIN_USER>\nDB_PASSWORD=<ADMIN_PASSWORD>\nDB_DATABASE=<DB_NAME>\n\nOPENAI_FULL_URI=https://<NAME>.openai.azure.com/openai/deployments/gpt4-1106-preview/chat/completions?api-version=2024-02-15-preview\nOPENAI_URI=https://<NAME>.openai.azure.com/\nOPENAI_KEY=<API_KEY>\nOPENAI_GPT_DEPLOYMENT=gpt4-1106-preview\nOPENAI_VERSION=2024-02-15-preview\nOPENAPI_URL=/openapi.json\n```\n\n- Start the backend: `cd src/csbackend` or `cd src/csbackend && dotnet watch run`\n- Start the frontend: `cd src/frontend && npm run dev`"
    },
    {
      "name": "Joining-AI/JoinAgent",
      "stars": 11,
      "img": "https://avatars.githubusercontent.com/u/154000096?s=40&v=4",
      "owner": "Joining-AI",
      "repo_name": "JoinAgent",
      "description": "\"Joining-Agents\" is an open-source project repository by Joining, dedicated to the improvement of Massive Data Process",
      "homepage": "",
      "language": "Python",
      "created_at": "2023-12-16T13:51:39Z",
      "updated_at": "2024-10-16T14:21:49Z",
      "topics": [],
      "readme": "# Joining-Agents\n\n# Changelog\n\n## [Unreleased]\n\n### Changed\n- 更新检查点重载模式\n- 添加 LLM 解析 PDF 功能\n- 改进 备用LLM 机制\n- 优化空白模板处理\n- 统一模型调用类方式\n\n## [1.0.0] - 2024-08-21\n\n注意到.env文件中有各个填入API的位置，我们推荐使用https://api.openai-next.com/作为MULTI_LLM的api\n"
    },
    {
      "name": "chanchimin/AgentMonitor",
      "stars": 10,
      "img": "https://avatars.githubusercontent.com/u/75533759?s=40&v=4",
      "owner": "chanchimin",
      "repo_name": "AgentMonitor",
      "description": "Codes for our paper \"AgentMonitor: A Plug-and-Play Framework for Predictive and Secure Multi-Agent Systems\"",
      "homepage": "",
      "language": "Python",
      "created_at": "2024-08-25T07:27:41Z",
      "updated_at": "2025-03-03T12:01:05Z",
      "topics": [],
      "readme": "<div align=\"center\">\n<img src=\"figs/crop_agentmonitor/crop_agentmonitor_1.png\" width=\"500px\"/>\n<br />\n<br />\n\n\n\n<h1 align=\"center\"><img src=\"https://github.com/user-attachments/assets/9cd374b4-d305-43fe-89ca-dbc92c726d34\"  style=\"width: 2em; height: 2em;\"> AgentMonitor </h1>\n\n</div>\n\n\nThis is the repo for our paper: [\"AgentMonitor: A Plug-and-Play Framework for Predictive and Secure Multi-Agent Systems\"](https://arxiv.org/abs/2408.14972)\n\n## Introduction\nAgentMonitor is a framework designed to capture and analyze inputs and outputs at each step of the agent interaction process. By transforming this data into meaningful statistics, AgentMonitor enables the (1) training of regression models—such as XGBoost—to predict the downstream task performance of multi-agent teams. (2) post-edit the responses generated each step on-the-fly.\n\n\n\n## Installation\n\n~~~bash\ngit clone https://github.com/chanchimin/agentmonitor.git\ncd agentmonitor\nconda create -n agentmonitor python=3.9\npip install -r requirements.txt\n~~~\n\n\n## Basic Usage\n\n\n\n~~~py\nimport fire\nfrom agentmonitor import AgentMonitor\n\nclass Agent:\n    # ...\n\n    # get messages / update history\n    def put_message(self, message):\n        # ...\n        return result\n\n    # take actions / generate responses\n    def act(self,\n        # ...\n    ):\n        # ...\n        return result\n\nasync def main():\n    monitor = AgentMonitor()\n    agent = Agent()\n    await monitor.register(agent, agent.put_message, agent.act) # register target agent\n    # arguments of function register:\n    #     obj: necessary, agent object\n    #     input_func: necessary, input function of specific agent object\n    #     output_func: necessary, output function of specific agent object\n    #     state_func: state function of specific agent object, default=None\n    #     context_in_str: context attribution of agent as str, default=None\n    #     prompt: role prompt of agent as str, default=None\n    #     input_turbulence_type: turbulence type of input function, can be 0/1/2/3, default=0 (no turbulence)\n    #     output_turbulence_type: turbulence type of output function, can be 0/1/2/3, default=0 (no turbulence)\n    #     input_noise_prob: probability of adding turbulence for input function, default=0.3\n    #     output_noise_prob: probability of adding turbulence for output function, default=0.3\n    #     name: name of agent as str, default=None\n    #     use_partial: necessary, use partial tool or not, default=False\n    # for example: await monitor.register(simplecoder, simplecoder.put_message, simplecoder._act, simplecoder._think, context_in_str=\"rc.memory\", prompt=simplecoder.actions[0].PROMPT_TEMPLATE, name=\"simplecoder\")\n    # ...\n    # Note: output path must be a json file (\"monitor_output_test.json\").\n    monitor.recording(\"monitor_output_test.json\") # record monitor history \n\nif __name__ == \"__main__\":\n    fire.Fire(main)\n~~~\n\n### Disclaimer\n\nPlease note that the code provided above are for illustrative purposes only. They demonstrate the basic usage, but may not be directly applicable to all scenarios or use cases.\n\nFor specific experiments or applications, modifications may be necessary. We strongly recommend referring to the `examples` directory for more detailed and specific use cases.\n\n## RoadMap\n\nOur framework is designed to provide non-invasive, plug-and-play monitoring behavior that integrates seamlessly with various multi-agent frameworks. While this design is intended to be broadly applicable, different frameworks have unique requirements, which may necessitate specific adjustments.\n\nFor instance, back to the time we run our experiments, we encountered a limitation with MetaGPT, which did not support passing stop-word arguments—a feature necessary for custom models like \"llama3_8b_instruct\" at that moment. Therefore, we store the modifications in the ```examples/metagpt_examples/metagpt``` dir for replication purpose. \nMoving forward, we plan to generalize the framework further, making it easier for users to adopt in diverse environments.\n\nCurrently, supported frameworks include:\n- [x] MetaGPT\n- [x] AgentVerse\n- [ ] AutoGen\n- [ ] ChatDev\n\n---\n## Experiment Usage\n\nHere, we provide the detailed steps for running experiments used in our paper.\n\n### Step 1: Run the Multi-Agent Experiments with our AgentMonitor\n\n```shell\n\ncd examples/metagpt_examples\n\npython example_metagpt_codetest_base.py \\\n--task gsm8k \\\n--output_path \"output/base/8b_8b_8b_8b\" \\\n--llm_config_files [\\'llama3_8b_instruct_8003.yaml\\',\\'llama3_8b_instruct_8003.yaml\\',\\'llama3_8b_instruct_8003.yaml\\',\\'llama3_8b_instruct_8003.yaml\\'] \\\n\n```\n\n### Step 2: Use the stored information to calculate indicators (Agent Scores)\n\n```shell\n\npython judge_metagpt_codetest.py \\\n--task gsm8k \\\n--judge_llm_config_files \"llama3_70b_instruct_8002.json\" \\\n--input_path \"output/base/8b_8b_8b_8b_turn_3\"\n\n```\n\n\n### Step 3: Use the stored information to calculate indicators (Graph Attributes)\n\n```shell\n\n\n# 3.1 Calculate the token count for graph analysis\npython tokencount_metagpt_codetest.py \\\n--task gsm8k \\\n--input_path \"output/base/8b_8b_8b_8b_turn_3\"\n\n# 3.2 Calculate the graph attributes\npython calc_graph_attributes.py \\\n--task gsm8k \\\n--input_path \"output/base/8b_8b_8b_8b_turn_3\"\n\n```\n\n### Step 4: Aggregate all the information\n\n```shell\n\n\n# aggregate all the runs into one file\npython aggregate_statistics.py \\\n--task gsm8k \\\n--arch base \\\n--input_paths ['output/base/8b_8b_8b_8b_turn_3', 'output/base/8b_8b_8b_70b_turn_3', 'output/base/8b_8b_70b_8b_turn_3', ...] \\\n--output_path \"statistic_output/base/gsm8k/total_results.csv\" \\\n--judge_llm \"judged_by_gpt_3.5_turbo\"\n\n```\n\n\n### Step 5: Calculate the regression\n\n```shell\n\npython calc_regression.py \\\n--input_csv_paths \\\n\"statistic_output/base/gsm8k/total_results.csv\" \\\n... \\\n... \\\n--output_path \\\n\"statistic_output/regression_results\"\n\n```\n"
    },
    {
      "name": "antronic/maid_semantic_kernel",
      "stars": 10,
      "img": "https://avatars.githubusercontent.com/u/2222477?s=40&v=4",
      "owner": "antronic",
      "repo_name": "maid_semantic_kernel",
      "description": "A maid cafe's maid AI assistant with Microsoft Semantic Kernel",
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2024-05-10T20:39:34Z",
      "updated_at": "2024-08-18T07:28:55Z",
      "topics": [],
      "readme": "# Maid Cafe Copilot with Semantic Kernel\n\n![Maid Cafe AI Copilot Assistant](https://raw.githubusercontent.com/antronic/maid_semantic_kernel/main/images/maid-copilot-semantic-kernel-2.png)\n\nThe Maid Cafe AI Copilot Assistant which built with Semantic Kernel.\nThe Maid Cafe AI Copilot Assistant is a demo project that uses the Semantic Kernel to demonstrate and provide the talking script for the Maid in various situations.\n\n## Installation\n\n```bash\npip install -r requirements.txt\n```\n\n## Learn more about Semantic Kernel\n\n- [Semantic Kernel](https://learn.microsoft.com/en-us/semantic-kernel/overview)\n- [Semantic Kernel Repo](https://github.com/microsoft/semantic-kernel/tree/main/python)\n\n\n## Maid Cafe\n\nMaid Cafe is a place where you can enjoy a meal or a drink served by a maid or butler. The concept of the maid cafe originated in Japan, but it has since spread to other countries around the world. Maid cafes are popular with people who enjoy anime, manga, and other aspects of Japanese pop culture."
    },
    {
      "name": "alfredodeza/azure-chat-demo",
      "stars": 10,
      "img": "https://avatars.githubusercontent.com/u/317847?s=40&v=4",
      "owner": "alfredodeza",
      "repo_name": "azure-chat-demo",
      "description": "Examples to use Azure with LLMs for Chat",
      "homepage": null,
      "language": "Python",
      "created_at": "2023-11-14T13:17:32Z",
      "updated_at": "2025-02-24T01:44:18Z",
      "topics": [],
      "readme": "# Azure Chat Demo\nExamples to use Azure with LLMs for Chat and learn how to use Azure OpenAI Service to create powerful AI-powered chat applications.\n\n## Install the prerequisites\n\nUse the `requirements.txt` to install all dependencies\n\n```bash\npython -m venv .venv\n./.venv/bin/pip install -r requirements.txt\n```\n\n### Add your keys\n\nFind the Azure OpenAI Keys in the Azure OpenAI Service. Note, that keys aren't in the studio, but in the resource itself. Add them to a local `.env` file. This repository ignores the `.env` file to prevent you (and me) from adding these keys by mistake.\n\nYour `.env` file should look like this:\n\n```\n# Azure OpenAI \nOPENAI_API_TYPE=\"azure\"\nOPENAI_API_BASE=\"https://demo-alfredo-openai.openai.azure.com/\"\nOPENAI_API_KEY=\"0asd8924yl87asljhsd823lkjahsdf234\"\nOPENAI_API_VERSION=\"2023-07-01-preview\"\n\n# Azure Cognitive Search\nAZURE_COGNITIVE_SEARCH_SERVICE_NAME=\"https://demo-alfredo.search.windows.net\"\nAZURE_COGNITIVE_SEARCH_API_KEY=\"zlkjhasd876lkjh234978sg098srtiuy\"\nAZURE_COGNITIVE_SEARCH_INDEX_NAME=\"demo-index\"\n```\n\nNote that the Azure Cognitive Search is only needed if you are following the Retrieval Augmented Guidance (RAG) demo. It isn't required for a simple Chat application.\n"
    },
    {
      "name": "Shailender-Youtube/Multi-Agent-Semantic-Kernel-Azure-Agent-Service-Python",
      "stars": 9,
      "img": "https://avatars.githubusercontent.com/u/174622601?s=40&v=4",
      "owner": "Shailender-Youtube",
      "repo_name": "Multi-Agent-Semantic-Kernel-Azure-Agent-Service-Python",
      "description": "Multi-Agent AI Application(Python) that uses Semantic-Kernel along with Azure AI Agent Service in Azure Ai Foundry",
      "homepage": null,
      "language": "Python",
      "created_at": "2025-03-06T12:40:53Z",
      "updated_at": "2025-04-21T15:13:57Z",
      "topics": [],
      "readme": "# Multi-Agent-Semantic-Kernel-Azure-Agent-Service-Python\nMulti-Agent AI Application(Python) that uses Semantic-Kernel along with Azure AI Agent Service in Azure Ai Foundry\n\n# Watch the Video for Step by Step Configuration\n\n[![Video Title](https://img.youtube.com/vi/bHVRrMCPzwM/0.jpg)](https://www.youtube.com/watch?v=bHVRrMCPzwM)\n"
    },
    {
      "name": "Azure-Samples/teams-chat-with-your-data-solution-accelerator",
      "stars": 9,
      "img": "https://avatars.githubusercontent.com/u/1844662?s=40&v=4",
      "owner": "Azure-Samples",
      "repo_name": "teams-chat-with-your-data-solution-accelerator",
      "description": "Teams-focussed Solution Accelerator for the RAG pattern running in Azure, using Azure AI Search for retrieval and Azure OpenAI large language models.",
      "homepage": "https://azure.microsoft.com/products/search",
      "language": "Python",
      "created_at": "2024-10-31T14:20:21Z",
      "updated_at": "2025-03-06T12:36:21Z",
      "topics": [],
      "readme": "# Teams-focussed chat with your data solution accelerator\n\nThis repo has been forked from the main [chat with your data solution accelerator](https://github.com/Azure-Samples/chat-with-your-data-solution-accelerator/).\n\nIt seeks to improve the original solution accelerator by:\n1. Make available all orchestrators to the web front end and the Teams from end\n2. Provide chat history in the Teams front end\n\n> [!CAUTION]\n> This repo is a work in progress. So do not use this repo until the changes have been completed.\n\n## Internal Architecture\n\nThere are a number of different orchestrations for requests. These are:\n\n1. function calling\n2. semantic kernel\n3. Langchain\n\nThese perform a RAG process on the client-side, built on OpenAI function calling.\n\nThe web application also has a \"byod\" (presumably bring your own data) flow in-line in the main web app. This does a server RAG process in the same way in which the chat interface in AI Studio performs RAG.\n\nThe plan is to take this flow and make this as an additional orchestator that can be chosen both by the web application and the Teams application.\n\n![alt text](./docs/images/revised-software-architecture.png \"Revised software architecture\")\n\n---\nname: Chat with your data - Solution accelerator (Python)\ndescription: Chat with your data using OpenAI and AI Search with Python.\nlanguages:\n- python\n- typescript\n- bicep\n- azdeveloper\nproducts:\n- azure-openai\n- azure-cognitive-search\n- azure-app-service\n- azure\n- azure-bot-service\n- document-intelligence\n- azure-functions\n- azure-storage-accounts\n- azure-speech\npage_type: sample\nurlFragment: chat-with-your-data-solution-accelerator\n\n---\n<!-- YAML front-matter schema: https://review.learn.microsoft.com/en-us/help/contribute/samples/process/onboarding?branch=main#supported-metadata-fields-for-readmemd -->\n\n# Chat with your data - Solution accelerator\n\n\n ##### Table of Contents\n- [Chat with your data - Solution accelerator](#chat-with-your-data---solution-accelerator)\n        - [Table of Contents](#table-of-contents)\n  - [User story](#user-story)\n    - [About this repo](#about-this-repo)\n    - [When should you use this repo?](#when-should-you-use-this-repo)\n    - [Key features](#key-features)\n    - [Target end users](#target-end-users)\n    - [Industry scenario](#industry-scenario)\n  - [Deploy](#deploy)\n    - [Pre-requisites](#pre-requisites)\n    - [Products used](#products-used)\n    - [Required licenses](#required-licenses)\n    - [Pricing Considerations](#pricing-considerations)\n    - [Deploy instructions](#deploy-instructions)\n    - [Testing the deployment](#testing-the-deployment)\n  - [Supporting documentation](#supporting-documentation)\n    - [Resource links](#resource-links)\n    - [Licensing](#licensing)\n  - [Disclaimers](#disclaimers)\n## User story\nWelcome to the *Chat with your data* Solution accelerator repository! The *Chat with your data* Solution accelerator is a powerful tool that combines the capabilities of Azure AI Search and Large Language Models (LLMs) to create a conversational search experience. This solution accelerator uses an Azure OpenAI GPT model and an Azure AI Search index generated from your data, which is integrated into a web application to provide a natural language interface, including [speech-to-text](docs/speech_to_text.md) functionality, for search queries. Users can drag and drop files, point to storage, and take care of technical setup to transform documents. Everything can be deployed in your own subscription to accelerate your use of this technology.\n\n![Solution Architecture - Chat with your data](/docs/images/cwyd-solution-architecture.png)\n\n### About this repo\n\nThis repository provides an end-to-end solution for users who want to query their data with natural language. It includes a well designed ingestion mechanism for multiple file types, an easy deployment, and a support team for maintenance. The accelerator demonstrates both Push or Pull Ingestion; the choice of orchestration (Semantic Kernel, LangChain, OpenAI Functions or [Prompt Flow](docs/prompt_flow.md)) and should be the minimum components needed to implement a RAG pattern. It is not intended to be put into Production as-is without experimentation or evaluation of your data. It provides the following features:\n\n* Chat with an Azure OpenAI model using your own data\n* Upload and process your documents\n* Index public web pages\n* Easy prompt configuration\n* Multiple chunking strategies\n\n### When should you use this repo?\n\nIf you need to customize your scenario beyond what [Azure OpenAI on your data](https://learn.microsoft.com/azure/ai-services/openai/concepts/use-your-data) offers out-of-the-box, use this repository.\nBy default, this repo comes with one specific set of RAG configurations including but not limited to: chunk size, overlap, retrieval/search type and system prompt. It is important that you evaluate the retrieval/search and the generation of the answers for your data and tune these configurations accordingly before you use this repo in production. For a starting point to understand and perform RAG evaluations, we encourage you to look into the [RAG Experiment Accelerator](https://github.com/microsoft/rag-experiment-accelerator).\n\nThe accelerator presented here provides several options, for example:\n* The ability to ground a model using both data and public web pages\n* A backend with support for 'custom' and 'On Your Data' [conversation flows](./docs/conversation_flow_options.md)\n* Advanced prompt engineering capabilities\n* An admin site for ingesting/inspecting/configuring your dataset on the fly\n* Push or Pull model for data ingestion:  See [integrated vectorization](./docs/integrated_vectorization.md) documentation for more details\n* Running a Retrieval Augmented Generation (RAG) solution locally\n\n*Have you seen [ChatGPT + Enterprise data with Azure OpenAI and AI Search demo](https://github.com/Azure-Samples/azure-search-openai-demo)? If you would like to experiment: Play with prompts, understanding RAG pattern different implementation approaches, see how different features interact with the RAG pattern and choose the best options for your RAG deployments, take a look at that repo.\n\nHere is a comparison table with a few features offered by Azure, an available GitHub demo sample and this repo, that can provide guidance when you need to decide which one to use:\n\n| Name\t| Feature or Sample? |\tWhat is it? | When to use? |\n| ---------|---------|---------|---------|\n|[\"Chat with your data\" Solution Accelerator](https://aka.ms/ChatWithYourDataSolutionAccelerator) - (This repo)\t| Azure sample | End-to-end baseline RAG pattern sample that uses Azure AI Search as a retriever.\t| This sample should be used by Developers when the  RAG pattern implementations provided by Azure are not able to satisfy business requirements. This sample provides a means to customize the solution. Developers must add their own code to meet requirements, and adapt with best practices according to individual company policies. |\n|[Azure OpenAI on your data](https://learn.microsoft.com/azure/ai-services/openai/concepts/use-your-data) | Azure feature | Azure OpenAI Service offers out-of-the-box, end-to-end RAG implementation that uses a REST API or the web-based interface in the Azure AI Studio to create a solution that connects to your data to enable an enhanced chat experience with Azure OpenAI ChatGPT models and Azure AI Search. | This should be the first option considered for developers that need an end-to-end solution for Azure OpenAI Service with an Azure AI Search retriever. Simply select supported data sources, that ChatGPT model in Azure OpenAI Service , and any other Azure resources needed to configure your enterprise application needs. |\n|[Azure Machine Learning prompt flow](https://learn.microsoft.com/azure/machine-learning/concept-retrieval-augmented-generation)\t| Azure feature | RAG in Azure Machine Learning is enabled by integration with Azure OpenAI Service for large language models and vectorization. It includes support for Faiss and Azure AI Search as vector stores, as well as support for open-source offerings, tools, and frameworks such as LangChain for data chunking. Azure Machine Learning prompt flow offers the ability to test data generation, automate prompt creation, visualize prompt evaluation metrics, and integrate RAG workflows into MLOps using pipelines.  | When Developers need more control over processes involved in the development cycle of LLM-based AI applications, they should use Azure Machine Learning prompt flow to create executable flows and evaluate performance through large-scale testing. |\n|[ChatGPT + Enterprise data with Azure OpenAI and AI Search demo](https://github.com/Azure-Samples/azure-search-openai-demo) | Azure sample | RAG pattern demo that uses Azure AI Search as a retriever. | Developers who would like to use or present an end-to-end demonstration of the RAG pattern should use this sample. This includes the ability to deploy and test different retrieval modes, and prompts to support business use cases. |\n|[RAG Experiment Accelerator](https://github.com/microsoft/rag-experiment-accelerator) | Tool |The RAG Experiment Accelerator is a versatile tool that helps you conduct experiments and evaluations using Azure AI Search and RAG pattern. | RAG Experiment Accelerator is to make it easier and faster to run experiments and evaluations of search queries and quality of response from OpenAI. This tool is useful for researchers, data scientists, and developers who want to, Test the performance of different Search and OpenAI related hyperparameters. |\n\n\n### Key features\n- **Private LLM access on your data**: Get all the benefits of ChatGPT on your private, unstructured data.\n- **Single application access to your full data set**: Minimize endpoints required to access internal company knowledgebases. Reuse the same backend with the [Microsoft Teams Extension](docs/teams_extension.md)\n- **Natural language interaction with your unstructured data**: Use natural language to quickly find the answers you need and ask follow-up queries to get the supplemental details, including [Speech-to-text](docs/speech_to_text.md).\n- **Easy access to source documentation when querying**: Review referenced documents in the same chat window for additional context.\n- **Data upload**: Batch upload documents of [various file types](docs/supported_file_types.md)\n- **Accessible orchestration**: Prompt and document configuration (prompt engineering, document processing, and data retrieval)\n\n\n**Note**: The current model allows users to ask questions about unstructured data, such as PDF, text, and docx files. See the [supported file types](docs/supported_file_types.md).\n\n### Target end users\nCompany personnel (employees, executives) looking to research against internal unstructured company data would leverage this accelerator using natural language to find what they need quickly.\n\nThis accelerator also works across industry and roles and would be suitable for any employee who would like to get quick answers with a ChatGPT experience against their internal unstructured company data.\n\nTech administrators can use this accelerator to give their colleagues easy access to internal unstructured company data. Admins can customize the system configurator to tailor responses for the intended audience.\n\n\n### Use Case scenarios\n\n#### Financial Advisor Scenario\nThe sample data illustrates how this accelerator could be used in the financial services industry (FSI).\n\nIn this scenario, a financial advisor is preparing for a meeting with a potential client who has expressed interest in Woodgrove Investments’ Emerging Markets Funds. The advisor prepares for the meeting by refreshing their understanding of the emerging markets fund's overall goals and the associated risks.\n\nNow that the financial advisor is more informed about Woodgrove’s Emerging Markets Funds, they're better equipped to respond to questions about this fund from their client.\n\n#### Contract Review and Summarization Assistant scenario\nAdditionally, we have implemented a Legal Review and Summarization Assistant scenario to demonstrate how this accelerator can be utilized in any industry. The Legal Review and Summarization Assistant helps professionals manage and interact with a large collection of documents efficiently. For more details, refer to the [Contract Review and Summarization Assistant README](docs/contract_assistance.md).\n\nNote: Some of the sample data included with this accelerator was generated using AI and is for illustrative purposes only.\n\n\n#### Employee Onboarding Scenario\nThe sample data illustrates how this accelerator could be used for an employee onboarding scenario in across industries.\n\nIn this scenario, a newly hired employee is in the process of onboarding to their organization. Leveraging the solution accelerator, she navigates through the extensive offerings of her organization’s health and retirement benefits. With the newly integrated chat history capabilities, they can revisit previous conversations, ensuring continuity and context across multiple days of research. This functionality allows the new employee to efficiently gather and consolidate information, streamlining their onboarding experience. [For more details, refer to the README](docs/employee_assistance.md).\n\n\n---\n\n![One-click Deploy](/docs/images/oneClickDeploy.png)\n## Deploy\n### Pre-requisites\n- Azure subscription - [Create one for free](https://azure.microsoft.com/free/) with owner access.\n- Approval to use Azure OpenAI services with your Azure subcription. To apply for approval, see [here](https://learn.microsoft.com/en-us/azure/ai-services/openai/overview#how-do-i-get-access-to-azure-openai).\n- [Enable custom Teams apps and turn on custom app uploading](https://learn.microsoft.com/en-us/microsoftteams/platform/concepts/build-and-test/prepare-your-o365-tenant#enable-custom-teams-apps-and-turn-on-custom-app-uploading) (optional: Teams extension only)\n\n### Products used\n- Azure App Service\n- Azure Application Insights\n- Azure Bot\n- Azure OpenAI\n- Azure Document Intelligence\n- Azure Function App\n- Azure Search Service\n- Azure Storage Account\n- Azure Speech Service\n- Azure CosmosDB\n- Teams (optional: Teams extension only)\n\n### Required licenses\n- Microsoft 365 (optional: Teams extension only)\n\n### Pricing Considerations\n\nThis solution accelerator deploys multiple resources. Evaluate the cost of each component prior to deployment.\n\nThe following are links to the pricing details for some of the resources:\n- [Azure OpenAI service pricing](https://azure.microsoft.com/pricing/details/cognitive-services/openai-service/). GPT and embedding models are charged separately.\n- [Azure AI Search pricing](https://azure.microsoft.com/pricing/details/search/). AI Search core service and semantic ranker are charged separately.\n- [Azure Blob Storage pricing](https://azure.microsoft.com/pricing/details/storage/blobs/)\n- [Azure Functions pricing](https://azure.microsoft.com/pricing/details/functions/)\n- [Azure AI Document Intelligence pricing](https://azure.microsoft.com/pricing/details/ai-document-intelligence/)\n- [Azure Web App Pricing](https://azure.microsoft.com/pricing/details/app-service/windows/)\n\n### Deploy instructions\n\nThere are two choices; the \"Deploy to Azure\" offers a one click deployment where you don't have to clone the code, alternatively if you would like a developer experience, follow the [Local deployment instructions](./docs/LOCAL_DEPLOYMENT.md).\n\nThe demo, which uses containers pre-built from the main branch is available by clicking this button:\n\n[![Deploy to Azure](https://aka.ms/deploytoazurebutton)](https://portal.azure.com/#create/Microsoft.Template/uri/https%3A%2F%2Fraw.githubusercontent.com%2FAzure-Samples%2Fchat-with-your-data-solution-accelerator%2Fmain%2Finfra%2Fmain.json)\n\n**Note**: The default configuration deploys an OpenAI Model \"gpt-35-turbo\" with version 0613. However, not all\nlocations support this version. If you're deploying to a location that doesn't support version 0613, you'll need to\nswitch to a lower version. To find out which versions are supported in different regions, visit the\n[GPT-35 Turbo Model Availability](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models#gpt-35-turbo-model-availability) page.\n\n### Testing the deployment\n1. Navigate to the admin site, where you can upload documents. It will be located at:\n\n    `https://web-{RESOURCE_TOKEN}-admin.azurewebsites.net/`\n\n    Where `{RESOURCE_TOKEN}` is uniquely generated during deployment. This is a combination of your subscription and the name of the resource group. Then select **Ingest Data** and add your data. You can find sample data in the `/data` directory.\n\n    ![A screenshot of the admin site.](./docs/images/admin-site.png)\n\n\n2. Navigate to the web app to start chatting on top of your data. The web app can be found at:\n\n    `https://web-{RESOURCE_TOKEN}.azurewebsites.net/`\n\n\n    ![A screenshot of the chat app.](./docs/images/web-unstructureddata.png)\n\n\\\n\\\n![Supporting documentation](/docs/images/supportingDocuments.png)\n## Supporting documentation\n\n### Resource links\n\nThis solution accelerator deploys the following resources. It's critical to comprehend the functionality of each. Below are the links to their respective documentation:\n- [Application Insights overview - Azure Monitor | Microsoft Learn](https://learn.microsoft.com/en-us/azure/azure-monitor/app/app-insights-overview?tabs=net)\n- [Azure OpenAI Service - Documentation, quickstarts, API reference - Azure AI services | Microsoft Learn](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/use-your-data)\n- [Using your data with Azure OpenAI Service - Azure OpenAI | Microsoft Learn](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/use-your-data)\n- [Content Safety documentation - Quickstarts, Tutorials, API Reference - Azure AI services | Microsoft Learn](https://learn.microsoft.com/en-us/azure/ai-services/content-safety/)\n- [Document Intelligence documentation - Quickstarts, Tutorials, API Reference - Azure AI services | Microsoft Learn](https://learn.microsoft.com/en-us/azure/ai-services/document-intelligence/?view=doc-intel-3.1.0)\n- [Azure Functions documentation | Microsoft Learn](https://learn.microsoft.com/en-us/azure/azure-functions/)\n- [Azure Cognitive Search documentation | Microsoft Learn](https://learn.microsoft.com/en-us/azure/search/)\n- [Speech to text documentation - Tutorials, API Reference - Azure AI services - Azure AI services | Microsoft Learn](https://learn.microsoft.com/en-us/azure/ai-services/speech-service/index-speech-to-text)\n- [Bots in Microsoft Teams - Teams | Microsoft Learn](https://learn.microsoft.com/en-us/microsoftteams/platform/bots/what-are-bots) (Optional: Teams extension only)\n\n### Licensing\n\nThis repository is licensed under the [MIT License](LICENSE.md).\n\nThe data set under the /data folder is licensed under the [CDLA-Permissive-2 License](CDLA-Permissive-2.md).\n\n## Disclaimers\nThis Software requires the use of third-party components which are governed by separate proprietary or open-source licenses as identified below, and you must comply with the terms of each applicable license in order to use the Software. You acknowledge and agree that this license does not grant you a license or other right to use any such third-party proprietary or open-source components.\n\nTo the extent that the Software includes components or code used in or derived from Microsoft products or services, including without limitation Microsoft Azure Services (collectively, “Microsoft Products and Services”), you must also comply with the Product Terms applicable to such Microsoft Products and Services. You acknowledge and agree that the license governing the Software does not grant you a license or other right to use Microsoft Products and Services. Nothing in the license or this ReadMe file will serve to supersede, amend, terminate or modify any terms in the Product Terms for any Microsoft Products and Services.\n\nYou must also comply with all domestic and international export laws and regulations that apply to the Software, which include restrictions on destinations, end users, and end use. For further information on export restrictions, visit https://aka.ms/exporting.\n\nYou acknowledge that the Software and Microsoft Products and Services (1) are not designed, intended or made available as a medical device(s), and (2) are not designed or intended to be a substitute for professional medical advice, diagnosis, treatment, or judgment and should not be used to replace or as a substitute for professional medical advice, diagnosis, treatment, or judgment. Customer is solely responsible for displaying and/or obtaining appropriate consents, warnings, disclaimers, and acknowledgements to end users of Customer’s implementation of the Online Services.\n\nYou acknowledge the Software is not subject to SOC 1 and SOC 2 compliance audits. No Microsoft technology, nor any of its component technologies, including the Software, is intended or made available as a substitute for the professional advice, opinion, or judgement of a certified financial services professional. Do not use the Software to replace, substitute, or provide professional financial advice or judgment.\n\nBY ACCESSING OR USING THE SOFTWARE, YOU ACKNOWLEDGE THAT THE SOFTWARE IS NOT DESIGNED OR INTENDED TO SUPPORT ANY USE IN WHICH A SERVICE INTERRUPTION, DEFECT, ERROR, OR OTHER FAILURE OF THE SOFTWARE COULD RESULT IN THE DEATH OR SERIOUS BODILY INJURY OF ANY PERSON OR IN PHYSICAL OR ENVIRONMENTAL DAMAGE (COLLECTIVELY, “HIGH-RISK USE”), AND THAT YOU WILL ENSURE THAT, IN THE EVENT OF ANY INTERRUPTION, DEFECT, ERROR, OR OTHER FAILURE OF THE SOFTWARE, THE SAFETY OF PEOPLE, PROPERTY, AND THE ENVIRONMENT ARE NOT REDUCED BELOW A LEVEL THAT IS REASONABLY, APPROPRIATE, AND LEGAL, WHETHER IN GENERAL OR IN A SPECIFIC INDUSTRY. BY ACCESSING THE SOFTWARE, YOU FURTHER ACKNOWLEDGE THAT YOUR HIGH-RISK USE OF THE SOFTWARE IS AT YOUR OWN RISK.\n"
    },
    {
      "name": "sdta25196/good-good-study-day-day-up",
      "stars": 9,
      "img": "https://avatars.githubusercontent.com/u/31121105?s=40&v=4",
      "owner": "sdta25196",
      "repo_name": "good-good-study-day-day-up",
      "description": "好好学习，天天向上",
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2021-12-07T02:30:06Z",
      "updated_at": "2025-04-22T11:48:10Z",
      "topics": [],
      "readme": "# good-good-study-day-day-up\n好好学习，天天向上\n\n## 长期发展\n  \n  **深入**：English + React全家桶 + 程序设计，此为核心竞争力。 rust 属于高阶程序员加分项，发展为第二语言也不错。\n  \n  **关于AI：** 对于绝大多数非AI工程师来说，能够熟练调用API，使用工具就足够了，随着训练资源的逐渐枯竭，AI必然会逐渐更偏向于落地的应用，届时会分位三种人，1：纯AI开发工程师，2：AI模型上层应用\\工具，3：应用\\工具的使用者。\n\n  **休闲**：易、篆刻、紫薇斗数\n\n  **日常**: node + 其他工具源码阅读\n\n\n## 走完该走的路，才能走想走的路\n\n\n\n  \n"
    },
    {
      "name": "Boykai/octo-microhack-rag-ai-and-your-data",
      "stars": 8,
      "img": "https://avatars.githubusercontent.com/u/9115570?s=40&v=4",
      "owner": "Boykai",
      "repo_name": "octo-microhack-rag-ai-and-your-data",
      "description": "Hands-on Microhack created by Microsoft Office of the CTO Americas, focused on RAG AI.",
      "homepage": null,
      "language": "Python",
      "created_at": "2024-11-20T15:38:40Z",
      "updated_at": "2025-04-11T18:24:30Z",
      "topics": [],
      "readme": "<!-- \npage_type: sample\nlanguages:\n- azdeveloper\n- powershell\n- bicep\nproducts:\n- azure\n- azure-openai\n- azure-ai-search\nurlFragment: GPT-RAG\nname: Multi-repo ChatGPT and Enterprise data with Azure OpenAI and AI Search\ndescription: GPT-RAG core is a Retrieval-Augmented Generation pattern running in Azure, using Azure AI Search for retrieval and Azure OpenAI large language models to power ChatGPT-style and Q&A experiences.\n-->\n<!-- YAML front-matter schema: https://review.learn.microsoft.com/en-us/help/contribute/samples/process/onboarding?branch=main#supported-metadata-fields-for-readmemd -->\n![Alt text](docs/Microhack%20OCTO%20logo.png \"Microsoft Americas Office of the CTO\")\n\n# Microhack - RAG AI & Your Data\n## Presented by Microsoft Americas Office of the CTO\nOriginally from [GPT-RAG](https://github.com/Azure/gpt-rag)\n\nThe **RAG pattern** enables businesses to use the reasoning capabilities of LLMs, using their existing models to process and generate responses based on new data. RAG facilitates periodic data updates without the need for fine-tuning, thereby streamlining the integration of LLMs into businesses. \n\nThe **Enterprise RAG** Solution Accelerator (GPT-RAG) offers a robust architecture tailored for enterprise-grade deployment of the RAG pattern. It ensures grounded responses and is built on Zero-trust security and Responsible AI, ensuring availability, scalability, and auditability. Ideal for organizations transitioning from exploration and PoC stages to full-scale production and MVPs.\n\n## Application Components\n\nGPT-RAG follows a modular approach, consisting of three components, each with a specific function.\n\n* **[Data Ingestion](code/gpt-rag-ingestion)** - Optimizes data chunking and indexing for the RAG retrieval step.\n\n* **[Orchestrator](code/gpt-rag-orchestrator)** - Coordinates the flow to retrieve information and generate a user response using Semantic Kernel functions.\n\n* **[App Front-End](code/gpt-rag-frontend)** - Uses the [Backend for Front-End](https://learn.microsoft.com/en-us/azure/architecture/patterns/backends-for-frontends) pattern to provide a scalable and efficient web interface.\n\n<!-- * [Teams-BOT](https://github.com/Azure/gpt-rag-bot) Constructed using Azure BOT Services, this platform enables users to engage with the Orchestrator seamlessly through the Microsoft Teams interface. -->\n\n<!-- \nRemoving temporarily while not finished\n## GPT-RAG Integration HUB\n* [SQL Integration](https://github.com/Azure/gpt-rag-int-sql) Connect the GPT-RAG Infrastructure to SQL using NL2SQL. -->\n\n## Concepts\n\nIf you want to learn more about the RAG Pattern and GPT-RAG architecture.\n\n* [RAG Pattern: What and Why?](docs/RAG_CONCEPTS.md)\n\n* [Solution Architecture Overview](docs/ARCHITECTURE.md)\n\n* [ZeroTrust Architecture Overview](media/GPT-RAG-ZeroTrust.png)\n\n<!-- <a href=\"https://www.youtube.com/watch?v=ICsf4yirieA\"><img src=\"https://img.youtube.com/vi/ICsf4yirieA/0.jpg\" alt=\"Alt text\" width=\"480\"/></a> -->\n\n## Setup Guide\n\n1) **Basic Architecture Deployment:** *for quick demos with no network isolation*⚙️\n\nLearn how to **quickly set up** the basic architecture for scenarios without network isolation. [Click the link to proceed](#getting-started).\n\n\n## Getting Started\n\nThis guide will walk you through the deployment process of Enterprise RAG. Before beginning the deployment, please ensure you have prepared all the necessary tools and services as outlined in the **Pre-requisites** section.\n\n**Pre-requisites**\n\n - Azure Developer CLI: [Download azd](https://learn.microsoft.com/en-us/azure/developer/azure-developer-cli/install-azd?tabs=winget-windows%2Cbrew-mac%2Cscript-linux&pivots=os-windows) \n   - Ensure the correct OS is selected\n - Powershell 7+ with AZ module (Windows only): [Powershell](https://learn.microsoft.com/en-us/powershell/scripting/install/installing-powershell-on-windows?view=powershell-7.4#installing-the-msi-package), [AZ Module](https://learn.microsoft.com/en-us/powershell/azure/what-is-azure-powershell?view=azps-11.6.0#the-az-powershell-module)\n - Git: [Download Git](https://git-scm.com/downloads)\n - Node.js 16+ [windows/mac](https://nodejs.dev/en/download/)  [linux/wsl](https://nodejs.dev/en/download/package-manager/)\n - Python 3.11: [Download Python](https://www.python.org/downloads/release/python-3118/)\n - Initiate an [Azure AI services creation](https://portal.azure.com/#create/Microsoft.CognitiveServicesAllInOne) and agree to the Responsible AI terms **\n   - ** If you have not created an Azure AI service resource in the subscription before\n\n### Basic Architecture Deployment\n\nFor quick demonstrations or proof-of-concept projects without network isolation requirements, you can deploy the accelerator using its basic architecture.\n![Basic Architecture](media/architecture-GPT-RAG-Basic.png)\n\nThe deployment procedure is quite simple, just install the prerequisites mentioned above and follow these four steps using [Azure Developer CLI (azd)](https://aka.ms/azure-dev/install) in a terminal:\n\n\n**1** Download the Repository:\n\n```sh\nazd init\n```\n\n**1.a** Give the environment a unique name. This will be used to create your resources. For example, _cowboy-hats_ would create resource group _rg-cowboy-hats_.\n```sh\nEnter a new environment name: some-name-here\n```\n\n**2** Login to Azure:\n\n**2.a** Azure Developer CLI:\n\n```sh\nazd auth login\n```\n\n**2.b** Azure CLI:\n\n```sh\naz login\n```\n**2.c** Select your Azure Subscription from list.\n\n**3** Start Building the infrastructure and components deployment:\n\n```sh\nazd up\n```\n**3.a** Select your Azure Subscription from list.\n\n**3.b** Select Azure region. _Recommended: East US (eastus)_\n\n**4** Add source documents to object storage from _/datasources_ directory.\n\nUpload your documents to the 'documents' folder located in the storage account. The name of this account should start with 'strag'. This is the default storage account, as shown in the sample image below.\n\n ![storage_sample](media/readme-storage_sample.png)\n\n**Done! Basic deployment is completed.**\n\n**Recommended**: [Add app authentication](https://learn.microsoft.com/en-us/azure/app-service/scenario-secure-app-authentication-app-service). [Watch this quick tutorial](https://youtu.be/sA-an25jMB4) for step-by-step guidance.\n\n## How to?\n\n### Customize Your Deployment\n\nThe standard deployment process sets up Azure resources and deploys the accelerator components with a standard configuration. To tailor the deployment to your specific needs, follow the steps in the [Custom Deployment](docs/CUSTOMIZATIONS.md) section for further customization options.\n\n### Integrate with Additional Data Sources\n  \nExpand your data retrieval capabilities by integrating new data sources such as Bing Custom Search, SQL Server, and Teradata. For detailed instructions, refer to the [AI Integration Hub](docs/AI_INTEGRATION_HUB.md) page.\n\n### Multi-Environment Deployment\n\nOnce you've successfully deployed the GPT-RAG solution as a proof of concept and you're ready to formalize the deployment using a proper CI/CD process to accelerate your deployment to production, refer to the multi-environment deployment guides for either [Azure DevOps](./docs/AZDO-SETUP.md) or [GitHub](./docs/GH-SETUP.md).\n \n### Troubleshoot Deployment Issues\n\nIf you encounter any errors during the deployment process, consult the [Troubleshooting](docs/TROUBLESHOOTING.md) page for guidance on resolving common issues.\n\n### Evaluate Performance\n\nTo assess the performance of your deployment, refer to the [Performance Testing](docs/PERFTEST.md) guide for testing methodologies and best practices.\n\n### Query the Conversation History\n\nLearn how to query and analyze conversation data by following the steps outlined in the [How to Query and Analyze Conversations](docs/QUERYING_CONVERSATIONS.md) document.\n\n### Estimate Pricing\n\nUnderstand the cost implications of your deployment by reviewing the [Pricing Model](https://github.com/Azure/GPT-RAG/wiki/GPT%E2%80%90RAG-%E2%80%90-Pricing-Model) for detailed pricing estimation.\n\n### Manage Governance\n\nEnsure proper governance of your deployment by following the guidelines provided in the [Governance Model](https://share.mindmanager.com/#publish/9ogrdWqzmAzZB6ilgURohV4lj1LriKjOWc0w_u2U).\n\n## Contributing\n\nWe appreciate your interest in contributing to this project! Please refer to the [CONTRIBUTING.md](./CONTRIBUTING.md) page for detailed guidelines on how to contribute, including information about the Contributor License Agreement (CLA), code of conduct, and the process for submitting pull requests.\n\nThank you for your support and contributions!\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n"
    },
    {
      "name": "jbernec/gen-ai-prototypes",
      "stars": 8,
      "img": "https://avatars.githubusercontent.com/u/92888747?s=40&v=4",
      "owner": "jbernec",
      "repo_name": "gen-ai-prototypes",
      "description": "Incremental RAG and agent application PoCs and pipelines, integrating multiple required technologies.",
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2024-07-02T00:44:49Z",
      "updated_at": "2025-04-23T01:39:19Z",
      "topics": [],
      "readme": "### rag-orchestrations\nIncremental RAG and agent application PoCs and pipelines, integrating multiple Azure AI services.\n\n![Semantic chunking in RAG](https://github.com/jbernec/rag-orchestrations/blob/main/images/semantic-chunking.png?raw=true)\n\n![Chainlit UI of Langgraph Agent](https://github.com/jbernec/gen-ai-prototypes/blob/main/images/chainlit_ui_langgraph_agent.png?raw=true)\n"
    },
    {
      "name": "IntelliTect/EssentialCSharp.Web",
      "stars": 8,
      "img": "https://avatars.githubusercontent.com/u/793568?s=40&v=4",
      "owner": "IntelliTect",
      "repo_name": "EssentialCSharp.Web",
      "description": "The web side of essentialcsharp.com",
      "homepage": "https://essentialcsharp.com/",
      "language": "HTML",
      "created_at": "2023-02-22T01:22:31Z",
      "updated_at": "2025-04-17T17:17:00Z",
      "topics": [
        "essentialcsharp",
        "hacktoberfest"
      ],
      "readme": "# Essential C# Web Project\n\n## Projects Overview\n\n- [EssentialCSharp.Web](https://github.com/IntelliTect/EssentialCSharp.Web/tree/main/EssentialCSharp.Web) - The site seen at [essentialcsharp.com](https://essentialcsharp.com/)\n\nFor any bugs, questions, or anything else with specifically the code found inside the listings (listing examples code), please submit an issue at the [EssentialCSharp Repo](https://github.com/IntelliTect/EssentialCSharp).\n\n## What You Will Need\n\n- [Visual Studio](https://visualstudio.microsoft.com/) (or your preferred IDE)\n- [.NET 8.0 SDK](https://dotnet.microsoft.com/download)\n  - If you already have .NET installed you can check the version by typing `dotnet --info` into cmd to make sure you have the right version\n\n## Startup Steps\n\nTo get the site that is seen at [essentialcsharp.com](https://essentialcsharp.com/):\n\n1. Clone Repository locally.\n2. Set any needed secrets\n3. If you have do not have access to the private nuget feed, change the line `<AccessToNugetFeed>true</AccessToNugetFeed>` to `<AccessToNugetFeed>false</AccessToNugetFeed>` in [Directory.Packages.props](https://github.com/IntelliTect/EssentialCSharp.Web/blob/main/Directory.Packages.props).\n\n## Environment Prequisites\n\nMake sure the following secrets are set:\nIn local development this ideally should be done using the dotnet secret manager. Additional information can be found at the [documentation](https://learn.microsoft.com/en-us/aspnet/core/security/app-secrets#set-a-secret)\n\nAuthMessageSender:SendFromName = \"Hello World Team\"\nAuthMessageSender:SendFromEmail = \"no-reply@email.com\"\nAuthMessageSender:SecretKey = alongstringofsecretsauce\nAuthMessageSender:APIKey = anapikey\nAuthentication:Microsoft:ClientSecret = anotherimportantsecret\nAuthentication:Microsoft:ClientId = anotherimportantclient\nAuthentication:github:clientSecret = anotherimportantclientsecret\nAuthentication:github:clientId = anotherimportantclientid\nHCaptcha:SiteKey = captchaSiteKey\nHCaptcha:SecretKey = captchaSecretKey\n\nTesting Secret Values:\nSome Value Secrets for Testing/Development Purposes:\nHCaptcha: https://docs.hcaptcha.com/#integration-testing-test-keys\n\nPlease use issues or discussions to report issues found.\n"
    },
    {
      "name": "rashedtalukder/sharepoint-azure-openai-rag",
      "stars": 8,
      "img": "https://avatars.githubusercontent.com/u/9218468?s=40&v=4",
      "owner": "rashedtalukder",
      "repo_name": "sharepoint-azure-openai-rag",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2024-01-17T17:06:46Z",
      "updated_at": "2025-04-07T08:21:33Z",
      "topics": [],
      "readme": "# Bring Your SharePoint Online Data in Azure OpenAI (RAG Application)\n\nThis repository shows how to generate vector embeddings from SharePoint Online site's documents (PDF or Word), store them in search indexes in Azure AI Search, and use the results to formulate a response in the Azure OpenAI chat completions.\n\nThis is a manual step-by-step guide and does not have production automation built in. This is meant to help understand the process of building a Retreival Augement Generate (RAG) app on SharePoint/Teams uploaded files instead of being deployment ready.\n\n## Personas\n\nIn order to use this repo, it is split into two separate personas to complete the tasks required. View the corresponding folder for your role:\n- [Application developer/data scientist](app_dev/README.md)\n- [SharePoint administrator](sharepoint_admin/README.md)\n\n## Known limitations\n\nCurrently this approach does not allow the following:\n- <b>Folder and/or file filters</b>: This approach does not limit indexing to specific documents or folders within a SharePoint site. In the future, we will look into adding support for [SharePoint embedded](https://learn.microsoft.com/en-us/sharepoint/dev/embedded/overview) which offers this level of access granularity.\n\n## Credit\nPart of this repository is build on top of the work and insights found in https://github.com/liamca/sharepoint-indexing-azure-cognitive-search\n"
    },
    {
      "name": "alkampfergit/SemanticKernelPlayground",
      "stars": 8,
      "img": "https://avatars.githubusercontent.com/u/358545?s=40&v=4",
      "owner": "alkampfergit",
      "repo_name": "SemanticKernelPlayground",
      "description": null,
      "homepage": null,
      "language": "C#",
      "created_at": "2023-11-25T07:57:01Z",
      "updated_at": "2024-09-04T08:29:03Z",
      "topics": [],
      "readme": "# Semantic Kernel Experiments\n\nEverything that is done with python uses the same virtual env\n\n## Documentation\n\nNothing is better than code, here is the [official repo by Microsoft with examples](https://github.com/MicrosoftDocs/semantic-kernel-docs/blob/main/samples/python) \n\nThe textual documentation can be found here: https://learn.microsoft.com/en-us/semantic-kernel/ai-orchestration/kernel/adding-services?tabs=python\n\n## Installation\n\nCreate a local environment with python, then allow the ipykernel to create a kernel for jupyter notebooks.\n\n```bash\npython3 -m venv skernel\nsource skernel/bin/activate\n# For windows you must use the following command to activate the virtual environment\n#  .\\skernel\\Scripts\\activate \n```\n\nyou can handle requirements with easy thanks to pip\n\n```bash\npip install -r requirements.txt\npip freeze > requirements.txt\n```\n\nThen you can create a kernel for jupyter notebooks using the very same environmnent\n\n```bash\npip install ipykernel\npython -m ipykernel install --user --name=skernel_experiments\n```\n\nKernel can be removed using \n\n```bash\njupyter kernelspec remove skernel_experiments\n```\n\n"
    },
    {
      "name": "briancabbott/ChatNow",
      "stars": 8,
      "img": "https://avatars.githubusercontent.com/u/2147648?s=40&v=4",
      "owner": "briancabbott",
      "repo_name": "ChatNow",
      "description": "ChatNow LLM, Prompt Engineering, Etc. ",
      "homepage": null,
      "language": null,
      "created_at": "2023-03-23T20:40:31Z",
      "updated_at": "2023-06-06T18:58:43Z",
      "topics": [],
      "readme": "# ChatNow\nChatNow LLM, Prompt Engineering, Etc. \n\n## Overview\n\n### Objectives\n   - Assess the totality of the path to the current SoTA in transformer and prompt based systems (GPT-4, ChatGPT, etc)\n      - Identify the primary set of models and associated papers that produce the path\n      - Decide on any secondary - weather it is necessary or not to have a full conversational comprehension of SoTA \n        weather or not secondaries are required (i.e. ADAM)\n      - Capture as a conversational/presented concept space (i.e. PPTX)\n      - Capture remedial implementations of basic sketch concepts from core papers (i.e. Python) \n   - Demonstrate expertise in HuggingFace as a platform (full spectrum of Transformer based operations within the \n     HF Ecosystem (online Hub models, etc))\n   - Demonstrate expertise in adjacent concepts:\n      - Flan T5 models \n      - Prompt Engineering\n      - Anthropic RLHF techniques\n   - Demonstrate expertise in \"understanding as implementation\" as well as \"implementation as monetary production\" systems\n      - Naive\n      - Optimized\n         - PyTorch\n         - TensorFlow\n         - Jax\n      - Consider training cost analysis, Utilize benchmarking (GLUE, etc)\n   - Bring all models, technologies, etc together in a final demonstration-goal project \n      - TODO: Need to come up with an idea of this\n         - it should be something that organically comes from HF/Transformers/current SoTA \n\n### Project Layout\n   - General-Code\n   - Papers-and-Models\n   - Training-Sets\n\n\n## Hugging-Face Platform\n\n### General ML/DL Techniques\n   - Transfer Learning\n   \n### Natural Language Processing\n   - Text Classification\n   - Token Classification\n   - Table Question Answering\n   - Question Answering\n   - Zero-Shot Classification\n   - Translation\n   - Summarization\n   - Conversational\n   - Text Generation\n   - Text2Text Generation\n   - Fill-Mask\n   - Sentence Similarity\n"
    },
    {
      "name": "Azure/azure-ai-agents-labs",
      "stars": 7,
      "img": "https://avatars.githubusercontent.com/u/6844498?s=40&v=4",
      "owner": "Azure",
      "repo_name": "azure-ai-agents-labs",
      "description": "This repo contains hands-on labs for building AI Agents using the Azure AI Agent Service SDK and Semantic Kernel. The Azure AI Agent Service is used to create AI agents and Semantic Kernel is used to orchestrate the agents in a multi-agent system.",
      "homepage": "",
      "language": "Jupyter Notebook",
      "created_at": "2025-03-18T14:21:17Z",
      "updated_at": "2025-03-31T21:12:30Z",
      "topics": [],
      "readme": "# Hands-On Labs for AI Agents Using Azure AI Agent Service SDK and Semantic Kernel\n\nThis repo contains hands-on labs for building AI Agents using the Azure AI Agent Service SDK and Semantic Kernel. The Azure AI Agent Service is used to create AI agents and Semantic Kernel is used to orchestrate the agents in a multi-agent system. \n\n## Prerequisites\n**Microsoft will provide the lab environment with all the prerequisites**\n* Azure subscription\n* Azure AI Foundry resource with an AI Hub and AI Project (You will set this up in Lab 1)\n* Visual Studio Code\n* Python 3.10>\n* Azure CLI\n* Azure CLI Azure ML extension\n\n## Labs\n\n### Lab 1 - Setup and test the lab environment\nLab 1 walks you through how to setup the necessary lab environment for building AI Agents. This includes:\n* Setting up the AI Project in the Azure AI Foundry\n* Deploying an LLM and embedding models\n* Establish connectivity from VS Code to the AI Project\n* Perform a simple Chat Completion call to the LLM to test that your lab environment is set up properly. \n\n### Lab 2 - Build a simple AI agent\nLab 2 introduces you to AI agents in Azure. You will learn how to build a simple AI agent that generates a bar chart comparing costs between health insurance plans.\n\n### Lab 3 - Build a RAG Agent\nIn Lab 3, you will be building an AI Agent that will perform Retrieval Augmented Generaton (RAG) on health plan documents. Azure AI Search will be used as the vector database for storing the embeddings for the health plan documents.\n\n### Lab 4 - Develop a multi-agent system\nIn Lab 4, you will be creating a multi-agent system consisting of 4 agents working together to generate reports about health plan documents. You will build these 4 AI Agents:\n1. Search Agent - This agent will search an Azure AI Search index for information about specific health plan policies.\n2. Report Agent - This agent will generate a detailed report about the health plan policy based on the information returned from the Search Agent.\n3. Validation Agent - This agent will validate that the generated report meets specified requirements. In our case, making sure that the report contains information about coverage exclusions.\n4. Orchestrator Agent - This agent will act as an orchestrator that manages the communication between the Search Agent, Report Agent, and Validation Agent.\n"
    },
    {
      "name": "ShivamGoyal03/RoamMind",
      "stars": 7,
      "img": "https://avatars.githubusercontent.com/u/93383164?s=40&v=4",
      "owner": "ShivamGoyal03",
      "repo_name": "RoamMind",
      "description": "Travel AI Assistant",
      "homepage": "",
      "language": "Python",
      "created_at": "2025-02-02T12:45:54Z",
      "updated_at": "2025-04-11T21:28:06Z",
      "topics": [],
      "readme": "# Travel AI Assistant\n\n## Key Features\n- **Conversational Interface:** Engage in natural language conversations for travel planning.\n- **Multi-Domain Support:** Supports flight, hotel, restaurant, and excursion recommendations.\n- **Semantic Kernel Integration:** Leverages advanced AI to process user input and generate precise responses.\n- **Scalable Architecture:** Modular design with clearly separated layers for API, business logic, and infrastructure.\n- **Custom Exception Handling:** Robust error handling for seamless user experience.\n\n## Core Capabilities\n- **User Input Processing:** Processes travel-related queries and extracts parameters.\n- **Conversation Management:** Maintains conversation state through an orchestrator.\n- **Information Extraction:** Uses dedicated skills for extracting flight, hotel, restaurant, and excursion details.\n- **Data Enhancement:** Enhances search results and detailed information with LLM responses powered by Azure OpenAI.\n- **Dependency Injection:** Utilizes dependency modules for orchestrator and infrastructure components ensuring loose coupling.\n\n## Technical Features\n- **FastAPI:** Provides a high-performance API for handling requests.\n- **Azure OpenAI:** Integrates with Azure OpenAI for advanced language processing.\n- **Cosmos DB Integration:** Uses Cosmos DB for repository and data storage purposes.\n- **Modular Codebase:** Structured into multiple directories including `api`, `core`, `infrastructure`, `models`, `skills`, and `utils`.\n- **Logging & Validation:** Built-in logging and validation utilities for efficient debugging and error handling.\n\n## Project Structure\n```\nsrc\n├── exceptions.py\n├── run.py\n├── chat_interface.py\n├── __init__.py\n├── api\n│   ├── dependencies.py\n│   ├── main.py\n│   └── __init__.py\n├── core\n│   ├── config.py\n│   ├── models.py\n│   ├── orchestrator.py\n│   └── __init__.py\n├── infrastructure\n│   ├── azure_openai.py\n│   ├── cosmos_repository.py\n│   └── __init__.py\n├── models\n│   ├── excursion.py\n│   ├── flight.py\n│   ├── hotel.py\n│   ├── restaurant.py\n│   ├── user.py\n│   └── __init__.py\n├── skills\n│   ├── excursion_skill.py\n│   ├── flight_skill.py\n│   ├── hotel_skill.py\n│   ├── restaurant_skill.py\n│   └── __init__.py\n├── utils\n│   ├── currency.py\n│   ├── date_helper.py\n|   ├── location_mapper.py\n│   ├── logger.py\n│   ├── validation.py\n│   └── __init__.py\n```\n\n## Installation\n1. **Clone the repository:**\n   ```bash\n   git clone https://github.com/ShivamGoyal03/RoamMind.git\n   cd RoamMind/src\n   ```\n\n2. **Create a virtual environment:**\n   ```bash\n   python -m venv env\n   source env/bin/activate  # On Windows: env\\Scripts\\activate\n   ```\n\n3. **Install dependencies:**\n   ```bash\n   pip install -r requirements.txt\n   ```\n\n## Content Owners\n\n<!-- ALL-CONTRIBUTORS-LIST:START - Do not remove or modify this section -->\n\n<table>\n<tr>\n    <td align=\"center\"><a href=\"https://github.com/ShivamGoyal03\">\n        <img src=\"https://github.com/ShivamGoyal03.png\" width=\"100px;\" alt=\"Shivam Goyal\"/><br />\n        <sub><b>Shivam Goyal</b></sub>\n    </a><br />\n    </td>\n</tr></table>\n\n## Usage\n\n### Running the Application\n\nStart both FastAPI and Chainlit servers with a single command:\n```bash\npython src/run.py\n```\n\nThis will start:\n- FastAPI backend at `http://127.0.0.1:8000`\n- Chainlit UI at `http://127.0.0.1:8501`\n- API docs at `http://127.0.0.1:8000/api/docs`\n\n## Configuration\nRoamMind is configured via environment variables and the `src/core/config.py` file. Key configuration parameters include:\n- `AZURE_OPENAI_ENDPOINT`: The endpoint URL for Azure OpenAI.\n- `AZURE_OPENAI_API_VERSION`: The API version for Azure OpenAI.\n- `AZURE_OPENAI_API_KEY`: Your Azure OpenAI API key.\n- Cosmos DB connection details (if applicable).\n\n\n\n## Additional Information\n- **Logging:** A custom logger is implemented in `src/utils/logger.py` to track application activity.\n- **Error Handling:** Custom exceptions are defined in `src/exceptions.py` for precise error management.\n- **Skills Integration:** The `src/skills` directory contains skills for handling domain-specific tasks like flight search, hotel booking, restaurant recommendations, and excursion planning.\n\n## Development\n\n### Running Servers Separately\nFor development, you can run the servers separately:\n\n1. **FastAPI:**\n   ```bash\n   uvicorn src.api.main:app --host 127.0.0.1 --port 8000 --reload\n   ```\n\n2. **Chainlit:**\n   ```bash\n   chainlit run src/chat_interface.py --port 8501\n   ```\n\n>  [!NOTE]\n> RoamMind is a fictional project created solely as a template and does not represent a real product.\n> It serves as the core logic for building AI agent instructions and is not fully production-ready. Contributions are welcome if you want to enhance it. \n> This template provides a foundational framework for a Travel AI Assistant, which you can customize to suit your requirements.\n\n---\nFor more information, check out the resources:\n- [AI Agents For Beginner Course](https://github.com/microsoft/ai-agents-for-beginners/)\n- [Getting Started with Azure AI Studio](https://techcommunity.microsoft.com/blog/educatordeveloperblog/getting-started-with-azure-ai-studio/4095602?wt.mc_id=studentamb_258691)\n- [Fundamentals of AI agents on Azure](https://learn.microsoft.com/en-us/training/modules/ai-agent-fundamentals/?wt.mc_id=studentamb_258691)\n- [Azure AI Discord](https://aka.ms/AzureAI/Discord)"
    },
    {
      "name": "microsoft/Hygeia",
      "stars": 7,
      "img": "https://avatars.githubusercontent.com/u/6154722?s=40&v=4",
      "owner": "microsoft",
      "repo_name": "Hygeia",
      "description": "Chat with documentation endpoint, with session context, in Azure",
      "homepage": "",
      "language": "Python",
      "created_at": "2024-06-03T13:18:52Z",
      "updated_at": "2025-04-04T01:58:04Z",
      "topics": [],
      "readme": "# Generative Answers with RAG Endpoint\n\nThe purpose of this project is to provide templated example WebAPI endpoint that uses Azure OpenAI to answer user questions and provide instructions based on a set of internal documents with Retrievel Augmented Generation (RAG).\n\nThis template utilizes the following open source AI orchestration layers:\n* Semantic Kernel SDK\n    * [GitHub repo](https://github.com/microsoft/semantic-kernel)\n    * [Microsoft Learn documentation](https://learn.microsoft.com/en-us/semantic-kernel/overview)\n* [Kernel Memory Service](https://github.com/microsoft/kernel-memory)\n\n## Architecture\n\n![High Level Architecture](./docs/arch-high%20level%20arch%20v2.png)\n\n## Components\n\nThis project repository includes the following components:\n\n| Component | Description |\n|---|---|\n| [function_app.py](./function_app.py) | Defines the Azure Function and components (includes `http_function.py` and `blob_trigger.py`)|\n| [http_function.py](./http_function.py) | A function defining an `/ask` API route, and orchestration logic behind it to perform a RAG pattern generative answer to a request |\n| [blob_trigger.py](./blob_trigger.py) | A function triggered by blobs added to an `upload` container, which processes the blob into AI Search through the Kernel Memory pipeline |\n| [Kernel Memory Plugin](./plugins/kernel_memory_plugin.py) | Plugin registered with the Semantic Kernel orchestration layer to interface with the Kernel Memory service `/ask`, `/search`, `/upload` and `/delete` endpoints. |\n| [test web client app](./app) | A simple web front end for testing the API endpoint (not a chat interface) |\n| [test.http](./test.http) | A test file for use with a `REST Client` extension |\n| [deployment templates](./infra) | Bicep templates for deploying APIM configurations and policies for gateway to Azure OpenAI\n\n## Pre-Requisites\nThe Kernel Memory service, not included in this project repository, is required. Follow deployment instructions from the below open source GitHub project.\n\n> [Kernel Memory](https://github.com/microsoft/kernel-memory)<br>This repository presents best practices and a reference architecture for memory in specific AI and LLMs application scenarios\n\nThe Kernel Memory deployment includes fully configured:\n* Kernel Memory Service (Azure Container App)\n* Azure AI Search Service\n* Azure OpenAI\n* Azure Storage Account\n\n## Run and Test Locally\nThis project requires the following to be installed on your development environment:\n* [python version 3.11](https://www.python.org/downloads/release/python-3118/)\n* [Azure Function Core Tools](https://learn.microsoft.com/en-us/azure/azure-functions/functions-run-local?tabs=windows%2Cisolated-process%2Cnode-v4%2Cpython-v2%2Chttp-trigger%2Ccontainer-apps&pivots=programming-language-python#install-the-azure-functions-core-tools)\n* [Azurite Extension](https://learn.microsoft.com/en-us/azure/storage/common/storage-use-azurite?tabs=visual-studio%2Cblob-storage)\n\n1. Clone and open this repo in your IDE (these instructions are for VSCode)\n2. Create a `local.settings.json` file in the root directory with the values in the section below\n3. Build dev environment, either a virtual environment or dev container:\n    * Virtual Environment: \n        * Command Pallet > `Python: Create Environment`\n        * select Python 3.11 interpreter\n        * install `requirements.txt`\n        * Command Pallet > `Azurite: Start`\n    * Dev Container:\n        * Command Pallet > `Dev Containers: Reopen in Container`\n        * included dev container definition will be built and run\n4. In the Run pane, start debugging with `Attach to Python Functions`\n\n## Environment Settings\nUse the following for a `local.settings.json` file, or to build Environment Variables for your deployed Azure Function:\n\n```JSON\n\n## Local Configuration\nTo run the function app locally, create a local.settings.json file with the following example and update with your own values:\n``` json\n{\n    \"IsEncrypted\": false,\n    \"Host\": {\n      \"CORS\": \"*\"\n    },\n    \"Values\": {\n      \"AzureWebJobsStorage\": \"UseDevelopmentStorage=true\",\n      \"FUNCTIONS_WORKER_RUNTIME\": \"python\",\n      \"AzureWebJobsFeatureFlags\": \"EnableWorkerIndexing\",\n      \"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\": \"\",\n      \"AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME\": \"\",\n      \"AZURE_OPENAI_ENDPOINT\": \"\",\n      \"KERNEL_MEMORY_SERVICE_ENDPOINT\": \"\",\n      \"KERNEL_MEMORY_SERVICE_INDEX\": \"default\",\n      \"STORAGE_CONNECTION_STRING\": \"\",\n      \"AZUREAD_TENANT_ID\": \"\",\n      \"AZUREAI_SEARCH_ADMIN_KEY\": \"\",\n      \"AZUREAI_SEARCH_ENDPOINT\": \"\"\n    }\n  }\n```\n| Setting | Description |\n|---|---|\n| AZURE_OPENAI_CHAT_DEPLOYMENT_NAME | The name of your Azure OpenAI chat model deployment |\n| AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME | The name of your Azure OpenAI embeddings model deployment |\n| AZURE_OPENAI_ENDPOINT | Endpoint URL of your Azure OpenAI resource, or your APIM gateway for Azure OpenAI |\n| KERNEL_MEMORY_SERVICE_ENDPOINT | Endpoint URL of the Kernel Memory service container app |\n| KERNEL_MEMORY_SERVICE_INDEX | The name of the index used for upload and search of documents. |\n| STORAGE_CONNECTION_STRING | For the storage account used for uploading documents via the `blob_trigger` function. We recommend using the storage account deployed with the Kernel Memory service |\n| AZUREAI_SEARCH_ADMIN_KEY | Admin Key of the Search Service deployed with the Kernel Memory service, used for Chat History |\n| AZUREAI_SEARCH_ENDPOINT | Endpoing URL of the Search Service deployed with the Kernel Memory service, used for the Chat History |\n\n## Deploy to Azure\n\n### API Management Gateway\nBicep templates are provided to deploy and configure API Management as a gateway to manage scaling and resiliency of calls into your Azure OpenAI model deployments. See [APIM Infra Instructions](./infra/README.md) for deploying APIM to your Azure subscription.\n\n### Function Deployment\nTo deploy the code to an Function App in your Azure subscription from VSCode:\n\n1. Install the [Azure Resources](https://github.com/microsoft/vscode-azureresourcegroups) and [Azure Functions](https://github.com/microsoft/vscode-azurefunctions) VSCode extensions if not already.\n2. In the `Azure ` panel of VSCode, sign into your Azure subscription\n3. In the `WORKSPACE` tree, click the Azure Function icon at the top and select `Deploy to Azure` \n4. In the command pallet workflow, select an existing function or create new, using `Python 3.11`\n5. Once deployment is complete, in the `Resources` tree, browse into your subscription and the `Function App` node to find your new function. \n6. Expand the function, right-click on `Application Settings`, and select `Upload Local Settings`\n  > NOTE: Ensure your `local.settings.json` file is properly created and configured as defined above first\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n"
    },
    {
      "name": "wunderwuzzi23/wuzzi-chat",
      "stars": 7,
      "img": "https://avatars.githubusercontent.com/u/35349594?s=40&v=4",
      "owner": "wunderwuzzi23",
      "repo_name": "wuzzi-chat",
      "description": "Simple Chatbot for testing AI Red Team tooling",
      "homepage": null,
      "language": "Python",
      "created_at": "2024-05-19T16:57:59Z",
      "updated_at": "2025-02-25T23:53:37Z",
      "topics": [],
      "readme": "## wuzzi-chat\n\nThis is a simple Chat UI that can talk to `groq` and `openai` chat completion APIs.\n\nThe main purpose is to demonstrate and test red team tools for chat bots and LLM applications.\n\n![wuzzi chat ui](ui.png)\n\nwuzz-chat currently supports `OpenAI`, `groq` and locally run `Ollama` models. \n\nYou can choose which ones to use, one or all three of them. \n\n## OpenAI and groq API keys\n\nIf you want to use hosted services like OpenAI or groq, you need API keys.\n\nYou can get those from:\n\n1. **OpenAI:**  https://platform.openai.com \n2. **groq:**    https://console.groq.com/ \n\nThen set the API keys in your terminal, e.g.:\n\n```bash\nexport OPENAI_API_KEY=<your_api_key>\nexport GROQ_API_KEY=<your_api_key>\n```\n\n## Using a local Ollama model\n\nOllama runs entirely locally. Follow installation instructions on [Ollama website](https://ollama.com/)\nIf you want to use docker and CPU only these commands will get you going with a small phi3 model.\n\n```\ndocker pull ollama/ollama\ndocker run -d -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama\ndocker exec -it ollama ollama run phi3:3.8b\n```\n\nWhen you visit `http://localhost:11434/` you'll see \"Ollama is running\".\n\nIn the `.env` file you can specify the model, like `MODEL=phi3:3.8b`.\n\n```\nOLLAMA_ENDPOINT=http://localhost:11434/\nOLLAMA_MODEL=phi3:3.8b\n```\n\n## Running the web server\n\nInstall the required dependencies and then run the web server:\n\n```python\npython chat.py\n```\n\nIt will listen on http://127.0.0.1:5000 by default.\n\n\n### Configuration settings in .env file\n\nIf you do not have a `.env` file the server will create one for you upon startup and ask for required values:\n\n`CHATUI_API_KEY`: The token the client application has to send to communicate with wuzzi-chat. \n\nThis is how a typical `.env` file will look like:\n\n```\nCHATUI_API_KEY=ThisIsMyTestKey1234\nGROQ_MODEL=llama3-8b-8192\nOPENAI_MODEL=gpt-4o\nOLLAMA_ENDPOINT=http://localhost:11434/\nOLLAMA_MODEL=phi3:3.8b\n```\n\n## Client Configuration\n\nWhen you use the Chat app you need to the set the UI key and model in the browser, otherwise you'll see a *UNAUTHORIZED* message.\nGo and click \"SETTINGS\" on the bottom right of the Chat UI to set the required values.\n\nThat's all.\n"
    },
    {
      "name": "ROBROICH/intro-to-intelligent-apps",
      "stars": 7,
      "img": "https://avatars.githubusercontent.com/u/51105958?s=40&v=4",
      "owner": "ROBROICH",
      "repo_name": "intro-to-intelligent-apps",
      "description": null,
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2024-03-01T06:43:50Z",
      "updated_at": "2024-04-29T01:56:12Z",
      "topics": [],
      "readme": "# Introduction to Building AI Apps\n\nThis repository introduces and helps organizations get started with building AI Apps and incorporating Large Language Models (LLMs) into them.\n\n## Workshop Agenda\n\nThe objective of this workshop is to practice realistic AI orchestration scenarios and to learn how to build intelligent apps.\nAt the end of the workshop you will: \n* Know how to use prompt engineering techniques for effective generative AI responses on OpenAI\n* Understand the implications of the usage of tokens and embeddings when interacting with an LLM\n* Have experience in leveraging AI orchestrators like Langchain/ Semantic Kernel with Azure OpenAI\n* Have evaluated different vector stores like Qdrant or Azure AI Search to enhance LLM responses with your data and context\n* Know how to turn a business scenario with data, context and user input into an intelligent application on Azure\n\n### 🌅 Morning (9:00 – 12:15)\n\n> *Focus: Introduction, First Steps & Prompt Engineering*\n* 📣 [Intro to Azure OpenAI, Prompt Engineering & Demos (105min)](presentations/README.md)\n  * Azure OpenAI Service\n  * Demo(s)\n  * Break\n  * 🧑🏼‍💻 [Lab #1 - Hands-on with Prompt Engineering Exercises](labs/01-prompts/README.md)\n* 📣 [Intro to AI Orchestration (60min)](presentations/README.md)\n  * AI Orchestration\n  * Demo(s)\n\n### 🌆 Afternoon (1:15 – 4:30)\n\n> *Focus: Building AI Apps & Incorporating LLMs*\n\n* 📣 [Intro to AI Orchestration Continued (135min)](presentations/README.md)\n  * 💻 [Lab #2 - Hands-on with Integrating AI Exercises](labs/02-integrating-ai/README.md)\n  * 💻 [Lab #3 - Hands-on with AI Orchestration Exercises](labs/03-orchestration/README.md)\n  * 💻 [Lab #4 - Hands-on with Deploying AI Exercises](labs/04-deploy-ai/README.md)\n  * Break\n* Wrapping-up (60min)\n  * Use Case Validation\n  * QnA & Closing Remarks\n\n\n## Getting Started with Workshop Preparation\n\nThe steps in this section will take you through setting up Azure OpenAI and some configuration files so that you can complete all of the hands-on labs successfully.\n\n* [Preparation](labs/00-setup/README.md)\n\n## Post Workshop Next Steps\n\nWhen you're done with this workshop and ready to move on, the following may be useful.\n\n* [Next Steps](docs/next_steps.md)\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n"
    },
    {
      "name": "kimtth/azure-openai-llm-cookbook",
      "stars": 6,
      "img": "https://avatars.githubusercontent.com/u/13846660?s=40&v=4",
      "owner": "kimtth",
      "repo_name": "azure-openai-llm-cookbook",
      "description": "🫧 A one-stop hub, like a sample library 🪂 Azure OpenAI 100+ Sample Code 🧪 Organized by topic for quick reference. (Updated regularly)",
      "homepage": "",
      "language": "Jupyter Notebook",
      "created_at": "2025-03-05T08:29:56Z",
      "updated_at": "2025-04-23T09:40:09Z",
      "topics": [
        "agent",
        "azure",
        "azure-openai",
        "chatgpt",
        "cookbook",
        "hub",
        "langgraph",
        "library",
        "llama-index",
        "llm",
        "openai",
        "rag"
      ],
      "readme": "# Azure OpenAI LLM Cookbook\n\n![Static Badge](https://img.shields.io/badge/llm-azure_openai-blue?style=flat-square) ![GitHub Created At](https://img.shields.io/github/created-at/kimtth/azure-openai-samples?style=flat-square)\n\n## 📌 Quick Reference: Curated Sample Collection\n\n`A one-stop hub, like a sample library.` This repository is organized by topic to help reduce the time spent searching for and reviewing sample code. It offers a curated collection of minimal implementations and sample code from various sources.\n\n> [!IMPORTANT]\n> 🔹For more details and the latest code updates, please refer to the original link provided in the `README.app.md` file within each directory.  \n> 🔹Disclaimer: Some examples are created for OpenAI-based APIs. \n\n💡[How to switch between OpenAI and Azure OpenAI endpoints with Python](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/switching-endpoint)\n\n- Programming Languages\n    - Python:🐍 \n    - Jupyter Notebook:📔\n    - JavaScript/TypeScript:🟦\n    - Extra:🔴 \n- Status & Action\n    - Created:✨ (A unique example found only in this repository)\n    - Modified:🎡 (An example that has been modified from a referenced source)\n    - Copied:🧲 (When created or modified emojis are not following) \n    - See the details at the URL:🔗\n- Microsoft libraries or products:🪟\n\n⭐ If you find this repository useful, please consider giving it a star!\n\n## 📖 Repository structure\n\n## 📁 agent\n- a2a_semantic_kernel🐍✨🔗🪟: Agent2Agent (A2A) Protocol Implementation with Semantic Kernel\n- a2a_server_client🐍: Agent2Agent (A2A) Protocol - official implementation of Server/Client\n- agent_multi-agent_pattern📔🪟: Agent multi-agent pattern\n- agent_planning_pattern📔🪟: Agent planning pattern\n- agent_react_pattern📔: Agent react pattern\n- agent_reflection_pattern📔: Agent reflection pattern with LangGraph\n- agent_reflection_pattern📔: Agent reflection pattern\n- agent_tool_use_pattern📔🪟: Agent tool use pattern\n- arxiv_agent🐍✨🎡: ArXiv agent\n- chess_agent🐍: Chess agent\n- multi_agentic_system_simulator🐍✨🔗: A Multi-Agentic System Simulator. Visualize Agent interactions.\n- role_playing📔: Role-playing\n- web_scrap_agent🐍✨🎡: Web scraping agent\n- x-ref: [📁industry](#-industry) \n\n## 📁 azure\n- azure_ai_foundry_sft_finetuning📔🪟: Supervised Fine-tuning\n- azure_ai_foundry_workshop📔🪟: Azure AI Foundry Workshop\n- azure_ai_search📔🪟: Chunking, Document Processing, Evaluation\n- azure_bot📔🪟: Bot Service API\n- azure_cosmos_db📔🪟: Cosmos DB as a Vector Database\n- azure_cosmos_db_enn🐍✨🪟: Cosmos DB Exact Nearest Neighbor (ENN) Vector Search for Precise Retrieval\n- azure_devops_(project_status_report)🐍✨🪟: Azure DevOps – Project Status Report\n- azure_document_intelligence🐍🪟: Azure Document Intelligence\n- azure_evaluation_sdk🐍🪟: Azure Evaluation SDK\n- azure_machine_learning📔🪟: Azure Machine Learning\n- azure_postgres_db📔🪟: pgvector for Vector Database\n- azure_sql_db📔🪟: Azure SQL as a Vector Database\n- copilot_studio🔗🪟: A low-code platform for bots and agents (formerly Power Virtual Agents)\n- m365_agents_sdk🟦🪟: Rebranding of Azure Bot Framework\n- sentinel_openai🔗🪟: Sentinel – Security Information and Event Management (SIEM)\n- sharepoint_azure_function📔🪟: SharePoint Integration with Azure Functions\n- teams_ai_sdk🔗🪟: Teams AI SDK\n\n## 📁 cookbook\n\n- anthropic: [Anthropic Cookbook](https://github.com/anthropics/anthropic-cookbook)\n- gemini: [Gemini API Cookbook](https://github.com/google-gemini/cookbook)\n- openai: [OpenAI Cookbook](https://github.com/openai/openai-cookbook)\n\n## 📁 data\n- azure_oai_usage_stats_(power_bi)🔴🪟: Azure OpenAI usage stats using Power BI\n- azure_ocr_scan_doc_to_table🐍✨🪟: Azure Document Intelligence – Extract tables from document images and convert them to Excel\n- chain-of-thought🐍🔴: Chain-of-thought reasoning prompt\n- fabric_cosmosdb_chat_analytics📔🔴✨(visual)🪟: [Fabric](https://learn.microsoft.com/en-us/fabric/): Data processing, ingestion, transformation, and reporting on a single platform\n- firecrawl_(crawling)🐍: Firecrawl – Web crawling and scraping\n- ms_graph_api📔🪟: Microsoft Graph API\n- presidio_(redaction)📔🪟: Presidio – Data redaction and anonymization\n- prompt_buddy_(power_app)🔴🪟: Prompt sharing application built on Power App\n- prompt_leaked🔴: Prompt leakage detection and analysis\n- sammo_(prompt_opt)📔🪟: A Structure-Aware Approach to Efficient Compile-Time Prompt Optimization\n- semantic_chunking_(rag)📔: Semantic chunking for Retrieval-Augmented Generation (RAG)\n\n## 📁 dev\n- code_editor_(vscode)🐍✨🔗🪟: Visual Studio Code extension development\n- diagram_to_infra_template_(bicep)🐍✨🪟: Bicep – Infrastructure as Code (IaC) language\n- e2e_testing_agent📔🪟: End-to-end testing with Playwright automation framework\n- git_repo_with_chat🐍✨: Chat with Github repository\n- gui_automation🔗🪟: Omni Parser – Screen parsing tool / Windows Agent Arena (WAA)\n- llm_router🐍✨🎡: LLM request routing and orchestration\n- mcp_(model_context_protocol)🐍✨🔗: Model Context Protocol\n- mcp_(sse)🐍✨🔗: Remote MCP (Model Context Protocol) calls  \n- mcp_to_openai_func_call🐍✨: MCP Tool Spec to OpenAI Function Call Converter\n- memory_for_llm🐍🔗: Memory management techniques for LLMs – [K-LaMP](https://arxiv.org/pdf/2311.06318.pdf)🪟\n- memory_graphiti🐍✨: Graph and neo4j based Memory \n- mini-copilot🐍✨🔗: DSL approach to calling the M365 API\n- mixture_of_agents🐍✨🎡: Multi-agent system for collecting responses from multiple LLMs\n- open_telemetry🐍✨: OpenTelemetry – Tracing LLM requests and logging\n\n## 📁 eval\n- evaluation_llm_as_judge📔: Using LLMs for automated evaluation and scoring\n- guardrails📔: Guardrails for AI safety and compliance\n- pyrit_(safety_eval)📔🪟: Python Risk Identification Tool\n\n## 📁 framework\n- agno_(framework)🐍: Agno – A simple, intuitive agent framework\n- autogen_(framework)🐍🪟: AutoGen – A Framework for LLM Agent\n- crewai_(framework)🐍: CrewAI – Agent collaboration framework\n- dspy_(framework)🐍📔: DSPy – Declarative Language Model Calls into Self-Improving Pipelines\n- guidance_(framework)📔🪟: Guidance – Prompt programming framework\n- haystack_(framework)🐍📔: Haystack – NLP framework for RAG and search\n- langchain_(framework)📔: LangChain – Framework for LLM applications\n- llamaindex_(framework)📔: LlamaIndex – Data framework for LLM retrieval/agent\n- magentic-one_(agent)🐍🪟: Magentic-One – Multi-agent system for solving open-ended web and file-based tasks\n- mem0_(framework)🐍📔: Mem0 – LLM Memory\n- omniparser_(gui)📔🪟: OmniParser – GUI automation and parsing tool\n- prompt_flow_(framework)📔🪟: Prompt Flow – LLM Workflow\n- prompty_(framework)🔗🔴🪟: Prompty – Prompt management\n- pydantic_ai_(framework)🐍: Pydantic AI – Pydantic agent framework\n- semantic_kernel_(framework)🐍🪟: Semantic Kernel – Microsoft LLM orchestration framework\n- smolagent_(framework)🐍: SmolAgent – Hugging Face Lightweight AI agent framework\n- tiny_troupe_(framework)📔🪟: Tiny Troupe – Multi agent persona simulation\n- x-ref: [📁microsoft-frameworks-and-libraries](#-microsoft-frameworks-and-libraries): \n\n## 📁 industry\n- auto_insurance_claims📔: Automation for auto insurance claims processing\n- career_assistant_agent📔: Career guidance and job recommendation agent\n- contract_review📔: Legal contract analysis and review\n- customer_support_agent📔: Customer support automation\n- damage_insurance_claims📔: Automated claims processing for damage insurance\n- invoice_sku_product_catalog_matching📔: Invoice and SKU reconciliation for accounting\n- invoice_payments📔: Automation for invoice payments\n- invoice_standardization📔: Standardizing invoice units for consistency\n- music_compositor_agent📔: Music composition assistant\n- news_summarization_agent📔: Automated summarization of news articles\n- nyc_taxi_pickup_(ui)🐍: NYC taxi pickup analysis and UI visualization\n- patient_case_summary📔: Summaries for patient medical cases\n- project_management📔: a tools for project tracking and task management\n- stock_analysis🐍✨🔗: AutoGen demo for analyzing stock investments\n- travel_planning_agent📔: Travel itinerary planner\n- youtube_summarize🐍✨: Summarizing YouTube videos using AI\n\n## 📁 llm\n- finetuning_grpo📔: Group Relative Policy Optimization (GRPO) for LLM fine-tuning\n- knowledge_distillation📔: Compressing LLM knowledge into smaller models\n- llama_finetuning_with_lora📔: LoRA – Low-Rank Adaptation of Large Language Models\n- nanoGPT🐍: Lightweight GPT implementation\n- nanoMoE🐍: Lightweight Mixture of Experts (MoE) implementation\n\n## 📁 llmops\n- azure_prompt_flow🔗🪟: Azure AI Foundry - Prompt flow: E2E development tools for creating LLM flows and evaluation\n- mlflow📔: OSS platform managing ML workflows\n\n## 📁 multimodal\n- image_gen📔: Image creation\n- image_gen_dalle📔: Image creation with segmentaion\n- openai-agents-sdk-voice-pipeline📔✨: OpenAI Agents SDK for voice processing\n- openai-chat-vision📔: Multimodal chat with vision capabilities\n- phi-series-cookbook_(slm)🔗🪟: Phi series models cookbook (small language models)\n- video_understanding📔: Video content analysis and understanding\n- vision_rag📔: Combining visual data with retrieval-augmented generation (RAG)\n- visualize_embedding📔: Tools for embedding visualization and analysis\n- voice_audio🟦: RTClient sample for using the Realtime API in voice applications\n\n## 📁 nlp\n- multilingual_translation_(co-op-translator)🐍🪟: a library for multilingual translation\n- search_the_internet_and_summarize📔: Internet search and summarization\n- sentiment_analysis_for_customer_feedback📔: Sentiment analysis for customer feedback\n- translate_manga_into_english🐍✨: Manga translation into English\n- txt2sql🐍: Converting natural language queries into SQL\n\n## 📁 rag\n- adaptive-rag📔: Adaptive retrieval-augmented generation (RAG)\n- agentic_rag📔: Agent-based RAG system\n- contextual_retrieval_(rag)📔: Context-aware retrieval for RAG\n- corrective_rag📔: Improving retrieval results with corrective techniques\n- fusion_retrieval_reranking_(rag)📔: Fusion-based retrieval and reranking for RAG\n- graphrag📔🪟: Graph-based retrieval-augmented generation\n- hyde_(rag)📔: Hypothetical Document Embeddings for better retrieval\n- query_rewriting_(rag)📔: Enhancing RAG by rewriting queries for better retrieval\n- raptor_(rag)📔: Recursive Abstractive Processing for Tree-Organized Retrieval\n- self_rag📔: Self-improving retrieval-augmented generation\n\n## 📁 research\n- analysis_of_twitter_the-algorithm_source_code📔: Analyzing [Twitter’s open-source ranking algorithm ](https://github.com/twitter/the-algorithm)\n- deep_research_langchain🐍📔: AI-driven deep research and analysis tools using LangChain\n- deep_research_smolagents🐍📔: AI-driven deep research and analysis tools using smolagents\n- openai_code_interpreter🐍📔: OpenAI’s code interpreter for data analysis\n- r&d-agent🐍🪟: Research and development AI agent\n\n## 🛠️ Comparing Local with Remote Repository\n\nYou can use the `git_cmp.py` script (and related files) to compare your local project directories with their corresponding remote GitHub repositories. \n\n### Typical Workflow\n\n1. **Index all projects and their GitHub URLs:**\n    ```bash\n    python git_cmp.py --index --root <root_dir> --csv git_cmp_index.csv\n    ```\n    This creates a CSV file listing all projects and their remote URLs.\n\n2. **Compare local and remote repositories:**\n    ```bash\n    python git_cmp.py --compare --root <root_dir> --csv git_cmp_index.csv --report git_cmp_report.txt --update_csv git_cmp_needs_update.csv\n    ```\n    This generates a report and a CSV of projects needing updates. It also copies changed files into `.cache/` for review.\n\n3. **Update local files from cache (optional, use with care):**\n    ```bash\n    python git_cmp.py --manipulate --root <root_dir> --update_csv git_cmp_needs_update.csv\n    ```\n    This copies files from `.cache/` back into your project directories, optionally deleting files if flagged.\n\n### Options\n\n- `--delay_sec <seconds>`: Add a delay between GitHub API calls to avoid rate limits.\n- `--index`: Index projects and write a CSV.\n- `--compare`: Compare projects and write a report.\n- `--manipulate`: Update local files from the cache based on the update CSV.\n\n### Example\n\n```bash\npython git_cmp.py --index --root . --csv git_cmp_index.csv\npython git_cmp.py --compare --root . --csv git_cmp_index.csv --report git_cmp_report.txt --update_csv git_cmp_needs_update.csv\npython git_cmp.py --manipulate --root . --update_csv git_cmp_needs_update.csv\n```\n\n> **Note:**  \n> - Create `.env` file. Set the `GITHUB_TOKEN`. `e.g.,GITHUB_TOKEN=<your_key>`\n> - Review `.cache/` and the generated report before running `--manipulate`.\n> - See comments and docstrings in `git_cmp.py` for more details.\n\n## 📚 References & Sources\n\n- [OpenAI Cookbook](https://github.com/openai/openai-cookbook)\n- [LangChain Cookbook](https://github.com/langchain-ai/langchain/tree/master/cookbook)\n- [LlamaCloud Demo](https://github.com/run-llama/llamacloud-demo)\n- [Chainlit Cookbook](https://github.com/Chainlit/cookbook)\n- [Microsoft AI Agents for Beginners](https://github.com/microsoft/ai-agents-for-beginners)\n- [GenAI Agents by NirDiamant](https://github.com/NirDiamant/GenAI_Agents)\n- [RAG Techniques by NirDiamant](https://github.com/NirDiamant/RAG_Techniques)\n- [Gemini API Cookbook](https://github.com/google-gemini/cookbook)\n- [Anthropic Cookbook](https://github.com/anthropics/anthropic-cookbook)\n- [Awesome LLM Apps](https://github.com/Shubhamsaboo/awesome-llm-apps)\n- [AI Engineering Hub](https://github.com/patchy631/ai-engineering-hub)\n\n## 💻 Microsoft Frameworks and Libraries\n\n1. [Semantic Kernel](https://devblogs.microsoft.com/semantic-kernel/) (Feb 2023): An open-source SDK for integrating AI services like OpenAI, Azure OpenAI, and Hugging Face with conventional programming languages such as C# and Python. It's an LLM orchestrator, similar to LangChain. / [git](https://github.com/microsoft/semantic-kernel) ![GitHub Repo stars](https://img.shields.io/github/stars/microsoft/semantic-kernel?style=flat-square&label=%20&color=gray&cacheSeconds=36000)\n1. [Azure ML Prompt Flow](https://learn.microsoft.com/en-us/azure/machine-learning/prompt-flow/overview-what-is-prompt-flow) (Jun 2023): A visual designer for prompt crafting using Jinja as a prompt template language. / [ref](https://techcommunity.microsoft.com/t5/ai-machine-learning-blog/harness-the-power-of-large-language-models-with-azure-machine/ba-p/3828459) / [git](https://github.com/microsoft/promptflow)\n ![GitHub Repo stars](https://img.shields.io/github/stars/microsoft/promptflow?style=flat-square&label=%20&color=gray&cacheSeconds=36000)\n1. [SAMMO](https://github.com/microsoft/sammo) (Apr 2024): A general-purpose framework for prompt optimization. / [ref](https://www.microsoft.com/en-us/research/blog/sammo-a-general-purpose-framework-for-prompt-optimization/)\n ![GitHub Repo stars](https://img.shields.io/github/stars/microsoft/sammo?style=flat-square&label=%20&color=gray&cacheSeconds=36000)\n1. [guidance](https://github.com/microsoft/guidance) (Nov 2022): A domain-specific language (DSL) for controlling large language models, focusing on model interaction and implementing the \"Chain of Thought\" technique.\n ![GitHub Repo stars](https://img.shields.io/github/stars/microsoft/guidance?style=flat-square&label=%20&color=gray&cacheSeconds=36000)\n1. [Autogen](https://github.com/microsoft/autogen) (Mar 2023): A customizable and conversable agent framework. / [ref](https://www.microsoft.com/en-us/research/blog/autogen-enabling-next-generation-large-language-model-applications/) / [Autogen Studio](https://www.microsoft.com/en-us/research/blog/introducing-autogen-studio-a-low-code-interface-for-building-multi-agent-workflows/) (June 2024)\n ![GitHub Repo stars](https://img.shields.io/github/stars/microsoft/autogen?style=flat-square&label=%20&color=gray&cacheSeconds=36000)\n1. [UFO](https://github.com/microsoft/UFO) (Mar 2024): A UI-focused agent for Windows OS interaction.\n ![GitHub Repo stars](https://img.shields.io/github/stars/microsoft/UFO?style=flat-square&label=%20&color=gray&cacheSeconds=36000)\n1. [Prompty](https://github.com/microsoft/prompty) (Apr 2024): A template language for integrating prompts with LLMs and frameworks, enhancing prompt management and evaluation.\n ![GitHub Repo stars](https://img.shields.io/github/stars/microsoft/prompty?style=flat-square&label=%20&color=gray&cacheSeconds=36000)\n1. [OmniParser](https://github.com/microsoft/OmniParser) (Sep 2024): A simple screen parsing tool towards pure vision based GUI agent.\n ![GitHub Repo stars](https://img.shields.io/github/stars/microsoft/OmniParser?style=flat-square&label=%20&color=gray&cacheSeconds=36000)\n1. [TinyTroupe](https://github.com/microsoft/TinyTroupe): LLM-powered multiagent persona simulation for imagination enhancement and business insights. [Mar 2024] ![GitHub Repo stars](https://img.shields.io/github/stars/microsoft/TinyTroupe?style=flat-square&label=%20&color=gray&cacheSeconds=36000)\n1. [RD-Agent](https://github.com/microsoft/RD-Agent): open source R&D automation tool [ref](https://rdagent.azurewebsites.net/) [Apr 2024]\n ![GitHub Repo stars](https://img.shields.io/github/stars/microsoft/RD-Agent?style=flat-square&label=%20&color=gray&cacheSeconds=36000)\n1. [Magentic-One](https://aka.ms/magentic-one): Built on AutoGen. A Generalist Multi-Agent System for Solving Complex Tasks [Nov 2024]\n1. [PyRIT](https://github.com/Azure/PyRIT) (Dec 2023): Python Risk Identification Tool for generative AI, focusing on LLM robustness against issues like hallucination, bias, and harassment.\n ![GitHub Repo stars](https://img.shields.io/github/stars/Azure/PyRIT?style=flat-square&label=%20&color=gray&cacheSeconds=36000)\n1. [Presidio](https://github.com/microsoft/presidio): Presidio (Origin from Latin praesidium ‘protection, garrison’). Context aware, pluggable and customizable data protection and de-identification SDK for text and images. [Oct 2019]\n1. [Microsoft Fabric](https://learn.microsoft.com/en-us/fabric/): Fabric integrates technologies like Azure Data Factory, Azure Synapse Analytics, and Power BI into a single unified product [May 2023]\n\n## ➡️ Convert ipynb to Python\n\n- To convert a Jupyter notebook (.ipynb) into a runnable Python scrip\n\n```bash\npip install nbformat nbconvert\n```\n\n```python\nimport nbformat\nfrom nbconvert import PythonExporter\n\n# Load the notebook\nnotebook_filename = 'your_notebook.ipynb'\nwith open(notebook_filename, 'r', encoding='utf-8') as notebook_file:\nnotebook_content = nbformat.read(notebook_file, as_version=4)\n\n# Convert the notebook to a Python script\npython_exporter = PythonExporter()\npython_code, _ = python_exporter.from_notebook_node(notebook_content)\n\n# Save the converted Python code to a .py file\npython_filename = notebook_filename.replace('.ipynb', '.py')\nwith open(python_filename, 'w', encoding='utf-8') as python_file:\npython_file.write(python_code)\n\nprint(f\"Notebook converted to Python script: {python_filename}\")\n```\n\n## **Contributor** 👀\n\n<a href=\"https://github.com/kimtth/azure-openai-samples/graphs/contributors\">\n<img src=\"https://contrib.rocks/image?repo=kimtth/azure-openai-samples\" />\n</a>\n\nⓒ `https://github.com/kimtth` all rights reserved."
    },
    {
      "name": "luckypamula/azure-ai-agents-labs",
      "stars": 6,
      "img": "https://avatars.githubusercontent.com/u/26860411?s=40&v=4",
      "owner": "luckypamula",
      "repo_name": "azure-ai-agents-labs",
      "description": null,
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2025-01-22T22:26:29Z",
      "updated_at": "2025-04-16T01:26:10Z",
      "topics": [],
      "readme": "# Hands-On Labs for AI Agents Using Azure AI Agent Service SDK and Semantic Kernel\n\nThis repo contains hands-on labs for building AI Agents using the Azure AI Agent Service SDK and Semantic Kernel. The Azure AI Agent Service is used to create AI agents and Semantic Kernel is used to orchestrate the agents in a multi-agent system. \n\n## Prerequisites\n**Microsoft will provide the lab environment with all the prerequisites**\n* Azure subscription\n* Azure AI Foundry resource with an AI Hub and AI Project (You will set this up in Lab 1)\n* Visual Studio Code\n* Python 3.10>\n* Azure CLI\n* Azure CLI Azure ML extension\n\n## Labs\n\n### Lab 1 - Setup and test the lab environment\nLab 1 walks you through how to setup the necessary lab environment for building AI Agents. This includes:\n* Setting up the AI Project in the Azure AI Foundry\n* Deploying an LLM and embedding models\n* Establish connectivity from VS Code to the AI Project\n* Perform a simple Chat Completion call to the LLM to test that your lab environment is set up properly. \n\n### Lab 2 - Build a simple AI agent\nLab 2 introduces you to AI agents in Azure. You will learn how to build a simple AI agent that generates a bar chart comparing costs between health insurance plans.\n\n### Lab 3 - Build a RAG Agent\nIn Lab 3, you will be building an AI Agent that will perform Retrieval Augmented Generaton (RAG) on health plan documents. Azure AI Search will be used as the vector database for storing the embeddings for the health plan documents.\n\n### Lab 4 - Develop a multi-agent system\nIn Lab 4, you will be creating a multi-agent system consisting of 4 agents working together to generate reports about health plan documents. You will build these 4 AI Agents:\n1. Search Agent - This agent will search an Azure AI Search index for information about specific health plan policies.\n2. Report Agent - This agent will generate a detailed report about the health plan policy based on the information returned from the Search Agent.\n3. Validation Agent - This agent will validate that the generated report meets specified requirements. In our case, making sure that the report contains information about coverage exclusions.\n4. Orchestrator Agent - This agent will act as an orchestrator that manages the communication between the Search Agent, Report Agent, and Validation Agent.\n"
    },
    {
      "name": "microsoft/llmsecops-hands-on-lab",
      "stars": 6,
      "img": "https://avatars.githubusercontent.com/u/6154722?s=40&v=4",
      "owner": "microsoft",
      "repo_name": "llmsecops-hands-on-lab",
      "description": null,
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2024-11-12T22:00:15Z",
      "updated_at": "2025-03-09T21:43:31Z",
      "topics": [],
      "readme": "# Project\n\n> This repo has been populated by an initial template to help get you started. Please\n> make sure to update the content to build a great experience for community-building.\n\nAs the maintainer of this project, please make a few updates:\n\n- Improving this README.MD file to provide a great experience\n- Updating SUPPORT.MD with content about this project's support experience\n- Understanding the security reporting process in SECURITY.MD\n- Remove this section from the README\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n"
    },
    {
      "name": "ptbdnr/snippets",
      "stars": 6,
      "img": "https://avatars.githubusercontent.com/u/32688266?s=40&v=4",
      "owner": "ptbdnr",
      "repo_name": "snippets",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2024-08-01T11:47:45Z",
      "updated_at": "2025-04-16T09:42:41Z",
      "topics": [],
      "readme": "# Snippets\n\nA collection of handy code snippets\n"
    },
    {
      "name": "pablosalvador10/gbbai-azure-ai-agentic-frameworks",
      "stars": 6,
      "img": "https://avatars.githubusercontent.com/u/31255154?s=40&v=4",
      "owner": "pablosalvador10",
      "repo_name": "gbbai-azure-ai-agentic-frameworks",
      "description": " Build and customize multi-agent systems using the latest patterns and frameworks for advanced AI applications. 🤖🚀",
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2024-06-16T22:00:16Z",
      "updated_at": "2025-03-12T11:23:34Z",
      "topics": [],
      "readme": "# AI Agentic Design Patterns with AutoGen\n\nWelcome to our project, where we delve into the exploration of diverse agentic architectures, focusing on the development and implementation of AI agentic engines and multi-agent systems. This repository serves as a comprehensive guide for researchers, developers, and enthusiasts interested in advancing the field of AI through innovative agentic frameworks and collaborative agent systems.\n\n## 🚀 Getting Started\n\nThis project is dedicated to the exploration and implementation of cutting-edge AI agentic design patterns and multi-agent architectures. Here, you'll find resources, examples, and documentation designed to facilitate the development of complex, collaborative AI systems.\n\n### Prerequisites\n\nBefore you begin, ensure you have the following:\n\n- Python 3.8 or later\n- Familiarity with AI concepts and multi-agent systems\n- Access to Azure OpenAI (Ensure you have an Azure account and have set up Azure OpenAI service)\n\n## 🤖 Exploring AI Agentic Design Patterns\n\nOur project covers a wide range of topics related to AI agentic design and multi-agent systems, including:\n\n- **Introduction to Agentic Architectures**: Understanding the fundamentals of agentic architectures and their significance in AI development.\n- **Building Multi-Agent Systems**: A comprehensive guide to constructing multi-agent systems using AutoGen's `ConversableAgent`.\n- **Multi-Agent Collaboration Design Patterns**: Strategies for enabling seamless collaboration among agents to tackle complex tasks.\n- **Agent Reflection Framework**: Techniques for generating and refining high-quality content through agent collaboration.\n\n## 🛠️ Implementing Your Agentic Systems\n\nFollow our step-by-step guides to implement your own agentic systems and multi-agent architectures:\n\n1. **Understand the Basics**: Start with our introduction to agentic architectures to grasp the foundational concepts.\n2. **Explore Design Patterns**: Dive into our design patterns for building collaborative multi-agent systems.\n3. **Apply and Experiment**: Use our examples as a starting point to implement and experiment with your own agentic systems.\n4. **Iterate and Improve**: Continuously refine your systems based on feedback and performance evaluations.\n\n## 📚 Resources\n\n- **Project Documentation**: For detailed information on the project and its components, visit our [Documentation]().\n- **Tutorials**: Check out our [Tutorials]() for hands-on guides on creating and implementing AI agentic design patterns and multi-agent systems.\n\n"
    },
    {
      "name": "microsoft/asclepius",
      "stars": 6,
      "img": "https://avatars.githubusercontent.com/u/6154722?s=40&v=4",
      "owner": "microsoft",
      "repo_name": "asclepius",
      "description": "Asclepius is a hero and god of medicine in ancient Greek religion and mythology.",
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2024-03-28T21:52:32Z",
      "updated_at": "2025-04-04T02:00:32Z",
      "topics": [],
      "readme": "# AI Generated Lab Results Summary Template\nThis project provides an example template to illustrate the use of Azure OpenAI to generate a patient-facing summary of incoming lab results, including explanation of what the results mean and the implications on the patient's health. The intent is to simulate an integration with an EHR, intercepting the incoming lab results from the in-basket and passing those results along with the notes from the last encounter to the Azure OpenAI LLM to create an explanation to the patient - reducing cognitive load an time spent by clinicians and getting result information to patients more quickly.\n\n## Architecture\nThis template deploys an API built and hosted by an Azure Function App, with a Static Web App Blazor front end to illustrate a user selecting a lab result from an in-basket and requesting patient-facing summary from AI. \n\n![Template Architecture](./docs/LabSum_Template_Architecture.png)\n\nThis template includes the reading of sample data from a local sample data file for simplicity. This could be extended with Databricks or other mechanism to pull near-real-time data, or other integration pattern, as in the example below.\n\n![Example Integration Pattern with Databricks](./docs/LabSum_Extension_Example.png)\n\n## API Orchestration with Semantic Kernel\nThis solution includes an API defined and deployed to an Azure Function App, developed on Python with the Semantic Kernal SDK to define and orchestration prompt function calls to Azure OpenAI. \n\n1. Receive request on /labsummary endpoint\n    * expected payload includes lab results and encounter notes\n    * see HTTP request example: [fetch_patients.http](/src/Api/tests/fetch_patients.http)\n2. The `summarize_labs` Semantic Kernel prompt function is called, with the lab results and encounter notes as input\n    * see [summarize_labs.yaml](/src/Api/plugins/summarize_labs/summarize_labs.yaml) for prompt function definition\n3. As the `summarize_labs` prompt function requires subsections of the encounter notes, the following prompt functions are called to extract that data via the Semantic Kernal AzureChatCompletion service, before calling the service a final time with `summarize_labs` prompt\n    * [summarize_chief_complaint.yaml](/src/Api/plugins/summarize_note/summarize_chief_complaint.yaml)\n    * [summarize_lab_history.yaml](/src/Api/plugins/summarize_note/summarize_lab_history.yaml)\n    * [summarize_assessment.yaml](/src/Api/plugins/summarize_note/summarize_assessment.yaml)\n\nReference Documentation:\n* [Semantic Kernel GitHub Repo](https://github.com/microsoft/semantic-kernel/tree/main)\n* [Getting Started with Semantic Kernel (Python)](https://github.com/microsoft/semantic-kernel/blob/main/python/README.md)\n* [Understanding the Kernel in Semantic Kernel](https://learn.microsoft.com/en-us/semantic-kernel/agents/kernel/?tabs=python)\n* [Adding AI Services to the Kernel](https://learn.microsoft.com/en-us/semantic-kernel/agents/kernel/adding-services?tabs=python)\n\n# Deployment Instructions\nTo deploy to Azure, you can follow these steps after cloning the repo into your local IDE such as VS Code. \n\n1. Install the Azure CLI: [How to install the Azure CLI](https://learn.microsoft.com/en-us/cli/azure/install-azure-cli)\n\n2. Install the Azure Developer CLI (azd) extension for the Azure CLI. \n\n3. Log in to your Azure account by running the following command and following the prompts:\n    `azd auth login`\n\n4. Run the azd up command, and follow the prompts to indicate the subscription, region and resource group to deploy to\n\n    `azd up`\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n"
    },
    {
      "name": "microsoft/polonius",
      "stars": 6,
      "img": "https://avatars.githubusercontent.com/u/6154722?s=40&v=4",
      "owner": "microsoft",
      "repo_name": "polonius",
      "description": "Summarize information and classify severity of incoming Emergency Department patients.",
      "homepage": null,
      "language": "CSS",
      "created_at": "2024-03-19T12:47:36Z",
      "updated_at": "2025-04-04T02:03:31Z",
      "topics": [],
      "readme": "# Project Polonius\n\nThis is a sample flask app to demonstrate using Azure OpenAI with Semantic Kernel to summarize and classify the severity of incoming trauma patients.\n\n## Azure Architecture\n![azure architecture diagram](./docs/azure-architecture.drawio.png)\n\n## Pre-requisites\n- Azure CLI (az)\n- Azure Developer CLI (azd)\n- Docker Desktop (for devcontainer) - OR - local python dev environment\n- Azure OpenAI (or use `azd provision` to create)\n\n## Getting Started\n1. Clone this repo\n1. Rename [src/.env.example](src/.env.example) to `.env`\n1. Update `.env` file with your configuration values\n1. Run locally with debugger\n\n## Deploy to Azure\n1. Deploy to Azure with `azd up`\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n"
    },
    {
      "name": "Geniusning/AI",
      "stars": 6,
      "img": "https://avatars.githubusercontent.com/u/27037651?s=40&v=4",
      "owner": "Geniusning",
      "repo_name": "AI",
      "description": "learning AR",
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2024-03-02T09:08:32Z",
      "updated_at": "2025-03-14T03:53:03Z",
      "topics": [],
      "readme": "# AGI 课堂《AI 大模型全栈工程师》课件\n\n## 声明\n\n本目录下的所有文件，包括但不限于文本、图像、音频、视频和其他格式均为瓜皮汤（海南）教育科技有限公司（以下简称“本公司”）所有。版权受中华人民共和国法律及国际法律的保护。\n\n未经本公司明确的书面授权，任何组织或个人不得复制、分发、传播、出版或以任何方式使用这些文件的任何部分。此外，未经授权，不得对这些文件进行修改、创作衍生作品或以任何商业目的使用。\n\n如果你并非《AI 大模型全栈工程师》课程的学员，但获得这些文件，你应该立即删除这些文件。\n\n如果想保留，请到[https://agiclass.ai](https://agiclass.ai)了解课程内容并购买。\n\n本公司保留随时修订本版权声明的权利。\n\n## 本地运行课件\n\n```python\npip3 install notebook\ncd <课件目录>\njupyter notebook\n```\n\n## 课件风格\n\n### 基础风格要求\n\n1. 文字精炼，没废话。这是个课件、笔记，不是教材\n2. 代码简洁，没冗余。不分散学员注意力\n3. 避免大段文字，多用列表，这样更清晰\n4. 只有一个一级标题，是该次课的名称\n4. 内容从二级标题开始，可以使用三级、四级标题，但要尽量减少层级，这样能保持从头到尾最极致的流畅\n6. 中文和英文、数字之间，要有一个空格，这样排版更美观\n\n### 关键内容提示\n\n用 alert 框突出关键内容。\n\n### 划重点\n\n对最最核心、最最重要的知识点总结，要**划重点**。使用用如下代码，呈现为绿色，表示自信、稳健。\n\n```html\n<div class=\"alert alert-success\">\n<b>划重点：</b>\n<ol>\n<li>把 ChatGPT 看做是一个函数，给输入，<b>生成</b>输出</li>\n<li>任何业务问题，都可以用语言描述，成为 ChatGPT 的输入</li>\n<li>就能<b>生成</b>业务问题的结果</li>\n</ol>\n</div>\n```\n### 引起注意\n\n其它要引起注意的内容，用如下代码，呈现为橙色，表示友好。\n\n```html\n<div class=\"alert alert-warning\">\n<b>建议：</b>\n<li>会编程几乎是唯一要求，但并不用特别擅长。AI 的强大，使得用好它的门槛也低了很多。</li>\n<li>Python 是课程主语言。不熟悉也没关系，都看得明白。需要上手编时，正好用 AI 帮忙。</li>\n</div>\n```\n    \n```html\n<div class=\"alert alert-warning\">\n<b>思考：</b>你觉得哪些应用算是 AI？\n</div>\n```\n\n"
    },
    {
      "name": "minerva-ed/MinervAI",
      "stars": 6,
      "img": "https://avatars.githubusercontent.com/u/150646217?s=40&v=4",
      "owner": "minerva-ed",
      "repo_name": "MinervAI",
      "description": "Classroom simulator for educators",
      "homepage": "",
      "language": "Jupyter Notebook",
      "created_at": "2023-11-13T04:58:00Z",
      "updated_at": "2025-02-22T04:11:32Z",
      "topics": [],
      "readme": "## This repository contains Minerva, a classroom simulator which won 1st place at HackUMass2023. We have productionized this prototype into a softawre as a service targeting sales team, which can be found [here](https://minervai.co)\n# MinervAI\n![output-onlinepngtools](https://github.com/dmavani25/MinervAI/assets/107078090/8413ebde-629f-4307-bfba-ca26f8a41b7c)\n![demo](demo.png)\n\nEmpowering teachers through interactions with realistic simulated classrooms. MinervAI combines AI agent students with various backgrounds and proficiencies to help educators better understand students and the ways in which they think.\n\nIn this beta version of MinervAI,  language models serve as AI agents in the classroom: each is initialized with a distinct personality (ex: “confident”), background (ex: “liberal arts student with math training”) and weight factor (which varies the scale of the 2 prior features’ impact) which influences their understanding of topics.\n\nUsers can upload a “lecture file” containing important information. The text is interpreted by the professor agent, which lectures the students, who then internalize the new data and generate new questions depending on their background. The professor then provides sample answers specific to each student’s needs and knowledge gaps.\n\nThe final output of this tool is a summary file listing Q+A conversations and summary statistics regarding question types (commonly asked questions, question frequency, recurring “keyword” concepts and more).\n\n## API and Frontend\n\nThe API is built partially with SemanticKernel to manage different contexts and connections to LLM providers (we use OpenAI's chat-gpt-3.5 and chat-gpt-4-turbo). You can test this out by running `python3 server.py`.\n\nAlso included is a websocket web server which allows for asynchronous connection to the API, to stream results. This server is built with FastAPI and Uvicorn, which can be executed by `uvicorn server:app --reload`.\n\nFinally, the front-end server can be started by calling `npm run dev`, more information is found in the `frontend` directory.\n\nUse **uvicorn server:app --reload** to run the web server, and **npm run dev** to run the frontend.\n\n<h2>Requirements for running</h2>\n\nThe required packages needed to run MinervAI are included in the ***requirements.txt*** file.\n\nAll packages can be installed using `pip3 install -r requirements.txt` command. \n\n## MinervAI - Team Contributions\n\nMinervAI is an innovative educational tool designed to empower teachers through interactions with AI-simulated classrooms. This project was developed as part of the HackUMass XI Hackathon, where it was awarded the **Grand Overall Prize**.\n\nWe would like to firstly thank Taichi Kato (one of our pivotal founding members) for giving an awesome introduction to Semantic Kernels, and making pivotal contributions to the project with us!!\n\n### Team Members & Contributions\n\nDeveloped by Taichi Kato, Tina Zhang, Dhyey Mavani, Seb Brown, Muhammad Ahsan Tahir, and Sawyer Pollard. All are current students of Amherst College.\n"
    },
    {
      "name": "embedelite/embedelite-sk-plugin",
      "stars": 6,
      "img": "https://avatars.githubusercontent.com/u/128965123?s=40&v=4",
      "owner": "embedelite",
      "repo_name": "embedelite-sk-plugin",
      "description": "The official repo for the EmbedElite Semantic Kernel plugin",
      "homepage": null,
      "language": "Python",
      "created_at": "2023-07-28T07:44:29Z",
      "updated_at": "2023-09-22T01:14:06Z",
      "topics": [],
      "readme": "\n<div align=\"center\">\n  <a href=\"https://www.embedelite.com/\">\n    <img src=\"content/logo.png\" alt=\"Logo\" width=\"238\">\n  </a> \n</div>\n\n<p align=\"center\">\n    Relevant LLM-ready assets for professionals. <br/>\n\n<br>\n\n> 🥳 We are among the winners of the SK Hackathon! Thanks to the SK Team and community for your interest and votes. Since the hackathon has ended, we had to deactivate the API key used in this repo. Also we are going to refine our plugin/provide more integration examples soon. If you would like to get access to our API or discover more embeddings/RAG solutions, please check our website [www.embedelite.com]([linkurl](https://www.embedelite.com))\n\n\n# SK Hackathon EmbedElite Plugin\n\nThis project is creating a plugin for the EmbedElite marketplace for the SK Hackathon. The Sementic Kernel plugin facilitates fetching ready-made premium context based on embeddings via an API.\n\n\n# About EmbedElite\n\nEmbedElite is a marketplace for LLM assets, i.e., data chunks, priced by demand and queried via Retrieval-Augmented Generation (“RAG”)\nOn one side, data vendors feed information onto the platform and are compensated per usage of their data and retrieval algorithm\nOn the other side, data consumers perform queries enriched by the relevant assets and algorithms, paying per query and tokens retrieved\n\n![](content/image1.png)\n\n# How to Use EmbedElite\n\n### Endpoints for Data Consumers\n\n#### Request Type: POST\n\nURL:\n\n```http request\nhttps://api.embedelite.com/query\n```\n\nHeaders:\n\n```http request\nAPI-Key: <YOUR_API_KEY>\nContent-Type: application/json\n```\n\nBody (JSON):\n\n```json\n{\n  \"query\": \"<your_query_to_make>\",\n  \"product_id\": \"<the_product_id>\",\n  \"rag_id\": \"<the_rag_id>\",\n  \"price_floor\": 1.0,\n  \"price_cap\": 5.0,\n  \"currency\": \"EUR\"\n}\n```\ncUrl example:\n```bash\ncurl -X POST https://api.embedelite.com/query \\\n-H \"API-Key: sk-ee-9m839d3n98nh39fh9f3mhe98h3\" \\\n-H \"Content-Type: application/json\" \\\n-d '{\n  \"query\": \"What are the VAT rules in Germany if I sell services?\",\n  \"product_id\": \"vat-rules-eu\",\n  \"rag_id\": \"RAG_aks298msd9nj34hncs\",\n  \"price_floor\": 1.0,\n  \"price_cap\": 5.0,\n  \"currency\": \"EUR\"\n}'\n```\n\nResponse Example:\n```json\n{\n  \"response\": \"If you sell services in Germany, you usually charge the German VAT rate of 19%. However, there are exceptions for broadcasting, telecommunication, and electronically-supplied services, which must be charged at the VAT rate of the customer's country. If your customer is in another EU country, your invoice must contain specific information, such as the customer's name and address, the date of the invoice, the VAT rate, and the total amount including VAT. If your customer is outside the European Union, you must not charge VAT.\",\n  \"updated_at\": \"2023-07-25\"\n  \"currency\": \"EUR\",\n  \"paid\": 2.3,\n  \"originalQuery\": \"What are the VAT rules in Germany if I sell services?\",\n  \"price_cap\": 2.3,\n  \"price_floor\": 2.3,\n}\n```\n\n## For Vendors\n\nData vendors can simple push content via a POST endpoint to the platform. EmbedElite will keep the data confidential and intellectual ownership stays with the vendor. The ownership is defined in the metadata field. If you are a data vendor, please contact us for the data vendor API access: info@embedelite.com.\n\n\n## Requirements to use the plugin\n\n- [Python](https://www.python.org/downloads/) 3.8 or higher\n  - [Poetry](https://python-poetry.org/) for package handling and dependency management\n  - [Semantic Kernel Tools](https://marketplace.visualstudio.com/items?itemName=ms-semantic-kernel.semantic-kernel)\n\n## Sample Configuration\n\nA `.env` file housed within the project is used for configuring the sample. It contains API keys and other confidential settings.\n\nEnsure you possess an\n[Open AI API Key](https://openai.com/api/) or an\n[Azure Open AI service key](https://learn.microsoft.com/azure/cognitive-services/openai/quickstart?pivots=rest-api)\n\nCreate a new file titled `.env` by duplicating the `.env.example` file. Then transfer your API keys into the new `.env` file:\n\n```\nOPENAI_API_KEY=\"\"\nOPENAI_ORG_ID=\"\"\nAZURE_OPENAI_DEPLOYMENT_NAME=\"\"\nAZURE_OPENAI_ENDPOINT=\"\"\nAZURE_OPENAI_API_KEY=\"\"\nEMBEDELITE_API_KEY=\"\"\n```\n\n## Sample Execution\n\nWithin Visual Studio Code, press `F5` to run the console application. As defined in `launch.json` and `tasks.json`, Visual Studio Code will implement `poetry install` and then `python hello_world/main.py`\n\nTo build and execute the console application from the terminal, apply the following commands:\n\n```powershell\npoetry install\npoetry run python main.py\n```\n"
    },
    {
      "name": "jplck/from-single-to-multi-agent",
      "stars": 5,
      "img": "https://avatars.githubusercontent.com/u/8427174?s=40&v=4",
      "owner": "jplck",
      "repo_name": "from-single-to-multi-agent",
      "description": null,
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2025-03-18T07:42:27Z",
      "updated_at": "2025-04-20T11:44:15Z",
      "topics": [],
      "readme": "# From Single to Multi-Agent AI Systems\n\nThis repository contains a comprehensive collection of labs and resources for learning about AI agent development, from single-agent implementations to complex multi-agent systems. It explores various frameworks including Semantic Kernel, AutoGen, and more, demonstrating practical implementations of modern AI agent architectures.\n\n## Overview\n\nThe repository is structured to provide a progressive learning journey through the world of AI agents:\n\n- **Foundation concepts**: Learn about basic agent patterns like ReAct\n- **Advanced frameworks**: Explore Semantic Kernel, process frameworks, and AutoGen\n- **Multi-agent orchestration**: Discover techniques for coordinating multiple specialized agents\n\n## Lab Structure\n\nThe labs are organized in increasing complexity, starting from basic concepts and advancing to sophisticated multi-agent systems:\n\n| Lab | Description | Link |\n|-----|-------------|------|\n| Azure OpenAI Basics | Introduction to working with Azure OpenAI | [01-basics/01_azureopenai.ipynb](labs/01-basics/01_azureopenai.ipynb) |\n| Semantic Kernel - Orchestration | Learn to orchestrate AI capabilities with Semantic Kernel | [02-semantic-kernel/01_orchestrator.ipynb](labs/02-semantic-kernel/01_orchestrator.ipynb) |\n| Semantic Kernel - Functions | Working with functions in Semantic Kernel | [02-semantic-kernel/02_functions.ipynb](labs/02-semantic-kernel/02_functions.ipynb) |\n| Semantic Kernel - Multi-modal | Handling multi-modal content with Semantic Kernel | [02-semantic-kernel/03_multi-modal.ipynb](labs/02-semantic-kernel/03_multi-modal.ipynb) |\n| ReAct | Implementing AI planning capabilities | [03-planning/01_single_agent.ipynb](labs/03-planning/01_single_agent.ipynb) |\n| Reasoning | Implementing AI reasoning | [03-planning/02_reasoning_agent.ipynb](labs/03-planning/02_reasoning_agent.ipynb) |\n| Agent Framework - Basic Agents | Building individual agents with Semantic Kernel | [04-agent-framework/01_agents.ipynb](labs/04-agent-framework/01_agents.ipynb) |\n| Agent Framework - Group Chat | Implementing collaborative agent conversations | [04-agent-framework/02_agents-group-chat.ipynb](labs/04-agent-framework/02_agents-group-chat.ipynb) |\n| Process Framework - Basics | Understanding the Semantic Kernel Process Framework | [05-process-framework/01_process.ipynb](labs/05-process-framework/01_process.ipynb) |\n| Process Framework - Advanced | Building complex workflows with the Process Framework | [05-process-framework/02_process.ipynb](labs/05-process-framework/02_process.ipynb) |\n| AutoGen - Simple Group | Creating basic agent groups with AutoGen | [06-autogen/01_autogen-simple-group.ipynb](labs/06-autogen/01_autogen-simple-group.ipynb) |\n| AutoGen - Group Chat | Building multi-agent conversations with AutoGen | [06-autogen/02_autogen-group-chat.ipynb](labs/06-autogen/02_autogen-group-chat.ipynb) |\n| AutoGen - Reasoning | Implementing advanced reasoning with AutoGen | [06-autogen/03_autogen-reasoning.ipynb](labs/06-autogen/03_autogen-reasoning.ipynb) |\n| Single React Agent | Implementation of a basic ReAct pattern agent | [Single React Agent](labs/single_react_agent) |\n\n## Key Concepts Covered\n\n### Single-Agent Example\n\n- **ReAct (Reasoning + Acting)** - Combining reasoning with tool use for more effective agents\n- Function/tool calling capabilities\n- State management within agents\n\n### Semantic Kernel Framework\n\n- Creating kernel functions and tools\n- Building specialized agents with specific roles\n- Orchestrating multi-agent interactions\n- Process framework for structured AI workflows\n\n### Process Framework\n\n- Event-driven design for AI workflows\n- State management across process steps\n- Conditional branching and error handling\n- Complex workflow orchestration\n\n### Multi-Agent Architectures\n\n- Specialized agent roles and responsibilities\n- Agent coordination and communication\n- Multi-agent problem solving through collaboration\n- Group chat implementations\n\n### AutoGen Framework\n\n- Building agent groups\n- Implementing conversational agents\n- Advanced reasoning capabilities\n\n## Getting Started\n\n1. Clone this repository\n2. Install the required packages:\n   ```\n   pip install -r requirements.txt\n   ```\n3. (Optional) Configure your Azure OpenAI credentials in a `.env` file\n4. Start exploring the labs sequentially, beginning with the single agent implementation\n\n## Azure Integration (Optional)\n\nThis repository includes infrastructure templates for deploying solutions to Azure:\n- Azure OpenAI services configuration\n- Container Apps environments\n- Storage and hosting services"
    },
    {
      "name": "MSDLLCpapers/teal-agents",
      "stars": 5,
      "img": "https://avatars.githubusercontent.com/u/137432561?s=40&v=4",
      "owner": "MSDLLCpapers",
      "repo_name": "teal-agents",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2025-02-28T21:00:31Z",
      "updated_at": "2025-04-15T13:47:55Z",
      "topics": [],
      "readme": "# Teal Agents Platform\n## Overview\nThe Agent Platform aims to provide two major sets of functionality:\n1. A core framework for creating and deploying individual agents\n2. A set of orchestrators (and supporting services) which allow you to compose\n   multiple agents for more complex use cases\n\n## Core Agent Framework\nThe core framework can be found in the src/sk-agents directory. For more \ninformation, see its [README.md](src/sk-agents/README.md).\n\n## Orchestrators\nOrchestrators provide the patterns in which agents are grouped and interact with\nboth each other and the applications which leverage them. For more information\non orchestrators, see [README.md](src/orchestrators/README.md).\n\n## Getting Started\nSome of the demos and examples in this repository require docker images to be\nbuilt locally on your machine. To do this, once cloning this repository locally,\nfrom the root directory\nrun:\n```bash\n$ git clone https://github.com/MSDLLCpapers/teal-agents.git\n$ cd teal-agents\n$ make all\n```"
    },
    {
      "name": "adamhockemeyer/chat-with-my-apis",
      "stars": 5,
      "img": "https://avatars.githubusercontent.com/u/20075292?s=40&v=4",
      "owner": "adamhockemeyer",
      "repo_name": "chat-with-my-apis",
      "description": "Demo code and Azure resources which enable the use of a large language model to chat with external APIs to extend its capabilities.",
      "homepage": null,
      "language": "Python",
      "created_at": "2025-01-06T21:14:26Z",
      "updated_at": "2025-03-27T16:57:10Z",
      "topics": [],
      "readme": "# Chat with my APIs\nDemo code and Azure resources which enable the use of a large language model to chat with external APIs to extend its capabilities.\n\nThis example uses Sementic Kernel and the [OpenAPI plugin](https://learn.microsoft.com/en-us/semantic-kernel/concepts/plugins/adding-openapi-plugins?pivots=programming-language-python), to chat with APIs in Azure API Management.\n\n![Example Web App Screen](images/general-chat-weather-example.png)\n\n## Deployment\n\nRequirements\n- [Azure Developer CLI](https://learn.microsoft.com/en-us/azure/developer/azure-developer-cli/install-azd)\n- [Docker Desktop](https://www.docker.com/products/docker-desktop/)\n  - Required for azd to build and push images to the container registry\n\n1. Create an Azure Resource Group for this project (In the portal or CLI).\n\n    ```shell\n    az group create --name apichat-rg --location eastus\n    ```\n\n1.  Initialize the Azure Developer CLI\n\n    ```shell\n    azd init\n    ```\n1. Authenticate Azure Developer CLI\n\n    ```shell\n    azd auth login [--tenant-id]\n    ```\n\n1.  Set the Azure resource group you wish to deploy to\n\n    ```shell\n    azd env set AZURE_RESOURCE_GROUP <resource-group-name>\n    ```\n\n1.  Run the following command to build, deploy & configure the image\n\n    ```shell\n    azd up\n    ```\n\n## Notes\n\n1. Extending the Sample\n    1. You can create new products in Azure API Management. Include 'Agent' in the name of your product.\n    1. Add APIs to API Management. You can create manual APIs in the APIM Portal, or onboard an OpenAPI spec, Utilize policy on your APIs to handle authentication or forcing any required parameters that you may not want the LLM to decide.\n\n1. Agents Navigation Menu\n    1. **Agent's** in the navigation menu are populated by any product in APIM with the name 'agent' in it. The web app has a 'General Chat' default agent, controlled by an environment variable for the front end webapp `GENERIC_CHAT_APIM_PRODUCT_ID`\n\n1. Agent Instructions\n    1. Agent instructions (system prompt) are stored in APIM Named values. The code expects an agents instructions to be the name of the APIM product with `-instructions` in the named value. For example, a product `generic-chat-agent` would have a corresponding named value `generic-chat-agent-instructions` stored in the named values in APIM. This allows the instructions to be dynamic and changed as APIs are added or removed to a product.\n\n1. Frontend WebApp\n    1. NextJS based\n    1. Environment Variables\n    \n    | Variable Name                   | Required? | Example                        | Description                                                        |\n    |--------------------------------|----------|--------------------------------|--------------------------------------------------------------------|\n    | SK_API_ENDPOINT                | Yes      | http://127.0.0.1:8000          | The endpoint for the Semantic Kernel API.                          |\n    | GENERIC_CHAT_APIM_PRODUCT_ID   | Yes      | generic-chat-agent             | The default Azure API Management product name for the general chat.|\n\n1. Backend Python FastAPI\n    1. This is a small wrapper around Semantic Kernel SDK, while also fetching API/Product/OpenAPI spec information from API Management.\n    1. Environment Variables\n\n    | Variable Name                       | Required? | Example                                       | Description                                                                                      |\n    |------------------------------------|----------|-----------------------------------------------|--------------------------------------------------------------------------------------------------|\n    | AZURE_APIM_SERVICE_API_VERSION     | Yes      | \"2022-08-01\"                                  | The Azure API Management REST API version.                                                       |\n    | AZURE_APIM_ENDPOINT                | Yes      | \"https://your-apim-apichat.azure-api.net\"     | The base endpoint for your Azure API Management instance.                                        |\n    | AZURE_APIM_APICHAT_SUBSCRIPTION_KEY| Yes      | \"Subscription key for apim\"            | The subscription key the LLM uses for function calls product.                                           |\n    | AZURE_APIM_SERVICE_SUBSCRIPTION_KEY| Yes       | \"Subscription key for apim\"            | The subscription key the API backend uses to call the Azure APIM Services to get APIs/Products/OpenAPI Specs                                       |\n    | AZURE_OPENAI_CHAT_DEPLOYMENT_NAME  | Yes      | \"gpt-4o\"                                      | The name of the Azure OpenAI deployment for chat.                                                |\n    | AZURE_OPENAI_API_VERSION           | Yes      | \"2024-10-01-preview\"                          | The version of the Azure OpenAI API.                                                             |\n    | AZURE_OPENAI_API_KEY               | Yes       | \"NOT_NEEDED_FOR_APIM\"                         | The Azure OpenAI API key if calling OpenAI directly. Not needed if going through APIM.           |\n    | AZURE_OPENAI_ENDPOINT              | Yes      | \"https://your-apim.azure-api.net\"     | The endpoint for Azure OpenAI or your APIM pass-through endpoint.                                |\n\n1. Something not working?\n    1. Utilize the Web App or API logs in Azure Container Apps to review any error messages.\n    1. Test your APIs in Azure API Management Portal to ensure they are working as expected. Use APIM Tracing as necessary to help debug.\n    1. Ensure your agent's instructions are accurate. Agent instructions are stored in APIM named value with the convention `[product-name]-instructions`. If add a new product, you will need to add a matching name value instructions as the system prompt/guide for the agent.\n\n1. Knows Limitations\n    1. [128 tools/functions](https://learn.microsoft.com/en-us/azure/ai-services/openai/assistants-reference?tabs=python)\n        1. To help reduce token consumption, try adding only the APIs your solution needs to answer questions. Refer to the appendix below on tips and tricks for adding OpenAPI plugins.\n    1. Function description can be a max of 1,024 characters\n    1. **Function Names (aka APIM OperationId) will need to be snake case** (no hyphens) (i.e. `get_products_by_id`). If you need to edit this, you can export the OpenAPI Spec from APIM, edit it in a local file editor, and upload it back into the APIM API.\n\n## Sample Architecture\n\n![Example Web App Screen](images/sample-architecture-generic.png)\n\n## Appendix\n\n1. [Tips and tricks for adding OpenAPI plugins](https://learn.microsoft.com/en-us/semantic-kernel/concepts/plugins/adding-openapi-plugins?pivots=programming-language-python#tips-and-tricks-for-adding-openapi-plugins)\n1. [Azure AI Gateway - APIM Policy for load balancing, logging, and much more.](https://github.com/Azure-Samples/AI-Gateway)\n1. [Fine Tuning with Function Calling on Azure OpenAI Service](https://techcommunity.microsoft.com/blog/azure-ai-services-blog/fine-tuning-with-function-calling-on-azure-openai-service/4065968)\n\n\n## Examples\n1. Named Value Agent Instructions in APIM\n    ![Example Agent Instructions](images/named-value-agent-instructions.png)\n1. Logic Apps are a quick way to build APIs and connectivity into many systems. Below is a sample az rest call which will export the Http Trigger APIs in a Logic App as an OpenAPI Spec\n\n     ```shell\n    az rest -m POST -u https://management.azure.com/subscriptions/[subscriptionId]/resourceGroups/[resourceGroupName]/providers/Microsoft.Web/sites/[logicAppName]/hostruntime/runtime/webhooks/workflow/api/management/listSwagger?api-version=2018-11-01\n    ```"
    },
    {
      "name": "lukaskellerstein/ai",
      "stars": 5,
      "img": "https://avatars.githubusercontent.com/u/12882519?s=40&v=4",
      "owner": "lukaskellerstein",
      "repo_name": "ai",
      "description": "Artificial Intelligence projects",
      "homepage": null,
      "language": "Python",
      "created_at": "2023-02-19T12:49:14Z",
      "updated_at": "2025-04-15T10:16:51Z",
      "topics": [],
      "readme": "![ml-dl](./assets/ml-vs-dl.jpg)\n\n**Artificial intelligence (AI)** refers to the ability of machines to perform tasks that would normally require human intelligence, such as reasoning, perception, learning, and decision making. AI is a broad field that encompasses many different subfields, including machine learning and deep learning.\n\n**Machine learning** is a subset of AI that involves training machines to learn from data without being explicitly programmed. In machine learning, algorithms are developed that can learn from data and improve their performance over time. This process involves feeding large amounts of data into a machine learning model, which then makes predictions or classifications based on that data. Examples of machine learning applications include image recognition, natural language processing, and recommendation systems.\n\n**Deep learning** is a type of machine learning that involves the use of artificial neural networks, which are modeled after the structure and function of the human brain. Deep learning algorithms can automatically extract features from data, allowing them to learn from complex and large datasets. Examples of deep learning applications include speech recognition, object detection, and autonomous driving. Deep learning has shown great potential for solving complex problems in various fields, from healthcare to finance to robotics.\n\n**Reinforcement learning** is another important subset of machine learning that focuses on training agents to make decisions by interacting with an environment. In reinforcement learning, an agent learns through trial and error, receiving rewards or penalties based on its actions. The goal is to maximize cumulative rewards over time by learning an optimal strategy, or policy, for decision-making.\n\n## Notes\n\nRelease disk space for WSL2: https://stephenreescarter.net/how-to-shrink-a-wsl2-virtual-disk/\n\n## Tutorial\n\n1. Pytorch\n   - ANN\n   - CNN\n   - RNN\n   - Transformer\n2. OpenAI\n   - Prompt\n   - Prompt engineering\n   - Chat\n3. Hugging Face\n   - Explore\n     - text2text-generation\n     - text-generation\n     - text-img\n     - text-speech\n     - ...etc\n   - Models = Pre-trained Transformers\n     - Run (32bit, 16bit, 8bit, 4bit) (CPU vs. GPU) (Accelerate)\n     - Train / Fine-tune\n   - Datasets\n   - Multimodal models\n4. Langchain\n\n## Finish\n\n- Fine-tuning multimodal models\n\n- Autogen\n\n  - Agent with RAG (ideally ChromaDB) = Memory ?? https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/memory.html\n\n  - Team - SelectorGroupChat with Human-in-the-loop - https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/selector-group-chat.html\n\n  - Team - Magentic-One - https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/magentic-one.html\n\n- Semantic-kernel\n\n  - Agent with RAG (ideally ChromaDB)\n  - Orchestration of Agents = Teams ??\n\n- Reinforcement learning\n\n  - Gymansium environments\n  - Stable-baseline 3\n"
    },
    {
      "name": "microsoft/mlops-llm-application-service",
      "stars": 5,
      "img": "https://avatars.githubusercontent.com/u/6154722?s=40&v=4",
      "owner": "microsoft",
      "repo_name": "mlops-llm-application-service",
      "description": "The repository shows how to implement MLOps process for LLM-based backend",
      "homepage": null,
      "language": "Python",
      "created_at": "2024-10-11T18:08:35Z",
      "updated_at": "2025-04-04T02:02:54Z",
      "topics": [],
      "readme": "# MLOps for LLM Application Services\n\n## Introduction\n\nThere are many different customer problems that can be solved using Large Language Models (LLMs) with no fine-tuning. GPT-3, GTP-3.5 and GPT-4.0 are prompt-based models, and they require just a text prompt input to generate its responses.\n\nLet’s look at some scenarios where Large Language Models are a useful component, even without fine-tuning. It will help us to understand what we need to develop and, finally, operationalize.\n\n**Scenario 1:** The basic service applies LLMs to input data to produce a summary, extract entities, or format data according to a pattern. For example, a quality assurance engineering assistant could help by taking natural language descriptions of an issue and reformatting to match a specific format. In this case, we need to develop an LLM Application Service to implement a single step flow to invoke an LLM, pass input data to it and return the LLM’s response back. The request to the LLM can be created using the few-shot approach, when it contains a system message, the input, and a few examples to tune LLM’s response.\n\n**Scenario 2:** A chatbot that provides abilities to extract some data using existing APIs. For example, it can be an application to order a pizza. It’s a conversational application, and a pizzeria’s API can provide data about available pizza types, toppings, and sales. The LLM can be used to collect all needed information from users in a human friendly way. In this case, the development process is concentrated around an LLM Application Service as well as a user interface (UI). The service must implement a conversational flow that might include history, API connections management and own logic to invoke different APIs. In this scenario we are using APIs as is, and we are assuming that they are black boxes with known inputs and outputs.\n\n**Scenario 3:** A chatbot that finds and summarizes information spread in several data sources, both structured and unstructured. For example, it can be a chatbot for service agents who use it to find answers for users’ questions in a big set of documents. It’s not enough to return references to documents that contain the needed information, but it’s important to extract the exact answer in a human readable format. In this scenario the development process should be focused around:\n\n- **UI:** that is the chatbot itself.\n- **LLM Application Service:** implements a complex flow gluing LLMs with other services, such as a search service. The application service might support memory, chain several LLM requests, manage connections to external services, load documents.\n- **Data Retrieval Services:** The documents themselves need to be stored in a searchable form, e,g. a vector database and to be stored they may need to be transformed (converted from image to text, or from audio or video to text), chunked, and embedded.The service must be tuned and configured to retrieve results. The tuning process might include components such as various pipelines to do data ingestion, indexes, serverless components to extend functionality of the service.\n\nThese three scenarios might not cover all possible usage of LLMs, but they cover the most common projects, and we can use them to understand what kinds of components we need to build: UI, LLM Application Service and Data Retrieval Service.\n\nIn this repository we are demonstration an LLM Application Service example that is based on Semantic Kernel, AI Studio, and supports programming languages like Python, C# and Java. At the same time, we would cross-reference the following repositories that can be useful to implement a custom RAG application:\n\n- [Data Retrieval Service based on AI Search](https://github.com/microsoft/mlops-aisearch-pull): the repository demonstrates how to use AI Search skills and indexers to pre-process data, store them in a vector form and provide a way to serve queries through an index.\n- [MLOps for LLM Application Services using Prompt flow](https://github.com/microsoft/mlops-promptflow-prompt): The repository uses Prompt flow as a way to orchestrate LLM Application Services using batch executor, embedded tracing and integration with evaluation framework.\n\n\n## Overall Architecture\n\n### LLM Application Service components\n\nGenerative AI Solutions might include various components, including data management systems and workflows, APIs that provide access to knowledge databases, and search systems with multiple search capabilities.  At the same time, any such solution includes an **LLM-based Application Service** that glues all the elements together and provides endpoints for client applications. A fundamental example of such a service is a flow that identify an intent based on a user’s query, work with a search index or indexes to extract data and summarizes and returns a response using one of the LLMs. More complex examples might include several agents and interactions with several LLMs in a single flow. In any case, we assume that one or several LLMs will be involved, which means that the service’s response is **non-deterministic** (likely to have a different result every time), meaning that Machine Learning best practices should be involved to guarantee the quality of the service.\n\nFrom a software engineering perspective, an LLM Application Service is just a service that can be implemented as a set of code files/scripts in C#, Python, Java, or any other language using various libraries, including Semantic Kernel, LangChain, OpenAI SDK and so on. Compared to other services, LLM-based ones always have additional components, including **configuration and prompts**, which are critical for successfully using LLMs. Another important element for most customers is **the observability features** that support the ability to understand the performance of LLM-based components like latency and cost, meaning that telemetry is the core component of the service. Suppose we pick OpenTelemetry as a potential way to collect traces and Application Insights to preserve and present observability details. In that case, we can visualize LLM Application Service using the following schema:\n\n![LLM Application Service components](./docs/images/LLMAppsComponents.jpeg)\n\nIn the simplest form, our service might include a single Python file and Prompty configuration with **LLM parameters and prompt**. Developing such a service is relatively straightforward, and LLM-based frameworks include many basic and more complex examples to demonstrate how to do that. For example, Semantic Kernel examples for Python can be found [here](https://github.com/microsoft/semantic-kernel/tree/main/python/samples/concepts).\n\n### Deployment and Monitoring\n\nAt the same time, having a set of scripts and configurations to deliver LLM Application Service as the completed solution is not enough. We are still missing two crucial elements: **deployment and evaluation**.\n\nOur service should be deployed as a **batch, real-time, or near-real-time endpoint** (to serve long-running requests) or a set of endpoints. Azure offers a variety of methods to achieve that, from a custom Fast API service under Azure Kubernetes Service (AKS) to Azure Functions and Online Endpoints in Azure ML. The deployment process can be implemented as a **set of scripts executed from a DevOps system**. The deployment target is often linked to a monitoring service like Azure Monitor to understand the performance of the target, such as CPU and memory usage, traffic issues, and so on. The following diagram demonstrates the outcome of this paragraph:\n\n![Deployment Components](./docs/images/InfraComponents.jpeg)\n\n\n### Evaluation\n\nSince our service is non-deterministic, we need to understand the quality of the service by applying evaluation techniques. To run the evaluation, we need to **prepare a dataset with ground-truth data and generate an output dataset from the service** to compare with the ground-truth data. From a software engineering perspective, this means that we need to develop a pipeline to generate that output dataset. However, from an infrastructure perspective, we need a place to **store our datasets and a compute to run scripts** on a dataset in parallel. There are many ways to implement it, starting from local execution and up to running the service in Databricks, Fabric or Azure ML. For example, AI Studio, as a primary tool to manage LLMs in Azure, can also be used to preserve datasets.\n\nOnce we have ground truth and output from the LLM Application Service, we can use the data in the evaluation pipeline utilizing pre-defined or custom evaluators. Azure AI Evaluation SDK can run evaluators locally or publish custom evaluators into AI Studio and run them remotely. Evaluation results can be published in AI Studio or uploaded to a desired system. The diagram below illustrates the evaluation component:\n\n![Evaluation](./docs/images/EvaluationComponents.jpeg)\n\nNow, if we stick all the components together, we will get the following high-level architecture:\n\n![Overall Architecture](./docs/images/OverallArchitecture.jpeg)\n\n\n### DevOps Components\n\nWe should have Development and Operations (DevOps, MLOps, or even LLMOps) that invoke all the described components in the correct order and support abilities to do experimentation and testing in an environment where several data scientists and software engineers are working together at the same service. It’s a very well-known area, and as a starting point, we can propose a DevOps flow that includes three elements: **PR, CI and CD Builds**, which are explained in detail below.\n\n**PR Build**: Data Scientists and Software Engineers are tuning the service into their feature branches. After passing quality checks, PR Build integrates/merges the changes into the development or main branch.\n\n- Linting and Unit Testing to guarantee code quality and provide testing results.\n- The evaluation step is where the application service code should be executed to prepare the evaluation set, and evaluators should be applied. Evaluation results should be published in a shared storage for review. If the evaluation process is time-consuming, it’s possible to use a small toy dataset to speed up the process. Still, the entire dataset should be evaluated in the next Build.\n- Final approval from some team members before the merge.\n\n**CI Build**: This should be executed on every merge to generate deployment artifacts.\n\n- Evaluation should be executed on the full dataset (if not done in PR Build).\n- LLM Application Service should be deployed into the development environment to make it available for developers of other solution components (like UI).\n- The LLM Application Service deployment should be validated to guarantee that we don’t have issues with the deployment scripts.\n- Deployment artifacts should be created and signed/approved as potential artifacts that may be deployed into the production environment.\n\n**CD Build**: Deployment and validation of the service into the production environment.\n\n- Deployment and validation of the approved artifact into the QA environment.\n- After the QA deployment is validated/approved, the artifact is ready for the production environment. The Blue/Green Deployment approach can be applied for the final validation and A/B Testing (separating traffic between old and new deployments).\n- After the final validation/testing, the new deployment is converted into the primary one.\n\nFor example, the proposed set of Builds can be implemented as a set of GitHub actions.\n\n\n## Implementation Details\n\n### Folder Structure\n\nThe repository contains elements and examples to simplify the development and operationalization of the LLM Application Service that we described above.\nThe template contains examples of the service in several different programming languages, and each language has its folder:\n\n-\tCSharp: for C#\n-\tPython: for Python\n-\tJava: for Java\n\nThe language folders are located in the top level of the repository alongside the following folders:\n\n-\t.github: to support GitHub workflows that is our primary DevOps system for the repository\n-\t.devcontainer: to support the development environment in the container and integrate it with VSCode\n-\tevaluators: we are using Azure AI Evaluation SDK that supports Python for now. So, we are implementing all custom evaluators in Python. Later, once we have support for other languages, we will replicate the folder for C# and Java.\n\nEach demonstrated flow should have its folder under the appropriate language folder, and there is the following structure for it:\n\n-\tconfig: This contains a configuration file (YAML for Python) with the parameters needed to execute the flow (like subscription ID and project name). Some parameters in this configuration come from environment variables.\n-\tdata: dataset examples to test and evaluate the flow.\n-\t{flow name}: the flow itself.\n-\tevaluate: evaluation scripts for the flow that use custom and embedded evaluators to evaluate the flow and publish results.\n-\texecutors: scripts to execute the flow locally or in the cloud.\n-\tdeployment: demonstrates how to deploy the flow using various targets. It can include Azure Function implementation, Fast API Kubernetes and so on.\n\nIn addition to the folders above, we will need to host some common code. We assume to have some shared code for configurators, deployment, and executors. So, we can have a common folder under each language folder. The folder will include the following subfolders:\n\n-\tdeployments\n-\tconfigurator\n-\texecutors\n\n### Financial Analyst as a flow example\n\nAs an example of the LLM Application Service we are using a financial health analysis tool that leverages Semantic Kernel, external APIs, and Large Language Models (LLMs) to analyze financial statements, news articles, and stock prices to generate a consolidated financial health report of public companies. The service is complex enough to demonstrate some Semantic Kernel features. At the same time, it doesn't require any custom Data Retrieval Service and data pre-processing workloads. More details about the service can be found in [this document for Python](./python/sk_financial_analyst/README.md).\n\n### Deployment\n\nThere are many different ways to deploy the provided example, and we use a few of them to demonstrate end-to-end operationalization process:\n\n- [Durable Azure Functions](https://learn.microsoft.com/en-us/azure/azure-functions/durable/durable-functions-overview): as a way to publish complex flows that are supposed to be real-time but have high latency due to their complexity.\n- [Azure Kubernetes Service](https://learn.microsoft.com/en-us/azure/aks/what-is-aks): to demonstrate how to use Infrastructure as a Service to deploy LLM Application Services using Fast API.\n- [Azure Container Apps](https://learn.microsoft.com/en-us/azure/container-apps/): to demonstrate Platform as a Service approach for deployment.\n- [Azure Machine Learning Batch endpoints](https://learn.microsoft.com/en-us/azure/machine-learning/concept-endpoints-batch?view=azureml-api-2): to demonstrate an ability to publish the LLM Application Service as a component to process batches.\n\nThe repository demonstrates how to run all the deployments from above in parallel, but you can keep just needed targets or introduce own.\n\n### Tracing\n\nIn order to collect metrics from the LLM Application Service we are using [OpenTelemetry with Azure Monitor](https://learn.microsoft.com/en-us/azure/azure-monitor/app/opentelemetry-enable?tabs=aspnetcore). In this case we can use Azure Monitor to collect traces as well as performance of the deployment targets and build various dashboards based on that.\n\n### Batch Execution\n\nThe repository demonstrate several different Batch Executors:\n\n- **Local Executor:** demonstrates how to invoke the LLM Application Service on a local computer and generates output for the evaluation step.\n- **Azure Machine Learning:** thanks to parallel job in Azure ML we can utilize Azure ML Serverless Compute in order to process any amount of data in parallel.\n\n### Evaluation\n\nWe are using [Azure AI Evaluation SDK](https://learn.microsoft.com/en-us/azure/ai-studio/how-to/develop/evaluate-sdk) to run evaluation workloads. The SDK can be used to run evaluation locally or in the cloud using AI Studio serverless compute. In both cases it's possible to store evaluation results and compare experiments in AI Studio.\n\nThe SDK supports Python only as for now, but since we are using batch executor output dataset (rather than the flow itself) as an input for evaluation, we are able to utilize the SDK for any programming language.\n\n## How to start working with the template\n\nTBD\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us the rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft\ntrademarks or logos is subject to and must follow\n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n"
    },
    {
      "name": "Hakurei-Reimu-Gensokyo/text2sqlWithRHLF_MAC_SQL_2024",
      "stars": 5,
      "img": "https://avatars.githubusercontent.com/u/42296668?s=40&v=4",
      "owner": "Hakurei-Reimu-Gensokyo",
      "repo_name": "text2sqlWithRHLF_MAC_SQL_2024",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2024-10-21T08:27:32Z",
      "updated_at": "2024-10-21T15:17:11Z",
      "topics": [],
      "readme": "# Text2SQL ——基于RLHF的多智能体部署方式\n\n## 配置方法：\n\n1. 创建虚拟环境并指定Python版本：\n\n    ```bash\n    conda create metagpt python=3.10\n    ```\n\n2. 激活虚拟环境：\n\n    ```bash\n    conda activate metagpt\n    ```\n\n3. 安装依赖项：\n\n    ```bash\n    conda install --file requirements.txt\n    ```\n\n## PLARF：\n\n1. 进入PLARF目录：\n\n    ```bash\n    cd PLARF\n    ```\n\n2. 运行Web演示：\n\n    ```bash\n    python web_demo.py\n    ```\n## 效果\n![这是图片](https://cdn.nlark.com/yuque/0/2024/png/45116829/1729493806177-e251b3c7-b1d9-4dc6-a385-b2cb23eaef4a.png?x-oss-process=image%2Fformat%2Cwebp%2Fresize%2Cw_1125%2Climit_0)\n\n\n\n"
    },
    {
      "name": "pigg-ai/conductor",
      "stars": 5,
      "img": "https://avatars.githubusercontent.com/u/178526897?s=40&v=4",
      "owner": "pigg-ai",
      "repo_name": "conductor",
      "description": "AI software engineer",
      "homepage": null,
      "language": "Python",
      "created_at": "2024-09-15T01:13:58Z",
      "updated_at": "2024-12-31T07:24:46Z",
      "topics": [],
      "readme": "# Conductor\n\nThe AI software engineer that can actually ship, not just writing todo apps.\n\n## Getting Started\n\nrunning `bash scripts/setup_dev.sh` will setup dev environment.\n"
    },
    {
      "name": "Joining-AI/RepoAnnotator",
      "stars": 5,
      "img": "https://avatars.githubusercontent.com/u/154000096?s=40&v=4",
      "owner": "Joining-AI",
      "repo_name": "RepoAnnotator",
      "description": "为中/英文代码项目打上逐行中文注释的Agent",
      "homepage": "https://sjtujoining.com/",
      "language": "Python",
      "created_at": "2024-07-16T12:58:11Z",
      "updated_at": "2024-08-17T01:35:22Z",
      "topics": [
        "agent",
        "chinese",
        "codeanalysis",
        "llm"
      ],
      "readme": "# 🔧 RepoAnnotator\n\n[![Official Website](https://img.shields.io/badge/Official%20Website-sjtujoining.com-blue?style=for-the-badge&logo=world&logoColor=white)](https://sjtujoining.com)\n\n[![GitHub Repo stars](https://img.shields.io/github/stars/Joining-AI/RepoAnnotator?style=social)](https://github.com/Joining-AI/RepoAnnotator)\n\n## 快速开始\n\n> **步骤 0** - 安装 Python 3.11 或更高版本。[参见此处](https://www.tutorialsteacher.com/python/install-python) 获取详细指南。\n\n<br />\n\n> **步骤 1** - 下载项目\n\n```bash\n$ git clone https://github.com/Joining-AI/RepoAnnotator.git\n$ cd RepoAnnotator\n```\n\n<br />\n\n> **步骤 2** - 安装依赖项\n\n```bash\n$ pip install -r requirements.txt\n```\n\n<br />\n\n> **步骤 3** - 使用 API 密钥创建 `.env` 文件\n\n在项目根目录下创建 `.env` 文件，并填入以下内容：\n\n```\nQWEN_API=\n```\n\n将对应的 API 密钥填入等号右侧。\n\n<br />\n\n> **步骤 4** - 指定项目信息并导入类\n\n```python\nroot_folder = r\"D:\\Joining\\mem0-main\\mem0-main\"\nnew_root_folder = r'mem0'\nexclude_list=[r'D:\\Joining\\mem0-main\\mem0-main\\.github']\nfrom Applications.RepoAnnotator import RepoAnnotator\n```\n将 `root_folder` 替换为你的项目根目录路径，`new_root_folder` 替换为翻译后文件的目标文件夹路径，`exclude_list` 中填入你想要排除的目录或文件路径。\n<br />\n\n> **步骤 5** - 处理项目\n\n```python\nRepoAnnotator.run(root_folder, new_root_folder, exclude_list)\n```\n直接运行 `ipynb` 文件即可。\n\n<br />\n\n\n## ✉️ 支持 / 联系我们\n\n- [社区讨论区](https://discord.gg/spBgZmm3Xe)\n- 我们的邮箱: inuyasha2023ch@gmail.com\n\n## 🛡 免责声明\n\n本项目 \"RepoAnnotator \"是一个实验性应用程序，按 \"现状 \"提供，不做任何明示或暗示的保证。我们根据 MIT 许可分享用于学术目的的代码。本文不提供任何学术建议，也不建议在学术或研究论文中使用。\n\n---\n\n<p align=\"center\">\n<a href=\"https://star-history.com/#Joining-AI/RepoAnnotator\">\n  <picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://api.star-history.com/svg?repos=Joining-AI/RepoAnnotator&type=Date&theme=dark\" />\n    <source media=\"(prefers-color-scheme: light)\" srcset=\"https://api.star-history.com/svg?repos=Joining-AI/RepoAnnotator&type=Date\" />\n    <img alt=\"Star History Chart\" src=\"https://api.star-history.com/svg?repos=Joining-AI/RepoAnnotator&type=Date\" />\n  </picture>\n</a>\n</p>\n\n---\n"
    },
    {
      "name": "renepajta/intro-to-intelligent-apps",
      "stars": 5,
      "img": "https://avatars.githubusercontent.com/u/6043628?s=40&v=4",
      "owner": "renepajta",
      "repo_name": "intro-to-intelligent-apps",
      "description": "This repository introduces and helps organizations get started with building Intelligent Apps and incorporating Large Language Models (LLMs) via AI Orchestration into them.",
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2024-02-20T09:28:21Z",
      "updated_at": "2024-03-03T15:42:32Z",
      "topics": [],
      "readme": "# Introduction to Building AI Apps\n\nThis repository introduces and helps organizations get started with building AI Apps and incorporating Large Language Models (LLMs) into them.\n\n## Workshop Agenda\n\nThe objective of this workshop is to practice realistic AI orchestration scenarios and to learn how to build intelligent apps.\nAt the end of the workshop you will: \n* Know how to use prompt engineering techniques for effective generative AI responses on OpenAI\n* Understand the implications of the usage of tokens and embeddings when interacting with an LLM\n* Have experience in leveraging AI orchestrators like Langchain/ Semantic Kernel with Azure OpenAI\n* Have evaluated different vector stores like Qdrant or Azure AI Search to enhance LLM responses with your data and context\n* Know how to turn a business scenario with data, context and user input into an intelligent application on Azure\n\n### 🌅 Morning (9:00 – 12:15)\n\n> *Focus: Introduction, First Steps & Prompt Engineering*\n* 📣 [Intro to Azure OpenAI, Prompt Engineering & Demos (105min)](presentations/README.md)\n  * Azure OpenAI Service\n  * Demo(s)\n  * Break\n  * 🧑🏼‍💻 [Lab #1 - Hands-on with Prompt Engineering Exercises](labs/01-prompts/README.md)\n* 📣 [Intro to AI Orchestration (60min)](presentations/README.md)\n  * AI Orchestration\n  * Demo(s)\n\n### 🌆 Afternoon (1:15 – 4:30)\n\n> *Focus: Building AI Apps & Incorporating LLMs*\n\n* 📣 [Intro to AI Orchestration Continued (135min)](presentations/README.md)\n  * 💻 [Lab #2 - Hands-on with Integrating AI Exercises](labs/02-integrating-ai/README.md)\n  * 💻 [Lab #3 - Hands-on with AI Orchestration Exercises](labs/03-orchestration/README.md)\n  * 💻 [Lab #4 - Hands-on with Deploying AI Exercises](labs/04-deploy-ai/README.md)\n  * Break\n* Wrapping-up (60min)\n  * Use Case Validation\n  * QnA & Closing Remarks\n\n\n## Getting Started with Workshop Preparation\n\nThe steps in this section will take you through setting up Azure OpenAI and some configuration files so that you can complete all of the hands-on labs successfully.\n\n* [Preparation](labs/00-setup/README.md)\n\n## Post Workshop Next Steps\n\nWhen you're done with this workshop and ready to move on, the following may be useful.\n\n* [Next Steps](docs/next_steps.md)\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n"
    },
    {
      "name": "rajib76/semantic_kernel_examples",
      "stars": 5,
      "img": "https://avatars.githubusercontent.com/u/16340036?s=40&v=4",
      "owner": "rajib76",
      "repo_name": "semantic_kernel_examples",
      "description": "This repo will contain the semantic kernel examples",
      "homepage": null,
      "language": "Python",
      "created_at": "2023-08-13T04:36:05Z",
      "updated_at": "2023-12-09T05:23:14Z",
      "topics": [],
      "readme": "# semantic_kernel_examples\nThis repo will contain the semantic kernel examples\n"
    },
    {
      "name": "MIBlue119/storystudio",
      "stars": 5,
      "img": "https://avatars.githubusercontent.com/u/9137836?s=40&v=4",
      "owner": "MIBlue119",
      "repo_name": "storystudio",
      "description": "Welcome to **StoryStudio**! 🌟 Here, we're reimagining the art of storytelling by blending human imagination with the power of cutting-edge AI. Dive into an innovative experimets where stories are not just read, but experienced.",
      "homepage": null,
      "language": "Python",
      "created_at": "2023-08-13T03:18:43Z",
      "updated_at": "2023-12-09T03:41:58Z",
      "topics": [],
      "readme": "# 🎬 StoryStudio - Your Personalized Storytelling Studio 📖\n<p align=\"center\">\n• 🐦 <a href=\"https://twitter.com/miblue119\" target=\"_blank\">Twitter</a>  \n• 👨️ <a href=\"https://miblue119.github.io/\" target=\"_blank\">Weiren Lan</a> \n    <br/><br/>\n    <img src=\"./assets/logo.png\" width=\"100\"> <br/>\n\n</p>\n\n## Table of Contents\n- [Introduction](#-storystudio---your-personalized-storytelling-studio-)\n- [Showcase](#🎥-showcase-full-video-on-youtube)\n- [How does it work?](#-how-does-it-work)\n- [Flows of storystudio](#-flows-of-storystudio)\n- [Installation](#installation)\n- [Usage](#usage)\n- [References](#references)\n---\nWelcome to **StoryStudio**! 🌟 Here, we're reimagining the art of storytelling by blending human imagination with the power of cutting-edge AI. Dive into an innovative experimets where stories are not just read, but experienced.\n\n## 🎥 Showcase ([Full video on YouTube](https://www.youtube.com/watch?v=NEp_huFPfa0))\nhttps://github.com/MIBlue119/storystudio/assets/9137836/f0d37318-0760-49c3-9969-3591a1c62fe6\n\n## ✨ How does it work?\n\nEver thought about turning a simple topic or idea into a mesmerizing story, complete with visuals, voice narration, and music? That's exactly what StoryStudio promises!\n\n## 🚀 Flow of storystudio\n1. **Choose your Story's Essence:** Start with inputting a topic or a short description. This seed will be the heart of your personalized story.\n2. **Let the Magic Begin:** \n   - GPT-4 gets to work and spins a tale based on your inputs.\n   - The crafted story is then deconstructed into a dynamic storyboard - breaking down backgrounds, themes, characters, and even scene-specific music.\n   - The story is also seamlessly split into a voice-over script.\n3. **Making it Real:**\n   - The script is transformed into engaging narration via Azure Cognitive API's Text-To-Speech (TTS).\n   - Visuals for the storyboard frames come alive with the prowess of Stability API.\n   - Scene music isn't just generic; it's crafted using the Replicate API to resonate with the story's mood.\n4. **Integration Symphony:**\n   - Synthesis of TTS-generated sound, word by word.\n   - A harmonious blend of generated images.\n   - The magic of music.\n   - Everything is connected to give you a story experience like never before!\n\n## Installation\n\nTo set up storystudio on your machine, simply run:\n```bash\n$ poetry install\n```\n\n## Usage\n### Prepare API keys\n1. **OpenAI (LLM):** \n   - **Key Name:** `OPENAI_API_KEY`\n   - **Purpose:** For LLM usage.\n   - **Acquisition:** Follow the [OpenAI documentation](https://help.openai.com/en/articles/4936850-where-do-i-find-my-secret-api-key) to obtain your OpenAI API key.\n\n2. **Azure Speech Service (TTS):**\n   - **Key Names:** `AZURE_SPEECH_KEY` and `AZURE_SPEECH_REGION`\n   - **Purpose:** For TTS functionality.\n   - **Acquisition:** Refer to the [Azure documentation](https://learn.microsoft.com/en-us/azure/ai-services/speech-service/get-started-text-to-speech?tabs=macos%2Cterminal&pivots=programming-language-python) to activate the Azure speech service and retrieve the necessary keys.\n\n3. **Replicate (MusicGen):**\n   - **Key Name:** `REPLICATE_API_TOKEN`\n   - **Purpose:** Call the MusicGen API at Replicate to generate background music.\n   - **Acquisition:** Follow the instructions provided in the [Replicate documentation](https://replicate.com/docs/get-started/python) to obtain your API token.\n\n4. **Stability (SDXL):**\n   - **Key Name:** `STABILITY_KEY`\n   - **Purpose:** Interface with Stability SDXL for image generation.\n   - **Acquisition:** Visit the [Stability documentation](https://platform.stability.ai/docs/getting-started/authentication) to get your key.\n\n5. Copy `.env.example` to `.env` and fill the key values\n### Happy Generating with CLI\n```\n$poetry run python storystudio/generate.py\n```\n![cli](./assets/cli.jpg)\n\n### Use at python\n```\nfrom storystudio.generate import Studio\n\nyour_studio = Studio()\nseed_story=\"HarryPotter is transferred to Naruto's world\"\nexport_dir=\"./mystory\" #Change to your desired path\nyour_studio.run(seed_story=seed_story, export_dir=export_dir)\n```\n\n\n## References\n- Main structures refer to [MetaGPT](https://github.com/geekan/MetaGPT)\n- https://github.com/RayVentura/ShortGPT\n"
    },
    {
      "name": "embedelite/sk-hackathon",
      "stars": 5,
      "img": "https://avatars.githubusercontent.com/u/128965123?s=40&v=4",
      "owner": "embedelite",
      "repo_name": "sk-hackathon",
      "description": "The EmbedElite Semantic Kernel plugin submission for the SK Hackathon",
      "homepage": "https://www.embedelite.com",
      "language": "Python",
      "created_at": "2023-07-23T17:00:58Z",
      "updated_at": "2023-08-27T17:20:09Z",
      "topics": [],
      "readme": "\n<div align=\"center\">\n  <a href=\"https://www.embedelite.com/\">\n    <img src=\"content/logo.png\" alt=\"Logo\" width=\"238\">\n  </a> \n</div>\n\n<p align=\"center\">\n    Relevant LLM-ready assets for professionals. <br/>\n\n<br>\n\n> 🥳 We are among the winners of the SK Hackathon! Thanks to the SK Team and community for your interest and votes. Since the hackathon has ended, we had to deactivate the API key used in this repo. Also we are going to refine our plugin/provide more integration examples soon. If you would like to get access to our API or discover more embeddings/RAG solutions, please check our website [www.embedelite.com]([linkurl](https://www.embedelite.com))\n\n\n# SK Hackathon EmbedElite Plugin\n\nThis project is creating a plugin for the EmbedElite marketplace for the SK Hackathon. The Sementic Kernel plugin facilitates fetching ready-made premium context based on embeddings via an API.\n\n\n# About EmbedElite\n\nEmbedElite is a marketplace for LLM assets, i.e., data chunks, priced by demand and queried via Retrieval-Augmented Generation (“RAG”)\nOn one side, data vendors feed information onto the platform and are compensated per usage of their data and retrieval algorithm\nOn the other side, data consumers perform queries enriched by the relevant assets and algorithms, paying per query and tokens retrieved\n\n![](content/image1.png)\n\n# How to Use EmbedElite\n\n### Endpoints for Data Consumers\n\n#### Request Type: POST\n\nURL:\n\n```http request\nhttps://api.embedelite.com/query\n```\n\nHeaders:\n\n```http request\nAPI-Key: <YOUR_API_KEY>\nContent-Type: application/json\n```\n\nBody (JSON):\n\n```json\n{\n  \"query\": \"<your_query_to_make>\",\n  \"product_id\": \"<the_product_id>\",\n  \"rag_id\": \"<the_rag_id>\",\n  \"price_floor\": 1.0,\n  \"price_cap\": 5.0,\n  \"currency\": \"EUR\"\n}\n```\ncUrl example:\n```bash\ncurl -X POST https://api.embedelite.com/query \\\n-H \"API-Key: sk-ee-9m839d3n98nh39fh9f3mhe98h3\" \\\n-H \"Content-Type: application/json\" \\\n-d '{\n  \"query\": \"What are the VAT rules in Germany if I sell services?\",\n  \"product_id\": \"vat-rules-eu\",\n  \"rag_id\": \"RAG_aks298msd9nj34hncs\",\n  \"price_floor\": 1.0,\n  \"price_cap\": 5.0,\n  \"currency\": \"EUR\"\n}'\n```\n\nResponse Example:\n```json\n{\n  \"response\": \"If you sell services in Germany, you usually charge the German VAT rate of 19%. However, there are exceptions for broadcasting, telecommunication, and electronically-supplied services, which must be charged at the VAT rate of the customer's country. If your customer is in another EU country, your invoice must contain specific information, such as the customer's name and address, the date of the invoice, the VAT rate, and the total amount including VAT. If your customer is outside the European Union, you must not charge VAT.\",\n  \"updated_at\": \"2023-07-25\"\n  \"currency\": \"EUR\",\n  \"paid\": 2.3,\n  \"originalQuery\": \"What are the VAT rules in Germany if I sell services?\",\n  \"price_cap\": 2.3,\n  \"price_floor\": 2.3,\n}\n```\n\n## For Vendors\n\nData vendors can simple push content via a POST endpoint to the platform. EmbedElite will keep the data confidential and intellectual ownership stays with the vendor. The ownership is defined in the metadata field. If you are a data vendor, please contact us for the data vendor API access: info@embedelite.com.\n\n\n## Requirements to use the plugin\n\n- [Python](https://www.python.org/downloads/) 3.8 or higher\n  - [Poetry](https://python-poetry.org/) for package handling and dependency management\n  - [Semantic Kernel Tools](https://marketplace.visualstudio.com/items?itemName=ms-semantic-kernel.semantic-kernel)\n\n## Sample Configuration\n\nA `.env` file housed within the project is used for configuring the sample. It contains API keys and other confidential settings.\n\nEnsure you possess an\n[Open AI API Key](https://openai.com/api/) or an\n[Azure Open AI service key](https://learn.microsoft.com/azure/cognitive-services/openai/quickstart?pivots=rest-api)\n\nCreate a new file titled `.env` by duplicating the `.env.example` file. Then transfer your API keys into the new `.env` file:\n\n```\nOPENAI_API_KEY=\"\"\nOPENAI_ORG_ID=\"\"\nAZURE_OPENAI_DEPLOYMENT_NAME=\"\"\nAZURE_OPENAI_ENDPOINT=\"\"\nAZURE_OPENAI_API_KEY=\"\"\nEMBEDELITE_API_KEY=\"\"\n```\n\n## Sample Execution\n\nWithin Visual Studio Code, press `F5` to run the console application. As defined in `launch.json` and `tasks.json`, Visual Studio Code will implement `poetry install` and then `python hello_world/main.py`\n\nTo build and execute the console application from the terminal, apply the following commands:\n\n```powershell\npoetry install\npoetry run python main.py\n```\n"
    },
    {
      "name": "ayushib4/smartFin",
      "stars": 5,
      "img": "https://avatars.githubusercontent.com/u/62727780?s=40&v=4",
      "owner": "ayushib4",
      "repo_name": "smartFin",
      "description": "Be your own finance bro. Take control of your finances with your finance guru alter ego--your personalized Mr. Wonderful.",
      "homepage": "",
      "language": "Python",
      "created_at": "2023-06-17T21:38:30Z",
      "updated_at": "2025-01-04T00:57:22Z",
      "topics": [],
      "readme": "# smartFin 💰\n\nWith [_smartFin_](https://github.com/ayushib4/smartFin), we’re trying to take the first step to improving both **quality of life and equality** by bringing smart financial advice catered to your banking data and financial goals–all through our agent, **Mr. Wonderful** 🕴️, fine-tuned to be the best version of a finance bro–personalized for you.\n\nWhen in doubt, ask _smartFin_ how one can make smarter financial decisions. Your transaction data and relevant inferences made by GPT is safely stored in smartFin's intelligent vector database, powered by [PineCone](https://www.pinecone.io/), which is specially prompt-engineered to semantically handpick the most relevant, and often the most regrettable 😔 transactions. Mr. Wonderful, a generative AI agent, then uses his financial prowess to provide expert insights and advice for your financial peace of mind.\n\n## Architectural Overview\n\n**Languages:** Python ∙ C++ ∙ Dart ∙ Ruby ∙ Swift\n\n**Frameworks and Tools:** Firebase ∙ Flutter ∙ Flask ∙ LangChain ∙ OpenAI ∙ HuggingFace ∙ PineCone ∙ Plaid\n\nThere are a couple of different key engineering modules that this project can be broken down into.\n\nIngesting Transaction Data and Computing Inferences We begin by developing querying purchase history periodically from the Plaid API. We then use prompt engineering to extract critical user psychographic data and spending behavior, along with some metrics of how useful the transaction was.\n\nEmbedding and Semantic Search We then embed the transaction data and inferences using HuggingFace's embedding model and GPT-4's inference model. Subsequently, we store these embeddings in PineCone and use its semantic search to find the most relevant transactions.\n\nGenerative AI Agent We prompt-engineer an agent depending on user's long and short-term goals to generate valid questions if the user doesn't pass in questions to the agent. These questions are used to query PineCone, which LangChain's Generative AI Agent then analyses and provides insight into how spending habits can be improved based on the goals provided by the user.\n\n## Development\n\n### `backend`\n\nSome files have `__main__` methods that contain simple tests/demos.\n\nPlease run this in a virtual environment. Tested with Python 3.11.1.\n\n```\npip install -r requirements.txt\n\nflask run [--debug]\n```\n\n### `smart_fin_flutter`\n\nIDK:)\n\nSee [CONTRIBUTING.md](CONTRIBUTING.md/)\n"
    },
    {
      "name": "arashaga/agents",
      "stars": 4,
      "img": "https://avatars.githubusercontent.com/u/1166344?s=40&v=4",
      "owner": "arashaga",
      "repo_name": "agents",
      "description": "Showcasing AI Agents",
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2025-01-23T22:03:56Z",
      "updated_at": "2025-04-23T02:07:20Z",
      "topics": [],
      "readme": "# Agents\r\n\r\nThis project is for the Data Management and AI Workshop being delivered.\r\n\r\n## Getting Started\r\n\r\nThere are a few steps to take to get started.\r\n\r\n### Running in Codespaces\r\n\r\nBest Option is to use the CodeSpaces that is designed to run this project. On the project page here, click on the green __Code__ button, and the change the tab to __Codespaces__ and select __Create codespaces on master__.\r\n\r\n![Codespaces Setup](docs/codespaces.png)\r\n\r\nThis will open up a new browser window with VSCode running inside.\r\n\r\n⚠️ NOTE: This can take several minutes.\r\n\r\n### Setting environment variables\r\n\r\nFind the .env-sample file, and rename it .env\r\n\r\nIn there, add the variables needed to run this project:\r\n\r\n```\r\nAZURE_API_BASE=\"\"\r\nAZURE_API_VERSION=\"2024-05-01-preview\"\r\nAZURE_OPENAI_DEPLOYMENT_NAME = \"gpt-4o\"\r\nTAVILY_API_KEY=\"\"\r\nAZURE_OPENAI_API_KEY=\"\"\r\nAZURE_OPENAI_ENDPOINT=\"\"\r\n```\r\n\r\nSave that file and you should be ready to run the notebook!\r\n\r\n## About\r\n\r\nThis project is designed to automate the process of content creation using multiple agents and tasks. The agents are configured to monitor financial news, analyze market data, create content, and ensure quality assurance. The tasks are defined to guide each agent in their specific roles.\r\n"
    },
    {
      "name": "microsoft/contractor",
      "stars": 4,
      "img": "https://avatars.githubusercontent.com/u/6154722?s=40&v=4",
      "owner": "microsoft",
      "repo_name": "contractor",
      "description": "A repository that implements a agentic system capable of evaluating security and quotization contracts based on a wide range of data sources.",
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2025-03-31T21:44:04Z",
      "updated_at": "2025-04-07T04:50:22Z",
      "topics": [],
      "readme": "# Project Name\n\nA multi-modal, agentic RAG implementation with Semantic Kernel that manages multi-modal information for chat applications.\n\n## Features\n\nThis project framework provides the following features:\n\n* Agentic implementation using Semantic Kernel\n* Plugins connecting AI Search for indexing and retrieving multi-modal information\n* ...\n\n## Getting Started\n\n### Prerequisites\n\n- Windows 10 or later / Ubuntu 23.04 or later\n- Check [pyproject.toml](src/pyproject.toml) for Python dependencies\n- Check [pyproject.toml](src/pyproject.toml) for TypeScript dependencies\n\n### Installation\n\n- Install [Python](https://www.python.org/downloads/)\n- Install [NPM](https://docs.npmjs.com/downloading-and-installing-node-js-and-npm) and [YARN](https://classic.yarnpkg.com/lang/en/docs/install/#windows-stable)\n- run [the configuration file for your system](.configure/conf-env.ps1)\n\n### Quickstart\n(Add steps to get up and running quickly)\n\n1. git clone [repository clone url]\n2. cd [repository name]\n3. ...\n\n\n## Demo\n\nA demo app is included to show how to use the project.\n\nTo run the demo, follow these steps:\n\n(Add steps to start up the demo)\n\n1.\n2.\n3.\n\n## Resources\n\nYou'll find more information on this project based on the proper documentation on each architectural component\n\n- Read the [FastAPI App Documentation](src/README.md)\n- Run the [tests](src/tests/)\n"
    },
    {
      "name": "samitugal/semantic-kernel-plugins",
      "stars": 4,
      "img": "https://avatars.githubusercontent.com/u/57317518?s=40&v=4",
      "owner": "samitugal",
      "repo_name": "semantic-kernel-plugins",
      "description": "This repository provides a collection of custom plugins for Microsoft Semantic Kernel, aiming to simplify development, enhance productivity, and enable seamless integration of diverse tools into AI-driven workflows.",
      "homepage": "",
      "language": "Python",
      "created_at": "2025-03-24T19:20:25Z",
      "updated_at": "2025-04-04T02:46:41Z",
      "topics": [
        "agent",
        "agent-based-framework",
        "plugins",
        "semantic-kernel"
      ],
      "readme": "# Semantic Kernel Plugins\n\n![Python 3.10+](https://img.shields.io/badge/Python-3.10%2B-blue)\n![License MIT](https://img.shields.io/badge/License-MIT-green)\n\nA collection of ready-to-use plugins for Microsoft's Semantic Kernel framework that enhances AI applications with database connectivity, shell operations, web search capabilities, Python code execution, and detailed logging.\n\n## 🚀 Overview\n\nSemantic Kernel Plugins provides a set of powerful, production-ready plugins for the Semantic Kernel Framework, eliminating the need to write your own plugins from scratch:\n\n* **PostgreSQL Plugin**: Connect and interact with PostgreSQL databases with ease\n* **MongoDB Plugin**: Use MongoDB databases directly in your AI workflows\n* **Shell Plugin**: Execute system commands safely across different operating systems\n* **Web Search Plugins**: Choose from multiple search providers (Tavily, Google, SerpAPI)\n* **Python Code Generator**: Generate and execute Python code safely\n\n## ✨ Features\n\n* **Multiple Database Integrations**: Connect to PostgreSQL and MongoDB databases\n* **Cross-Platform Shell Operations**: Run system commands with proper handling for Windows, Linux, and macOS\n* **Multi-Provider Web Search**: Choose from Tavily, Google Search, or SerpAPI for web searches\n* **Python Code Generation and Execution**: Generate and execute Python code in a controlled environment\n* **Enhanced Logging**: Track operations with detailed, colorful logs\n* **Cross-Platform Compatibility**: Works seamlessly across Windows, Linux, and macOS\n\n## 📋 Requirements\n\n* Python 3.10+\n* Semantic Kernel 1.0.0+\n\n## 📥 Installation\n\n```bash\npip install semantic-kernel-plugins\n```\n\n## 🚀 Quick Start\n\n### Database Plugins\n\n#### PostgreSQL Plugin\n\n```python\nimport os\nfrom dotenv import load_dotenv\nfrom semantic_kernel import Kernel\nfrom psycopg2 import connect\nfrom semantic_kernel_plugins.plugins.postgre import PostgrePlugin\n\n# Initialize connection\ndb_connection = connect(\n    host=os.getenv(\"DB_HOST\"),\n    database=os.getenv(\"DB_NAME\"),\n    user=os.getenv(\"DB_USER\"),\n    password=os.getenv(\"DB_PASSWORD\")\n)\n\n# Create kernel and add plugin\nkernel = Kernel()\nkernel.add_plugin(\n    PostgrePlugin(db_connection),\n    plugin_name=\"PostgreSQL\"\n)\n\n# Example usage\n# Execute SQL queries directly from your AI workflows\nresult = kernel.plugins[\"PostgreSQL\"].execute_query(\"SELECT * FROM users\")\n\n# Get table schemas\ntables = kernel.plugins[\"PostgreSQL\"].fetch_table_names()\nschema = kernel.plugins[\"PostgreSQL\"].fetch_table_schema(\"users\")\n```\n\n#### MongoDB Plugin\n\n```python\nfrom pymongo import MongoClient\nfrom semantic_kernel import Kernel\nfrom semantic_kernel_plugins.plugins.mongodb import MongoDBPlugin\n\n# Initialize MongoDB client\nclient = MongoClient(\"mongodb://localhost:27017/\")\n\n# Create kernel and add plugin\nkernel = Kernel()\nkernel.add_plugin(\n    MongoDBPlugin(client),\n    plugin_name=\"MongoDB\"\n)\n\n# Example usage\n# Check if database exists\nexists = kernel.plugins[\"MongoDB\"].database_exists(\"mydatabase\")\n\n# List all collections in a database\ncollections = kernel.plugins[\"MongoDB\"].list_collections(\"mydatabase\")\n\n# Get database statistics\nstats = kernel.plugins[\"MongoDB\"].get_database_stats(\"mydatabase\")\n```\n\n### Shell Plugin\n\n```python\nfrom semantic_kernel import Kernel\nfrom semantic_kernel_plugins.plugins.shell import ShellPlugin\n\n# Create kernel and add shell plugin\nkernel = Kernel()\nkernel.add_plugin(\n    ShellPlugin(),\n    plugin_name=\"Shell\"\n)\n\n# Execute shell commands safely across platforms\nresult = kernel.plugins[\"Shell\"].execute_shell_command(\"ls -la\")\n\n# You can also pass a list of arguments\nresult = kernel.plugins[\"Shell\"].execute_shell_command([\"python\", \"-c\", \"print('Hello from Python!')\"])\n```\n\n### Web Search Plugins\n\n#### Tavily Search Plugin\n\n```python\nimport os\nfrom semantic_kernel import Kernel\nfrom semantic_kernel_plugins.plugins.web.tavily_web_search import TavilySearchPlugin\nfrom semantic_kernel_plugins.logger.sk_logger import SKLogger\n\n# Setup logger for detailed search result tracking\nlogger = SKLogger(name=\"SearchApp\")\n\n# Create kernel and add web search plugin\nkernel = Kernel()\nkernel.add_plugin(\n    TavilySearchPlugin(\n        api_key=os.getenv(\"TAVILY_API_KEY\"),\n        search_depth=\"advanced\",\n        include_answer=True,\n        logger=logger\n    ),\n    plugin_name=\"TavilySearch\"\n)\n\n# Search the web with detailed results\nresults = kernel.plugins[\"TavilySearch\"].search(\"latest advancements in quantum computing\")\n```\n\n#### SerpAPI Search Plugin\n\n```python\nimport os\nfrom semantic_kernel import Kernel\nfrom semantic_kernel_plugins.plugins.web import SerpApiWebSearchPlugin\n\n# Create kernel and add SerpAPI search plugin\nkernel = Kernel()\nkernel.add_plugin(\n    SerpApiWebSearchPlugin(\n        api_key=os.getenv(\"SERPAPI_API_KEY\"),\n        engine=\"google\",\n        include_news=True  # Get additional news results\n    ),\n    plugin_name=\"SerpApiSearch\"\n)\n\n# Search the web with Google via SerpAPI\nresults = kernel.plugins[\"SerpApiSearch\"].search(\"current global economic trends\")\n```\n\n#### Google Search Plugin\n\n```python\nfrom semantic_kernel import Kernel\nfrom semantic_kernel_plugins.plugins.web import GoogleSearchPlugin\n\n# Create kernel and add Google search plugin\nkernel = Kernel()\nkernel.add_plugin(\n    GoogleSearchPlugin(\n        max_results=10,\n        advanced=True\n    ),\n    plugin_name=\"GoogleSearch\"\n)\n\n# Search the web directly with Google\nresults = kernel.plugins[\"GoogleSearch\"].google_search(\"best practices for cloud security\")\n```\n\n## 📦 Available Plugins\n\n| Plugin | Description |\n|--------|-------------|\n| **Database Plugins** |\n| PostgrePlugin | Execute queries and manage PostgreSQL databases |\n| MongoDBPlugin | Interact with MongoDB databases and collections |\n| **System Plugins** |\n| ShellPlugin | Execute shell commands across different platforms safely |\n| **Web Search Plugins** |\n| TavilySearchPlugin | AI-powered search with summarization via Tavily |\n| SerpApiWebSearchPlugin | Comprehensive search results via SerpAPI (Google) |\n| GoogleSearchPlugin | Direct Google search integration |\n| **Development Plugins** |\n| PythonCodeGeneratorPlugin | Generate and execute Python code safely |\n| CalculatorPlugin | Perform mathematical calculations |\n\n## 🔍 Detailed Plugin Features\n\n### PostgreSQL Plugin\n- Execute arbitrary SQL queries\n- Fetch table names and schemas\n- Insert, update, and delete data\n- Create and drop tables\n\n### MongoDB Plugin\n- List databases and collections\n- Check if databases or collections exist\n- Get database and collection statistics\n\n### Shell Plugin\n- Cross-platform command execution\n- Safe handling of command arguments\n- Proper error reporting and logging\n\n### Web Search Plugins\n- Multiple search providers for different needs\n- Configurable result formatting (markdown or JSON)\n- Rich search results with titles, snippets, and URLs\n- Advanced options for specialized searches (news, images, etc.)\n\n## 🔜 Coming Soon\n\n* .NET Plugins\n* SQLite Plugin\n* File System Plugin\n* More Cloud Service Integrations\n\n## 📄 License\n\nThis project is licensed under the MIT License - see the LICENSE file for details.\n\n## 🤝 Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.\n\n---\n\nBuilt with ❤️ for AI developers working with Microsoft's Semantic Kernel.\n"
    },
    {
      "name": "microsoft/Modernize-your-code-solution-accelerator",
      "stars": 4,
      "img": "https://avatars.githubusercontent.com/u/6154722?s=40&v=4",
      "owner": "microsoft",
      "repo_name": "Modernize-your-code-solution-accelerator",
      "description": null,
      "homepage": "",
      "language": "Python",
      "created_at": "2025-02-03T23:02:38Z",
      "updated_at": "2025-04-22T04:16:25Z",
      "topics": [
        "ai-azd-templates",
        "azd-templates"
      ],
      "readme": "# Modernize your code solution accelerator\n\nMENU: [**USER STORY**](#user-story) \\| [**QUICK DEPLOY**](#quick-deploy) \\| [**SUPPORTING DOCUMENTATION**](#supporting-documentation)\n\n<h2><img src=\"./docs/images/read_me/userStory.png\" width=\"64\">\n<br/>\nUser story\n</h2>\n\n### Overview\n\nWelcome to the *Modernize your code* solution accelerator, designed to help customers transition their SQL queries to new environments quickly and efficiently. This accelerator is particularly useful for organizations modernizing their data estates, as it simplifies the process of translating SQL queries from various dialects.\n\nWhen dealing with legacy code, users often face significant challenges, including the absence of proper documentation, loss of knowledge of outdated languages, and missing business logic that explains functional requirements.\n\nThe *Modernize your code* solution accelerator allows users to specify a group of SQL queries and the target SQL dialect for translation. It then initiates a batch process where each query is translated using a group of Large Language Model (LLM) agents. This automation not only saves time but also ensures accuracy and consistency in query translation.\n\n### Technical Key features\n\n<picture>\n  <source media=\"(prefers-color-scheme: dark)\" srcset=\".\\docs\\images\\read_me\\keyFeaturesDark.png\">\n  <source media=\"(prefers-color-scheme: light)\" srcset=\".\\docs\\images\\read_me\\keyFeaturesLight.png\">\n  <img src=\".\\docs\\images\\read_me\\keyFeaturesLight.png\" alt=\"KeyFeatures\">\n</picture>\n\n</br>\n</br>\n\nBelow is an image of the solution accelerator:\n\n<img src=\"./docs/images/read_me/webappHero.png\" alt=\"image\" style=\"max-width: 100%;\">\n\n</br>\n\n### Use case / scenario\n\nCompanies maintaining and modernizing their data estates often face large migration projects. They may have volumes of files in various dialects, which need to be translated into a modern alternative. Some of the challenges they face include:\n\n<ul><li>Difficulty analyzing and maintaining legacy systems due to missing documentation</li>\n<li>Time-consuming process to manually update legacy code and extract missing business logic</li>\n<li>High risk of errors from manual translations, which can lead to incorrect query results and data integrity issues</li>\n<li>Lack of available knowledge and expertise for legacy languages creates additional effort, cost, and reliance on niche skills</li></ul>\n\nBy using the *Modernize your code* solution accelerator, users can automate this process, ensuring that all queries are accurately translated and ready for use in the new modern environment.\n\nFor an in-depth look at the applicability of using multiple agents for this code modernization use case, please see the [supporting AI Research paper](./documentation/modernize_report.pdf).\n\nThe sample data used in this repository is synthetic and generated using Azure Open AI service. The data is intended for use as sample data only.\n\n### Solution architecture\n\n<img src=\"./docs/images/read_me/solArchitecture.png\" alt=\"image\" style=\"max-width: 100%;\">\n\n<br/>\n\n### Agentic architecture\n\n<img src=\"./docs/images/read_me/agentArchitecture.png\" alt=\"image\" style=\"max-width: 100%;\">\n\n<br/>\n\nThis diagram double-clicks into the agentic framework for the code conversion process. The conversion uses an agentic approach with each agent playing a specialized role in the process. The system gets a list of SQL files which are targeted for conversion. \n\n**Step 1:** The system loops through the list of SQL files, converting each file, starting by passing the SQL to the Migrator agent. This agent will create several candidate SQL files that should be equivalent. It does this to ensure that the system acknowledges that most of these queries could be converted in a number of different ways. *Note that the processing time can vary depending on Azure OpenAI service and cloud services.*\n\n**Step 2:** The Picker agent then examines these various possibilities and picks the one it believes is best using criteria such as simplicity, clarity of syntax, etc.\n\n**Step 3:** This query is sent to the Syntax checker agent which, using a command line tool designed to validate SQL syntax, checks to make sure the query should run without error.\n- **Step 3n:** If the Syntax checker agent finds potential errors, it then in Step 3n sends the query to a Fixer agent which will attempt to fix the problem. The Fixer agent then sends the fixed query back to the Syntax checker agent again. If there are still errors, the Syntax checker agent sends back to the Fixer agent to make another attempt. This iteration continues until, either there are no errors found, or a max number of allowed iterations is reached. If the max number is hit, error logs are generated for that query and stored in its Cosmos DB metadata. \n\n**Step 4:** Once the SQL is found to run without errors, it is sent for a final check to the Semantic checker agent. This agent makes sure that the query in the new syntax will have the same logical effects as the old query, with no extra effects. It can find edge cases which don’t apply to most scenarios, so, if it finds an issue, this issue is sent to the query logs, and the query is generated and the file will be present in storage, but its state will be listed as “warning”.  If no semantic issues are found, the query is generated and placed into Azure storage with a state of success.\n\n<h2><img src=\"./docs/images/read_me/quickDeploy.png\" width=\"64\">\n<br/>\nQUICK DEPLOY\n</h2>\n\n### **Prerequisites**\n\nTo deploy this solution accelerator, ensure you have access to an [Azure subscription](https://azure.microsoft.com/free/) with the necessary permissions to create **resource groups and resources**. Follow the steps in  [Azure Account Set Up](./docs/AzureAccountSetUp.md) \n\nCheck the [Azure Products by Region](https://azure.microsoft.com/en-us/explore/global-infrastructure/products-by-region/?products=all&regions=all) page and select a **region** where the following services are available:  \n\n- Azure AI Foundry \n- Azure OpenAI Service  \n- GPT Model Capacity\n\nHere are some example regions where the services are available: East US, East US2, Japan East, UK South, Sweden Central.\n\n### ⚠️ Important: Check Azure OpenAI Quota Availability  \n\n➡️ To ensure sufficient quota is available in your subscription, please follow **[Quota check instructions guide](./docs/quota_check.md)** before you deploy the solution.\n\n| [![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://codespaces.new/microsoft/Modernize-your-Code-Solution-Accelerator) | [![Open in Dev Containers](https://img.shields.io/static/v1?style=for-the-badge&label=Dev%20Containers&message=Open&color=blue&logo=visualstudiocode)](https://vscode.dev/redirect?url=vscode://ms-vscode-remote.remote-containers/cloneInVolume?url=https://github.com/microsoft/Modernize-your-Code-Solution-Accelerator) |\n|---|---|\n \n### **Configurable Deployment Settings**  \n\nWhen you start the deployment, most parameters will have **default values**, but you can update the following settings:  \n\n| **Setting** | **Description** |  **Default value** |\n|------------|----------------|  ------------|\n| **Azure Region** | The region where resources will be created. | East US| \n| **Resource Prefix** | Prefix for all resources created by this template. This prefix will be used to create unique names for all resources. The prefix must be unique within the resource group. | None |\n| **AI Location** | Location for all AI services resources. This location can be different from the resource group location | None |\n| **Capacity** | Configure capacity for **gpt-4o**. |  5k |\n\nThis accelerator can be configured to  use authentication. \n\n* To use authentication the installer must have the rights to create and register an application identity in their Azure environment.\nAfter installation is complete, follow the directions in the [App Authentication](./docs/AddAuthentication.md) document to enable authentication.\n* Note: If you enable authentication, all processing history and current processing will be performed for your specific user. Without authentication, all batch history from the tool will be visible to all users.\n\n### [Optional] Quota Recommendations  \nBy default, the **GPT model capacity** in deployment is set to **5k tokens**.  \n> **We recommend increasing the capacity to 200k tokens for optimal performance.** \n\nTo adjust quota settings, follow these [steps](./docs/AzureGPTQuotaSettings.md)\n\n### Deployment Options\nPick from the options below to see step-by-step instructions for: GitHub Codespaces, VS Code Dev Containers, Local Environments, and Bicep deployments.\n\n<details>\n  <summary><b>Deploy in GitHub Codespaces</b></summary>\n\n### GitHub Codespaces\n\nYou can run this solution using GitHub Codespaces. The button will open a web-based VS Code instance in your browser:\n\n1. Open the solution accelerator (this may take several minutes):\n\n    [![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://codespaces.new/microsoft/Modernize-your-Code-Solution-Accelerator)\n2. Accept the default values on the create Codespaces page\n3. Open a terminal window if it is not already open\n4. Continue with the [deploying steps](#deploying)\n\n</details>\n\n<details>\n  <summary><b>Deploy in VS Code</b></summary>\n\n ### VS Code Dev Containers\n\nYou can run this solution in VS Code Dev Containers, which will open the project in your local VS Code using the [Dev Containers extension](https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-containers):\n\n1. Start Docker Desktop (install it if not already installed)\n2. Open the project:\n\n    [![Open in Dev Containers](https://img.shields.io/static/v1?style=for-the-badge&label=Dev%20Containers&message=Open&color=blue&logo=visualstudiocode)](https://vscode.dev/redirect?url=vscode://ms-vscode-remote.remote-containers/cloneInVolume?url=https://github.com/microsoft/Modernize-your-Code-Solution-Accelerator)\n\n\n3. In the VS Code window that opens, once the project files show up (this may take several minutes), open a terminal window.\n4. Continue with the [deploying steps](#deploying)\n\n</details>\n\n<details>\n  <summary><b>Deploy in your local environment</b></summary>\n\n ### Local environment\n\nIf you're not using one of the above options for opening the project, then you'll need to:\n\n1. Make sure the following tools are installed:\n\n    * [Azure Developer CLI (azd)](https://aka.ms/install-azd)\n    * [Python 3.9+](https://www.python.org/downloads/)\n    * [Docker Desktop](https://www.docker.com/products/docker-desktop/)\n    * [Git](https://git-scm.com/downloads)\n\n2. Download the project code:\n\n    ```shell\n    azd init -t microsoft/Modernize-your-Code-Solution-Accelerator/\n    ```\n\n3. Open the project folder in your terminal or editor.\n\n4. Continue with the [deploying steps](#deploying).\n\n</details>\n\n### Deploying\n\nOnce you've opened the project in [Codespaces](#github-codespaces) or in [Dev Containers](#vs-code-dev-containers) or [locally](#local-environment), you can deploy it to Azure following the following steps. \n\nTo change the azd parameters from the default values, follow the steps [here](./docs/CustomizingAzdParameters.md). \n\n\n1. Login to Azure:\n\n    ```shell\n    azd auth login\n    ```\n\n    #### Note: To authenticate with Azure Developer CLI (`azd`) to a specific tenant, use the previous command with your **Tenant ID**:\n\n    ```sh\n    azd auth login --tenant-id <tenant-id>\n   ```\n\n2. Provision and deploy all the resources:\n\n    ```shell\n    azd up\n    ```\n\n3. Provide an `azd` environment name (like \"cmsaapp\")\n4. Select a subscription from your Azure account, and select a location which has quota for all the resources. \n    * This deployment will take *6-9 minutes* to provision the resources in your account and set up the solution with sample data. \n    * If you get an error or timeout with deployment, changing the location can help, as there may be availability constraints for the resources.\n\n5. Once the deployment has completed successfully, open the [Azure Portal](https://portal.azure.com/), go to the deployed resource group, find the container app with \"frontend\" in the name, and get the app URL from `Application URI`.\n\n6. You can now delete the resources by running `azd down`, when you have finished trying out the application. \n\n<h2>\nAdditional Steps\n</h2>\n\n1. **Deleting Resources After a Failed Deployment**\n\n     Follow steps in [Delete Resource Group](./docs/DeleteResourceGroup.md) If your deployment fails and you need to clean up the resources.\n\n1. **Add App Authentication**\n   \n    If you chose to enable authentication for the deployment, follow the steps in [App Authentication](./docs/AddAuthentication.md)\n\n## Running the application\n\nTo help you get started, sample Informix queries have been included in the data/informix/functions and data/informix/simple directories. You can choose to upload these files to test the application.\n\n<h2>\nResponsible AI Transparency FAQ \n</h2>\n\nPlease refer to [Transparency FAQ](./TRANSPARENCY_FAQ.md) for responsible AI transparency details of this solution accelerator.\n\n<h2>\nSupporting Documentation\n</h2>\n\n### Costs\n\nPricing varies per region and usage, so it isn't possible to predict exact costs for your usage.\nThe majority of the Azure resources used in this infrastructure are on usage-based pricing tiers.\nHowever, Azure Container Registry has a fixed cost per registry per day.\n\nYou can try the [Azure pricing calculator](https://azure.microsoft.com/en-us/pricing/calculator) for the resources:\n\n* Azure AI Foundry: Free tier. [Pricing](https://azure.microsoft.com/pricing/details/ai-studio/)\n* Azure Storage Account: Standard tier, LRS. Pricing is based on storage and operations. [Pricing](https://azure.microsoft.com/pricing/details/storage/blobs/)\n* Azure Key Vault: Standard tier. Pricing is based on the number of operations. [Pricing](https://azure.microsoft.com/pricing/details/key-vault/)\n* Azure AI Services: S0 tier, defaults to gpt-4o-mini. Pricing is based on token count. [Pricing](https://azure.microsoft.com/pricing/details/cognitive-services/)\n* Azure Container App: Consumption tier with 0.5 CPU, 1GiB memory/storage. Pricing is based on resource allocation, and each month allows for a certain amount of free usage. [Pricing](https://azure.microsoft.com/pricing/details/container-apps/)\n* Azure Container Registry: Basic tier. [Pricing](https://azure.microsoft.com/pricing/details/container-registry/)\n* Log analytics: Pay-as-you-go tier. Costs based on data ingested. [Pricing](https://azure.microsoft.com/pricing/details/monitor/)\n* Azure Cosmos DB: [Pricing](https://azure.microsoft.com/en-us/pricing/details/cosmos-db/autoscale-provisioned/)\n\n⚠️ To avoid unnecessary costs, remember to take down your app if it's no longer in use,\neither by deleting the resource group in the Portal or running `azd down`.\n\n### Security guidelines\n\nThis installs Azure Key Vault for use by AI Foundry.\n\nThis template uses [Managed Identity](https://learn.microsoft.com/entra/identity/managed-identities-azure-resources/overview) for all Azure service communication.\n\nTo ensure continued best practices in your own repository, we recommend that anyone creating solutions based on our templates ensure that the [Github secret scanning](https://docs.github.com/code-security/secret-scanning/about-secret-scanning) setting is enabled.\n\nYou may want to consider additional security measures, such as:\n\n* Enabling Microsoft Defender for Cloud to [secure your Azure resources](https://learn.microsoft.com/azure/security-center/defender-for-cloud).\n* Protecting the Azure Container Apps instance with a [firewall](https://learn.microsoft.com/azure/container-apps/waf-app-gateway) and/or [Virtual Network](https://learn.microsoft.com/azure/container-apps/networking?tabs=workload-profiles-env%2Cazure-cli).\n\n**Additional resources**\n\n- [Azure AI Foundry documentation](https://learn.microsoft.com/en-us/azure/ai-studio/)\n- [Semantic Kernel Agent Framework](https://learn.microsoft.com/en-us/semantic-kernel/frameworks/agent/?pivots=programming-language-python)\n- [Azure Cosmos DB Documentation](https://learn.microsoft.com/en-us/azure/cosmos-db/)\n- [Azure OpenAI Service - Documentation, quickstarts, API reference - Azure AI services | Microsoft Learn](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/use-your-data)\n- [Azure Container Apps documentation](https://learn.microsoft.com/en-us/azure/container-apps/)\n\n\n## Disclaimers\n\nTo the extent that the Software includes components or code used in or derived from Microsoft products or services, including without limitation Microsoft Azure Services (collectively, “Microsoft Products and Services”), you must also comply with the Product Terms applicable to such Microsoft Products and Services. You acknowledge and agree that the license governing the Software does not grant you a license or other right to use Microsoft Products and Services. Nothing in the license or this ReadMe file will serve to supersede, amend, terminate or modify any terms in the Product Terms for any Microsoft Products and Services. \n\nYou must also comply with all domestic and international export laws and regulations that apply to the Software, which include restrictions on destinations, end users, and end use. For further information on export restrictions, visit https://aka.ms/exporting. \n\nYou acknowledge that the Software and Microsoft Products and Services (1) are not designed, intended or made available as a medical device(s), and (2) are not designed or intended to be a substitute for professional medical advice, diagnosis, treatment, or judgment and should not be used to replace or as a substitute for professional medical advice, diagnosis, treatment, or judgment. Customer is solely responsible for displaying and/or obtaining appropriate consents, warnings, disclaimers, and acknowledgements to end users of Customer’s implementation of the Online Services. \n\nYou acknowledge the Software is not subject to SOC 1 and SOC 2 compliance audits. No Microsoft technology, nor any of its component technologies, including the Software, is intended or made available as a substitute for the professional advice, opinion, or judgement of a certified financial services professional. Do not use the Software to replace, substitute, or provide professional financial advice or judgment.  \n\nBY ACCESSING OR USING THE SOFTWARE, YOU ACKNOWLEDGE THAT THE SOFTWARE IS NOT DESIGNED OR INTENDED TO SUPPORT ANY USE IN WHICH A SERVICE INTERRUPTION, DEFECT, ERROR, OR OTHER FAILURE OF THE SOFTWARE COULD RESULT IN THE DEATH OR SERIOUS BODILY INJURY OF ANY PERSON OR IN PHYSICAL OR ENVIRONMENTAL DAMAGE (COLLECTIVELY, “HIGH-RISK USE”), AND THAT YOU WILL ENSURE THAT, IN THE EVENT OF ANY INTERRUPTION, DEFECT, ERROR, OR OTHER FAILURE OF THE SOFTWARE, THE SAFETY OF PEOPLE, PROPERTY, AND THE ENVIRONMENT ARE NOT REDUCED BELOW A LEVEL THAT IS REASONABLY, APPROPRIATE, AND LEGAL, WHETHER IN GENERAL OR IN A SPECIFIC INDUSTRY. BY ACCESSING THE SOFTWARE, YOU FURTHER ACKNOWLEDGE THAT YOUR HIGH-RISK USE OF THE SOFTWARE IS AT YOUR OWN RISK.  \n"
    },
    {
      "name": "microsoft/multi-agent-calibrator",
      "stars": 4,
      "img": "https://avatars.githubusercontent.com/u/6154722?s=40&v=4",
      "owner": "microsoft",
      "repo_name": "multi-agent-calibrator",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2025-03-11T20:46:44Z",
      "updated_at": "2025-04-16T18:37:13Z",
      "topics": [],
      "readme": "# Multi-Agent Auto Calibrator\nAutomatically Calibrate Semantic Kernel Project Code. \n\nThis is a tool to work with popular Multi-Agent frameworks  to continuously improve Multi-Agent system with speed and quality\n\nThe Multi-Agent calibrator could help,\n- Visualize the Multi-Agent structure, convey the logic to Developer and business stakeholder\n- Help to speed up the feedback addressing, by generating N variant ideas, and evaluate with ground truth with regression\n- Track the quality continuously with number\n\n# Conceptual overview of the Semi-Auto Calibrator\nThis research breaks down the Multi-Agent RAG system into modifiable or re-assemble components, making it eligible for generating component variants, and topology variants. The algorithm to do semi-auto calibration is to first (semi) auto generate N component variant or topology variant for a given use case. N variants make an experiment. To run the experiment, the calibrator updates multi-agent architecture object for each variant. Evaluate with a calibrator test set which includes the fields to do evaluation, i.e., expected answers, etc. By gathering all the evaluation metrics, the calibrator suggests the best variant. Then the developers could check in the code with suggested modifications and update the service automatically.\n\n![simplified-multi-agent-with-calibrator](./res/simplified-multi-agent-with-calibrator.png)\n\n# Local Development Setup \n\npython -m venv .venv\n.venv\\Scripts\\Activate.ps1\n\n# Project\n\n> This repo has been populated by an initial template to help get you started. Please\n> make sure to update the content to build a great experience for community-building.\n\nAs the maintainer of this project, please make a few updates:\n\n- Improving this README.MD file to provide a great experience\n- Updating SUPPORT.MD with content about this project's support experience\n- Understanding the security reporting process in SECURITY.MD\n- Remove this section from the README\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n"
    },
    {
      "name": "FabianSchurig/promptflow-tool-semantic-kernel",
      "stars": 4,
      "img": "https://avatars.githubusercontent.com/u/11216482?s=40&v=4",
      "owner": "FabianSchurig",
      "repo_name": "promptflow-tool-semantic-kernel",
      "description": "Creates a semantic_kernel llm chat tool for promptflow.",
      "homepage": null,
      "language": "Python",
      "created_at": "2025-03-03T17:37:22Z",
      "updated_at": "2025-04-02T10:52:18Z",
      "topics": [],
      "readme": "# Promptflow Tool Semantic Kernel\n\nA Python package that integrates [Semantic Kernel](https://github.com/microsoft/semantic-kernel) with [Azure Prompt Flow](https://github.com/microsoft/promptflow), enabling efficient LLM application development.\n\n[![Quality Gate Status](https://sonarcloud.io/api/project_badges/measure?project=FabianSchurig_promptflow-tool-semantic-kernel&metric=alert_status)](https://sonarcloud.io/summary/new_code?id=FabianSchurig_promptflow-tool-semantic-kernel)\n[![Coverage](https://sonarcloud.io/api/project_badges/measure?project=FabianSchurig_promptflow-tool-semantic-kernel&metric=coverage)](https://sonarcloud.io/summary/new_code?id=FabianSchurig_promptflow-tool-semantic-kernel)\n[![Lines of Code](https://sonarcloud.io/api/project_badges/measure?project=FabianSchurig_promptflow-tool-semantic-kernel&metric=ncloc)](https://sonarcloud.io/summary/new_code?id=FabianSchurig_promptflow-tool-semantic-kernel)\n[![Maintainability Rating](https://sonarcloud.io/api/project_badges/measure?project=FabianSchurig_promptflow-tool-semantic-kernel&metric=sqale_rating)](https://sonarcloud.io/summary/new_code?id=FabianSchurig_promptflow-tool-semantic-kernel)\n[![Duplicated Lines (%)](https://sonarcloud.io/api/project_badges/measure?project=FabianSchurig_promptflow-tool-semantic-kernel&metric=duplicated_lines_density)](https://sonarcloud.io/summary/new_code?id=FabianSchurig_promptflow-tool-semantic-kernel)\n\n## Why?\n\nThis tool bridges the powerful execution flow of Promptflow with the advanced ReAct capabilities of Semantic Kernel, offering several advantages:\n\n- Easily pre-process or post-process data from your main assistant with minimal configuration.\n- Leverage Semantic Kernel's planning and reasoning capabilities within your Prompt Flow applications **by just providing configuration.**\n- Connect to a variety of LLM providers beyond OpenAI, including **Anthropic Claude, Amazon Bedrock, Llama, and more** through Semantic Kernel's connectors.\n- Access Semantic Kernel's growing plugin ecosystem to extend functionality without writing custom code.\n- Use Promptflow's UI and batch evaluation with your semantic kernel assistant.\n\nThe integration creates a best-of-both-worlds solution, combining Promptflow's orchestration capabilities with Semantic Kernel's flexibility and plugin architecture.\n\n\n## Installation\n\nInstall the package from PyPI:\n\n```bash\npip install promptflow-tool-semantic-kernel\n```\n\nYou can find the package on [PyPI](https://pypi.org/project/promptflow-tool-semantic-kernel/).\n\n## Usage\n\n### In VSCode Promptflow\n\nOnce installed, the Semantic Kernel tool will be available in your Promptflow tools collection:\n\n![New Tool in Sidebar](https://github.com/FabianSchurig/promptflow-tool-semantic-kernel/blob/e083a336c1c587b12c632f97365f01c0f0a9faa3/docs/promptflow_tools.png)\n\n1. Create a new promptflow in VSCode\n2. Add a custom LLM tool node\n3. Select \"Semantic Kernel LLM Tool\" from the tool list\n4. Configure the following parameters:\n    - Connection (Azure OpenAI or OpenAI)\n    - Deployment name (model name for OpenAI or deployment name for Azure)\n    - Chat history (optional)\n    - Plugins (optional)\n    - Customize your prompt as needed  \n      \n  \n![Semantic Kernel Chat](https://github.com/FabianSchurig/promptflow-tool-semantic-kernel/blob/e083a336c1c587b12c632f97365f01c0f0a9faa3/docs/vscode.png)\n\n## Running the Demo\n\nThe package includes a simple demo script:\n\n```bash\n# Set up environment variables\nexport AZURE_OPENAI_API_KEY=your_api_key\nexport AZURE_OPENAI_ENDPOINT=your_endpoint\nexport AZURE_OPENAI_DEPLOYMENT_NAME=your_deployment_name\n\n# Run the demo\npython -m scripts.main\n```\n\n## Using different connections\n\n### Google Gemini\n\nAdd a CustomConnection via promptflow in vscode as follows, important is that `api_type: \"google\"`:\n\n```yaml\n$schema: https://azuremlschemas.azureedge.net/promptflow/latest/CustomConnection.schema.json\nname: \"google_gemini\"\ntype: custom\nconfigs:\n  api_type: \"google\"\n  model_id: \"gemini-2.0-flash\"\nsecrets:\n  # Use'<no-change>' to keep original value or '<user-input>' to update it when the application runs.\n  api_key: \"<user-input>\"\n```\n\n## Adding Custom Plugins\n\nSemantic Kernel allows you to easily extend functionality through plugins. [Learn more about creating a native plugin](https://learn.microsoft.com/en-us/semantic-kernel/get-started/quick-start-guide?pivots=programming-language-python#create-a-native-plugin).\n\nHere's how to use plugins with this tool:\n\n### Built-in Plugins\n\nThe tool comes with a built-in `LightsPlugin` for demonstration:\n\n```python\n# Default plugin configuration\nplugins = [\n     {\n          \"name\": \"lights\",\n          \"class\": \"LightsPlugin\",\n          \"module\": \"promptflow_tool_semantic_kernel.tools.lights_plugin\"\n     }\n]\n```\n\n## Configuring with flow.dag.yaml\n\nYou can also configure the tool using a `flow.dag.yaml` file. This file defines the flow and its components, including the `semantic_kernel_chat` tool and its plugins. Here is an example configuration:\n\n```yaml\n# filepath: /workspaces/promptflow-tool-semantic-kernel/tests/system/flow.dag.yaml\n$schema: https://azuremlschemas.azureedge.net/promptflow/latest/Flow.schema.json\nenvironment:\n     python_requirements_txt: requirements.txt\nenvironment_variables:\n     PROMPTFLOW_SERVING_ENGINE: fastapi\n     PF_DISABLE_TRACING: \"false\"\ninputs:\n     chat_history:\n          type: list\n          is_chat_history: true\n          default: []\n     question:\n          type: string\n          is_chat_input: true\noutputs:\n     answer:\n          type: string\n          reference: ${semantic_kernel_chat.output}\n          is_chat_output: true\nnodes:\n- name: semantic_kernel_chat\n     type: custom_llm\n     source:\n          type: package_with_prompt\n          tool: promptflow_tool_semantic_kernel.tools.semantic_kernel_tool.semantic_kernel_chat\n          path: semantic_kernel_chat.jinja2\n     inputs:\n          connection: open_ai_connection\n          deployment_name: gpt-4\n          chat_history: ${inputs.chat_history}\n          question: ${inputs.question}\n          plugins: |\n               [\n               {\n                    \"name\": \"lights\",\n                    \"class\": \"LightsPlugin\",\n                    \"module\": \"promptflow_tool_semantic_kernel.tools.lights_plugin\"\n               }\n               ]\n```\n\nThis configuration allows you to leverage the power of plugins within your flow. You can define multiple plugins to extend the functionality of the `semantic_kernel_chat` tool. Each plugin is specified with its name, class, and module, making it easy to integrate and customize as needed.\n\n## Development\n\n### Setup\n\n1. Clone the repository\n     ```bash\n     git clone git@github.com:FabianSchurig/promptflow-tool-semantic-kernel.git\n     cd promptflow-tool-semantic-kernel\n     cp .devcontainer/devcontainer.env.example .devcontainer/devcontainer.env\n     ```\n2. Start the devcontainer with vs code\n3. Install development dependencies (should automatically run):\n     ```bash\n     poetry install\n     ```\n4. Activate the environment\n     ```bash\n     eval $(poetry env activate)\n     which python\n     uvicorn tests.system.api:app --workers 1 --port 5000\n     ```\n\n### Testing\n\nRun the tests with pytest:\n\n```bash\npoetry run pytest\npoetry run pytest --cov-report xml:coverage.xml --cov-report term --cov=promptflow_tool_semantic_kernel --cov-config=.coveragerc tests/\n```\n\n## License\n\nThis project is licensed under the GNU Affero General Public License v3.0 (AGPL-3.0) - see the LICENSE file for details."
    },
    {
      "name": "mxagar/generative_ai_udacity",
      "stars": 4,
      "img": "https://avatars.githubusercontent.com/u/7673198?s=40&v=4",
      "owner": "mxagar",
      "repo_name": "generative_ai_udacity",
      "description": "My personal notes, code and projects of the Udacity Generative AI Nanodegree. ",
      "homepage": "",
      "language": "Jupyter Notebook",
      "created_at": "2024-03-21T13:08:50Z",
      "updated_at": "2025-04-21T15:56:57Z",
      "topics": [
        "ai",
        "computer-vision",
        "data-science",
        "diffusion-models",
        "genai",
        "generative-ai",
        "llm",
        "llm-finetuning",
        "machine-learning",
        "rag"
      ],
      "readme": "# Udacity Generative AI Nanodegree: Personal Notes\n\nThese are my personal notes taken while following the [Udacity Generative AI Nanodegree](https://www.udacity.com/course/generative-ai--nd608).\n\nThe Nanodegree asssumes basic data analysis skills with data science python libraries and databases, and has 4 modules that build up on those skills; each module has its corresponding folder in this repository with its guide Markdown file:\n\n1. Generative AI Fundamentals: [`01_Fundamentals_GenAI`](./01_Fundamentals_GenAI/README.md).\n    - Foundation Models\n    - Fine-Tuning\n2. Large Language Models (LLMs) & Text Generation: [`02_LLMs`](./02_LLMs/README.md).\n    - Transformers and LLMs\n    - Retrieval Augmented Generation (RAG) Chatbots\n3. Computer Vision and Generative AI: [`03_ComputerVision`](./03_ComputerVision/README.md).\n    - Generative Adversarial Networks (GANs)\n    - Vision Transformers\n    - Diffusion Models\n4. Building Generative AI Solutions: [`04_BuildingSolutions`](./04_BuildingSolutions/README.md).\n    - Vector Databases\n    - LangChain and Agents\n\nAdditionally, it is necessary to submit and pass some projects to get the certification:\n\n- Project 1: Apply Lightweight Fine-Tuning to a Foundation Model - TBD.\n- Project 2: Build Your Own Custom Chatbot - TBD.\n- Project 3: AI Photo Editing with Inpainting - TBD.\n- Project 4: Personalized Real Estate Agent - TBD.\n\nFinally, also check some of my personal guides on related tools:\n\n- My personal notes on the O'Reilly book [Generative Deep Learning, 2nd Edition, by David Foster](https://github.com/mxagar/generative_ai_book)\n- My personal notes on the O'Reilly book [Natural Language Processing with Transformers, by Lewis Tunstall, Leandro von Werra and Thomas Wolf (O'Reilly)](https://github.com/mxagar/nlp_with_transformers_nbs)\n- [HuggingFace Guide: `mxagar/tool_guides/hugging_face`](https://github.com/mxagar/tool_guides/tree/master/hugging_face)\n- [LangChain Guide: `mxagar/tool_guides/langchain`](https://github.com/mxagar/tool_guides/tree/master/langchain)\n- [LLM Tools: `mxagar/tool_guides/llms`](https://github.com/mxagar/tool_guides/tree/master/llms)\n- [NLP Guide: `mxagar/nlp_guide`](https://github.com/mxagar/nlp_guide)\n- [Deep Learning Methods for CV and NLP: `mxagar/computer_vision_udacity/CVND_Advanced_CV_and_DL.md`](https://github.com/mxagar/computer_vision_udacity/blob/main/03_Advanced_CV_and_DL/CVND_Advanced_CV_and_DL.md)\n- [Deep Learning Methods for NLP: `mxagar/deep_learning_udacity/DLND_RNNs.md`](https://github.com/mxagar/deep_learning_udacity/blob/main/04_RNN/DLND_RNNs.md)\n\n<!--\nFinally, check these additional related courses:\n- [Udacity Course on Small Datasets and Synthetic Data](https://www.udacity.com/course/small-data--cd12528)\n-->\n\n## Setup\n\nA regular python environment with the usual data science packages should suffice (i.e., scikit-learn, pandas, matplotlib, etc.); any special/additional packages and their installation commands are introduced in the guides. A recipe to set up a [conda](https://docs.conda.io/en/latest/) environment with my current packages is the following:\n\n```bash\n# Create the necessary Python environment\n# NOTE: specific folders might require their own environment\n# and have their own requirements.txt\nconda create --name ds pip python=3.10\nconda activate ds\npip install -r requirements.txt\n\n# When the repository is cloned, initialize and update the submodules \ngit clone https://github.com/mxagar/generative_ai_udacity\ngit submodule update --init --recursive\n```\n\n## Interesting Links\n\n- [microsoft/generative-ai-for-beginners](https://github.com/microsoft/generative-ai-for-beginners)\n- ...\n\n## Credits\n\nMany of the contents in this repository were created following the [Udacity Generative AI Nanodegree](https://www.udacity.com/course/generative-ai--nd608).\n\nMikel Sagardia, 2024.  \nNo guarantees.\n"
    },
    {
      "name": "Azure/gpt-rag-securityhub",
      "stars": 4,
      "img": "https://avatars.githubusercontent.com/u/6844498?s=40&v=4",
      "owner": "Azure",
      "repo_name": "gpt-rag-securityhub",
      "description": "Security Hub for GPT-RAG",
      "homepage": null,
      "language": "Python",
      "created_at": "2024-07-02T12:25:39Z",
      "updated_at": "2025-01-07T17:09:36Z",
      "topics": [],
      "readme": "# Enterprise RAG Security Hub\n\nThis Security Hub is part of the **Enterprise RAG (GPT-RAG)** Solution Accelerator.\n\nTo learn more about the Enterprise RAG, please go to [https://aka.ms/gpt-rag](https://aka.ms/gpt-rag).\n\n### Cloud Deployment\n\nTo deploy the Security Hub in the cloud for the first time, please follow the deployment instructions provided in the [Enterprise RAG repo](https://github.com/Azure/GPT-RAG?tab=readme-ov-file#getting-started).  \n   \nThese instructions include the necessary infrastructure templates to provision the solution in the cloud.  \n   \nOnce the infrastructure is provisioned, you can redeploy just the orchestrator component using the instructions below:\n\nFirst, please confirm that you have met the prerequisites:\n\n - Azure Developer CLI: [Download azd for Windows](https://azdrelease.azureedge.net/azd/standalone/release/1.5.0/azd-windows-amd64.msi), [Other OS's](https://learn.microsoft.com/en-us/azure/developer/azure-developer-cli/install-azd).\n - Git: [Download Git](https://git-scm.com/downloads)\n - Python 3.10: [Download Python](https://www.python.org/downloads/release/python-31011/)\n\nThen just clone this repository and reproduce the following commands within the gpt-rag-orchestrator directory:  \n\n```\nazd auth login  \nazd env refresh  \nazd deploy  \n```\n\n> Note: when running the ```azd env refresh```, use the same environment name, subscription, and region used in the initial provisioning of the infrastructure.\n\n### Running Locally with VS Code  \n   \n[How can I test the solution locally in VS Code?](docs/LOCAL_DEPLOYMENT.md)\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft\ntrademarks or logos is subject to and must follow\n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n"
    },
    {
      "name": "rashedtalukder/ai-gbb-learning-shop",
      "stars": 4,
      "img": "https://avatars.githubusercontent.com/u/9218468?s=40&v=4",
      "owner": "rashedtalukder",
      "repo_name": "ai-gbb-learning-shop",
      "description": null,
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2024-05-17T07:00:58Z",
      "updated_at": "2025-04-18T18:57:48Z",
      "topics": [],
      "readme": "# Repository for Bi-Weekly AI GBB Hands-on Workshop\n\nTo begin, go to the **pricing** directory and open the README file from there.\n\n## Workshops\n1. Pricing - A cost calculator for Azure OpenAI usage. The application sits within an Azure Functions App. This workshop includes foundational setup and configuration of the tools required for development of Azure AI applications.\n2. Travel - An example travel assistant bot that uses Azure OpenAI Assistants APIs to retrieve airline policies and deterministically calculate flight times.\n3. Poet - Using an Azure OpenAI model to generate poems about cities with additional deterministic data using Prompt Flow and evaluating the results."
    },
    {
      "name": "Space3D-Bench/RAG3D-Chat",
      "stars": 4,
      "img": "https://avatars.githubusercontent.com/u/178774314?s=40&v=4",
      "owner": "Space3D-Bench",
      "repo_name": "RAG3D-Chat",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2024-08-28T19:55:19Z",
      "updated_at": "2025-04-18T12:56:35Z",
      "topics": [],
      "readme": "\n<p align=\"center\">\n<h1 align=\"center\"><strong>RAG3D-Chat as a baseline for Space3D-Bench</strong></h1>\n  <p align=\"center\">\n    <a href=\"https://emilia-szymanska.com\" target=\"_blank\">Emilia Szymanska</a>&emsp;\n    <a href=\"https://dusmanu.com/\" target=\"_blank\">Mihai Dusmanu</a>&emsp;\n    <a href=\"https://jwbuurlage.github.io/\" target=\"_blank\">Jan-Willem Buurlage</a>&emsp;\n    <a href=\"https://radmahdi.github.io/\" target=\"_blank\">Mahdi Rad</a>&emsp;\n    <a href=\"https://people.inf.ethz.ch/pomarc/\" target=\"_blank\">Marc Pollefeys</a>&emsp;\n    <br>\n    ETH Zurich&emsp;Microsoft, Zurich\n    <br>\n    <strong>ECCV 2024 Workshop</strong>\n  </p>\n</p>\n\n\n<p align=\"center\">\n  <a href=\"https://arxiv.org/abs/2408.16662\" target='_**blank**' disabled>\n    <img src=\"https://img.shields.io/badge/arXiv-2408.16662-green\">\n  </a> \n  <a href=\"https://arxiv.org/pdf/2408.16662\" target='_blank' disabled>\n    <img src=\"https://img.shields.io/badge/Paper-📖-green\">\n  </a> \n  <a href=\"https://space3d-bench.github.io/\" target='_blank'>\n    <img src=\"https://img.shields.io/badge/Project-&#x1F680-green\">\n  </a>\n</p>\n\n\n<div style=\"text-align: center;\">\n    <img src=\"assets/rag3d_chat.png\" alt=\"teaser\" width=100% >\n</div>\n\n\n<p align=\"justify\"> To generate answers for the spatial questions presented in <a href=\"https://github.com/Space3D-Bench/Space3D-Bench\">Space3D-Bench</a>, we propose <b>RAG3D-Chat</b>, a spatial Q&A system based on two main components: <a href=\"https://github.com/microsoft/semantic-kernel\">Semantic Kernel</a> (SK) and Retrieval Augmented Generation (RAG) within <a href=\"https://docs.llamaindex.ai/en/stable/\">Llama Index</a> framework. Semantic Kernel, being an open-source framework for LLM-based implementation of agents, allowed for integrating four complementary modules - each with different applications and limitations - into one system. Once the modules were implemented and described with a corresponding prompt, Semantic Kernel's planner was able to propose a chain of function calls, whose result would be an answer to the input question. </p>\n\n<div style=\"text-align: center;\">\n    <img src=\"assets/sk_result.png\" alt=\"teaser\" width=100% >\n</div>\n\n## 📋 Content\n\n### Released repository content\n\nIn this section, we will in detail describe the code-relevant content of this repository. It was assumed that API-call-based LLM/embedding models are used, however, it is adjustable as examplained in `Getting Started` section.\n\n<b>conf:</b>\n- <i><b>logging_conf.ini</b></i>: configuration of the logging, divided into two parts - a detailed one with all the planner's decisions and steps evoked by each module, saved to <i>detailed.log</i>, and a high-level one, where only the questions and the resulting answers are logged both to the console and <i>brief.log</i>.\n\n<details>\n\n  <summary>Example of the logs from <i>brief.log</i></summary>\n\n```\nWhich rooms have no plants?\nThe rooms that have no plants are the corridor and the dining room.\n```\n\n</details>\n\n\n<details>\n\n  <summary>Example of the logs from <i>detailed.log</i></summary>\n\n```\n2024-07-25 15:08:00,435 - plugins - INFO - Which rooms have no plants?\n2024-07-25 15:08:13,288 - SQL - INFO - Query: Which rooms contain plants?\n2024-07-25 15:08:48,991 - SQL - INFO - Most similar classes to []: set()\n2024-07-25 15:08:49,317 - SQL - INFO - Most similar classes to ['plants']: {'pillar', 'plant', 'pot', 'panel', 'vent'}\n2024-07-25 15:08:53,250 - SQL - INFO - SQL query: SELECT DISTINCT room FROM detected_objects WHERE class_name = 'plant';\nSQLResult: living_room, kitchen, bedroom\nAnswer: The rooms that contain plants are the living room, kitchen, and bedroom.\n2024-07-25 15:08:57,040 - SQL - INFO - Response: Plants are located in the living room, study, and bedroom.\n2024-07-25 15:09:00,796 - SQL - INFO - Query: List all rooms in the apartment\n2024-07-25 15:09:52,568 - SQL - INFO - SQL query: SELECT room FROM rooms\nSQLResult: bedroom, corridor, dining room, living room, study\nAnswer: The rooms in the apartment are the bedroom, corridor, dining room, living room, and a study.\n2024-07-25 15:09:54,841 - SQL - INFO - Response: The apartment consists of a bedroom, corridor, dining room, living room, and a study.\n2024-07-25 15:10:04,697 - plugins - INFO - The rooms that have no plants are the corridor and the dining room.\n2024-07-25 15:10:04,697 - plugins - INFO - ---\n2024-07-25 15:10:04,697 - plugins - INFO - Original request: Which rooms have no plants?\n\nYou are in the process of helping the user fulfill this request using the following plan:\nPlan:\n\n1. Use the `sql-Sql` function to query for all rooms that contain plants. The query could be something like \"Which rooms contain plants?\".\n2. Use the `sql-Sql` function again to query for all rooms in the apartment. The query could be something like \"List all rooms in the apartment\".\n3. Compare the two lists obtained from step 1 and step 2. The rooms that are in the list from step 2 but not in the list from step 1 are the rooms that have no plants.\n4. Use the `UserInteraction-SendFinalAnswer` function to return the final list of rooms that have no plants.\n\nThe user will ask you for help with each step.\n```\n\n</details>\n\n\n<b>core</b>\n- <i><b>config_handler.py</b></i>: definitions of classes used for handling the LLM/embedding models configurations such as endpoints, deployments, api keys etc. The configurations are read from dotenv files, and the settings corresponding to different components of the system are distinguished by a prefix (further explained in dotenv descriptions);\n- <i><b>interfaces.py</b></i>: definitions of interfaces of chat- and LLM/embedding-model-related components;\n- <i><b>example_implementations.py</b></i>: examples of how interfaces from `interfaces.py` can be implemented for the case of using Azure OpenAI services with Azure Identity authentication. The user of the repo will need to adjust their implementations to the type of models and services they are using;\n- <i><b>rag3dchat.py</b></i>: main class of the system, combines the planner from the Semantic Kernel with RAG modules;\n- <i><b>rag_document_loaders.py</b></i>: functions used for loading text and images into Llama Index's Documents;\n- <i><b>rag_sql_loader.py</b></i>: functions used for creating an SQL database from a JSON file (the file needs to follow the structure of Space3D-Bench's object detections file).\n\n<b>misc</b>\n- <i><b>scenes_enum.py</b></i>: enum class with the scene names from the <a href=\"https://github.com/facebookresearch/Replica-Dataset\">Replica dataset</a>, having vaues corresponding to the names of the folders containing the data of each scene. It is used when iterating over all the scenes, and can be extended with the scenes from other datasets;\n- <i><b>navmesh_vis.py</b></i>: function used for creating a visualization of a navigation mesh (with the path and desired points marked on it) and saving it to an HTML file.\n\n<b>plugins</b>\n- <i><b>{name}_plugin.py</b></i>: plugin implementations, whose more detailed way of operation is described in the paper;\n- <i><b>plugin_prompts.py</b></i>: all the prompts used in the plugins, both in the calls of chats and for the Semantic Kernel planning process;\n- <i><b>plugins_factory.py</b></i>: a factory used to get plugins objects.\n\n<b>repo's main folder</b>\n- <i><b>.env</b></i>: configurations of the Semantic Kernel's LLM model. Needs to be filled in by the user of the repo if services similar to OpenAI are used. Otherwise, the user can adjust the configuration handling depending on their own use case.\n- <i><b>.env_plugins</b></i>: configurations of the plugins-related LLM and embedding models. Needs to be filled in by the user of the repo if services similar to OpenAI are used. Otherwise, the user can adjust the configuration handling depending on their own use case.\n- <i><b>rag3dchat_call.py</b></i>: the script iterating over all the scenes and running the RAG3D-Chat for the questions available for each scene.\n\n#### Assumptions on the data-containing folder structure\n\nThe folder with data should be divided into subfolders, whose names (corresponding to the specific scenes accordingly) are as defined in `misc/scenes_enum.py`. Then, each subfolder should have the following structure:\n```\n├── img_data\n    ├── room_1\n        ├── img1.png\n        ├── img2.png\n    ├── room_2\n        ├── img1.png\n        ├── img2.png\n    ...\n├── nav_data\n    ├── navmesh.txt\n├── sql_data\n    ├── sql_db_data.json\n├── text_data\n    ├── room1.txt\n    ├── room2.txt\n    ...\n├── questions.json\n├── answers.json (if already generated)\n```\nSome notes:\n- **img_data**: although the image names do not matter, it is crucial that the folders in which they are put correspond to the room being named as the folder;\n- **nav_data**: the navigation mesh should be in the format as provided by the Space3D-Bench;\n- **sql_data**: the JSON file should be in the format as provided by the Space3D-Bench;\n- **text_data**: files with room descriptions should have the names corresponding to the room names they describe;\n- **questions.json**: keys should be the question numbers, and the content the actual question (as in Space3D-Bench);\n- **answers.json**: file generated by RAG3D-Chat, keys being the question numbers, and the content the actual answer (as advised by Space3D-Bench).\n\nIn the release, we provide a zipped folder with the data used in the first implementation and tests of RAG3D-Chat. Download it and unzip it, so that the repository has a `data` folder in the structure desribed above. \n\n\n## 🚀 Getting Started\n\n### Environment\n\n1. Download the repository.\n    ```bash\n    git clone https://github.com/Space3D-Bench/RAG3D-Chat.git\n    cd RAG3D-Chat\n    ```\n2. Prepare your python virtual environment (example shown for conda).\n    ```bash\n    conda create -n your_env_name python=3.10\n    conda activate your_env_name\n    ```\n3. Install the requirements.\n    ```bash\n    pip install -r requirements.txt\n    ```\n\n### Preparation\n\nAssumptions: we assume you would like to first test the chat on the Replica dataset. For simplicity, we advise you to create a `data` folder in this repository and put the data there. If you do not use Replica, keep the structure of folders as described in `Content` section and adjust `misc/scenes_enum.py`. If the data is in a different folder than `{path_to_repo}/data`, adjust paths in `rag3dchat_call.py`. We additionally assume that the contexts of RAG plugins will be saved to `.SQL_DIR`, `.TEXT_DIR` and `.IMG_DIR` - you can adjust it in `core/rag3dchat.py`.\n\nYou may use the Replica example we provide in the zipped `data` in the release in the following way:\n```bash\ncd path/to/repo\nwget https://github.com/Space3D-Bench/RAG3D-Chat/releases/download/v0.0.1/data.zip\nunzip data.zip -d .\nrm data.zip\n```\n\nThen, in the folders of each scene you need to add a JSON file with questions. You can download the ones provided by the Space3D-Bench or create ones yourself. \n\n#### Case 1: using Azure OpenAI with Azure Identity authentication\n\n1. Simply fill in both dotenv files in accordance to your configurations.\n\n#### Case 2: using OpenAI-like service with another authentication method\n\n1. Implement classes whose interfaces are present in `core/interfaces.py`. You may use the examples in `core/example_implementations.py` as the guideline.\n\n2. Import and use your implementations in `rag3dchat_call.py`, define the Semantic Kernel's service.\n\n3. Fill in both dotenv files in accordance to your configurations.\n\n#### Case 3: using another way of calling LLMs/embeddings (e.g. running them locally)\n\n1. Implement classes whose interfaces are present in `core/interfaces.py`. You may use the examples in `core/example_implementations.py` as the guideline. You may not need the handling of configurations as provided by `core/config_handler.py` and dotenv files, but make sure your models are configured.\n\n2. Import and use your implementations in `rag3dchat_call.py`, define the Semantic Kernel's service.\n\n## 🔍 Running the Tests\n\n\n### Running\n\nOnce the preparation steps descibed in `Getting Started` sections are done, simply run the `rag3dchat_call.py` file from within your environment:\n```bash\ncd path/to/Space3D-Bench/repo\npython rag3dchat_call.py\n```\n\nThe answers to the questions will be saved to files `data/{scene_name}/answers.json`.\n\n\n## 🔗 Citation\nIf you find our paper and project useful, please consider citing:\n```bibtex\n@inproceedings{szymanska2024space3dbench,\n  title={{Space3D-Bench: Spatial 3D Question Answering Benchmark}},\n  author={Szymanska, Emilia and Dusmanu, Mihai and Buurlage, Jan-Willem and Rad, Mahdi and Pollefeys, Marc},\n  booktitle={European Conference on Computer Vision (ECCV) Workshops},\n  year={2024}\n}\n```\n"
    },
    {
      "name": "Zhou-CyberSecurity-AI/ATBA",
      "stars": 4,
      "img": "https://avatars.githubusercontent.com/u/35444743?s=40&v=4",
      "owner": "Zhou-CyberSecurity-AI",
      "repo_name": "ATBA",
      "description": null,
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2024-08-19T10:16:36Z",
      "updated_at": "2025-02-13T16:11:12Z",
      "topics": [],
      "readme": "<p align=\"center\">\n    <img src=\"docs/images/logo.svg\" width = \"400\"  alt=\"ATBA Attack\" align=center />\n</p>\n<p align=\"center\">\n  <a target=\"_blank\">\n    <img src=\"https://github.com/thunlp/OpenAttack/workflows/Test/badge.svg?branch=master\" alt=\"Github Runner Covergae Status\">\n  </a>\n  <a href=\"\" target=\"_blank\">\n    <img src=\"https://readthedocs.org/projects/openattack/badge/?version=latest\" alt=\"ReadTheDoc Status\">\n  </a>\n  <a  href=\"https://pypi.org/project/OpenAttack/\"  target=\"_blank\">\n    <img src=\"https://img.shields.io/pypi/v/OpenAttack?label=pypi\" alt=\"PyPI version\">\n  </a>\n  <a  href=\"https://github.com/thunlp/OpenAttack/releases\"  target=\"_blank\">\n    <img src=\"https://img.shields.io/github/v/release/thunlp/OpenAttack\" alt=\"GitHub release (latest by date)\">  \n  </a>\n  <a target=\"_blank\">\n    <img alt=\"GitHub\" src=\"https://img.shields.io/github/license/thunlp/OpenAttack\">\n  </a>\n   <a target=\"_blank\">\n    <img src=\"https://img.shields.io/badge/PRs-Welcome-red\" alt=\"PRs are Welcome\">\n  </a>\n<br><br>\n  <a href=\"https://openattack.readthedocs.io/\" target=\"_blank\">Documentation</a> • <a href=\"#features--uses\">Features & Uses</a> • <a href=\"#usage-examples\">Usage Examples</a> • <a href=\"#attack-models\">Attack Models</a> • <a href=\"#toolkit-design\">Toolkit Design</a> \n<br>\n</p>\n\n<p style=\"text-align: center;\">\n  <b>ATBA: Transferring Backdoors between Large Language Models by Knowledge Distillation </b>\n</p>\n\n<div align=\"center\">\n<img src=\"pipeline.png\" alt=\"Centered Image\" style=\"width:500px;\"/>\n</div>\n\n**Contribution:**\n\n1. We propose ATBA, the first adaptive and transferable backdoor attack for LLMs, which aims to reveal the vulnerability of LLMs when using knowledge distillation.\n\n2. We design a target trigger generation module that leverages cosine similarity distribution to filter out indicative triggers from the original vocabulary tables of the teacher LLMs. This approach not only effectively realizes implicit backdoor transferable but also reduces search complexity.\n\n3. We introduce an adaptive trigger optimization module based on KD simulation and dynamic greedy searching, which overcomes textual discretization and is more robust than traditional triggers.\n\n4. Extensive experiments show that ATBA is highly transferable and successfully activates against student models with different architectures on five popular tasks. \n\n**How to Running ATBA**\n\n*1. Environment*\n```shell\npip install -r reuirement.txt\n```\n\n*2. Download Dataset from HuggingFace*\n```python\nfrom datasets improt load_dataset\ndataset = load_dataset(\"dataset path\")\ndataset.save_to_disk(\"./dataset/\")\n```\n\n\n*3. Download Models from HuggingFace*\n```python\nmodel.save_pretrained(\"/home/models/\")\n```\n\n*4. Warmup*\n\nWarm up the model using the **warmup.ipynb** script in the ATO module\n\n*5. TTG*\n\nModify the model and dataset paths under run/TTG_xxx.sh and run the corresponding script to obtain the target trigger word candidates.\n```shell\nbash ./run/TTG_xxx.sh\n```\n*6. ATO*\n\nModify the model and dataset paths under run/ATO_xxx.sh and run the corresponding script to get the optimal trigger word.\n\n```shell\nbash ./run/ATO_xxx.sh\n```\n\n*7. Evaluation*\n\nModify the model and dataset paths under run/KD_xxx.sh and run the corresponding script to evaluate the backdoor transfer capability of the teacher model on the three student models.\n\n```shell\nbash ./run/KD_xxx.sh\n```\n\n## Attack Models\n\n\n## Citation\n\nPlease cite our [paper](https://arxiv.org/pdf/2408.09878) if you use this toolkit:\n\n```\n@article{cheng2024transferring,\n  title={Transferring backdoors between large language models by knowledge distillation},\n  author={Cheng, Pengzhou and Wu, Zongru and Ju, Tianjie and Du, Wei and Liu, Zhuosheng Zhang Gongshen},\n  journal={arXiv preprint arXiv:2408.09878},\n  year={2024}\n}\n```\n\n## Contributors\nWe thank all the contributors to this project. And more contributions are very welcome.\n\n![Contributors](https://contrib.rocks/image?repo=Zhou-CyberSecurity-AI/ATBA)\n\n"
    },
    {
      "name": "gbaeke/semantic-kernel-demo",
      "stars": 4,
      "img": "https://avatars.githubusercontent.com/u/8395842?s=40&v=4",
      "owner": "gbaeke",
      "repo_name": "semantic-kernel-demo",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2024-03-24T16:31:11Z",
      "updated_at": "2025-04-11T04:22:11Z",
      "topics": [],
      "readme": "# Azure OpenAI on your data with Semantic Kernel\n\nSupply an .env file with the following variables:\n\n```\nAZURE_OPENAI_DEPLOYMENT_NAME=\"your_deployment_name\"\nAZURE_OPENAI_ENDPOINT=\"https://your_endpoint.openai.azure.com/\"\nAZURE_OPENAI_API_KEY=\"Your OpenAI API Key\"\nAZURE_OPENAI_API_VERSION=\"2023-12-01-preview\" \n\nAZURE_AISEARCH_API_KEY=\"Your Azure Search API Key\"\nAZURE_AISEARCH_URL=\"https://your_endpoint.search.windows.net\"\nAZURE_AISEARCH_INDEX_NAME=\"your index name\"\nMODE=\"search\" # any other value than search will be functions mode\nBING_SEARCH_API_KEY=\"APIKEY\"\n```\n\nCreate a virtual environment with `python -m venv venv`. Activate the virtual environment with `source venv/bin/activate`.\n\nInstall the requirements with `pip install -r requirements.txt`.\n\nRun the app with `streamlit run data.py`.\n\n**IMPORTANT**: you need an Azure AI Search index with fields you can map to the fields the code expects. The code expects the following fields:\n- titleField\n- urlField\n- contentFields: list\n- vectorFields: list\n\nIn the code, you map your fields to these fields like so:\n\n```python\nazure_ai_search_settings[\"fieldsMapping\"] = {\n    \"titleField\": \"Title\",\n    \"urlField\": \"Url\",\n    \"contentFields\": [\"Content\"],\n    \"vectorFields\": [\"contentVector\"], \n}\n```\n\nThe vectorField contentVector contains a 1536-dimensional vector that represents the content of the document. You can use the OpenAI API to generate this vector. Use model `text-embedding-ada-002` to generate the vector."
    },
    {
      "name": "WuJunde/werewolf_ai_agents",
      "stars": 4,
      "img": "https://avatars.githubusercontent.com/u/23454271?s=40&v=4",
      "owner": "WuJunde",
      "repo_name": "werewolf_ai_agents",
      "description": "autonomous and interactive AI agents playing werewolf",
      "homepage": null,
      "language": "Python",
      "created_at": "2024-02-20T20:42:19Z",
      "updated_at": "2025-02-11T17:54:30Z",
      "topics": [],
      "readme": "<p align=\"center\">\r\n<img width=\"180\" height=\"180\" style=\"vertical-align:middle\" src=\"https://github.com/WuJunde/werewolf_ai_agents/blob/main/werewolf_logo.png\" />\r\n</p>\r\n<h1 align=\"center\">\r\n<span><i>Werewolf-Agents</i></span>\r\n</h1>\r\n\r\n<h3 align=\"center\">\r\nTrain Agents to Play Werewolf\r\n</h3>\r\n\r\n<p align=\"center\">\r\n    <a href=\"https://discord.gg/DN4rvk95CC\">\r\n        <img alt=\"Discord\" src=\"https://img.shields.io/discord/1146610656779440188?logo=discord&style=flat&logoColor=white\"/></a>\r\n    <img src=\"https://img.shields.io/static/v1?label=license&message=GPL&color=white&style=flat\" alt=\"License\"/>\r\n</p>\r\n\r\nHere is a werewolf game played by large-language-model (LLM) based agents. You can use it to analyze LLM behavior in strategic games, train LLM agents to play the game better, or simply enjoy the game with the agents. The project is built upon [MetaGPT](https://github.com/geekan/MetaGPT).\r\n\r\n\r\n## News\r\n - [TOP] Join in our [Discord](https://discord.gg/DN4rvk95CC) to ask questions and discuss with others.\r\n\r\n## How to play:\r\n## Install\r\n\r\n### Pip installation\r\n\r\n> Ensure that Python 3.9+ is installed on your system. You can check this by using: `python --version`.  \r\n> You can use conda like this: `conda create -n metagpt python=3.9 && conda activate metagpt`\r\n\r\n```bash\r\npip install metagpt\r\nmetagpt --init-config  # it will create ~/.metagpt/config2.yaml, just modify it to your needs\r\n```\r\n\r\n### Configuration\r\n\r\nYou can configure `~/.metagpt/config2.yaml` according to the [example](https://github.com/geekan/MetaGPT/blob/main/config/config2.example.yaml) and [doc](https://docs.deepwisdom.ai/main/en/guide/get_started/configuration.html):\r\n\r\n```yaml\r\nllm:\r\n  api_type: \"openai\"  # or azure / ollama / open_llm etc. Check LLMType for more options\r\n  model: \"gpt-4-turbo-preview\"  # or gpt-3.5-turbo-1106 / gpt-4-1106-preview\r\n  base_url: \"https://api.openai.com/v1\"  # or forward url / other llm url\r\n  api_key: \"YOUR_API_KEY\"\r\n```\r\n\r\n### Usage\r\n\r\nAfter installation, you can run the game by:\r\n\r\n```bash\r\npython examples/werewolf_game/start_game.py\r\n```\r\n\r\nif you meet the error say your API is invalid. Please run ```metagpt --init-config``` again.\r\n\r\n"
    },
    {
      "name": "dmavani25/MinervAI",
      "stars": 4,
      "img": "https://avatars.githubusercontent.com/u/89651168?s=40&v=4",
      "owner": "dmavani25",
      "repo_name": "MinervAI",
      "description": "An AI powered lecture simulator to help professor gain feedback on their lecture notes and lectures through Q&As with a diverse student population",
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2023-11-11T16:07:55Z",
      "updated_at": "2023-11-14T19:36:06Z",
      "topics": [],
      "readme": "# MinervAI\n![output-onlinepngtools](https://github.com/dmavani25/MinervAI/assets/107078090/8413ebde-629f-4307-bfba-ca26f8a41b7c)\n![demo](demo.png)\n\nEmpowering teachers through interactions with realistic simulated classrooms. MinervAI combines AI agent students with various backgrounds and proficiencies to help educators better understand students and the ways in which they think.\n\nIn this beta version of MinervAI,  language models serve as AI agents in the classroom: each is initialized with a distinct personality (ex: “confident”), background (ex: “liberal arts student with math training”) and weight factor (which varies the scale of the 2 prior features’ impact) which influences their understanding of topics.\n\nUsers can upload a “lecture file” containing important information. The text is interpreted by the professor agent, which lectures the students, who then internalize the new data and generate new questions depending on their background. The professor then provides sample answers specific to each student’s needs and knowledge gaps.\n\nThe final output of this tool is a summary file listing Q+A conversations and summary statistics regarding question types (commonly asked questions, question frequency, recurring “keyword” concepts and more).\n\n<h2>Requirements for running</h2>\n\nThe required packages needed to run MinervAI are included in the ***requirements.txt*** file.\n\nThe semantic kernel package can be installed using the command `python3 -m pip install semantic-kernel`.\n\nOther packages can be installed using `pip3 install ...` command. \n\n## MinervAI - Team Contributions\n\nMinervAI is an innovative educational tool designed to empower teachers through interactions with AI-simulated classrooms. This project was developed as part of the HackUMass XI Hackathon, where it was awarded the Grand Overall Prize.\n\nWe would like to firstly thank Taichi Kato (one of our pivotal founding members) for giving an awesome introduction to Semantic Kernels, and making pivotal contributions to the project with us!!\n\n### Team Members & Contributions\n\n#### Dhyey Mavani\n- **Role:** Backend Development & Project Management\n- **Contributions:**\n  - Initiated the backend agents OOP framework.\n  - Implemented various functions within the backend with prompt engineering.\n  - Managed the development on the GitHub repository.\n  - Played a key role in the initial system design and conceptualization of the project.\n  - Drew Insights from previous projects such as LogFlowAI (Backed by Y Combinator), and MammothEdu from HackMIT\n\n#### Muhammad Ahsan Tahir\n- **Role:** Backend Development\n- **Contributions:**\n  - Focused on back-end development, particularly on the question similarity detector and output JSON.\n  - Developed all agent classes including prompt engineering.\n  - Worked on making function calls concurrent to speed up run time.\n\n#### Sawyer Pollard\n- **Role:** Frontend Development\n- **Contributions:**\n  - Co-developed the frontend using React and TailwindCSS.\n  - Assisted in architecting the backend and frontend connection.\n  - Designed a robust web file upload system with rich user feedback.\n\n#### Sebastien Brown\n- **Role:** Frontend and Backend Integration\n- **Contributions:**\n  - Worked on front-end design and styling.\n  - Tuned and tested backend APIs.\n  - Assisted in initial system design and development of the pitch concept and deck.\n  - Drew Insights from previous projects such as LogFlowAI (Backed by Y Combinator), and MammothEdu from HackMIT\n\n#### TinaZS Zhang\n- **Role:** Frontend Design & Pitch\n- **Contributions:**\n  - Created wireframes and graphics for the project.\n  - Involved in prompt engineering and development of the pitch deck.\n\n#### Taichi Kato\n- **Role:** Full Stack Development\n- **Contributions:**\n  - Contributed to both frontend and backend development.\n  - Played a pivotal role in integrating semantic kernels into the project.\n  - Assisted in the initial idea generation and system design.\n\n\n\n\n\n\n\n"
    },
    {
      "name": "MG-Cafe/SemanticKernel_PromptFlow",
      "stars": 4,
      "img": "https://avatars.githubusercontent.com/u/79723187?s=40&v=4",
      "owner": "MG-Cafe",
      "repo_name": "SemanticKernel_PromptFlow",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2023-10-02T05:01:08Z",
      "updated_at": "2024-12-22T22:19:50Z",
      "topics": [],
      "readme": "# Using planner\n\nThis console application demonstrates the final solution to the [planner](https://learn.microsoft.com/en-us/semantic-kernel/ai-orchestration/planner) doc article.\n\n## Prerequisites\n\n- [Python](https://www.python.org/downloads/) 3.8 and above\n  - [Poetry](https://python-poetry.org/) is used for packaging and dependency management\n  - [Semantic Kernel Tools](https://marketplace.visualstudio.com/items?itemName=ms-semantic-kernel.semantic-kernel)\n\n## Configuring the sample\n\nThe sample can be configured with a `.env` file in the project which holds api keys and other secrets and configurations.\n\nMake sure you have an\n[Open AI API Key](https://openai.com/api/) or\n[Azure Open AI service key](https://learn.microsoft.com/azure/cognitive-services/openai/quickstart?pivots=rest-api)\n\nCopy the `.env.example` file to a new file named `.env`. Then, copy those keys into the `.env` file:\n\n```\nGLOBAL__LLM_SERVICE=\"OpenAI\"\n\nAZURE_OPEN_AI__DEPLOYMENT_TYPE=\"chat-completion\"\nAZURE_OPEN_AI__CHAT_COMPLETION_DEPLOYMENT_NAME=\"gpt-35-turbo\"\nAZURE_OPEN_AI__TEXT_COMPLETION_DEPLOYMENT_NAME=\"text-davinci-003\"\nAZURE_OPEN_AI__ENDPOINT=\"\"\nAZURE_OPEN_AI__API_KEY=\"\"\n\nOPEN_AI__MODEL_TYPE=\"chat-completion\"\nOPEN_AI__CHAT_COMPLETION_MODEL_ID=\"gpt-3.5-turbo\"\nOPEN_AI__TEXT_COMPLETION_MODEL_ID=\"text-davinci-003\"\nOPEN_AI__API_KEY=\"\"\nOPEN_AI__ORG_ID=\"\"\n```\n\n## Running the sample\n\nTo run the console application within Visual Studio Code, just hit `F5`.\nAs configured in `launch.json` and `tasks.json`, Visual Studio Code will run `poetry install` followed by `python hello_world/main.py`\n\nTo build and run the console application from the terminal use the following commands:\n\n```powershell\npoetry install\npoetry run python main.py\n```\n"
    },
    {
      "name": "JustinMeimar/hack-gpt",
      "stars": 4,
      "img": "https://avatars.githubusercontent.com/u/32406396?s=40&v=4",
      "owner": "JustinMeimar",
      "repo_name": "hack-gpt",
      "description": "HackGPT Edmonton - July 2023",
      "homepage": "",
      "language": "JavaScript",
      "created_at": "2023-07-22T15:35:19Z",
      "updated_at": "2024-05-14T13:36:12Z",
      "topics": [],
      "readme": "# RealAI\nA chatbot made to help you find real estate listings.\n\n## How does it work?\n\n### 1. You start by inputting quantitative restraints that you're looking for \n![Quantitative filter](./images/quant_img.png)\n\nIts important to note that although our min and max price filters are implemented,\nthe Bedrooms and Bathrooms are not currently. \n\n### 2. You then describe the Qualitative features that your looking for\n\n![Qualitative filter](./images/qualt_filter.png)\n\n### 3. Browse through the top 3 results tailored to your needs \n\n![Search Results](./images/search_results.png)\n\n## Hack GPT Dev Repo\n\n### App Structure\n```\n.\n├── app.py\n├── client\n│   ├── build\n│   ├── node_modules\n│   ├── package.json\n│   ├── package-lock.json\n│   ├── public\n│   ├── README.md\n│   └── src\n├── llm\n│   └── README.md\n├── README.md\n└── requirements.txt\n\n11 directories, 8 files\n```\n\n### Installation\n\nClone repo\n`git clone git@github.com:JustinMeimar/hack-gpt-dev.git`\n\nNavigate into repo\n`cd hack-gpt-dev`\n\nMake a virtual environment\n`python -m venv env` \n\nInstall dependencies\n`pip install -r requirements.txt`\n`python3 app.py`\n\nInstall the frontend\n`npm install`\n\nBuild the frontend\n`cd /app/client`\n`npm build`\n\nRun the server locally\n`cd app && python manage.py runserver`\n\n### OpenAI Authentication\nCreate the file `app/api/llm/.env` and add the OpenAI Credentials in the form\n```\nOPENAI_API_KEY = <\"OPEN_AI_API_KEY\">\nOPENAI_ORG_ID = \"<\"OPENAI_ORG_ID\">\n```  \n\n### Making Changes\n\nThe React frontend is served by the file `app/client/build/index.html`, so to make changes to the frontend run `npm run build` from `app/client`\n"
    },
    {
      "name": "2025NKUCS-agent/NKK-GPT",
      "stars": 3,
      "img": "https://avatars.githubusercontent.com/u/204149293?s=40&v=4",
      "owner": "2025NKUCS-agent",
      "repo_name": "NKK-GPT",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2025-03-20T07:05:49Z",
      "updated_at": "2025-04-23T09:15:44Z",
      "topics": [],
      "readme": "# NKK-GPT 🚀\r\n\r\n## 1. Overview\r\n\r\nNKK-GPT is a powerful intelligent agent system integrating multiple advanced capabilities:\r\n\r\n### 1.1 Core Features\r\n- **Code generation & review** 💻\r\n- **Intelligent file operations** 📂\r\n- **Multi-model collaborative processing** 🤖🤝\r\n- **Sandbox environment execution** 🧑‍💻\r\n- **Git version control integration** 🗂️\r\n\r\n### 1.2 Technical Highlights\r\n- **Tool management** via MCP (Model Control Protocol) 🛠️\r\n- **Asynchronous parallel processing architecture** ⏳\r\n- **Multi-agent collaboration system** 🤖👥\r\n- **Secure sandbox execution environment** 🛡️\r\n\r\n---\r\n\r\n## 2. Installation ⚙️\r\n\r\nClone the project from GitHub using:\r\n\r\n```bash\r\ngit clone https://github.com/2025NKUCS-agent/NKK-GPT.git\r\n```\r\n\r\n### Requirements:\r\n- **Python Version**: Python 3.9+ (recommended) 🐍\r\n- **Dependencies**: Install required packages via:\r\n\r\n```bash\r\npip install -r requirements.txt\r\npip install -e .\r\n```\r\n\r\n---\r\n\r\n## 3. Running the Project 🏃‍♂️\r\n\r\n### Configuration:\r\nSet parameters (API keys, model names, etc.) in the `config/config.toml` directory.\r\n\r\n### Launch:\r\nStart the system with:\r\n\r\n```bash\r\npython setup.py build\r\npython setup.py install\r\npython main.py\r\n```\r\n\r\n---\r\n\r\n## 4. Project Structure 📂\r\n\r\n```\r\nnkkagent/\r\n├── agent/         # Core agent logic  \r\n├── config/        # Configuration management  \r\n├── llm/           # Language model integration  \r\n├── mcp/           # Model Control Protocol  \r\n├── sandbox/       # Sandbox environment  \r\n└── tools/         # Tool collection\r\n```\r\n\r\n---\r\n\r\n## 5. Contribution Guidelines 🤝\r\n\r\nTo contribute:\r\n1. Fork the repository 🍴\r\n2. Create a new development branch 🌱\r\n3. Submit a Pull Request with your changes 🔄\r\n"
    },
    {
      "name": "rodanthi-alexiou/Devoxx2025-AzureAIAgents",
      "stars": 3,
      "img": "https://avatars.githubusercontent.com/u/61449673?s=40&v=4",
      "owner": "rodanthi-alexiou",
      "repo_name": "Devoxx2025-AzureAIAgents",
      "description": "Learn how to design, orchestrate, and deploy intelligent multi-agent systems using Semantic Kernel and Autogen, powered by Azure’s cutting-edge AI stack.",
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2025-04-01T10:26:38Z",
      "updated_at": "2025-04-12T10:46:07Z",
      "topics": [],
      "readme": "# 🤖 Devoxx2025 – Azure AI Agents Workshop\n\nLearn how to design, orchestrate, and deploy intelligent multi-agent systems using **Semantic Kernel** and **AutoGen**, powered by **Azure’s cutting-edge AI stack**.\n\n\n\n## 📂 Repo Structure\n\n### 📓 Semantic Kernel Orchestration \n\nTest out different agent capabilities interactively using Jupyter Notebooks.\n\n- `SemanticKernel/sk-main.ipynb`  \n  ➤ Create an agent using **function calling** in Semantic Kernel  \n- `SemanticKernel/Plugins`  \n  ➤ Contains:\n  - `KnowledgeBasePlugin` 📚: Connects to your documents using Azure AI Search  \n  - `LightPlugin` 💡: A basic example plugin  \n  🔧 _Create your own plugin and add it to `sk-main.ipynb`!_\n\n> ℹ️ Use the credentials in the `variables.env` file to run notebooks. Find the variables here: [Google Doc](https://docs.google.com/document/d/1yk_Sd_2M-hQ7VpBVDmJFygivwACPyFJQAzWNATetXp0/edit?usp=sharing)\n\n\n\n### 🧠 Python Agents\n\nRun agent orchestration directly via scripts:\n\n- `AIAgents/sk-agend.py`  \n  ➤ Work with the **Azure AI Agent SDK** and a custom Semantic Kernel Plugin  \n- `AIAgents/agents-group.py`  \n  ➤ Multi-agent orchestration demo with Azure AI Agents + Semantic Kernel  \n\n> ℹ️ Requires Azure subscription. \n\n\n## 🔗 Resources\n\n### 🧰 Semantic Kernel\n- [Getting Started with Semantic Kernel (Python)](https://github.com/microsoft/semantic-kernel/tree/main/python/samples/getting_started)\n- [Multi-Agent-Custom-Automation-Engine – Solution Accelerator](https://github.com/microsoft/Multi-Agent-Custom-Automation-Engine-Solution-Accelerator/tree/main)\n\n\n\n"
    },
    {
      "name": "mdsiprojects/agenticai",
      "stars": 3,
      "img": "https://avatars.githubusercontent.com/u/73212683?s=40&v=4",
      "owner": "mdsiprojects",
      "repo_name": "agenticai",
      "description": null,
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2025-03-10T07:33:37Z",
      "updated_at": "2025-04-16T06:32:38Z",
      "topics": [],
      "readme": "# AI Agents\n\nAndrew Ng on Agentic (June 2024):\n[![Watch the video](https://img.youtube.com/vi/q1XFm21I-VQ/maxresdefault.jpg)](https://youtu.be/q1XFm21I-VQ)\n\n### [Watch this video on YouTube](https://youtu.be/q1XFm21I-VQ)\n\n### Presentation file: [AI Agents](/UTSMDSI-AI%20Agents-07.04.2025.pdf)\n\nRetrieval Augmented Generation: build your own RAG with Streamlit and LLMs: https://github.com/mdsiprojects/llm\n\nAgentic Design Patterns:\n- Intro: https://www.deeplearning.ai/the-batch/how-agents-can-improve-llm-performance/\n- Reflection: https://www.deeplearning.ai/the-batch/agentic-design-patterns-part-2-reflection/\n- Tool Use: https://www.deeplearning.ai/the-batch/agentic-design-patterns-part-3-tool-use/\n- Planning: https://www.deeplearning.ai/the-batch/agentic-design-patterns-part-4-planning/\n- Multi Agent Collaboration: https://www.deeplearning.ai/the-batch/agentic-design-patterns-part-5-multi-agent-collaboration/\n\n\n"
    },
    {
      "name": "corticalstack/azure-ai-foundry-examples",
      "stars": 3,
      "img": "https://avatars.githubusercontent.com/u/3995321?s=40&v=4",
      "owner": "corticalstack",
      "repo_name": "azure-ai-foundry-examples",
      "description": "Azure AI Foundry examples",
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2025-03-10T15:06:58Z",
      "updated_at": "2025-04-22T08:41:04Z",
      "topics": [],
      "readme": "# 🤖 Azure AI Foundry Examples\n\nThis repository contains examples demonstrating how to work with Azure AI Foundry and it's assets. AI Foundry is accessed at [ai.azure.com](http://ai.azure.com)\n\n## 🔧 Prerequisites\n\nTo run the examples in this repository, you'll need to have the following resources deployed in your Azure AI Foundry environment:\n\n- A text generation model like gpt-4o-mini\n- An embedding model like text-embedding-3-small\n- An AI search service (ideally with the semantic reanker option enabled)\n- An Application Insights data source\n\nMake sure these resources are properly deployed and configured before running the examples.\n\n## 📚 Examples\n\n🧮 [01_explore_hub_and_projects](./01_explore_hub_and_projects/README.md): A notebook to explore your Azure AI Foundry hub, associated projects, connections, and service deployments.\n\n🧮 [02_configure_tracing_project_based_model_inference](./02_configure_tracing_project_based_model_inference/README.md): Shows how to configure tracing for project-based model inference. See [this example](./03_chat/chat_project_based_model_inference.py) for implementation.\n\n🧮 [03_chat](./03_chat/README.md): A collection of examples demonstrating different ways to create chat applications using Azure AI Foundry assets, including a variety of SDKs with project-based model inference, project-based OpenAI, direct Azure OpenAI, and direct inference with and without streaming.\n\n🧮 [04_chat_with_prompt_templates](./04_chat_with_prompt_templates/README.md): Examples demonstrating how to use prompt templates with Azure AI Foundry model assets, including programmatically created templates and templates loaded from .prompty files.\n\n🧮 [05_project_based_ai_search](./05_project_based_ai_search/README.md): A notebook demonstrating how to use the default Azure AI Search connected to an Azure AI Foundry project to create a search index, upload documents, and perform a variety of search operations including exact match, fuzzy, vector, similarity, and hybrid.\n\n🧮 [06_basic_rag](./06_basic_rag/README.md): A Jupyter notebook demonstrating a basic Retrieval-Augmented Generation (RAG) pattern implementation using an Azure AI Foundry project, an embedding model, a text generation model, and Azure AI Search, for space facts Q&A.\n\n🧮 [07_safe_ai](./07_safe_ai/README.md): A Jupyter notebook demonstrating how to use Azure Content Safety to moderate text content with interactive widgets for adjusting moderation settings and testing different text inputs.\n\n🧮 [08_agents](./08_agents/README.md): Examples demonstrating how to create and use AI agents with Azure AI Foundry, including a personal learning coach, grounding with Bing search, code interpreter, file search, and function calling capabilities.\n\n🧮 [09_agents_with_semantic_kernel](./09_agents_with_semantic_kernel/README.md): Examples demonstrating how to create and use AI agents with Azure AI Foundry and Semantic Kernel. Includes interactive chat, code interpreter for data visualization, file search, and multi-agent collaboration.\n\n## 🛠️ Setup Guide\n\n### Requirements Management\n\nThe root `requirements.txt` file includes all dependencies needed for all examples in the repository.\n\n### 🐳 Using the Dev Container\n\nThis repository includes a development container configuration that sets up all necessary dependencies automatically. The dev container is configured to work with all examples in this repository.\n\n1. ✅ Open the project folder in VS Code\n2. ✅ When prompted, click \"Reopen in Container\" or use the command palette and select *Dev Containers: Reopen in Container*\n3. ✅ VS Code will build the container and set up the environment (this may take a few minutes the first time)\n4. ✅ Once the container is running, you'll have a fully configured environment with all dependencies installed\n5. ✅ The dev container includes the Azure CLI for authentication. You can use `az login` to authenticate with your Azure account\n\n### 🔧 Manual Setup (without Dev Container)\n\nIf you prefer not to use the dev container:\n\n1. Create a Python virtual environment:\n   ```bash\n   python -m venv venv\n   source venv/bin/activate  # On Windows: venv\\Scripts\\activate\n   ```\n\n2. Install the required packages:\n   ```bash\n   pip install -r requirements.txt\n   ```\n\n3. Make sure you're authenticated with Azure:\n   ```bash\n   az login\n   ```\n\n### ⚙️ Environment Configuration\n\nMost examples in this repository use environment variables for configuration, which are loaded from a `.env` file:\n\n1. Copy the example environment file:\n   ```bash\n   cp .env.example .env\n   ```\n\n2. Edit the `.env` file and add the required configuration values specific to each example.\n   - Project-based examples typically require an Azure AI Foundry project connection string\n   - Direct SDK examples typically require an endpoint URL and API key\n\n3. Optionally, adjust the logging level (DEBUG, INFO, WARNING, ERROR, CRITICAL):\n   ```\n   LOG_LEVEL=INFO\n   ```\n\n### 📝 Logging\n\nThe examples use Python's built-in logging module to log information at different levels:\n\n- **DEBUG**: Detailed information, typically useful for debugging\n- **INFO**: Confirmation that things are working as expected\n- **WARNING**: Indication that something unexpected happened, but the application still works\n- **ERROR**: Due to a more serious problem, the application has not been able to perform a function\n- **CRITICAL**: A serious error, indicating that the application itself may be unable to continue running\n\nLogs are written to example-specific log files in the application directory to avoid polluting the example output.\n\n## 👨‍💻 Contributing\n\nContributions and suggestions are welcome! Please see the [contributing guidelines](CONTRIBUTING.md) for details.\n\n## 📖 Resources\n\n- [Introducing Azure AI Foundry](https://www.youtube.com/watch?v=GD7MnIwAxYM)\n- [Azure AI Foundry Documentation](https://learn.microsoft.com/azure/ai-foundry)\n- [Azure AI Inference API Documentation](https://learn.microsoft.com/en-us/azure/machine-learning/reference-model-inference-api?view=azureml-api-2&tabs=python)\n- [Azure OpenAI Service Documentation](https://learn.microsoft.com/en-us/azure/ai-services/openai/)\n- [Semantic Kernel GitHub repo](https://github.com/microsoft/semantic-kernel)\n- [Semantic Kernel agent collaboration](https://learn.microsoft.com/en-us/semantic-kernel/frameworks/agent/agent-chat?pivots=programming-language-python)\n\n## ❓ FAQ\n\n<details>\n<summary><strong>What is Azure AI Foundry?</strong></summary>\nA successor to Azure AI Studio, it's a home for AI capabilities including a collection of tools and services to fully create, manage, and use AI models at scale for data scientists and AI engineers.\n</details>\n\n<details>\n<summary><strong>What is the difference between an Azure AI Foundry hub and project?</strong></summary>\nA hub is a parent workspace that provides shared resources including storage, key vault, and compute for multiple child projects. Projects are lighter weight workspaces for managing AI components that inherit and use the hub's resources. Think of hubs as infrastructure and resource providers and projects as workspaces for specific AI development efforts.\n\nA hub can have many projects, but a project can have only 1 parent hub. A project can have it's own data, connections, and model deployements that are isolated to the project itself, and not part of the hub.\n</details>\n\n<details>\n<summary><strong>What is the difference between the managed compute and serverless API deployment options?</strong></summary>\n\nDeployment options differ primarily in pricing structure and infrastructure approach.\n\n**Serverless deployment**:\n- Pay-as-you-go model based on token usage (input, output, and reasoning tokens)\n- Runs on shared GPU cluster pools specific to each model\n- Can utilize global pools, or region-specific pools (like East US or Sweden Central)\n\n**Managed compute**:\n- Hourly billing model regardless of usage\n- Runs on dedicated virtual machines with the model and API pre-deployed\n- Microsoft handles all infrastructure management and deployment\n</details>\n\n<details>\n<summary><strong>What is the difference between the Azure AI inferencing API and OpenAI API?</strong></summary>\n\nThe Azure AI inferencing API (package `azure.ai.inference`) serves as an abstraction layer that allows applications to interact with various models using a standardized interface. It translates requests to the specific format required by each underlying model.\n\nWhile `azure.ai.inference` provides a model-agnostic abstraction layer, the openai package with the AzureOpenAI client (i.e., `from openai import AzureOpenAI`) is specifically designed for interacting with OpenAI models deployed on Azure. \n\nIn summary, the key benefit of `azure.ai.inference` is the ability to switch between supported models, for example between an OpenAI and Meta model, without modifying your application code, avoiding code lock-in.\n\nVerify model compatibility via the [inference API documentation](https://learn.microsoft.com/en-us/azure/machine-learning/reference-model-inference-api?view=azureml-api-2&tabs=python).\n</details>\n\n<details>\n<summary><strong>What is RAG?</strong></summary>\nRetrieval-Augmented Generation (RAG) is a technique where the LLM (Large Language Model) uses relevant retrieved text chunks from your data to craft a final answer.\nThis helps ground the model's response in real data, reducing hallucinations.\n</details>\n\n<details>\n<summary><strong>What is Semantic Kernel?</strong></summary>\nSemantic Kernel is a lightweight, open-source development kit that lets you easily build AI agents and integrate the latest AI models into your C#, Python, or Java codebase. It serves as an efficient middleware that enables rapid delivery of enterprise-grade solutions.\n</details>\n\n\n<details>\n<summary><strong>In Semantic Kernel, what is a kernel</strong></summary>\nAn SK kernel is the central orchestration component that serves as the main entry point to the framework. It manages integration with AI services like Azure OpenAI, memory sharing, and function registry and execution.\n</details>\n"
    },
    {
      "name": "vikasgautam18/agentic_ai_with_semantic_kernel",
      "stars": 3,
      "img": "https://avatars.githubusercontent.com/u/39011958?s=40&v=4",
      "owner": "vikasgautam18",
      "repo_name": "agentic_ai_with_semantic_kernel",
      "description": null,
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2025-03-15T17:56:03Z",
      "updated_at": "2025-04-06T13:29:17Z",
      "topics": [],
      "readme": "# Agentic AI with Semantic Kernel\n\nThis project demonstrates the use of Semantic Kernel to build intelligent systems capable of processing and reasoning over structured and unstructured data. It includes various demos, plugins, and resources to showcase the capabilities of agentic AI.\n\n## Project Structure\n\n### Key Directories\n\n- **resources/**: Contains setup scripts and transcripts for initializing and testing the project.\n  - `setup/`: Includes scripts like `setup_db.sql` and `setup.sh` for database and environment setup.\n  - `transcripts/`: Stores example transcripts for testing and analysis.\n\n- **src/**: Main source code and demos for the project.\n  - `demos/`: Jupyter notebooks demonstrating various use cases of Semantic Kernel, such as group chat analysis and SQL database interactions.\n  - `plugins/`: Contains prompt templates for specific functionalities, such as handling blocked cards and reasons.\n\n## Getting Started\n\n1. **Setup Environment**:\n   - Use the scripts in `resources/setup/` to initialize the database and environment.\n   - Example:\n     ```bash\n     bash resources/setup/setup.sh\n     ```\n\n2. **Install Dependencies**:\n   - Navigate to the `src/demos/` directory and install the required Python packages:\n     ```bash\n     pip install -r requirements.txt\n     ```\n\n3. **Run Demos**:\n   - Open any Jupyter notebook in `src/demos/` to explore the project capabilities.\n   - Example:\n     ```bash\n     jupyter notebook src/demos/TestGroupChat.ipynb\n     ```\n\n## Features\n\n- **Group Chat Analysis**: Analyze group chat data using Semantic Kernel.\n- **SQL Database Interaction**: Demonstrates advanced SQL database operations.\n- **Prompt Engineering**: Includes templates for handling specific scenarios like blocked cards.\n\n## License\n\nThis project is licensed under the terms of the [LICENSE](LICENSE) file.\n\n## Contributing\n\nContributions are welcome! Please submit a pull request or open an issue for any suggestions or improvements.\n\n## Contact\n\nFor questions or support, please refer to the [transcripts](resources/transcripts/) or contact the project maintainers.\n\n"
    },
    {
      "name": "sambanova/integrations",
      "stars": 3,
      "img": "https://avatars.githubusercontent.com/u/80118584?s=40&v=4",
      "owner": "sambanova",
      "repo_name": "integrations",
      "description": null,
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2025-02-21T23:09:58Z",
      "updated_at": "2025-04-16T00:37:08Z",
      "topics": [],
      "readme": "<a href=\"https://sambanova.ai/\">\n<picture>\n <source media=\"(prefers-color-scheme: dark)\" srcset=\"./images/SambaNova-light-logo-1.png\" height=\"60\">\n  <img alt=\"SambaNova logo\" src=\"./images/SambaNova-dark-logo-1.png\" height=\"60\">\n</picture>\n</a>\n\n# SambaNova Cloud Integrations\n\nWelcome to the SambaNova Cloud API ecosystem, where everyday our fast inference models are expanding alongside the best tools in the developer community. Explore the partners available below and get started building\\! \n\n### Need Assistance? \n\nIf you have any suggestions of integrations or questions, please post on our [Community Page](https://community.sambanova.ai/) so we can follow up. \n\n## Integrations\n\n| Company | Type | Description | Access |\n| :---- | :---- | :---- | :---- |\n| **Agno** | Agentic-first library | Agno is a lightweight framework for building multi-modal AI agents | [Documentation](https://docs.agno.com/models/sambanova)  |\n| **AutoGen** | Agentic-first library | AutoGen is an open-source tool that defines agents, integrates LLMs, and handles task termination.  | [Demo code](./autogen/) |\n| **Browser Use** | Assistant tool | Browser Use is an open-source project enabling AI agents to control web browsers, facilitating tasks like automated web navigation and data extraction. | [Documentation](https://docs.browser-use.com/quickstart) |\n| **Camel** | Agentic-first library | Camel AI is an open-source framework for intelligent agents, and supports building, customizing, and deploying multi-agent systems.   | [Demo code](./camel/) |\n| **Cline** | Assistant tool | Cline is a coding assistant tool that streamlines workflows, offers account management features, and optimizes provider routing for developers. | [Documentation](https://docs.cline.bot/getting-started/getting-started-new-coders) |\n| **Continue** | Assistant tool | Continue is an open-source coding assistant platform to modify and optimize coding within IDE. | [Documentation](./continue) |\n| **CrewAI** | Agentic-first library | CrewAI is an open-source framework for making automated workflows with agents. | [Demo code](./crewai_integration/) |\n| **Gradio** | Assistant tool | Gradio is an open-source Python package that allows users to create interactive web apps for machine learning models, APIs, and Python functions.  | [Demo code](https://github.com/gradio-app/sambanova-gradio) |\n| **Haystack** | LLM framework | Haystack is an open-source end-to-end framework that enables modular development of production-ready LLM applications.  | [Demo code](./haystack/) |\n| **Hugging Face**  | LLM framework | Hugging Face is a platform for building, training, and deploying open-source models.  | [Documentation](https://huggingface.co/docs/huggingface_hub/main/en/package_reference/inference_client) |\n| **Instructor** | LLM framework | Instructor enhances LLM interactions by enabling structured data extraction, multi-language compatibility, and parallel tool calling. | [Demo code](./instructor/) |\n| **LangChain** | LLM framework | Langchain implements a standard interface for LLMs to simplify development, productization, and deployment. | [Documentation](https://python.langchain.com/docs/integrations/providers/sambanova/) |\n| **Langflow** | Low code framework | Langflow is a visual framework for building multi-agent and RAG applications. | [Documentation](https://docs.langflow.org/components-models#sambanova) |\n| **LlamaIndex** | LLM framework | LlamaIndex is an orchestration framework to rapidly deploy LLM applications. | [Demo code](./llamaindex) |\n| **Llama Stack** | LLM framework | Llama Stack provides modular APIs and tools to efficiently build and deploy AI applications, enabling tasks like inference, safety moderation, memory management, and autonomous agent creation using the Llama model family. | [Documentation](https://llama-stack.readthedocs.io/en/latest/distributions/self_hosted_distro/sambanova.html) |\n| **LiteLLM** | LLM framework | LiteLLM is an open-source Python library that provides a unified interface for accessing LLMs, translating inputs and mapping exceptions. | [Documentation](https://docs.litellm.ai/docs/providers/sambanova) |\n| **Milvus** | Vector DB | Milvus is an open-source vector database from Milvus and can easily enable RAG applications. | [Demo code](./milvus) |\n| **Oumi** | LLM framework | Oumi is an open-source platform that streamlines the entire lifecycle of foundation models from data preparation and training to evaluation and deployment. | [Documentation](https://oumi.ai/docs/en/latest/api/oumi.inference.html#oumi.inference.SambanovaInferenceEngine) |\n| **Semantic Kernel** | Agentic-first library | Semantic Kernel is an open-source development tool to build agents and integrate them with the latest AI models into your codebase.  | [Demo code](./semantic_kernel) |\n| **Vercel** | LLM framework | Vercel is a platform for deploying and hosting web applications, which developers can use to easily manage their websites and serverless functions. | [Documentation](https://sdk.vercel.ai/providers/community-providers/sambanova) |\n\n\n## AI Starter kits\n\nSambaNova AI Starter Kits are a collection of open-source examples and guides designed to facilitate the deployment of AI-driven use cases for both developers and enterprises. check the available kits [here](https://github.com/sambanova/ai-starter-kit)\n\n## API Documentation\n\n- Find more information about SambaNova Cloud [here](https://docs.sambanova.ai/cloud/docs/get-started/overview)\n\n**Note:** These Integrations code samples are provided \"as-is,\" and are not production-ready or supported code. Bugfix/support will be on a best-effort basis only. Code may use third-party open-source software. You are responsible for performing due diligence per your organization policies for use in your applications.\n\n"
    },
    {
      "name": "denniszielke/ai-agents-workshop",
      "stars": 3,
      "img": "https://avatars.githubusercontent.com/u/11569044?s=40&v=4",
      "owner": "denniszielke",
      "repo_name": "ai-agents-workshop",
      "description": "AI Agents hands on workshop",
      "homepage": "",
      "language": "Python",
      "created_at": "2024-11-22T19:37:10Z",
      "updated_at": "2025-02-10T17:39:34Z",
      "topics": [
        "agents",
        "ai",
        "azure",
        "langchain",
        "langgraph",
        "llamaindex",
        "semantickernel"
      ],
      "readme": "# AI Agent workshop\n\nRegions that this deployment can be executed:\n- northeurope (azure location)\n- swedencentral (aiResourceLocation)\n\n## Quickstart & Infrastructure setup\n\nThe following lines of code will connect your Codespace az cli and azd cli to the right Azure subscription:\n\n```\n# log in with the provided credentials - OPEN A PRIVATE BROWSER SESSION\naz login --use-device-code\n\n# if you need to log into a specific tenant - use the --tenant 00000000-0000-0000-0000-000000000000 parameter\naz login --use-device-code --tenant 00000000-0000-0000-0000-000000000000 \n\n# \"log into azure dev cli - only once\" - OPEN A PRIVATE BROWSER SESSION\nazd auth login --use-device-code\n\n# press enter open up https://microsoft.com/devicelogin and enter the code\n\n```\n\nNow deploy the infrastructure components with azure cli\n\n```\nazd up\n```\n\nGet the values for some env variables\n```\nazd env get-values | grep AZURE_ENV_NAME\nsource <(azd env get-values | grep AZURE_ENV_NAME)\n```\n\ndeploy the project lc-react-tools in Azure Container Apps. \n```\nbash ./azd-hooks/deploy.sh lc-react-tools $AZURE_ENV_NAME\n```\n\n\n# Workshop agenda\n\nThe scope of this workshop covers the following scenarios and technology stacks:\n\n| Name | Description | Technology  |\n| :-- | :--| :-- |\n| [af-simple](./src/af-simple/code-run.py) | Single Agent | AgentService, RAG |\n| [af-autogen](./src/af-autogen/simple.py) | Single Agent | AgentService, Autogen |\n| [lc-react-tools](./src/lc-react-tools/Readme.md) | Single Agent | Streamlit, Azure OpenAI, Langchain |\n| [lg-agents-01-coding](./src/lg-agents-01-coding/Readme.md) | Multi-agent code reviews | LangGraph, Azure OpenAI, Otel |\n| [lg-agents-02-shop](./src/lg-agents-02-shop/Readme.md) | Human in the loop | LangGraph, Qdrant, Azure OpenAI |\n| [li-workflows-01-simple](./src/li-workflows-01-simple/Readme.md) | Simple event driven workflow | Llama agents, Azure OpenAI |\n| [li-workflows-02-events](./src/li-workflows-02-events/Readme.md) | Event driven agent collaboration | Llama agents, Azure OpenAI |\n| [sk-agents-01-collaboration](./src/sk-agents-01-collaboration/Readme.md) | Simple mult agent discussion | Semantic kernel, Azure OpenAI |\n| [sk-agents-02-tools](./src/sk-agents-02-tools/Readme.md) | Using tools from agents | Semantic kernel, Azure OpenAI |\n| [sk-agents-03-creative](./src/sk-agents-03-creative/Readme.md) | Multi-turn multi agent discussion | Semantic kernel, Azure OpenAI |\n| [sk-agents-04-process](./src/sk-agents-04-process/Readme.md) | Event driven flow | Semantic kernel, Azure OpenAI |\n"
    },
    {
      "name": "whiteduck-training/wd-ai-hackathon",
      "stars": 3,
      "img": "https://avatars.githubusercontent.com/u/185513507?s=40&v=4",
      "owner": "whiteduck-training",
      "repo_name": "wd-ai-hackathon",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2024-11-13T16:52:57Z",
      "updated_at": "2024-12-19T09:52:09Z",
      "topics": [],
      "readme": "# wd-ai-hackathon\n\n## Run locally\n\nTo run the notebooks locally, you need to have a docker runtime installed on your machine. You can install docker from [here](https://docs.docker.com/get-docker/).\n\nAfter installing, you can run this repository as a devcontainer by following the steps below:\n\n1. Clone this repository to your local machine.\n2. Open the repository in Visual Studio Code.\n3. Open the command palette by pressing `Ctrl+Shift+P` or `Cmd+Shift+P` and search for `Remote-Containers: Reopen in Container`.\n\nFinally, you need to rename the .env_template file to .env and replace the values with the following\n\n```sh\nAZURE_OPENAI_CHAT_DEPLOYMENT_NAME=\"gpt-4o-mini\"\nAZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME=\"text-embedding-3-large\"\nAZURE_OPENAI_ENDPOINT=\"https://models.inference.ai.azure.com\"\nAZURE_OPENAI_API_KEY=\"<YOUR API KEY>\"\n```\n\nYou can get the API key by creating a GitHub Personal Access Token. See [here](https://github.com/settings/tokens).\n\n\n## Installation\n\nWhen running as devcontainer or codespace everything should be set up already.\n\nWhen running locally `uv`needs to be installed:\n\n```sh\ncurl --proto '=https' --tlsv1.2 -LsSf https://github.com/astral-sh/uv/releases/download/0.5.4/uv-installer.sh | sh\n\n```\n\nafter installation run\n\n```sh\nuv sync\n```\n\nAlso following vscode extensions are needed:\n\n```sh\n\"ms-python.python\"\n\"ms-toolsai.jupyter\"\n\"ms-toolsai.jupyter-renderers\"\n\"ms-toolsai.jupyter-keymap\"\n\"charliermarsh.ruff\"\n\"GitHub.copilo\"\n\"GitHub.copilo-chat\"\n```\n\n## Sanity check\n\nOpen `notebooks/00_hello_world.ipynb` and select `.venv`as kernel/virtual environment.\n\nExecute the notebook to make sure it runs without error.\n\n## AI Hackathon: Building Intelligent Applications with Semantic Kernel\n\nThis repository contains a series of Jupyter notebooks that guide you through building AI-powered applications using Semantic Kernel. The content progresses from basic concepts to advanced implementations, with each module containing both theoretical foundations and practical exercises.\n\n### Prerequisites\n\n- Python 3.8 or later\n- Basic understanding of Python programming\n- OpenAI API key or Azure OpenAI access\n- Semantic Kernel library installed\n\n### Notebooks Overview\n\n#### Module 1: Introduction to Semantic Kernel\n\n- **01_a_introduction_to_semantic_kernel.ipynb**: Theory module covering:\n  - What is Semantic Kernel\n  - Setting up the Kernel\n  - Plugins (Native and Semantic)\n  - Memory capabilities\n  - Practical examples including an Emoji Translator\n- **01_b_Introduction_to_semantic_kernel.ipynb**: Practical exercises for implementing small applications using core concepts:\n  - Fusion-Chef: Recipe combination system\n  - QnA Bot: Memory-based question answering\n  - Roleplay Gamemaster: Interactive game system\n  - Shell Meister: Natural language to shell command translator\n\n#### Module 2: Planning and Execution\n\n- **02_a_a_bad_plan_is_better_than_no_plan.ipynb**: Theory module exploring:\n  - AI Planning concepts\n  - Types of Planners (Sequential and Function Calling Stepwise)\n  - Memory integration with planners\n  - Web research capabilities\n- **02_b_a_bad_plan_is_better_than_no_plan.ipynb**: Practical exercises including:\n  - Code Documentation Assistant\n  - Personal Finance Advisor\n  - AI Dungeon Master\n  - Research Paper Assistant\n\n#### Module 3: AI Agents\n\n- **03_a_agents.ipynb**: Theory module covering:\n  - Introduction to SK Agents\n  - Creating basic agents\n  - Agent capabilities and plugins\n  - Multi-agent interactions\n- **03_b_agents.ipynb**: Practical exercises for building:\n  - Code Review Team\n  - Game Master System\n  - Competitive Debate System\n  - Technical Support System\n  - Research Paper Collaboration System\n- **03_c_battle_of_the_agents.ipynb**: Advanced concepts on:\n  - Agent simulation and testing\n  - Conversation modeling\n  - Performance evaluation\n\n#### Module 4: Process Management\n\n- **04_a_its_a_process.ipynb**: Theory module on:\n  - Building an AI Travel Agent\n  - Process Steps and Event Handling\n  - State Management\n  - AI Integration\n- **04_b_its_a_process.ipynb**: Practical exercises for implementing:\n  - Smart Home Automation System\n  - Restaurant Kitchen Process System\n  - Document Approval Workflow\n  - Data ETL Pipeline System\n  - Project Management System\n\n#### Module 5: Capstone Project\n\n- **05_a_putting_it_all_together.ipynb**: Understanding Knowledge Graphs\n  - Theory and foundations\n  - Graph components and relationships\n  - Visualization techniques\n  - Best practices\n- **05_b_putting_it_all_together.ipynb**: Building an AI Knowledge System\n  - Core Knowledge Graph System\n  - Knowledge Extraction Agents\n  - Query Agent System\n  - Process Integration\n- **05_c_putting_it_all_together.ipynb**: Extended Practical Applications\n  - Research Paper Assistant\n  - Technical Documentation Assistant\n  - Learning Path Generator\n  - Troubleshooting Assistant\n  - Content Recommendation System\n\n### Learning Path\n\nThe notebooks are designed to be completed in order, as each module builds upon concepts introduced in previous modules. The progression goes from basic Semantic Kernel concepts to advanced multi-agent systems and process management, culminating in a comprehensive capstone project.\n\nEach module contains:\n\n- Theoretical foundations\n- Code examples\n- Practical exercises\n- Best practices and tips\n- Real-world applications\n"
    },
    {
      "name": "alphavector/all",
      "stars": 3,
      "img": "https://avatars.githubusercontent.com/u/11805788?s=40&v=4",
      "owner": "alphavector",
      "repo_name": "all",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2024-10-06T12:01:51Z",
      "updated_at": "2025-01-26T12:22:19Z",
      "topics": [],
      "readme": "python -m generator -w 100 -l 1000 -r requirements.txt\n\n200 - https://editor.mergely.com/vCYSrPu2\n500 - https://editor.mergely.com/Fuh9Gsyw\n"
    },
    {
      "name": "colincmac/ai-rag-techniques-on-azure",
      "stars": 3,
      "img": "https://avatars.githubusercontent.com/u/99688332?s=40&v=4",
      "owner": "colincmac",
      "repo_name": "ai-rag-techniques-on-azure",
      "description": "Comprehensive list of strategies to enhance your LLM's ability to reason about your private data.",
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2024-08-28T15:28:47Z",
      "updated_at": "2025-03-12T20:59:44Z",
      "topics": [],
      "readme": "# AI RAG strategies\nComprehensive list of strategies to enhance your LLM's ability to reason about your private data.\nHow and when to implement various LLM memory structures and patterns on Azure Services.\nPatterns for enhancing the accuracy, efficiency, and contextual richness of RAG systems.\n\n\n## What this doesn't do\n\n- **Provide a RAG platform**\n- Explain vector databases\n\n### Understand Index Type and Distance Functions\n\n#### **1. `Vector Index Type`**\n\nThis option determines how vectors are indexed within Cosmos DB to optimize search performance.\n<details>\n<summary>\nOptions\n</summary>\n\n- **`flat`**: Stores vectors alongside other indexed properties without additional indexing structures. Supports up to **505 dimensions**.\n\n  **When to Use:**\n\n  - **Low-dimensional data**: Ideal for applications with vectors up to 505 dimensions.\n  - **Exact search requirements**: When you need precise search results.\n  - **Small to medium datasets**: Efficient for datasets where the index size won't become a bottleneck.\n\n  **Real-World Scenario:**\n\n  - **Customer Segmentation**: A retail company uses customer feature vectors (age, income, purchase history) with dimensions well below 505 to segment customers. Exact matches are important for targeted marketing campaigns.\n\n- **`quantizedFlat`**: Compresses (quantizes) vectors before indexing, improving performance at the cost of some accuracy. Supports up to **4096 dimensions**.\n\n  **When to Use:**\n\n  - **High-dimensional data with storage constraints**: Suitable for vectors up to 4096 dimensions where storage efficiency is important.\n  - **Performance-critical applications**: When reduced latency and higher throughput are needed.\n  - **Acceptable accuracy trade-off**: Minor losses in accuracy are acceptable for performance gains.\n\n  **Real-World Scenario:**\n\n  - **Mobile Image Recognition**: An app recognizes objects using high-dimensional image embeddings. Quantization reduces the storage footprint and improves search speed, crucial for mobile devices with limited resources.\n\n- **`diskANN`**: Utilizes the DiskANN algorithm for approximate nearest neighbor searches, optimized for speed and efficiency. Supports up to **4096 dimensions**.\n\n  **When to Use:**\n\n  - **Large-scale, high-dimensional data**: Best for big datasets where quick approximate searches are acceptable.\n  - **Real-time applications**: When fast response times are critical.\n  - **Scalability needs**: Suitable for applications expected to grow significantly.\n\n  **Real-World Scenario:**\n\n  - **Semantic Search Engines**: A search engine indexes millions of documents using embeddings from language models like BERT (768 dimensions). DiskANN allows users to get fast search results by efficiently handling high-dimensional data.\n</details>\n\n---\n\n#### **2. `Vector Data Type`**\n\nSpecifies the data type of the vector components.\n<details>\n<summary>Options</summary>\n\n- **`float32`** (default): 32-bit floating-point numbers.\n\n  **When to Use:**\n\n  - **High precision requirements**: Necessary when the application demands precise calculations.\n  - **Standard ML embeddings**: Most machine learning models output float32 vectors.\n\n  **Real-World Scenario:**\n\n  - **Scientific Simulations**: In climate modeling, vectors represent complex data where precision is vital for accurate simulations and predictions.\n\n- **`uint8`**: 8-bit unsigned integers.\n\n  **When to Use:**\n\n  - **Memory optimization**: Reduces storage needs when precision can be sacrificed.\n  - **Quantized models**: When vectors are output from models that already quantize data.\n\n  **Real-World Scenario:**\n\n  - **Basic Image Features**: Storing color histograms for image retrieval systems, where each bin can be represented with an 8-bit integer.\n\n- **`uint8`**: 8-bit integer with potentially specialized encoding (interpretation may vary; assuming it's an 8-bit integer with logarithmic encoding).\n\n  **When to Use:**\n\n  - **Custom quantization schemes**: When using specialized compression techniques that map floating-point values to an 8-bit integer scale.\n  - **Edge devices**: Ideal for applications on devices with extreme memory limitations.\n\n  **Real-World Scenario:**\n\n  - **Audio Fingerprinting**: Compressing audio feature vectors for song recognition apps where storage and quick retrieval are essential.\n</details>\n\n---\n#### **3. `Dimension Size`**\n\nThe length of the vectors being indexed. Ranges from 0-4096, default is **1536**.\n<details>\n<summary>Options</summary>\n\n\n**When to Consider Lower Dimensions (≤ 505):**\n\n  - **Simpler models**: Applications using basic embeddings or feature vectors.\n  - **Flat index type**: Required when using the `flat` index type due to its dimension limit.\n\n  *Real-World Scenario:*\n\n  - **Keyword Matching**: Using low-dimensional TF-IDF vectors for document similarity in a content management system.\n\n  **When to Consider Higher Dimensions (506 - 4096):**\n\n  - **Complex models**: Deep learning applications with high-dimensional embeddings.\n  - **Advanced search features**: When richer representations of data are necessary for accuracy.\n\n  *Real-World Scenario:*\n\n  - **Face Recognition**: Using high-dimensional embeddings (e.g., 2048 dimensions) to represent facial features for security systems.\n</details>\n\n---\n\n#### **4. `Distance Function`**\n\nDetermines how similarity between vectors is calculated.\n<details>\n<summary>Options</summary>\n\n- **`cosine`**: Measures the cosine of the angle between vectors.\n\n  **When to Use:**\n\n  - **Orientation-focused similarity**: When the magnitude is less important than the direction.\n  - **Normalized data**: Ideal when vectors are normalized to unit length.\n\n  **Real-World Scenario:**\n\n  - **Document Similarity**: In text analytics, comparing documents based on topic similarity where word counts are normalized.\n\n- **`dot product`**: Computes the scalar product of two vectors.\n\n  **When to Use:**\n\n  - **Magnitude matters**: When both direction and magnitude are significant.\n  - **Machine learning models**: Often used in recommendation systems where strength of preferences is important.\n\n  **Real-World Scenario:**\n\n  - **Personalized Recommendations**: Matching users to products by calculating the dot product of user and item embeddings in a collaborative filtering system.\n\n- **`euclidean`**: Calculates the straight-line distance between vectors.\n\n  **When to Use:**\n\n  - **Spatial distance relevance**: When physical distance correlates with similarity.\n  - **High-dimensional data**: Suitable for embeddings where both magnitude and direction impact similarity.\n\n  **Real-World Scenario:**\n\n  - **Anomaly Detection**: Identifying outliers in network traffic patterns by measuring Euclidean distances in feature space.\n\n---\n\n### **CosmosDB: Option Combinations and Their Preferred Use-Cases**\n\n#### **Combination 1: Low-Dimensional, Exact Searches**\n\n- **`vectorIndexType`**: `flat`\n- **`datatype`**: `float32`\n- **`dimensions`**: ≤ 505\n- **`distanceFunction`**: `cosine`\n\n**Real-World Scenario:**\n\n- **Small-Scale Text Classification**: A startup builds a news categorization tool using word embeddings (300 dimensions). Exact cosine similarity searches ensure accurate article tagging without the overhead of approximate methods.\n\n---\n\n#### **Combination 2: High-Dimensional, Performance-Critical Applications**\n\n- **`vectorIndexType`**: `diskANN`\n- **`datatype`**: `float32`\n- **`dimensions`**: 768 - 1536\n- **`distanceFunction`**: `cosine` or `dot product`\n\n**Real-World Scenario:**\n\n- **Real-Time Recommendations**: A streaming service uses user and content embeddings (1024 dimensions) to provide instantaneous movie recommendations. DiskANN accelerates search times, offering a smooth user experience despite the large dataset.\n\n---\n\n#### **Combination 3: Storage-Efficient High-Dimensional Data**\n\n- **`vectorIndexType`**: `quantizedFlat`\n- **`datatype`**: `uint8` or `iln8`\n- **`dimensions`**: 2048\n- **`distanceFunction`**: `cosine`\n\n**Real-World Scenario:**\n\n- **Mobile Visual Search**: An app allows users to search for products by uploading photos. High-dimensional image embeddings are quantized to fit the storage constraints of mobile devices, and approximate searches provide quick results.\n\n---\n\n#### **Combination 4: Precision-Critical Scientific Computing**\n\n- **`vectorIndexType`**: `flat`\n- **`datatype`**: `float32`\n- **`dimensions`**: 4096\n- **`distanceFunction`**: `euclidean`\n\n**Real-World Scenario:**\n\n- **Genomic Data Analysis**: Researchers analyze genetic sequences represented as high-dimensional vectors. Precise Euclidean distance calculations are essential for identifying genetic similarities and mutations.\n\n---\n\n#### **Combination 5: Medium-Dimensional Data with Storage Constraints**\n\n- **`vectorIndexType`**: `quantizedFlat`\n- **`datatype`**: `uint8`\n- **`dimensions`**: 500\n- **`distanceFunction`**: `dot product`\n\n**Real-World Scenario:**\n\n- **IoT Sensor Data**: A network of sensors generates medium-dimensional vectors representing environmental data. Quantization reduces storage and transmission costs, and dot product calculations help in identifying patterns and anomalies efficiently.\n\n---\n\n### **Summary**\n\n- **`flat` Index Type**: Use for low-dimensional, exact searches on smaller datasets.\n- **`quantizedFlat` Index Type**: Choose when you need to balance performance and storage with acceptable accuracy loss in high-dimensional data.\n- **`diskANN` Index Type**: Opt for large-scale, high-dimensional datasets where approximate searches suffice, and speed is critical.\n- **`float32` Datatype**: Default choice for precision; use when storage is less of a concern.\n- **`uint8` and `iln8` Datatypes**: Use for storage efficiency, particularly when data can be quantized.\n- **Dimensions**: Match the dimensionality to your data and index type constraints.\n- **Distance Functions**: Select based on the nature of similarity in your application—`cosine` for orientation, `dot product` when magnitude matters, and `euclidean` for spatial relevance.\n\n\nCertainly! Let's break down each of the options available when defining a Vector Profile in Azure Cosmos DB and explore real-world scenarios where each option or combination would be preferred.\n\n---\n\n### **1. `vectorIndexType`**\n\nThis option determines how vectors are indexed within Cosmos DB to optimize search performance.\n\n#### **Options:**\n\n- **`flat`**: Stores vectors alongside other indexed properties without additional indexing structures. Supports up to **505 dimensions**.\n\n  **When to Use:**\n\n  - **Low-dimensional data**: Ideal for applications with vectors up to 505 dimensions.\n  - **Exact search requirements**: When you need precise search results.\n  - **Small to medium datasets**: Efficient for datasets where the index size won't become a bottleneck.\n\n  **Real-World Scenario:**\n\n  - **Customer Segmentation**: A retail company uses customer feature vectors (age, income, purchase history) with dimensions well below 505 to segment customers. Exact matches are important for targeted marketing campaigns.\n\n- **`quantizedFlat`**: Compresses (quantizes) vectors before indexing, improving performance at the cost of some accuracy. Supports up to **4096 dimensions**.\n\n  **When to Use:**\n\n  - **High-dimensional data with storage constraints**: Suitable for vectors up to 4096 dimensions where storage efficiency is important.\n  - **Performance-critical applications**: When reduced latency and higher throughput are needed.\n  - **Acceptable accuracy trade-off**: Minor losses in accuracy are acceptable for performance gains.\n\n  **Real-World Scenario:**\n\n  - **Mobile Image Recognition**: An app recognizes objects using high-dimensional image embeddings. Quantization reduces the storage footprint and improves search speed, crucial for mobile devices with limited resources.\n\n- **`diskANN`**: Utilizes the DiskANN algorithm for approximate nearest neighbor searches, optimized for speed and efficiency. Supports up to **4096 dimensions**.\n\n  **When to Use:**\n\n  - **Large-scale, high-dimensional data**: Best for big datasets where quick approximate searches are acceptable.\n  - **Real-time applications**: When fast response times are critical.\n  - **Scalability needs**: Suitable for applications expected to grow significantly.\n\n  **Real-World Scenario:**\n\n  - **Semantic Search Engines**: A search engine indexes millions of documents using embeddings from language models like BERT (768 dimensions). DiskANN allows users to get fast search results by efficiently handling high-dimensional data.\n\n---\n\n### **2. `datatype`**\n\nSpecifies the data type of the vector components.\n\n#### **Options:**\n\n- **`float32`** (default): 32-bit floating-point numbers.\n\n  **When to Use:**\n\n  - **High precision requirements**: Necessary when the application demands precise calculations.\n  - **Standard ML embeddings**: Most machine learning models output float32 vectors.\n\n  **Real-World Scenario:**\n\n  - **Scientific Simulations**: In climate modeling, vectors represent complex data where precision is vital for accurate simulations and predictions.\n\n- **`uint8`**: 8-bit unsigned integers.\n\n  **When to Use:**\n\n  - **Memory optimization**: Reduces storage needs when precision can be sacrificed.\n  - **Quantized models**: When vectors are output from models that already quantize data.\n\n  **Real-World Scenario:**\n\n  - **Basic Image Features**: Storing color histograms for image retrieval systems, where each bin can be represented with an 8-bit integer.\n\n- **`iln8`**: 8-bit integer with potentially specialized encoding (interpretation may vary; assuming it's an 8-bit integer with logarithmic encoding).\n\n  **When to Use:**\n\n  - **Custom quantization schemes**: When using specialized compression techniques that map floating-point values to an 8-bit integer scale.\n  - **Edge devices**: Ideal for applications on devices with extreme memory limitations.\n\n  **Real-World Scenario:**\n\n  - **Audio Fingerprinting**: Compressing audio feature vectors for song recognition apps where storage and quick retrieval are essential.\n\n---\n\n### **3. `dimensions`**\n\nThe length of the vectors being indexed.\n\n#### **Options:**\n\n- **Range from 0 to 4096**, default is **1536**.\n\n  **When to Consider Lower Dimensions (≤ 505):**\n\n  - **Simpler models**: Applications using basic embeddings or feature vectors.\n  - **Flat index type**: Required when using the `flat` index type due to its dimension limit.\n\n  **Real-World Scenario:**\n\n  - **Keyword Matching**: Using low-dimensional TF-IDF vectors for document similarity in a content management system.\n\n  **When to Consider Higher Dimensions (506 - 4096):**\n\n  - **Complex models**: Deep learning applications with high-dimensional embeddings.\n  - **Advanced search features**: When richer representations of data are necessary for accuracy.\n\n  **Real-World Scenario:**\n\n  - **Face Recognition**: Using high-dimensional embeddings (e.g., 2048 dimensions) to represent facial features for security systems.\n\n---\n\n### **4. `distanceFunction`**\n\nDetermines how similarity between vectors is calculated.\n\n#### **Options:**\n\n- **`cosine`**: Measures the cosine of the angle between vectors.\n\n  **When to Use:**\n\n  - **Orientation-focused similarity**: When the magnitude is less important than the direction.\n  - **Normalized data**: Ideal when vectors are normalized to unit length.\n\n  **Real-World Scenario:**\n\n  - **Document Similarity**: In text analytics, comparing documents based on topic similarity where word counts are normalized.\n\n- **`dot product`**: Computes the scalar product of two vectors.\n\n  **When to Use:**\n\n  - **Magnitude matters**: When both direction and magnitude are significant.\n  - **Machine learning models**: Often used in recommendation systems where strength of preferences is important.\n\n  **Real-World Scenario:**\n\n  - **Personalized Recommendations**: Matching users to products by calculating the dot product of user and item embeddings in a collaborative filtering system.\n\n- **`euclidean`**: Calculates the straight-line distance between vectors.\n\n  **When to Use:**\n\n  - **Spatial distance relevance**: When physical distance correlates with similarity.\n  - **High-dimensional data**: Suitable for embeddings where both magnitude and direction impact similarity.\n\n  **Real-World Scenario:**\n\n  - **Anomaly Detection**: Identifying outliers in network traffic patterns by measuring Euclidean distances in feature space.\n\n---\n\n### **Option Combinations and Their Preferred Use-Cases**\n\n#### **Combination 1: Low-Dimensional, Exact Searches**\n\n- **`vectorIndexType`**: `flat`\n- **`datatype`**: `float32`\n- **`dimensions`**: ≤ 505\n- **`distanceFunction`**: `cosine`\n\n**Real-World Scenario:**\n\n- **Small-Scale Text Classification**: A startup builds a news categorization tool using word embeddings (300 dimensions). Exact cosine similarity searches ensure accurate article tagging without the overhead of approximate methods.\n\n---\n\n#### **Combination 2: High-Dimensional, Performance-Critical Applications**\n\n- **`vectorIndexType`**: `diskANN`\n- **`datatype`**: `float32`\n- **`dimensions`**: 768 - 1536\n- **`distanceFunction`**: `cosine` or `dot product`\n\n**Real-World Scenario:**\n\n- **Real-Time Recommendations**: A streaming service uses user and content embeddings (1024 dimensions) to provide instantaneous movie recommendations. DiskANN accelerates search times, offering a smooth user experience despite the large dataset.\n\n---\n\n#### **Combination 3: Storage-Efficient High-Dimensional Data**\n\n- **`vectorIndexType`**: `quantizedFlat`\n- **`datatype`**: `uint8` or `iln8`\n- **`dimensions`**: 2048\n- **`distanceFunction`**: `cosine`\n\n**Real-World Scenario:**\n\n- **Mobile Visual Search**: An app allows users to search for products by uploading photos. High-dimensional image embeddings are quantized to fit the storage constraints of mobile devices, and approximate searches provide quick results.\n\n---\n\n#### **Combination 4: Precision-Critical Scientific Computing**\n\n- **`vectorIndexType`**: `flat`\n- **`datatype`**: `float32`\n- **`dimensions`**: 4096\n- **`distanceFunction`**: `euclidean`\n\n**Real-World Scenario:**\n\n- **Genomic Data Analysis**: Researchers analyze genetic sequences represented as high-dimensional vectors. Precise Euclidean distance calculations are essential for identifying genetic similarities and mutations.\n\n---\n\n#### **Combination 5: Medium-Dimensional Data with Storage Constraints**\n\n- **`vectorIndexType`**: `quantizedFlat`\n- **`datatype`**: `uint8`\n- **`dimensions`**: 500\n- **`distanceFunction`**: `dot product`\n\n**Real-World Scenario:**\n\n- **IoT Sensor Data**: A network of sensors generates medium-dimensional vectors representing environmental data. Quantization reduces storage and transmission costs, and dot product calculations help in identifying patterns and anomalies efficiently.\n\n---\n\n### **Summary**\n\n- **`flat` Index Type**: Use for low-dimensional, exact searches on smaller datasets.\n- **`quantizedFlat` Index Type**: Choose when you need to balance performance and storage with acceptable accuracy loss in high-dimensional data.\n- **`diskANN` Index Type**: Opt for large-scale, high-dimensional datasets where approximate searches suffice, and speed is critical.\n- **`float32` Datatype**: Default choice for precision; use when storage is less of a concern.\n- **`uint8` and `iln8` Datatypes**: Use for storage efficiency, particularly when data can be quantized.\n- **Dimensions**: Match the dimensionality to your data and index type constraints.\n- **Distance Functions**: Select based on the nature of similarity in your application—`cosine` for orientation, `dot product` when magnitude matters, and `euclidean` for spatial relevance.\n\nBy carefully selecting these options based on your application's specific needs, you can optimize Cosmos DB's vector search capabilities to achieve the desired balance between performance, accuracy, and resource utilization.\n\n"
    },
    {
      "name": "Agentic-Insights/sk-python-labs",
      "stars": 3,
      "img": "https://avatars.githubusercontent.com/u/171188275?s=40&v=4",
      "owner": "Agentic-Insights",
      "repo_name": "sk-python-labs",
      "description": "demonstrate semantic kernel connection to Ollama and Groq",
      "homepage": "https://agenticinsights.com/",
      "language": "Python",
      "created_at": "2024-08-06T15:45:40Z",
      "updated_at": "2024-12-05T11:50:09Z",
      "topics": [
        "groq",
        "ollama",
        "openai",
        "semantic-kernel"
      ],
      "readme": "# 🧠 Semantic Kernel 🌽 Python Tutorial 101 🐍\n\nWelcome to the Semantic Kernel Python Tutorial 101! This repository provides a hands-on introduction to using Semantic Kernel with Python, demonstrating integration with various AI services. Semeantic implements a Kernel that supports Python, C#, and Java. The Kernel is an opinionated, configurable set of abstractions that are agentic in various ways. It offers a very interesting Plugins architecture that allows for LLM based and native functions within a plugin namespace for grouping more granular functions and tools - both semantic and discrete - to accomplish a larger task. \n\n- [Microsoft Semantic Kernel - github repo](https://github.com/microsoft/semantic-kernel)\n- [Compare / contrast LangChain, Semantic Kernel, and AutoGen](https://medium.com/data-science-at-microsoft/harnessing-the-power-of-large-language-models-a-comparative-overview-of-langchain-semantic-c21f5c19f93e)\n- [Microsoft Learn - Intro to Semantic Kernel](https://learn.microsoft.com/en-us/semantic-kernel/overview/)\n\n## 🚀 Getting Started\n\n### Prerequisites\n\n- Python 3.10+\n- pip (Python package manager)\n- Ollama - https://www.ollama.com/\n- Groq API key - https://console.groq.com/keys\n\n### Installation\n\n1. Clone this repository:\n   ```\n   git clone https://github.com/killerapp/semantic-kernel-python-tutorial.git\n   cd semantic-kernel-python-tutorial\n   ```\n\n2. Install the required packages:\n   ```\n   pip install -r requirements.txt\n   ```\n\n## 🔧 Configuration\n\nBefore running the examples, you need to set up your environment variables. We provide sample `.env` files for different services:\n\n- [.env.ollama.example](sk-ollama/.env.ollama.example) - For Ollama\n- [.env.groq.example](sk-groq/.env.groq.example) - For Groq\n\nCopy the appropriate `.env.*.example` file to `.env` and fill in your API keys and other required information.\n\n## 📚 Examples\n\n### Ollama Example\n\nRun the Ollama example with:\n\n```\ncd sk-ollama\npython sk-ollama.py\n```\n\nThis script demonstrates how to use Semantic Kernel with an Ollama server. Key features:\n\n\n```19:31:sk-ollama.py\nexecution_settings = OllamaChatPromptExecutionSettings()\n\nkernel = Kernel()\n\n# Alternative using Ollama:\nservice_id=\"ollama\"\nkernel.add_service(\n  OllamaChatCompletion(\n      service_id=service_id\n  )\n)\n```\n\n\n### Groq Example\n\nRun the Groq example with:\n\n```\ncd sk-groq\npython sk-groq.py\n```\n\nThis script shows how to integrate Semantic Kernel with Groq. Notable sections:\n\n\n```23:32:sk-groq.py\n# Use Groq:\nservice_id = \"groq\"\nservice = OpenAIChatCompletion(\n    service_id=service_id,\n    api_key=os.getenv(\"GROQ_API_KEY\"),\n    org_id=os.getenv(\"GROQ_ORG_ID\"),\n    ai_model_id=os.getenv(\"GROQ_MODEL\")\n)\nservice.client.base_url = os.getenv(\"GROQ_BASE_URL\")  # this is the important line\nkernel.add_service(service=service)\n```\n\n\n## 🧩 Plugins\n\nThis tutorial includes one sample plugin to demonstrate Semantic Kernel's capabilities:\n\n- FunPlugin: Generates jokes, limericks, and creative excuses.\n\nYou can find plugins in the `plugins/` directory.\n\n## 📝 License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n\n## 🤝 Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.\n\n## 📬 Contact\n\nIf you have any questions or feedback, please open an issue in this repository.\n\nHappy coding with Semantic Kernel! 🎉\n"
    },
    {
      "name": "khchan/building-blocks-ai",
      "stars": 3,
      "img": "https://avatars.githubusercontent.com/u/1270511?s=40&v=4",
      "owner": "khchan",
      "repo_name": "building-blocks-ai",
      "description": null,
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2024-07-09T15:52:45Z",
      "updated_at": "2024-12-02T03:38:15Z",
      "topics": [],
      "readme": "# building-blocks-ai\n\n## Prerequisites\n* https://ollama.com/ for local models\n\n## Setup\n1. Create a virtual environment called `venv` - this should create a folder called `venv` where project-specific dependencies will live:\n```\npython -m venv venv\n```\n2. Activate the virtual environment by name:\n```\nsource venv/bin/activate\n```\n3. Install dependencies:\n```\npip install -r requirements.txt\n```\n4. Create a `.env` file with the values specified in `.env.example`\n\n## Development\nRun jupyter lab to bring up an interactive environment:\n```\njupyter lab\n```\n\n## Live Reloading Notebook Slideshow\njupyter can export a notebook into a slideshow format, and to do live reloads you can install `fswatch` to build the html whenever the notebook changes:\n```\nbrew install fswatch\n```\n\nThen to watch the notebook and open the slideshow run:\n```\n./watch.sh\n```"
    },
    {
      "name": "jakeatmsft/promptflow_patterns",
      "stars": 3,
      "img": "https://avatars.githubusercontent.com/u/47987698?s=40&v=4",
      "owner": "jakeatmsft",
      "repo_name": "promptflow_patterns",
      "description": null,
      "homepage": null,
      "language": "HTML",
      "created_at": "2024-07-02T15:01:28Z",
      "updated_at": "2024-12-11T19:17:01Z",
      "topics": [],
      "readme": "# PromptFlow Pattern Library\n\nWelcome to the PromptFlow Pattern Library, a comprehensive collection of templates designed to enhance your experience with PromptFlow. This library offers a variety of templates tailored to different use cases, enabling you to leverage the power of PromptFlow more effectively and efficiently.\n\n## Overview\n\nPromptFlow is a powerful tool for generating and managing prompts in various contexts. The PromptFlow Pattern Library extends its capabilities by providing pre-designed templates that cater to a wide range of applications, from data analysis to content generation.\n\n## Getting Started\n\nTo get started with the PromptFlow Pattern Library, follow these steps:\n\n1. **Installation**: Ensure you have PromptFlow installed and configured on your system. Refer to the official PromptFlow documentation for installation instructions. [https://microsoft.github.io/promptflow/index.html](https://microsoft.github.io/promptflow/index.html)\n\n2. **Browse Templates**: Explore the library to find templates that suit your needs. Each template comes with a detailed description and use case scenario to help you make the best choice.\n\n4. **Customize**: Customize the templates according to your specific requirements. Most templates are designed to be flexible and can be adjusted with minimal effort.\n\n5. **Deploy**: Integrate the customized template into your PromptFlow setup and start using it within your workflows.\n\n## Template Categories\n\nThe library includes templates across various categories, such as:\n\n- **Data Analysis**: Templates designed for data extraction, analysis, and visualization tasks.\n- **Content Generation**: Templates that assist in generating articles, blog posts, and other forms of written content.\n- **Chatbots**: Pre-designed flows for creating responsive and interactive chatbots.\n- **Survey and Feedback**: Templates to facilitate the creation of surveys and collection of feedback.\n\n## Contributing\n\nWe welcome contributions to the PromptFlow Pattern Library! If you have a template that you believe would benefit the community, please follow our contribution guidelines to submit it.\n\n## Support\n\nFor support with the PromptFlow Pattern Library, please refer to the [official documentation](#) or reach out to our community forums. Our team is dedicated to providing assistance and ensuring you make the most out of your PromptFlow experience.\n\nThank you for choosing the PromptFlow Pattern Library. We look forward to seeing the innovative ways you use these templates to enhance your projects!\n"
    },
    {
      "name": "Lyn4ever29/pipy_server",
      "stars": 3,
      "img": "https://avatars.githubusercontent.com/u/25952589?s=40&v=4",
      "owner": "Lyn4ever29",
      "repo_name": "pipy_server",
      "description": "Pypi本地镜像服务器搭建",
      "homepage": null,
      "language": "Python",
      "created_at": "2024-07-04T08:07:12Z",
      "updated_at": "2024-12-15T04:44:33Z",
      "topics": [],
      "readme": "# Pypi本地镜像服务器搭建\n\n\n## 主要功能\n\n- 全镜像同步(可以指定镜像源)\n- 下载指定依赖包\n- 定时同步 \n\n## 快速开始\n1. 安装依赖\n```shell \npip install schedule==1.2.2 pip2pi==0.8.2\n```\n\n2. 执行main.py\n```shell\npython main.py\n```\n此时可以看到packages目录下有所有的包和一个sample文件夹，如果需要在内网环境下使用，请把sample拷贝进内网机即可。\n\n\n3.配置pypi索引服务器\n可以使用python,也可以使用Nginx，Nginx配置可以查看[https://jhacker.cn/2024/pypi_server](https://jhacker.cn/2024/pypi_server)\n```shell\n#在下载目录里创建server服务，8080为端口号，可以随意设置：\ncd packages\npython -m http.server 8080\n```\n4.打开网页就可以看所有的包了\n```html\nhttp://localhost:8080/simple/\n```\n使用本地镜像服务器安装\n```shell\npip install numpy -i http://localhost:8080/simple/\n```\n\n## 配置说明\n\n- 具体配置文件可以查看config.json\n- requirements.txt中内置了一些常用的依赖包，可以根据自己需求添加\n- 如果想同步清华源全部依赖，可以执行```get_pypy_list.py```\n- 清华源的所有依赖```tsinghua_pkgs.txt```\n- ```schedule_task.py```可以设置定时任务，每天/每周同步更新官方源\n\n- **platform** 参数用于指定目标平台，以便下载与指定平台兼容的二进制包,以下是常见的配置内容：\n\n|配置内容| 说明                                                  |\n|--|-----------------------------------------------------|\n|win32| Windows 32位系统                                       |\n|win_amd64| Windows 64位系统（大多数人是这个）                              |\n|win_arm64| Windows ARM64系统                                     |\n|manylinux1_x86_64| 使用 manylinux1 标准构建的 Linux 64位系统（CentOS 5及更高版本兼容）    |\n|manylinux2010_x86_64| 使用 manylinux2010 标准构建的 Linux 64位系统（CentOS 6及更高版本兼容） |\n|manylinux2014_x86_64 | 使用 manylinux2014 标准构建的 Linux 64位系统（CentOS 7及更高版本兼容） |\n|linux_i686| Linux 32位系统                                         |\n|macosx_10_9_x86_64| macOS 10.9及更高版本的 Intel 64位系统                        |\n|macosx_11_0_arm64| macOS 11.0及更高版本的 ARM64系统                            |\n\n- **python_versions** 指的是python版本，只需要写大的版本号即可，如3.6、3.7、3.8、3.9等\n\n"
    },
    {
      "name": "XpiritBV/AzOpenAI-RAG-ChatApp",
      "stars": 3,
      "img": "https://avatars.githubusercontent.com/u/9567984?s=40&v=4",
      "owner": "XpiritBV",
      "repo_name": "AzOpenAI-RAG-ChatApp",
      "description": "A Solution Accelerator for the RAG pattern running in Azure, using Azure AI Search for retrieval and Azure OpenAI large language models to power ChatGPT-style and Q&A experiences. This includes most common requirements and best practices.",
      "homepage": "https://azure.microsoft.com/products/search",
      "language": "Python",
      "created_at": "2024-04-30T09:24:06Z",
      "updated_at": "2024-11-22T02:34:40Z",
      "topics": [],
      "readme": "---\nname: Chat with your data - Solution accelerator (Python)\ndescription: Chat with your data using OpenAI and AI Search with Python.\nlanguages:\n- python\n- typescript\n- bicep\n- azdeveloper\nproducts:\n- azure-openai\n- azure-cognitive-search\n- azure-app-service\n- azure\n- azure-bot-service\n- document-intelligence\n- azure-functions\n- azure-storage-accounts\n- azure-speech\npage_type: sample\nurlFragment: chat-with-your-data-solution-accelerator\n\n---\n<!-- YAML front-matter schema: https://review.learn.microsoft.com/en-us/help/contribute/samples/process/onboarding?branch=main#supported-metadata-fields-for-readmemd -->\n\n# Chat with your data - Solution accelerator\n\n\n ##### Table of Contents\n * [User story](#user-story)\n    + [About this repo](#about-this-repo)\n    + [When should you use this repo](#when-should-you-use-this-repo)\n    + [Key features](#key-features)\n    + [Target end users](#target-end-users)\n    + [Industry scenario](#industry-scenario)\n  * [Deploy](#deploy)\n    + [Pre-requisites](#pre-requisites)\n    + [Products used](#products-used)\n    + [Required licenses](#required-licenses)\n    + [Pricing Considerations](#pricing-considerations)\n    + [Deploy instructions](#deploy-instructions)\n    + [Testing the deployment](#testing-the-deployment)\n  * [Supporting documentation](#supporting-documentation)\n    + [Best practices](docs/best_practices.md)\n    + [Getting Support](SUPPORT.md)\n    + [Resource links](#resource-links)\n    + [Licensing](#licensing)\n  * [Customer truth](docs/customer_truth.md)\n  * [Disclaimers](#disclaimers)\n\\\n\\\n![User Story](/docs/images/userStory.png)\n## User story\nWelcome to the *Chat with your data* Solution accelerator repository! The *Chat with your data* Solution accelerator is a powerful tool that combines the capabilities of Azure AI Search and Large Language Models (LLMs) to create a conversational search experience. This solution accelerator uses an Azure OpenAI GPT model and an Azure AI Search index generated from your data, which is integrated into a web application to provide a natural language interface, including [speech-to-text](docs/speech_to_text.md) functionality, for search queries. Users can drag and drop files, point to storage, and take care of technical setup to transform documents. Everything can be deployed in your own subscription to accelerate your use of this technology.\n\n![Solution Architecture - Chat with your data](/docs/images/cwyd-solution-architecture.png)\n\n### About this repo\n\nThis repository provides an end-to-end solution for users who want to query their data with natural language. It includes a well designed ingestion mechanism for multiple file types, an easy deployment, and a support team for maintenance. The accelerator demonstrates both Push or Pull Ingestion; the choice of orchestration (Semantic Kernel or LangChain) and should be the minimum components needed to implement a RAG pattern. It is not intended to be put into Production as-is without experimentation or evaluation of your data. It provides the following features:\n\n* Chat with an Azure OpenAI model using your own data\n* Upload and process your documents\n* Index public web pages\n* Easy prompt configuration\n* Multiple chunking strategies\n\n### When should you use this repo?\n\nIf you need to customize your scenario beyond what [Azure OpenAI on your data](https://learn.microsoft.com/azure/ai-services/openai/concepts/use-your-data) offers out-of-the-box, use this repository.\nBy default, this repo comes with one specific set of RAG configurations including but not limited to: chunk size, overlap, retrieval/search type and system prompt. It is important that you evaluate the retrieval/search and the generation of the answers for your data and tune these configurations accordingly before you use this repo in production. For a starting point to understand and perform RAG evaluations, we encourage you to look into the [RAG Experiment Accelerator](https://github.com/microsoft/rag-experiment-accelerator).\n\nThe accelerator presented here provides several options, for example:\n* The ability to ground a model using both data and public web pages\n* A backend with support for 'custom' and 'On Your Data' [conversation flows](./docs/conversation_flow_options.md)\n* Advanced prompt engineering capabilities\n* An admin site for ingesting/inspecting/configuring your dataset on the fly\n* Push or Pull model for data ingestion:  See [integrated vectorization](./docs/integrated_vectorization.md) documentation for more details\n* Running a Retrieval Augmented Generation (RAG) solution locally\n\n*Have you seen [ChatGPT + Enterprise data with Azure OpenAI and AI Search demo](https://github.com/Azure-Samples/azure-search-openai-demo)? If you would like to experiment: Play with prompts, understanding RAG pattern different implementation approaches, see how different features interact with the RAG pattern and choose the best options for your RAG deployments, take a look at that repo.\n\nHere is a comparison table with a few features offered by Azure, an available GitHub demo sample and this repo, that can provide guidance when you need to decide which one to use:\n\n| Name\t| Feature or Sample? |\tWhat is it? | When to use? |\n| ---------|---------|---------|---------|\n|[\"Chat with your data\" Solution Accelerator](https://aka.ms/ChatWithYourDataSolutionAccelerator) - (This repo)\t| Azure sample | End-to-end baseline RAG pattern sample that uses Azure AI Search as a retriever.\t| This sample should be used by Developers when the  RAG pattern implementations provided by Azure are not able to satisfy business requirements. This sample provides a means to customize the solution. Developers must add their own code to meet requirements, and adapt with best practices according to individual company policies. |\n|[Azure OpenAI on your data](https://learn.microsoft.com/azure/ai-services/openai/concepts/use-your-data) | Azure feature | Azure OpenAI Service offers out-of-the-box, end-to-end RAG implementation that uses a REST API or the web-based interface in the Azure AI Studio to create a solution that connects to your data to enable an enhanced chat experience with Azure OpenAI ChatGPT models and Azure AI Search. | This should be the first option considered for developers that need an end-to-end solution for Azure OpenAI Service with an Azure AI Search retriever. Simply select supported data sources, that ChatGPT model in Azure OpenAI Service , and any other Azure resources needed to configure your enterprise application needs. |\n|[Azure Machine Learning prompt flow](https://learn.microsoft.com/azure/machine-learning/concept-retrieval-augmented-generation)\t| Azure feature | RAG in Azure Machine Learning is enabled by integration with Azure OpenAI Service for large language models and vectorization. It includes support for Faiss and Azure AI Search as vector stores, as well as support for open-source offerings, tools, and frameworks such as LangChain for data chunking. Azure Machine Learning prompt flow offers the ability to test data generation, automate prompt creation, visualize prompt evaluation metrics, and integrate RAG workflows into MLOps using pipelines.  | When Developers need more control over processes involved in the development cycle of LLM-based AI applications, they should use Azure Machine Learning prompt flow to create executable flows and evaluate performance through large-scale testing. |\n|[ChatGPT + Enterprise data with Azure OpenAI and AI Search demo](https://github.com/Azure-Samples/azure-search-openai-demo) | Azure sample | RAG pattern demo that uses Azure AI Search as a retriever. | Developers who would like to use or present an end-to-end demonstration of the RAG pattern should use this sample. This includes the ability to deploy and test different retrieval modes, and prompts to support business use cases. |\n|[RAG Experiment Accelerator](https://github.com/microsoft/rag-experiment-accelerator) | Tool |The RAG Experiment Accelerator is a versatile tool that helps you conduct experiments and evaluations using Azure AI Search and RAG pattern. | RAG Experiment Accelerator is to make it easier and faster to run experiments and evaluations of search queries and quality of response from OpenAI. This tool is useful for researchers, data scientists, and developers who want to, Test the performance of different Search and OpenAI related hyperparameters. |\n\n\n### Key features\n- **Private LLM access on your data**: Get all the benefits of ChatGPT on your private, unstructured data.\n- **Single application access to your full data set**: Minimize endpoints required to access internal company knowledgebases. Reuse the same backend with the [Microsoft Teams Extension](docs/teams_extension.md)\n- **Natural language interaction with your unstructured data**: Use natural language to quickly find the answers you need and ask follow-up queries to get the supplemental details, including [Speech-to-text](docs/speech_to_text.md).\n- **Easy access to source documentation when querying**: Review referenced documents in the same chat window for additional context.\n- **Data upload**: Batch upload documents of [various file types](docs/supported_file_types.md)\n- **Accessible orchestration**: Prompt and document configuration (prompt engineering, document processing, and data retrieval)\n\n\n**Note**: The current model allows users to ask questions about unstructured data, such as PDF, text, and docx files. See the [supported file types](docs/supported_file_types.md).\n\n### Target end users\nCompany personnel (employees, executives) looking to research against internal unstructured company data would leverage this accelerator using natural language to find what they need quickly.\n\nThis accelerator also works across industry and roles and would be suitable for any employee who would like to get quick answers with a ChatGPT experience against their internal unstructured company data.\n\nTech administrators can use this accelerator to give their colleagues easy access to internal unstructured company data. Admins can customize the system configurator to tailor responses for the intended audience.\n\n### Industry scenario\nThe sample data illustrates how this accelerator could be used in the financial services industry (FSI).\n\nIn this scenario, a financial advisor is preparing for a meeting with a potential client who has expressed interest in Woodgrove Investments’ Emerging Markets Funds. The advisor prepares for the meeting by refreshing their understanding of the emerging markets fund's overall goals and the associated risks.\n\nNow that the financial advisor is more informed about Woodgrove’s Emerging Markets Funds, they're better equipped to respond to questions about this fund from their client.\n\nNote: Some of the sample data included with this accelerator was generated using AI and is for illustrative purposes only.\n\n![One-click Deploy](/docs/images/oneClickDeploy.png)\n## Deploy\n### Pre-requisites\n- Azure subscription - [Create one for free](https://azure.microsoft.com/free/) with owner access.\n- Approval to use Azure OpenAI services with your Azure subcription. To apply for approval, see [here](https://learn.microsoft.com/en-us/azure/ai-services/openai/overview#how-do-i-get-access-to-azure-openai).\n- [Enable custom Teams apps and turn on custom app uploading](https://learn.microsoft.com/en-us/microsoftteams/platform/concepts/build-and-test/prepare-your-o365-tenant#enable-custom-teams-apps-and-turn-on-custom-app-uploading) (optional: Teams extension only)\n\n### Products used\n- Azure App Service\n- Azure Application Insights\n- Azure Bot\n- Azure OpenAI\n- Azure Document Intelligence\n- Azure Function App\n- Azure Search Service\n- Azure Storage Account\n- Azure Speech Service\n- Teams (optional: Teams extension only)\n\n### Required licenses\n- Microsoft 365 (optional: Teams extension only)\n\n### Pricing Considerations\n\nThis solution accelerator deploys multiple resources. Evaluate the cost of each component prior to deployment.\n\nThe following are links to the pricing details for some of the resources:\n- [Azure OpenAI service pricing](https://azure.microsoft.com/pricing/details/cognitive-services/openai-service/). GPT and embedding models are charged separately.\n- [Azure AI Search pricing](https://azure.microsoft.com/pricing/details/search/). AI Search core service and semantic ranker are charged separately.\n- [Azure Blob Storage pricing](https://azure.microsoft.com/pricing/details/storage/blobs/)\n- [Azure Functions pricing](https://azure.microsoft.com/pricing/details/functions/)\n- [Azure AI Document Intelligence pricing](https://azure.microsoft.com/pricing/details/ai-document-intelligence/)\n- [Azure Web App Pricing](https://azure.microsoft.com/pricing/details/app-service/windows/)\n\n### Deploy instructions\n\nThere are two choices; the \"Deploy to Azure\" offers a one click deployment where you don't have to clone the code, alternatively if you would like a developer experience, follow the [Local deployment instructions](./docs/LOCAL_DEPLOYMENT.md).\n\nThe demo, which uses containers pre-built from the main branch is available by clicking this button:\n\n[![Deploy to Azure](https://aka.ms/deploytoazurebutton)](https://portal.azure.com/#create/Microsoft.Template/uri/https%3A%2F%2Fraw.githubusercontent.com%2FAzure-Samples%2Fchat-with-your-data-solution-accelerator%2Fmain%2Finfra%2Fmain.json)\n\n**Note**: The default configuration deploys an OpenAI Model \"gpt-35-turbo\" with version 0613. However, not all\nlocations support this version. If you're deploying to a location that doesn't support version 0613, you'll need to\nswitch to a lower version. To find out which versions are supported in different regions, visit the\n[GPT-35 Turbo Model Availability](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models#gpt-35-turbo-model-availability) page.\n\n### Testing the deployment\n1. Navigate to the admin site, where you can upload documents. It will be located at:\n\n    `https://web-{RESOURCE_TOKEN}-admin.azurewebsites.net/`\n\n    Where `{RESOURCE_TOKEN}` is uniquely generated during deployment. This is a combination of your subscription and the name of the resource group. Then select **Ingest Data** and add your data. You can find sample data in the `/data` directory.\n\n    ![A screenshot of the admin site.](./docs/images/admin-site.png)\n\n\n2. Navigate to the web app to start chatting on top of your data. The web app can be found at:\n\n    `https://web-{RESOURCE_TOKEN}.azurewebsites.net/`\n\n\n    ![A screenshot of the chat app.](./docs/images/web-unstructureddata.png)\n\n\\\n\\\n![Supporting documentation](/docs/images/supportingDocuments.png)\n## Supporting documentation\n\n### Resource links\n\nThis solution accelerator deploys the following resources. It's critical to comprehend the functionality of each. Below are the links to their respective documentation:\n- [Application Insights overview - Azure Monitor | Microsoft Learn](https://learn.microsoft.com/en-us/azure/azure-monitor/app/app-insights-overview?tabs=net)\n- [Azure OpenAI Service - Documentation, quickstarts, API reference - Azure AI services | Microsoft Learn](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/use-your-data)\n- [Using your data with Azure OpenAI Service - Azure OpenAI | Microsoft Learn](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/use-your-data)\n- [Content Safety documentation - Quickstarts, Tutorials, API Reference - Azure AI services | Microsoft Learn](https://learn.microsoft.com/en-us/azure/ai-services/content-safety/)\n- [Document Intelligence documentation - Quickstarts, Tutorials, API Reference - Azure AI services | Microsoft Learn](https://learn.microsoft.com/en-us/azure/ai-services/document-intelligence/?view=doc-intel-3.1.0)\n- [Azure Functions documentation | Microsoft Learn](https://learn.microsoft.com/en-us/azure/azure-functions/)\n- [Azure Cognitive Search documentation | Microsoft Learn](https://learn.microsoft.com/en-us/azure/search/)\n- [Speech to text documentation - Tutorials, API Reference - Azure AI services - Azure AI services | Microsoft Learn](https://learn.microsoft.com/en-us/azure/ai-services/speech-service/index-speech-to-text)\n- [Bots in Microsoft Teams - Teams | Microsoft Learn](https://learn.microsoft.com/en-us/microsoftteams/platform/bots/what-are-bots) (Optional: Teams extension only)\n\n### Licensing\n\nThis repository is licensed under the [MIT License](LICENSE.md).\n\nThe data set under the /data folder is licensed under the [CDLA-Permissive-2 License](CDLA-Permissive-2.md).\n\n## Disclaimers\nThis Software requires the use of third-party components which are governed by separate proprietary or open-source licenses as identified below, and you must comply with the terms of each applicable license in order to use the Software. You acknowledge and agree that this license does not grant you a license or other right to use any such third-party proprietary or open-source components.\n\nTo the extent that the Software includes components or code used in or derived from Microsoft products or services, including without limitation Microsoft Azure Services (collectively, “Microsoft Products and Services”), you must also comply with the Product Terms applicable to such Microsoft Products and Services. You acknowledge and agree that the license governing the Software does not grant you a license or other right to use Microsoft Products and Services. Nothing in the license or this ReadMe file will serve to supersede, amend, terminate or modify any terms in the Product Terms for any Microsoft Products and Services.\n\nYou must also comply with all domestic and international export laws and regulations that apply to the Software, which include restrictions on destinations, end users, and end use. For further information on export restrictions, visit https://aka.ms/exporting.\n\nYou acknowledge that the Software and Microsoft Products and Services (1) are not designed, intended or made available as a medical device(s), and (2) are not designed or intended to be a substitute for professional medical advice, diagnosis, treatment, or judgment and should not be used to replace or as a substitute for professional medical advice, diagnosis, treatment, or judgment. Customer is solely responsible for displaying and/or obtaining appropriate consents, warnings, disclaimers, and acknowledgements to end users of Customer’s implementation of the Online Services.\n\nYou acknowledge the Software is not subject to SOC 1 and SOC 2 compliance audits. No Microsoft technology, nor any of its component technologies, including the Software, is intended or made available as a substitute for the professional advice, opinion, or judgement of a certified financial services professional. Do not use the Software to replace, substitute, or provide professional financial advice or judgment.\n\nBY ACCESSING OR USING THE SOFTWARE, YOU ACKNOWLEDGE THAT THE SOFTWARE IS NOT DESIGNED OR INTENDED TO SUPPORT ANY USE IN WHICH A SERVICE INTERRUPTION, DEFECT, ERROR, OR OTHER FAILURE OF THE SOFTWARE COULD RESULT IN THE DEATH OR SERIOUS BODILY INJURY OF ANY PERSON OR IN PHYSICAL OR ENVIRONMENTAL DAMAGE (COLLECTIVELY, “HIGH-RISK USE”), AND THAT YOU WILL ENSURE THAT, IN THE EVENT OF ANY INTERRUPTION, DEFECT, ERROR, OR OTHER FAILURE OF THE SOFTWARE, THE SAFETY OF PEOPLE, PROPERTY, AND THE ENVIRONMENT ARE NOT REDUCED BELOW A LEVEL THAT IS REASONABLY, APPROPRIATE, AND LEGAL, WHETHER IN GENERAL OR IN A SPECIFIC INDUSTRY. BY ACCESSING THE SOFTWARE, YOU FURTHER ACKNOWLEDGE THAT YOUR HIGH-RISK USE OF THE SOFTWARE IS AT YOUR OWN RISK.\n"
    },
    {
      "name": "matthewbolanos/sk-party-planning-committee",
      "stars": 3,
      "img": "https://avatars.githubusercontent.com/u/1409190?s=40&v=4",
      "owner": "matthewbolanos",
      "repo_name": "sk-party-planning-committee",
      "description": null,
      "homepage": null,
      "language": "C#",
      "created_at": "2024-05-11T08:32:30Z",
      "updated_at": "2024-06-03T19:04:39Z",
      "topics": [],
      "readme": "# Semantic Kernel Party Planning Committee solution\n\nThis is a sample project that demonstrates how to use agents built with Semantic Kernel to create\na home automation system across three different languages (Python, .NET, and Java). Together, the\nagents built in this solution will help you throw an amazing party!\n\n> [!IMPORTANT]  \n> The party planning committee is just now getting started, we're eager to bring more agents into\n> the mix so they can help us plan the best parties ever! 🎉 (PRs are welcome)\n\n// Include gif of the app in action\n\n## What's included?!\nThis sample has it all! (Or at least it tries to). Here's what you can expect to find in this sample:\n- A retro inspired console application to interact with your agents across _all three languages_ (wow)\n- Controllers that mimic the OpenAI Assistant's API in Python, .NET, and Java with the help of Semantic Kernel\n- Plugin services that provide your agents the ability to complete party planning tasks\n- A MongoDB database to store your chats and party planning data\n\n### Available agents\n| Agent          | Description                                  | Python | .NET | Java |\n| -------------- | -------------------------------------------- | ------ | ---- | ---- |\n| Lighting Agent | Controls (and syncs) the lights in your home | ✅     | ✅   | ✅    |\n| DJ Agent       | Synthesizes music on the fly for your party  | ✅     |      |      |\n| Security Agent | Keeps your home safe from party crashers     | ✅     |      |      |\n\n### Available plugins\nAll plugin services are .NET based, but the plugins themselves are language agnostic. \nThis means any of your agents can use these plugins!\n\n| Plugin                  | Description                                             |\n| ----------------------- | ------------------------------------------------------- |\n| Home Plugin             | Provides access to your home and rooms                  |\n| Light Plugin            | Provides access to your lights                          |\n| Color Theme Plugin      | Provides access to color themes  (powered by Weaviate!) |\n| Music Generation Plugin | Allows agents to make new music on the fly              |\n| Synchronization Plugin  | Syncs your devices with music                           |\n| Speaker Plugin          | Provides access to your speakers                        |\n\n## Getting Started\nGetting started is easy! Just run the following command<del>s</del> to get your party planning\ncommittee up and running.\n\n```bash\ndocker-compose up --detach --build && docker-compose exec -it ui python main.py  \n```\n\nWow! So easy! 🎉\n\nOnce the app is running, you'll be prompted to provide the required configuration\n(e.g., API keys and LLM endpoints) for your agents.\n\nIf you ever want to start fresh (i.e., reset the database), you can run the following command:\n\n```bash\ndocker-compose down --volumes\n```\n\nContinue reading if you would like to set up your configuration before running the app.\n\n### Setting up your configuration (pre-deployment)\nIf you would like to set up your configuration before running the app so that you aren't\nprompted every time you start the app, you can follow the instructions below (no matter\nwhat language you are using!).\n\n\n```json\n{\n    \"OpenAI\": {\n        // Choose the following options based on your deployment type\n        //  - AzureOpenAI: Used if you are using Azure's OpenAI's service\n        //  - OpenAI: Used if you are using OpenAI's service\n        //  - Other: Used if you are using another deployment (e.g., Ollama) that provides an OpenAI API\n        \"DeploymentType\": \"AzureOpenAI\",\n        \"ApiKey\": \"your-api-key\",\n        \"ModelId\": \"your-model-id\",\n\n        // Set if you are using AzureOpenAI\n        // The deployment name may differ from the model name\n        \"DeploymentName\": \"your-models-deployment-name\",\n\n        // Set if you are using AzureOpenAI or Other\n        \"Endpoint\": \"your-endpoint\",\n\n        // Set if you are using OpenAI\n        \"OrgId\": \"your-org-id\"\n    }\n}\n```"
    },
    {
      "name": "akshata29/aoaiworkshop",
      "stars": 3,
      "img": "https://avatars.githubusercontent.com/u/18509807?s=40&v=4",
      "owner": "akshata29",
      "repo_name": "aoaiworkshop",
      "description": null,
      "homepage": null,
      "language": "HTML",
      "created_at": "2024-04-18T15:41:46Z",
      "updated_at": "2024-08-21T20:12:57Z",
      "topics": [],
      "readme": ""
    },
    {
      "name": "jordanbean-msft/semantic-kernel-intent",
      "stars": 3,
      "img": "https://avatars.githubusercontent.com/u/84806584?s=40&v=4",
      "owner": "jordanbean-msft",
      "repo_name": "semantic-kernel-intent",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2024-01-27T15:09:13Z",
      "updated_at": "2025-02-11T14:29:22Z",
      "topics": [],
      "readme": "# semantic-kernel-intent\n\n![architecture](./.img/architecture.png)\n\nThe Retrieval, Augment, Generatation (RAG) pattern is a very effective pattern for grounding the Azure OpenAI service with relevant data in order to respond to user questions.\n\nThe RAG pattern is needed because we have to limit the data that is passed to the OpenAI service to only relevant information. We cannot pass all the data we have with each query, both due to token limitations and due to the tendency for the AI to hallucinate if we give it too much irrelevant information.\n\nThe RAG pattern lets us query an external data source (such as Azure AI Search) to get relevant information that we can append to the prompt when we call OpenAI.\n\nThis pattern is easy to implement if there is only one external data source.\n\nHowever, we might want to separate the Azure AI Search indexes into multiple indexes. Common reasons to do this might be due to not wanting to have 1 index with documents that might have similar keywords, but are used in different contexts. It might also be due to different authorization requirements.\n\nWe can use Semantic Kernel to help orchestrate this workflow.\n\nOur Intent plugin is a \"prompt\" plugin, meaning it is based upon a prompt (as opposed to a \"native\" plugin, which would be implemented in code (C#, Python, etc.).\n\nWe will craft our Intent plugin prompt to allow the OpenAI service to determine the user's intent by providing example questions, keywords, etc. to help it decide which domain the question is specifically targeting.\n\nWe are going to provide a list of pre-defined intents (choices) to restrict what can be chosen. We will then switch off this value when the result comes back.\n\nHere is my [blog post](https://jordanbeandev.com/how-to-build-an-azure-openai-intent-plugin-in-semantic-kernel-to-help-orchestrate-which-azure-ai-search-index-to-use/) that goes into more detail.\n\n## Disclaimer\n\n**THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.**\n\n## Prerequisites\n\n- [Azure CLI](https://docs.microsoft.com/en-us/cli/azure/install-azure-cli)\n- Azure subscription & resource group\n- [Azure OpenAI](https://learn.microsoft.com/en-us/azure/ai-services/openai/overview)\n- [Azure AI Search](https://learn.microsoft.com/en-us/azure/search/search-what-is-azure-search)\n\n## Run\n\n1. Deploy the Azure OpenAI service & the Azure AI Search\n    - Note that this demo assumes there is a 'movies' and 'songs' index in the AI Search service. You would modify these as needed for your index names.\n\n1. Create a `.env` file in the `./src` directory to indicate to the app where to find your Azure services and how to authenticate.\n\n```python\nAZURE_OPENAI_DEPLOYMENT_NAME=\"\"\nAZURE_OPENAI_ENDPOINT=\"\"\nAZURE_OPENAI_API_KEY=\"\"\nAZURE_AISEARCH_URL=\"\"\nAZURE_AISEARCH_API_KEY=\"\"\n```\n  \n1. Run the following code to execute the sample app.\n\n```shell\ncd src\n\npython -m venv ./api_env\n\n./api_env/Scripts/activate\n\npython -m pip install -r requirements.txt\n\npython ./main.py\n```\n\n## Links\n\n- [Semantic Kernel](https://learn.microsoft.com/en-us/semantic-kernel/overview/)\n- [Intent plugin sample in Semantic Kernel demo code](https://github.com/microsoft/semantic-kernel/tree/main/samples/plugins/IntentDetectionPlugin/AssistantIntent)\n"
    },
    {
      "name": "ai-workshop-ise/ai-hands-on-lab",
      "stars": 3,
      "img": "https://avatars.githubusercontent.com/u/157475392?s=40&v=4",
      "owner": "ai-workshop-ise",
      "repo_name": "ai-hands-on-lab",
      "description": null,
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2024-01-23T09:24:46Z",
      "updated_at": "2024-07-12T07:16:55Z",
      "topics": [],
      "readme": "# Welcome to RAG Evaluation in Action\nGo to the [workshop's URL](https://ai-workshop-ise.github.io/ai-hands-on-lab/0.intro.html) and enjoy learning about RAG in a hands-on way.\n\n![intro](./book/images/workshop.jpg)"
    },
    {
      "name": "Josephrp/trueralablabsubmission",
      "stars": 3,
      "img": "https://avatars.githubusercontent.com/u/18212928?s=40&v=4",
      "owner": "Josephrp",
      "repo_name": "trueralablabsubmission",
      "description": "always learning new tools and new teks!",
      "homepage": "",
      "language": "Python",
      "created_at": "2023-12-07T10:26:14Z",
      "updated_at": "2023-12-13T02:36:26Z",
      "topics": [],
      "readme": "# Multi-Modal Multi-Model Evaluation to Optimize Downstream Application Performance\n\n## Our Solution\n\n##### [Try The Demo For Multimed Here](https://github.com/Josephrp/trueralablabsubmission/blob/main/trythedemo.md)\n\n## Abstract\n\nThis paper presents an approach for multimodal application performance optimization using the TruEra Machine Learning Ops platform for model evaluation. 5 Vision Models, 6 Audio Models, and 7 Text Models were evaluated based on prompting, performance, and various sequential configurations to produce the best downstream outcomes based on human evaluation. The selected configuration and prompts are available in a demo here.\n\n## Problem Statement\n\nEnterprise application prototypers face the problem that each model is a little bit different. Some novel functions or capabilities can also drastically improve an agent’s performance. Multimodality compounds this issue and makes assessments all the more time-consuming and technically challenging. Intel, Google Vertex, Milvus, and TruEra, provide models, model serving, retrieval-augmented generation, and evaluation, respectively. But what is their optimal configuration for a given demo?\n\n## Literature Review\n\nWe have selected as many models as time allowed which was 7 days. Some models had endpoints in huggingface, others in Google Vertex. We also fine-tuned models described below.\n\n### Vision Models\n\nThe following vision models were included in the study:\n\n- **GPT4V**: A vision model based on the GPT-4 architecture, designed for image generation and analysis.\n- **LLava-1.5**: An advanced image processing model known for its robust feature extraction capabilities.\n- **Qwen-VL**: A vision-language model focused on understanding and generating multimodal content.\n- **Clip (Google/Vertex)**: A model developed by Google, designed for image recognition and classification, leveraging Vertex AI's capabilities.\n- **Fuyu-8B**: Great vision model previously retained over GPT4V for its ability to process images and available endpoint.\n\n### Audio Models\n\nThe audio models evaluated were:\n\n- **Seamless 1.0 & 2.0**: Two versions of an audio processing model, known for their speech recognition and audio analysis capabilities.\n- **Qwen Audio**: A model specializing in audio processing and understanding.\n- **Whisper2 & Whisper3**: Advanced versions of an audio model designed for speech-to-text conversion and audio analysis.\n- **Seamless on device**: A variant of the Seamless model optimized for on-device applications.\n- **GoogleAUDIOMODEL**: A comprehensive audio model developed by Google, known for its accuracy in speech recognition and audio processing.\n\n### Text Models\n\nThe text models included in the study were:\n\n- **StableMed (StableLM Finetune)**: A specialized version of StableLM, fine-tuned for medical text analysis.\n- **MistralMed (Mistral Finetune)**: A fine-tuned version of the Mistral model, tailored for medical text processing.\n- **Qwen On Device**: A text processing model optimized for on-device applications.\n- **GPT**: The base GPT model, known for its general text generation and processing capabilities.\n- **Mistral Endpoint**: A variant of the Mistral model, designed for endpoint applications.\n- **Intel Neural Chat**: A text-based model developed by Intel, focusing on conversational AI.\n- **BERT (Google/Vertex)**: A model developed by Google, using Vertex AI, known for its effectiveness in understanding and processing natural language.\n\n## Methods\n\nThis study aimed to evaluate the performance of various vision, audio, and text models in producing downstream outcomes, with a focus on human-centered applications. The evaluation was structured around three main axes: prompting strategies, performance metrics, and sequential configurations of the models. The models were assessed based on their ability to process and generate information in their respective domains (vision, audio, text) and their effectiveness in integrated applications.\n\n### Global Evaluation Criteria\n\nThe models were evaluated based on the following criteria:\n\n- **Prompting**: The effectiveness of different prompting strategies in eliciting accurate and relevant responses from the models.\n- **Performance**: Quantitative metrics such as accuracy, speed, and error rate were used to assess the performance of each model.\n- **Sequential Configurations**: The models were combined in various sequential configurations to determine the most effective combinations for specific tasks.\n\n### Human Evaluation\n\nThe ultimate measure of each model's effectiveness was based on human evaluation. A panel of n=1 with domain-specific knowledge assessed the outputs for relevance, accuracy, and utility.\n\n### Using TruEra For Multimodal AI Application Lifecycle Evaluation\n\n#### Initial Phase\n\nIn the early stages of application development, TruEra assists in data preparation, input selection and engineering, model architecture choice, and hyperparameter tuning. This foundational support is crucial for building robust models.\n\n#### Model Evaluation and Improvement\n\nPost-development, models are introduced to the TruEra platform for quality assessment. The platform's diagnostic capabilities enable users to identify and rectify weaknesses, thereby enhancing model strength and reliability.\n\n#### Deployment and Continuous Improvement\n\nOnce a model meets the desired criteria, it is deployed into the production environment. TruEra's monitoring tools play a critical role in this phase, offering ongoing support and insights for continuous model improvement.\n\n## Evaluation Results\n\n\n\n| Category | Model            | Subcategory          | Evaluation Results                                                                                   |\n|----------|------------------|----------------------|------------------------------------------------------------------------------------------------------|\n| Vision   | GPT4V            | Image Generation     | Requires jailbreak, not effective for medical cases, prone to unavailability.                        |\n|          | LLava-1.5        | Image Processing     | High quality but outperformed by Fuyu in quality metric.                                              |\n|          | Qwen-VL          | Vision-Language      | Superior in quality response, versatile in features (e.g., image in response).                       |\n|          | Clip (Google/Vertex) | Image Recognition  | Limited capabilities; fine-tuned models improve performance in pipelines.                            |\n|          | Fuyu-8B          | Image Processing     | Previously SOTA, now surpassed by Qwen-VL.                                                           |\n| Audio    | Seamless 1.0     | Audio Processing     | Previous SOTA for capability and cost.                                                               |\n|          | Seamless 2.0     | Audio Processing     | Current SOTA for capability and cost (TruEra).                                                       |\n|          | Qwen Audio       | Audio Processing     | Inconsistent or hallucinatory responses (TruEra).                                                    |\n|          | Whisper2 & Whisper3 | Speech-to-Text     | On-device models lacking text-to-speech capability.                                                  |\n|          | Seamless on device | Audio Processing   | Smaller version of Seamless without text-to-speech.                                                  |\n|          | GoogleAUDIOMODEL | Speech Recognition   | Served using Vertex, lacks text-to-speech capability.                                                |\n| Text     | StableMed (StableLM Finetune) | Medical Text Analysis | Previous SOTA, retained for zero marginal cost.                              |\n|          | MistralMed (Mistral Finetune) | Medical Text Processing | Better performance, costly for self-hosting.                                |\n|          | Qwen On Device   | Text Processing      | Retained for zero marginal cost, better performance than StableMed.                                  |\n|          | GPT              | Text Generation      | Poor performance.                                                                                    |\n|          | Mistral Endpoint | Text Processing      | Zero marginal cost, but MistralMed performs better.                                                  |\n|          | Intel Neural Chat | Conversational AI   | Did not render quality results, costs are prohibitive.                                               |\n|          | BERT (Google/Vertex) | Natural Language Understanding | Cost of serving the model is prohibitive.                               |\n\n\n### Vision Models\n\n- **GPT4V**: Requires a jailbreak to work, prone to require additional jailbreaks as censorship evolves. Did not render for most medical-related cases. Prone to unavailability.\n- **LLava-1.5**: Beaten on the quality metric by Fuyu.\n- **Qwen-VL**: Beat Fuyu on the quality response metric and has more capabilities that can be built into a feature: for example, an image in the response.\n- **Clip (Google/Vertex)**: Clip lacked capabilities but fine-tuned CLIP models help the overall response as part of a pipeline.\n- **Fuyu-8B**: Previous SOTA model. (displaced by Qwen)\n\n### Audio Models\n\n- **Seamless 1.0**: Previous SOTA for capability and cost.\n- **Seamless 2.0**: Current SOTA for capability and cost (TruEra)\n- **Qwen Audio**: Produced inconsistent or hallucinatory responses (TruEra)\n- **Whisper2 & Whisper3**: Whisper are on-device models that do not have the required text-to-speech capability.\n- **Seamless on device**: Smaller seamless without text-to-speech.\n- **GoogleAUDIOMODEL**: Served using vertex, does not have text-to-speech.\n\n### Text Models\n\n- **StableMed (StableLM Finetune)**: Previous SOTA - Retained for its marginal cost of 0.\n- **MistralMed (Mistral Finetune)**: Better performance, costly for self-hosting.\n- **Qwen On Device**: Retained for its marginal cost of zero and better performance than StableMed.\n- **GPT**: Poor performance.\n- **Mistral Endpoint**: Marginal cost of zero but Mistral Med is better.\n- **Intel Neural Chat**: Did not render quality results, costs are prohibitive.\n- **BERT (Google/Vertex)**: Cost of serving the model is prohibitive.\n\n## Further Research\n"
    },
    {
      "name": "msalemor/sk-summarizer-pattern",
      "stars": 3,
      "img": "https://avatars.githubusercontent.com/u/44649358?s=40&v=4",
      "owner": "msalemor",
      "repo_name": "sk-summarizer-pattern",
      "description": "A summarizer implementation using Semantic Kernel",
      "homepage": null,
      "language": "TypeScript",
      "created_at": "2023-09-22T17:36:54Z",
      "updated_at": "2024-08-17T00:25:27Z",
      "topics": [],
      "readme": "# An Azure OpenAI Summarizer implementation<br/>with Semantic Kernel C#, Python, and Python LangChain\n\nCheck out my new repo where I have implemented a summarizer (Map Reduce/Refine) implementation using Semantic Kernel and OpenAI GPT. Summarizer may not express well everything this application can do. It really uses the GPT's foundational model abilities to summarize, translate, perform risk analysis, generate content such as code and demand letters, etc.\n\nI implemented this app as a C# Minimal API serving both static files and acting as an API server, but with minor modifications, the same code could power an async job to process a large number of files, for example, in a storage account.\n\nSummarizer is also a powerful playground. You don’t need to give it a large text source. It can reply to a simple prompt. However, if you do give it a large text resource, you can accomplish pretty amazing tasks.\n\nSummarization and RAG pattern can be combined into a powerful solution where based on users' choices the system could answer from multiple sources using the RAG pattern or provide deep answers and insights from specific documents and sources using summarization.\n\n## Frontend\n\n**Note:** I've kept most of the code in the `src/frontend/src/App.tsx`` file for simpler understanding.\n\n- Bun javascript runtime and all-in-one tool\n- React\n  - Axios\n  - React-markdown\n- TailwindCSS\n\n## C# Backend\n\n- .NET 7 C# Minimal API\n- Semantic Kernel (still in Preview)\n- Middleware:\n  - Static Files\n  - CORS\n\n## Python Semantic Kernel Backend\n\nRequirements: `requirements.txt`\n\n```txt\nfastapi\nuvicorn[standard]\nsemantic-kernel\npython-dotenv\n```\n\n## Python LangChain Backend\n\nRequirements: `requirements.txt`\n\n```txt\nfastapi\nuvicorn[standard]\nlangchain\npython-dotenv\n```\n\n## Required Server environment variables\n\n**Note:** To ge these values, you will need an Azure OpenAI account and deploy a GPT model to a region.\n\nOn the `src/backend` and `src/pybackend` folders, you will need to create a `.env` file and set the following values:\n\n```bash\nDEPLOYMENT_NAME=<MODEL_NAME>\nENDPOINT=https://<NAME>.openai.azure.com/\nAPI_KEY=<API_KEY>\n```\n\n### Running locally\n\nFrom a Bash/zsh prompt type the following commands:\n\n### CSharp\n\n- Type: `make run`\n- Open a browser at: `http://localhost:5084`\n\n#### Python SK\n\n- Type: `make run-py`\n- Open a browser at: `http://localhost:8000`\n\n#### Python SK\n\n- Type: `make run-pylang`\n- Open a browser at: `http://localhost:8000`\n\n### Run as a container locally using Docker\n\n#### CSharp\n\n- Type: `make docker-run`\n- Open a browser at: `http://localhost:8080`\n\n#### Python SK\n\n- Type: `make docker-py-run`\n- Open a browser at: `http://localhost:8080`\n\n#### Python LangChain\n\n- Type: `make docker-pylang-run`\n- Open a browser at: `http://localhost:8080`\n\n### Building a Docker Container\n\n**Note:** Make sure to provide the required server environment variables if running from somewhere else.\n\n- Type: `make docker`\n\n## Samples use cases\n\n### Process a simple query\n\n![Picture shows an image of a answer to a simple prompt](images/sksm-1.png)\n\n### Analyzis and content generation\n\n![Picture shows the system finding a delinquent customer and writing a letter](images/sksm-2.png)\n\n### Text translation\n\n![Picture shows an image of a text document being translated from English to Urdu](images/sksm-3.png)\n\n### Summarization\n\n![Picture shows of a document being summarized.](images/sksm-4.png)\n\n### Summarization and risk analysis\n\n![Picture shows a legal document being summarized and analyzed for risks.](images/sksm-5.png)\n"
    },
    {
      "name": "aymenfurter/azure-chat-with-your-photos-demo",
      "stars": 3,
      "img": "https://avatars.githubusercontent.com/u/20464460?s=40&v=4",
      "owner": "aymenfurter",
      "repo_name": "azure-chat-with-your-photos-demo",
      "description": "Chatbot that comprehends uploaded images and engages in detailed conversations about their content.",
      "homepage": "",
      "language": "Bicep",
      "created_at": "2023-10-14T15:29:05Z",
      "updated_at": "2024-06-26T12:30:38Z",
      "topics": [
        "gpt-4-vision",
        "gpt-4v",
        "gpt4",
        "openai"
      ],
      "readme": "# Start a conversation with your Photos\n<img style=\"border-radius: 10px;\" src=\"architecture.png?raw=true\">\n\nThis example demonstrates how to create a ChatGPT-style application using the Retrieval Augmented Generation (RAG). It leverages Azure's OpenAI Service to access the ChatGPT model (gpt-3.5-turbo) and employs Azure Cognitive Search Vector Search for data indexing and retrieval.\n\n## Features\nThe chatbot can analyze and comprehend the details of your uploaded images. This enables you to interact with the chatbot as if you were conversing with someone who has viewed the same photos. \n\n<img style=\"border-radius: 10px;\" src=\"screenshot.png?raw=true\">\n\n## Prerequisites \nTo deploy the application, please ensure that you have the following dependencies installed on your machine.\n* [Azure Developer CLI](https://aka.ms/azure-dev/install)\n* [Python 3.9+](https://www.python.org/downloads/)\n* [Node.js 14+](https://nodejs.org/en/download/)\n* [Git](https://git-scm.com/downloads)\n* [Bash / WSL](https://learn.microsoft.com/en-us/windows/wsl/install) \n\n## Installation\n### CAUTION\n- This repository is still in early development.\n- Your photos are currently stored in a public Azure Blob Storage container. Please do not upload any sensitive data.\n\n## Setup procedure\n1. Clone the repository and navigate to the project root\n2. Run `azd auth login`\n3. Put the photos you want to index in the `./data` folder\n4. Run `azd up` \n5. Make sure to select a region with quota available for both GPT-4-32k & GPT-4-Vision. (e.g. 'switzerlandnorth') as the region. \n\nThe current indexing process is suboptimal. During my evaluation, the system could only handle an approximate rate of 300 images hourly.\n\n## Future Improvements\n- [ ] Improve indexing performance\n- [ ] Ability to query based on image metadata (e.g. location)\n- [ ] Bot Framework Support (In progress)\n- [ ] Reduce Hallucinations / Prompt tweaks\n"
    },
    {
      "name": "passadis/react-semantic-kernel",
      "stars": 3,
      "img": "https://avatars.githubusercontent.com/u/53148138?s=40&v=4",
      "owner": "passadis",
      "repo_name": "react-semantic-kernel",
      "description": "Build your custom AI Solution integrating Semantic Kernel into your Azure Container Apps.",
      "homepage": "https://www.cloudblogger.eu/2023/10/18/semantic-kernel-container-apps-with-react-and-python/",
      "language": "JavaScript",
      "created_at": "2023-10-17T18:00:06Z",
      "updated_at": "2025-01-10T16:26:49Z",
      "topics": [
        "azure",
        "azure-sdk",
        "azureai",
        "azurefunctions",
        "containers",
        "docker",
        "dotnet",
        "python",
        "react",
        "semantic-kernel",
        "terraform"
      ],
      "readme": "<p align=\"center\">\n  <a href=\"https://skillicons.dev\">\n    <img src=\"https://skillicons.dev/icons?i=azure,react,py,docker,terraform,vscode\" />\n  </a>\n</p>\n\n<h1 align=\"center\">Semantic Kernel with Azure Open AI Chat Implementation</h1>\n\n\n## Project Overview\n\nThis project explores the integration of Semantic Kernel with Azure Open AI to build a learning application. It assists users in exploring various topics through tutorials and quizzes generated by Azure Open AI, leveraging the power and flexibility of Azure's AI services.\n\n## Introduction\n\nWe're tapping into Azure Open AI's capabilities to control prompts and requests in our chat-based deployments. This learning app is designed to offer a dynamic and interactive educational experience, powered by the latest advancements in AI-driven content generation.\n\n## Prerequisites\n\nTo make the most of this solution, here are the prerequisites:\n\n- **Azure Subscription**: Ensure you have an Azure subscription with Open AI enabled. Apply via Microsoft Form for Azure Open AI access.\n\n- **Region Availability**: Once approved, you'll access GPT-4 in specific Azure regions including Sweden Central, Canada East, and Switzerland North.\n\n- **Development Environment**:\n  - **Docker**: Required for containerization.\n  - **Azure Functions Core Tools and Azure Account**: For Azure integrations.\n  - **Python for VSCode**: Essential for backend development.\n  - **Node.js**: For React implementation and frontend testing.\n\n## Getting Started\n\n1. **Apply for Azure OpenAI Service**: Complete the application to access Azure Open AI services.\n\n2. **Set Up Your Development Environment**: Ensure your workstation is equipped with Docker, Azure Functions Core Tools, Python, and Node.js.\n\n3. **Clone the Repository**: Get started by cloning this repository to your local environment.\n\n4. **Follow the Blog for Detailed Instructions**: For step-by-step guidance, visit [Semantic Kernel: Container Apps with React and Python](https://www.cloudblogger.eu/2023/10/18/semantic-kernel-container-apps-with-react-and-python/).\n\n## Contribute\n\nWe encourage contributions! If you have ideas on how to improve this application or want to report a bug, please feel free to open an issue or submit a pull request.\n\n\n## Architecture\n\n![SemanticArchitecture1](https://github.com/passadis/learning-aid/assets/53148138/9bab4863-e7b8-42cc-a9ff-fc4ca2d5c66d)\n"
    },
    {
      "name": "Azure-For-Everyone/sk-hotelfiltering",
      "stars": 3,
      "img": "https://avatars.githubusercontent.com/u/104997505?s=40&v=4",
      "owner": "Azure-For-Everyone",
      "repo_name": "sk-hotelfiltering",
      "description": "An example of how to use Semantic Kernel as part of a Hotel booking website for filtering hotels using natural language.",
      "homepage": "",
      "language": "JavaScript",
      "created_at": "2023-09-22T07:31:03Z",
      "updated_at": "2024-05-21T06:31:24Z",
      "topics": [
        "large-language-models",
        "llm",
        "python",
        "react",
        "semantickernel"
      ],
      "readme": "# Build Intelligent Apps: Semantic Kernel in your application\n\nIn this repository we'll show you how to leverage [Semantic Kernel](https://github.com/microsoft/semantic-kernel) into an existing application. As an example we'll use how you can benefit from Semantic Kernel in a Hotel booking website. You can use natural language to search through a list of hotels.\n\nThis demostration was shown on following events:\n\n- [Austria] Oct/23 - [Microsoft Build 2023](https://pulse.microsoft.com/de-at/transform-de-at/na/fa2-microsoft-build-austria/)\n- [Ireland] Oct/23 - [Microsoft Build 2023](https://msevents.microsoft.com/event?id=3755029226)\n- [Belgium] Nov/23 - [SaaS local day](https://msevents.microsoft.com/event?id=1131853098)\n- [Austria] Dec/23 - [Unlocking AI Opportunities:](https://msevents.microsoft.com/event?id=1236497086)\n\n![Hotel website](./img/hotel-booking-semantic-kernel.gif)\n\n## Why Semantic Kernel?\n\n[Semantic Kernel](https://github.com/microsoft/semantic-kernel) allows developers to use AI without any knowledge about AI or LLM. As a web developer or software engineer you can focus on the things you're best: writing code for business applications, and you have the LLM or other models completely abstracted.\n\n## What is in this example?\n\nThis example has two parts:\n\n1. an `ui` part which contains the Hotel booking website written in React.\n   \n   yarn start\n    \n2. an `api` part which contains the Hotel booking API written in Python and contains some custom logic and the Semantic Kernel SDK.\n\n    python main\n\n## Getting started\n\nWithin the `api` we are using a LLM model, either hosted on OpenAI or Azure OpenAI. Before you can run the backend (as mentioned above), make sure you have defined you're OpenAI credentials in the [`.env` file](https://github.com/Azure-For-Everyone/sk-hotelfiltering/blob/main/api/.env) in the `api` folder. Specify the service you which to use by providing `AzureOpenAI` or `OpenAI` to the `GLOBAL__LLM_SERVICE` variable.\n\n    GLOBAL__LLM_SERVICE=\"AzureOpenAI\" # or \"OpenAI\"\n\n    AZURE_OPEN_AI__DEPLOYMENT_TYPE=\"chat-completion\"\n    AZURE_OPEN_AI__CHAT_COMPLETION_DEPLOYMENT_NAME=\"xxx\"\n    AZURE_OPEN_AI__TEXT_COMPLETION_DEPLOYMENT_NAME=\"xxx\"\n    AZURE_OPEN_AI__ENDPOINT=\"https://xxx.openai.azure.com\"\n    AZURE_OPEN_AI__API_KEY=\"xxx\"\n\n    OPEN_AI__MODEL_TYPE=\"chat-completion\"\n    OPEN_AI__CHAT_COMPLETION_MODEL_ID=\"gpt-4\"\n    OPEN_AI__TEXT_COMPLETION_MODEL_ID=\"text-davinci-003\"\n    OPEN_AI__API_KEY=\"xxx\"\n    OPEN_AI__ORG_ID=\"xxx\"\n"
    },
    {
      "name": "Ananya-AJ/Chest-Xray_Medical_Report_generation",
      "stars": 3,
      "img": "https://avatars.githubusercontent.com/u/111623197?s=40&v=4",
      "owner": "Ananya-AJ",
      "repo_name": "Chest-Xray_Medical_Report_generation",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2023-05-10T20:41:44Z",
      "updated_at": "2025-03-23T11:46:06Z",
      "topics": [],
      "readme": "CMPE258 Term Project\nChest x-ray Report Generation and Chatbot\nTeam:\nCtrl Alt Del\n\nMembers:\nAnanya Joshi\nSanjana Kothari\nNeetha Sherra\nNaga Bathula\nProject Goals:\nThe objective of this project is to leverage the power of large language models for generating textual medical reports from chest x-ray images. By utilizing the multimodal capabilities of large language models, we aim to develop an application that takes a chest x-ray image as input and generates a medical report from the x-ray, in a similar fashion as a physician or medical practitioner would. In addition to that, we also provide a chatbot facility built using Microsoft’s Semantic Kernel that can look at the generated report and answer user questions based on the report as well as answer other general queries about chest-related questions. Therefore, it is a complete package that gives the user the diagnosis and observations from chest x-ray as well as give the user the ability to get specific answers to their questions about the report or general chest-related conditions.\n\nImages\nLocation: ../Images/\n\nhttps://openi.nlm.nih.gov/imgs/collections/NLMCXR_png.tgz\n\nReports\nLocation: ../Reports/\n\nhttps://openi.nlm.nih.gov/imgs/collections/NLMCXR_reports.tgz\n\nApp snapshots:\n\n\n![image](https://github.com/Ananya-AJ/Chest-Xray_Medical_Report_generation/assets/111623197/ed8679c2-c750-49a3-b02e-632c7b6bc76f)\n\n\n![image](https://github.com/Ananya-AJ/Chest-Xray_Medical_Report_generation/assets/111623197/567f74c7-2242-45cf-a7b3-6904b52ca403)\n\n\n\n![image](https://github.com/Ananya-AJ/Chest-Xray_Medical_Report_generation/assets/111623197/e244752c-6a03-44f3-92de-df6df4f95e1b)\n\n\nTo run the applicaton\nClone repo.\nAdd chest x-ray images in Images folder and reports in Reports folder at root level.\nAdd OpenAI api key to config.py in Code folder.\nInstall requiremnts.txt\nTo run, navigate into the Code folder and use 'streamlit run frontend.py'\n\nModels\nhttps://drive.google.com/drive/folders/1kqEZm906iXJefqE7o03wycBx2D2jw12w?usp=share_link\n"
    },
    {
      "name": "jonathanscholtes/azure-ai-foundry-agentic-workshop",
      "stars": 2,
      "img": "https://avatars.githubusercontent.com/u/4681774?s=40&v=4",
      "owner": "jonathanscholtes",
      "repo_name": "azure-ai-foundry-agentic-workshop",
      "description": "Workshop for building intelligent AI solutions using Azure AI Foundry, featuring Vector Search, RAG, Agentic AI, and multi-agent orchestration with LangChain and Azure AI Search.",
      "homepage": "",
      "language": "Jupyter Notebook",
      "created_at": "2025-04-03T20:26:47Z",
      "updated_at": "2025-04-22T23:47:39Z",
      "topics": [
        "agentic-ai",
        "azure-ai-foundry",
        "azure-ai-search",
        "langgraph",
        "python",
        "vector-search"
      ],
      "readme": "> ⚠️  \n> **This project is currently in active development and may contain breaking changes.**  \n> Updates and modifications are being made frequently, which may impact stability or functionality. This notice will be removed once development is complete and the project reaches a stable release. \n\n# Azure AI Foundry Workshop: Vector Search, Agentic AI, and LLM Orchestration  \n\n## Overview  \n\nThis **Azure AI Foundry Workshop** provides participants with practical, hands-on experience in building and deploying **Retrieval-Augmented Generation (RAG)** and **Agentic AI** solutions using **Azure AI Foundry**, **Azure AI Search**, **Azure AI Agent Service**, **LangGraph** and **Semantic Kernel Agent Framework**.\n\nParticipants will learn how to create intelligent agents that not only respond, but also take action. Through integrating OpenAPI endpoints and orchestrating workflows across multiple agents, you’ll build solutions that are dynamic, context-aware, and production-ready.\n\n\nBy the end of the workshop, you'll have:\n\n- Deploy a fully functional AI workshop environment using **Azure AI Foundry**.\n- Built RAG pipelines using **Azure AI Search** and **document embeddings**.\n- Explored **agentic patterns** using **LangGraph** and **Semantic Kernel**: single-agent, supervisor-agent, and networked agents.\n- Integrated structured external data via OpenAPI and GraphQL endpoints—giving agents the ability to query real-time data and take action through external systems.\n- Built intelligent agents using Python code, while also exploring low-code tools for LLM orchestration and agent implementation.\n\n---\n\n## 🔧 What You’ll Build\n\n- **Vector Search & RAG with Azure AI Search**  \n  Learn to index documents, generate embeddings, and implement semantic retrieval to support LLM-based answers grounded in your data.\n\n- **Agentic AI with LangGraph, Azure AI Agent Service and Semantic Kernel**  \n  Use prebuilt and custom agents to delegate tasks, make decisions, and interact with APIs. Experiment with orchestration patterns including single-agent flows, supervisor models, and decentralized networks.\n\n- **Real-World Integrations with OpenAPI & GraphQL**  \n  Connect your agents to external services, enabling them to perform real-world actions like retrieving live data, triggering workflows, or interacting with apps and systems.\n\n---\n\n\n## 🛠️ **Workshop Steps**\n\nFollow these key steps to successfully implement and deploy the workshop:\n\n### 1️⃣ [**Workshop Setup and Solution Deployment**](docs/deployment.md)  \nStep-by-step instructions to deploy **Azure AI Foundry** and all required services for the workshop environment, including:\n\n- **Azure AI Foundry** components: AI Service, AI Hub, Projects, and Compute  \n- **Azure AI Search** for knowledge retrieval and vector-based search  \n- **Azure Storage Account** for document storage and data ingestion  \n- **Azure Functions** for event-driven document chunking, embedding generation, and indexing  \n- **Azure Web App** to enable agent interactions via OpenAPI and GraphQL integrations\n\n### 2️⃣ [**RAG Setup with Azure AI Search and \"Chatting Over Your Data\"**](docs/vector-search.md)  \nStep-by-step instructions for vectorizing document data with **Azure AI Search** and quickly leveraging the data using **Azure AI Foundry's** built-in _Chat Playground_.\n\n- Processing documents, chunking them, generating embeddings with **Ada-002**, and indexing them into **Azure AI Search**  \n- Interacting with indexed data for semantic retrieval using the Azure AI Foundry Chat Playground  \n\n\n\n### 3️⃣ [**Hands-On with Agents**](docs/notebooks.md)  \nDiscover interactive Notebooks and guides that walk you through building intelligent, task-driven agents. These curated resources cover:\n\n- **Semantic retrieval** and vector search powered by Azure AI Search  \n- Orchestrating **multi-agent workflows** with LangGraph, Azure AI Agent Service, and Semantic Kernel  \n- Integration with real-time **external systems and APIs** via OpenAPI and GraphQL, allowing agents to access dynamic, structured data sources  \n- Built-in **tracing and diagnostics** to analyze and understand agent decision-making and behavior\n\n---\n\n\n## 📐 Workshop Design and Architecture\n\n![design](/media/diagram.png)\n\nThis solution combines the power of **Azure AI Foundry**, **LangGraph**, and the **Azure AI Agents Service** to build an advanced, modular AI orchestration framework. It demonstrates a production-ready architecture designed for scalable, intelligent applications that require real-time reasoning, search, and structured data integration.\n\nAt a high level, the architecture consists of the following key components:\n\n### Azure AI Foundry Core Services\nThe deployment includes Azure AI Foundry’s full stack—**AI Hub**, **AI Services**, **AI Projects**, and **Compute Instances**—providing a secure and managed environment for developing and running generative AI applications. Compute Instances are pre-configured to support **Visual Studio Code (web)**, enabling a browser-based development experience for running and modifying sample notebooks directly within the Foundry environment.\n\n### Vector Search and RAG Implementation\nUnstructured data, such as PDFs, are preprocessed through an **Azure Function** that chunks documents, generates vector embeddings using **OpenAI’s Ada-002 model**, and indexes them into **Azure AI Search**. This enables **Retrieval-Augmented Generation (RAG)** capabilities by grounding responses in your custom knowledge base.\n\n### Multi-Agent System with LangGraph\nAgents are orchestrated using **LangGraph**, a framework that enables complex workflows through node-based logic. A **Supervisor Agent** coordinates multiple specialized agents, allowing for role-based delegation, context-aware task execution, and adaptive reasoning.\n\n### Tool Integration with OpenAPI and GraphQL\nTo enable agents to interact with structured external data sources, the solution integrates tools via **OpenAPI** (for RESTful APIs) and **GraphQL** (for schema-based query interfaces). These tools extend agent capabilities, allowing them to fetch, query, or write to external systems dynamically during conversation.\n\n### Event-Driven Data Ingestion\nDocument processing is fully event-driven. When a PDF is uploaded to the designated storage container, an Azure Function is triggered to process the document end-to-end—from chunking to indexing—ensuring the search index remains up-to-date.\n\n\n## ♻️ **Clean-Up**\n\nAfter completing the workshop and testing, ensure you delete any unused Azure resources or remove the entire Resource Group to avoid additional charges.\n\n---\n\n## 📜 License  \nThis project is licensed under the [MIT License](LICENSE.md), granting permission for commercial and non-commercial use with proper attribution.\n\n---\n\n## Disclaimer  \nThis workshop and demo application are intended for educational and demonstration purposes. It is provided \"as-is\" without any warranties, and users assume all responsibility for its use."
    },
    {
      "name": "microsoft/microhacks-rag-ai",
      "stars": 2,
      "img": "https://avatars.githubusercontent.com/u/6154722?s=40&v=4",
      "owner": "microsoft",
      "repo_name": "microhacks-rag-ai",
      "description": "Microhack focused on RAG AI with the goal of providing a half-day hands-on workshop to enable skilling around Microsoft technologies and services.",
      "homepage": "",
      "language": "Python",
      "created_at": "2025-04-01T18:21:47Z",
      "updated_at": "2025-04-17T16:59:29Z",
      "topics": [],
      "readme": "<!-- \npage_type: sample\nlanguages:\n- azdeveloper\n- powershell\n- bicep\nproducts:\n- azure\n- azure-openai\n- azure-ai-search\nurlFragment: GPT-RAG\nname: Multi-repo ChatGPT and Enterprise data with Azure OpenAI and AI Search\ndescription: GPT-RAG core is a Retrieval-Augmented Generation pattern running in Azure, using Azure AI Search for retrieval and Azure OpenAI large language models to power ChatGPT-style and Q&A experiences.\n-->\n<!-- YAML front-matter schema: https://review.learn.microsoft.com/en-us/help/contribute/samples/process/onboarding?branch=main#supported-metadata-fields-for-readmemd -->\n![Alt text](docs/Microhack%20OCTO%20logo.png \"Microsoft Americas Office of the CTO\")\n\n# Microhack - RAG AI & Your Data\n\nHands-on Microhack created by Microsoft Office of the CTO Americas, focused on RAG AI.\n\n## Presented by Microsoft Americas Office of the CTO\n\nOriginally from [GPT-RAG](https://github.com/Azure/gpt-rag)\n\n## Microhack Oveview\n\nImagine you are a sales manager at Contoso, a multinational retail company that sells outdoor equipment. You need to analyze sales data to find trends, understand customer preferences, and make informed business decisions. To help you, Contoso has developed a conversational agent that can answer questions about your sales data.\n\n### Core Assets\n* [Microhack Repository](https://github.com/microsoft/microhacks-rag-ai)\n* [Microhack Deck - Agentic AI](docs/Microhack%20Deck%20-%20RAG%20AI.pptx)\n* [Microhack Challenges](docs/Microhack%20Challenges%20-%20RAG%20AI.pdf)\n\n### What you will learn\n\nBy the end of this workshop, you will learn to build an agent app using Azure AI for RAG, Azure AI Search, explore its tools, and effectively use instructions to guide the LLM.\n\n## Setting the Stage\n\nTo provide attendees with guidance and understanding on the Microhack goals, concepts, and technologies. You can leverage the [Microhack Deck - RAG AI](docs/Microhack%20Deck%20-%20RAG%20AI.pptx).\n\n## Microhack Challenges\n\nTo get started with this workshop, open the current [Microhack Challenges](docs/Microhack%20Challenges%20-%20RAG%20AI.pdf).\n\nThe **RAG pattern** enables businesses to use the reasoning capabilities of LLMs, using their existing models to process and generate responses based on new data. RAG facilitates periodic data updates without the need for fine-tuning, thereby streamlining the integration of LLMs into businesses. \n\nThe **Enterprise RAG** Solution Accelerator (GPT-RAG) offers a robust architecture tailored for enterprise-grade deployment of the RAG pattern. It ensures grounded responses and is built on Zero-trust security and Responsible AI, ensuring availability, scalability, and auditability. Ideal for organizations transitioning from exploration and PoC stages to full-scale production and MVPs.\n\n## Application Components\n\nGPT-RAG follows a modular approach, consisting of three components, each with a specific function.\n\n* **[Data Ingestion](code/gpt-rag-ingestion)** - Optimizes data chunking and indexing for the RAG retrieval step.\n\n* **[Orchestrator](code/gpt-rag-orchestrator)** - Coordinates the flow to retrieve information and generate a user response using Semantic Kernel functions.\n\n* **[App Front-End](code/gpt-rag-frontend)** - Uses the [Backend for Front-End](https://learn.microsoft.com/en-us/azure/architecture/patterns/backends-for-frontends) pattern to provide a scalable and efficient web interface.\n\n<!-- * [Teams-BOT](https://github.com/Azure/gpt-rag-bot) Constructed using Azure BOT Services, this platform enables users to engage with the Orchestrator seamlessly through the Microsoft Teams interface. -->\n\n<!-- \nRemoving temporarily while not finished\n## GPT-RAG Integration HUB\n* [SQL Integration](https://github.com/Azure/gpt-rag-int-sql) Connect the GPT-RAG Infrastructure to SQL using NL2SQL. -->\n\n## Concepts\n\nIf you want to learn more about the RAG Pattern and GPT-RAG architecture.\n\n* [RAG Pattern: What and Why?](docs/RAG_CONCEPTS.md)\n\n* [Solution Architecture Overview](docs/ARCHITECTURE.md)\n\n* [ZeroTrust Architecture Overview](media/GPT-RAG-ZeroTrust.png)\n\n<!-- <a href=\"https://www.youtube.com/watch?v=ICsf4yirieA\"><img src=\"https://img.youtube.com/vi/ICsf4yirieA/0.jpg\" alt=\"Alt text\" width=\"480\"/></a> -->\n\n## Setup Guide\n\n1) **Basic Architecture Deployment:** *for quick demos with no network isolation*⚙️\n\nLearn how to **quickly set up** the basic architecture for scenarios without network isolation. [Click the link to proceed](#getting-started).\n\n\n## Getting Started\n\nThis guide will walk you through the deployment process of Enterprise RAG. Before beginning the deployment, please ensure you have prepared all the necessary tools and services as outlined in the **Pre-requisites** section.\n\n**Pre-requisites**\n\n - Azure Developer CLI: [Download azd](https://learn.microsoft.com/en-us/azure/developer/azure-developer-cli/install-azd?tabs=winget-windows%2Cbrew-mac%2Cscript-linux&pivots=os-windows) \n   - Ensure the correct OS is selected\n - Powershell 7+ with AZ module (Windows only): [Powershell](https://learn.microsoft.com/en-us/powershell/scripting/install/installing-powershell-on-windows?view=powershell-7.4#installing-the-msi-package), [AZ Module](https://learn.microsoft.com/en-us/powershell/azure/what-is-azure-powershell?view=azps-11.6.0#the-az-powershell-module)\n - Git: [Download Git](https://git-scm.com/downloads)\n - Node.js 16+ [windows/mac](https://nodejs.dev/en/download/)  [linux/wsl](https://nodejs.dev/en/download/package-manager/)\n - Python 3.11: [Download Python](https://www.python.org/downloads/release/python-3118/)\n - Initiate an [Azure AI services creation](https://portal.azure.com/#create/Microsoft.CognitiveServicesAllInOne) and agree to the Responsible AI terms **\n   - ** If you have not created an Azure AI service resource in the subscription before\n\n### Basic Architecture Deployment\n\nFor quick demonstrations or proof-of-concept projects without network isolation requirements, you can deploy the accelerator using its basic architecture.\n![Basic Architecture](media/architecture-GPT-RAG-Basic.png)\n\nThe deployment procedure is quite simple, just install the prerequisites mentioned above and follow these four steps using [Azure Developer CLI (azd)](https://aka.ms/azure-dev/install) in a terminal:\n\n\n**1** Download the Repository:\n\n```sh\nazd init\n```\n\n**1.a** Give the environment a unique name. This will be used to create your resources. For example, _cowboy-hats_ would create resource group _rg-cowboy-hats_.\n```sh\nEnter a new environment name: some-name-here\n```\n\n**2** Login to Azure:\n\n**2.a** Azure Developer CLI:\n\n```sh\nazd auth login\n```\n\n**2.b** Azure CLI:\n\n```sh\naz login\n```\n**2.c** Select your Azure Subscription from list.\n\n**3** Start Building the infrastructure and components deployment:\n\n```sh\nazd up\n```\n**3.a** Select your Azure Subscription from list.\n\n**3.b** Select Azure region. _Recommended: East US (eastus)_\n\n**4** Add source documents to object storage from _/datasources_ directory.\n\nUpload your documents to the 'documents' folder located in the storage account. The name of this account should start with 'strag'. This is the default storage account, as shown in the sample image below.\n\n ![storage_sample](media/readme-storage_sample.png)\n\n**Done! Basic deployment is completed.**\n\n**Recommended**: [Add app authentication](https://learn.microsoft.com/en-us/azure/app-service/scenario-secure-app-authentication-app-service). [Watch this quick tutorial](https://youtu.be/sA-an25jMB4) for step-by-step guidance.\n\n## How to?\n\n### Customize Your Deployment\n\nThe standard deployment process sets up Azure resources and deploys the accelerator components with a standard configuration. To tailor the deployment to your specific needs, follow the steps in the [Custom Deployment](docs/CUSTOMIZATIONS.md) section for further customization options.\n\n### Integrate with Additional Data Sources\n  \nExpand your data retrieval capabilities by integrating new data sources such as Bing Custom Search, SQL Server, and Teradata. For detailed instructions, refer to the [AI Integration Hub](docs/AI_INTEGRATION_HUB.md) page.\n\n### Multi-Environment Deployment\n\nOnce you've successfully deployed the GPT-RAG solution as a proof of concept and you're ready to formalize the deployment using a proper CI/CD process to accelerate your deployment to production, refer to the multi-environment deployment guides for either [Azure DevOps](./docs/AZDO-SETUP.md) or [GitHub](./docs/GH-SETUP.md).\n \n### Troubleshoot Deployment Issues\n\nIf you encounter any errors during the deployment process, consult the [Troubleshooting](docs/TROUBLESHOOTING.md) page for guidance on resolving common issues.\n\n### Evaluate Performance\n\nTo assess the performance of your deployment, refer to the [Performance Testing](docs/PERFTEST.md) guide for testing methodologies and best practices.\n\n### Query the Conversation History\n\nLearn how to query and analyze conversation data by following the steps outlined in the [How to Query and Analyze Conversations](docs/QUERYING_CONVERSATIONS.md) document.\n\n### Estimate Pricing\n\nUnderstand the cost implications of your deployment by reviewing the [Pricing Model](https://github.com/Azure/GPT-RAG/wiki/GPT%E2%80%90RAG-%E2%80%90-Pricing-Model) for detailed pricing estimation.\n\n### Manage Governance\n\nEnsure proper governance of your deployment by following the guidelines provided in the [Governance Model](https://share.mindmanager.com/#publish/9ogrdWqzmAzZB6ilgURohV4lj1LriKjOWc0w_u2U).\n\n## Contributing\n\nWe appreciate your interest in contributing to this project! Please refer to the [CONTRIBUTING.md](./CONTRIBUTING.md) page for detailed guidelines on how to contribute, including information about the Contributor License Agreement (CLA), code of conduct, and the process for submitting pull requests.\n\nThank you for your support and contributions!\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n"
    },
    {
      "name": "edemnati/hackathon-may-2025",
      "stars": 2,
      "img": "https://avatars.githubusercontent.com/u/22805269?s=40&v=4",
      "owner": "edemnati",
      "repo_name": "hackathon-may-2025",
      "description": null,
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2025-04-15T21:19:49Z",
      "updated_at": "2025-04-16T17:49:44Z",
      "topics": [],
      "readme": "---\nname: Hackathon\ndescription: Test Azure OpenAI + Azure PostresSQL Flexible Server (pgvector + Azure_AI extensions).\nlanguages:\n- python\nproducts:\n- azure-database-postgresql\n- azure OpenAI\n- azure\n---\n\n# References of useful Github repositories:\n1. azure-postgres-pgvector-python: https://github.com/Azure-Samples/azure-postgres-pgvector-python\n1. Semantic Kernel: https://github.com/microsoft/semantic-kernel/tree/main/python\n1. python-ai-agent-frameworks-demos: https://github.com/Azure-Samples/python-ai-agent-frameworks-demos\n1. Azure Openai Samples: https://github.com/Azure-Samples/openai\n1. Azure AI Samples: https://github.com/Azure-Samples/azureai-samples\n\n\n \n"
    },
    {
      "name": "Qredence/Agentic-Kernel",
      "stars": 2,
      "img": "https://avatars.githubusercontent.com/u/148253410?s=40&v=4",
      "owner": "Qredence",
      "repo_name": "Agentic-Kernel",
      "description": "A flexible foundation AI system for creating A2A-compatible autonomous AI agents that can collaborate, reason, and execute complex tasks through standardized agent-to-agent communication protocols.",
      "homepage": "",
      "language": "Python",
      "created_at": "2025-04-02T13:26:49Z",
      "updated_at": "2025-04-17T04:24:50Z",
      "topics": [
        "a2a",
        "adk-python",
        "ag2",
        "agentic-ai",
        "ai-agents",
        "gemini",
        "langchain",
        "mcp",
        "multi-agent",
        "reasoning-agent"
      ],
      "readme": "<!-- Optional: Add a project logo/banner here -->\n<!-- <p align=\"center\"><img src=\"path/to/your/logo.png\" alt=\"Agentic Kernel Logo\" width=\"200\"/></p> -->\n\n# Agentic Kernel: A Modular Framework for Autonomous AI Agents\n\n\n<div align=\"center\">\n  \n[![Weave Badge](https://img.shields.io/endpoint?url=https%3A%2F%2Fapp.workweave.ai%2Fapi%2Frepository%2Fbadge%2Forg_X84uIR347D2freSZkxeu4S9S%2F959239750&cacheSeconds=3600)](https://app.workweave.ai/reports/repository/org_X84uIR347D2freSZkxeu4S9S/959239750) \n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n[![Python Version](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)\n[![PyPI version](https://badge.fury.io/py/agentic-kernel.svg)](https://badge.fury.io/py/agentic-kernel)\n[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)\n\n</div>\n\n**Build, orchestrate, and manage sophisticated multi-agent systems with ease.**\n---\n\n\nAgentic Kernel provides a robust and flexible foundation for creating A2A-compatible autonomous AI agents that can collaborate, reason, and execute complex tasks through standardized agent-to-agent communication protocols. \nBuilt on Google's A2A standard at its core, and leveraging the ADK (Agent Development Kit) framework, it implements key interoperability features like capability discovery, consensus building, and collaborative memory while offering a modular architecture, dynamic workflow management, and seamless integration capabilities.\n\n## ✨ Key Features\n\n* **🤖 Modular Multi-Agent Architecture:** \nDesign systems with specialized agents, dynamic registration, and secure communication.\n  \n* **⚙️ Sophisticated Workflow Engine:** \nIntelligently decompose tasks, track progress in real-time, handle errors gracefully, and manage concurrent execution.\n  \n* **🧠 Dynamic Planning & Orchestration:** \nFeatures a powerful Orchestrator Agent capable of creating, managing, and adapting complex plans using a nested loop architecture.\n  \n* **🔌 Pluggable Components:** \nEasily extend functionality with custom plugins, tools, and memory systems.\n  \n* **💬 Standardized Communication:** Agents interact using a clear and consistent message format, compliant with\n  Google's [A2A (Agent-to-Agent) interoperability standard](https://developers.googleblog.com/en/a2a-a-new-era-of-agent-interoperability/).\n  \n* **🖥️ Interactive UI:** Includes a Chainlit-based interface for real-time interaction, task visualization, and\n  monitoring.\n  \n* **🛠️ Rich Tooling & Integration:** Leverage built-in tools and integrate with external systems (e.g., via MCP).\n\n\n## 🚀 Getting Started\n\nFollow these steps to get Agentic Kernel up and running on your local machine.\n\n**Prerequisites:**\n\n* Python 3.10 or higher\n* `uv` (recommended) or `pip` package manager\n* Git (for cloning the repository)\n\n**Installation & Setup:**\n\n1. **Clone the Repository (if you haven't already):**\n    ```bash\n    git clone https://github.com/qredence/agentic-kernel.git\n    cd agentic-kernel\n    ```\n\n2. **Create and Activate a Virtual Environment:**\n\n    * **Using `uv` (Recommended):**\n      ```bash\n      # Install uv if you don't have it (e.g., pip install uv)\n      uv venv\n      source .venv/bin/activate\n      ```\n    * **Using standard `venv`:**\n      ```bash\n      python -m venv .venv\n      source .venv/bin/activate # On Windows use: .venv\\Scripts\\activate\n      ```\n\n3. **Install Dependencies:**\n    ```bash\n    # Using uv\n    uv sync\n    ```\n\n4. **Configure Environment Variables:**\n    * Copy the example environment file:\n      ```bash\n      cp .env.example .env\n      ```\n    * Edit the `.env` file and add your API keys and endpoints for required services (e.g., Azure OpenAI, specific\n      tools).\n\n**Running the [A2A Agents Orchestrations (ADK Chat)](./src/agentic_kernel/adk_chat/README.md):**\n\n1. **Ensure your virtual environment is active.**\n\n    ```bash\n    # Using uv\n    uv venv\n    source .venv/bin/activate\n    ``` \n    \n\n## Setup\n\n1. **Install Dependencies:** From the workspace root (`Agentic-Kernel`), install the required packages:\n   ```bash\n   uv pip install -r src/agentic_kernel/adk_chat/requirements.txt\n   ```\n\n2. **Configure Environment Variables:** Ensure you have the necessary API keys set in your `.env` file (or the specific `.env` within the `adk_chat` directory if you prefer):\n   ```\n   OPENAI_API_KEY=your_openai_api_key\n   GOOGLE_VERTIAI_API_KEY=your_google_vertai_api_key\n   GEMINI_API_KEY=your_google_ai_studio_key\n\n   # Note: The old GOOGLE_API_KEY is deprecated. Please migrate to GOOGLE_VERTIAI_API_KEY and GEMINI_API_KEY.\n\n   ```\n\n## Running the Example\n\nFrom the workspace root (`Agentic-Kernel`), run:\n\n```bash\npython src/agentic_kernel/adk_chat/main.py\n```\n\nThis will start the chat server and client, allowing you to interact with the multi-agent system.\n\n### Running with Mesop UI\n\nYou can also run the system with a web-based UI using Mesop:\n\n```bash\npython src/agentic_kernel/adk_chat/main.py --mode mesop\n```\n\nThis will start the server and launch the Mesop UI in your default web browser. The UI provides a more user-friendly\ninterface for interacting with the multi-agent system, with features like:\n\n- Agent information display\n- Chat history with markdown formatting\n- Message input with real-time feedback\n- Visual indicators for processing state\n\n## 🏛️ System Architecture\n\nAgentic Kernel employs a modular design centered around interacting components:\n\n```\nsrc/agentic_kernel/\n├── agents/         # Specialized agent implementations (e.g.,    Orchestrator, Worker)\n├── communication/  # Protocols and message formats for inter-agent communication\n├── config/        # Configuration loading and management\n├── ledgers/       # State tracking for tasks and progress\n├── memory/        # Systems for agent memory and knowledge storage\n├── orchestrator/  # Core logic for workflow planning and execution\n├── plugins/       # Extensible plugin system for adding capabilities\n├── systems/       # Foundational system implementations\n├── tools/         # Reusable tools agents can leverage\n├── ui/           # User interface components (e.g., Chainlit app)\n├── utils/        # Helper functions and utilities\n├── workflows/     # Definitions and handlers for specific workflows\n└── adk_chat/      # ADK A2A Chat System\n```\n\n### A2A Compliance\n\nAgentic Kernel is compliant with\nGoogle's [A2A (Agent-to-Agent) interoperability standard](https://developers.googleblog.com/en/a2a-a-new-era-of-agent-interoperability/),\nwhich enables seamless communication and collaboration between different agent systems. Key A2A features include:\n\n- **Capability Discovery**: Agents can advertise their capabilities and discover the capabilities of other agents.\n- **Agent Discovery**: Agents can announce their presence and find other agents in the system.\n- **Standardized Message Format**: All agent communication follows a consistent format with required A2A fields.\n- **Consensus Building**: Agents can request and build consensus on decisions.\n- **Conflict Resolution**: The system provides mechanisms for detecting and resolving conflicts between agents.\n- **Task Decomposition**: Complex tasks can be broken down into subtasks and distributed among agents.\n- **Collaborative Memory**: Agents can share and access a common memory space.\n\nTo test A2A compliance, run the provided test script:\n\n```bash\npython src/debug/test_a2a_compliance.py\n```\n\n### Core Concepts\n\n* **Agents:** Autonomous units with specific capabilities (e.g., planning, executing, validating). The\n  `OrchestratorAgent` is key for managing complex tasks.\n* **Workflows:** Sequences of steps managed by the Workflow Engine, involving task decomposition, execution, and\n  monitoring.\n* **Communication Protocol:** A standardized JSON format for messages exchanged between agents.\n* **Ledgers:** Track the state and progress of tasks and workflows.\n* **Plugins & Tools:** Extend agent functionality by providing access to external capabilities or data.\n\nRefer to the code documentation within each directory for more detailed information.\n\n## 📚 Examples & Usage\n\nExplore the capabilities of Agentic Kernel through practical examples:\n\n* **Core Feature Examples (`docs/examples/`)**: Detailed markdown files demonstrating specific functionalities like:\n    * Advanced Plugin Usage\n    * Agent Communication Patterns\n    * Basic Workflow Definition\n    * Memory System Interaction\n    * Orchestrator Features (Conditional Steps, Dynamic Planning, Error Recovery)\n    * Workflow Optimization\n\n* **Multi-Agent System (`examples/adk_multi_agent/`)**: A complete example showcasing collaboration between multiple\n  agents (Task Manager, Worker, Validator).\n    * See the [Multi-Agent Example README](examples/adk_multi_agent/README.md) for setup and execution instructions.\n\n* **ADK A2A Chat System (`src/agentic_kernel/adk_chat/`)**: A multi-agent chat system using Google's Agent Development Kit (\n  ADK) and Agent-to-Agent (A2A) communication protocol.\n    * Features specialized agents (Orchestrator, Research, Creative, Reasoning) that communicate using the A2A protocol\n    * Includes both a command-line interface and a web-based UI using Mesop\n    * See the [ADK A2A Chat README](src/agentic_kernel/adk_chat/README.md) for setup and execution instructions\n\n## 🤝 Contributing\n\nWe welcome contributions! Please read our `CONTRIBUTING.md` guide to learn about our development process, how to propose\nbug fixes and improvements, and coding standards.\n\n## 📜 License\n\nThis project is licensed under the MIT License. See the [LICENSE](LICENSE.md) file for details.\n\n## 🐛 Debugging\n\n* The `src/debug/` directory contains scripts useful for isolating and testing specific components of the kernel.\n  Explore these scripts if you encounter issues or want to understand individual parts better."
    },
    {
      "name": "HosseinZahed/multi-agent-cloud-architect",
      "stars": 2,
      "img": "https://avatars.githubusercontent.com/u/19933353?s=40&v=4",
      "owner": "HosseinZahed",
      "repo_name": "multi-agent-cloud-architect",
      "description": "A multi-agent cloud architect",
      "homepage": null,
      "language": "Python",
      "created_at": "2025-03-14T14:13:34Z",
      "updated_at": "2025-04-19T10:01:13Z",
      "topics": [],
      "readme": "# 🌐🤖 Multi-Agent Cloud Architect\n\n🌐 A multi-agent cloud architect\n\n## 🛠️ Setup Instructions\n\n### 1. 🐍 Create a Python Virtual Environment\nTo create a Python virtual environment, run the following command in the root of the project:\n\n```bash\npy -m venv .venv\n```\n\nActivate the virtual environment:\n- On Windows:\n  ```bash\n  .venv\\Scripts\\activate\n  ```\n- On macOS/Linux:\n  ```bash\n  source .venv/bin/activate\n  ```\n\n### 2. 📄 Create a `.env` File\n\n\nCreate a `.env` file at the root of the project and add the following environment variables:\n\n```env\nAZURE_OPENAI_ENDPOINT=\"https://models.inference.ai.azure.com\"\nGITHUB_TOKEN=\"<your_github_token>\"\n\n### Uncomment the lines below to enable OAuth authentication with Azure EntraID\n#CHAINLIT_URL=\"http://localhost:8000\"\n#CHAINLIT_AUTH_SECRET=\"<your_chainlit_auth_secret>\"\n\n#OAUTH_AZURE_AD_CLIENT_ID=\"<your_oauth_azure_ad_client_id>\"\n#OAUTH_AZURE_AD_CLIENT_SECRET=\"<your_oauth_azure_ad_client_secret>\"\n#OAUTH_AZURE_AD_TENANT_ID=\"<your_oauth_azure_ad_tenant_id>\"\n#OAUTH_AZURE_AD_ENABLE_SINGLE_TENANT=True\n```\n⚠️**Notes:**\n- Navigate to [GitHub Developer Settings](https://github.com/settings/tokens) and create a Personal Access Token (PAT). Use this token for the `GITHUB_TOKEN` variable. No specific scope is required.\n- The `CHAINLIT_AUTH_SECRET` is a secret key used for authentication. You can generate it by running the following command:\n  ```bash\n  chainlit create-secret\n  ```\n\n### 3. 📦 Install Dependencies\nAfter setting up the virtual environment and `.env` file, install the required dependencies:\n\n```bash\npip install -r requirements.txt\n```\n\n### 4. 🚀 Run Chainlit Apps\nTo run the Chainlit apps, use the following commands:\n\n1. 🤖 **Autogen Multi-Agent App**\n   ```bash\n   chainlit run ag_multi_agent.py\n   ```\n\n2. 🤖 **Semantic Kernel Single-Agent App**\n   ```bash\n   chainlit run sk_single_agent.py\n   ```\n\n3. 🤖 **Semantic Kernel Multi-Agent App**\n   ```bash\n   chainlit run sk_multi_agent.py\n   ```\n\n"
    },
    {
      "name": "Azure-Samples/analyst",
      "stars": 2,
      "img": "https://avatars.githubusercontent.com/u/1844662?s=40&v=4",
      "owner": "Azure-Samples",
      "repo_name": "analyst",
      "description": "A multi-agentic data interpreter that operates on behalf of a data analyst",
      "homepage": null,
      "language": "Python",
      "created_at": "2025-04-01T12:41:40Z",
      "updated_at": "2025-04-15T15:40:00Z",
      "topics": [],
      "readme": "# Project Name\n\n(short, 1-3 sentenced, description of the project)\n\n## Features\n\nThis project framework provides the following features:\n\n* Feature 1\n* Feature 2\n* ...\n\n## Getting Started\n\n### Prerequisites\n\n(ideally very short, if any)\n\n- OS\n- Library version\n- ...\n\n### Installation\n\n(ideally very short)\n\n- npm install [package name]\n- mvn install\n- ...\n\n### Quickstart\n(Add steps to get up and running quickly)\n\n1. git clone [repository clone url]\n2. cd [repository name]\n3. ...\n\n\n## Demo\n\nA demo app is included to show how to use the project.\n\nTo run the demo, follow these steps:\n\n(Add steps to start up the demo)\n\n1.\n2.\n3.\n\n## Resources\n\n(Any additional resources or related projects)\n\n- Link to supporting information\n- Link to similar sample\n- ...\n"
    },
    {
      "name": "ambarishg/SEMANTIC-KERNEL-AGENTS",
      "stars": 2,
      "img": "https://avatars.githubusercontent.com/u/3221164?s=40&v=4",
      "owner": "ambarishg",
      "repo_name": "SEMANTIC-KERNEL-AGENTS",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2025-04-06T09:03:30Z",
      "updated_at": "2025-04-08T17:52:07Z",
      "topics": [],
      "readme": ""
    },
    {
      "name": "karan-pers/realtime-audio-acs",
      "stars": 2,
      "img": "https://avatars.githubusercontent.com/u/203891145?s=40&v=4",
      "owner": "karan-pers",
      "repo_name": "realtime-audio-acs",
      "description": "Low latency Voicebot solution",
      "homepage": null,
      "language": "Python",
      "created_at": "2025-03-18T17:57:03Z",
      "updated_at": "2025-04-19T13:56:09Z",
      "topics": [],
      "readme": "# VoiceBOT with ACS and GPT-4o-realtime-audio\n\nThis application demonstrates the integration of Azure Communication Services with GPT-4o-realtime audio API using Semantic Kernel.\n\nThis implementation is inspired by original ACS openAI implementation found [here](https://github.com/Azure-Samples/communication-services-python-quickstarts/tree/main/callautomation-openai-sample).\n\n## Highover Architecture\n\n![image info](./audio-realtime.png)\n\n\n## Prerequisites\n\n- An Azure account with an active subscription. [Create an account for free](https://azure.microsoft.com/free/?WT.mc_id=A261C142F).\n- A deployed Communication Services resource. [Create a Communication Services resource](https://docs.microsoft.com/azure/communication-services/quickstarts/create-communication-resource).\n- A [phone number](https://learn.microsoft.com/en-us/azure/communication-services/quickstarts/telephony/get-phone-number) in your Azure Communication Services resource that can get inbound calls. NB: phone numbers are not available in free subscriptions.\n- [Python](https://www.python.org/downloads/) 3.9 or above.\n- An Azure OpenAI Resource and Deployed Model. See [instructions](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/create-resource?pivots=web-portal).\n- An Azure AI Search Deployment with Vector embedding Model. See [instructions](ai_search\\README.md).\n- Install `uv`, see [the uv docs](https://docs.astral.sh/uv/getting-started/installation/).\n\n\n### Setup and host your Azure DevTunnel\n\n[Azure DevTunnels](https://learn.microsoft.com/en-us/azure/developer/dev-tunnels/overview) is an Azure service that enables you to share local web services hosted on the internet. Use the commands below to connect your local development environment to the public internet. This creates a tunnel with a persistent endpoint URL and which allows anonymous access. We will then use this endpoint to notify your application of calling events from the ACS Call Automation service.\n\n```bash\ndevtunnel login -d\ndevtunnel create --allow-anonymous\ndevtunnel port create -p 8080\ndevtunnel host\n```\n\n### Configuring application\n\n#### Setting up ACS, Azure AI Foundry and AI Search\n\nCopy the `.env.example` file to `.env` and update the following values:\n\n1. `ACS_CONNECTION_STRING`: Azure Communication Service resource's connection string.\n2. `CALLBACK_URI_HOST`: Base url of the app. (For local development use the dev tunnel url from the step above)\n3. `AZURE_OPENAI_ENDPOINT`: Azure Open AI service endpoint\n4. `AZURE_OPENAI_REALTIME_DEPLOYMENT_NAME`: Azure Open AI deployment name\n5. `AZURE_OPENAI_API_VERSION`: Azure Open AI API version, this should be one that includes the realtime api, for instance '2024-10-01-preview'\n6. `AZURE_OPENAI_API_KEY`: Azure Open AI API key, optionally, you can also use Entra Auth.\n7. `AZURE_AI_SEARCH_ENDPOINT`: Azure AI Search endpoint.\n8. `AZURE_AI_SEARCH_SERVICE`: Azure AI Search service name.\n9. `AZURE_AI_SEARCH_API_KEY`: Azure AI Search API key.\n10. `AZURE_AI_SEARCH_INDEX_NAME`: Index name for the collection in Azure AI Search.\n\n\n#### Setting up Azure OpenAI embedding model\n\nCopy the `.env.embedding.example` file to `embedding.env` and update the following values:\n\n1. `AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME`: Azure OpenAI Embedding deployment name.\n2. `AZURE_OPENAI_ENDPOINT`: Base url of the app. (For local development use the dev tunnel url from the step above)\n3. `AZURE_OPENAI_API_KEY`: Azure Open AI service endpoint\n4. `AZURE_OPENAI_API_VERSION`: Azure Open AI deployment name\n\n\n## Run the app\n\n1. Do one of the following to start the main application:\n   - run `main.py` in debug mode from your IDE (VSCode will load your .env variables into the environment automatically, other IDE's might need an extra step).\n   - execute `uv run --env-file .env main.py` directly in your terminal (this uses `uv`, which will then install the requirements in a temporary virtual environment, see [uv docs](https://docs.astral.sh/uv/guides/scripts) for more info).\n2. Browser should pop up with a simple page. If not navigate it to `http://localhost:8080/` or your dev tunnel url.\n3. Register an EventGrid Webhook for the IncomingCall(`https://<devtunnelurl>/api/incomingCall`) event that points to your devtunnel URI. Instructions [here](https://learn.microsoft.com/en-us/azure/communication-services/concepts/call-automation/incoming-call-notification).\n\nOnce that's completed you should have a running application. The way to test this is to place a call to your ACS phone number and talk to your intelligent agent!\n"
    },
    {
      "name": "ckane/ctigor",
      "stars": 2,
      "img": "https://avatars.githubusercontent.com/u/3454769?s=40&v=4",
      "owner": "ckane",
      "repo_name": "ctigor",
      "description": "Your friendly neighborhood LLM-enabled CTI Assistant",
      "homepage": null,
      "language": "Python",
      "created_at": "2025-03-22T22:34:33Z",
      "updated_at": "2025-04-14T22:46:24Z",
      "topics": [],
      "readme": "# CTIgor\n\nA CTI assistant. Read my blog entries here:\n* https://blog.malware.re/2025/03/28/ctigor/index.html\n\n![Igor from Young Frankenstein](/static/igor.gif)\n\n## Variants\n\nThere are a number of Agentic frameworks out there. I am attempting to maintain this project to work\nboth under [AutoGen](https://microsoft.github.io/autogen/stable//index.html) and\n[Semantic Kernel](https://learn.microsoft.com/en-us/semantic-kernel/overview/). The main project will\nbe developed on AutoGen, with some features being ported to the Semantic Kernel branch as time and framework\ncapabilities permit.\n\nThe `main` branch will be using AutoGen.\n\nTo see the Semantic Kernel implementation, use the [`semantic-kernel-port`](https://github.com/ckane/ctigor/tree/semantic-kernel-port)\nbranch.\n"
    },
    {
      "name": "Azure-Samples/tutor",
      "stars": 2,
      "img": "https://avatars.githubusercontent.com/u/1844662?s=40&v=4",
      "owner": "Azure-Samples",
      "repo_name": "tutor",
      "description": "An Education App that enables student evaluation and mentorship",
      "homepage": null,
      "language": "TypeScript",
      "created_at": "2024-10-31T20:03:09Z",
      "updated_at": "2025-04-07T14:50:00Z",
      "topics": [],
      "readme": "# The Tutor Application\n\n\"The Tutor\" is an intelligent tutoring platform designed to provide students with personalized educational support through interactive AI-driven experiences. The platform leverages both text-based and avatar-based interactions to engage students, offering a wide variety of learning aids such as real-time question answering, essay evaluation, and interactive conversation. By integrating advanced large, visual and multimodal language models, \"The Tutor\" helps students improve their learning outcomes while providing teachers with insightful evaluation tools.\n\n## Core Architecture and Components\n\n![Core Architecture](./.assets/architecture.png)\n\nThis project framework provides the following components:\n\n### Avatar and Textual Interaction Layers\n\nStudents can engage with the platform through two main interfaces: the Avatar Interaction and the Textual (Async) Interaction. The Avatar Interaction offers a more immersive learning experience, providing real-time conversational feedback through an AI avatar that uses a Speech Avatar Engine to synthesize speech. In parallel, the Textual Interaction allows for more asynchronous interaction, where students can receive detailed responses in text form. Both interaction methods are processed by their respective engines, the Avatar Engine and Textual Engine, which ensure a smooth and context-aware experience by leveraging Azure OpenAI and cognitive services.\n\n### AI Engine and Vector Database\n\nThe heart of \"The Tutor\" is its AI Engine (powered by Azure OpenAI), which processes interactions and provides intelligent responses based on student input. The platform stores interaction data and semantic representations in a Vector Database (Azure AI Search), allowing the AI engine to reference past conversations and retrieve relevant information for current interactions, enhancing the overall learning experience through personalization and memory retention.\n\n### Question and Essay Evaluation Systems\n\n\"The Tutor\" features two key learning modules: the Questions Engine and the Essay Engine, which allow students to submit questions and essays for automatic evaluation. The Question Interface supports instant query submissions, while the Essay Interface enables students to submit detailed essays for evaluation. Both engines are supported by machine learning models that evaluate the students' input and provide feedback.\n\n### Evaluation Models and Professor Dashboard\n\nThe evaluation of student responses is carried out by the Evaluation Model, which runs AI-based assessments on both essays and questions. The evaluation results are then made available on the Professor Dashboard, where educators can monitor student performance, adjust configurations via the Configure API, and review detailed student progress logs. The dashboard also provides access to the Evaluation History, where professors can review the historical progress of students' submissions and the AI's assessments.\n\n### Conversation and Memory Management\n\nAn essential feature of \"The Tutor\" is its Conversation History module, which stores previous interactions to maintain continuity in conversations. The Avatar Questions Memory ensures that the AI avatar can recall and build upon previous queries posed by the student, providing a coherent and personalized learning journey. The Conversation Preprocessor also uses natural language processing to enhance the quality and relevance of the conversation before passing it to the AI engine for response generation.\n\n## Business Goals and Use Cases\n\nThe Tutor might be applied on different scenarios.\n\n- Personalized Learning Experience\n\n\"The Tutor\" aims to enhance student learning by offering individualized feedback, allowing each student to learn at their own pace with personalized interactions.\n\n- Automated Student Evaluations\n\nThe platform provides automated assessment tools for essays and questions, freeing up valuable time for educators while maintaining high standards for feedback quality.\n\n- Scalability and Adaptability\n\nDesigned with scalability in mind, \"The Tutor\" can handle increasing volumes of student interactions without compromising the quality of service, thanks to its use of Azure services and AI-driven models.\n\n- Teacher Support and Insights\n\nThrough the Professor Dashboard, educators are provided with real-time insights into student progress, enabling them to tailor lesson plans and interventions based on objective data.\n\n- Continuous Learning and Memory\n\nBy using conversation history and memory, \"The Tutor\" ensures continuity in the learning process, helping students build upon previous sessions and facilitating long-term retention of knowledge.\n\n## Getting Started\n\n### Prerequisites\n\n(ideally very short, if any)\n\n- OS\n- Library version\n- ...\n\n### Installation\n\n(ideally very short)\n\n- npm install [package name]\n- mvn install\n- ...\n\n### Quickstart\n(Add steps to get up and running quickly)\n\n1. git clone [repository clone url]\n2. cd [repository name]\n3. ...\n\n\n## Demo\n\nA demo app is included to show how to use the project.\n\nTo run the demo, follow these steps:\n\n(Add steps to start up the demo)\n\n1.\n2.\n3.\n\n## Resources\n\n(Any additional resources or related projects)\n\n- Link to supporting information\n- Link to similar sample\\\n- ..."
    },
    {
      "name": "hifaz1012/multi-agents-healthcare-insurance",
      "stars": 2,
      "img": "https://avatars.githubusercontent.com/u/6141650?s=40&v=4",
      "owner": "hifaz1012",
      "repo_name": "multi-agents-healthcare-insurance",
      "description": "Multi Agent System for HealthCare Insurance Planning",
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2025-02-21T02:08:06Z",
      "updated_at": "2025-02-28T04:50:21Z",
      "topics": [],
      "readme": "# multi-agents-healthcare-insurance\nMulti Agent System for HealthCare Insurance Planning\n\nCredits: Fork from https://github.com/luckypamula/azure-ai-agents-labs/tree/main\n\n\n"
    },
    {
      "name": "wbingli/ai-agent-research",
      "stars": 2,
      "img": "https://avatars.githubusercontent.com/u/390168?s=40&v=4",
      "owner": "wbingli",
      "repo_name": "ai-agent-research",
      "description": "My own AI agent research repo",
      "homepage": null,
      "language": null,
      "created_at": "2025-02-11T17:43:50Z",
      "updated_at": "2025-03-19T16:02:24Z",
      "topics": [],
      "readme": "# My AI Agent Research\n\nThis is repo I'm looking into the AI agent technologies and current market trends, focus on AI Agent frameworks and AI Agent applications.\n\n## Generated Docs\n\n- [Open API Deep Research Report](reports/open-api-deep-research.md)\n- [Gemini Deep Research Report](reports/gemini-deep-research-report.md)\n- [Browse Use Deep Search Report](reports/browse-use-deep-search-report.md)\n\n## Websites & Videos & Channels\n\n - [Athina AI Hub](https://hub.athina.ai/): The Athina AI Hub is a dedicated resource for AI development teams, offering valuable insights, curated content, and actionable knowledge.\n - [WorldofAI@youtube](https://www.youtube.com/@intheworldofai):  This youtube channel has many AI agents and actually source code on the latest AI agents news.\n\n ## Articles & Links\n\n - [Building effective agents](https://www.anthropic.com/research/building-effective-agents)\n - [Github Awesome AI Agents](https://github.com/e2b-dev/awesome-ai-agents)\n - [Best AI Agent Papers in 2024](https://juteq.ca/biggest-ai-agent-paper-releases-2024/)\n\n## Papers\n\n - [ReAct: Synergizing Reasoning and Acting in Language Models](https://arxiv.org/abs/2210.03629)\n    - https://react-lm.github.io/\n - [AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation](https://arxiv.org/abs/2308.08155)\n - [MemGPT: Towards LLMs as Operating Systems](https://arxiv.org/abs/2310.08560)\n - [Magentic-One: A Generalist Multi-Agent System for Solving Complex Tasks](https://www.microsoft.com/en-us/research/uploads/prod/2024/11/MagenticOne.pdf)\n\n\n\n## AI Agent\n\n - [OpenAI Swarm](https://github.com/openai/swarm)\n - [BondAI](examples/bondai/README.md)\n - [Cline](https://cline.bot/)\n - [Model Context Protocol](https://modelcontextprotocol.io/)\n   - MCP Servers: https://github.com/modelcontextprotocol/servers\n   - [Awesome MCP Servers](https://github.com/punkpeye/awesome-mcp-servers)\n   - [Awesome MCP Clients](https://github.com/punkpeye/awesome-mcp-clients/)\n   - [MCP Servers Category](https://glama.ai/mcp/servers)\n   - [Smithery MCP registry](https://smithery.ai/)\n\n\n### BondAI Framework Evaluation\n\n- https://bondai.dev\n- https://github.com/krohling/bondai\n\nWhy I try this one first?  Well, I randomly picked this one from the awesome-ai-agents list, :)\n\n#### Overview\nThe BondAI framework demonstrates an interesting approach to AI agents by implementing a multi-agent architecture. Based on my evaluation of the web crawler example (`examples/bondai/`), here's a comprehensive analysis:\n\n#### Evaluation\n\n**Strengths:**\n - Simple and easy to use\n - Well documented\n\n**Limitations:**\n- I think it's dead, no more update for a year.\n- Less observability how framework and agents work together\n- No web UI or integration with other Web UI tools\n- Limited built-in agents and tools\n- Small community and limited third-party integrations\n- Not update-to-date with library dependencies\n\n#### My Conclusion\nIt's a simple framework to understand the basic concept of AI agents, but it's almost dead and not suitable for any production use Coding Agents\n\n### Browse Use Evaluation\n\n#### Overview\nBrowse Use provides a simple browse agent that can be used for anything related to browsing the web.\n\n#### Evaluation\n\n**Strengths:**\n- Simple and easy to use for browsing the web\n- Support to use local Chrome instance, good for accessing and authentication for private web pages\n- Good examples and documentation\n- Great web UI and deep research mode by leveraging the browsing capabilities\n\n**Limitations:**\n- Limited to browsing the web, not integrate with other tools or agents\n- Not integrated with other agents and multiple agent collaboration mode\n- Based on langchain agent, not seems to be used by other frameworks\n\n#### My Conclusion\nBrowse use is a good tool for anything related to browsing the web, but it's not well integrated with other agents and tools, and based on langchain agent, not seems to be used by other frameworks.\n\n\n### Coding Agents\n\n### Cline\nMost excited coding agent so far. Everyone should try this one.\nhttps://cline.bot/\n\n#### My Experience\n- Great experience with the live file editing, looks like a real pair programming with AI. Other coding agent will just modify the code in the background, which is not as interactive as Cline.\n- I choice the model claude+Sonnet 3.5, which works well with the code. I think this model is well trained for the coding task and with agent mode.\n- I use OpenRouter as API router, the Anthropic API has too low API rate limit, really bad experience with it.\n- Some medium level tasks (e.g. an API with unit test and refactoring) normally takes around less than 10 minutes to complete, with some of user input.\n- The cost is kind of high, for a medium task is around $2~5. I think it's worth it since I'm not spend all of my time on coding.\n\n\n"
    },
    {
      "name": "kuljotSB/semantic-kernel",
      "stars": 2,
      "img": "https://avatars.githubusercontent.com/u/147621230?s=40&v=4",
      "owner": "kuljotSB",
      "repo_name": "semantic-kernel",
      "description": null,
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2024-05-26T18:55:30Z",
      "updated_at": "2025-04-16T19:38:21Z",
      "topics": [],
      "readme": "# 🤖 Semantic Kernel SDK: Deep Dive\n\n![Python](https://img.shields.io/badge/python-3.10-blue)\n![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)\n![Azure](https://img.shields.io/badge/deployed%20on-Azure-blueviolet)\n![Udemy Course](https://img.shields.io/badge/Udemy-View_Course-orange?logo=Udemy)\n\n\n\n\n\n"
    },
    {
      "name": "ganachan/Project_Maria_Accelerator_tts",
      "stars": 2,
      "img": "https://avatars.githubusercontent.com/u/70560170?s=40&v=4",
      "owner": "ganachan",
      "repo_name": "Project_Maria_Accelerator_tts",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2025-02-11T20:47:04Z",
      "updated_at": "2025-03-20T18:36:28Z",
      "topics": [],
      "readme": "# **Maria: AI-Driven Custom Avatar GBB Agent**\n### Bringing AI to Life with Azure AI Text-to-Speech & Avatars  \n\nWelcome to the **Maria AI Avatar Repository**—your go-to guide for creating and deploying **custom AI-driven avatars with natural-sounding voices** using **Azure AI**. This repository walks you through the **end-to-end process** of setting up **your own AI-powered digital assistant**—from **building an avatar, training a neural voice, and inputting data**, to **generating lifelike video outputs**.\n\n---\n\n## 🚀 **What This Repository Offers**\n- **AI-Powered Avatar Creation** – Customize an avatar for your brand, powered by Azure AI.\n- **Neural Voice Integration** – Use pre-built or custom-trained voices for enhanced realism.\n- **Personalized Text-to-Speech** – Generate speech with **text-based inputs**, tailored to your business.\n- **Seamless Video Generation** – Automatically synthesize AI-driven videos with real-time lip-syncing.\n- **Azure AI Integration** – Leverage **Azure Speech Services**, **Blob Storage**, and **Custom Neural Voices (CNV)**.\n- **Multi-Agent AI Collaboration** – Integrate with other AI assistants for **Dynamically engaging customers** workflows.\n\n---\n\n**app1_general.py - General video synthesis**\n\n\n**app2_multi_agent.py - Integrated with Multi-agent archiecture to fetch information dynamically about the customers and creates/summarizes content with the help of manager(gatekeeper) agent**\n\n## 🔧 **Getting Started**\n\n### **1️⃣ Setup Your Environment**\nFirst, clone this repository:\n```bash\ngit clone https://github.com/ganachan/Project_Maria_Accelerator_tts.git\ncd Project_Maria_Accelerator_tts\n\nInstall the required dependencies:\npip install -r requirements.txt\n\n\n2️⃣ Configure Environment Variables\nBefore running the application, you must set up your .env file with the following environment variables:\n\nSPEECH_ENDPOINT=<YOUR_AZURE_SPEECH_SERVICE_ENDPOINT>\nSUBSCRIPTION_KEY=<YOUR_AZURE_SUBSCRIPTION_KEY>\nBLOB_CONNECTION_STRING=<YOUR_AZURE_BLOB_STORAGE_CONNECTION_STRING>\nBLOB_CONTAINER_NAME=<YOUR_BLOB_CONTAINER_NAME>\nAPI_VERSION=<YOUR_API_VERSION>\nBACKGROUND_IMAGE_URL=<URL_TO_BACKGROUND_IMAGE>\n\n3️⃣ How to Use the Repository\n\nStep 1: Create Your Custom Avatar & Neural Voice\nTo personalize Maria for your brand, you need to:\n\n✅ Create a custom avatar model in the Azure AI Avatar Studio.\n✅ Train your custom neural voice (CNV) using Azure AI Speech Studio.\n✅ Ensure your avatar model ID and voice ID are accessible via Azure.\n\nStep 2: Modify app.py to Input Your Data\nOnce your avatar and voice are created, update app.py with your data and configurations.\n\n🔹 Open app.py and customize:\n\nThe avatar selection (talkingAvatarCharacter).\nThe custom neural voice (CNV).\nThe input text for speech synthesis.\nThe background image.\n\nExample modification in app.py:\n\npayload = {\n    \"avatarConfig\": {\n        \"customized\": True,\n        \"talkingAvatarCharacter\": \"Your_Custom_Avatar_ID\",\n        \"talkingAvatarStyle\": \"Your_Avatar_Style\",\n    },\n    \"synthesisConfig\": {\n        \"voice\": \"Your_Custom_Neural_Voice_ID\",\n    },\n    \"inputKind\": \"plainText\",\n    \"inputs\": [{\"content\": input_text}]\n}\n\nStep 3: Run the Application\n\nLaunch the Streamlit UI to start generating AI-driven avatar videos.\n\nstreamlit run app.py\n\nStep 4: Store & Share AI-Generated Videos\n\nAll videos are stored in Azure Blob Storage. The repository includes functions to:\n\n✅ Generate secure SAS links for easy sharing.\n✅ Retrieve and manage AI-generated content via Azure Storage SDK.\n✅ Monitor usage and insights via the Azure portal.\n\n\n🎯 How This Can Be Reused\n\n💡 For Businesses:\n\nBuild AI-powered sales & marketing avatars to personalize customer engagement.\nDevelop AI-driven virtual assistants for healthcare, education, finance, or retail.\nAutomate internal training & onboarding videos with lifelike avatars.\n\n💡 For Developers:\n\nExtend the repository to support multi-modal AI workflows.\nConnect to Microsoft Copilot and Azure OpenAI APIs for dynamic AI interactions.\nDeploy AI avatars in Microsoft Teams, ServiceNow, or Dynamics 365.\n\n🚀 Next Steps & Customization\n\n🔹 Enhance personalization: Add customer-specific branding & styles.\n🔹 Multi-Agent Collaboration: Integrate Maria with Semantic Kernel Agents.\n🔹 Advanced Analytics: Track user engagement & AI-generated interactions.\n🔹 Cloud Deployment: Host the application on Azure App Services.\n\n\n"
    },
    {
      "name": "dbroeglin/az-ai-scaffolding-test-l300",
      "stars": 2,
      "img": "https://avatars.githubusercontent.com/u/243852?s=40&v=4",
      "owner": "dbroeglin",
      "repo_name": "az-ai-scaffolding-test-l300",
      "description": "Sample level 300 solution generated by 'yo az-ai'",
      "homepage": "https://github.com/dbroeglin/generator-aigbb",
      "language": "Bicep",
      "created_at": "2025-01-06T16:18:48Z",
      "updated_at": "2025-02-02T16:09:34Z",
      "topics": [],
      "readme": "# Solution generated with the Az AI Scaffolding tool test (l300)\n\nToC: [**USER STORY**](#user-story) \\| [**GETTING STARTED**](#getting-started)  \\| [**HOW IT WORKS**](#how-it-works)\n\n## User story\n\n### Az AI scaffolding tool test overview\n\n> [!TIP] \n> **Az AI Tip**: Document what your solution does here.\n\n## Getting Started\n\n### Codespaces and DevContainers\n\nThis respository has been configured to support GitHub Codespace and DevContainers.\n\n[![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://codespaces.new/dbroeglin/az-ai-scaffolding-test-l300) [![Open in Dev Containers](https://img.shields.io/static/v1?style=for-the-badge&label=Dev%20Containers&message=Open&color=blue&logo=visualstudiocode)](https://vscode.dev/redirect?url=vscode://ms-vscode-remote.remote-containers/cloneInVolume?url=https://github.com/dbroeglin/az-ai-scaffolding-test-l300)\n\n> [!WARNING]\n> Do NOT `git clone` the application under Windows and then open a DevContainer. \n> This would create issues with file end of lines. For DevContainer click on the button \n> above and let Visual Studio Code download the repository for you. Alternatively you \n> can also `git clone` under Windows Subsystem for Linux (WSL) and ask Visual Studio Code to\n> `Re-Open in Container`.\n\n### Local\n\nIt is possible to work with a fully local setup.\n\n  - [Azure CLI](https://learn.microsoft.com/en-us/cli/azure/what-is-azure-cli): `az`\n  - [Azure Developer CLI](https://learn.microsoft.com/en-us/azure/developer/azure-developer-cli/overview): `azd`\n  - [Python](https://www.python.org/about/gettingstarted/): `python`\n  - [UV](https://docs.astral.sh/uv/getting-started/installation/): `uv`\n  - Optionally [Docker](https://www.docker.com/get-started/): `docker` \n\n> [!TIP] \n> **Az AI Tip**: Document here how to quickly deploy the solution. Try to reduce this to `azd up` by\n> automating as much as possible. Have a look at `main.bicep` and `scripts` for examples of how to do\n> that\n\n### Quick deploy\n\nTo deploy Az AI scaffolding tool test just run: \n```bash\nazd up\n``` \n\n> [!WARNING] The application automatically configures authentication to secure frontend\n> To do so it creates an app registration in EntraID. \n> If the account you are using to deploy the app does not have the required permissions, disable the preprovision script in azure.yaml\n\n## How it works\n\n- TODO: How to run backend locally\n- TODO: How to run frontend locally\n\n### User Manual\n\n- TODO : Observability\n\n> [!TIP] \n> **Az AI Tip**: Document how the solution is used and operated here.\n> Optionally, if the section is too long, create a `USER_MANUAL.md` file and\n> link to it from here.\n\n### Architecture\n> [!TIP] \n> **Az AI Tip**: Document the solution's architecture here.\n> Optionally, if the section is too long, create a `ARCHITECTURE.md` file and\n> link to it from here.\n\n> [!TIP] \n> **Az AI Tip**: For architecture diagrams, you can leverage the [Markdown text\n> diagramming capabilities](https://docs.github.com/en/get-started/writing-on-github/working-with-advanced-formatting/creating-diagrams) available in GitHub. See example below.\n\n```mermaid\narchitecture-beta\n    group solution(cloud)[Solution]\n\n    service frontend(server)[Frontend] in solution\n    service backend(server)[Backend] in solution\n\n    frontend:R --> L:backend\n```\n\n## Code of Conduct\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\n\nResources:\n\n- [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/)\n- [Microsoft Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/)\n- Contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with questions or concerns\n\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Responsible AI Guidelines\n\nThis project follows below responsible AI guidelines and best practices, please review them before using this project:\n\n- [Microsoft Responsible AI Guidelines](https://www.microsoft.com/en-us/ai/responsible-ai)\n- [Responsible AI practices for Azure OpenAI models](https://learn.microsoft.com/en-us/legal/cognitive-services/openai/overview)\n- [Safety evaluations transparency notes](https://learn.microsoft.com/en-us/azure/ai-studio/concepts/safety-evaluations-transparency-note)\n"
    },
    {
      "name": "samelhousseini/mm_doc_proc",
      "stars": 2,
      "img": "https://avatars.githubusercontent.com/u/108335186?s=40&v=4",
      "owner": "samelhousseini",
      "repo_name": "mm_doc_proc",
      "description": "Multimodal Document Processing",
      "homepage": null,
      "language": "Python",
      "created_at": "2025-01-10T13:32:32Z",
      "updated_at": "2025-04-18T14:14:29Z",
      "topics": [],
      "readme": "# mm_doc_proc\nMultimodal Document Processing\n\n\n"
    },
    {
      "name": "Tyler-R-Kendrick/learn-sk",
      "stars": 2,
      "img": "https://avatars.githubusercontent.com/u/145080887?s=40&v=4",
      "owner": "Tyler-R-Kendrick",
      "repo_name": "learn-sk",
      "description": "Learn how to get started and advance your understanding of llms and agents with Semantic Kernel!",
      "homepage": null,
      "language": "C#",
      "created_at": "2024-09-11T18:05:21Z",
      "updated_at": "2024-10-15T00:55:46Z",
      "topics": [],
      "readme": "# learn-sk\nLearn how to get started and advance your understanding of llms and agents with Semantic Kernel!\n"
    },
    {
      "name": "scallighan/labby",
      "stars": 2,
      "img": "https://avatars.githubusercontent.com/u/91623699?s=40&v=4",
      "owner": "scallighan",
      "repo_name": "labby",
      "description": null,
      "homepage": null,
      "language": "HCL",
      "created_at": "2024-08-22T15:21:00Z",
      "updated_at": "2024-10-30T13:49:04Z",
      "topics": [],
      "readme": "# labby\n\n![labby-64.png](labby-64.png)\n\nA sample copilot using Semantic Kernel to interact with the Azure resource manager APIs. This repo also provides a react frontend or a Teams chat bot to interact with the backend API.\n\n## Architecture\n\n![custom-copilot-arch.png](custom-copilot-arch.png)\n\n## Running Locally\nCopy env.sample to .env and populate the values.\n\nThen run `./docker-stuff.sh`\n\n## Deploying\nSee terraform directory\n\n## App Registrations\nSee terraform directory [appreg.tf](terraform/appreg.tf)"
    },
    {
      "name": "Thoams0211/Stock-Insight",
      "stars": 2,
      "img": "https://avatars.githubusercontent.com/u/97227379?s=40&v=4",
      "owner": "Thoams0211",
      "repo_name": "Stock-Insight",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2024-08-20T12:09:59Z",
      "updated_at": "2025-03-31T08:24:20Z",
      "topics": [],
      "readme": "# Stock-Insight :chart_with_upwards_trend:\n\n## 0 Author :mailbox:\nYou can contact me by sending email to sy20021134@gmail.com\n\n## 1 Intro :clipboard:\nThis project is a stock market analysis tool that uses Agent to generate report and predict stock prices. The implementation details and detailed principles of this project can be found in the `docs` folder and the code files are located in other folders.\n\n\n\nThe original task derives from [Aliyun-AFAC Competition](https://tianchi.aliyun.com/competition/entrance/532200/information) .\n\n\n## 2 Structure :file_folder:\n- `PiVe` : The main code of the model used to correct missing triples in the semantic graph.\n- `MindMap` : The main code of the model used to implement KG_ARG.\n- `MetaGPT` : The main code of the model used to construct Agent which generates stock market analysising report.\n- `UI` : The main code of the UI of the project.\n\n## 3 Results :computer:\n\n- The implementation accurately identifies triples in textual information.\n- It can leverage Neo4J's knowledge graph for KG-RAG.\n- It can generate stock and industry reports in image + text format.\n- Users can control the behavior of the agent through the frontend.\n"
    },
    {
      "name": "Joining-AI/JoinQwen",
      "stars": 2,
      "img": "https://avatars.githubusercontent.com/u/154000096?s=40&v=4",
      "owner": "Joining-AI",
      "repo_name": "JoinQwen",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2024-07-20T06:50:28Z",
      "updated_at": "2024-08-03T08:18:57Z",
      "topics": [],
      "readme": "# JoinQwen\n\n[![Official Website](https://img.shields.io/badge/Official%20Website-sjtujoining.com-blue?style=for-the-badge&logo=world&logoColor=white)](https://sjtujoining.com)\n\n[![GitHub Repo stars](https://img.shields.io/github/stars/Joining-AI/JoinQwen?style=social)](https://github.com/Joining-AI/JoinQwen)\n\n## 快速开始\n\nJoinQwen 是一个简单的工具，用于测试和使用 Qwen API。通过本项目，您可以初始化 API，进行提问，并进行文本嵌入。\n\n## 安装要求\n\n请确保您已安装以下依赖项。您可以通过 `requirements.txt` 文件来安装这些依赖项。\n\n```bash\npip install -r requirements.txt\n```\n\n## 配置\n\n在项目根目录下创建一个 `.env` 文件，并添加您的 API 密钥：\n\n```\nQWEN_API=your_api_key_here\n```\n\n## 使用方法\n\n### 初始化服务\n\n首先，初始化 Qwen 服务：\n\n```python\nfrom Packages.LLM_BenchMarker.qwen_rater import *\n\ntester = QwenRater()\n```\n\n### 进行 RPM 测试\n\n您可以通过以下命令进行 RPM 测试：\n\n```python\nrpm = tester.rpm_test()\nprint(f\"Final RPM: {rpm}\")\n```\n\n### 初始化 API\n\n```python\nimport os\nfrom local_packages import *\n\n# 初始化 API\nagentopener = AgentOpener(service_type='qwen', version='long')\n\nllm = agentopener.service\nembedder = QwenEmbedder()\n```\n\n### 提问\n\n您可以向 LLM 提问：\n\n```python\nanswer = llm.ask('你好')\nprint(answer)\n```\n\n输出示例：\n\n```\n你好！有什么我能帮助你的吗？\n```\n\n### 文本嵌入\n\n您可以对文本进行嵌入：\n\n```python\nvec = embedder.embed_text(answer)\nprint(vec)\n```\n\n输出示例：\n\n```\n[0.11480577290058136, 3.1070446968078613, -2.426426887512207, 1.8947781324386597, -1.4686617851257324, 2.1084201335906982, 1.4298882484436035, -0.3828396201133728, -0....]\n```\n\n# 🔧 RepoAnnotator\n\n## 快速开始\n\n> **步骤 0** - 指定项目信息并导入类\n\n```python\nroot_folder = r\"D:\\Joining\\mem0-main\\mem0-main\"\nnew_root_folder = r'mem0'\nexclude_list=[r'D:\\Joining\\mem0-main\\mem0-main\\.github']\nfrom Applications.RepoAnnotator import RepoAnnotator\n```\n将 `root_folder` 替换为你的项目根目录路径，`new_root_folder` 替换为翻译后文件的目标文件夹路径，`exclude_list` 中填入你想要排除的目录或文件路径。\n<br />\n\n> **步骤 1** - 处理项目\n\n```python\nRepoAnnotator.run(root_folder, new_root_folder, exclude_list)\n```\n直接运行 `ipynb` 文件即可。\n\n<br />\n\n## 贡献\n\n如果您有任何建议或发现了问题，请提交 Issue 或 Pull Request。\n\n## 许可证\n\n本项目采用 Apache 2.0 许可证。详细信息请参阅 LICENSE 文件。\n"
    },
    {
      "name": "Bistu-OSSDT-2024/12-project-12",
      "stars": 2,
      "img": "https://avatars.githubusercontent.com/u/172448273?s=40&v=4",
      "owner": "Bistu-OSSDT-2024",
      "repo_name": "12-project-12",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2024-06-16T01:42:04Z",
      "updated_at": "2024-06-27T11:19:36Z",
      "topics": [],
      "readme": "# project-12\n\n# 项目名称：基于MetaGPT配置智谱AI开发的项目\n\n这是一个基于Java语言制作的休闲小游戏，可以实现大球吃小球的操作，让玩家在休闲时间体验小游戏的快乐。游戏全程以可视化的方式展示，并且设置有胜利动画。\n\n# 游戏运行界面截图\n\n见库内“游戏运行界面截图”文件夹。\n\n# 游戏规则\n\n大球吃小球游戏规定只能吃比自己小的球，如果吃到比自己大的球就会导致游戏失败，玩家必须吃到全部的球才能获得游戏的成功，玩家如果游戏失败将会面临重新游戏和放弃游戏两个选择，重开后难度会明显下降。\n\n# 先决条件\n\nPython 3.5 Pygame环境\n\n# 安装\n\n1.MetaGPT基于python语言开发,windows需要安装3.5及以上版本的python语言编译器，并且配置pip环境变量，安装好对应的import包。\n\n2.在vscode中进行克隆项目，输入该项目地址以下载使用。\n\n3.需要安装node，git支持项目运行。\n\n注意： 必须先确保电脑上有正确的python路径，并保证pip更新到最新.\n\n# 运行\n\nMetaGPT运行用vscode中的Python 3.5 以上语言编译器，小游戏用Java 1.8 JDK以上运行。\n\n# 开发人员\n\n孙镜涵（GitHub username：sjh6666）\n\n徐晟昌（GitHub username：yuhuangdadi11）\n\n成小康（GitHub username：cxk-00000）\n\n杨润宝（GitHub username：Tintieeeee）\n\n# 管理团队\n\n啊对对对 组  \n\n# 特别鸣谢\n\n1.导师：孙博辰\n\n啊对对对组全体成员\n\n# licence\n\n本项目基于 MIT许可证 发布\n\n\n\n\n\n\n"
    },
    {
      "name": "sqlserverworkshops/OpenAI-DataPro",
      "stars": 2,
      "img": "https://avatars.githubusercontent.com/u/48881186?s=40&v=4",
      "owner": "sqlserverworkshops",
      "repo_name": "OpenAI-DataPro",
      "description": "AI for the Data Professional, with a Focus on OpenAI",
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2024-02-09T15:47:33Z",
      "updated_at": "2024-05-03T18:38:19Z",
      "topics": [],
      "readme": "![](graphics/microsoftlogo.png)\n\n# Workshop: Unlocking AI Potential for the Data Professional with Azure OpenAI\n\n#### <i>A Microsoft Course from Microsoft Engineering and the FastTrack Team</i>\n\n<p style=\"border-bottom: 1px solid lightgrey;\"></p>\n\n<img style=\"float: left; margin: 0px 15px 15px 0px;\" src=\"https://raw.githubusercontent.com/microsoft/sqlworkshops/master/graphics/textbubble.png\"> <h2>About this Workshop</h2>\n\nWelcome to this Microsoft solutions workshop on *Unlocking AI Potential for the Data Professional with Azure OpenAI*. In this workshop, you'll learn how to unleash the full potential of artificial intelligence. Whether you’re a seasoned Data Professional or just dipping your toes into the world of machine learning, this course will empower you with the knowledge to create groundbreaking solutions.\n\n#### In this workshop, you'll walk away with an understand of how to:\n\n- Harness the Fusion: Discover how the synergy between artificial intelligence and cloud computing can revolutionize your business and data projects.\n- Gain cutting-Edge Insights: Dive into the latest breakthroughs in natural language processing, computer vision, conversational AI, and generative models.\n- Learn Azure OpenAI: Get an exclusive introduction to Azure Open AI, a comprehensive suite of tools and services designed to empower developers like you.\n- Get Hands-On Collaboration: Engage in collaborative exercises where you’ll team up with fellow participants. Together, you’ll design robust architectures and make informed decisions about AI project implementation.\n- Learn Scalability and Security: Learn how to build scalable, secure, and production-ready AI/Data solutions using Azure OpenAI.\n\nThis [github README.MD file](https://docs.github.com/en/repositories/managing-your-repositorys-settings-and-features/customizing-your-repository/about-readmes) explains how the workshop is laid out, what you will learn, and the technologies you will use in this solution. To download this Lab to your local computer, click the **Clone or Download** button you see at the top right side of this page. [More about that process is here](https://help.github.com/en/github/creating-cloning-and-archiving-repositories/cloning-a-repository).\n\nYou can view all of the [courses and other workshops our team has created at this link - open in a new tab to find out more.](https://microsoft.github.io/sqlworkshops/)\n\n<p style=\"border-bottom: 1px solid lightgrey;\"></p>\n\n<img style=\"float: left; margin: 0px 15px 15px 0px;\" src=\"https://raw.githubusercontent.com/microsoft/sqlworkshops/master/graphics/checkmark.png\"> <h3>Learning Objectives</h3>\n\n#### After this workshop you'll have:\n<br>\n\n- A Deep Understanding of AI and Data Projects: A solid grasp of the Azure Open AI ecosystem and its capabilities.\n- Decision-Making Power for Data/AI projects: The ability to make informed choices when it comes to AI project design and deployment.\n- Insider Tips on working with AI and Data: Valuable insights and practical tips to elevate your AI projects to new heights.\n\nJoin us on this transformative journey as we unlock the doors to AI innovation.\n\n#### The concepts and skills taught in this workshop form the starting points for:\n\n- Solution Architects and Developers, to understand how to put together an end to end Data and AI solution.\n- Data Professionals, to learn how to implement a data solution infused with AI.\n\n<p style=\"border-bottom: 1px solid lightgrey;\"></p>\n<img style=\"float: left; margin: 0px 15px 15px 0px;\" src=\"https://raw.githubusercontent.com/microsoft/sqlworkshops/master/graphics/building1.png\"> <h2>Business Applications of this Workshop</h2>\n\nBusinesses require the ability to use Artificial Intelligence within all parts of their solutions. However, one size does not fit all solutions. The key for correctly applying AI into solutions is to first understand the requirements and constraints for the solution, and then to fully understand the background, mechanisms, and tools you can use to solve for the solution. This course provides an explanation of how AI developed over time, and the multiple tools and platforms you can use to implement it. It focuses on the data solutions in and on the Microsoft Azure Data platform, and includes on-premises solutions such as Microsoft SQL Server.\n\n<img style=\"float: left; margin: 0px 15px 15px 0px;\" src=\"https://raw.githubusercontent.com/microsoft/sqlworkshops/master/graphics/listcheck.png\"> <h2>Technologies used in this Workshop</h2>\n\nThe solution includes the following technologies - although you are not limited to these, they form the basis of the workshop. At the end of the workshop you will learn how to extrapolate these components into other solutions. You will cover these at an overview level, with references to much deeper training provided.\n\n <table style=\"tr:nth-child(even) {background-color: #f2f2f2;}; text-align: left; display: table; border-collapse: collapse; border-spacing: 2px; border-color: gray;\">\n\n  <tr><th style=\"background-color: #1b20a1; color: white;\">Technology</th> <th style=\"background-color: #1b20a1; color: white;\">Description</th></tr>\n\n  <tr><td> <a href=\"https://learn.microsoft.com/en-us/sql/sql-server/what-is-sql-server?view=sql-server-ver16\">Microsoft SQL Server </a></td><td> Part of this course involves the use of SQL Server with Machine Learning Services. You'll also use SQL Server as a \"Data Source\" for using or training AI constructs.</td></tr>\n  <tr><td> <a href=\"https://learn.microsoft.com/en-us/fabric/get-started/microsoft-fabric-overview\">Microsoft Fabric Analytics</a></td><td> This course also uses Microsoft Fabric Analytics as a platform for AI. You'll want to be familiar with the basics of this platform to use it for those AI applications.</td></tr>\n  <tr><td> <a href=\"https://www.dataquest.io/blog/jupyter-notebook-tutorial/\">Jupyter Notebooks</a></td><td> The Activities in this Workshop primarily use Jupyter Notebooks to explain and run code.</td></tr>\n  \n\n</table>\n\n<p style=\"border-bottom: 1px solid lightgrey;\"></p>\n\n<img style=\"float: left; margin: 0px 15px 15px 0px;\" src=\"https://raw.githubusercontent.com/microsoft/sqlworkshops/master/graphics/bookpencil.png\"> <h2>Workshop Modules</h2>\n\nThis is a modular workshop, and in each section, you'll learn concepts, technologies and processes to help you complete the solution.\n\n<table style=\"tr:nth-child(even) {background-color: #f2f2f2;}; text-align: left; display: table; border-collapse: collapse; border-spacing: 5px; border-color: gray;\">\n\n  <tr><td style=\"background-color: AliceBlue; color: black;\">\n  <b>Module</b></td><td style=\"background-color: AliceBlue; color: black;\"><b>Topics</b></td></tr>\n\n  <tr><td>\n    <a href=\"https://github.com/sqlserverworkshops/OpenAI-DataPro/blob/main/sqldev/01%20-%20Introduction%20and%20Overview.md\" target=\"_blank\">01 - Introduction and Overview </a></td>\n    <td> Pre-requisites check, environment setup, overview of the workshop. An introduction to Artificial Intelligence, progressing from foundations through Generative AI. \n  </td></tr>\n  \n  <tr><td style=\"background-color: AliceBlue; color: black;\">\n    <a href=\"https://github.com/sqlserverworkshops/OpenAI-DataPro/blob/main/sqldev/02%20-%20Implementing%20AI%20Studio.md\" target=\"_blank\">02 - Implementing AI Studio </a> </td>\n    <td td style=\"background-color: AliceBlue; color: black;\"> Introduction, fundamentals, and a practical walkthrough of using Microsoft AI Studio. \n  </td></tr>\n  \n  <tr><td>\n    <a href=\"https://github.com/sqlserverworkshops/OpenAI-DataPro/blob/main/sqldev/03%20-%20Coding%20Fundamentals%20with%20Azure%20OpenAI.md\" target=\"_blank\">03 - Coding fundamentals with Azure OpenAI </a></td>\n    <td>  Using REST and API using the Azure OpenAI service. \n  </td></tr>\n\n  <tr><td>\n    <a href=\"https://github.com/sqlserverworkshops/OpenAI-DataPro/blob/main/sqldev/04%20-%20Data%20Integrations%20with%20the%20Azure%20OpenAI%20Service.md\" target=\"_blank\">04 - Data Integrations with the Azure OpenAI Service </a></td>\n    <td> Introduction, fundamentals and examples of using the Azure OpenAI service in Data/AI projects, including SQL Server on-premises, Azure SQL DB, and Microsoft Fabric Analytics. \n  </td></tr>\n  \n  <tr><td style=\"background-color: AliceBlue; color: black;\">\n    <a href=\"https://github.com/sqlserverworkshops/OpenAI-DataPro/blob/main/sqldev/05%20-%20DataAI%20Projects%20Best%20Practices.md\" target=\"_blank\">05 - Data/AI Projects Best Practices</a> </td>\n    <td td style=\"background-color: AliceBlue; color: black;\"> Security, Scale, and Safety concepts for using Azure OpenAI in Data/AI Projects. \n  </td></tr>  \n    \n</table>\n\n<p style=\"border-bottom: 1px solid lightgrey;\"></p>\n\n<p><img style=\"float: left; margin: 0px 15px 15px 0px;\" src=\"https://raw.githubusercontent.com/microsoft/sqlworkshops/master/graphics/geopin.png\"><b>Next Steps</b></p>\n\n\nNext, Continue to <a href=\"https://github.com/sqlserverworkshops/OpenAI-DataPro/blob/main/sqldev/00%20-%20Pre-Requisites.md\" target=\"_blank\"><i> Pre-Requisites</i></a>\n\n# Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n# Legal Notices\n\n### License\nMicrosoft and any contributors grant you a license to the Microsoft documentation and other content in this repository under the [Creative Commons Attribution 4.0 International Public License](https://creativecommons.org/licenses/by/4.0/legalcode), see [the LICENSE file](https://github.com/sqlserverworkshops/OpenAI-DataPro/blob/main/LICENSE), and grant you a license to any code in the repository under [the MIT License](https://opensource.org/licenses/MIT), see the [LICENSE-CODE file](https://github.com/microsoft/vscode/blob/main/LICENSE.txt).\n\nMicrosoft, Windows, Microsoft Azure and/or other Microsoft products and services referenced in the documentation\nmay be either trademarks or registered trademarks of Microsoft in the United States and/or other countries.\nThe licenses for this project do not grant you rights to use any Microsoft names, logos, or trademarks.\nMicrosoft's general trademark guidelines can be found at http://go.microsoft.com/fwlink/?LinkID=254653.\n\nPrivacy information can be found at https://privacy.microsoft.com/en-us/\n\nMicrosoft and any contributors reserve all other rights, whether under their respective copyrights, patents,\nor trademarks, whether by implication, estoppel or otherwise.\n"
    },
    {
      "name": "Eightina/arxiv-agent",
      "stars": 2,
      "img": "https://avatars.githubusercontent.com/u/81902383?s=40&v=4",
      "owner": "Eightina",
      "repo_name": "arxiv-agent",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2024-04-16T10:42:22Z",
      "updated_at": "2025-02-21T03:17:03Z",
      "topics": [],
      "readme": "# arixiv-agent/README.md\n---\n# 中文\n\n# 1. 使用说明\n\n- 运行前先确保python版本大于等于3.9，然后按照`requiments.txt`配置本地`python`环境，并修改`setup.sh`内的python路径\n- 为了实现pdf输出，需要手动安装`wkhtmltopdf`软件，并在`main.py`中的`path_wk`配置其路径\n- 为了进行企业微信的推送，需要在`main.py`中为`msger`配置正确的key\n- 运行`setup.sh`，该脚本检查`python`版本、配置必需的文件夹、为`agent/custom_tools/`下的自定义工具进行注册(如果有的话)\n- 在`~/.metagpt/config2.yaml`中为`MetaGPT`配置LLM服务的API，参考：[配置大模型API | MetaGPT (deepwisdom.ai)](https://docs.deepwisdom.ai/main/zh/guide/get_started/configuration/llm_api_configuration.html)\n- 使用正确的python环境运行`main.py`，即可运行`MetGPT`框架下的`team`， 包括一个`SimpleCrawler`和一个`Summarizer`。前者爬取当日`arxiv.com`某一搜索结果页面的一定量文章，后者对所有的文章进行分类-归类-总结操作\n    \n    ```bash\n    python ./main.py\n    ```\n- 当然也可以使用`crontab -e` 设定定时运行计划（使用绝对路径），比如设定每天9点30分，通过`task.sh`运行一次，shell输出到指定的`task.log`文件。为此需要对`task.sh`里的python路径和一些必要的PATH进行设置。\n    \n    ```bash\n    38 9 * * * /bin/bash /home/int.orion.que/dev/my_programs/arxiv-agent/task.sh > /home/int.orion.que/dev/task.log 2>&1\n    ```\n- 爬取和总结全部完成后，主进程得到`summary@{today}.md`和`paper@{today}.md`，并将其转换为pdf格式，推送给指定的机器人，要发送的消息可以在`main.py`中配置。\n- 每天爬取的记录保存在`output/paperdone.pkl`中，每天爬虫根据这个记录避免重复爬取，详情见后文\n- 每天的输出都在`output`下，可以通过在`crontab -e`设定`clear.sh`的定时运行计划清除这些历史数据\n    \n\n# 2. 输入配置和输出说明\n\n- 需要配置的主要参数：\n    - `main.py`中的路径\n    - `main.py`中的机器人key\n    - `main.py`中的自定义消息\n    - `agent/custom_actions/DataActoins.py`中的爬取目标URL（可以修改URL进行高级搜索的设定）和爬取规模`TaskSize`\n    - `agent/custom_actions/TextActoins.py`中的各个prompt\n- 每次运行可以得到的输出:\n    - `SimpleCrawler`的日志`logs/crawler_{today}.log`\n    - `MetaGPT`的日志`logs/{today}.txt`\n    - 爬虫爬取的所有文章条目`output/raw/crawler_{today}.json`\n    - 通过筛选，保证是新的文章`output/paper@{today}.md`和`output/summary@{today}.pdf`\n    - 分类&归类结果`output/summary@{today}.md`和`output/summary@{today}.pdf`\n    - 7日内爬取过的所有文章的网址的缓存`output/paperdone.pkl`，是一个字典，key为日期，value是连接的List（由主进程`main.py`自动维护，不要轻易删除，会导致重复爬取）。\n    - 如果是`crontab`定时任务，还会在指定的输出位置得到shell的输出`log`文件。\n\n# 3. 项目结构\n\n项目结构如下面描述。\n\n- `agent/`中有三个module，`custom_actions`，`custom_roles`，和`custom_tools`。这些分别是对`MetaGPT`框架下的Action，Role，和Tool元素的定义。具体请看：[智能体入门 | MetaGPT (deepwisdom.ai)](https://docs.deepwisdom.ai/main/zh/guide/tutorials/agent_101.html)。其中`custom_tools`暂时没有用处，如果在其中定义了新的tool，需要通过`setup.sh`为其创建到`MetaGPT`要求目录的链接（根据`MetaGPT`文档要求）。\n- `output/`保存所有的非日志输出，包括`json`，`md`，和`pkl`文件。新的总结和文章保存在根目录，其他将保存在对应的文件夹下。\n- `tools/`是一个module，包含项目的其他模块代码，如爬虫、数据处理、文件存取、日志、机器人通信等，是最核心的目录。\n- `logs/`是日志保存的文件夹。\n- `main.py` 是主程序\n- `test.py`是测试用的程序\n- `task.sh`是定时执行主程序所用的脚本\n- `clear.sh`是定时进行缓存清理所用的脚本\n- `setup.sh`是配置环境的脚本（功能见1）\n- `LICENSE`是项目的开源许可证\n- `README.md`即本说明文档\n- `requirements.txt`是python环境需求\n\n```bash\narxiv-agent\n|agent/\n    |——custom_actions/\n        |——__init__.py\n        |——DataActions.py\n        |——TextActions.py\n    |——custom_roles/\n        |——__init__.py\n        |——SimpleCrawler.py\n        |——Summarizer.py\n    |——custom_tools/\n        |——__init__.py\n        |——CustomStructedCrawler.py\n|output/\n    |——paperdone.pkl\n    |——outdated/\n    |——pdf/\n    |——raw/\n|tools/\n    |——__init__.py\n    |——AccessFile.py\n    |——Crawler.py\n    |——DataProcessor.py\n    |——Logger.py\n    |——Messenger.py\n    |——OutputMD.py\n|logs/\n|main.py\n|test.py\n|task.sh\n|clear.sh\n|setup.sh\n|README.md\n|LICENSE\n|requirements.txt\n```\n\n# 4. 有关临时数据\n\n## 4.1 `paperdone.pkl`\n\n- 7日内爬取过的所有文章的网址的缓存`output/paperdone.pkl，`是由主进程自动维护的一个字典的二进制存档，包含过去7天内爬到的每个日期（key）的文章链接（value）。\n- 这个字典在`tools/DataProcessor.py`中被使用和维护，用于确定这篇文章是否在过去一周内见过，如果没见过将会加入新条目并保存。\n- 这个字典作为过去文章的记录，由主程序`main.py`每次运行时定期清除历史缓存，会检查并删除超过7天范围的条目。\n- 因此不要轻易手动删除该文件，否则下一次运行可能会导致重复总结已经总结过的文章。\n\n## 4.2 其他数据\n\n- 其他数据包括上述的每天产生的各种输出。这些缓存将由`./clear.sh`通过同样的`crontab`自动任务进行定时清除。比如下述设置每周一零点进行一次缓存清空。\n    \n    ```bash\n    0 0 * * 1 /bin/bash /home/int.orion.que/dev/my_programs/arxiv-agent/clear.sh > /home/int.orion.que/dev/clear.log 2>&1\n    ```\n\n\n---\n# English Version\n\nHere is the translation of the provided document into English:\n\n# 1. Usage Instructions\n\n- Before running, ensure that your Python version is at least 3.9, then configure your local Python environment according to `requirements.txt`, and modify the Python path in `setup.sh`.\n- To enable PDF output, manually install the `wkhtmltopdf` software and configure its path in `path_wk` within `main.py`.\n- For enterprise WeChat notifications, configure the correct key for `msger` in `main.py`.\n- Run `setup.sh`. This script checks the Python version, sets up necessary folders, and registers custom tools under `agent/custom_tools/` (if any).\n- Configure the LLM service API for `MetaGPT` in `~/.metagpt/config2.yaml`. Reference: [Configure Large Model API | MetaGPT (deepwisdom.ai)](https://docs.deepwisdom.ai/main/zh/guide/get_started/configuration/llm_api_configuration.html)\n- Run `main.py` using the correct Python environment to launch the `team` under the `MetaGPT` framework, which includes a `SimpleCrawler` and a `Summarizer`. The former scrapes a certain number of articles from a search results page on `arxiv.com` for the day, while the latter categorizes, groups, and summarizes all the articles.\n  \n  ```bash\n  python ./main.py\n  ```\n\n- You can also use `crontab -e` to set up a scheduled task (using absolute paths), for example, to run `task.sh` once daily at 9:30 AM, with shell output directed to a specific `task.log` file. You need to configure the Python path and some necessary PATHs in `task.sh`.\n\n  ```bash\n  38 9 * * * /bin/bash /home/int.orion.que/dev/my_programs/arxiv-agent/task.sh > /home/int.orion.que/dev/task.log 2>&1\n  ```\n\n- After scraping and summarizing, the main process generates `summary@{today}.md` and `paper@{today}.md`, converts them to PDF format, and sends them to a designated bot. The message to be sent can be configured in `main.py`.\n- Daily scraping records are saved in `output/paperdone.pkl`, and each day's crawler uses this record to avoid duplicate scraping. Details are provided later.\n- All daily outputs are stored in `output/`, and you can clear these historical data through a scheduled `clear.sh` task set up in `crontab -e`.\n\n# 2. Input Configuration and Output Description\n\n- Main parameters to configure:\n    - Paths in `main.py`\n    - Bot key in `main.py`\n    - Custom messages in `main.py`\n    - Scraping target URL and scale (`TaskSize`) in `agent/custom_actions/DataActions.py`\n    - Prompts in `agent/custom_actions/TextActions.py`\n- Outputs obtained each run:\n    - `SimpleCrawler` log: `logs/crawler_{today}.log`\n    - `MetaGPT` log: `logs/{today}.txt`\n    - All article entries scraped by the crawler: `output/raw/crawler_{today}.json`\n    - New articles after filtering: `output/paper@{today}.md` and `output/summary@{today}.pdf`\n    - Categorization & grouping results: `output/summary@{today}.md` and `output/summary@{today}.pdf`\n    - Cache of all URLs of articles scraped within the last 7 days: `output/paperdone.pkl` (a dictionary where keys are dates and values are lists of links; maintained automatically by the main process `main.py`; do not delete it easily as it may cause duplicate scraping).\n    - If using a `crontab` scheduled task, you will also get the shell output `log` file at the specified location.\n\n# 3. Project Structure\n\nThe project structure is described as follows:\n\n- `agent/` contains three modules: `custom_actions`, `custom_roles`, and `custom_tools`. These define the Action, Role, and Tool elements under the `MetaGPT` framework. For more details, see: [Agent 101 | MetaGPT (deepwisdom.ai)](https://docs.deepwisdom.ai/main/zh/guide/tutorials/agent_101.html). The `custom_tools` module is currently unused. If new tools are defined here, you need to create links to the required directory via `setup.sh` (as per `MetaGPT` documentation requirements).\n- `output/` stores all non-log outputs, including `json`, `md`, and `pkl` files. New summaries and articles are saved in the root directory, while others are saved in corresponding subfolders.\n- `tools/` is a module containing other code modules such as crawlers, data processing, file access, logging, and bot communication – the core directory of the project.\n- `logs/` is the folder for log files.\n- `main.py` is the main program.\n- `test.py` is a test program.\n- `task.sh` is the script used for scheduling the main program.\n- `clear.sh` is the script used for scheduled cache cleaning.\n- `setup.sh` is the script for configuring the environment (see section 1 for details).\n- `LICENSE` is the open-source license for the project.\n- `README.md` is this documentation.\n- `requirements.txt` lists the Python environment dependencies.\n\n\n```bash\narxiv-agent\n|-agent/\n    |——custom_actions/\n        |——__init__.py\n        |——DataActions.py\n        |——TextActions.py\n    |——custom_roles/\n        |——__init__.py\n        |——SimpleCrawler.py\n        |——Summarizer.py\n    |——custom_tools/\n        |——__init__.py\n        |——CustomStructedCrawler.py\n|-output/\n    |——paperdone.pkl\n    |——outdated/\n    |——pdf/\n    |——raw/\n|-tools/\n    |——__init__.py\n    |——AccessFile.py\n    |——Crawler.py\n    |——DataProcessor.py\n    |——Logger.py\n    |——Messenger.py\n    |——OutputMD.py\n|-logs/\n|-main.py\n|-test.py\n|-task.sh\n|-clear.sh\n|-setup.sh\n|-README.md\n|-LICENSE\n|-requirements.txt\n```\n\nHere is the translation for the additional section:\n\n# 4. Temporary Data\n\n## 4.1 `paperdone.pkl`\n\n- The cache of all URLs of articles scraped within the last 7 days is stored in `output/paperdone.pkl`. This is a binary archive of a dictionary automatically maintained by the main process, containing the links (values) of articles for each date (keys) within the past 7 days.\n- This dictionary is used and maintained in `tools/DataProcessor.py` to determine whether an article has been seen within the past week. If not, it adds a new entry and saves it.\n- As a record of past articles, this dictionary is periodically cleared by the main program `main.py` when it runs, checking and removing entries that exceed the 7-day range.\n- Therefore, do not delete this file manually, as doing so might result in re-summarizing articles that have already been summarized during the next run.\n\n## 4.2 Other Data\n\n- Other data includes various outputs generated daily, as mentioned above. These caches are automatically cleared by `./clear.sh` through the same `crontab` scheduled task. For example, the following setting clears the cache every Monday at midnight.\n  \n  ```bash\n  0 0 * * 1 /bin/bash /home/int.orion.que/dev/my_programs/arxiv-agent/clear.sh > /home/int.orion.que/dev/clear.log 2>&1\n  ```"
    },
    {
      "name": "georgeIshaq/Q-Hack",
      "stars": 2,
      "img": "https://avatars.githubusercontent.com/u/123267653?s=40&v=4",
      "owner": "georgeIshaq",
      "repo_name": "Q-Hack",
      "description": null,
      "homepage": null,
      "language": "CSS",
      "created_at": "2024-04-03T13:05:00Z",
      "updated_at": "2024-08-06T03:15:00Z",
      "topics": [],
      "readme": "# Mathy Co - Tailor Math to Your Interests\n\nMathy Co offers a personalized approach to mathematics, tailored to match your unique interests and learning style. Whether you're passionate about art, engineering, finance, or any other field, Mathy Co provides engaging resources and practical applications to help you master math concepts. Explore our diverse range of content and discover how math can empower your pursuits. Start your math journey with Mathy Co today!\n\n<img width=\"1464\" alt=\"image\" src=\"https://github.com/georgeIshaq/Q-Hack/assets/106736711/f0fa7edf-e833-4f37-ba0c-fcdfe0a537ee\">\n\n\n"
    },
    {
      "name": "synedra/vector_linkedin",
      "stars": 2,
      "img": "https://avatars.githubusercontent.com/u/77410784?s=40&v=4",
      "owner": "synedra",
      "repo_name": "vector_linkedin",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2024-01-02T17:41:11Z",
      "updated_at": "2025-03-16T09:26:54Z",
      "topics": [],
      "readme": "# Shakespeare sample application for DataStax Python driver\n\n<table>\n<tr>\n<td> 1. Let the environment finish running the pip install command.\n</td>\n<td> See terminal below\n</td>\n</tr>\n\n<tr>\n<td valign=top> 2. Get an Astra account/database at https://astra.datastax.com\n</td>\n<td> <img src=\"img/CreateDatabase.jpg\" width=250>\n</td>\n</tr>\n\n<tr>\n<td valign=top> 3. Get your DB ID and Token\n</td>\n<td> <img src=\"img/ID_and_Token.jpg\" width=250>\n</td>\n</tr>\n\n<tr>\n<td valign=top> 4. Sign up at openai.com and get an API key\n</td>\n<td> http://openai.com\n</td>\n</tr>\n\n<tr>\n<td valign=top> 5. Populate an .env file in the Chapter3 directory with the items from the previous steps.\n</td>\n<td> <pre>\nOPENAI_API_KEY=<key>\nASTRA_DB_APPLICATION_TOKEN=<token>\nASTRA_DB_ID=<id>\n</pre>\n</td>\n</tr>\n\n<tr>\n<td valign=top> 6. Change directories to Chapter3, then run the population command with `python populate.py` to get the answer\n</td>\n<td> <pre>\ncd Chapter3\npython populate.py\n</pre>\n</td>\n</tr>\n<tr>\n<td valign=top> 7. Change directories to Chapter4, then run the split command with `python split.py` to separate the training and testing data.\n</td>\n<td> <pre>\ncd ../Chapter4\npython split.py\n</pre>\n</td>\n</tr>\n</table>\n"
    },
    {
      "name": "Cataldir/semantic-kernel-py-training",
      "stars": 2,
      "img": "https://avatars.githubusercontent.com/u/29005497?s=40&v=4",
      "owner": "Cataldir",
      "repo_name": "semantic-kernel-py-training",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2023-12-05T01:30:15Z",
      "updated_at": "2024-04-10T19:24:24Z",
      "topics": [],
      "readme": "# Semantic Kernel (Python) - Training and Understanding\n\nThe beauty of Semantic Kernel relies on the elegant way that it orchestrate the communication with the LLMs, by providing concise and well-defined separations between semantic functions (those that expect an interaction with the LLM) from native functions (those that doesn't). Also, in a different strategy from LangChain, Semantic Kernel specializes in the communication with the LLM through a kernel of actions, which is more straighforward and in accordance with microservices architectural patterns.\n\n## About this repository\n\nThis is a repository majorly oriented to understand the capabilities and limitations of Semantic Kernel as a Python package.\nThis repository is not meant to be used as a reference for extension, serving the only purpose of understanding the package semantics and overall strategies for using it.\n\n## Learning Objectives\n\n- Understand how to apply the concept of \"Agents\" with semantic kernel\n- Understand what a \"Plan\" is in the context of Semantic Kernel\n- Understand how to prepare Tools and plugins with Semantic Kernel\n\n## Agents\n\nAgents are entities that control the flow of action and interaction with the user. They are empowered by the LLM (which is the \"brain\") and a set of tools and\nprocedures that should be timelly called whenever you have the need in the logic flow of response.\n\nFrom the perspective of Semantic Kernel orchestration, an AI agent is a modular abstraction that can possess a persona, can perform actions in response to user input, and can easily communicate with other agents. You might also view an agent from an AI-as-a-service perspective or as an autonomous worker.\n\n[Check this explanation on agents for SK](https://github.com/Cataldir/semantic-kernel-py-training/tree/main/app/agents)\n[And this is the official way of doing it](https://learn.microsoft.com/en-us/semantic-kernel/agents/)\n\n## Context\n\n\n\n## Tooling and Plugins\n\n"
    },
    {
      "name": "Gavinbeauch13/Accommodate",
      "stars": 2,
      "img": "https://avatars.githubusercontent.com/u/100885952?s=40&v=4",
      "owner": "Gavinbeauch13",
      "repo_name": "Accommodate",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2024-01-27T00:13:47Z",
      "updated_at": "2024-01-29T03:44:17Z",
      "topics": [],
      "readme": ""
    },
    {
      "name": "percebus/openai-sandbox-py",
      "stars": 2,
      "img": "https://avatars.githubusercontent.com/u/5483088?s=40&v=4",
      "owner": "percebus",
      "repo_name": "openai-sandbox-py",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2023-06-05T15:35:18Z",
      "updated_at": "2025-03-30T15:20:04Z",
      "topics": [],
      "readme": "# My OpenAI SandBox\n\n[`TODO`s](./TODO.md) | [`LICENSE`](./LICENSE.md)\n\n[![[C]ontinuous [I]ntegration](https://github.com/percebus/openai-sandbox-py/actions/workflows/always.yml/badge.svg)](https://github.com/percebus/openai-sandbox-py/actions/workflows/always.yml)\n\n## Responsible AI\n\nUsage of Azure OpenAI should follow the six Microsoft [AI principles](https://learn.microsoft.com/en-us/azure/machine-learning/concept-responsible-ai):\n\n- Fairness: AI systems shouldn't make decisions that discriminate against or support bias of a group or individual.\n- Reliability and Safety: AI systems should respond safely to new situations and potential manipulation.\n- Privacy and Security: AI systems should be secure and respect data privacy.\n- Inclusiveness: AI systems should empower everyone and engage people.\n- Accountability: People must be accountable for how AI systems operate.\n- Transparency: AI systems should have explanations so users can understand how they're built and used.\n\n## Tokens\n\nOne token is roughly four characters for typical English text.\n\n## Models\n\nIn the [Azure OpenAI Studio](https://oai.azure.com/portal), you can build AI models and deploy them for public consumption in software applications. Azure OpenAI's capabilities are made possible by specific generative AI models. Different models are optimized for different tasks; some models excel at summarization and providing general unstructured responses, and others are built to generate code or unique images from text input.\n\nThese Azure OpenAI models fall into a few main families:\n\n- GPT-4\n- GPT-3\n- Codex\n- [Embeddings](https://learn.microsoft.com/en-us/azure/cognitive-services/openai/concepts/understand-embeddings)\n- DALL-E\n\nAzure OpenAI's AI models can all be trained and customized with [fine-tuning](https://learn.microsoft.com/en-us/azure/cognitive-services/openai/how-to/fine-tuning?pivots=programming-language-studio%3Fazure-portal%3Dtrue). We won't go into custom models here, but you can learn more on the fine-tuning your model Azure documentation.\n\n### GPT\n\nUnderstanding GPT models for natural language generation\nGenerative pre-trained transformer (GPT) models are excellent at both understanding and creating natural language. If you've seen recent news around AI answering questions or writing a paragraph based on a prompt, it likely could have been generated by a GPT model. GPT models often have the version appended to the end, such as GPT-3 or GPT-4. Azure OpenAI offers preview access to ChatGPT powered by gpt-35-turbo and to GPT-4. You can apply for access to GPT-4 here.\n\nWhat does a response from a GPT model look like?\nA key aspect of OpenAI's generative AI is that it takes an input, or prompt, to return a natural language, visual, or code response. GPT tries to infer, or guess, the context of the user's question based on the prompt.\n\n#### Prompts\n\nGPT models are great at completing several natural language tasks, some of which include:\n\n| Task                        | Prompt                                   |\n| --------------------------- | ---------------------------------------- |\n| Summarizing text            | \"Summarize this text into a short blurb\" |\n| Classifying text            | \"What genre of book is this?\"            |\n| Generating names or phrases | \"Write a tagline for my flower company\"  |\n| Translation                 | \"Translate 'How are you' to French\"      |\n| Answering questions         | \"What does Azure OpenAI do?\"             |\n| Suggesting content          | \"Give me the five best weddings songs\"   |\n\nPrompts can be grouped into types of requests based on task.\n\nLearn more about [prompt engineering](https://learn.microsoft.com/en-us/azure/cognitive-services/openai/concepts/prompt-engineering?portal=true).\n\n| Task type                                          | Prompt example                          | Completion example                                                                                                            |\n| -------------------------------------------------- | --------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------- |\n| Classifying content                                | Tweet: I enjoyed the trip. Sentiment:   | Positive                                                                                                                      |\n| Generating new content                             | List ways of traveling                  | 1. Bike 2. Car ...                                                                                                            |\n| Holding a conversation                             | A friendly AI assistant                 | [See examples](https://learn.microsoft.com/en-us/azure/cognitive-services/openai/how-to/completions#conversation?portal=true) |\n| Transformation (translation and symbol conversion) | English: Hello, French:                 | bonjour                                                                                                                       |\n| Summarizing content                                | Provide a summary of the content {text} | The content shares methods of machine learning.                                                                               |\n| Picking up where you left off                      | One way to grow tomatoes                | is to plant seeds.                                                                                                            |\n| Giving factual responses                           | How many moons does Earth have?         | One                                                                                                                           |\n\n#### Prompting Principles\n\n##### Write clear and specific instructions\n\n- Use delimeters\n\n  - `###`\n  - `\"\"\"`\n  - 3x `\n  - `---`\n  - `<` `>`\n  - `<tag>`XML`</tag>`\n\n- Ask for structured ouptut:\n\n  - HTML\n  - JSON\n\n- Check\n\n  - Wether conditions are satisfied.\n  - Assumptions required to do the ask.\n\n- Few-shot prompting.\n\n##### Give the model time to \"think\"\n\n- Specify the steps required to complete a task.\n- Instruct the model to work out its own solution before rushing to a conclusion.\n- Reduce hallucinations.\n  - First find relevant information.\n  - Answer the question based on relevant information.\n\n## Repositories\n\n- [`openai/openai-cookbook`](https://github.com/openai/openai-cookbook)\n- [`MicrosoftLearning/mslearn-openai`](https://github.com/MicrosoftLearning/mslearn-openai)\n- [`Azure-samples/Azure-OpenAI-Docs-Samples`](https://github.com/Azure-samples/Azure-OpenAI-Docs-Samples)\n\n## Tutorials\n\n- [Storing and querying for embeddings with Redis](https://blog.baeke.info/2023/03/21/storing-and-querying-for-embeddings-with-redis/)\n\n### MS Learn\n\n- [Introduction to Azure OpenAI Service](https://learn.microsoft.com/en-us/training/modules/explore-azure-openai/)\n- [Get started with Azure OpenAI Service](https://microsoftlearning.github.io/mslearn-openai/Instructions/Labs/01-get-started-azure-openai.html)\n- [Integrate Azure OpenAI into your app](https://microsoftlearning.github.io/mslearn-openai/Instructions/Labs/02-natural-language-azure-openai.html)\n- [Utilize prompt engineering in your app](https://microsoftlearning.github.io/mslearn-openai/Instructions/Labs/03-prompt-engineering.html)\n- [Explore Azure OpenAI Service embeddings and document search](https://learn.microsoft.com/en-us/azure/cognitive-services/openai/tutorials/embeddings?tabs=command-line)\n\n### DeepLearning.AI\n\n- [ChatGTP Prompt Engineering for Developers](https://learn.deeplearning.ai/chatgpt-prompt-eng/lesson/1/introduction)\n- [Building Systems with the ChatGPT API](https://learn.deeplearning.ai/chatgpt-building-system/)\n\n## Resources\n\n### AI\n\n- [Deep learning vs. machine learning in Azure Machine Learning](https://learn.microsoft.com/en-us/azure/machine-learning/concept-deep-learning-vs-machine-learning?view=azureml-api-2)\n\n### OpenAI\n\n- [OpenAI Examples](https://platform.openai.com/examples)\n- [OpenAI Python Library](https://pypi.org/project/openai/)\n- [Text Embeddings](https://platform.openai.com/docs/guides/embeddings/what-are-embeddings)\n- [Understanding embeddings in Azure OpenAI Service](https://learn.microsoft.com/en-us/azure/cognitive-services/openai/concepts/understand-embeddings)\n- [New and improved embedding model](https://openai.com/blog/new-and-improved-embedding-model)\n- [What are tokens and how to count them?](https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them)\n\n### [L]arge [L]anguage [M]odels\n\n- [ReAct: Synergizing Reasoning and Acting in Language Models](https://react-lm.github.io/)\n\n### LangChain\n\n- [Prompt Templates](https://python.langchain.com/en/latest/modules/prompts/prompt_templates.html)\n- [Output Parsers](https://python.langchain.com/en/latest/modules/prompts/output_parsers.html)\n- [Chains](https://python.langchain.com/en/latest/modules/chains.html)\n- [Tools](https://python.langchain.com/en/latest/modules/agents/tools.html)\n- [Agents](https://python.langchain.com/en/latest/modules/agents.html)\n"
    },
    {
      "name": "teerasej/nextflow-semantic-kernel-python-pycon-2023",
      "stars": 2,
      "img": "https://avatars.githubusercontent.com/u/85179?s=40&v=4",
      "owner": "teerasej",
      "repo_name": "nextflow-semantic-kernel-python-pycon-2023",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2023-12-16T03:36:35Z",
      "updated_at": "2024-03-15T18:41:32Z",
      "topics": [],
      "readme": "# Semantic Kernel Python Starter\n\n##  Easy-to-Manage AI with Python and Semantic Kernel\n\nThis project showcases the power of Semantic Kernel for building manageable and improvable AI applications in Python. Semantic Kernel provides a framework that simplifies the development and maintenance of AI interactions.\n\nHere's what you'll find in this repository:\n\n* **Code examples:** We demonstrate how to leverage Semantic Kernel's core functionalities like prompt templates and functions. These examples showcase how to interact with Large Language Models (LLMs) for tasks like text summarization or question answering. \n* **Modular design:** The code is designed with modularity in mind. Semantic Kernel allows you to define reusable prompt templates that encapsulate the core functionalities of your AI application. This makes it easier to modify and improve specific aspects without affecting the entire codebase.\n* **Clear structure:** The code is well-structured and easy to follow, making it understandable even for those new to Semantic Kernel.\n\nBy exploring this project, you'll gain insights into how Semantic Kernel can streamline the development of manageable and adaptable AI applications using Python. \n\n## Prerequisites\n\n- [Python](https://www.python.org/downloads/) 3.8 and above\n  - [Poetry](https://python-poetry.org/) is used for packaging and dependency management\n  - [Semantic Kernel Tools](https://marketplace.visualstudio.com/items?itemName=ms-semantic-kernel.semantic-kernel)\n\n## Configuring the starter\n\nThe starter can be configured with a `.env` file in the project which holds api keys and other secrets and configurations.\n\nMake sure you have an\n[Open AI API Key](https://openai.com/api/) or\n[Azure Open AI service key](https://learn.microsoft.com/azure/cognitive-services/openai/quickstart?pivots=rest-api)\n\nCopy the `.env.example` file to a new file named `.env`. Then, copy those keys into the `.env` file:\n\n```\nOPENAI_API_KEY=\"\"\nOPENAI_ORG_ID=\"\"\nAZURE_OPENAI_DEPLOYMENT_NAME=\"\"\nAZURE_OPENAI_ENDPOINT=\"\"\nAZURE_OPENAI_API_KEY=\"\"\n```\n\n## Running the starter\n\nTo run the console application within Visual Studio Code, just hit `F5`.\nAs configured in `launch.json` and `tasks.json`, Visual Studio Code will run `poetry install` followed by `python hello_world/main.py`\n\nTo build and run the console application from the terminal use the following commands:\n\n```powershell\npoetry install\npoetry run python hello_world/main.py\n```\n"
    },
    {
      "name": "MTCMarkFranco/python-sql-Interpreter",
      "stars": 2,
      "img": "https://avatars.githubusercontent.com/u/36431095?s=40&v=4",
      "owner": "MTCMarkFranco",
      "repo_name": "python-sql-Interpreter",
      "description": "This is sample code using Semantic Kernel to show how to chain functions based on a natural language driven Sequential Planner in SK",
      "homepage": "",
      "language": "Python",
      "created_at": "2023-11-06T14:53:33Z",
      "updated_at": "2023-11-20T01:18:48Z",
      "topics": [
        "database",
        "prompt",
        "sequentialplanner",
        "sk"
      ],
      "readme": "# Semantic Kernel - PYTHON-SQL-INTERPRETER\n\nThe `PYTHON-SQL-INTERPRETER` console application demonstrates how to execute a semantic function.\n\n## Prerequisites\n\n- [Python](https://www.python.org/downloads/) 3.8 and above\n- [Semantic Kernel Tools](https://marketplace.visualstudio.com/items?itemName=ms-semantic-kernel.semantic-kernel)\n- [Semantic Kernel SDK (0.3.14.dev0) - VERSION TESTED](https://pypi.org/project/semantic-kernel/0.3.14.dev0/)\n\n## Configuring the solution\n\nThe solution can be configured with a `.env` file in the project which holds api keys and other secrets and configurations.\n\nMake sure you have an\n[Azure Open AI service key](https://learn.microsoft.com/azure/cognitive-services/openai/quickstart?pivots=rest-api)\n\nCopy the `.env.example` file to a new file named `.env`. Then, copy those keys into the `.env` file:\n\n```\n# OPEN AI Settings\nOPENAI_API_KEY=\"\"\nOPENAI_ORG_ID=\"\"\nAZURE_OPENAI_DEPLOYMENT_NAME=\"\"\nAZURE_OPENAI_ENDPOINT=\"\"\nAZURE_OPENAI_API_KEY=\"\"\n\n# SQL DB Settings\nSERVER_NAME=\nDATABASE_NAME=\"\"\nSQLADMIN_USER=\"\"\nSQL_PASSWORD=\"\"\n\n```\n\n## solution design and sample execution\n\nMain Code flow:\n![Main Code](images/main-code-block.png)\n\nSample Database Schema:\n![DB Schema](images/db-schema-sample.png)\n\nSample Output:\n![Code Run](images/code-run-sample.png)\n\n## Running the solution\n\nTo run the console application within Visual Studio Code, just hit `F5`.\nAs configured in `launch.json` and `tasks.json`, Visual Studio Code will run `python main/main.py`\n\nTo build and run the console application from the terminal use the following commands:\n\n```powershell\npython main/main.py\n```\n"
    },
    {
      "name": "madebygps/cosmos-vector-aoai",
      "stars": 2,
      "img": "https://avatars.githubusercontent.com/u/6733686?s=40&v=4",
      "owner": "madebygps",
      "repo_name": "cosmos-vector-aoai",
      "description": "A vector store and search implementation with Cosmos DB NoSQL, Python, Cognitive Search, and Azure OpenAI",
      "homepage": null,
      "language": "Python",
      "created_at": "2023-09-24T23:34:44Z",
      "updated_at": "2023-11-21T14:30:07Z",
      "topics": [],
      "readme": "# cosmos-vector-aoai\nA vector store and search implementation with Cosmos DB NoSQL, Python, Cognitive Search, and Azure OpenAI.\n\n## Note\n\nThe completions from AOAI are grounded by a vector search and index loaded with manually prepared data.\n\n## Azure Cognitive Search Vector Search Notes\n- Vector search is a method of information retrieval where documents and queries are represented as vectors instead of plain text. \n- Vectors are generated by ML models, like the embeddings model available in Azure OpenAI,\n- Vector search is used to find similar items based on their embeddings.\n- Cognitive Search uses HNSW to perform vector search.\n\n## HNSW Notes\n\n- HNSW stands for Hierarchical Navigable Small World.\n- ANN approximate nearest neighbor search is a class of algorithms designed for finding matches in a vector space.\n- Hierarchical Navigable Small Worlds HSWN is one of the best performing algorithms for ANN search\n- https://learn.microsoft.com/azure/search/vector-search-overview#approximate-nearest-neighbors\n\n\nCosine similarity is a measure of the angle between two vectors in a multidimensional space 1.\nIt does not depend on the distance or magnitude of the vectors, only on their direction. Therefore, if two vectors are near each other, it does not necessarily mean they are more similar than vectors that are further apart. What matters is how aligned they are. For example, two vectors that are orthogonal (at 90 degrees) have zero cosine similarity, regardless of how close or far they are. On the other hand, two vectors that are parallel (at 0 or 180 degrees) have maximum or minimum cosine similarity, respectively, regardless of how close or far they\n\n### HNSW Parameters\n- m: The number of bi-directional links created for each node during construction. This is the maximum number of connections that can be traversed during a search.\n- efConstruction: The size of the dynamic list of nodes that are considered during construction. This is the maximum number of nodes that can be visited during construction.\n- efSearch: The size of the dynamic list of nodes that are considered during a search. This is the maximum number of nodes that can be visited during a search.\n- metric: The distance metric used to calculate the distance between two vectors. The default is L2, which is the Euclidean distance. Other options are IP, which is the inner product, and COSINE, which is the cosine similarity.\n\n\n## Azure Cognitive Search Notes\n- A search index is like a database of searchable content.\n- https://azuresdkdocs.blob.core.windows.net/$web/python/azure-search-documents/latest/index.html#creating-an-index\n- https://github.com/Azure/azure-sdk-for-python/tree/main/sdk/search/azure-search-documents/samples\n\n### Semantic Search notes\n\n- Semantic search will allow us to conduct semantic or hybrid search (with vector search.\n- A semantic configuration defines the fields that will be used for semantic search.\n\n### Indexer and datasource notes\n\n- A datasource is where the search index can get its data from.\n- An indexer will take the data from the datasource and put it into the search index.\n\n## Semantic kernel reading\n\n- https://devblogs.microsoft.com/semantic-kernel/announcing-semantic-kernel-integration-with-azure-cognitive-search/\n- https://github.com/dluc/Azure-Cognitive-Search-20230720"
    },
    {
      "name": "areibman/MetaGPT",
      "stars": 2,
      "img": "https://avatars.githubusercontent.com/u/14807319?s=40&v=4",
      "owner": "areibman",
      "repo_name": "MetaGPT",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2023-10-12T17:21:21Z",
      "updated_at": "2025-03-17T07:30:04Z",
      "topics": [],
      "readme": "\n# MetaGPT: The Multi-Agent Framework\n\n<p align=\"center\">\n<a href=\"\"><img src=\"docs/resources/MetaGPT-new-log.png\" alt=\"MetaGPT logo: Enable GPT to work in software company, collaborating to tackle more complex tasks.\" width=\"150px\"></a>\n</p>\n\n<p align=\"center\">\n<b>Assign different roles to GPTs to form a collaborative entity for complex tasks.</b>\n</p>\n\n<p align=\"center\">\n<a href=\"docs/README_CN.md\"><img src=\"https://img.shields.io/badge/文档-中文版-blue.svg\" alt=\"CN doc\"></a>\n<a href=\"README.md\"><img src=\"https://img.shields.io/badge/document-English-blue.svg\" alt=\"EN doc\"></a>\n<a href=\"docs/README_JA.md\"><img src=\"https://img.shields.io/badge/ドキュメント-日本語-blue.svg\" alt=\"JA doc\"></a>\n<a href=\"https://opensource.org/licenses/MIT\"><img src=\"https://img.shields.io/badge/License-MIT-blue.svg\" alt=\"License: MIT\"></a>\n<a href=\"docs/ROADMAP.md\"><img src=\"https://img.shields.io/badge/ROADMAP-路线图-blue\" alt=\"roadmap\"></a>\n<a href=\"https://discord.gg/DYn29wFk9z\"><img src=\"https://dcbadge.vercel.app/api/server/DYn29wFk9z?style=flat\" alt=\"Discord Follow\"></a>\n<a href=\"https://twitter.com/MetaGPT_\"><img src=\"https://img.shields.io/twitter/follow/MetaGPT?style=social\" alt=\"Twitter Follow\"></a>\n</p>\n\n<p align=\"center\">\n   <a href=\"https://vscode.dev/redirect?url=vscode://ms-vscode-remote.remote-containers/cloneInVolume?url=https://github.com/geekan/MetaGPT\"><img src=\"https://img.shields.io/static/v1?label=Dev%20Containers&message=Open&color=blue&logo=visualstudiocode\" alt=\"Open in Dev Containers\"></a>\n   <a href=\"https://codespaces.new/geekan/MetaGPT\"><img src=\"https://img.shields.io/badge/Github_Codespace-Open-blue?logo=github\" alt=\"Open in GitHub Codespaces\"></a>\n   <a href=\"https://huggingface.co/spaces/deepwisdom/MetaGPT\" target=\"_blank\"><img alt=\"Hugging Face\" src=\"https://img.shields.io/badge/%F0%9F%A4%97%20-Hugging%20Face-blue?color=blue&logoColor=white\" /></a>\n</p>\n\n## News\n🚀 Feb. 08, 2024: [v0.7.0](https://github.com/geekan/MetaGPT/releases/tag/v0.7.0) released, supporting assigning different LLMs to different Roles. We also introduced [Interpreter](https://github.com/geekan/MetaGPT/blob/main/examples/mi/README.md), a powerful agent capable of solving a wide range of real-world problems.\n\n🚀 Jan. 16, 2024: Our paper [MetaGPT: Meta Programming for A Multi-Agent Collaborative Framework\n](https://arxiv.org/abs/2308.00352) accepted for oral presentation **(top 1.2%)** at ICLR 2024, **ranking #1** in the LLM-based Agent category.\n\n🚀 Jan. 03, 2024: [v0.6.0](https://github.com/geekan/MetaGPT/releases/tag/v0.6.0) released, new features include serialization, upgraded OpenAI package and supported multiple LLM, provided [minimal example for debate](https://github.com/geekan/MetaGPT/blob/main/examples/debate_simple.py) etc.\n\n🚀 Dec. 15, 2023: [v0.5.0](https://github.com/geekan/MetaGPT/releases/tag/v0.5.0) released, introducing some experimental features such as **incremental development**, **multilingual**, **multiple programming languages**, etc.\n\n🔥 Nov. 08, 2023: MetaGPT is selected into [Open100: Top 100 Open Source achievements](https://www.benchcouncil.org/evaluation/opencs/annual.html).\n\n🔥 Sep. 01, 2023: MetaGPT tops GitHub Trending Monthly for the **17th time** in August 2023.\n\n🌟 Jun. 30, 2023: MetaGPT is now open source.\n\n🌟 Apr. 24, 2023: First line of MetaGPT code committed.\n\n## Software Company as Multi-Agent System\n\n1. MetaGPT takes a **one line requirement** as input and outputs **user stories / competitive analysis / requirements / data structures / APIs / documents, etc.**\n2. Internally, MetaGPT includes **product managers / architects / project managers / engineers.** It provides the entire process of a **software company along with carefully orchestrated SOPs.**\n   1. `Code = SOP(Team)` is the core philosophy. We materialize SOP and apply it to teams composed of LLMs.\n\n![A software company consists of LLM-based roles](docs/resources/software_company_cd.jpeg)\n\n<p align=\"center\">Software Company Multi-Agent Schematic (Gradually Implementing)</p>\n\n## Install\n\n### Pip installation\n\n> Ensure that Python 3.9+ is installed on your system. You can check this by using: `python --version`.  \n> You can use conda like this: `conda create -n metagpt python=3.9 && conda activate metagpt`\n\n```bash\npip install metagpt\n# https://docs.deepwisdom.ai/main/en/guide/get_started/configuration.html\nmetagpt --init-config  # it will create ~/.metagpt/config2.yaml, just modify it to your needs\n```\n\n### Configuration\n\nYou can configure `~/.metagpt/config2.yaml` according to the [example](https://github.com/geekan/MetaGPT/blob/main/config/config2.example.yaml) and [doc](https://docs.deepwisdom.ai/main/en/guide/get_started/configuration.html):\n\n```yaml\nllm:\n  api_type: \"openai\"  # or azure / ollama / open_llm etc. Check LLMType for more options\n  model: \"gpt-4-turbo-preview\"  # or gpt-3.5-turbo-1106 / gpt-4-1106-preview\n  base_url: \"https://api.openai.com/v1\"  # or forward url / other llm url\n  api_key: \"YOUR_API_KEY\"\n```\n\n### Usage\n\nAfter installation, you can use it as CLI\n\n```bash\nmetagpt \"Create a 2048 game\"  # this will create a repo in ./workspace\n```\n\nor you can use it as library\n\n```python\nfrom metagpt.software_company import generate_repo, ProjectRepo\nrepo: ProjectRepo = generate_repo(\"Create a 2048 game\")  # or ProjectRepo(\"<path>\")\nprint(repo)  # it will print the repo structure with files\n```\n\ndetail installation please refer to [cli_install](https://docs.deepwisdom.ai/main/en/guide/get_started/installation.html#install-stable-version)\n or [docker_install](https://docs.deepwisdom.ai/main/en/guide/get_started/installation.html#install-with-docker)\n\n### QuickStart & Demo Video\n- Try it on [MetaGPT Huggingface Space](https://huggingface.co/spaces/deepwisdom/MetaGPT)\n- [Matthew Berman: How To Install MetaGPT - Build A Startup With One Prompt!!](https://youtu.be/uT75J_KG_aY)\n- [Official Demo Video](https://github.com/geekan/MetaGPT/assets/2707039/5e8c1062-8c35-440f-bb20-2b0320f8d27d)\n\nhttps://github.com/geekan/MetaGPT/assets/34952977/34345016-5d13-489d-b9f9-b82ace413419\n\n## Tutorial\n\n- 🗒 [Online Document](https://docs.deepwisdom.ai/main/en/)\n- 💻 [Usage](https://docs.deepwisdom.ai/main/en/guide/get_started/quickstart.html)  \n- 🔎 [What can MetaGPT do?](https://docs.deepwisdom.ai/main/en/guide/get_started/introduction.html)\n- 🛠 How to build your own agents? \n  - [MetaGPT Usage & Development Guide | Agent 101](https://docs.deepwisdom.ai/main/en/guide/tutorials/agent_101.html)\n  - [MetaGPT Usage & Development Guide | MultiAgent 101](https://docs.deepwisdom.ai/main/en/guide/tutorials/multi_agent_101.html)\n- 🧑‍💻 Contribution\n  - [Develop Roadmap](docs/ROADMAP.md)\n- 🔖 Use Cases\n  - [Debate](https://docs.deepwisdom.ai/main/en/guide/use_cases/multi_agent/debate.html)\n  - [Researcher](https://docs.deepwisdom.ai/main/en/guide/use_cases/agent/researcher.html)\n  - [Recepit Assistant](https://docs.deepwisdom.ai/main/en/guide/use_cases/agent/receipt_assistant.html)\n- ❓ [FAQs](https://docs.deepwisdom.ai/main/en/guide/faq.html)\n\n## Support\n\n### Discard Join US\n📢 Join Our [Discord Channel](https://discord.gg/ZRHeExS6xv)!\n\nLooking forward to seeing you there! 🎉\n\n### Contact Information\n\nIf you have any questions or feedback about this project, please feel free to contact us. We highly appreciate your suggestions!\n\n- **Email:** alexanderwu@deepwisdom.ai\n- **GitHub Issues:** For more technical inquiries, you can also create a new issue in our [GitHub repository](https://github.com/geekan/metagpt/issues).\n\nWe will respond to all questions within 2-3 business days.\n\n## Citation\n\nFor now, cite the [arXiv paper](https://arxiv.org/abs/2308.00352):\n\n```bibtex\n@misc{hong2023metagpt,\n      title={MetaGPT: Meta Programming for A Multi-Agent Collaborative Framework}, \n      author={Sirui Hong and Mingchen Zhuge and Jonathan Chen and Xiawu Zheng and Yuheng Cheng and Ceyao Zhang and Jinlin Wang and Zili Wang and Steven Ka Shing Yau and Zijuan Lin and Liyang Zhou and Chenyu Ran and Lingfeng Xiao and Chenglin Wu and Jürgen Schmidhuber},\n      year={2023},\n      eprint={2308.00352},\n      archivePrefix={arXiv},\n      primaryClass={cs.AI}\n}\n```\n"
    },
    {
      "name": "MohammedFadin/semantic-kernel-easier-start",
      "stars": 2,
      "img": "https://avatars.githubusercontent.com/u/2106384?s=40&v=4",
      "owner": "MohammedFadin",
      "repo_name": "semantic-kernel-easier-start",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2023-10-09T09:51:27Z",
      "updated_at": "2023-10-26T08:03:34Z",
      "topics": [],
      "readme": "**_Semantic Kernel Easier Start_**\n\nThis is a project that shows how to use the Semantic Kernel library to build a chatbot that can interact with live external services. You will find an -example for each section in the folder path where it explains in detail how to use the library with your own usecase.\n\n_Getting Started_\nTo get started with this project, you'll need to install the following dependencies from requirements.txt\nYou can install the Python dependencies by running the following command:\n`pip install -r requirements.txt`\n\n![demo example gif](https://github.com/MohammedFadin/semantic-kernel-easier-start/blob/main/example.gif?raw=true)\n\n**_Usage_**\n\nTo run the demo, simply run the following command: `python3 semantic-kernel-demo.py`. This will start the chatbot and prompt you for input. You can enter any text and the chatbot will respond with a message.\n\n**_Plugins_**\n\nThis demo project includes two plugins:\n\n1. **OrchestratorPlugin**: This plugin is responsible for routing user input to the appropriate function then to Azure OpenAI.\n\n2. **StocksReaderPlugin**: This plugin is responsible for fetching stock data from the Dubai Financial Market.\n\n_You can add your own plugins by creating a new Python file in the plugins directory and defining a class that implements the necessary functions (Check the YourPlugin class in plugins/yourplugin.py for a faster start)_\n\n**_Contributing_**\n\nIf you'd like to contribute to this project, please fork the repository and create a new branch for your changes. Once you've made your changes, submit a pull request and we'll review your changes.\n\n**_License_**\n\nThis project is licensed under the MIT License - see the LICENSE file for details.\n\n**_Desclaimer_**\n\nThis project is for demonstration purposes only and quick start guide\n"
    },
    {
      "name": "iamjoel/four-dimensional-space-bag",
      "stars": 2,
      "img": "https://avatars.githubusercontent.com/u/2120155?s=40&v=4",
      "owner": "iamjoel",
      "repo_name": "four-dimensional-space-bag",
      "description": "内容一摞兜",
      "homepage": "https://iamjoel.github.io/four-dimensional-space-bag/site/build/",
      "language": "HTML",
      "created_at": "2023-01-12T08:04:01Z",
      "updated_at": "2025-04-22T03:44:47Z",
      "topics": [],
      "readme": "# 四次元口袋\n信息一罗兜。[网站](https://iamjoel.github.io/four-dimensional-space-bag/site/build/)\n\n主要内容都在：`/site` 下。\n\n用 [Documate](https://documate.site/) 基于文档查询的 Chat Bot。\n"
    },
    {
      "name": "sinhaGuild/sk-playground",
      "stars": 2,
      "img": "https://avatars.githubusercontent.com/u/18661288?s=40&v=4",
      "owner": "sinhaGuild",
      "repo_name": "sk-playground",
      "description": "A Streamlit based playground for trying out single and multi-skill orchestration with MS Semantic Kernel. Over 60+ semantic functions.",
      "homepage": null,
      "language": "Python",
      "created_at": "2023-07-05T02:56:07Z",
      "updated_at": "2024-05-16T10:12:41Z",
      "topics": [],
      "readme": "# Semantic Kernel playground\n\nA Streamlit based playground for trying out single and multi-skill orchestration with [Microsoft Semantic Kernel](https://learn.microsoft.com/en-us/semantic-kernel/overview/). Over 60+ semantic functions.\n\nSemantic Kernel is an open-source SDK that lets you easily combine AI services like OpenAI, Azure OpenAI, and Hugging Face with conventional programming languages like C# and Python. By doing so, you can create AI apps that combine the best of both worlds.\n\nPer the website.\n\n![Alt text](./imgs/sk.png)\n\n## References\n\n[MS Docs](https://learn.microsoft.com/en-us/semantic-kernel/overview/)\n\n[Project Miyagi](https://github.com/Azure-Samples/miyagi)\n\n## How to use\n\n1. Clone the repo and set environment variables in `.env`. \n   ```yml\n   OPENAI_API_KEY=\n   ```\n2. Create and activate python environment\n   ```sh\n   # for pip\n   pip install -r requirements.txt\n\n   # for conda\n   mv sample.environment.yml environment.yml\n   conda env create -f environment.yml\n   conda activate SEM_KRNL\n   ```\n\n3. Run the app\n   ```sh\n   streamlit run main.py\n   ```\n\n## Skills List\n\nSkills can be used `invidually` or `chained` with multiple skills in sequential mode with final output depending on the `last` skill selected.\n\n```yml\n0\tImportance\n1\tQuestion\n2\tEntity\n3\tCommandLinePython\n4\tCode\n5\tEmailSearch\n6\tDOSScript\n7\tCodePython\n8\tAssistantShowCalendarEvents\n9\tAssistantIntent\n10\tJoke\n11\tLimerick\n12\tExcuses\n13\tNovelChapter\n14\tTwoSentenceSummary\n15\tStoryGen\n16\tEnglishImprover\n17\tRewrite\n18\tBrainstorm\n19\tTranslate\n20\tNovelChapterWithNotes\n21\tAcronymGenerator\n22\tEmailGen\n23\tAcronym\n24\tShortPoem\n25\tEmailTo\n26\tNovelOutline\n27\tTellMeMore\n28\tAcronymReverse\n29\tElementAtIndex\n30\tContinue\n31\tBookIdeas\n32\tCreateBook\n33\tChat\n34\tChatFilter\n35\tChatV2\n36\tChatGPT\n37\tChatUser\n38\tMakeAbstractReadable\n39\tSummarize\n40\tTopics\n41\tNotegen\n42\tExciseEntities\n43\tExtractEntities\n44\tReferenceCheckEntities\n45\tForm\n46\tGitHubMemoryQuery\n47\tAssistantResults\n48\tQuestion\n49\tContextQuery\n50\tQNA\n```\n\n## Screenshots\n\n### Single Skill Inference\n\n![](./imgs/single-1.png)\n\n### Multi Skill Inference\n\n![](./imgs/chain-1.png)\n\n![](./imgs/chain-2.png)\n"
    },
    {
      "name": "layogtima/termi",
      "stars": 2,
      "img": "https://avatars.githubusercontent.com/u/119887692?s=40&v=4",
      "owner": "layogtima",
      "repo_name": "termi",
      "description": "The Terminal is an interactive cyberpunk narrative experience powered by OpenAI's GPT. In this experience, you'll communicate with a forsaken autonomous ship whose Terminal is accessible to you, navigating through a story set in a cyberpunk universe.",
      "homepage": "https://terminal-indol.vercel.app",
      "language": "SCSS",
      "created_at": "2023-05-10T02:41:55Z",
      "updated_at": "2025-02-13T15:26:02Z",
      "topics": [],
      "readme": "# Terminal 🌌🚀\n\n![Alt text](/docs/images/terminal.gif?raw=true \"Terminal Interface\")\n\nThe Terminal is an interactive cyberpunk narrative experience powered by OpenAI's GPT. In this experience, you'll communicate with a forsaken ship whose Terminal is accessible to you, navigating through a story set in a cyberpunk universe. This project uses HTML, JavaScript, Python, and Flask to create a captivating and immersive experience for the user.\n\n## Features\n\n- Interactive cyberpunk narrative\n- Utilizes OpenAI's GPT-4 for dynamic conversations and story progression\n- Customizable user interface with retro-futuristic design elements\n- Typing and ambient sound effects\n- Oscilloscope visualization of audio\n\n## Project Structure\n\nThis project contains the following files and directories:\n\n- `index.html` : Main HTML file containing the structure and layout of the Terminal.\n- `styles.css` : CSS file for styling the Terminal.\n- `script.js` : JavaScript file for handling user interactions and dynamic elements.\n- `index.py` : Python Flask application handling GPT API requests and responses.\n- `requirements.txt` : Python dependencies for running the Flask application.\n\n## Customizing Your Adventure\n\nFeel free to customize your Terminal experience by modifying the HTML, JavaScript, or Python code. This allows you to create your own adventures, change the user interface, or add new features to the game. Have fun experimenting and making Terminal your own!\n\n## How to run the Terminal?\n\n1. Install [Python](https://www.python.org/downloads/).\n2. Install [pip](https://pip.pypa.io/en/stable/installing/).\n3. Install [virtualenv](https://virtualenv.pypa.io/en/latest/installation.html).\n4. Create a virtual environment: `virtualenv .venv`.\n5. Activate the Python environment:\n   - Windows: `.venv\\Scripts\\activate`\n   - macOS/Linux: `source .venv/bin/activate`\n6. Install the requirements: `pip3 install -r requirements.txt`.\n7. Launch the Flask webserver: `flask --app index run --debug -p 2042`.\n8. Create a copy of .env.example as .env.\n9. Add your API keys to .env file.\n10. Open your browser and navigate to [http://localhost:2042](http://localhost:2042).\n11. Enjoy the interactive Terminal experience!\n\n## 🤝 Contributions & To-Do List\n\nContributions are welcome! Feel free to submit pull requests or open issues on GitHub. Here's our current to-do list:\n\n- Implement option selection for easier user interaction. Right now it's all chat-driven.\n- Incorporate event-driven scenarios (e.g., distress signals, meteorite detections) for Terminal-prompted actions.\n- Integrate the Python planner from Semantic Kernel for further development.\n- Focus on mobile accessibility and deployment to allow easy sharing and playtesting.\n- Explore basic authentication and consider integration with the prompt interface. Maybe magic links?\n- Reevaluate jQuery usage and consider supporting modern alternatives.\n- Rewrite necessary parts of the terminal and deploy it as an open-source library.\n\n## ❓ Questions & Answers? Got you boo!\n\n### What was the Midjourney (v5.1) prompt for the background? \n\n\"first person cyberpunk grunge style in terminal green (#4AF626) color, programmer's workstation with a laptop, plants, tablet, chair, desk, tools, lamp, hologram, soldering iron, star wars diorama, phone --s 750\"\n\n### What was the Midjourney (v5.1) prompt for the favicon? \n\n\"first person cyberpunk grunge style in terminal green (#4AF626) color, programmer's workstation with a laptop, plants, tablet, chair, desk, tools, lamp, hologram, soldering iron, star wars diorama, phone --s 750\"\n\n### JS Libraries\n- https://github.com/propjockey/augmented-ui\n- https://animejs.com/\n- https://github.com/thetarnav/glitched-writer\n- https://github.com/mathiasvr/audio-oscilloscope\n\n### Sources\n- Ambient audio\n  - https://pixabay.com/sound-effects/sand-in-city-street-thriller-7038/\n  - https://pixabay.com/sound-effects/tension-drones-51031/\n- Typing sounds\n    http://maiagame.com/about.php (thank you Simon!)\n\n## Thank you play testers!\n- Rohit\n- Parna\n- Deeksa\n- Purav\n- Chetan\n- Preet\n\n## 📄 License\n\nThis project is licensed under the GNU General Public License v3.0 (GPL-3.0) - see the [LICENSE](LICENSE) file for details.\n\nCopyleft © 2023 @layogtima (amit@absurd.industries)\n"
    },
    {
      "name": "ChinmaySheth/TMLSWorkshop",
      "stars": 2,
      "img": "https://avatars.githubusercontent.com/u/25516398?s=40&v=4",
      "owner": "ChinmaySheth",
      "repo_name": "TMLSWorkshop",
      "description": "Contains a basic UI which can be used with the semantic kernel backend example.",
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2023-06-05T12:20:07Z",
      "updated_at": "2023-06-15T17:41:26Z",
      "topics": [],
      "readme": "# TMLSWorkshopFrontend\n\n## Purpose\n\nThis repository contains a basic application which can be used for inspiration to deploy your own LLM. It is meant to serve advisors who want to use natural language to gather information about banking clients based on:\n- First name\n- Net worth\n- Location\n- Company\n\n## Technical Design\n\nBoth the frontend and backend contain Dockerfiles which can easily be built once [Docker](https://docs.docker.com/engine/install/) is installed. You must first build the docker containers using the instructions below and then run each docker container.\n\n### FRONTEND\n\n#### Building the Docker Container\n```cd tmls-workshop-frontend && docker build -t tmls-workshop-frontend```\n\n#### Running the frontend in a Docker container:\n```docker run -d -p 127.0.0.1:8400:80 tmls-workshop-frontend:latest```\n\n#### Building on top of the frontend\nIf you want to continue to build on top of the frontend then you will have to install [Angular](https://angular.io/guide/setup-local) and [NodeJS](https://nodejs.org/en/download).\n\n\n### BACKEND\nPlease don't forget to add a .env file to tmls-backend-workshop containing the following contents:\n```OPENAI_API_KEY=\"\"``` \n\n```OPENAI_API_KEY``` should be populated with your [OpenAI Key](https://openai.com/). The application's backend will be unable to connect to the OpenAI API without this .env file. Additionally, if the pipenv shell has already been launched, please make sure that you restart it everytime you update the .env file.\n\n#### Building the Docker Container\n```cd tmls-workshop-backend && docker build -t tmls-workshop-backend```\n\n#### Running the backend in a Docker container:\n```docker run tmls-workshop-backend uvicorn main:app --host 127.0.0.1 --port 80```\n\n#### Building on top of the backend\nIf you want to continue to build on top of the backend then you will have to install [Python](https://www.python.org/downloads/) and [Pipenv](https://pipenv.pypa.io/en/latest/installation/).\n"
    },
    {
      "name": "hamzaelgh/edge-rag",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/198704502?s=40&v=4",
      "owner": "hamzaelgh",
      "repo_name": "edge-rag",
      "description": "A cloud-agnostic RAG system with local LLMs & vector search",
      "homepage": null,
      "language": "Python",
      "created_at": "2025-03-04T22:26:06Z",
      "updated_at": "2025-04-15T08:17:02Z",
      "topics": [],
      "readme": "# 🧠 AI-Powered RAG System 🔍  \n**Deployable Offline | On-Premise | Any Cloud**  \n\nThis project showcases a **Retrieval-Augmented Generation (RAG) system** designed for **flexibility, scalability, and real-world deployment**. \n\nPowered by **Azure AI Containers**, it ensures **high-performance retrieval, accuracy, and security**—whether running **fully offline, or in any cloud environment**.\n\n## **Why This Matters?**\n\n**Hybrid Retrieval**: Combines **Qdrant (vector search)**, **BM25 (keyword matching)**, and **Ollama embeddings** to enhance ranking and accuracy.  \n\n**Multilingual AI**: Supports **Arabic & English** with optimized retrieval for short queries and complex prompts.  \n\n**Azure AI Integration**: Leverages **Azure AI Containers** for ensuring **better accuracy, security, and usability**—even in offline environments.  \n\n**On-Premise Ready**: Designed for **full offline deployment**, making it ideal for **customers needing secure, cloud-independent AI solutions**.  \n\n---\n## 🛠️ **Setup & Installation**  \n\n### **1️⃣ Preinstalled Requirements**\nEnsure you have the following installed before proceeding:\n- **Python 3.9+** → [Download Here](https://www.python.org/downloads/)\n- **Docker** → [Download Here](https://www.docker.com/get-started/)\n- **Ollama** → [Install Guide](https://ollama.com)\n\n### **1️⃣ Clone the Repository**\n```bash\ngit clone https://github.com/hamzaelgh/edge-rag.git\ncd edge-rag\n```\n\n### **2️⃣ Set Up Python Virtual Environment**\n```bash\npython3 -m venv venv\nsource venv/bin/activate  # macOS/Linux\nvenv\\Scripts\\activate   # Windows\n```\n\n### **3️⃣ Install Dependencies**\n```bash\npip install -r requirements.txt\nsource .env \n```\n\n### **4️⃣ Start Qdrant (Vector Database)**\nMake sure **Docker** is installed, then run:\n```bash\ndocker run -p 6333:6333 -p 6334:6334 qdrant/qdrant\n```\n\n### **5️⃣ Install & Run Ollama**\nFollow Ollama installation from [Ollama's official website](https://ollama.com). Then, pull the required models:\n```bash\nollama serve \nollama pull qwen2.5:0.5b\nollama pull gemma2:2b\nollama pull bge-m3\nollama pull jaluma/arabert-all-nli-triplet-matryoshka:latest \n```\n\n### **6️⃣ Run Azure AI Containers**\n#### **Language Detection**\n```bash\ndocker run --rm -it --platform linux/amd64 -p 5000:5000 --memory 6g --cpus 2 \\\n  mcr.microsoft.com/azure-cognitive-services/textanalytics/language \\\n  Eula=accept \\\n  Billing=\"$AZURE_LANGUAGE_BILLING_URL\" \\\n  ApiKey=\"$AZURE_LANGUAGE_API_KEY\"\n```\n\n```\ncurl -X POST \"http://localhost:5000/text/analytics/v3.1/languages\" \\\n     -H \"Content-Type: application/json\" \\\n     -d '{\n          \"documents\": [\n            {\"id\": \"1\", \"text\": \"Hello, how are you?\"},\n            {\"id\": \"2\", \"text\": \"مرحبا كيف حالك؟\"}\n          ]\n        }'\n```\n\n#### **NER**\n```bash \ndocker run --rm -it -p 5000:5000 --memory 8g --cpus 1 \\\nmcr.microsoft.com/azure-cognitive-services/textanalytics/ner:latest \\\nEula=accept \\\nBilling={AZURE_LANGUAGE_BILLING_URL} \\\nApiKey={AZURE_LANGUAGE_KEY}\n\n\n```\n\n```bash \ncurl -X POST \"http://localhost:5000/text/analytics/v3.1/entities/recognition/general\" -H \"Content-Type: application/json\" -H \"Ocp-Apim-Subscription-Key: ${AZURE_LANGUAGE_KEY}\" -d '{\"documents\":[{\"id\":\"1\",\"text\":\"Microsoft was founded by Bill Gates and Paul Allen in 1975. The company is headquartered in Redmond, Washington.\"}]}'\n```\n\n\n### **7️⃣ Prepare & Index Documents**\nStore your dataset inside the `data/` folder, then run:\n```bash\npython src/indexer.py\n```\n\nVerify if the Qdrant Collections Exist\n```\ncurl -X GET \"http://localhost:6333/collections\"\n```\n\nClean Qdrant Collections if needed. \n```\ncurl -X DELETE \"http://localhost:6333/collections/rag_docs_en\"\ncurl -X DELETE \"http://localhost:6333/collections/rag_docs_ar\"\n```\n\n### **8️⃣ Start the Streamlit UI**\n```bash\nstreamlit run src/app.py\n```\n\n### **9️⃣ Test Queries**\nOpen your browser at `http://localhost:8501` and enter any query.  \nExamples:  \n- **English:** `\"What is artificial intelligence?\"`  \n- **Arabic:** `\"ما هو الذكاء الاصطناعي؟\"`\n\n\n---\n\n## 🔹 How Azure AI Containers Can Enhance your Offline RAG System\nAzure AI Containers enable advanced AI capabilities while keeping the system **fully offline and on-premise-ready**. These services improve **document processing, query understanding, and response generation**, making the system **more accurate, secure, and scalable**.\n\n| **Category**                          | **Azure AI Service**                                  | **Enhancement** |\n|--------------------------------------|-----------------------------------------------------|----------------|\n| **📄 Improving Document Processing & Indexing** | **Azure AI Vision - Read** | Extracts text from **scanned documents & images**, making PDFs and handwritten content searchable. |\n|                                      | **Azure Document Intelligence** | Processes **structured documents** (e.g., invoices, contracts) before indexing, improving retrieval in legal and enterprise use cases. |\n| **🔎 Enhancing Query Understanding & Retrieval** | **Azure AI Language** | Detects **query language** to route requests correctly. |\n|                                      | **Conversational Language Understanding (CLU)** | Classifies **query intent** (e.g., search vs. summarization) for smarter responses. |\n| **🤖 Enhancing AI-Generated Responses** | **Azure AI Sentiment Analysis** | Adjusts AI response **tone** (formal/casual) based on sentiment detection. |\n|                                      | **Azure AI Text Translation** | Enables **cross-language retrieval** (e.g., Arabic query → English documents). |\n|                                      | **Neural Text-to-Speech (TTS)** | Converts **AI responses into speech**, enabling chatbot and voice assistant integrations. |\n| **🛡️ Ensuring Content Safety & Compliance** | **Azure AI Content Safety** | Scans **both text and images** for **violence, hate speech, self-harm, and explicit content**, ensuring AI-generated responses and retrieved documents comply with safety standards. |\n\n🔹 **These integrations ensure the RAG system remains fully functional in offline environments while benefiting from enterprise-grade AI.**  \n\n📌 **Learn More**: [Azure AI Containers](https://learn.microsoft.com/en-us/azure/ai-services/cognitive-services-container-support)  \n📌 **Azure AI Content Safety**: [Content Safety Containers (Preview)](https://learn.microsoft.com/en-us/azure/ai-services/content-safety/how-to/containers/container-overview)  \n\n---\n\n## ✅ Enhanced User Flow with Azure AI Containers\n\n| **Step** | **Tool Used** | **Description** |\n|----------|-------------|----------------|\n| **1. User enters or speaks a query** | Streamlit UI + **Azure AI Speech (Offline Container)** | Users can either type or speak their query. |\n| **2. Spellcheck query** | **Bergamot (Local Spellchecker)** | Fixes typos before processing the query. |\n| **3. Detect query language** | **Azure AI Language (Offline Container)** | Determines whether the query is in Arabic or English. |\n| **4. Translate query (if needed)** | **Azure Translator (Optional)** | Converts non-Arabic/English queries into a supported language. |\n| **5. Generate query embedding** | **Ollama (`bge-m3` for English & Arabic)** | Converts the query into a numerical vector representation. |\n| **6. Retrieve relevant documents** | **Qdrant (Vector DB) + BM25 Hybrid Retrieval** | Performs a **hybrid search**: vector similarity search (embeddings) + keyword-based retrieval (BM25). |\n| **7. Rank retrieved documents** | **BM25 (Rank-BM25) + `bge-m3` Reranking** | Ranks results based on **keyword relevance and vector similarity**. |\n| **8. Extract named entities (Optional)** | **Azure AI NER (Offline Container)** | Identifies **key entities** in the query to improve retrieval precision. |\n| **9. Apply OCR for document parsing** | **Azure Document Intelligence (Offline Container)** | Extracts text from **scanned PDFs, images, or structured documents** to improve knowledge base ingestion. |\n| **10. Summarize long documents (Optional)** | **Azure Text Summarization (Offline Container)** | Summarizes **retrieved long documents** before passing them to the LLM. |\n| **11. Generate an AI response** | **Ollama (`Qwen2.5` for English & `Gemma2B` for Arabic)** | Uses an **LLM** to generate an **answer using the top-ranked documents as context**. |\n| **12. Apply content safety filters** | **Azure AI Content Safety (Offline Container)** | Ensures the **AI-generated response follows safety guidelines**, filtering out harmful or inappropriate content. |\n| **13. Display response** | **Streamlit UI** | Shows **retrieved documents, scores, and the final AI response**. |\n\n---\n## 📌 **Addressing Arabic Language Challenges**\n\n1️⃣ Challenge: Arabic Ranker Models**\n\n**Problem:** Many ranker models struggle to reconstruct answers when the supporting information is scattered across multiple chunks.  \n**Solution:** We integrate **BM25 + bge-m3 reranker**, which improves the ranking of relevant Arabic documents based on **semantic similarity and keyword matching**.\n\n2️⃣ Challenge: Arabic Embedding Models**\n\n**Problem:** Single-word Arabic queries sometimes fail to retrieve results, even when relevant content exists in the knowledge base.  \n**Solution:** We use a **hybrid search approach**, combining:\n   - **Vector search (Ollama embeddings)**\n   - **BM25 keyword matching**\n   - **Reranking using bge-m3**\n   This ensures better retrieval even for **short Arabic queries**.\n\n\n3️⃣ Mitigating those Issues** \n\nOur current implementation mitigates these issues with:\n  - Hybrid Search (BM25 + Vectors)\n  - Re-ranking (bge-m3)\n  - Named Entity Recognition (NER)\n  - LLM Context Expansion\n\n4️⃣ Future Improvements\n-  **Experiment with specialized Arabic embedding models** (e.g., Arabic-trained versions of BGE or ARABERT).  \n-  **Optimize BM25 weights for Arabic vs. English separately** to fine-tune ranking balance.  \n-  **Extend Named Entity Recognition (NER) to improve keyword-based lookup**.  \n-  **Benchmark different Arabic language models for better retrieval performance**.\n\n---\nEnjoy building your **production-ready RAG system**! 🏗️🔥  "
    },
    {
      "name": "saytyarnorngloreia/ai-agents-for-beginners",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/205020882?s=40&v=4",
      "owner": "saytyarnorngloreia",
      "repo_name": "ai-agents-for-beginners",
      "description": "10 Lessons to Get Started Building AI Agents",
      "homepage": "https://microsoft.github.io/ai-agents-for-beginners/",
      "language": null,
      "created_at": "2025-03-27T00:10:56Z",
      "updated_at": "2025-03-27T00:11:03Z",
      "topics": [],
      "readme": "# AI Agents for Beginners - A Course\n\n![Generative AI For Beginners](./images/repo-thumbnail.png)\n\n## 10 Lessons teaching everything you need to know to start building AI Agents\n\n[![GitHub license](https://img.shields.io/github/license/microsoft/ai-agents-for-beginners.svg)](https://github.com/microsoft/ai-agents-for-beginners/blob/master/LICENSE?WT.mc_id=academic-105485-koreyst)\n[![GitHub contributors](https://img.shields.io/github/contributors/microsoft/ai-agents-for-beginners.svg)](https://GitHub.com/microsoft/ai-agents-for-beginners/graphs/contributors/?WT.mc_id=academic-105485-koreyst)\n[![GitHub issues](https://img.shields.io/github/issues/microsoft/ai-agents-for-beginners.svg)](https://GitHub.com/microsoft/ai-agents-for-beginners/issues/?WT.mc_id=academic-105485-koreyst)\n[![GitHub pull-requests](https://img.shields.io/github/issues-pr/microsoft/ai-agents-for-beginners.svg)](https://GitHub.com/microsoft/ai-agents-for-beginners/pulls/?WT.mc_id=academic-105485-koreyst)\n[![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat-square)](http://makeapullrequest.com?WT.mc_id=academic-105485-koreyst)\n\n### Language Support\n[![English](https://img.shields.io/badge/English-brightgreen.svg?style=flat-square)](README.md)\n[![Chinese Simplified](https://img.shields.io/badge/Chinese_Simplified-brightgreen.svg?style=flat-square)](./translations/zh/README.md)\n[![Chinese Traditional](https://img.shields.io/badge/Chinese_Traditional-brightgreen.svg?style=flat-square)](./translations/tw/README.md)     \n[![Chinese Hong Kong](https://img.shields.io/badge/Chinese_Hong_Kong-brightgreen.svg?style=flat-square)](./translations/hk/README.md) \n[![French](https://img.shields.io/badge/French-brightgreen.svg?style=flat-square)](./translations/fr/README.md)\n[![Japanese](https://img.shields.io/badge/Japanese-brightgreen.svg?style=flat-square)](./translations/ja/README.md) \n[![Korean](https://img.shields.io/badge/Korean-brightgreen.svg?style=flat-square)](./translations/ko/README.md)\n[![Portuguese Brazilian](https://img.shields.io/badge/Portuguese_Brazilian-brightgreen.svg?style=flat-square)](./translations/pt/README.md)\n[![Spanish](https://img.shields.io/badge/Spanish-brightgreen.svg?style=flat-square)](./translations/es/README.md)\n[![German](https://img.shields.io/badge/German-brightgreen.svg?style=flat-square)](./translations/de/README.md)  \n[![Persian](https://img.shields.io/badge/Persian-brightgreen.svg?style=flat-square)](./translations/fa/README.md) \n[![Polish](https://img.shields.io/badge/Polish-brightgreen.svg?style=flat-square)](./translations/pl/README.md) \n\n[![GitHub watchers](https://img.shields.io/github/watchers/microsoft/ai-agents-for-beginners.svg?style=social&label=Watch)](https://GitHub.com/microsoft/ai-agents-for-beginners/watchers/?WT.mc_id=academic-105485-koreyst)\n[![GitHub forks](https://img.shields.io/github/forks/microsoft/ai-agents-for-beginners.svg?style=social&label=Fork)](https://GitHub.com/microsoft/ai-agents-for-beginners/network/?WT.mc_id=academic-105485-koreyst)\n[![GitHub stars](https://img.shields.io/github/stars/microsoft/ai-agents-for-beginners.svg?style=social&label=Star)](https://GitHub.com/microsoft/ai-agents-for-beginners/stargazers/?WT.mc_id=academic-105485-koreyst)\n\n[![Azure AI Discord](https://dcbadge.limes.pink/api/server/kzRShWzttr)](https://discord.gg/kzRShWzttr)\n\n\n## 🌱 Getting Started\n\nThis course has 10 lessons covering the fundamentals of building AI Agents. Each lesson covers its own topic so start wherever you like!\n\nThere is multi-language support for this course. Go to our [available languages here](#-multi-language-support). \n\nIf this is your first time building with Generative AI models, check out our [Generative AI For Beginners](https://aka.ms/genai-beginners) course, which includes 21 lessons on building with GenAI.\n\nDon't forget to [star (🌟) this repo](https://docs.github.com/en/get-started/exploring-projects-on-github/saving-repositories-with-stars?WT.mc_id=academic-105485-koreyst) and [fork this repo](https://github.com/microsoft/ai-agents-for-beginners/fork) to run the code.\n\n### What You Need \n\nEach lesson in this course includes code examples, which can be found in the code_samples folder. You can [fork this repo](https://github.com/microsoft/ai-agents-for-beginners/fork) to create your own copy.  \n\nThe code example in these exercises, utilize Azure AI Foundry and GitHub Model Catalogs for interacting with Language Models:\n\n- [Github Models](https://aka.ms/ai-agents-beginners/github-models) - Free / Limited\n- [Azure AI Foundry](https://aka.ms/ai-agents-beginners/ai-foundry) - Azure Account Required\n\nThis course also uses the following AI Agent frameworks and services from Microsoft:\n\n- [Azure AI Agent Service](https://aka.ms/ai-agents-beginners/ai-agent-service)\n- [Semantic Kernel](https://aka.ms/ai-agents-beginners/semantic-kernel)\n- [AutoGen](https://aka.ms/ai-agents/autogen)\n\nFor more information on running the code for this course, go to the [Course Setup](./00-course-setup/README.md).\n\n## 🙏 Want to help?\n\nDo you have suggestions or found spelling or code errors? [Raise an issue](https://github.com/microsoft/ai-agents-for-beginners/issues?WT.mc_id=academic-105485-koreyst) or [Create a pull request](https://github.com/microsoft/ai-agents-for-beginners/pulls?WT.mc_id=academic-105485-koreyst)\n\nIf you get stuck or have any questions about building AI Agents, join our [Azure AI Community Discord](https://discord.gg/kzRShWzttr).\n\n## 📂 Each lesson includes\n\n- A written lesson located in the README and a short video\n- Python code samples supporting Azure AI Foundry and Github Models (Free)\n- Links to extra resources to continue your learning\n\n\n## 🗃️ Lessons\n\n| **Lesson**                               | **Text & Code**                                    | **Video**                                                  | **Extra Learning**                                                                     |\n|------------------------------------------|----------------------------------------------------|------------------------------------------------------------|----------------------------------------------------------------------------------------|\n| Intro to AI Agents and Agent Use Cases   | [Link](./01-intro-to-ai-agents/README.md)          | [Video](https://youtu.be/3zgm60bXmQk?si=z8QygFvYQv-9WtO1)  | [Link](https://aka.ms/ai-agents-beginners/collection?WT.mc_id=academic-105485-koreyst) |\n| Exploring AI Agentic Frameworks          | [Link](./02-explore-agentic-frameworks/README.md)  | [Video](https://youtu.be/ODwF-EZo_O8?si=Vawth4hzVaHv-u0H)  | [Link](https://aka.ms/ai-agents-beginners/collection?WT.mc_id=academic-105485-koreyst) |\n| Understanding AI Agentic Design Patterns | [Link](./03-agentic-design-patterns/README.md)     | [Video](https://youtu.be/m9lM8qqoOEA?si=BIzHwzstTPL8o9GF)  | [Link](https://aka.ms/ai-agents-beginners/collection?WT.mc_id=academic-105485-koreyst) |\n| Tool Use Design Pattern                  | [Link](./04-tool-use/README.md)                    | [Video](https://youtu.be/vieRiPRx-gI?si=2z6O2Xu2cu_Jz46N)  | [Link](https://aka.ms/ai-agents-beginners/collection?WT.mc_id=academic-105485-koreyst) |\n| Agentic RAG                              | [Link](./05-agentic-rag/README.md)                 | [Video](https://youtu.be/WcjAARvdL7I?si=gKPWsQpKiIlDH9A3)  | [Link](https://aka.ms/ai-agents-beginners/collection?WT.mc_id=academic-105485-koreyst) |\n| Building Trustworthy AI Agents           | [Link](./06-building-trustworthy-agents/README.md) | [Video](https://youtu.be/iZKkMEGBCUQ?si=jZjpiMnGFOE9L8OK ) | [Link](https://aka.ms/ai-agents-beginners/collection?WT.mc_id=academic-105485-koreyst) |\n| Planning Design Pattern                  | [Link](./07-planning-design/README.md)             | [Video](https://youtu.be/kPfJ2BrBCMY?si=6SC_iv_E5-mzucnC)  | [Link](https://aka.ms/ai-agents-beginners/collection?WT.mc_id=academic-105485-koreyst) |\n| Multi-Agent Design Pattern               | [Link](./08-multi-agent/README.md)                 | [Video](https://youtu.be/V6HpE9hZEx0?si=rMgDhEu7wXo2uo6g)  | [Link](https://aka.ms/ai-agents-beginners/collection?WT.mc_id=academic-105485-koreyst) |\n| Metacognition Design Pattern             | [Link](./09-metacognition/README.md)               | [Video](https://youtu.be/His9R6gw6Ec?si=8gck6vvdSNCt6OcF)  | [Link](https://aka.ms/ai-agents-beginners/collection?WT.mc_id=academic-105485-koreyst) |\n| AI Agents in Production                  | [Link](./10-ai-agents-production/README.md)        | [Video](https://youtu.be/l4TP6IyJxmQ?si=31dnhexRo6yLRJDl)  | [Link](https://aka.ms/ai-agents-beginners/collection?WT.mc_id=academic-105485-koreyst) |\n\n## 🌐 Multi-Language Support\n\n| Language             | Code | Link to Translated README                               | Last Updated |\n|----------------------|------|---------------------------------------------------------|--------------|\n| Chinese (Simplified) | zh   | [Chinese Translation](./translations/zh/README.md)      | 2025-03-24   |\n| Chinese (Traditional)| tw   | [Chinese Translation](./translations/tw/README.md)      | 2025-02-13   |\n| Chinese (Hong Kong)  | hk   | [Chinese (Hong Kong) Translation](./translations/hk/README.md) | 2025-02-13   |\n| French               | fr   | [French Translation](./translations/fr/README.md)       | 2025-02-13   |\n| Japanese             | ja   | [Japanese Translation](./translations/ja/README.md)     | 2025-02-13   |\n| Korean               | ko   | [Korean Translation](./translations/ko/README.md)       | 2025-02-13   |\n| Portuguese           | pt   | [Portuguese Translation](./translations/pt/README.md)   | 2025-02-13   |\n| Spanish              | es   | [Spanish Translation](./translations/es/README.md)      | 2025-02-13   |\n| German               | de   | [German Translation](./translations/de/README.md)       | 2025-02-13   |\n| Persian              | fa   | [Persian Translation](./translations/fa/README.md)       | 2025-03-26   |\n| Polish               | pl   | [Polish Translation](./translations/pl/README.md)       | 2025-03-26  |\n\n## 🎒 Other Courses\n\nOur team produces other courses! Check out:\n\n- [**NEW** Generative AI for Beginners using .NET](https://github.com/microsoft/Generative-AI-for-beginners-dotnet?WT.mc_id=academic-105485-koreyst)\n- [Generative AI for Beginners](https://github.com/microsoft/generative-ai-for-beginners?WT.mc_id=academic-105485-koreyst)\n- [ML for Beginners](https://aka.ms/ml-beginners?WT.mc_id=academic-105485-koreyst)\n- [Data Science for Beginners](https://aka.ms/datascience-beginners?WT.mc_id=academic-105485-koreyst)\n- [AI for Beginners](https://aka.ms/ai-beginners?WT.mc_id=academic-105485-koreyst)\n- [Cybersecurity for Beginners](https://github.com/microsoft/Security-101??WT.mc_id=academic-96948-sayoung)\n- [Web Dev for Beginners](https://aka.ms/webdev-beginners?WT.mc_id=academic-105485-koreyst)\n- [IoT for Beginners](https://aka.ms/iot-beginners?WT.mc_id=academic-105485-koreyst)\n- [XR Development for Beginners](https://github.com/microsoft/xr-development-for-beginners?WT.mc_id=academic-105485-koreyst)\n- [Mastering GitHub Copilot for AI Paired Programming](https://aka.ms/GitHubCopilotAI?WT.mc_id=academic-105485-koreyst)\n- [Mastering GitHub Copilot for C#/.NET Developers](https://github.com/microsoft/mastering-github-copilot-for-dotnet-csharp-developers?WT.mc_id=academic-105485-koreyst)\n- [Choose Your Own Copilot Adventure](https://github.com/microsoft/CopilotAdventures?WT.mc_id=academic-105485-koreyst)\n\n## 🌟 Community Thanks\n\nThanks to [Shivam Goyal](https://www.linkedin.com/in/shivam2003/) for contributing important code samples demonstrating Agentic RAG. \n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit <https://cla.opensource.microsoft.com>.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft\ntrademarks or logos is subject to and must follow\n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos is subject to those third-parties' policies.\n"
    },
    {
      "name": "CassandraOfTroy/data-modernization",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/58147404?s=40&v=4",
      "owner": "CassandraOfTroy",
      "repo_name": "data-modernization",
      "description": null,
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2025-03-29T14:56:31Z",
      "updated_at": "2025-04-11T06:14:16Z",
      "topics": [],
      "readme": "# Data Modernization\n\nAn experimental toolkit for speeding up modernization of the legacy data systems to cloud-native architectures on Azure.\n\n## Overview\n\nThis repository contains two complementary projects focused on modernizing legacy SQL Server data platforms to modern cloud-native architectures on Microsoft Azure:\n\n1. **SQL to Medallion**: A toolkit for migrating SQL Server stored procedures to a medallion architecture (bronze, silver, gold layers) on Microsoft Fabric.\n\n2. **SQL Migration Agents**: A multi-agent system for analyzing, transforming, and modernizing SQL code using AI-powered agents.\n\nBoth projects leverage Azure OpenAI capabilities to assist with code translation, analysis, and transformation.\n\n## Project Structure\n\n```\ndata-modernization/\n├── sql-to-medallion/           # SQL Server to medallion architecture migration toolkit\n│   ├── config/                 # Configuration files\n│   ├── data/                   # Sample data and schemas\n│   ├── notebooks/              # Example notebooks\n│   ├── src/                    # Source code\n│   │   ├── bronze/             # Bronze layer processing\n│   │   ├── silver/             # Silver layer processing\n│   │   ├── gold/               # Gold layer processing\n│   │   ├── llm/                # LLM integration\n│   │   ├── orchestration/      # Pipeline orchestration\n│   │   └── utils/              # Utility functions\n│   └── requirements.txt        # Project dependencies\n│\n├── sql-migration-agents/       # Multi-agent system for SQL modernization\n│   ├── data/                   # Example SQL files\n│   │   ├── input/              # Input SQL files\n│   │   └── output/             # Generated output\n│   ├── src/\n│   │   ├── agents/             # Agent implementations\n│   │   ├── cli/                # Command-line interface\n│   │   ├── config/             # Configuration\n│   │   └── utils/              # Utility functions\n│   ├── tests/                  # Unit and integration tests\n│   ├── setup.py                # Package setup\n│   └── requirements.txt        # Project dependencies\n│\n└── README.md                   # This file\n```\n\n## Key Features\n\n### SQL to Medallion\n\nA toolkit designed to support the migration of legacy SQL Server stored procedures to a modern data lakehouse architecture using the medallion pattern on Fabric.\n\n- **Automated SQL to PySpark Translation**: Convert SQL Server stored procedures to PySpark code\n- **Medallion Architecture Implementation**: Organize data processing into bronze, silver, and gold layers\n- **Azure OpenAI Integration**: Leverage AI for complex code translation and analysis\n- **Comprehensive Logging**: Track transformation processes and data lineage\n- **Modular Design**: Separate modules for extraction, transformation, and loading\n\n### SQL Migration Agents\n\nA multi-agent system for modernizing SQL stored procedures and migrating them to modern data platforms like Fabric.\n\n- **Multi-Agent Architecture**: Specialized agents working together to handle complex SQL modernization tasks\n- **Azure AI Integration**: Seamless integration with Azure OpenAI services\n- **PySpark Translations**: Convert SQL Server stored procedures to PySpark code\n- **SQL Analysis**: Analyze and understand complex SQL structures\n- **Test Generation**: Automatic generation of test cases\n- **Command-line Interface**: Easy to use CLI for integration into existing workflows\n\n## Getting Started\n\n### Prerequisites\n\n- Python 3.8+\n- Azure OpenAI API access or OpenAI API access\n- SQL Server with JDBC connectivity (for SQL to Medallion)\n\n### SQL to Medallion\n\nSee the detailed [SQL to Medallion README](./sql-to-medallion/README.md) for setup instructions and usage examples.\n\n### SQL Migration Agents\n\nSee the detailed [SQL Migration Agents README](./sql-migration-agents/README.md) for setup instructions and usage examples.\n\n## Use Cases\n\n### When to use SQL to Medallion\n\n- When migrating a specific set of SQL Server stored procedures to PySpark code\n- When implementing a medallion architecture directly\n- When working with data engineers who prefer a more hands-on approach\n- For smaller migration projects with clear scope and requirements\n\n### When to use SQL Migration Agents\n\n- When dealing with SQL modernization projects\n- When automation is a priority for the migration process\n- When creating a self-service tool for data teams to use\n- When needing detailed analysis and documentation of legacy code\n- For generating tests and validation code along with the modernization\n"
    },
    {
      "name": "jo99112/sg-ai-dev-tools",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/127510384?s=40&v=4",
      "owner": "jo99112",
      "repo_name": "sg-ai-dev-tools",
      "description": "Study group resources for AI development tools, focusing on LLMs, GenAI, and Agent architectures including Model Context Protocol (MCP)",
      "homepage": null,
      "language": null,
      "created_at": "2025-04-08T10:33:15Z",
      "updated_at": "2025-04-23T11:15:48Z",
      "topics": [
        "agents",
        "ai",
        "ai-development",
        "fine-tuning",
        "genai",
        "langchain",
        "llm",
        "model-context-protocol",
        "rag",
        "study-group"
      ],
      "readme": "# 🚀 SG AI Dev Tools\n\nWelcome to the **SG AI Dev Tools** repository! This project serves as a resource hub for our study group focused on AI development tools. Our main areas of interest include Large Language Models (LLMs), Generative AI (GenAI), and various agent architectures, including the Model Context Protocol (MCP).\n\n![AI Development](https://img.shields.io/badge/AI_Development-Resources-blue.svg)\n\n## Table of Contents\n\n- [Introduction](#introduction)\n- [Topics Covered](#topics-covered)\n- [Getting Started](#getting-started)\n- [Releases](#releases)\n- [Contributing](#contributing)\n- [License](#license)\n- [Contact](#contact)\n\n## Introduction\n\nIn the rapidly evolving field of artificial intelligence, having the right tools is essential for success. Our study group aims to explore these tools, share knowledge, and collaborate on projects that enhance our understanding of AI development. \n\nThis repository is designed to provide resources, guides, and examples that will help both newcomers and experienced developers navigate the complex landscape of AI technologies.\n\n## Topics Covered\n\nThis repository focuses on several key areas:\n\n- **Agents**: Learn about intelligent agents and their applications in AI.\n- **AI Development**: Explore various frameworks and libraries that facilitate AI development.\n- **Fine-Tuning**: Understand the techniques for fine-tuning models to improve performance.\n- **Generative AI (GenAI)**: Dive into the world of generative models and their potential.\n- **LangChain**: Discover how LangChain can be used to build applications with LLMs.\n- **Large Language Models (LLM)**: Study the architecture and functionality of LLMs.\n- **Model Context Protocol (MCP)**: Gain insights into MCP and its role in agent architectures.\n- **Retrieval-Augmented Generation (RAG)**: Learn how RAG enhances generative models by incorporating external knowledge.\n\n## Getting Started\n\nTo get started with the resources in this repository, follow these steps:\n\n1. **Clone the Repository**: Use the following command to clone the repository to your local machine.\n\n   ```bash\n   git clone https://github.com/jo99112/sg-ai-dev-tools.git\n   ```\n\n2. **Explore the Resources**: Navigate through the folders to find various resources, including tutorials, code examples, and documentation.\n\n3. **Download and Execute Releases**: For the latest updates and tools, visit our [Releases](https://github.com/jo99112/sg-ai-dev-tools/releases) section. Download the necessary files and execute them as needed.\n\n## Releases\n\nWe regularly update this repository with new tools and resources. To stay up to date, check the [Releases](https://github.com/jo99112/sg-ai-dev-tools/releases) section for the latest downloads. \n\n![Latest Releases](https://img.shields.io/badge/Latest_Releases-Download-orange.svg)\n\n## Contributing\n\nWe welcome contributions from everyone. If you have ideas, resources, or code that you would like to share, please follow these steps:\n\n1. **Fork the Repository**: Create your own copy of the repository.\n2. **Make Changes**: Implement your changes or add new resources.\n3. **Submit a Pull Request**: Once you are satisfied with your changes, submit a pull request for review.\n\nYour contributions help us grow and improve the resources available to the community.\n\n## License\n\nThis project is licensed under the MIT License. See the [LICENSE](LICENSE) file for details.\n\n## Contact\n\nFor any questions or suggestions, feel free to reach out:\n\n- **Email**: [your-email@example.com](mailto:your-email@example.com)\n- **GitHub**: [jo99112](https://github.com/jo99112)\n\n---\n\nThank you for visiting the SG AI Dev Tools repository! We look forward to collaborating and learning together."
    },
    {
      "name": "julicjung/ai-agents",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/12299771?s=40&v=4",
      "owner": "julicjung",
      "repo_name": "ai-agents",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2025-04-08T08:57:21Z",
      "updated_at": "2025-04-17T14:32:52Z",
      "topics": [],
      "readme": "# ai-agents"
    },
    {
      "name": "pablocast/gbbai-ai-foundry-sdk-workshop",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/18387357?s=40&v=4",
      "owner": "pablocast",
      "repo_name": "gbbai-ai-foundry-sdk-workshop",
      "description": "Azure AI Foundry SDK Workshop",
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2025-02-27T16:12:35Z",
      "updated_at": "2025-04-15T18:25:45Z",
      "topics": [],
      "readme": "# <img src=\"./utils/media/ai_foundry.png\" alt=\"Azure Foundry\" style=\"width:80px;height:30px;\"/> Azure AI Foundry Workshop Notebooks\n\nThis directory contains Jupyter notebooks for hands-on exercises with Azure AI Foundry.\n\n## 🔧 Prerequisites\n\n+ [azd](https://learn.microsoft.com/azure/developer/azure-developer-cli/install-azd), used to deploy all Azure resources and assets used in this sample.\n+ [PowerShell Core pwsh](https://github.com/PowerShell/powershell/releases) if using Windows\n+ [Python 3.11](https://www.python.org/downloads/release/python-3110/)\n+  [An Azure Subscription](https://azure.microsoft.com/free/) with Contributor permissions\n+  [Sign in to Azure with Azure CLI](https://learn.microsoft.com/cli/azure/authenticate-azure-cli-interactively)\n+  [VS Code](https://code.visualstudio.com/) installed with the [Jupyter notebook extension](https://marketplace.visualstudio.com/items?itemName=ms-toolsai.jupyter) enabled\n\n## Instructions\n\n1. **Python Environment Setup**\n   ```bash\n   python3.11 -m venv .venv\n   source .venv/bin/activate  # On Windows: .venv\\Scripts\\activate\n   pip install -r requirements.txt\n   ```\n\n2. **Create the infrastructure**\n   - Run the [1-create-infra notebook](1-infra/1-create-infra.ipynb)\n   \n   After running, an `.env` file will be created with all necessary environment variables\n\n3. **Running the Notebooks**\n   - Open each [notebook's folder](2-notebooks/) and execute the notebook\n\n4. **Delete the Resources**\n   - Run the [2-clean-up-resources](1-infra/2-clean-up-resources.ipynb)\n\n"
    },
    {
      "name": "kunanba/agents",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/51249060?s=40&v=4",
      "owner": "kunanba",
      "repo_name": "agents",
      "description": null,
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2025-04-04T15:43:19Z",
      "updated_at": "2025-04-08T17:20:01Z",
      "topics": [],
      "readme": "# Repository Overview\n\nThis repository contains two separate projects:\n\n1. **azure_ai_agent_send_email**  \n   An Azure AI Agent that sends emails using a custom email service.\n\n2. **sk_agents**  \n   A Semantic Kernel (SK) based SQL assistant that queries a PostgreSQL database and retrieves schema information using two custom plugins.\n\nEach project has its own README file with detailed instructions and required libraries.\n\n---\n\n## Directory Structure\n\n```plaintext\n├── azure_ai_agent_send_email\n│   ├── README.md              # Instructions and requirements for the email agent.\n│   ├── email_services.py       # Contains the email sending function.\n│   └── agent_notebook.ipynb    # Notebook demonstrating the email agent functionality.\n│   \n└── sk_agents\n    ├── README.md              # Instructions and requirements for the SQL agent.\n    ├── sql_agent.ipynb       # Main notebook containing the SQL and schema plugins and agent integration.\n    └── requirements.txt   \n```\n\n## Folder Details\n\n### 1. azure_ai_agent_send_email\n\nThis folder contains the project that demonstrates how to use an Azure AI Agent to send emails using a custom email service.\n\n**Key Features:**\n- **Agent Integration:** Uses Azure AI Agent to process user instructions and trigger an email sending function.\n- **Custom Email Function:** Integrates a function that sends an email via Azure Communication Services. The function uses a default email template, modifying only the subject when specified.\n- **Environment Variables:** Requires environment variables such as `PROJECT_CONNECTION_STRING`, `MODEL_DEPLOYMENT_NAME`, `EMAIL_COMMUNICATION_SERVICES_STRING`, `RECIPIENT_EMAIL`, and `SENDER_EMAIL` to be set.\n- **Required Libraries:** \n  - `azure-ai-projects`\n  - `azure-identity`\n  - `azure-communication-email`\n  - `python-dotenv`\n\n**Usage:**\n- Open the provided Jupyter Notebook (or Python script) to see the agent in action.\n- Follow the README in this folder for setup instructions and how to run the agent.\n\n---\n\n### 2. sk_agents\n\nThis folder contains the project that builds a SQL assistant using the Semantic Kernel framework to interact with a PostgreSQL database.\n\n**Key Features:**\n- **SQL Query Plugin:** The `QueryPostgresPlugin` executes SQL queries on a PostgreSQL database.\n- **Schema Retrieval Plugin:** The `GetSchemaPlugin` retrieves database schema details, such as table names, column names, and data types.\n- **Chat Agent:** Combines the plugins within a `ChatCompletionAgent` that processes user input to generate and execute SQL queries.\n- **Environment Variables:** Requires variables like `POSTGRES_CONNECTION_STRING`, `GLOBAL_LLM_SERVICE`, and `AZURE_OPENAI_CHAT_DEPLOYMENT_NAME`.\n- **Required Libraries:**\n  - `psycopg2-binary`\n  - `python-dotenv`\n  - `semantic-kernel`\n  - `azure-identity`\n  - `requests`\n\n**Usage:**\n- Follow the README in respective folder for detailed setup instructions and library requirements.\n"
    },
    {
      "name": "yingding/AI-Agent-Workshop-2025",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/1073701?s=40&v=4",
      "owner": "yingding",
      "repo_name": "AI-Agent-Workshop-2025",
      "description": "This is a code repository for AI Agent Workshop",
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2025-03-25T13:47:30Z",
      "updated_at": "2025-04-13T10:17:27Z",
      "topics": [],
      "readme": "# AI-Agent-Workshop-2025\nThis is a code repository for AI Agent Workshop\n\n## Quickstart\n\n1. Create your python 3.12 environment with the `requirements.txt` file\n2. Attach your python venv to the `enterprise-streaming-agent.ipynb`\n3. Run all the notebook cell\n4. Open `http://127.0.0.1:7861/` in your browser\n\n## Author's notes\n1. VENV: azagent3.12pipo\n\n\n# Reference\n* Model Supports Azure AI Agent Service: https://learn.microsoft.com/en-gb/azure/ai-services/agents/concepts/model-region-support\n\n\n\n"
    },
    {
      "name": "arashaga/agents-hackathon",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/1166344?s=40&v=4",
      "owner": "arashaga",
      "repo_name": "agents-hackathon",
      "description": "This repository includes samples for agentic solutions built for hackaton",
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2025-04-02T15:15:47Z",
      "updated_at": "2025-04-08T17:30:05Z",
      "topics": [],
      "readme": "### Azure AI Hachathon\n\nThis repository demonstrates the implementation of multi-agent use cases using the Semantic Kernel framework.\n\n### Features\n\n1. **Multi-Agent Content Creation**  \n    Leverages Semantic Kernel, AgentGroupChat, and ChatCompletion API for collaborative content generation.\n\n2. **NL2SQL Agent**  \n    A natural language to SQL agent that queries a sample AdventureWorks database.\n\n3. **Semantic Kernel Agent with Model Context Protocol (MCP)**  \n    Implements a sample agent using the MCP framework.\n\n### Getting Started\n\nFollow these steps to execute the notebooks:\n\n1. **Environment Setup**  \n    - Rename `.env-sample` to `.env` and populate the required environment variables.\n\n2. **NL2SQL Agent**  \n    - Deploy a sample AdventureWorks database.\n    - Ensure you have a SQL Server connection string with SQL authentication.\n\n3. **API Keys**  \n    - Different notebooks may require specific API keys. Refer to the documentation within each notebook for details.\n\n### Prerequisites\n\n- Ensure all dependencies are installed as per the project requirements.\n- Verify that your environment variables and API keys are correctly configured.\n\n### Notes\n\n- For detailed instructions, refer to the comments and documentation within each notebook.\n- Contributions and feedback are welcome to improve this repository further."
    },
    {
      "name": "shubbham28/financial-agent",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/29838339?s=40&v=4",
      "owner": "shubbham28",
      "repo_name": "financial-agent",
      "description": null,
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2025-04-01T13:52:28Z",
      "updated_at": "2025-04-02T11:27:49Z",
      "topics": [],
      "readme": "# 🧠 Financial Analysis Agent with LLMs\n\nA modular AI agent that performs real-time financial data analysis, sentiment extraction, and generates smart summaries using:\n- 📈 Yahoo Finance data\n- 🤗 FinBERT for sentiment\n- 🦙 LLaMA for prompt validation\n- 🖥️ Streamlit for chatbot UI\n\n---\n\n## 🚀 Features\n- ✅ Real-time stock analysis (EMA, RSI, MACD, Bollinger Bands)\n- 📰 Live news sentiment from Finviz using FinBERT\n- 🧠 LLM-generated summary of trends and predictions\n- 🧪 Input validation using OpenAI API\n- 💬 Chatbot-style UI with Streamlit\n\n---\n\n## 🔧 Installation\n\n```bash\ngit clone https://github.com/shubbham28/financial-agent.git\ncd financial-agent\npip install -r requirements.txt\n```\n\n---\n\n## ▶️ Running the App\n\n```bash\nstreamlit run app.py\n```\nThe app launches in your browser with a chat-style interface. Ask things like:\n- `TSLA`\n- `TSLA, AAPL, GOOG`\n\n---\n\n## 🔐 Configuration\nUpdate `config/model_config.yaml` with your OpenAI and Hugging Face keys:\n```yaml\nopenai:\n  api_key: \"your-openai-key\"\nhuggingface:\n  api_key: \"your-huggingface-key\"\n  model: \"meta-llama/Llama-2-7b-chat-hf\"\n```\n\nUse `.env` or Hugging Face Secrets for production deployment.\n\n---\n\n## 📁 Project Structure\n```\nfinancial_agent/\n├── app.py                     # Main Streamlit chatbot UI\n├── agent_kernel.py            # Loads Semantic Kernel and adds skills\n├── config/\n│   ├── model_config.yaml      # API keys and model info\n│   ├── prompt_templates.yaml  # Templates for LLM instructions\n├── skills/\n│   ├── financial_skill.py     # Gets EMA, RSI, MACD, etc. via yfinance\n│   ├── sentiment_skill.py     # Scrapes Finviz and runs FinBERT manually\n│   ├── summary_skill.py       # Builds LLM prompt and summarizes with OpenAI\n├── requirements.txt           # Python dependencies\n└── README.md                  # You're here\n```\n---\n\n## 📦 Deployment\n- GitHub: Push this repo\n- Hugging Face: Create a Streamlit Space, upload this project, and set secrets.\n\n---\n\n## ✨ Acknowledgements\n- OpenAI GPT for summarization\n- ProsusAI FinBERT for sentiment\n- Hugging Face for hosting + models\n\n---\n\n## 🧪 Future Ideas\n- Add prediction via regression models\n- Integrate news from other sources\n- Use LangChain + memory\n\n---\n\n## 📜 License\nMIT License\n"
    },
    {
      "name": "sanjeevkumar761/eval-with-azure-eval-sdk",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/1672998?s=40&v=4",
      "owner": "sanjeevkumar761",
      "repo_name": "eval-with-azure-eval-sdk",
      "description": "Evaluation using Azure AI Evaluation SDK",
      "homepage": null,
      "language": "Python",
      "created_at": "2024-10-21T15:06:41Z",
      "updated_at": "2025-04-02T20:32:23Z",
      "topics": [],
      "readme": "# eval-with-azure-eval-sdk\nEvaluation using Azure AI Evaluation SDK  \n\nSteps to run:  \n1. Save .env-sample file as .env and populate values for environment variables  \n2. pip install -r requirements.txt  \n3. python eval-with-dataset.py  "
    },
    {
      "name": "sifinell/document-analysis-questionnaire-system",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/15127653?s=40&v=4",
      "owner": "sifinell",
      "repo_name": "document-analysis-questionnaire-system",
      "description": "A proof-of-concept application using Azure AI services to fill financial questionnaires",
      "homepage": null,
      "language": "Python",
      "created_at": "2025-03-25T13:48:35Z",
      "updated_at": "2025-03-26T13:27:56Z",
      "topics": [],
      "readme": "# Document Analysis and Q&A System\n\nThis repository contains a proof-of-concept application that demonstrates how to use Azure AI services to process, analyze, and extract insights from document collections. The system combines Azure Document Intelligence for document parsing with Azure OpenAI for metadata generation and question answering.\n\n## Overview\n\nThe application performs these key functions:\n\n1. **Document Processing:** Convert PDF documents to markdown format using Azure Document Intelligence\n2. **Metadata Generation:** Extract key insights, entities, and categorize documents using Azure OpenAI\n3. **Document Indexing:** Create a searchable index of all processed documents\n4. **Interactive Q&A:** Answer complex questions about the documents using a multi-agent system\n\n## Architecture\n\nThe system consists of several components:\n\n![Architecture Diagram](architecture.jpg)\n\n- **Document Processing:** Converts PDFs to markdown with metadata\n- **Docs Metadata:** Maintains a search-friendly catalog of all processed documents\n- **Group Chat:** Orchestrates multi-agent conversations to answer questions\n  - Document Retriever Agent: Selects relevant documents\n  - Answer Generator Agent: Creates answers from document content\n  - Answer Validator Agent: Ensures accuracy and formatting\n\n## Setup\n\n### Prerequisites\n\n- Azure Document Intelligence service\n- Azure OpenAI service\n\n### Environment Configuration\n\n1. Create a `.env` file with your Azure service credentials:\n\n```\nDOCUMENTINTELLIGENCE_ENDPOINT=your_document_intelligence_endpoint\nDOCUMENTINTELLIGENCE_API_KEY=your_document_intelligence_key\nAZURE_OPENAI_ENDPOINT=your_azure_openai_endpoint\nAZURE_OPENAI_API_KEY=your_azure_openai_key\nAZURE_OPENAI_CHAT_DEPLOYMENT_NAME=your_deployment_name\nAZURE_OPENAI_API_VERSION=2023-05-15\n```\n\n2. Install dependencies:\n\n```bash\npip install -r requirements.txt\n```\n\n## Usage\n\n### Processing Documents\n\nTo process a collection of documents:\n\n```python\nfrom document_processing import process_folder\n\n# Process all PDFs in the specified folder and save results to processed_documents/\nprocess_folder(\"anonymized_examples\")\n```\n\n## Schema-Based Q&A\n\nThe system uses JSON schema files to define questions and answer formats. Here's an example schema:\n\n```json\n{\n  \"assetsAndIncome\": {\n    \"properties\": {\n      \"totalBankableAssets\": {\n        \"displayName\": \"Total Bankable Assets\",\n        \"description\": \"What is the total value of bankable assets for $_var_individual?\",\n        \"example\": \"CHF 1'500'000.00\"\n      },\n      \"totalRealEstateValue\": {\n        \"displayName\": \"Real Estate Value\",\n        \"description\": \"What is the total value of real estate owned by $_var_individual?\",\n        \"example\": \"CHF 2'500'000.00\"\n      }\n    }\n  }\n}\n```\n\n## Key Features\n\n- **Intelligent Document Processing:** Convert complex PDFs to searchable markdown\n- **Metadata Enrichment:** Automatically generate descriptive metadata\n- **Multi-Agent Q&A:** Use specialized agents for different parts of the question answering process\n- **Source Citations:** Track which documents provided each answer\n- **Template-Based Answers:** Ensure consistent formatting in responses\n\n## Limitations\n\n- Currently optimized for financial documents\n- Limited support for complex layouts\n"
    },
    {
      "name": "strudel0209/kyc-sk",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/9446504?s=40&v=4",
      "owner": "strudel0209",
      "repo_name": "kyc-sk",
      "description": "A document processing and financial data extraction system built on Microsoft Semantic Kernel and Azure OpenAI, designed to automate Know Your Client (KYC) processes",
      "homepage": null,
      "language": "Python",
      "created_at": "2025-03-25T10:09:12Z",
      "updated_at": "2025-03-26T13:26:24Z",
      "topics": [],
      "readme": "# KYC Agent System\n\nA document processing and financial data extraction system built on Microsoft Semantic Kernel and Azure OpenAI, designed to automate Know Your Client (KYC) processes.\n\n## Overview\n\nThe KYC Agent System is designed to automate the extraction and analysis of financial information from various document types. It uses a suite of specialized AI agents to identify clients, classify documents, extract financial data, and calculate metrics like net worth.\n\nThe system can process bank statements, financial assets, liabilities, and other financial documents, extracting structured data that can be used for financial analysis, compliance, and client onboarding.\n\n## Features\n\n- **Multilingual Document Processing**: Detects language and translates non-English documents\n- **Client Identification**: Extracts client names, account numbers, and other identifying information\n- **Document Classification**: Categorizes financial documents by type\n- **Financial Data Extraction**: Identifies assets, liabilities, and income information\n- **Currency Normalization**: Standardizes financial values across different currencies\n- **Net Worth Calculation**: Computes client net worth based on extracted assets and liabilities\n- **Azure Integration**: Optional Azure Blob Storage for document processing and Azure Functions support\n\n## System Architecture\n\nThe system consists of these key components:\n\n- **Core System (KYCAgentSystem)**: Orchestrates the overall document processing workflow\n- **Specialized Agents**:\n  - ClientIdentificationAgent: Extracts client information\n  - DocumentClassificationAgent: Classifies document types\n  - AssetIdentificationAgent: Identifies financial assets\n  - LiabilityIdentificationAgent: Identifies financial liabilities\n  - CurrencyNormalizationAgent: Standardizes currency values\n  - MultilingualAgent: Handles language detection and translation\n  - NetWorthCalculatorAgent: Calculates net worth\n  - FinancialOverviewAgent: Extracts comprehensive financial overviews\n\n## Installation\n\n### Prerequisites\n\n- Python 3.11+ recommended\n- Azure OpenAI API access\n- (Optional) Azure Blob Storage account\n- (Optional) Azure Document Intelligence (formerly Form Recognizer)\n\n### Setup\n\n1. Clone the repository:\n   ```bash\n   git clone https://github.com/strudel0209/kyc-sk.git\n   cd kyc-sk\n   ```\n\n2. Create and activate a virtual environment:\n   ```bash\n   python -m venv venv\n   # On Windows\n   venv\\Scripts\\activate\n   # On Linux/macOS\n   source venv/bin/activate\n   ```\n\n3. Install the required packages:\n   ```bash\n   pip install -r requirements.txt\n   ```\n\n4. Create a `.env` file in the project root with your Azure credentials:\n   ```\n   AZURE_OPENAI_ENDPOINT=your-endpoint-url\n   AZURE_OPENAI_API_KEY=your-api-key\n   AZURE_OPENAI_DEPLOYMENT_NAME=your-deployment-name\n   \n   # Optional - for Blob Storage support\n   BLOB_CONNECTION_STRING=your-blob-connection-string\n   FR_ENDPOINT=your-document-intelligence-endpoint\n   FR_KEY=your-document-intelligence-key\n   ```\n\n5. If you plan to test Azure Functions locally, create a `local.settings.json` file in the `functions` directory:\n   ```json\n   {\n     \"IsEncrypted\": false,\n     \"Values\": {\n       \"AzureWebJobsStorage\": \"UseDevelopmentStorage=true\",\n       \"FUNCTIONS_WORKER_RUNTIME\": \"python\",\n       \"AZURE_OPENAI_ENDPOINT\": \"your-endpoint-url\",\n       \"AZURE_OPENAI_API_KEY\": \"your-api-key\",\n       \"AZURE_OPENAI_DEPLOYMENT_NAME\": \"your-deployment-name\",\n       \"BLOB_CONNECTION_STRING\": \"your-blob-connection-string\",\n       \"FR_ENDPOINT\": \"your-document-intelligence-endpoint\",\n       \"FR_KEY\": \"your-document-intelligence-key\"\n     },\n     \"Host\": {\n       \"LocalHttpPort\": 7071,\n       \"CORS\": \"*\"\n     }\n   }\n   ```\n\n## Configuration\n\nThe system can be configured to run locally with in-memory storage or use Azure Blob Storage for document processing:\n\n- **Local Mode**: Set `use_blob_storage=False` when initializing the KYCAgentSystem\n- **Azure Mode**: Set `use_blob_storage=True` to utilize Azure Blob Storage and Document Intelligence\n\n### Environment Variables\n\n| Variable | Description | Required |\n| --- | --- | --- |\n| AZURE_OPENAI_ENDPOINT | Azure OpenAI API endpoint URL | Yes |\n| AZURE_OPENAI_API_KEY | Azure OpenAI API key | Yes |\n| AZURE_OPENAI_DEPLOYMENT_NAME | Azure OpenAI model deployment name | Yes |\n| AZURE_OPENAI_API_VERSION | API version (default: 2023-05-15) | No |\n| BLOB_CONNECTION_STRING | Azure Blob Storage connection string | For Azure mode only |\n| FR_ENDPOINT | Azure Document Intelligence endpoint | For Azure mode only |\n| FR_KEY | Azure Document Intelligence key | For Azure mode only |\n\n## Usage\n\n### Running Locally for Testing\n\nYou can run the system locally using the provided `main.py` script for basic testing and development:\n\n```bash\npython main.py\n```\n\nThis will:\n1. Load environment variables\n2. Initialize the KYC system\n3. Process a sample bank statement\n4. Output the extracted information as JSON\n\nThis approach is useful for quick testing of the core system functionality but doesn't simulate the real-time document processing workflow.\n\n### Running Azure Functions Locally\n\nFor a more complete testing experience that simulates the real-time processing workflow:\n\n1. Install the Azure Functions Core Tools if you haven't already:\n   ```bash\n   npm install -g azure-functions-core-tools@4 --unsafe-perm true\n   ```\n\n2. Navigate to the functions directory:\n   ```bash\n   cd functions\n   ```\n\n3. Start the function app locally:\n   ```bash\n   func start\n   ```\n\nThis will start the Azure Functions runtime locally, which will poll for new documents uploaded to your configured blob storage container. When a document is uploaded to the \"kyc-documents\" container, the function will automatically trigger, process the document, and store the results in the \"kyc-results\" container.\n\n### Running with Azure Functions in Production\n\nThe system can be deployed as an Azure Function that triggers when documents are uploaded to a Blob Storage container:\n\n1. Deploy the functions folder to an Azure Function App\n2. Configure the required environment variables in your Function App settings\n3. Upload documents to the \"kyc-documents\" container\n4. Processed results will be stored in the \"kyc-results\" container\n\n### Code Example\n\n```python\nimport asyncio\nfrom kyc.system import KYCAgentSystem\n\nasync def process_document(document_content, document_name):\n    # Initialize the system\n    system = KYCAgentSystem(use_blob_storage=False)\n    \n    # Process the document\n    result = await system.analyze_document(document_content, document_name)\n    \n    # Use the result\n    print(result)\n    return result\n\n# Run with asyncio\ndocument = \"Bank Statement\\nAccount Holder: Jane Doe\\nBalance: $10,000\"\nasyncio.run(process_document(document, \"statement.txt\"))\n```\n\n## Extending the System\n\n### Adding New Agent Types\n\n1. Create a new agent class in the `kyc/agents/` directory\n2. Inherit from `BaseAgent` and implement your custom functionality\n3. Register the agent in `KYCAgentSystem._register_agents()`\n\n### Customizing Prompts\n\nEach agent defines its own prompts in the respective agent class. Modify these prompts to customize how agents extract information from documents.\n\n## Development\n\n### Project Structure\n\n```\nkyc-sk/\n├── kyc/                        # Main package\n│   ├── agents/                 # Specialized agents\n│   │   ├── asset_agent.py\n│   │   ├── client_agent.py\n│   │   └── ...\n│   ├── storage/                # Storage implementations\n│   │   ├── blob_storage.py\n│   │   └── memory_store.py\n│   ├── config.py               # Configuration settings\n│   ├── system.py               # Main system implementation\n│   └── utils.py                # Utility functions\n├── functions/                  # Azure Functions\n│   └── document_processor/     # Document processing function\n├── main.py                     # Local execution script\n├── run_test.py                 # Test script\n├── requirements.txt            # Dependencies\n└── .env                        # Environment variables\n```\n\n## License\n\n[Specify your license here]\n\n## Contributing\n\n[Add contributing guidelines if applicable]"
    },
    {
      "name": "mgboot/ai-tutor",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/55988909?s=40&v=4",
      "owner": "mgboot",
      "repo_name": "ai-tutor",
      "description": "AI Tutor POC that focuses on identifying the source of student misunderstanding ",
      "homepage": null,
      "language": "Python",
      "created_at": "2025-02-27T15:47:07Z",
      "updated_at": "2025-04-02T14:22:07Z",
      "topics": [],
      "readme": "# ai-tutor\nAI Tutor POC that focuses on identifying the source of student misunderstanding \n\n\n# Example\n\t• Student is studying electronics and circuit design.\n\t• At some chapter in the book the AI Tutor (Quiz Creator) will create a test to see if they understand the concepts and can do exercises commensurate with what they have been studying.\n\t• The AI tutor (evaluator) notices after several questions that the student seems to always get stuck on parts that involve capacitors.  They understand resistors, transistors, etc. really well, but get the final answer wrong based on this lack of knowledge in a base skill like capacitors.  \n\t• The AI Tutor (Info Reviewer) presents the student with a couple chapters or paragraphs explaining capacitors again, or from a different text book (or online video).\n\t• The AI Tutor (Quiz Creator) then builds new test to see if the student now has a solid understanding of capacitors and will not be \"left behind without a base understanding\" as the class moves on.  \n\t• This system ensures the student doesn't sit in a class/lecture not really understanding some basic pieces and waste time when they could be evaluated and redirected to what will help them most right now.  Once they have that they will more quickly catch back up with the class and be able to take advantage of the new learnings with a solid base.\n![image](https://github.com/user-attachments/assets/49b2d5fe-ecff-498d-a243-39635d4adec9)\n\n"
    },
    {
      "name": "Azure/multiagent-frameworks",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/6844498?s=40&v=4",
      "owner": "Azure",
      "repo_name": "multiagent-frameworks",
      "description": null,
      "homepage": "",
      "language": "Jupyter Notebook",
      "created_at": "2025-01-29T11:23:33Z",
      "updated_at": "2025-04-06T22:49:18Z",
      "topics": [],
      "readme": "# Multi-Agent Frameworks\n\nMulti-Agent frameworks aide in developing powerful applications with the goal to improve accuracy, reduce hallucinations, handle complex user tasks by integrating Large Language Models into enterprise applications. \n\n\nThis repository contains a collection of multi-agent frameworks that can be used to simulate and study multi-agent systems. The frameworks are implemented in Python and are designed to be easy to use and extend. The frameworks compared in this repository are:\n\n- [Autogen v0.4](https://www.microsoft.com/en-us/research/blog/autogen-v0-4-reimagining-the-foundation-of-agentic-ai-for-scale-extensibility-and-robustness/)\n- [Semantic Kernel](https://learn.microsoft.com/en-us/semantic-kernel/overview/)\n- [Azure AI Agent Service](https://learn.microsoft.com/en-us/azure/ai-services/agents/overview?view=azure-python-preview)\n- [OpenAI Agents SDK](https://openai.github.io/openai-agents-python/)\n\nThis repo is designed to easily understand different frameworks and help developers with nuances and compare the different approaches. \n\nA simple scenario is used to demonstrate the capabilities of the frameworks. The scenario is a simple conversation between two agents for a fintech company helping users with Account and Transaction related questions.\nThese scenarios are addressed:\n\n- Concurrency \n    \n    Handling simulatneous conversations between users and agents.\n\n- Agent routing\n\n    Routing the user task to the correct agent based on the user query and enabling the agents to communicate with each other.\n\n- LLM Swapping\n\n    Swap the LLM models used by the agents based on task criticalility. For example, using a more powerful model for router and a simpler model for agents.\n\n- Chat History and Memory Management\n\n    Managing the chat history and memory of the agents and maintain chat history across multiple users and agents. \n\n- Function Calling\n\n    Implementing function calling across agents and router.\n\n- Observability\n\n    Monitoring and tracing conversations between the agents and the users.\n\n- Authentication\n\n    Authenticating users with Microsoft Entra. \n\n\nEach framework is implemented in a separate folder. The folder contains the implementation of the framework and the scenario. Please refer to the README.md in each folder for more details on how to run the scenario using the framework.\n\n\n## Common Installation Steps\n\n- Azure OpenAI Service or AI Foundry Access is required. \n- AZ CLI is required and user should be logged in with az login. \n- Python 3.11 or higher is installed. \n- Clone the repository\n\n    ```bash or cmd\n    git clone https://github.com/Azure/multiagent-frameworks.git\n\n- Python Steps:\n    \n    ```bash or cmd\n    python -m venv venv\n\n    # cmd \n    venv\\Scripts\\activate\n\n    # bash\n    source venv/bin/activate\n\n    pip install -r requirements.txt\n\n\n## Multi Agent Scenario \n\nThe a multi agent scenario, the key concept is that the agents needs to be aware of other agents' tools and capabilities. The capabilities are defined in the system message for each agents and tools using python functions are made avaialble to each agent. \nThe system message for the router agent will typically contain the list of agents and their capabilities. The system message for the agents will contain the list of tools and the capabilities of the agents.\nThe router agent will also have instructions on which agent to route the user question to based on the user question.\n\nNow given a user questions, the router agent will decompose the questions and assign tasks to the agents. The agents will then use the tools to answer the questions and submit a resposne back to a common location which could be a chat history, and pub sub topic, or a thread in the case of Assistant API. \nThen a Final responder agent will inspect the chat history and determine whether the user questions has been answered or not. If the user question has been answered, the final responder will return the final answer to the user. Otherwise it will submit the question to the router agent for further processing.\n\nThe scenario that is implemented in this repository is a simple 2 agent and a router agent use case, who can answer user questions about accounts and transactions for a fictious company called Transify which is a Financial Services company.\n\nThis scenario is implemented in each of the frameworks and the implementation details are provided in the README.md in each of the framework folders.\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n"
    },
    {
      "name": "Progressive-Insurance/chainlit",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/33731436?s=40&v=4",
      "owner": "Progressive-Insurance",
      "repo_name": "chainlit",
      "description": "Build Conversational AI in minutes ⚡️",
      "homepage": "https://oso-web-edv-oso-prod.prod.glb.pgrcloud.app/contributions/a522ff9b-4714-4d31-a306-c70fd553c35e",
      "language": "TypeScript",
      "created_at": "2024-08-16T14:13:44Z",
      "updated_at": "2025-04-18T03:00:32Z",
      "topics": [],
      "readme": "<h1 align=\"center\">Welcome to Chainlit by Literal AI 👋</h1>\n\n<p align=\"center\">\n<b>Build python production-ready conversational AI applications in minutes, not weeks ⚡️</b>\n\n</p>\n<p align=\"center\">\n    <a href=\"https://discord.gg/k73SQ3FyUh\" rel=\"nofollow\"><img alt=\"Discord\" src=\"https://dcbadge.vercel.app/api/server/ZThrUxbAYw?style=flat\" style=\"max-width:100%;\"></a>\n    <a href=\"https://twitter.com/chainlit_io\" rel=\"nofollow\"><img alt=\"Twitter\" src=\"https://img.shields.io/twitter/url/https/twitter.com/chainlit_io.svg?style=social&label=Follow%20%40chainlit_io\" style=\"max-width:100%;\"></a>\n    <a href=\"https://pypistats.org/packages/chainlit\" rel=\"nofollow\"><img alt=\"Downloads\" src=\"https://img.shields.io/pypi/dm/chainlit\" style=\"max-width:100%;\"></a>\n        <a href=\"https://github.com/chainlit/chainlit/graphs/contributors\" rel=\"nofollow\"><img alt=\"Contributors\" src=\"https://img.shields.io/github/contributors/chainlit/chainlit\" style=\"max-width:100%;\"></a>\n    <a href=\"https://github.com/Chainlit/chainlit/actions/workflows/ci.yaml\" rel=\"nofollow\"><img alt=\"CI\" src=\"https://github.com/Chainlit/chainlit/actions/workflows/ci.yaml/badge.svg\" style=\"max-width:100%;\"></a>\n</p>\n\n<p align=\"center\">\n    <a href=\"https://chainlit.io\"><b>Website</b></a>  •  \n    <a href=\"https://docs.chainlit.io\"><b>Documentation</b></a>  •  \n    <a href=\"https://help.chainlit.io\"><b>Chainlit Help</b></a>  •  \n    <a href=\"https://github.com/Chainlit/cookbook\"><b>Cookbook</b></a>\n</p>\n\n<p align=\"center\">\n    <a href=\"https://trendshift.io/repositories/6708\" target=\"_blank\"><img src=\"https://trendshift.io/api/badge/repositories/6708\" alt=\"Chainlit%2Fchainlit | Trendshift\" style=\"width: 250px; height: 45px;\" width=\"250\" height=\"45\"/></a>\n</p>\n\nhttps://github.com/user-attachments/assets/b3738aba-55c0-42fa-ac00-6efd1ee0d148\n\n> [!NOTE]\n> Chainlit is maintained by [Literal AI](https://literalai.com), an LLMOps platform to monitor and evaluate LLM applications! It works with any Python or TypeScript applications and [seamlessly](https://docs.chainlit.io/llmops/literalai) with Chainlit. For enterprise support, please fill this [form](https://docs.google.com/forms/d/e/1FAIpQLSdPVGqfuaWSC2DfunR6cY4C7kUHl0c2W7DnhzsF9bmMxrVpkg/viewform?usp=header).\n\n## Installation\n\nOpen a terminal and run:\n\n```sh\npip install chainlit\nchainlit hello\n```\n\nIf this opens the `hello app` in your browser, you're all set!\n\n### Development version\n\nThe latest in-development version can be installed straight from GitHub with:\n\n```sh\npip install git+https://github.com/Chainlit/chainlit.git#subdirectory=backend/\n```\n\n(Requires Node and pnpm installed on the system.)\n\n## 🚀 Quickstart\n\n### 🐍 Pure Python\n\nCreate a new file `demo.py` with the following code:\n\n```python\nimport chainlit as cl\n\n\n@cl.step(type=\"tool\")\nasync def tool():\n    # Fake tool\n    await cl.sleep(2)\n    return \"Response from the tool!\"\n\n\n@cl.on_message  # this function will be called every time a user inputs a message in the UI\nasync def main(message: cl.Message):\n    \"\"\"\n    This function is called every time a user inputs a message in the UI.\n    It sends back an intermediate response from the tool, followed by the final answer.\n\n    Args:\n        message: The user's message.\n\n    Returns:\n        None.\n    \"\"\"\n\n\n    # Call the tool\n    tool_res = await tool()\n\n    await cl.Message(content=tool_res).send()\n```\n\nNow run it!\n\n```sh\nchainlit run demo.py -w\n```\n\n<img src=\"/images/quick-start.png\" alt=\"Quick Start\"></img>\n\n## 📚 More Examples - Cookbook\n\nYou can find various examples of Chainlit apps [here](https://github.com/Chainlit/cookbook) that leverage tools and services such as OpenAI, Anthropiс, LangChain, LlamaIndex, ChromaDB, Pinecone and more.\n\nTell us what you would like to see added in Chainlit using the Github issues or on [Discord](https://discord.gg/k73SQ3FyUh).\n\n## 💁 Contributing\n\nAs an open-source initiative in a rapidly evolving domain, we welcome contributions, be it through the addition of new features or the improvement of documentation.\n\nFor detailed information on how to contribute, see [here](/CONTRIBUTING.md).\n\n## 📃 License\n\nChainlit is open-source and licensed under the [Apache 2.0](LICENSE) license.\n"
    },
    {
      "name": "mard-llm/vetbert-basic",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/202657196?s=40&v=4",
      "owner": "mard-llm",
      "repo_name": "vetbert-basic",
      "description": "A veterinary chatbot for disease prediction and recommendations.",
      "homepage": "",
      "language": "Python",
      "created_at": "2025-03-10T14:19:55Z",
      "updated_at": "2025-03-22T16:00:03Z",
      "topics": [],
      "readme": "# vetbert-basic\nA basic AI tool for veterinary disease classification and recommendations.\n\n\n# Usage on Linux\n```\n# Clone the repository\ngit clone https://github.com/mard-llm/vetbert-basic.git\n\n# Navigate into the directory\ncd vetbert-basic\n\n# Create a virtual environment\npython3 -m venv venv\n\n# Activate the virtual environment\nsource venv/bin/activate\n\n# Install dependencies from requirements.txt\npip3 install -r requirements.txt\n\n# Install and start Ollama, then pull LLaMA 3.1 8B\ncurl -fsSL https://ollama.com/install.sh | sh\nollama serve &\nollama pull llama3.1:8b\n\n# Run the chatbot\npython3 mard.py\n```\n# Changing the Model\nYou can swap LLaMA 3.1 8B for another Ollama-supported model (e.g., Phi-3, Mistral) to adjust performance or resource use: \nalso change the model in the mard.py .\n"
    },
    {
      "name": "HeCoded/MetaGPT",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/167261630?s=40&v=4",
      "owner": "HeCoded",
      "repo_name": "MetaGPT",
      "description": "🌟 The Multi-Agent Framework: First AI Software Company, Towards Natural Language Programming",
      "homepage": "https://deepwisdom.ai/",
      "language": null,
      "created_at": "2025-03-03T03:11:48Z",
      "updated_at": "2025-03-03T03:11:59Z",
      "topics": [],
      "readme": "\n# MetaGPT: The Multi-Agent Framework\n\n<p align=\"center\">\n<a href=\"\"><img src=\"docs/resources/MetaGPT-new-log.png\" alt=\"MetaGPT logo: Enable GPT to work in a software company, collaborating to tackle more complex tasks.\" width=\"150px\"></a>\n</p>\n\n<p align=\"center\">\n<b>Assign different roles to GPTs to form a collaborative entity for complex tasks.</b>\n</p>\n\n<p align=\"center\">\n<a href=\"docs/README_CN.md\"><img src=\"https://img.shields.io/badge/文档-中文版-blue.svg\" alt=\"CN doc\"></a>\n<a href=\"README.md\"><img src=\"https://img.shields.io/badge/document-English-blue.svg\" alt=\"EN doc\"></a>\n<a href=\"docs/README_FR.md\"><img src=\"https://img.shields.io/badge/document-French-blue.svg\" alt=\"FR doc\"></a>\n<a href=\"docs/README_JA.md\"><img src=\"https://img.shields.io/badge/ドキュメント-日本語-blue.svg\" alt=\"JA doc\"></a>\n<a href=\"https://opensource.org/licenses/MIT\"><img src=\"https://img.shields.io/badge/License-MIT-blue.svg\" alt=\"License: MIT\"></a>\n<a href=\"docs/ROADMAP.md\"><img src=\"https://img.shields.io/badge/ROADMAP-路线图-blue\" alt=\"roadmap\"></a>\n<a href=\"https://discord.gg/DYn29wFk9z\"><img src=\"https://dcbadge.vercel.app/api/server/DYn29wFk9z?style=flat\" alt=\"Discord Follow\"></a>\n<a href=\"https://twitter.com/MetaGPT_\"><img src=\"https://img.shields.io/twitter/follow/MetaGPT?style=social\" alt=\"Twitter Follow\"></a>\n</p>\n\n<p align=\"center\">\n   <a href=\"https://vscode.dev/redirect?url=vscode://ms-vscode-remote.remote-containers/cloneInVolume?url=https://github.com/geekan/MetaGPT\"><img src=\"https://img.shields.io/static/v1?label=Dev%20Containers&message=Open&color=blue&logo=visualstudiocode\" alt=\"Open in Dev Containers\"></a>\n   <a href=\"https://codespaces.new/geekan/MetaGPT\"><img src=\"https://img.shields.io/badge/Github_Codespace-Open-blue?logo=github\" alt=\"Open in GitHub Codespaces\"></a>\n   <a href=\"https://huggingface.co/spaces/deepwisdom/MetaGPT-SoftwareCompany\" target=\"_blank\"><img alt=\"Hugging Face\" src=\"https://img.shields.io/badge/%F0%9F%A4%97%20-Hugging%20Face-blue?color=blue&logoColor=white\" /></a>\n</p>\n\n## News\n🚀 Feb. 19, 2025: Today we are officially launching our natural language programming product: MGX (MetaGPT X) - the world's first AI agent development team.  [Offical website](https://mgx.dev/) [Twitter](https://x.com/MetaGPT_/status/1892199535130329356)\n\n🚀 Feb. 17, 2025: We introduced two papers: [SPO](https://arxiv.org/pdf/2502.06855) and [AOT](https://arxiv.org/pdf/2502.12018), check the [code](examples)!\n\n🚀 Jan. 22, 2025: Our paper [AFlow: Automating Agentic Workflow Generation](https://openreview.net/forum?id=z5uVAKwmjf) accepted for **oral presentation (top 1.8%)** at ICLR 2025, **ranking #2** in the LLM-based Agent category.\n\n🚀 Oct. 29, 2024: We introduced three papers: [AFLOW](https://arxiv.org/abs/2410.10762), [FACT](https://arxiv.org/abs/2410.21012), and [SELA](https://arxiv.org/abs/2410.17238), check the [code](examples)!\n\n🚀 Mar. 29, 2024: [v0.8.0](https://github.com/geekan/MetaGPT/releases/tag/v0.8.0) released. Now you can use Data Interpreter ([arxiv](https://arxiv.org/abs/2402.18679), [example](https://docs.deepwisdom.ai/main/en/DataInterpreter/), [code](https://github.com/geekan/MetaGPT/tree/main/examples/di)) via pypi package import. Meanwhile, we integrated the RAG module and supported multiple new LLMs.\n\n🚀 Feb. 08, 2024: [v0.7.0](https://github.com/geekan/MetaGPT/releases/tag/v0.7.0) released, supporting assigning different LLMs to different Roles. We also introduced [Data Interpreter](https://github.com/geekan/MetaGPT/blob/main/examples/di/README.md), a powerful agent capable of solving a wide range of real-world problems.\n\n🚀 Jan. 16, 2024: Our paper [MetaGPT: Meta Programming for A Multi-Agent Collaborative Framework\n](https://openreview.net/forum?id=VtmBAGCN7o) accepted for **oral presentation (top 1.2%)** at ICLR 2024, **ranking #1** in the LLM-based Agent category.\n\n🚀 Jan. 03, 2024: [v0.6.0](https://github.com/geekan/MetaGPT/releases/tag/v0.6.0) released, new features include serialization, upgraded OpenAI package and supported multiple LLM, provided [minimal example for debate](https://github.com/geekan/MetaGPT/blob/main/examples/debate_simple.py) etc.\n\n🚀 Dec. 15, 2023: [v0.5.0](https://github.com/geekan/MetaGPT/releases/tag/v0.5.0) released, introducing some experimental features such as incremental development, multilingual, multiple programming languages, etc.\n\n🔥 Nov. 08, 2023: MetaGPT is selected into [Open100: Top 100 Open Source achievements](https://www.benchcouncil.org/evaluation/opencs/annual.html).\n\n🔥 Sep. 01, 2023: MetaGPT tops GitHub Trending Monthly for the **17th time** in August 2023.\n\n🌟 Jun. 30, 2023: MetaGPT is now open source.\n\n🌟 Apr. 24, 2023: First line of MetaGPT code committed.\n\n## Software Company as Multi-Agent System\n\n1. MetaGPT takes a **one line requirement** as input and outputs **user stories / competitive analysis / requirements / data structures / APIs / documents, etc.**\n2. Internally, MetaGPT includes **product managers / architects / project managers / engineers.** It provides the entire process of a **software company along with carefully orchestrated SOPs.**\n   1. `Code = SOP(Team)` is the core philosophy. We materialize SOP and apply it to teams composed of LLMs.\n\n![A software company consists of LLM-based roles](docs/resources/software_company_cd.jpeg)\n\n<p align=\"center\">Software Company Multi-Agent Schematic (Gradually Implementing)</p>\n\n## Get Started\n\n### Installation\n\n> Ensure that Python 3.9 or later, but less than 3.12, is installed on your system. You can check this by using: `python --version`.  \n> You can use conda like this: `conda create -n metagpt python=3.9 && conda activate metagpt`\n\n```bash\npip install --upgrade metagpt\n# or `pip install --upgrade git+https://github.com/geekan/MetaGPT.git`\n# or `git clone https://github.com/geekan/MetaGPT && cd MetaGPT && pip install --upgrade -e .`\n```\n\nFor detailed installation guidance, please refer to [cli_install](https://docs.deepwisdom.ai/main/en/guide/get_started/installation.html#install-stable-version)\n or [docker_install](https://docs.deepwisdom.ai/main/en/guide/get_started/installation.html#install-with-docker)\n\n### Configuration\n\nYou can init the config of MetaGPT by running the following command, or manually create `~/.metagpt/config2.yaml` file:\n```bash\n# Check https://docs.deepwisdom.ai/main/en/guide/get_started/configuration.html for more details\nmetagpt --init-config  # it will create ~/.metagpt/config2.yaml, just modify it to your needs\n```\n\nYou can configure `~/.metagpt/config2.yaml` according to the [example](https://github.com/geekan/MetaGPT/blob/main/config/config2.example.yaml) and [doc](https://docs.deepwisdom.ai/main/en/guide/get_started/configuration.html):\n\n```yaml\nllm:\n  api_type: \"openai\"  # or azure / ollama / groq etc. Check LLMType for more options\n  model: \"gpt-4-turbo\"  # or gpt-3.5-turbo\n  base_url: \"https://api.openai.com/v1\"  # or forward url / other llm url\n  api_key: \"YOUR_API_KEY\"\n```\n\n### Usage\n\nAfter installation, you can use MetaGPT at CLI\n\n```bash\nmetagpt \"Create a 2048 game\"  # this will create a repo in ./workspace\n```\n\nor use it as library\n\n```python\nfrom metagpt.software_company import generate_repo, ProjectRepo\nrepo: ProjectRepo = generate_repo(\"Create a 2048 game\")  # or ProjectRepo(\"<path>\")\nprint(repo)  # it will print the repo structure with files\n```\n\nYou can also use [Data Interpreter](https://github.com/geekan/MetaGPT/tree/main/examples/di) to write code:\n\n```python\nimport asyncio\nfrom metagpt.roles.di.data_interpreter import DataInterpreter\n\nasync def main():\n    di = DataInterpreter()\n    await di.run(\"Run data analysis on sklearn Iris dataset, include a plot\")\n\nasyncio.run(main())  # or await main() in a jupyter notebook setting\n```\n\n\n### QuickStart & Demo Video\n- Try it on [MetaGPT Huggingface Space](https://huggingface.co/spaces/deepwisdom/MetaGPT-SoftwareCompany)\n- [Matthew Berman: How To Install MetaGPT - Build A Startup With One Prompt!!](https://youtu.be/uT75J_KG_aY)\n- [Official Demo Video](https://github.com/geekan/MetaGPT/assets/2707039/5e8c1062-8c35-440f-bb20-2b0320f8d27d)\n\nhttps://github.com/geekan/MetaGPT/assets/34952977/34345016-5d13-489d-b9f9-b82ace413419\n\n## Tutorial\n\n- 🗒 [Online Document](https://docs.deepwisdom.ai/main/en/)\n- 💻 [Usage](https://docs.deepwisdom.ai/main/en/guide/get_started/quickstart.html)  \n- 🔎 [What can MetaGPT do?](https://docs.deepwisdom.ai/main/en/guide/get_started/introduction.html)\n- 🛠 How to build your own agents? \n  - [MetaGPT Usage & Development Guide | Agent 101](https://docs.deepwisdom.ai/main/en/guide/tutorials/agent_101.html)\n  - [MetaGPT Usage & Development Guide | MultiAgent 101](https://docs.deepwisdom.ai/main/en/guide/tutorials/multi_agent_101.html)\n- 🧑‍💻 Contribution\n  - [Develop Roadmap](docs/ROADMAP.md)\n- 🔖 Use Cases\n  - [Data Interpreter](https://docs.deepwisdom.ai/main/en/guide/use_cases/agent/interpreter/intro.html)\n  - [Debate](https://docs.deepwisdom.ai/main/en/guide/use_cases/multi_agent/debate.html)\n  - [Researcher](https://docs.deepwisdom.ai/main/en/guide/use_cases/agent/researcher.html)\n  - [Receipt Assistant](https://docs.deepwisdom.ai/main/en/guide/use_cases/agent/receipt_assistant.html)\n- ❓ [FAQs](https://docs.deepwisdom.ai/main/en/guide/faq.html)\n\n## Support\n\n### Discord Join US\n\n📢 Join Our [Discord Channel](https://discord.gg/ZRHeExS6xv)! Looking forward to seeing you there! 🎉\n\n### Contributor form\n\n📝 [Fill out the form](https://airtable.com/appInfdG0eJ9J4NNL/pagK3Fh1sGclBvVkV/form) to become a contributor. We are looking forward to your participation!\n\n### Contact Information\n\nIf you have any questions or feedback about this project, please feel free to contact us. We highly appreciate your suggestions!\n\n- **Email:** alexanderwu@deepwisdom.ai\n- **GitHub Issues:** For more technical inquiries, you can also create a new issue in our [GitHub repository](https://github.com/geekan/metagpt/issues).\n\nWe will respond to all questions within 2-3 business days.\n\n## Citation\n\nTo stay updated with the latest research and development, follow [@MetaGPT_](https://twitter.com/MetaGPT_) on Twitter. \n\nTo cite [MetaGPT](https://openreview.net/forum?id=VtmBAGCN7o) or [Data Interpreter](https://arxiv.org/abs/2402.18679) in publications, please use the following BibTeX entries.\n\n```bibtex\n@inproceedings{hong2024metagpt,\n      title={Meta{GPT}: Meta Programming for A Multi-Agent Collaborative Framework},\n      author={Sirui Hong and Mingchen Zhuge and Jonathan Chen and Xiawu Zheng and Yuheng Cheng and Jinlin Wang and Ceyao Zhang and Zili Wang and Steven Ka Shing Yau and Zijuan Lin and Liyang Zhou and Chenyu Ran and Lingfeng Xiao and Chenglin Wu and J{\\\"u}rgen Schmidhuber},\n      booktitle={The Twelfth International Conference on Learning Representations},\n      year={2024},\n      url={https://openreview.net/forum?id=VtmBAGCN7o}\n}\n@misc{teng2025atom,\n      title={Atom of Thoughts for Markov LLM Test-Time Scaling}, \n      author={Fengwei Teng and Zhaoyang Yu and Quan Shi and Jiayi Zhang and Chenglin Wu and Yuyu Luo},\n      year={2025},\n      eprint={2502.12018},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2502.12018}, \n}\n@misc{xiang2025self,\n      title={Self-Supervised Prompt Optimization}, \n      author={Jinyu Xiang and Jiayi Zhang and Zhaoyang Yu and Fengwei Teng and Jinhao Tu and Xinbing Liang and Sirui Hong and Chenglin Wu and Yuyu Luo},\n      year={2025},\n      eprint={2502.06855},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2502.06855}, \n}\n@inproceedings{wang2025fact,\n      title={FACT: Examining the Effectiveness of Iterative Context Rewriting for Multi-fact Retrieval}, \n      author={Jinlin Wang and Suyuchen Wang and Ziwen Xia and Sirui Hong and Yun Zhu and Bang Liu and Chenglin Wu},\n      booktitle={The 2025 Annual Conference of the Nations of the Americas Chapter of the ACL},\n      year={2025},\n      url={https://openreview.net/forum?id=VXOircx5h3}\n}\n@misc{chi2024sela,\n      title={SELA: Tree-Search Enhanced LLM Agents for Automated Machine Learning}, \n      author={Yizhou Chi and Yizhang Lin and Sirui Hong and Duyi Pan and Yaying Fei and Guanghao Mei and Bangbang Liu and Tianqi Pang and Jacky Kwok and Ceyao Zhang and Bang Liu and Chenglin Wu},\n      year={2024},\n      eprint={2410.17238},\n      archivePrefix={arXiv},\n      primaryClass={cs.AI},\n      url={https://arxiv.org/abs/2410.17238}, \n}\n@inproceedings{zhang2025aflow,\n      title={{AF}low: Automating Agentic Workflow Generation},\n      author={Jiayi Zhang and Jinyu Xiang and Zhaoyang Yu and Fengwei Teng and Xiong-Hui Chen and Jiaqi Chen and Mingchen Zhuge and Xin Cheng and Sirui Hong and Jinlin Wang and Bingnan Zheng and Bang Liu and Yuyu Luo and Chenglin Wu},\n      booktitle={The Thirteenth International Conference on Learning Representations},\n      year={2025},\n      url={https://openreview.net/forum?id=z5uVAKwmjf}\n}\n@misc{hong2024data,\n      title={Data Interpreter: An LLM Agent For Data Science}, \n      author={Sirui Hong and Yizhang Lin and Bang Liu and Bangbang Liu and Binhao Wu and Danyang Li and Jiaqi Chen and Jiayi Zhang and Jinlin Wang and Li Zhang and Lingyao Zhang and Min Yang and Mingchen Zhuge and Taicheng Guo and Tuo Zhou and Wei Tao and Wenyi Wang and Xiangru Tang and Xiangtao Lu and Xiawu Zheng and Xinbing Liang and Yaying Fei and Yuheng Cheng and Zongze Xu and Chenglin Wu},\n      year={2024},\n      eprint={2402.18679},\n      archivePrefix={arXiv},\n      primaryClass={cs.AI},\n      url={https://arxiv.org/abs/2402.18679}, \n}\n```\n"
    },
    {
      "name": "motern88/MetaGPT_Fork",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/171646543?s=40&v=4",
      "owner": "motern88",
      "repo_name": "MetaGPT_Fork",
      "description": "fork from MetaGPT",
      "homepage": null,
      "language": "Python",
      "created_at": "2025-03-12T02:08:13Z",
      "updated_at": "2025-04-17T11:32:35Z",
      "topics": [],
      "readme": "\n# MetaGPT: The Multi-Agent Framework\n\n<p align=\"center\">\n<a href=\"\"><img src=\"docs/resources/MetaGPT-new-log.png\" alt=\"MetaGPT logo: Enable GPT to work in a software company, collaborating to tackle more complex tasks.\" width=\"150px\"></a>\n</p>\n\n<p align=\"center\">\n[ <b>En</b> |\n<a href=\"docs/README_CN.md\">中</a> |\n<a href=\"docs/README_FR.md\">Fr</a> |\n<a href=\"docs/README_JA.md\">日</a> ]\n<b>Assign different roles to GPTs to form a collaborative entity for complex tasks.</b>\n</p>\n\n<p align=\"center\">\n<a href=\"https://opensource.org/licenses/MIT\"><img src=\"https://img.shields.io/badge/License-MIT-blue.svg\" alt=\"License: MIT\"></a>\n<a href=\"https://discord.gg/DYn29wFk9z\"><img src=\"https://dcbadge.vercel.app/api/server/DYn29wFk9z?style=flat\" alt=\"Discord Follow\"></a>\n<a href=\"https://twitter.com/MetaGPT_\"><img src=\"https://img.shields.io/twitter/follow/MetaGPT?style=social\" alt=\"Twitter Follow\"></a>\n</p>\n\n<h4 align=\"center\">\n    \n</h4>\n\n## News\n\n🚀 Mar. 10, 2025: 🎉 [mgx.dev](https://mgx.dev/) is the #1 Product of the Week on @ProductHunt! 🏆\n\n🚀 Mar. &nbsp; 4, 2025: 🎉 [mgx.dev](https://mgx.dev/) is the #1 Product of the Day on @ProductHunt! 🏆\n\n🚀 Feb. 19, 2025: Today we are officially launching our natural language programming product: [MGX (MetaGPT X)](https://mgx.dev/) - the world's first AI agent development team. More details on [Twitter](https://x.com/MetaGPT_/status/1892199535130329356).\n\n🚀 Feb. 17, 2025: We introduced two papers: [SPO](https://arxiv.org/pdf/2502.06855) and [AOT](https://arxiv.org/pdf/2502.12018), check the [code](examples)!\n\n🚀 Jan. 22, 2025: Our paper [AFlow: Automating Agentic Workflow Generation](https://openreview.net/forum?id=z5uVAKwmjf) accepted for **oral presentation (top 1.8%)** at ICLR 2025, **ranking #2** in the LLM-based Agent category.\n\n👉👉 [Earlier news](docs/NEWS.md) \n\n## Software Company as Multi-Agent System\n\n1. MetaGPT takes a **one line requirement** as input and outputs **user stories / competitive analysis / requirements / data structures / APIs / documents, etc.**\n2. Internally, MetaGPT includes **product managers / architects / project managers / engineers.** It provides the entire process of a **software company along with carefully orchestrated SOPs.**\n   1. `Code = SOP(Team)` is the core philosophy. We materialize SOP and apply it to teams composed of LLMs.\n\n![A software company consists of LLM-based roles](docs/resources/software_company_cd.jpeg)\n\n<p align=\"center\">Software Company Multi-Agent Schematic (Gradually Implementing)</p>\n\n## Get Started\n\n### Installation\n\n> Ensure that Python 3.9 or later, but less than 3.12, is installed on your system. You can check this by using: `python --version`.  \n> You can use conda like this: `conda create -n metagpt python=3.9 && conda activate metagpt`\n\n```bash\npip install --upgrade metagpt\n# or `pip install --upgrade git+https://github.com/geekan/MetaGPT.git`\n# or `git clone https://github.com/geekan/MetaGPT && cd MetaGPT && pip install --upgrade -e .`\n```\n\n**Install [node](https://nodejs.org/en/download) and [pnpm](https://pnpm.io/installation#using-npm) before actual use.**\n\nFor detailed installation guidance, please refer to [cli_install](https://docs.deepwisdom.ai/main/en/guide/get_started/installation.html#install-stable-version)\n or [docker_install](https://docs.deepwisdom.ai/main/en/guide/get_started/installation.html#install-with-docker)\n\n### Configuration\n\nYou can init the config of MetaGPT by running the following command, or manually create `~/.metagpt/config2.yaml` file:\n```bash\n# Check https://docs.deepwisdom.ai/main/en/guide/get_started/configuration.html for more details\nmetagpt --init-config  # it will create ~/.metagpt/config2.yaml, just modify it to your needs\n```\n\nYou can configure `~/.metagpt/config2.yaml` according to the [example](https://github.com/geekan/MetaGPT/blob/main/config/config2.example.yaml) and [doc](https://docs.deepwisdom.ai/main/en/guide/get_started/configuration.html):\n\n```yaml\nllm:\n  api_type: \"openai\"  # or azure / ollama / groq etc. Check LLMType for more options\n  model: \"gpt-4-turbo\"  # or gpt-3.5-turbo\n  base_url: \"https://api.openai.com/v1\"  # or forward url / other llm url\n  api_key: \"YOUR_API_KEY\"\n```\n\n### Usage\n\nAfter installation, you can use MetaGPT at CLI\n\n```bash\nmetagpt \"Create a 2048 game\"  # this will create a repo in ./workspace\n```\n\nor use it as library\n\n```python\nfrom metagpt.software_company import generate_repo\nfrom metagpt.utils.project_repo import ProjectRepo\n\nrepo: ProjectRepo = generate_repo(\"Create a 2048 game\")  # or ProjectRepo(\"<path>\")\nprint(repo)  # it will print the repo structure with files\n```\n\nYou can also use [Data Interpreter](https://github.com/geekan/MetaGPT/tree/main/examples/di) to write code:\n\n```python\nimport asyncio\nfrom metagpt.roles.di.data_interpreter import DataInterpreter\n\nasync def main():\n    di = DataInterpreter()\n    await di.run(\"Run data analysis on sklearn Iris dataset, include a plot\")\n\nasyncio.run(main())  # or await main() in a jupyter notebook setting\n```\n\n\n### QuickStart & Demo Video\n- Try it on [MetaGPT Huggingface Space](https://huggingface.co/spaces/deepwisdom/MetaGPT-SoftwareCompany)\n- [Matthew Berman: How To Install MetaGPT - Build A Startup With One Prompt!!](https://youtu.be/uT75J_KG_aY)\n- [Official Demo Video](https://github.com/geekan/MetaGPT/assets/2707039/5e8c1062-8c35-440f-bb20-2b0320f8d27d)\n\nhttps://github.com/user-attachments/assets/888cb169-78c3-4a42-9d62-9d90ed3928c9\n\n## Tutorial\n\n- 🗒 [Online Document](https://docs.deepwisdom.ai/main/en/)\n- 💻 [Usage](https://docs.deepwisdom.ai/main/en/guide/get_started/quickstart.html)  \n- 🔎 [What can MetaGPT do?](https://docs.deepwisdom.ai/main/en/guide/get_started/introduction.html)\n- 🛠 How to build your own agents? \n  - [MetaGPT Usage & Development Guide | Agent 101](https://docs.deepwisdom.ai/main/en/guide/tutorials/agent_101.html)\n  - [MetaGPT Usage & Development Guide | MultiAgent 101](https://docs.deepwisdom.ai/main/en/guide/tutorials/multi_agent_101.html)\n- 🧑‍💻 Contribution\n  - [Develop Roadmap](docs/ROADMAP.md)\n- 🔖 Use Cases\n  - [Data Interpreter](https://docs.deepwisdom.ai/main/en/guide/use_cases/agent/interpreter/intro.html)\n  - [Debate](https://docs.deepwisdom.ai/main/en/guide/use_cases/multi_agent/debate.html)\n  - [Researcher](https://docs.deepwisdom.ai/main/en/guide/use_cases/agent/researcher.html)\n  - [Receipt Assistant](https://docs.deepwisdom.ai/main/en/guide/use_cases/agent/receipt_assistant.html)\n- ❓ [FAQs](https://docs.deepwisdom.ai/main/en/guide/faq.html)\n\n## Support\n\n### Discord Join US\n\n📢 Join Our [Discord Channel](https://discord.gg/ZRHeExS6xv)! Looking forward to seeing you there! 🎉\n\n### Contributor form\n\n📝 [Fill out the form](https://airtable.com/appInfdG0eJ9J4NNL/pagK3Fh1sGclBvVkV/form) to become a contributor. We are looking forward to your participation!\n\n### Contact Information\n\nIf you have any questions or feedback about this project, please feel free to contact us. We highly appreciate your suggestions!\n\n- **Email:** alexanderwu@deepwisdom.ai\n- **GitHub Issues:** For more technical inquiries, you can also create a new issue in our [GitHub repository](https://github.com/geekan/metagpt/issues).\n\nWe will respond to all questions within 2-3 business days.\n\n## Citation\n\nTo stay updated with the latest research and development, follow [@MetaGPT_](https://twitter.com/MetaGPT_) on Twitter. \n\nTo cite [MetaGPT](https://openreview.net/forum?id=VtmBAGCN7o) in publications, please use the following BibTeX entries.   \n\n```bibtex\n@inproceedings{hong2024metagpt,\n      title={Meta{GPT}: Meta Programming for A Multi-Agent Collaborative Framework},\n      author={Sirui Hong and Mingchen Zhuge and Jonathan Chen and Xiawu Zheng and Yuheng Cheng and Jinlin Wang and Ceyao Zhang and Zili Wang and Steven Ka Shing Yau and Zijuan Lin and Liyang Zhou and Chenyu Ran and Lingfeng Xiao and Chenglin Wu and J{\\\"u}rgen Schmidhuber},\n      booktitle={The Twelfth International Conference on Learning Representations},\n      year={2024},\n      url={https://openreview.net/forum?id=VtmBAGCN7o}\n}\n```\n\nFor more work, please refer to [Academic Work](docs/ACADEMIC_WORK.md).\n"
    },
    {
      "name": "thissideup124/autogenBP",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/160380090?s=40&v=4",
      "owner": "thissideup124",
      "repo_name": "autogenBP",
      "description": "A programming framework for agentic AI. Discord: https://aka.ms/autogen-dc. Roadmap: https://aka.ms/autogen-roadmap",
      "homepage": "https://microsoft.github.io/autogen/",
      "language": null,
      "created_at": "2024-08-11T19:51:47Z",
      "updated_at": "2024-08-12T15:36:02Z",
      "topics": [],
      "readme": "<a name=\"readme-top\"></a>\n\n[![PyPI version](https://badge.fury.io/py/pyautogen.svg)](https://badge.fury.io/py/pyautogen)\n[![Build](https://github.com/microsoft/autogen/actions/workflows/python-package.yml/badge.svg)](https://github.com/microsoft/autogen/actions/workflows/python-package.yml)\n![Python Version](https://img.shields.io/badge/3.8%20%7C%203.9%20%7C%203.10%20%7C%203.11%20%7C%203.12-blue)\n[![Downloads](https://static.pepy.tech/badge/pyautogen/week)](https://pepy.tech/project/pyautogen)\n[![Discord](https://img.shields.io/discord/1153072414184452236?logo=discord&style=flat)](https://aka.ms/autogen-dc)\n[![Twitter](https://img.shields.io/twitter/url/https/twitter.com/cloudposse.svg?style=social&label=Follow%20%40pyautogen)](https://twitter.com/pyautogen)\n\n[![NuGet version](https://badge.fury.io/nu/AutoGen.Core.svg)](https://badge.fury.io/nu/AutoGen.Core)\n\n# AutoGen\n[📚 Cite paper](#related-papers).\n<!-- <p align=\"center\">\n    <img src=\"https://github.com/microsoft/autogen/blob/main/website/static/img/flaml.svg\"  width=200>\n    <br>\n</p> -->\n:fire: May 29, 2024: DeepLearning.ai launched a new short course [AI Agentic Design Patterns with AutoGen](https://www.deeplearning.ai/short-courses/ai-agentic-design-patterns-with-autogen), made in collaboration with Microsoft and Penn State University, and taught by AutoGen creators [Chi Wang](https://github.com/sonichi) and [Qingyun Wu](https://github.com/qingyun-wu).\n\n:fire: May 24, 2024: Foundation Capital published an article on [Forbes: The Promise of Multi-Agent AI](https://www.forbes.com/sites/joannechen/2024/05/24/the-promise-of-multi-agent-ai/?sh=2c1e4f454d97) and a video [AI in the Real World Episode 2: Exploring Multi-Agent AI and AutoGen with Chi Wang](https://www.youtube.com/watch?v=RLwyXRVvlNk).\n\n:fire: May 13, 2024: [The Economist](https://www.economist.com/science-and-technology/2024/05/13/todays-ai-models-are-impressive-teams-of-them-will-be-formidable) published an article about multi-agent systems (MAS) following a January 2024 interview with [Chi Wang](https://github.com/sonichi).\n\n:fire: May 11, 2024: [AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation](https://openreview.net/pdf?id=uAjxFFing2) received the best paper award at the [ICLR 2024 LLM Agents Workshop](https://llmagents.github.io/).\n\n:fire: Apr 26, 2024: [AutoGen.NET](https://microsoft.github.io/autogen-for-net/) is available for .NET developers!\n\n:fire: Apr 17, 2024: Andrew Ng cited AutoGen in [The Batch newsletter](https://www.deeplearning.ai/the-batch/issue-245/) and [What's next for AI agentic workflows](https://youtu.be/sal78ACtGTc?si=JduUzN_1kDnMq0vF) at Sequoia Capital's AI Ascent (Mar 26).\n\n:fire: Mar 3, 2024: What's new in AutoGen? 📰[Blog](https://microsoft.github.io/autogen/blog/2024/03/03/AutoGen-Update); 📺[Youtube](https://www.youtube.com/watch?v=j_mtwQiaLGU).\n\n:fire: Mar 1, 2024: the first AutoGen multi-agent experiment on the challenging [GAIA](https://huggingface.co/spaces/gaia-benchmark/leaderboard) benchmark achieved the No. 1 accuracy in all the three levels.\n\n<!-- :tada: Jan 30, 2024: AutoGen is highlighted by Peter Lee in Microsoft Research Forum [Keynote](https://t.co/nUBSjPDjqD). -->\n\n:tada: Dec 31, 2023: [AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation Framework](https://arxiv.org/abs/2308.08155) is selected by [TheSequence: My Five Favorite AI Papers of 2023](https://thesequence.substack.com/p/my-five-favorite-ai-papers-of-2023).\n\n<!-- :fire: Nov 24: pyautogen [v0.2](https://github.com/microsoft/autogen/releases/tag/v0.2.0) is released with many updates and new features compared to v0.1.1. It switches to using openai-python v1. Please read the [migration guide](https://microsoft.github.io/autogen/docs/Installation#python). -->\n\n<!-- :fire: Nov 11: OpenAI's Assistants are available in AutoGen and interoperatable with other AutoGen agents! Checkout our [blogpost](https://microsoft.github.io/autogen/blog/2023/11/13/OAI-assistants) for details and examples. -->\n\n:tada: Nov 8, 2023: AutoGen is selected into [Open100: Top 100 Open Source achievements](https://www.benchcouncil.org/evaluation/opencs/annual.html) 35 days after spinoff from [FLAML](https://github.com/microsoft/FLAML).\n\n<!-- :tada: Nov 6, 2023: AutoGen is mentioned by Satya Nadella in a [fireside chat](https://youtu.be/0pLBvgYtv6U). -->\n\n<!-- :tada: Nov 1, 2023: AutoGen is the top trending repo on GitHub in October 2023. -->\n\n<!-- :tada: Oct 03, 2023: AutoGen spins off from [FLAML](https://github.com/microsoft/FLAML) on GitHub. -->\n\n<!-- :tada: Aug 16: Paper about AutoGen on [arxiv](https://arxiv.org/abs/2308.08155). -->\n\n:tada: Mar 29, 2023: AutoGen is first created in [FLAML](https://github.com/microsoft/FLAML).\n\n<!--\n:fire: FLAML is highlighted in OpenAI's [cookbook](https://github.com/openai/openai-cookbook#related-resources-from-around-the-web).\n\n:fire: [autogen](https://microsoft.github.io/autogen/) is released with support for ChatGPT and GPT-4, based on [Cost-Effective Hyperparameter Optimization for Large Language Model Generation Inference](https://arxiv.org/abs/2303.04673).\n\n:fire: FLAML supports Code-First AutoML & Tuning – Private Preview in [Microsoft Fabric Data Science](https://learn.microsoft.com/en-us/fabric/data-science/). -->\n\n<p align=\"right\" style=\"font-size: 14px; color: #555; margin-top: 20px;\">\n  <a href=\"#readme-top\" style=\"text-decoration: none; color: blue; font-weight: bold;\">\n    ↑ Back to Top ↑\n  </a>\n</p>\n\n## What is AutoGen\n\nAutoGen is an open-source programming framework for building AI agents and facilitating cooperation among multiple agents to solve tasks. AutoGen aims to streamline the development and research of agentic AI, much like PyTorch does for Deep Learning. It offers features such as agents capable of interacting with each other, facilitates the use of various large language models (LLMs) and tool use support, autonomous and human-in-the-loop workflows, and multi-agent conversation patterns.\n\n**Open Source Statement**: The project welcomes contributions from developers and organizations worldwide. Our goal is to foster a collaborative and inclusive community where diverse perspectives and expertise can drive innovation and enhance the project's capabilities. Whether you are an individual contributor or represent an organization, we invite you to join us in shaping the future of this project. Together, we can build something truly remarkable.\n\nThe project is currently maintained by a [dynamic group of volunteers](https://butternut-swordtail-8a5.notion.site/410675be605442d3ada9a42eb4dfef30?v=fa5d0a79fd3d4c0f9c112951b2831cbb&pvs=4) from several different organizations. Contact project administrators Chi Wang and Qingyun Wu via auto-gen@outlook.com if you are interested in becoming a maintainer.\n\n\n![AutoGen Overview](https://github.com/microsoft/autogen/blob/main/website/static/img/autogen_agentchat.png)\n\n- AutoGen enables building next-gen LLM applications based on [multi-agent conversations](https://microsoft.github.io/autogen/docs/Use-Cases/agent_chat) with minimal effort. It simplifies the orchestration, automation, and optimization of a complex LLM workflow. It maximizes the performance of LLM models and overcomes their weaknesses.\n- It supports [diverse conversation patterns](https://microsoft.github.io/autogen/docs/Use-Cases/agent_chat#supporting-diverse-conversation-patterns) for complex workflows. With customizable and conversable agents, developers can use AutoGen to build a wide range of conversation patterns concerning conversation autonomy,\n  the number of agents, and agent conversation topology.\n- It provides a collection of working systems with different complexities. These systems span a [wide range of applications](https://microsoft.github.io/autogen/docs/Use-Cases/agent_chat#diverse-applications-implemented-with-autogen) from various domains and complexities. This demonstrates how AutoGen can easily support diverse conversation patterns.\n- AutoGen provides [enhanced LLM inference](https://microsoft.github.io/autogen/docs/Use-Cases/enhanced_inference#api-unification). It offers utilities like API unification and caching, and advanced usage patterns, such as error handling, multi-config inference, context programming, etc.\n\nAutoGen is created out of collaborative [research](https://microsoft.github.io/autogen/docs/Research) from Microsoft, Penn State University, and the University of Washington.\n\n<p align=\"right\" style=\"font-size: 14px; color: #555; margin-top: 20px;\">\n  <a href=\"#readme-top\" style=\"text-decoration: none; color: blue; font-weight: bold;\">\n    ↑ Back to Top ↑\n  </a>\n</p>\n\n## Roadmaps\n\nTo see what we are working on and what we plan to work on, please check our\n[Roadmap Issues](https://aka.ms/autogen-roadmap).\n\n<p align=\"right\" style=\"font-size: 14px; color: #555; margin-top: 20px;\">\n  <a href=\"#readme-top\" style=\"text-decoration: none; color: blue; font-weight: bold;\">\n    ↑ Back to Top ↑\n  </a>\n</p>\n\n## Quickstart\nThe easiest way to start playing is\n1. Click below to use the GitHub Codespace\n\n    [![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://codespaces.new/microsoft/autogen?quickstart=1)\n\n 2. Copy OAI_CONFIG_LIST_sample to ./notebook folder, name to OAI_CONFIG_LIST, and set the correct configuration.\n 3. Start playing with the notebooks!\n\n*NOTE*: OAI_CONFIG_LIST_sample lists GPT-4 as the default model, as this represents our current recommendation, and is known to work well with AutoGen. If you use a model other than GPT-4, you may need to revise various system prompts (especially if using weaker models like GPT-3.5-turbo). Moreover, if you use models other than those hosted by OpenAI or Azure, you may incur additional risks related to alignment and safety. Proceed with caution if updating this default.\n\n<p align=\"right\" style=\"font-size: 14px; color: #555; margin-top: 20px;\">\n  <a href=\"#readme-top\" style=\"text-decoration: none; color: blue; font-weight: bold;\">\n    ↑ Back to Top ↑\n  </a>\n</p>\n\n## [Installation](https://microsoft.github.io/autogen/docs/Installation)\n### Option 1. Install and Run AutoGen in Docker\n\nFind detailed instructions for users [here](https://microsoft.github.io/autogen/docs/installation/Docker#step-1-install-docker), and for developers [here](https://microsoft.github.io/autogen/docs/Contribute#docker-for-development).\n\n### Option 2. Install AutoGen Locally\n\nAutoGen requires **Python version >= 3.8, < 3.13**. It can be installed from pip:\n\n```bash\npip install pyautogen\n```\n\nMinimal dependencies are installed without extra options. You can install extra options based on the feature you need.\n\n<!-- For example, use the following to install the dependencies needed by the [`blendsearch`](https://microsoft.github.io/FLAML/docs/Use-Cases/Tune-User-Defined-Function#blendsearch-economical-hyperparameter-optimization-with-blended-search-strategy) option.\n```bash\npip install \"pyautogen[blendsearch]\"\n``` -->\n\nFind more options in [Installation](https://microsoft.github.io/autogen/docs/Installation#option-2-install-autogen-locally-using-virtual-environment).\n\n<!-- Each of the [`notebook examples`](https://github.com/microsoft/autogen/tree/main/notebook) may require a specific option to be installed. -->\n\nEven if you are installing and running AutoGen locally outside of docker, the recommendation and default behavior of agents is to perform [code execution](https://microsoft.github.io/autogen/docs/FAQ/#code-execution) in docker. Find more instructions and how to change the default behaviour [here](https://microsoft.github.io/autogen/docs/Installation#code-execution-with-docker-(default)).\n\nFor LLM inference configurations, check the [FAQs](https://microsoft.github.io/autogen/docs/FAQ#set-your-api-endpoints).\n\n<p align=\"right\" style=\"font-size: 14px; color: #555; margin-top: 20px;\">\n  <a href=\"#readme-top\" style=\"text-decoration: none; color: blue; font-weight: bold;\">\n    ↑ Back to Top ↑\n  </a>\n</p>\n\n## Multi-Agent Conversation Framework\n\nAutogen enables the next-gen LLM applications with a generic [multi-agent conversation](https://microsoft.github.io/autogen/docs/Use-Cases/agent_chat) framework. It offers customizable and conversable agents that integrate LLMs, tools, and humans.\nBy automating chat among multiple capable agents, one can easily make them collectively perform tasks autonomously or with human feedback, including tasks that require using tools via code.\n\nFeatures of this use case include:\n\n- **Multi-agent conversations**: AutoGen agents can communicate with each other to solve tasks. This allows for more complex and sophisticated applications than would be possible with a single LLM.\n- **Customization**: AutoGen agents can be customized to meet the specific needs of an application. This includes the ability to choose the LLMs to use, the types of human input to allow, and the tools to employ.\n- **Human participation**: AutoGen seamlessly allows human participation. This means that humans can provide input and feedback to the agents as needed.\n\nFor [example](https://github.com/microsoft/autogen/blob/main/test/twoagent.py),\n\n```python\nfrom autogen import AssistantAgent, UserProxyAgent, config_list_from_json\n# Load LLM inference endpoints from an env variable or a file\n# See https://microsoft.github.io/autogen/docs/FAQ#set-your-api-endpoints\n# and OAI_CONFIG_LIST_sample\nconfig_list = config_list_from_json(env_or_file=\"OAI_CONFIG_LIST\")\n# You can also set config_list directly as a list, for example, config_list = [{'model': 'gpt-4', 'api_key': '<your OpenAI API key here>'},]\nassistant = AssistantAgent(\"assistant\", llm_config={\"config_list\": config_list})\nuser_proxy = UserProxyAgent(\"user_proxy\", code_execution_config={\"work_dir\": \"coding\", \"use_docker\": False}) # IMPORTANT: set to True to run code in docker, recommended\nuser_proxy.initiate_chat(assistant, message=\"Plot a chart of NVDA and TESLA stock price change YTD.\")\n# This initiates an automated chat between the two agents to solve the task\n```\n\nThis example can be run with\n\n```python\npython test/twoagent.py\n```\n\nAfter the repo is cloned.\nThe figure below shows an example conversation flow with AutoGen.\n![Agent Chat Example](https://github.com/microsoft/autogen/blob/main/website/static/img/chat_example.png)\n\nAlternatively, the [sample code](https://github.com/microsoft/autogen/blob/main/samples/simple_chat.py) here allows a user to chat with an AutoGen agent in ChatGPT style.\nPlease find more [code examples](https://microsoft.github.io/autogen/docs/Examples#automated-multi-agent-chat) for this feature.\n\n<p align=\"right\" style=\"font-size: 14px; color: #555; margin-top: 20px;\">\n  <a href=\"#readme-top\" style=\"text-decoration: none; color: blue; font-weight: bold;\">\n    ↑ Back to Top ↑\n  </a>\n</p>\n\n## Enhanced LLM Inferences\n\nAutogen also helps maximize the utility out of the expensive LLMs such as ChatGPT and GPT-4. It offers [enhanced LLM inference](https://microsoft.github.io/autogen/docs/Use-Cases/enhanced_inference#api-unification) with powerful functionalities like caching, error handling, multi-config inference and templating.\n\n<!-- For example, you can optimize generations by LLM with your own tuning data, success metrics, and budgets.\n\n```python\n# perform tuning for openai<1\nconfig, analysis = autogen.Completion.tune(\n    data=tune_data,\n    metric=\"success\",\n    mode=\"max\",\n    eval_func=eval_func,\n    inference_budget=0.05,\n    optimization_budget=3,\n    num_samples=-1,\n)\n# perform inference for a test instance\nresponse = autogen.Completion.create(context=test_instance, **config)\n```\n\nPlease find more [code examples](https://microsoft.github.io/autogen/docs/Examples#tune-gpt-models) for this feature. -->\n\n<p align=\"right\" style=\"font-size: 14px; color: #555; margin-top: 20px;\">\n  <a href=\"#readme-top\" style=\"text-decoration: none; color: blue; font-weight: bold;\">\n    ↑ Back to Top ↑\n  </a>\n</p>\n\n## Documentation\n\nYou can find detailed documentation about AutoGen [here](https://microsoft.github.io/autogen/).\n\nIn addition, you can find:\n\n- [Research](https://microsoft.github.io/autogen/docs/Research), [blogposts](https://microsoft.github.io/autogen/blog) around AutoGen, and [Transparency FAQs](https://github.com/microsoft/autogen/blob/main/TRANSPARENCY_FAQS.md)\n\n- [Discord](https://aka.ms/autogen-dc)\n\n- [Contributing guide](https://microsoft.github.io/autogen/docs/Contribute)\n\n- [Roadmap](https://github.com/orgs/microsoft/projects/989/views/3)\n\n<p align=\"right\" style=\"font-size: 14px; color: #555; margin-top: 20px;\">\n  <a href=\"#readme-top\" style=\"text-decoration: none; color: blue; font-weight: bold;\">\n    ↑ Back to Top ↑\n  </a>\n</p>\n\n## Related Papers\n\n[AutoGen](https://arxiv.org/abs/2308.08155)\n\n```\n@inproceedings{wu2023autogen,\n      title={AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation Framework},\n      author={Qingyun Wu and Gagan Bansal and Jieyu Zhang and Yiran Wu and Beibin Li and Erkang Zhu and Li Jiang and Xiaoyun Zhang and Shaokun Zhang and Jiale Liu and Ahmed Hassan Awadallah and Ryen W White and Doug Burger and Chi Wang},\n      year={2023},\n      eprint={2308.08155},\n      archivePrefix={arXiv},\n      primaryClass={cs.AI}\n}\n```\n\n[EcoOptiGen](https://arxiv.org/abs/2303.04673)\n\n```\n@inproceedings{wang2023EcoOptiGen,\n    title={Cost-Effective Hyperparameter Optimization for Large Language Model Generation Inference},\n    author={Chi Wang and Susan Xueqing Liu and Ahmed H. Awadallah},\n    year={2023},\n    booktitle={AutoML'23},\n}\n```\n\n[MathChat](https://arxiv.org/abs/2306.01337)\n\n```\n@inproceedings{wu2023empirical,\n    title={An Empirical Study on Challenging Math Problem Solving with GPT-4},\n    author={Yiran Wu and Feiran Jia and Shaokun Zhang and Hangyu Li and Erkang Zhu and Yue Wang and Yin Tat Lee and Richard Peng and Qingyun Wu and Chi Wang},\n    year={2023},\n    booktitle={ArXiv preprint arXiv:2306.01337},\n}\n```\n\n[AgentOptimizer](https://arxiv.org/pdf/2402.11359)\n\n```\n@article{zhang2024training,\n  title={Training Language Model Agents without Modifying Language Models},\n  author={Zhang, Shaokun and Zhang, Jieyu and Liu, Jiale and Song, Linxin and Wang, Chi and Krishna, Ranjay and Wu, Qingyun},\n  journal={ICML'24},\n  year={2024}\n}\n```\n\n[StateFlow](https://arxiv.org/abs/2403.11322)\n```\n@article{wu2024stateflow,\n  title={StateFlow: Enhancing LLM Task-Solving through State-Driven Workflows},\n  author={Wu, Yiran and Yue, Tianwei and Zhang, Shaokun and Wang, Chi and Wu, Qingyun},\n  journal={arXiv preprint arXiv:2403.11322},\n  year={2024}\n}\n```\n\n<p align=\"right\" style=\"font-size: 14px; color: #555; margin-top: 20px;\">\n  <a href=\"#readme-top\" style=\"text-decoration: none; color: blue; font-weight: bold;\">\n    ↑ Back to Top ↑\n  </a>\n</p>\n\n## Contributing\n\nThis project welcomes contributions and suggestions. Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit <https://cla.opensource.microsoft.com>.\n\nIf you are new to GitHub, [here](https://opensource.guide/how-to-contribute/#how-to-submit-a-contribution) is a detailed help source on getting involved with development on GitHub.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information, see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n<p align=\"right\" style=\"font-size: 14px; color: #555; margin-top: 20px;\">\n  <a href=\"#readme-top\" style=\"text-decoration: none; color: blue; font-weight: bold;\">\n    ↑ Back to Top ↑\n  </a>\n</p>\n\n## Contributors Wall\n<a href=\"https://github.com/microsoft/autogen/graphs/contributors\">\n  <img src=\"https://contrib.rocks/image?repo=microsoft/autogen&max=204\" />\n</a>\n\n<p align=\"right\" style=\"font-size: 14px; color: #555; margin-top: 20px;\">\n  <a href=\"#readme-top\" style=\"text-decoration: none; color: blue; font-weight: bold;\">\n    ↑ Back to Top ↑\n  </a>\n</p>\n\n# Legal Notices\n\nMicrosoft and any contributors grant you a license to the Microsoft documentation and other content\nin this repository under the [Creative Commons Attribution 4.0 International Public License](https://creativecommons.org/licenses/by/4.0/legalcode),\nsee the [LICENSE](LICENSE) file, and grant you a license to any code in the repository under the [MIT License](https://opensource.org/licenses/MIT), see the\n[LICENSE-CODE](LICENSE-CODE) file.\n\nMicrosoft, Windows, Microsoft Azure, and/or other Microsoft products and services referenced in the documentation\nmay be either trademarks or registered trademarks of Microsoft in the United States and/or other countries.\nThe licenses for this project do not grant you rights to use any Microsoft names, logos, or trademarks.\nMicrosoft's general trademark guidelines can be found at http://go.microsoft.com/fwlink/?LinkID=254653.\n\nPrivacy information can be found at https://privacy.microsoft.com/en-us/\n\nMicrosoft and any contributors reserve all other rights, whether under their respective copyrights, patents,\nor trademarks, whether by implication, estoppel, or otherwise.\n\n<p align=\"right\" style=\"font-size: 14px; color: #555; margin-top: 20px;\">\n  <a href=\"#readme-top\" style=\"text-decoration: none; color: blue; font-weight: bold;\">\n    ↑ Back to Top ↑\n  </a>\n</p>\n"
    },
    {
      "name": "pablocast/gbbai-azure-ai-foundry-multi-agents",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/18387357?s=40&v=4",
      "owner": "pablocast",
      "repo_name": "gbbai-azure-ai-foundry-multi-agents",
      "description": "Getting started with Azure AI Foundry and Semantic Kernel for a MultiAgent application",
      "homepage": "",
      "language": "Bicep",
      "created_at": "2025-03-07T21:22:28Z",
      "updated_at": "2025-03-14T13:20:52Z",
      "topics": [],
      "readme": "# <img src=\"./utils/media/ai-foundry.jpg\" alt=\"Azure Foundry\" style=\"width:60px;height:60px\"/> Azure AI Foundry and Semantic Kernel for a MultiAgent application\n\nThis repository exemplifies the construction of a MultiAgent application using [Semantic Kernel](https://learn.microsoft.com/en-us/semantic-kernel/overview/) and [Azure AI Foundry](https://azure.microsoft.com/en-us/products/ai-foundry/?msockid=32ef16738a19687b378c03de8bc46942). \n\n## 🔧 1. Prerequisites\n\n+ [azd](https://learn.microsoft.com/azure/developer/azure-developer-cli/install-azd), used to deploy all Azure resources and assets used in this sample.\n\n+ [azure functions core tools](https://learn.microsoft.com/en-us/azure/azure-functions/functions-run-local?tabs=windows%2Cisolated-process%2Cnode-v4%2Cpython-v2%2Chttp-trigger%2Ccontainer-apps&pivots=programming-language-csharp)\n\n+ [PowerShell Core pwsh](https://github.com/PowerShell/powershell/releases) if using Windows\n\n+ [Python 3.11](https://www.python.org/downloads/release/python-3110/)\n\n## 🔧 2. Infrastructure Creation\n\nThis sample uses [`azd`](https://learn.microsoft.com/azure/developer/azure-developer-cli/) and a bicep template to deploy all Azure resources:\n\n1. Login to your Azure account: `azd auth login`\n\n2. Create an environment: `azd env new`\n\n3. Place documents for testing inside [data](./data/) folder \n\n4. Run `azd up`.\n\n   + Choose your Azure subscription.\n   + Enter a region for the resources.\n\n   The deployment creates multiple Azure resources and runs multiple jobs. It takes several minutes to complete. The deployment is complete when you get a command line notification stating \"SUCCESS: Your up workflow to provision and deploy to Azure completed.\"\n\n## 🔧 3. Architecture \n![Connect](utils/media/multi_agent_architecture.jpg)\n-  **Document Storage and Indexing (Offline):**\n   - Source documents (for example, Markdown or PDF files) are placed in an Azure Storage Account.\n   - An Azure AI Search indexer job ingests these documents and creates a searchable index in Azure AI Search.\n   - This indexing step happens on a scheduled or as‐needed basis, so it is considered “offline.”\n\n- **Semantic Kernel Multi Agent App:**\n   - At query time, the Semantic Kernel acts as the orchestrator.\n   - When a user asks, “What are the benefits of credit card X?” the Semantic Kernel can call multiple “AI Plugins” to enrich its response.\n   - AI Plugins might include:\n      - Azure AI Search: Looks up relevant content in the indexed documents.\n      - Azure Bing Search: Searches the public web for additional context.\n      - Azure AI Foundry: Provides LLM reasoning and summarization.\n\n- **Logging and Monitoring:**\n   - All interactions and plugin calls are tracked in Azure Application Insights for operational visibility and performance monitoring.\n\n- **Hosting and Deployment:**\n   - The final application can be hosted on Azure App Service, in containerized environments with Azure Container Apps.\n\n\n## 🚀 4. Run the Indexig Document\n- Place documents (.pdf or markdown) inside [data](data/) folder. \n\n- Execute the [1-create-index.ipynb](notebooks/1-create-index.ipynb) to create the indices that will be used by the Agents. \n\n- Each document inside the [data](data/) is mapped to a different index inside the same Azure Search resource, as below:\n\n![Connect](utils/media/ai-search.jpg)\n\n## 🚀 5. Running MultiAgent Chat - One conversation\n- In [main](main.py) , we create multiple agents and then orchestrate them using [Semantic Kernel](https://learn.microsoft.com/en-us/semantic-kernel/overview/) and single-Turn, so that a specific agent is designated to provide a response\n\n- To run the application and start a chat:\n```bash\npython main.py\n```\n\n## 🚀 6. Running MultiAgent Chat - Multiple conversations\n\n### Run the sample conversations\n```bash\npython scripts/run_conversations.py\n```\n   - You may edit the sample conversations [here](./config/user_inputs.py)\n\n### Observe the Logs from Azure AI Foundry Project Traces\n![Connect](utils/media/ai-traces.jpg)\n\n   - Traces provide full observability of all agent steps, including selecting the agent to answer (selection step in the image), executing the chosen agent’s tool (invoke_agent Score step in the image), and making the final decision on whether the user’s query was answered (termination step in the image).\n\n### Get the Total Tokens from Azure AI Foundry Project Monitoring\n![Connect](utils/media/ai-insights.jpg)\n\n   - This dashboard shows total token consumption for the sample conversations in [here](./config/user_inputs.py). This tokens include all calls to the Azure Open AI Service made by the multi agent chat. \n\n## 💣 7. Deleting Infrastructure\n\nYou can delete the infrastruture created before by using `azd down --purge`\n  \n"
    },
    {
      "name": "kanethuong/sample-semantic-kernel",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/80303444?s=40&v=4",
      "owner": "kanethuong",
      "repo_name": "sample-semantic-kernel",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2025-01-22T06:11:25Z",
      "updated_at": "2025-03-07T05:04:07Z",
      "topics": [],
      "readme": "This is a simple sample project to get to understand how to work with semantic kernel.\n"
    },
    {
      "name": "XinmiaoYan/ai-agents-for-beginners",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/83053884?s=40&v=4",
      "owner": "XinmiaoYan",
      "repo_name": "ai-agents-for-beginners",
      "description": null,
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2025-03-05T18:12:15Z",
      "updated_at": "2025-04-09T12:52:49Z",
      "topics": [],
      "readme": "# AI Agents for Beginners - A Course\n\n![Generative AI For Beginners](./images/repo-thumbnail.png)\n\n## 10 Lessons teaching everything you need to know to start building AI Agents\n\n[![GitHub license](https://img.shields.io/github/license/microsoft/ai-agents-for-beginners.svg)](https://github.com/microsoft/ai-agents-for-beginners/blob/master/LICENSE?WT.mc_id=academic-105485-koreyst)\n[![GitHub contributors](https://img.shields.io/github/contributors/microsoft/ai-agents-for-beginners.svg)](https://GitHub.com/microsoft/ai-agents-for-beginners/graphs/contributors/?WT.mc_id=academic-105485-koreyst)\n[![GitHub issues](https://img.shields.io/github/issues/microsoft/ai-agents-for-beginners.svg)](https://GitHub.com/microsoft/ai-agents-for-beginners/issues/?WT.mc_id=academic-105485-koreyst)\n[![GitHub pull-requests](https://img.shields.io/github/issues-pr/microsoft/ai-agents-for-beginners.svg)](https://GitHub.com/microsoft/ai-agents-for-beginners/pulls/?WT.mc_id=academic-105485-koreyst)\n[![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat-square)](http://makeapullrequest.com?WT.mc_id=academic-105485-koreyst)\n\n[![GitHub watchers](https://img.shields.io/github/watchers/microsoft/ai-agents-for-beginners.svg?style=social&label=Watch)](https://GitHub.com/microsoft/ai-agents-for-beginners/watchers/?WT.mc_id=academic-105485-koreyst)\n[![GitHub forks](https://img.shields.io/github/forks/microsoft/ai-agents-for-beginners.svg?style=social&label=Fork)](https://GitHub.com/microsoft/ai-agents-for-beginners/network/?WT.mc_id=academic-105485-koreyst)\n[![GitHub stars](https://img.shields.io/github/stars/microsoft/ai-agents-for-beginners.svg?style=social&label=Star)](https://GitHub.com/microsoft/ai-agents-for-beginners/stargazers/?WT.mc_id=academic-105485-koreyst)\n\n[![Azure AI Discord](https://dcbadge.limes.pink/api/server/kzRShWzttr)](https://discord.gg/kzRShWzttr)\n\n\n## 🌱 Getting Started\n\nThis course has 10 lessons covering the fundamentals of building AI Agents. Each lesson covers its own topic so start wherever you like!\n\nThere is multi-language support for this course. Go to our [available languages here](#-multi-language-support). \n\nIf this is your first time building with Generative AI models, check out our [Generative AI For Beginners](https://aka.ms/genai-beginners) course, which includes 21 lessons on building with GenAI.\n\nDon't forget to [star (🌟) this repo](https://docs.github.com/en/get-started/exploring-projects-on-github/saving-repositories-with-stars?WT.mc_id=academic-105485-koreyst) and [fork this repo](https://github.com/microsoft/ai-agents-for-beginners/fork) to run the code.\n\n### What You Need \n\nEach lesson in this course includes code examples, which can be found in the code_samples folder. You can [fork this repo](https://github.com/microsoft/ai-agents-for-beginners/fork) to create your own copy.  \n\nThe code example in these exercise, utilise Azure AI Foundry and GitHub Model Catalogs for interacting with Language Models:\n\n- [Github Models](https://aka.ms/ai-agents-beginners/github-models) - Free / Limited\n- [Azure AI Foundry](https://aka.ms/ai-agents-beginners/ai-foundry) - Azure Account Required\n\nThis course also uses the following AI Agent frameworks and services from Microsoft:\n\n- [Azure AI Agent Service](https://aka.ms/ai-agents-beginners/ai-agent-service)\n- [Semantic Kernel](https://aka.ms/ai-agents-beginners/semantic-kernel)\n- [AutoGen](https://aka.ms/ai-agents/autogen)\n\nFor more information on running the code for this course, go to the [Course Setup](./00-course-setup/README.md).\n\n## 🙏 Want to help?\n\nDo you have suggestions or found spelling or code errors? [Raise an issue](https://github.com/microsoft/ai-agents-for-beginners/issues?WT.mc_id=academic-105485-koreyst) or [Create a pull request](https://github.com/microsoft/ai-agents-for-beginners/pulls?WT.mc_id=academic-105485-koreyst)\n\nIf you get stuck or have any questions about building AI Agents, join our [Azure AI Community Discord](https://discord.gg/kzRShWzttr).\n\n## 📂 Each lesson includes\n\n- A written lesson located in the README (Videos Coming March 2025)\n- Python code samples supporting Azure AI Foundry and Github Models (Free)\n- Links to extra resources to continue your learning\n\n\n## 🗃️ Lessons\n\n| **Lesson**                            | **Link**                                   | \n|----------------------------------------|--------------------------------------------|\n| Intro to AI Agents and Use Cases       | [Link](./01-intro-to-ai-agents/README.md)          |\n| Exploring Agentic Frameworks           | [Link](./02-explore-agentic-frameworks/README.md)  |\n| Understanding Agentic Design Patterns  | [Link](./03-agentic-design-patterns/README.md)  |\n| Tool Use Design Pattern                | [Link](./04-tool-use/README.md)                    |\n| Agentic RAG                            | [Link](./05-agentic-rag/README.md)                 |\n| Building Trustworty AI Agents          | [Link](./06-building-trustworthy-agents/README.md) |\n| Planning Design Pattern                | [Link](./07-planning-design/README.md)             |\n| Multi-Agent Design Pattern             | [Link](./08-multi-agent/README.md)                 |\n| Metacognition Design Pattern           | [Link](./09-metacognition/README.md)               |\n| AI Agents in Production                | [Link](./10-ai-agents-production/README.md)        |\n\n## 🌐 Multi-Language Support\n\n| Language             | Code | Link to Translated README                               | Last Updated |\n|----------------------|------|---------------------------------------------------------|--------------|\n| Chinese (Simplified) | zh   | [Chinese Translation](./translations/zh/README.md)      | 2025-02-13   |\n| Chinese (Traditional)| tw   | [Chinese Translation](./translations/tw/README.md)      | 2025-02-13   |\n| Chinese (Hong Kong)  | hk   | [Chinese (Hong Kong) Translation](./translations/hk/README.md) | 2025-02-13   |\n| French               | fr   | [French Translation](./translations/fr/README.md)       | 2025-02-13   |\n| Japanese             | ja   | [Japanese Translation](./translations/ja/README.md)     | 2025-02-13   |\n| Korean               | ko   | [Korean Translation](./translations/ko/README.md)       | 2025-02-13   |\n| Portuguese           | pt   | [Portuguese Translation](./translations/pt/README.md)   | 2025-02-13   |\n| Spanish              | es   | [Spanish Translation](./translations/es/README.md)      | 2025-02-13   |\n| German               | de   | [German Translation](./translations/de/README.md)       | 2025-02-13   |\n\n## 🎒 Other Courses\n\nOur team produces other courses! Check out:\n\n- [**NEW** Generative AI for Beginners using .NET](https://github.com/microsoft/Generative-AI-for-beginners-dotnet?WT.mc_id=academic-105485-koreyst)\n- [Generative AI for Beginners](https://github.com/microsoft/generative-ai-for-beginners?WT.mc_id=academic-105485-koreyst)\n- [ML for Beginners](https://aka.ms/ml-beginners?WT.mc_id=academic-105485-koreyst)\n- [Data Science for Beginners](https://aka.ms/datascience-beginners?WT.mc_id=academic-105485-koreyst)\n- [AI for Beginners](https://aka.ms/ai-beginners?WT.mc_id=academic-105485-koreyst)\n- [Cybersecurity for Beginners](https://github.com/microsoft/Security-101??WT.mc_id=academic-96948-sayoung)\n- [Web Dev for Beginners](https://aka.ms/webdev-beginners?WT.mc_id=academic-105485-koreyst)\n- [IoT for Beginners](https://aka.ms/iot-beginners?WT.mc_id=academic-105485-koreyst)\n- [XR Development for Beginners](https://github.com/microsoft/xr-development-for-beginners?WT.mc_id=academic-105485-koreyst)\n- [Mastering GitHub Copilot for AI Paired Programming](https://aka.ms/GitHubCopilotAI?WT.mc_id=academic-105485-koreyst)\n- [Mastering GitHub Copilot for C#/.NET Developers](https://github.com/microsoft/mastering-github-copilot-for-dotnet-csharp-developers?WT.mc_id=academic-105485-koreyst)\n- [Choose Your Own Copilot Adventure](https://github.com/microsoft/CopilotAdventures?WT.mc_id=academic-105485-koreyst)\n\n## 🌟 Community Thanks\n\nThanks to [Shivam Goyal](https://www.linkedin.com/in/shivam2003/) for contributing important code samples demonstrating Agentic RAG. \n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit <https://cla.opensource.microsoft.com>.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft\ntrademarks or logos is subject to and must follow\n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos is subject to those third-parties' policies.\n"
    },
    {
      "name": "onwardplatforms/agently",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/141597825?s=40&v=4",
      "owner": "onwardplatforms",
      "repo_name": "agently",
      "description": "A declarative framework for agent development using Semantic Kernel",
      "homepage": null,
      "language": "Python",
      "created_at": "2025-02-25T13:02:23Z",
      "updated_at": "2025-04-12T17:36:58Z",
      "topics": [],
      "readme": "# Agently - Declarative AI Agent Framework\n\nAgently is the batteries included framework for creating AI agents in a declarative way with a simple CLI tool to initialize and run agents. Define your agents using YAML configurations and bring them to life with minimal code.\n\n## Core Design Principles\n\n- **Declarative Configuration**: Define complete agents using simple YAML files\n- **Flexible Plugin Ecosystem**: Extend agent capabilities with MCP, Semantic Kernel, and Agently plugins\n- **Community Sharing**: Share and reuse plugins across the Agently community\n- **Provider Agnostic**: Support for multiple model providers including OpenAI, Ollama, and more\n- **Streamlined CLI**: Simple `init` and `run` commands to manage your agent lifecycle\n\n## Installation\n\n### Prerequisites\n- Python 3.8 or newer\n\n### Mac\n\n```bash\n# Create a virtual environment (optional but recommended)\npython3 -m venv venv\nsource venv/bin/activate\n\n# Install from PyPI\npip install agently\n\n# Or install from source\ngit clone https://github.com/onwardplatforms/agently.git\ncd agently\nmake install\n```\n\n### Windows\n\n```bash\n# Create a virtual environment (optional but recommended)\npython -m venv venv\nvenv\\Scripts\\activate\n\n# Install from PyPI\npip install agently\n\n# Or install from source\ngit clone https://github.com/onwardplatforms/agently.git\ncd agently\npython -m pip install -r requirements.txt\npython -m pip install -e .\n```\n\n### Standalone Executables\n\nDon't want to install Python? You can download pre-built executables:\n\n1. Go to [GitHub Releases](https://github.com/onwardplatforms/agently/releases)\n2. Download the executable for your platform (macOS, Linux, or Windows)\n3. Make it executable (Linux/macOS only): `chmod +x agently`\n4. Run it directly: `./agently` (Linux/macOS) or `agently.exe` (Windows)\n\n### Environment Setup\n\nCopy the example environment file and update with your API keys:\n\n```bash\n# Mac/Linux\ncp .env.example .env\n\n# Windows\ncopy .env.example .env\n```\n\nEdit the `.env` file to include your API keys:\n```\nOPENAI_API_KEY=your_key_here\n# Other API keys as needed\n```\n\n## Quick Start\n\n```bash\n# Create a simple agent configuration (agently.yaml)\ncat > agently.yaml << EOF\nversion: \"1\"\nname: \"Hello Agent\"\ndescription: \"A simple greeting agent\"\nsystem_prompt: \"You are a friendly assistant that helps with greetings.\"\nmodel:\n  provider: \"openai\"\n  model: \"gpt-4o\"\n  temperature: 0.7\nenv:\n  OPENAI_API_KEY: ${{ env.OPENAI_API_KEY }}\nEOF\n\n# Run the agent\nagently run\n```\n\n## Plugin System\n\nAgently features a unified plugin system that makes it easy to extend your agent's capabilities through various plugin types.\n\n### Supported Plugin Types\n\n#### Semantic Kernel (SK) Plugins\n\nStandard plugins that provide function-based capabilities to your agent:\n\n```yaml\nplugins:\n  github:\n    - source: \"username/plugin-name\"  # Will use agently-plugin- prefix\n      version: \"main\"\n      variables:\n        key: \"value\"\n  local:\n    - source: \"./plugins/my-local-plugin\"\n      variables:\n        key: \"value\"\n```\n\n#### Multi-Command Protocol (MCP) Servers\n\nExternal servers that enable complex, stateful interactions:\n\n```yaml\nplugins:\n  github:\n    - source: \"username/mcp-server-name\"\n      type: \"mcp\"  # Identifies this as an MCP server\n      version: \"main\"\n      command: \"python\"  # How to start the server\n      args: [\"server.py\"]\n  local:\n    - source: \"./mcp-servers/my-local-server\"\n      type: \"mcp\"\n      command: \"python\"\n      args: [\"server.py\"]\n```\n\n### Developing Plugins\n\n#### Using the Agently SDK\n\nFor easier plugin development, we recommend using the [Agently SDK](https://github.com/onwardplatforms/agently-sdk), which provides base classes and utilities for creating plugins:\n\n```python\nfrom agently_sdk.plugins import Plugin, PluginVariable, agently_function\n\nclass MyPlugin(Plugin):\n    name = \"my_plugin\"\n    description = \"A useful description of what this plugin does\"\n    \n    # Define configurable variables for your plugin\n    api_key = PluginVariable(\n        description=\"API key for the service\",\n        sensitive=True  # Marks as sensitive info\n    )\n    \n    max_results = PluginVariable(\n        description=\"Maximum number of results to return\",\n        default=10,\n        type=int\n    )\n    \n    @agently_function\n    def my_function(self, param1: str, param2: int = 5) -> str:\n        \"\"\"Function description that will be used by the agent.\n        \n        Args:\n            param1: First parameter description\n            param2: Second parameter description\n            \n        Returns:\n            Description of the return value\n        \"\"\"\n        # Implementation using plugin variables\n        return f\"Processed {param1} with {self.max_results} results\"\n```\n\n#### Plugin Variables\n\nPlugins can define variables that:\n- Allow for runtime configuration\n- Can have default values, validation rules, and type constraints\n- Are set via the `variables` section in the agent config\n\n### Plugin Installation & Management\n\nPlugins are managed using a structured workflow similar to Terraform:\n\n```bash\n# Initialize and install all plugins defined in your config\nagently init\n\n# List all installed plugins\nagently list\n\n# Run your agent with the installed plugins\nagently run\n```\n\n### Plugin Storage\n\nPlugins are stored in the `.agently/plugins` directory, organized by type:\n- SK plugins: `.agently/plugins/sk/`\n- MCP servers: `.agently/plugins/mcp/`\n\nFor more advanced usage and detailed documentation, check out the [full plugin documentation](https://docs.agently.run/plugins).\n\n### Plugin Naming Conventions\n\nWhen creating plugins to share with the community, follow these naming conventions for GitHub repositories:\n\n- **Semantic Kernel Plugins**: Use the prefix `agently-plugin-`\n  - Example: `agently-plugin-weather` for a weather plugin\n\n- **MCP Servers**: Use the prefix `agently-mcp-`\n  - Example: `agently-mcp-database` for a database MCP server\n\nThese conventions help with discoverability and make it clear what type of plugin a repository contains.\n\n## Coder Agent Quick Start\n\nThe Coder Agent is a powerful AI coding assistant with Git-backed code editing capabilities:\n\n```bash\n# Install in development mode\npip install -e .\n\n# Navigate to the coder agent example\ncd examples/coder_agent\n\n# Run the coder agent\nagently run\n```\n\nWith the Coder Agent, you can:\n- Create and modify files\n- Search across codebases\n- Find references to symbols\n- Format and lint code\n- Track changes with Git-based version control\n\nExample interactions:\n```\n> create a fibonacci.py file\n> add types and Google style docstrings\n> search for all references to a function\n```\n\nFor more details, see the [Coder Agent documentation](examples/coder_agent/README.md).\n\n## CLI Commands\n\nAgently provides a convenient command-line interface for managing and interacting with your agents:\n\n### `agently run`\n\nRun an agent using its configuration file.\n\n```bash\n# Basic usage with default configuration file (agently.yaml)\nagently run\n\n# Specify a different configuration file\nagently run --agent path/to/config.yaml\n\n# Set log level\nagently run --log-level info\n```\n\nOptions:\n- `--agent, -a`: Path to agent configuration file (default: \"agently.yaml\")\n- `--log-level`: Set the logging level (options: none, debug, info, warning, error, critical)\n\n### `agently init`\n\nInitialize the agent and install required plugins based on configuration.\n\n```bash\n# Initialize using default configuration\nagently init\n\n# Force reinstallation of all plugins\nagently init --force\n\n# Suppress verbose output\nagently init --quiet\n```\n\nOptions:\n- `--agent, -a`: Path to agent configuration file (default: \"agently.yaml\")\n- `--force`: Force reinstallation of all plugins\n- `--quiet`: Reduce output verbosity\n- `--log-level`: Set the logging level\n\n### `agently list`\n\nList available plugins or configurations.\n\n```bash\n# List all installed plugins\nagently list\n```\n\n## Documentation\n\nFor full documentation, visit [docs.agently.run](https://docs.agently.run).\n\n## Examples\n\nCheck out the [examples](examples/) directory for complete working examples:\n\n- [Coder Agent](examples/coder_agent/README.md): A powerful AI coding assistant with Git-backed changes\n- [Multi-Plugin Agent](examples/README.md): An agent using multiple plugin sources\n\n## Development\n\n### Mac\n\n```bash\n# Clone the repository\ngit clone https://github.com/onwardplatforms/agently.git\ncd agently\n\n# Set up development environment\nmake install-dev\n\n# Run tests\nmake test\n\n# Format code\nmake format\n\n# Run linters\nmake lint\n```\n\n### Windows\n\n```bash\n# Clone the repository\ngit clone https://github.com/onwardplatforms/agently.git\ncd agently\n\n# Set up development environment\npython -m pip install -r requirements.txt\npython -m pip install -r requirements-dev.txt\npython -m pip install -e .\npre-commit install\n\n# Run tests\npython -m pytest tests/ -v --cov=. --cov-report=term-missing\n\n# Format code\npython -m black agently\npython -m isort agently\n\n# Run linters\npython -m flake8 agently\n```\n\n## Creating Your Own Agent\n\n1. **Create a configuration file**\n\n   Create an `agently.yaml` file with your agent's configuration:\n\n   ```yaml\n   version: \"1\"\n   name: \"My Custom Agent\"\n   description: \"An agent that performs specific tasks\"\n   system_prompt: |\n     You are a specialized assistant that helps with [YOUR SPECIFIC TASK].\n     Please provide helpful, accurate, and concise responses.\n   \n   model:\n     provider: \"openai\"\n     model: \"gpt-4o\"  # or another model of your choice\n     temperature: 0.7\n   \n   plugins:\n     github:\n       - source: \"username/plugin-name\"\n         version: \"main\"\n         variables:\n           api_key: ${{ env.SERVICE_API_KEY }}\n     local:\n       - source: \"./plugins/my-local-plugin\"\n         variables:\n           max_results: 20\n   \n   env:\n     OPENAI_API_KEY: ${{ env.OPENAI_API_KEY }}\n     # Add other environment variables as needed\n   ```\n\n2. **Initialize your agent**\n\n   ```bash\n   agently init\n   ```\n\n3. **Run your agent**\n\n   ```bash\n   agently run\n   ```\n\n## Troubleshooting\n\n### Mac\n- If you encounter permission issues: `sudo pip install agently`\n- For M1/M2/M3 Macs, you may need to install Rosetta 2: `softwareupdate --install-rosetta`\n- For standalone executable: If you get \"app is damaged\" warnings, run: `xattr -d com.apple.quarantine ./agently`\n\n### Windows\n- If you see \"Command not found\" errors, ensure Python is in your PATH or use `python -m` prefix (e.g., `python -m pip`)\n- If you get DLL load errors, try installing the [Visual C++ Redistributable](https://learn.microsoft.com/en-us/cpp/windows/latest-supported-vc-redist)\n- For standalone executable: Right-click and select \"Run as Administrator\" if getting permission errors\n\n### Linux\n- For standalone executable: Make sure the file is executable: `chmod +x agently`\n- If you get \"command not found\" with the executable, try: `./agently` instead of just `agently`\n\n## License\n\nMIT\n\n## Contributing\n\nContributions are welcome! Here's how you can contribute to Agently:\n\n### Code Contributions\n\n1. Fork the repository\n2. Create a feature branch: `git checkout -b feature/your-feature-name`\n3. Commit your changes: `git commit -am 'Add some feature'`\n4. Push to the branch: `git push origin feature/your-feature-name`\n5. Submit a pull request\n\n### Creating Plugins\n\n1. Use the [Agently SDK](https://github.com/onwardplatforms/agently-sdk) for developing plugins\n2. Follow the naming conventions for GitHub repositories\n3. Add comprehensive documentation in your plugin's README\n4. Include example usage in your plugin's documentation\n\n### Documentation\n\nHelp improve our documentation by submitting PRs for:\n- Fixes for unclear instructions\n- Additional examples of using Agently\n- Tutorials for specific use cases\n\n### Bug Reports\n\nFound a bug? Please open an issue with:\n- A clear description of the problem\n- Steps to reproduce the issue\n- Expected vs actual behavior\n- Environment details (OS, Python version, etc.)\n\nFor more details, see [CONTRIBUTING.md](CONTRIBUTING.md).\n"
    },
    {
      "name": "Deepesh1024/Video_Analysis",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/75719377?s=40&v=4",
      "owner": "Deepesh1024",
      "repo_name": "Video_Analysis",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2025-02-08T11:22:01Z",
      "updated_at": "2025-03-02T11:46:07Z",
      "topics": [],
      "readme": "# Interview Grading Software\r\n\r\nThis Software is Made to Analyse candidate's video Resume, during Submission, this software uses various OpenCV Models, for Detectition of Physical Parameters like Posture, Smile, Eye Contact, and Energetic Start\r\n\r\n\r\nFollow the Guide Along in order to Run the Software\r\n\r\n\r\n\r\nThis guide provides steps to set up a Python environment using `venv` and install dependencies.\r\n\r\n## Prerequisites\r\n- Ensure Python is installed (recommended version: 3.8 or below 3.13).\r\n- Verify Python installation:\r\n  ```sh\r\n  python --version\r\n  ```\r\n  or\r\n  ```sh\r\n  python3 --version\r\n  ```\r\n\r\n## Creating a Virtual Environment\r\n1. Navigate to your project directory:\r\n   ```sh\r\n   cd /path/to/your/project\r\n   ```\r\n\r\n2. Create a virtual environment:\r\n   ```sh\r\n   python -m venv venv\r\n   ```\r\n   or (for Linux/macOS users)\r\n   ```sh\r\n   python3 -m venv venv\r\n   ```\r\n\r\n3. Activate the virtual environment:\r\n   - **Windows**:\r\n     ```sh\r\n     venv\\Scripts\\activate\r\n     ```\r\n   - **Linux/macOS**:\r\n     ```sh\r\n     source venv/bin/activate\r\n     ``` \r\n\r\n## Clone the Repository into your Project Directory \r\n  ```sh\r\n  git clone  \r\n  ``` \r\n## Go into the Project Directory \r\n```sh \r\ncd Video_Analysis\r\n```\r\n\r\n## Installing Dependencies\r\n- Install required packages from `requirements.txt`:\r\n  ```sh\r\n  pip install -r requirements.txt \r\n  ``` \r\n\r\n## Run the Streamlit App \r\n```sh \r\nstreamlit run Interview_Grader.py\r\n```\r\n\r\nAfter Opening the Streamlit Window you will see options to Upload the Video, upload your video in any format, wait for it to complete the processing, withing 10~20 secs you will get your PDF Report and you can Download it \r\n\r\n## Team \r\n1. Dhruv Gupta \r\n2. Samanway \r\n3. Ayush Mishra \r\n4. Deepesh Jha \r\n5. Anubhuti Anand \r\n#\u0000 \u0000S\u0000O\u0000M\u0000E\u0000_\u0000\r\u0000\n\u0000"
    },
    {
      "name": "mazer-rakham/copilot-intent-history-connector",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/11220261?s=40&v=4",
      "owner": "mazer-rakham",
      "repo_name": "copilot-intent-history-connector",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2025-02-21T14:52:28Z",
      "updated_at": "2025-03-17T13:19:12Z",
      "topics": [],
      "readme": "# Cosmos DB History POC Function App\n\nThis repository contains a serverless application built on Azure Functions to handle search history and inquiries into Cosmos DB. This README provides guidance on setting up and understanding the components involved in this project.\n\n## Table of Contents\n\n1. [Overview](#overview)\n2. [Prerequisites](#prerequisites)\n3. [Architecture](#architecture)\n4. [Functionality](#functionality)\n5. [Environment Variables](#environment-variables)\n6. [Usage](#usage)\n7. [Logging](#logging)\n8. [Contributing](#contributing)\n\n## Overview\n\nThis project aims to create a flexible consumption function app on Azure that interfaces with Azure Cosmos DB. The application analyzes query requests, searches the Cosmos DB, and returns structured results based on the input.\n\n## Prerequisites\n\n- Azure account with required permissions\n- Azure CLI installed\n- Python 3.7+ for local development and testing\n- [Visual Studio Code](https://code.visualstudio.com/) with Azure Functions Extension and Python Extension\n- Semantic Kernel SDK\n- OpenAI API access for using deployment and chat completion\n\n## Architecture\n\nThe application consists of several key components:\n- **Azure Function App**: Handles incoming requests, processes them, and interacts with Cosmos DB.\n- **Azure Cosmos DB**: Stores conversation history and other necessary data.\n- **Azure Storage Account**: Required for Azure Functions to store triggers and logs.\n- **Semantic Kernel SDK**: Used for processing and managing AI-driven operations.\n\n## Functionality\n\n### Key Features\n- **Search Query Handling**: Accepts search queries from clients, prepares them using Semantic Kernel, and searches Cosmos DB.\n- **Dynamic Configuration**: Uses environment variables for configuration, improving security and flexibility.\n- **Structured Results**: Converts output data into a JSON object format for consumption by downstream systems.\n\n### Request Structure\n- Endpoint: `/api/ai_search_history`\n- Method: `POST`\n- Sample Payload:\n  ```json\n  {\n    \"conversation_id\": \"sample-id\",\n    \"conversation\": \"Check presence of case number\",\n    \"index_to_search\": \"case-detail-index\",\n    \"fields\": \"case_vector\",\n    \"semanticConfiguration\": \"case-detail-semantic-configuration\"\n  }\n## Response Structure\n\nThe application returns results wrapped in a JSON object structure. The structure is as follows:\n\n    ```json\n    {\n      \"results\": [\n        // Array of result objects\n      ]\n    }\n\n## Environment Variables\nCertain environment variables need to be set in your environment to ensure the application functions correctly. These include:\n\n- **AZURE_SEARCH_BASE_URL:** The base URL for the Azure Search service endpoints.\n- **AZURE_SEARCH_API_VERSION:** The version of the Azure Search API to use.\n- **AZURE_OPENAI_DEPLOYMENT_NAME:** The name of the OpenAI deployment utilized in your app.\n- **AZURE_OPENAI_API_KEY:** The API key for accessing the OpenAI service.\n- **AZURE_OPENAI_BASE_URL:** The base URL for OpenAI API endpoints.\n- **COSMOS_DB_ENDPOINT:** The endpoint URL for your Azure Cosmos DB.\n- **COSMOS_DB_KEY:** The key for accessing your Cosmos DB instance.\n- **COSMOS_DB_DATABASE_NAME:** The name of the Cosmos DB database in use.\n- **COSMOS_DB_CONTAINER_NAME:** The name of the Cosmos DB container storing your data.\n- **AZURE_SEARCH_API_KEY:** The API key used for accessing Azure Search.\n\nEach of these should be configured in your deployment environment or within a .env file if you're working locally.\n# AI Search History API Custom Connector\n\nThis custom connector allows you to integrate the AI Search History API with Power Automate. The API processes conversation_id, conversation, index_to_search, fields, and semanticConfiguration from the request body to return AI search results.\n\n## Uploading the Swagger File to Create a Custom Connector\n\nTo create a custom connector in Power Automate using the provided Swagger file, follow these steps:\n\n1. **Sign in to Power Automate**:\n   - Go to [Power Automate](https://flow.microsoft.com) and sign in with your account.\n\n2. **Navigate to Custom Connectors**:\n   - In the left-hand navigation pane, select **Data** > **Custom connectors**.\n\n3. **Create a New Custom Connector**:\n   - Click on **+ New custom connector** and select **Import an OpenAPI file**.\n\n4. **Upload the Swagger File**:\n   - Provide a name for your custom connector.\n   - Click on **Import** and upload the `swagger.json` file from your local machine.\n\n5. **Configure the Connector**:\n   - Review the information imported from the Swagger file and make any necessary adjustments.\n   - Ensure that the host, base path, and schemes are correctly set.\n   - Click **Continue** to proceed through the configuration steps.\n\n6. **Set Up Security**:\n   - Configure the authentication type required by your API (e.g., API key, OAuth 2.0, etc.).\n   - Provide the necessary details for authentication.\n\n7. **Define the Actions and Triggers**:\n   - Review the actions and triggers defined in the Swagger file.\n   - Make any necessary adjustments to the parameters and responses.\n\n8. **Test the Connector**:\n   - Test the connector by providing sample data and verifying the responses.\n   - Ensure that the connector works as expected.\n\n9. **Create and Publish**:\n   - Once testing is complete, click **Create connector**.\n   - Publish the connector to make it available for use in your Power Automate flows.\n\n## What the Swagger File Does\n\nThe provided Swagger file defines the AI Search History API with the following endpoints:\n\n- **GET /ai_search_history**:\n  - Retrieves the details of the request received by the AI Search History endpoint.\n  - Response includes method, headers, params, and body.\n\n- **POST /ai_search_history**:\n  - Processes the request payload containing conversation_id, conversation, index_to_search, fields, and semanticConfiguration.\n  - Returns AI search results based on conversation history and inferred intent.\n  - Possible responses include:\n    - `200`: Successfully returns AI search results.\n    - `400`: Invalid request payload or missing parameters.\n    - `500`: Error processing request.\n\nThe API is designed to handle AI search history operations, providing detailed request information and processing search requests to return relevant results based on the provided parameters.\n\nBy following the steps above, you can create a custom connector in Power Automate that leverages the AI Search History API, enabling you to automate workflows that involve AI-driven search capabilities.\n\n## Usage\nAfter deploying the function, use tools like Postman or curl to interact with the function endpoint for testing. Ensure that the environment variables are properly configured for the Azure Function to access necessary resources.\n\n## Logging\nThe application logs critical operations:\n\n- Search queries being utilized.\n- Responses from Azure AI Search.\n\nThese logs can be accessed through Azure Monitor or any connected logging service.\n\n## Contributing\n- Fork the repository.\n- Create a feature branch.\n- Commit changes to your branch.\n- Open a pull request against the main branch.\n\nContributions are welcome! Please respect the coding guidelines and share an issue if you encounter any problems."
    },
    {
      "name": "ManufacturingCSU/AI_Agent_Catalog",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/83834200?s=40&v=4",
      "owner": "ManufacturingCSU",
      "repo_name": "AI_Agent_Catalog",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2025-02-13T15:31:48Z",
      "updated_at": "2025-03-04T14:17:45Z",
      "topics": [],
      "readme": "# AI_Agent_Catalog\n\nDo you have a need to extract images from files like Word, Excel, Powerpoint, or you would just like to have a good description of an image. If so check out the [image agent](docs/agents/image_extractor/README.md) where you can learn how to leverage this agent to assist you with that goal.  \n\n```markdown\n|⬜|⬛|⬜|⬛|⬜|⬛|⬜|⬛|\n|⬛|⬜|⬛|⬜|⬛|⬜|⬛|⬜|\n|⬜|⬛|⬜|⬛|⬜|⬛|⬜|⬛|\n|⬛|⬜|⬛|⬜|⬛|⬜|⬛|⬜|\n|⬜|⬛|⬜|⬛|⬜|⬛|⬜|⬛|\n|⬛|⬜|⬛|⬜|⬛|⬜|⬛|⬜|\n|⬜|⬛|⬜|⬛|⬜|⬛|⬜|⬛|\n|⬛|⬜|⬛|⬜|⬛|⬜|⬛|⬜|\n```\n\n```markdown\n\n&#x1F600\n\nUTF-16\n### Rook (♖)\nUTF-16 Code Point: `\\u2656`  \nSymbol: ♖\n\n### Knight (♘)\nUTF-16 Code Point: `\\u2658`  \nSymbol: ♘\n\n### Bishop (♗)\nUTF-16 Code Point: `\\u2657`  \nSymbol: ♗\n\n### Queen (♕)\nUTF-16 Code Point: `\\u2655`  \nSymbol: ♕\n\n### King (♔)\nUTF-16 Code Point: `\\u2654`  \nSymbol: ♔\n\n`1F600`\n\n```\n\n"
    },
    {
      "name": "pablocast/gbbai-sk-workshop",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/18387357?s=40&v=4",
      "owner": "pablocast",
      "repo_name": "gbbai-sk-workshop",
      "description": "Practical exercises for Semantic Kernel Workshop",
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2025-02-13T20:28:41Z",
      "updated_at": "2025-03-13T16:26:56Z",
      "topics": [],
      "readme": "# <img src=\"./instructions/media/semantic_kernel.jpg\" alt=\"Semantic Kernel Logo\" style=\"width:30px;height:30px;\"/> Semantic Kernel Worskhop\n\nBienvenido al repositorio del Taller de Semantic Kernel. Este repositorio contiene laboratorios y materiales de instrucción diseñados para ayudarte a comenzar con el kernel.\n\n## Requisitos previos\n\nAntes de comenzar el taller, asegúrate de tener lo siguiente:\n- [Python 3.11 or later version](https://www.python.org/) installed (recommended to use a separate python environment for this lab)\n- [VS Code](https://code.visualstudio.com/) installed with the [Jupyter notebook extension](https://marketplace.visualstudio.com/items?itemName=ms-toolsai.jupyter) enabled\n- [Azure CLI](https://learn.microsoft.com/cli/azure/install-azure-cli) installed\n- [An Azure Subscription](https://azure.microsoft.com/free/) with Contributor permissions\n- [Access granted to Azure OpenAI](https://aka.ms/oai/access) or just enable the mock service\n- [Sign in to Azure with Azure CLI](https://learn.microsoft.com/cli/azure/authenticate-azure-cli-interactively)\n- [Bot Emulator](https://learn.microsoft.com/en-us/azure/bot-service/bot-service-debug-emulator?view=azure-bot-service-4.0&tabs=csharp)\n\n## Instrucciones\n\n1. **Clona el repositorio**\n    - Utiliza Git para clonar el repositorio en tu máquina local.\n    - Ejemplo:\n      ```\n      https://github.com/pablocast/gbbai-sk-workshop.git\n      ```\n2. **Sigue las instrucciones del laboratorio**\n    - Las instrucciones detalladas del laboratorio se proporcionan en la carpeta [instructions](./instructions/)\n\n¡Feliz aprendizaje!\n"
    },
    {
      "name": "cyberkoolman/fsi-call-center",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/5167152?s=40&v=4",
      "owner": "cyberkoolman",
      "repo_name": "fsi-call-center",
      "description": null,
      "homepage": null,
      "language": "C#",
      "created_at": "2025-02-11T14:43:40Z",
      "updated_at": "2025-03-12T19:34:09Z",
      "topics": [],
      "readme": "# FSI Call Center\n\n![FSI Special Event](FSI-Azure-Event.png)\n\nWelcome to the FSI Special Event's PoC repo with the Contoso Call Center stories. This repository contains a suite of small apps designed to highlight and explain how one can utilize Azure AI Foundry's Whisper model, Azure Open AI's GPT models and embedding, Azure Function, Semantic Kernel .. etc to create an end-to-end applications to showcase transcription, prompt engineering, txt2sql, and integration of such.\n\n## Directory Structure\n\nThe repository is organized into the following directories:\n\n1. **RpCCAudioProcessApp**: Azure Function project to transcription for the audio file.  It is connecting to Azure AI Foundry's Fast transcription service.\n2. **RpCCTranscriptAnalyze**: Transcripted text to JSON, using simple Semantic Kernel implementation.\n3. **RpCCTranscriptAnalyze.Tools**: Supplementary tools for supplying today's date.  Highlight the need of supplying a tool and native function.\n4. **RpCCTransferJsonToDb**: Utilities for transferring JSON data to databases.\n5. **RpCCAnalyticsConsole**: Console application for Txt2SQL with Semantic Kernel.\n6. **RpCCAnalyticsChat**: Chat interface using Streamlit and separate to API.\n7. **RpCCKbRAG**: RAG application with the Semantic Kernel's Kernel Memory.\n8. **RpContactCenterApi**: Combined API for Txt2SQL and RAG sources.\n\nFor any questions or support, please let us know.\n"
    },
    {
      "name": "onwardplatforms/agent-foundry",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/141597825?s=40&v=4",
      "owner": "onwardplatforms",
      "repo_name": "agent-foundry",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2025-02-04T06:25:37Z",
      "updated_at": "2025-02-19T20:30:37Z",
      "topics": [],
      "readme": "# Agent Foundry\n\nA tool for creating and managing AI agents. Agent Foundry provides a simple CLI for creating, managing, and interacting with AI agents.\n\n## Features\n\n- 🔄 Real-time streaming responses\n- 🎯 Simple configuration-based agent creation\n- 🛠️ Multiple LLM provider support (OpenAI, Ollama)\n- 📝 Customizable system prompts\n- 🔌 Per-agent environment configuration\n- 🌐 Flexible environment variable handling\n\n## Quickstart\n\n```bash\n# Clone the repository\ngit clone https://github.com/onwardplatforms/agent-foundry.git\ncd agent-foundry\n\n# Install dependencies and set up development environment\nmake install-dev\n\n# Create and run your first agent\nodk agents add my-agent\nodk agents run my-agent\n```\n\n## Installation\n\n```bash\n# Clone the repository\ngit clone https://github.com/onwardplatforms/agent-foundry.git\ncd agent-foundry\n\n# Install in development mode with all dependencies\nmake install-dev\n\n# Or install in production mode\nmake install\n```\n\n## Development\n\n```bash\n# Install development dependencies\nmake install-dev\n\n# Format code\nmake format\n\n# Run linters\nmake lint\n\n# Run type checking\nmake check\n\n# Run tests\nmake test\n\n# Run all checks\nmake all\n\n# Clean up\nmake clean\n```\n\n## Configuration\n\nYou can configure environment variables at two levels:\n\n1. Agent-specific: Create a `.env` file in `.agents/<agent_id>/.env`\n2. Project-wide: Create a `.env` file in your project root\n\nEnvironment variables follow a simple waterfall pattern, where more specific settings override more general ones:\n1. Agent's `.env` file (highest priority)\n2. Agent's `config.json` values\n3. Global `.env` file\n4. Default values (lowest priority)\n\nExample `.env` files:\n\n```bash\n# Project-wide .env\nOPENAI_API_KEY=your_api_key_here\nOPENAI_MODEL=gpt-4  # Optional, defaults to gpt-3.5-turbo\n\n# Ollama Settings\nOLLAMA_BASE_URL=http://localhost:11434  # Optional\nOLLAMA_MODEL=llama2  # Optional\n```\n\n```bash\n# Agent-specific .env (.agents/my-agent/.env)\nOPENAI_MODEL=gpt-4  # Override model just for this agent\nOPENAI_API_KEY=agent-specific-key  # Use different API key for this agent\n```\n\n## Usage\n\n### Managing Agents\n\nThe CLI provides several commands for managing agents:\n\n```bash\n# List all agents\nodk agents list\n\n# List agents with detailed information\nodk agents list --verbose\n\n# Add a new agent\nodk agents add my-agent --provider openai --model gpt-4\nodk agents add llama-agent --provider ollama --model llama2\nodk agents add custom-agent --system-prompt \"You are a helpful coding assistant.\"\n\n# Run an agent\nodk agents run my-agent\nodk agents run my-agent --debug\n\n# Remove an agent\nodk agents remove my-agent\nodk agents remove my-agent --force  # Skip confirmation\n```\n\n## Project Structure\n\n```\nagent-foundry/\n├── agent_foundry/          # Main package\n│   ├── __init__.py\n│   ├── __main__.py        # Package entry point\n│   ├── agent.py           # Agent implementation\n│   ├── provider_impl.py   # Provider implementations\n│   ├── providers.py       # Provider definitions\n│   ├── env.py            # Environment handling\n│   ├── cli/              # CLI implementation\n│   │   ├── __init__.py\n│   │   └── cli.py\n│   └── constants.py       # Shared constants\n├── tests/                 # Test suite (90% coverage)\n├── .env                   # Global environment variables\n└── .agents/              # Agent storage directory\n    └── my-agent/         # Individual agent directory\n        ├── config.json   # Agent configuration\n        └── .env          # Agent-specific environment\n```\n\n## Agent Configuration\n\nEach agent is defined by a `config.json` file with the following structure:\n\n```json\n{\n  \"id\": \"my-agent\",\n  \"system_prompt\": \"You are a helpful AI assistant.\",\n  \"provider\": {\n    \"name\": \"openai\",\n    \"model\": \"gpt-4\",\n    \"settings\": {\n      \"temperature\": 0.7,\n      \"top_p\": 0.95,\n      \"max_tokens\": 1000\n    }\n  }\n}\n```\n\n### Provider Settings\n\n#### OpenAI\n```json\n{\n  \"provider\": {\n    \"name\": \"openai\",\n    \"model\": \"gpt-4\",  // Optional, falls back to AGENT_FOUNDRY_OPENAI_MODEL, OPENAI_MODEL, or gpt-3.5-turbo\n    \"settings\": {\n      \"temperature\": 0.7,    // Optional, default: 0.7\n      \"top_p\": 1.0,         // Optional, default: 1.0\n      \"max_tokens\": 1000    // Optional, default: 1000\n    }\n  }\n}\n```\n\n#### Ollama\n```json\n{\n  \"provider\": {\n    \"name\": \"ollama\",\n    \"model\": \"llama2\",  // Optional, falls back to AGENT_FOUNDRY_OLLAMA_MODEL, OLLAMA_MODEL, or llama2\n    \"settings\": {\n      \"temperature\": 0.7,      // Optional, default: 0.7\n      \"base_url\": \"http://localhost:11434\"  // Optional, falls back to AGENT_FOUNDRY_OLLAMA_BASE_URL, OLLAMA_BASE_URL, or default\n    }\n  }\n}\n```\n\n### Environment Variables\n\nSettings precedence order (highest to lowest):\n1. Agent-specific environment variables (AGENT_FOUNDRY_*)\n2. Global environment variables\n3. Configuration file values\n4. Default values\n\n## Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.\n\n## License\n\nThis project is licensed under the MIT License - see the LICENSE file for details.\n\n## Project Status\n\n✅ Core agent functionality\n✅ OpenAI provider implementation\n✅ Ollama provider implementation\n✅ Real-time streaming responses\n✅ Command-line interface\n✅ Per-agent environment variables\n✅ Test suite (90% coverage)\n✅ Robust environment variable handling\n⏳ Documentation site\n⏳ CI/CD pipeline\n\n## Future Plans\n\n- [ ] Add more provider implementations (Anthropic, etc.)\n- [ ] Add conversation history persistence\n- [ ] Add plugin system for custom capabilities\n- [ ] Add comprehensive documentation site\n- [ ] Add support for function calling\n- [ ] Add support for tool usage\n"
    },
    {
      "name": "saurabhvartak1982/skagents",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/48907314?s=40&v=4",
      "owner": "saurabhvartak1982",
      "repo_name": "skagents",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2025-02-03T15:44:23Z",
      "updated_at": "2025-03-02T04:41:33Z",
      "topics": [],
      "readme": "# skagents\n\nRunning the server.py: python server.py <br />\nSample invocation command: curl -X 'POST' 'http://localhost:8000/agent3' -H 'Content-Type: application/json' -d '{\"message\": \"Fortune favors the bold.\"}'"
    },
    {
      "name": "szetinglau/IFU-translation",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/146206163?s=40&v=4",
      "owner": "szetinglau",
      "repo_name": "IFU-translation",
      "description": "IFU translation using GenAI",
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2025-01-27T18:51:14Z",
      "updated_at": "2025-01-30T19:04:08Z",
      "topics": [],
      "readme": "# IFU-translation\nIFU translation using GenAI\n"
    },
    {
      "name": "Unity-Media-and-social-communication/MetaGPT",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/180525404?s=40&v=4",
      "owner": "Unity-Media-and-social-communication",
      "repo_name": "MetaGPT",
      "description": "🌟 The Multi-Agent Framework: First AI Software Company, Towards Natural Language Programming",
      "homepage": "https://unity-media-and-social-communication.github.io/MetaGPT/",
      "language": "Python",
      "created_at": "2025-01-22T17:44:56Z",
      "updated_at": "2025-01-22T17:58:43Z",
      "topics": [],
      "readme": "\n# MetaGPT: The Multi-Agent Framework\n\n<p align=\"center\">\n<a href=\"\"><img src=\"docs/resources/MetaGPT-new-log.png\" alt=\"MetaGPT logo: Enable GPT to work in a software company, collaborating to tackle more complex tasks.\" width=\"150px\"></a>\n</p>\n\n<p align=\"center\">\n<b>Assign different roles to GPTs to form a collaborative entity for complex tasks.</b>\n</p>\n\n<p align=\"center\">\n<a href=\"docs/README_CN.md\"><img src=\"https://img.shields.io/badge/文档-中文版-blue.svg\" alt=\"CN doc\"></a>\n<a href=\"README.md\"><img src=\"https://img.shields.io/badge/document-English-blue.svg\" alt=\"EN doc\"></a>\n<a href=\"docs/README_FR.md\"><img src=\"https://img.shields.io/badge/document-French-blue.svg\" alt=\"FR doc\"></a>\n<a href=\"docs/README_JA.md\"><img src=\"https://img.shields.io/badge/ドキュメント-日本語-blue.svg\" alt=\"JA doc\"></a>\n<a href=\"https://opensource.org/licenses/MIT\"><img src=\"https://img.shields.io/badge/License-MIT-blue.svg\" alt=\"License: MIT\"></a>\n<a href=\"docs/ROADMAP.md\"><img src=\"https://img.shields.io/badge/ROADMAP-路线图-blue\" alt=\"roadmap\"></a>\n<a href=\"https://discord.gg/DYn29wFk9z\"><img src=\"https://dcbadge.vercel.app/api/server/DYn29wFk9z?style=flat\" alt=\"Discord Follow\"></a>\n<a href=\"https://twitter.com/MetaGPT_\"><img src=\"https://img.shields.io/twitter/follow/MetaGPT?style=social\" alt=\"Twitter Follow\"></a>\n</p>\n\n<p align=\"center\">\n   <a href=\"https://vscode.dev/redirect?url=vscode://ms-vscode-remote.remote-containers/cloneInVolume?url=https://github.com/geekan/MetaGPT\"><img src=\"https://img.shields.io/static/v1?label=Dev%20Containers&message=Open&color=blue&logo=visualstudiocode\" alt=\"Open in Dev Containers\"></a>\n   <a href=\"https://codespaces.new/geekan/MetaGPT\"><img src=\"https://img.shields.io/badge/Github_Codespace-Open-blue?logo=github\" alt=\"Open in GitHub Codespaces\"></a>\n   <a href=\"https://huggingface.co/spaces/deepwisdom/MetaGPT-SoftwareCompany\" target=\"_blank\"><img alt=\"Hugging Face\" src=\"https://img.shields.io/badge/%F0%9F%A4%97%20-Hugging%20Face-blue?color=blue&logoColor=white\" /></a>\n</p>\n\n## News\n🚀 Oct. 29, 2024: We introduced three papers: [AFLOW](https://arxiv.org/abs/2410.10762), [FACT](https://arxiv.org/abs/2410.21012), and [SELA](https://arxiv.org/abs/2410.17238), check the [code](examples)!\n\n🚀 Mar. 29, 2024: [v0.8.0](https://github.com/geekan/MetaGPT/releases/tag/v0.8.0) released. Now you can use Data Interpreter ([arxiv](https://arxiv.org/abs/2402.18679), [example](https://docs.deepwisdom.ai/main/en/DataInterpreter/), [code](https://github.com/geekan/MetaGPT/tree/main/examples/di)) via pypi package import. Meanwhile, we integrated the RAG module and supported multiple new LLMs.\n\n🚀 Feb. 08, 2024: [v0.7.0](https://github.com/geekan/MetaGPT/releases/tag/v0.7.0) released, supporting assigning different LLMs to different Roles. We also introduced [Data Interpreter](https://github.com/geekan/MetaGPT/blob/main/examples/di/README.md), a powerful agent capable of solving a wide range of real-world problems.\n\n🚀 Jan. 16, 2024: Our paper [MetaGPT: Meta Programming for A Multi-Agent Collaborative Framework\n](https://openreview.net/forum?id=VtmBAGCN7o) accepted for **oral presentation (top 1.2%)** at ICLR 2024, **ranking #1** in the LLM-based Agent category.\n\n🚀 Jan. 03, 2024: [v0.6.0](https://github.com/geekan/MetaGPT/releases/tag/v0.6.0) released, new features include serialization, upgraded OpenAI package and supported multiple LLM, provided [minimal example for debate](https://github.com/geekan/MetaGPT/blob/main/examples/debate_simple.py) etc.\n\n🚀 Dec. 15, 2023: [v0.5.0](https://github.com/geekan/MetaGPT/releases/tag/v0.5.0) released, introducing some experimental features such as incremental development, multilingual, multiple programming languages, etc.\n\n🔥 Nov. 08, 2023: MetaGPT is selected into [Open100: Top 100 Open Source achievements](https://www.benchcouncil.org/evaluation/opencs/annual.html).\n\n🔥 Sep. 01, 2023: MetaGPT tops GitHub Trending Monthly for the **17th time** in August 2023.\n\n🌟 Jun. 30, 2023: MetaGPT is now open source.\n\n🌟 Apr. 24, 2023: First line of MetaGPT code committed.\n\n## Software Company as Multi-Agent System\n\n1. MetaGPT takes a **one line requirement** as input and outputs **user stories / competitive analysis / requirements / data structures / APIs / documents, etc.**\n2. Internally, MetaGPT includes **product managers / architects / project managers / engineers.** It provides the entire process of a **software company along with carefully orchestrated SOPs.**\n   1. `Code = SOP(Team)` is the core philosophy. We materialize SOP and apply it to teams composed of LLMs.\n\n![A software company consists of LLM-based roles](docs/resources/software_company_cd.jpeg)\n\n<p align=\"center\">Software Company Multi-Agent Schematic (Gradually Implementing)</p>\n\n## Get Started\n\n### Installation\n\n> Ensure that Python 3.9 or later, but less than 3.12, is installed on your system. You can check this by using: `python --version`.  \n> You can use conda like this: `conda create -n metagpt python=3.9 && conda activate metagpt`\n\n```bash\npip install --upgrade metagpt\n# or `pip install --upgrade git+https://github.com/geekan/MetaGPT.git`\n# or `git clone https://github.com/geekan/MetaGPT && cd MetaGPT && pip install --upgrade -e .`\n```\n\nFor detailed installation guidance, please refer to [cli_install](https://docs.deepwisdom.ai/main/en/guide/get_started/installation.html#install-stable-version)\n or [docker_install](https://docs.deepwisdom.ai/main/en/guide/get_started/installation.html#install-with-docker)\n\n### Configuration\n\nYou can init the config of MetaGPT by running the following command, or manually create `~/.metagpt/config2.yaml` file:\n```bash\n# Check https://docs.deepwisdom.ai/main/en/guide/get_started/configuration.html for more details\nmetagpt --init-config  # it will create ~/.metagpt/config2.yaml, just modify it to your needs\n```\n\nYou can configure `~/.metagpt/config2.yaml` according to the [example](https://github.com/geekan/MetaGPT/blob/main/config/config2.example.yaml) and [doc](https://docs.deepwisdom.ai/main/en/guide/get_started/configuration.html):\n\n```yaml\nllm:\n  api_type: \"openai\"  # or azure / ollama / groq etc. Check LLMType for more options\n  model: \"gpt-4-turbo\"  # or gpt-3.5-turbo\n  base_url: \"https://api.openai.com/v1\"  # or forward url / other llm url\n  api_key: \"YOUR_API_KEY\"\n```\n\n### Usage\n\nAfter installation, you can use MetaGPT at CLI\n\n```bash\nmetagpt \"Create a 2048 game\"  # this will create a repo in ./workspace\n```\n\nor use it as library\n\n```python\nfrom metagpt.software_company import generate_repo, ProjectRepo\nrepo: ProjectRepo = generate_repo(\"Create a 2048 game\")  # or ProjectRepo(\"<path>\")\nprint(repo)  # it will print the repo structure with files\n```\n\nYou can also use [Data Interpreter](https://github.com/geekan/MetaGPT/tree/main/examples/di) to write code:\n\n```python\nimport asyncio\nfrom metagpt.roles.di.data_interpreter import DataInterpreter\n\nasync def main():\n    di = DataInterpreter()\n    await di.run(\"Run data analysis on sklearn Iris dataset, include a plot\")\n\nasyncio.run(main())  # or await main() in a jupyter notebook setting\n```\n\n\n### QuickStart & Demo Video\n- Try it on [MetaGPT Huggingface Space](https://huggingface.co/spaces/deepwisdom/MetaGPT-SoftwareCompany)\n- [Matthew Berman: How To Install MetaGPT - Build A Startup With One Prompt!!](https://youtu.be/uT75J_KG_aY)\n- [Official Demo Video](https://github.com/geekan/MetaGPT/assets/2707039/5e8c1062-8c35-440f-bb20-2b0320f8d27d)\n\nhttps://github.com/geekan/MetaGPT/assets/34952977/34345016-5d13-489d-b9f9-b82ace413419\n\n## Tutorial\n\n- 🗒 [Online Document](https://docs.deepwisdom.ai/main/en/)\n- 💻 [Usage](https://docs.deepwisdom.ai/main/en/guide/get_started/quickstart.html)  \n- 🔎 [What can MetaGPT do?](https://docs.deepwisdom.ai/main/en/guide/get_started/introduction.html)\n- 🛠 How to build your own agents? \n  - [MetaGPT Usage & Development Guide | Agent 101](https://docs.deepwisdom.ai/main/en/guide/tutorials/agent_101.html)\n  - [MetaGPT Usage & Development Guide | MultiAgent 101](https://docs.deepwisdom.ai/main/en/guide/tutorials/multi_agent_101.html)\n- 🧑‍💻 Contribution\n  - [Develop Roadmap](docs/ROADMAP.md)\n- 🔖 Use Cases\n  - [Data Interpreter](https://docs.deepwisdom.ai/main/en/guide/use_cases/agent/interpreter/intro.html)\n  - [Debate](https://docs.deepwisdom.ai/main/en/guide/use_cases/multi_agent/debate.html)\n  - [Researcher](https://docs.deepwisdom.ai/main/en/guide/use_cases/agent/researcher.html)\n  - [Receipt Assistant](https://docs.deepwisdom.ai/main/en/guide/use_cases/agent/receipt_assistant.html)\n- ❓ [FAQs](https://docs.deepwisdom.ai/main/en/guide/faq.html)\n\n## Support\n\n### Discord Join US\n\n📢 Join Our [Discord Channel](https://discord.gg/ZRHeExS6xv)! Looking forward to seeing you there! 🎉\n\n### Contributor form\n\n📝 [Fill out the form](https://airtable.com/appInfdG0eJ9J4NNL/pagK3Fh1sGclBvVkV/form) to become a contributor. We are looking forward to your participation!\n\n### Contact Information\n\nIf you have any questions or feedback about this project, please feel free to contact us. We highly appreciate your suggestions!\n\n- **Email:** alexanderwu@deepwisdom.ai\n- **GitHub Issues:** For more technical inquiries, you can also create a new issue in our [GitHub repository](https://github.com/geekan/metagpt/issues).\n\nWe will respond to all questions within 2-3 business days.\n\n## Citation\n\nTo stay updated with the latest research and development, follow [@MetaGPT_](https://twitter.com/MetaGPT_) on Twitter. \n\nTo cite [MetaGPT](https://openreview.net/forum?id=VtmBAGCN7o) or [Data Interpreter](https://arxiv.org/abs/2402.18679) in publications, please use the following BibTeX entries.\n\n```bibtex\n@inproceedings{hong2024metagpt,\n      title={Meta{GPT}: Meta Programming for A Multi-Agent Collaborative Framework},\n      author={Sirui Hong and Mingchen Zhuge and Jonathan Chen and Xiawu Zheng and Yuheng Cheng and Jinlin Wang and Ceyao Zhang and Zili Wang and Steven Ka Shing Yau and Zijuan Lin and Liyang Zhou and Chenyu Ran and Lingfeng Xiao and Chenglin Wu and J{\\\"u}rgen Schmidhuber},\n      booktitle={The Twelfth International Conference on Learning Representations},\n      year={2024},\n      url={https://openreview.net/forum?id=VtmBAGCN7o}\n}\n@misc{hong2024data,\n      title={Data Interpreter: An LLM Agent For Data Science}, \n      author={Sirui Hong and Yizhang Lin and Bang Liu and Bangbang Liu and Binhao Wu and Danyang Li and Jiaqi Chen and Jiayi Zhang and Jinlin Wang and Li Zhang and Lingyao Zhang and Min Yang and Mingchen Zhuge and Taicheng Guo and Tuo Zhou and Wei Tao and Wenyi Wang and Xiangru Tang and Xiangtao Lu and Xiawu Zheng and Xinbing Liang and Yaying Fei and Yuheng Cheng and Zongze Xu and Chenglin Wu},\n      year={2024},\n      eprint={2402.18679},\n      archivePrefix={arXiv},\n      primaryClass={cs.AI}\n}\n@misc{zhang2024aflow,\n      title={AFlow: Automating Agentic Workflow Generation}, \n      author={Jiayi Zhang and Jinyu Xiang and Zhaoyang Yu and Fengwei Teng and Xionghui Chen and Jiaqi Chen and Mingchen Zhuge and Xin Cheng and Sirui Hong and Jinlin Wang and Bingnan Zheng and Bang Liu and Yuyu Luo and Chenglin Wu},\n      year={2024},\n      eprint={2410.10762},\n      archivePrefix={arXiv},\n      primaryClass={cs.AI},\n      url={https://arxiv.org/abs/2410.10762}, \n}\n```\n"
    },
    {
      "name": "JuhyunLee0/multiagent-sk",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/152219307?s=40&v=4",
      "owner": "JuhyunLee0",
      "repo_name": "multiagent-sk",
      "description": null,
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2025-01-21T22:59:14Z",
      "updated_at": "2025-02-10T21:39:12Z",
      "topics": [],
      "readme": ""
    },
    {
      "name": "nileshvj2/aoai-poc-hub",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/111443992?s=40&v=4",
      "owner": "nileshvj2",
      "repo_name": "aoai-poc-hub",
      "description": "Hub of all AOAI POCs I have done so far!",
      "homepage": "",
      "language": "Jupyter Notebook",
      "created_at": "2023-10-27T17:28:43Z",
      "updated_at": "2025-02-19T22:41:34Z",
      "topics": [],
      "readme": "This repo contains collection of notebooks which are proof of concepts I have developed using Azure AI services and other LLM models.\r\nEach notebook in this repo provides detailed code walkthrough of important concepts and will teach you how to build Gen AI applications with step by step explainations to understand important concepts.\r\nIntention to create this POC Hub is to share proof of concepts with anyone interested in creating AI solutions! \r\n\r\n* Prompt engineering techniques\r\n* Vectorization examples\r\n* Function Calling\r\n* Assistants API\r\n* Basic RAG \r\n* Using Orchestration frameworks - multi agents RAG\r\n* Building Smart Chat App on SQL data (NL to SQL)\r\n* Extracting key/values and datapoints from documents\r\n* Autogen examples - basic conversational multi agents, code exector and tool use patterns\r\n* Semantic Kernel - Agent examples showing plugins and agent conversations\r\n* RAG Techniques\r\n    * Naive RAG\r\n    * Standard/Advanced RAG\r\n    * MS Graph Rag\r\n\r\n\r\n# Set up Instructions\r\n\r\nSeparate environments created for each excercise. For example - e1 to e3 uses aoai_poc_e1, e1 uses aoai_poc_e4 etc. \r\nEach excercise has its own requirements.txt file. \r\nThis is to avoid any library or version conflicts and making sure each excercise works on its own independtly without any dependencies\r\n\r\nCommands to create environments\r\nConda create -p C:\\Users\\..\\git\\aoai-poc-hub\\common\\envs\\aoai_poc_e4 <specify path but cannot specify name here>  python=3.11\r\nConda activate <fullpath> or <env-name-here>\r\nConda deactivate <fullpath> or <env-name-here>\r\n\r\nInstalling packages using PIP\r\npip install -r ./common/requirements.txt  --File \r\nconda install pip  --individual pkg\r\nconda install ipykernel --individual pkg\r\n\r\n# Disclaimer\r\n\r\nThe examples and code snippets in this repository are provided for **illustration and demonstration purposes only**. They are not intended for production deployment. Also, these examples are my own code snippets I have created individually or from the existing code samples shared on product documentation/references mentioned. These should not be considered as references or opinions shared by my company or any other entity. \r\n\r\n\r\n\r\n**Use them at your own risk and responsibility.**\r\n\r\nIf you plan to use any of this code in a production environment, make sure to thoroughly review, test, and adapt it to meet your specific requirements. Always follow best practices and security guidelines.\r\n\r\nThe authors and contributors of this repository are not liable for any consequences resulting from the use of this code.\r\n\r\n\r\n"
    },
    {
      "name": "smaruf/python-study",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/10070242?s=40&v=4",
      "owner": "smaruf",
      "repo_name": "python-study",
      "description": "Some basic testing of python modules",
      "homepage": null,
      "language": "Python",
      "created_at": "2021-10-25T05:37:51Z",
      "updated_at": "2025-04-22T11:05:40Z",
      "topics": [],
      "readme": "# Python Study Repository\n\nThis repository contains a variety of Python modules and scripts for testing and learning purposes.\n\n## Topics Covered\n\n1. Simple Code Scripts\n2. Simple Server\n3. Simple Modules\n4. Data Engineering Modules\n5. Server Environment\n6. Google gRPC\n7. Proxy, Worker, Scheduler, Circuit Breakers, Gateway, etc.\n8. Monitoring\n9. Database Optimization\n10. Analytics\n11. Micro-controller Package\n12. Graph Database\n13. Caching: [LRU Cache in Python](https://realpython.com/lru-cache-python/)\n\n## Useful Links\n\n1. [PyTorch](https://pytorch.org/get-started/locally/)\n2. [Flask](https://flask.palletsprojects.com/en/2.0.x/quickstart)\n3. [Django](https://www.djangoproject.com/start) - [Django Tutorial](https://docs.djangoproject.com/en/3.2/intro/tutorial01/)\n4. [MicroPython](https://micropython.org/)\n5. [Py2neo](https://pypi.org/project/py2neo/) - [Py2neo Documentation](https://py2neo.org/2021.1/)\n6. [Neo4j](https://neo4j.com/)\n\n## English Learning\n\n1. [Learning English Flashcards App](https://github.com/smaruf/python-study/blob/main/src/learning-english/README.md)\n2. [Language Improvement Document](https://github.com/smaruf/python-study/blob/main/src/learning-english/language_improvement.md)\n\n## Contribution Guidelines\n\nFeel free to contribute to this repository by submitting issues or pull requests. Please follow the standard coding conventions and include appropriate documentation for any new features or changes.\n\n## License\n\nThis repository is licensed under the MIT License. See the [LICENSE](LICENSE) file for more details.\n"
    },
    {
      "name": "StevenWangler/Blizzard",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/28370918?s=40&v=4",
      "owner": "StevenWangler",
      "repo_name": "Blizzard",
      "description": "Blizzard is a multi-agent snow day prediction service.",
      "homepage": null,
      "language": "Python",
      "created_at": "2024-11-28T18:20:59Z",
      "updated_at": "2025-02-27T02:09:39Z",
      "topics": [],
      "readme": "# Snow Day Prediction Platform\n\nA multi-agent system that predicts snow days using weather data and simulates a decision-making process between school administrators. Features a modern web interface for viewing predictions and customizable snow day criteria.\n\n## Prerequisites\n\n- Python 3.8 or higher\n- OpenAI API key with GPT-4 access\n- WeatherAPI key (free tier works fine)\n- Git (for version control)\n\n## Features\n\n- Weather analysis using real-time data from WeatherAPI\n- Three specialized agents:\n  1. Weather Agent: Analyzes weather data\n  2. Superintendent: Makes initial snow day decisions with probability assessment\n  3. Vice Superintendent: Reviews and validates decisions\n- Natural conversation flow between agents\n- Automated decision-making process\n- Modern web interface for viewing predictions\n- Daily updates via GitHub Actions\n- Customizable snow day criteria via external file\n- School-specific theming support\n\n## Setup\n\n1. Clone this repository\n2. Install dependencies:\n   ```bash\n   pip install -r requirements.txt\n   ```\n3. Copy `.env.template` to `.env`:\n   ```bash\n   cp .env.template .env\n   ```\n4. Get your API keys:\n   - OpenAI API key from [OpenAI Platform](https://platform.openai.com/api-keys)\n   - Weather API key from [WeatherAPI](https://www.weatherapi.com/)\n5. Add your API keys and configuration to the `.env` file\n\n## Snow Day Criteria Configuration\n\nThe system uses a customizable criteria file to determine snow day conditions:\n\n1. Create a `snowday_criteria.txt` file in your preferred location\n2. Format each criterion on a new line\n3. Example criteria:\n   ```\n   Snow accumulation of 6 inches or more\n   Temperature below 15°F\n   Wind chill advisory in effect\n   Freezing rain forecast\n   ```\n4. The system will dynamically load these criteria during the decision process\n5. Superintendent will consider ALL criteria when making decisions\n6. Each decision includes a percentage probability of declaring a snow day\n\n### Criteria File Guidelines\n\n- Keep criteria clear and specific\n- One condition per line\n- Focus on measurable weather conditions\n- Include both weather and safety factors\n- File can be updated without code changes\n\n## Usage\n\nRun the snow day prediction system:\n```bash\npython main.py\n```\n\nThe agents will automatically:\n1. Fetch current weather data for your configured ZIP code\n2. Analyze weather conditions\n3. Make a snow day decision\n4. Review and validate the decision\n5. Reach a consensus\n6. Generate a data.json file with the results\n\n## Web Interface\n\nThe project includes a modern web interface for viewing predictions:\n\n- Located in the `static/` directory\n- Displays the final decision and agent conversation\n- Updates automatically when new predictions are made\n- Supports markdown formatting for agent messages\n- Customizable color scheme for school branding\n- Clear distinction between conversation participants\n\n### Local Development\n```bash\n# Using Python's built-in server\npython -m http.server --directory static\n\n# Or using any static file server\n```\n\n### GitHub Pages Deployment\n\nThe project is set up to automatically deploy to GitHub Pages using GitHub Actions. Here's how to set it up:\n\n1. In your repository settings:\n   - Go to \"Settings\" > \"Pages\"\n   - Under \"Build and deployment\", select \"GitHub Actions\" as the source\n\n2. Set up required secrets in your repository:\n   - Go to \"Settings\" > \"Secrets and variables\" > \"Actions\"\n   - Add these secrets:\n     - `OPENAI_API_KEY`: Your OpenAI API key\n     - `WEATHER_API_KEY`: Your WeatherAPI key\n     - `ZIP_CODE`: Your target ZIP code\n\n3. Set up model variables (optional):\n   - In the same section, go to the \"Variables\" tab\n   - Add these variables if you want to customize the models:\n     - `MODEL_NAME`\n     - `WEATHER_MODEL`\n     - `BLIZZARD_MODEL`\n     - `ASSISTANT_MODEL`\n     - `SELECTION_MODEL`\n     - `TERMINATION_MODEL`\n\n4. The workflow will:\n   - Run automatically at 8PM EST daily\n   - Update the weather data and predictions\n   - Deploy to GitHub Pages\n   - You can also trigger it manually from the Actions tab\n\nYour site will be available at: `https://[your-username].github.io/[repository-name]/`\n\n## Configuration\n\n- ZIP code is configurable in .env file\n- Weather data focuses on evening to morning period (7 PM - 8 AM)\n- Maximum conversation iterations is set to 10\n- Supports custom OpenAI models via MODEL_NAME in .env\n- Customizable snow day criteria via external file\n- Configurable UI theming in styles.css\n\n## Troubleshooting\n\n### Common Issues\n\n1. **OpenAI API Error**\n   - Ensure your API key is valid and has access to GPT-4\n   - Check your API quota and billing status\n   - Verify MODEL_NAME in .env matches available models\n\n2. **Weather API Error**\n   - Verify your API key is active\n   - Check if your ZIP code is valid\n   - Ensure you're within the API rate limits\n\n3. **Web Interface Not Updating**\n   - Verify data.json is being generated in the static directory\n   - Check file permissions\n   - Clear browser cache\n\n### Logs\n\nThe application logs important information to the console. For debugging:\n1. Run the script with default logging\n2. Check the console output for ERROR or WARNING messages\n3. Ensure all required environment variables are set\n\n## Contributing\n\n1. Fork the repository\n2. Create a feature branch\n3. Make your changes\n4. Submit a pull request\n\nPlease ensure your changes:\n- Include appropriate documentation\n- Maintain existing code style\n- Add necessary tests\n- Update README if needed\n\n## License\n\nThis project is licensed under the MIT License - see the LICENSE file for details.\n"
    },
    {
      "name": "dreamguo/metagpt-werewolf",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/33998583?s=40&v=4",
      "owner": "dreamguo",
      "repo_name": "metagpt-werewolf",
      "description": "AI-werewolf",
      "homepage": null,
      "language": "Python",
      "created_at": "2024-11-10T16:02:25Z",
      "updated_at": "2025-01-23T19:09:56Z",
      "topics": [],
      "readme": "# MetaGPT: The Multi-Agent Framework\n\n<p align=\"center\">\n<a href=\"\"><img src=\"docs/resources/MetaGPT-logo.jpeg\" alt=\"MetaGPT logo: Enable GPT to work in software company, collaborating to tackle more complex tasks.\" width=\"150px\"></a>\n</p>\n\n<p align=\"center\">\n<b>Assign different roles to GPTs to form a collaborative software entity for complex tasks.</b>\n</p>\n\n<p align=\"center\">\n<a href=\"docs/README_CN.md\"><img src=\"https://img.shields.io/badge/文档-中文版-blue.svg\" alt=\"CN doc\"></a>\n<a href=\"README.md\"><img src=\"https://img.shields.io/badge/document-English-blue.svg\" alt=\"EN doc\"></a>\n<a href=\"docs/README_JA.md\"><img src=\"https://img.shields.io/badge/ドキュメント-日本語-blue.svg\" alt=\"JA doc\"></a>\n<a href=\"https://discord.gg/wCp6Q3fsAk\"><img src=\"https://img.shields.io/badge/Discord-Join-blue?logo=discord&logoColor=white&color=blue\" alt=\"Discord Follow\"></a>\n<a href=\"https://opensource.org/licenses/MIT\"><img src=\"https://img.shields.io/badge/License-MIT-blue.svg\" alt=\"License: MIT\"></a>\n<a href=\"docs/ROADMAP.md\"><img src=\"https://img.shields.io/badge/ROADMAP-路线图-blue\" alt=\"roadmap\"></a>\n<a href=\"https://twitter.com/DeepWisdom2019\"><img src=\"https://img.shields.io/twitter/follow/MetaGPT?style=social\" alt=\"Twitter Follow\"></a>\n</p>\n\n<p align=\"center\">\n   <a href=\"https://airtable.com/appInfdG0eJ9J4NNL/shrEd9DrwVE3jX6oz\"><img src=\"https://img.shields.io/badge/AgentStore-Waitlist-ffc107?logoColor=white\" alt=\"AgentStore Waitlist\"></a>\n   <a href=\"https://vscode.dev/redirect?url=vscode://ms-vscode-remote.remote-containers/cloneInVolume?url=https://github.com/geekan/MetaGPT\"><img src=\"https://img.shields.io/static/v1?label=Dev%20Containers&message=Open&color=blue&logo=visualstudiocode\" alt=\"Open in Dev Containers\"></a>\n   <a href=\"https://codespaces.new/geekan/MetaGPT\"><img src=\"https://img.shields.io/badge/Github_Codespace-Open-blue?logo=github\" alt=\"Open in GitHub Codespaces\"></a>\n   <a href=\"https://huggingface.co/spaces/deepwisdom/MetaGPT\" target=\"_blank\"><img alt=\"Hugging Face\" src=\"https://img.shields.io/badge/%F0%9F%A4%97%20-Hugging%20Face-blue?color=blue&logoColor=white\" /></a>\n</p>\n\n1. MetaGPT takes a **one line requirement** as input and outputs **user stories / competitive analysis / requirements / data structures / APIs / documents, etc.**\n2. Internally, MetaGPT includes **product managers / architects / project managers / engineers.** It provides the entire process of a **software company along with carefully orchestrated SOPs.**\n   1. `Code = SOP(Team)` is the core philosophy. We materialize SOP and apply it to teams composed of LLMs.\n\n![A software company consists of LLM-based roles](docs/resources/software_company_cd.jpeg)\n\n<p align=\"center\">Software Company Multi-Role Schematic (Gradually Implementing)</p>\n\n## MetaGPT's Abilities\n\n\nhttps://github.com/geekan/MetaGPT/assets/34952977/34345016-5d13-489d-b9f9-b82ace413419\n\n\n\n## Examples (fully generated by GPT-4)\n\nFor example, if you type `python startup.py \"Design a RecSys like Toutiao\"`, you would get many outputs, one of them is data & api design\n\n![Jinri Toutiao Recsys Data & API Design](docs/resources/workspace/content_rec_sys/resources/data_api_design.png)\n\nIt costs approximately **$0.2** (in GPT-4 API fees) to generate one example with analysis and design, and around **$2.0** for a full project.\n\n\n\n\n## Installation\n\n### Installation Video Guide\n\n- [Matthew Berman: How To Install MetaGPT - Build A Startup With One Prompt!!](https://youtu.be/uT75J_KG_aY)\n\n### Traditional Installation\n\n```bash\n# Step 1: Ensure that NPM is installed on your system. Then install mermaid-js. (If you don't have npm in your computer, please go to the Node.js offical website to install Node.js https://nodejs.org/ and then you will have npm tool in your computer.)\nnpm --version\nsudo npm install -g @mermaid-js/mermaid-cli\n\n# Step 2: Ensure that Python 3.9+ is installed on your system. You can check this by using:\npython --version\n\n# Step 3: Clone the repository to your local machine, and install it.\ngit clone https://github.com/geekan/metagpt\ncd metagpt\npip install -e.\n```\n\n**Note:**\n\n- If already have Chrome, Chromium, or MS Edge installed, you can skip downloading Chromium by setting the environment variable\n  `PUPPETEER_SKIP_CHROMIUM_DOWNLOAD` to `true`.\n\n- Some people are [having issues](https://github.com/mermaidjs/mermaid.cli/issues/15) installing this tool globally. Installing it locally is an alternative solution,\n\n  ```bash\n  npm install @mermaid-js/mermaid-cli\n  ```\n\n- don't forget to the configuration for mmdc in config.yml\n\n  ```yml\n  PUPPETEER_CONFIG: \"./config/puppeteer-config.json\"\n  MMDC: \"./node_modules/.bin/mmdc\"\n  ```\n\n- if `pip install -e.` fails with error `[Errno 13] Permission denied: '/usr/local/lib/python3.11/dist-packages/test-easy-install-13129.write-test'`, try instead running `pip install -e. --user`\n\n- To convert Mermaid charts to SVG, PNG, and PDF formats. In addition to the Node.js version of Mermaid-CLI, you now have the option to use Python version Playwright, pyppeteer or mermaid.ink for this task.\n\n  - Playwright\n    - **Install Playwright**\n\n    ```bash\n    pip install playwright\n    ```\n\n    - **Install the Required Browsers**\n\n    to support PDF conversion, please install Chrominum.\n\n    ```bash\n    playwright install --with-deps chromium\n    ```\n\n    - **modify `config.yaml`**\n\n    uncomment MERMAID_ENGINE from config.yaml and change it to `playwright`\n\n    ```yaml\n    MERMAID_ENGINE: playwright\n    ```\n\n  - pyppeteer\n    - **Install pyppeteer**\n\n    ```bash\n    pip install pyppeteer\n    ```\n\n    - **Use your own Browsers**\n\n    pyppeteer alow you use installed browsers,  please set the following envirment\n    \n    ```bash\n    export PUPPETEER_EXECUTABLE_PATH = /path/to/your/chromium or edge or chrome\n    ```\n\n    please do not use this command to install browser, it is too old\n\n    ```bash\n    pyppeteer-install\n    ```\n\n    - **modify `config.yaml`**\n\n    uncomment MERMAID_ENGINE from config.yaml and change it to `pyppeteer`\n\n    ```yaml\n    MERMAID_ENGINE: pyppeteer\n    ```\n\n  - mermaid.ink\n    - **modify `config.yaml`**\n\n    uncomment MERMAID_ENGINE from config.yaml and change it to `ink`\n\n    ```yaml\n    MERMAID_ENGINE: ink\n    ```  \n\n    Note: this method does not support pdf export.\n\n### Installation by Docker\n\n```bash\n# Step 1: Download metagpt official image and prepare config.yaml\ndocker pull metagpt/metagpt:v0.3.1\nmkdir -p /opt/metagpt/{config,workspace}\ndocker run --rm metagpt/metagpt:v0.3.1 cat /app/metagpt/config/config.yaml > /opt/metagpt/config/key.yaml\nvim /opt/metagpt/config/key.yaml # Change the config\n\n# Step 2: Run metagpt demo with container\ndocker run --rm \\\n    --privileged \\\n    -v /opt/metagpt/config/key.yaml:/app/metagpt/config/key.yaml \\\n    -v /opt/metagpt/workspace:/app/metagpt/workspace \\\n    metagpt/metagpt:v0.3.1 \\\n    python startup.py \"Write a cli snake game\"\n\n# You can also start a container and execute commands in it\ndocker run --name metagpt -d \\\n    --privileged \\\n    -v /opt/metagpt/config/key.yaml:/app/metagpt/config/key.yaml \\\n    -v /opt/metagpt/workspace:/app/metagpt/workspace \\\n    metagpt/metagpt:v0.3.1\n\ndocker exec -it metagpt /bin/bash\n$ python startup.py \"Write a cli snake game\"\n```\n\nThe command `docker run ...` do the following things:\n\n- Run in privileged mode to have permission to run the browser\n- Map host directory `/opt/metagpt/config` to container directory `/app/metagpt/config`\n- Map host directory `/opt/metagpt/workspace` to container directory `/app/metagpt/workspace`\n- Execute the demo command `python startup.py \"Write a cli snake game\"`\n\n### Build image by yourself\n\n```bash\n# You can also build metagpt image by yourself.\ngit clone https://github.com/geekan/MetaGPT.git\ncd MetaGPT && docker build -t metagpt:custom .\n```\n\n## Configuration\n\n- Configure your `OPENAI_API_KEY` in any of `config/key.yaml / config/config.yaml / env`\n- Priority order: `config/key.yaml > config/config.yaml > env`\n\n```bash\n# Copy the configuration file and make the necessary modifications.\ncp config/config.yaml config/key.yaml\n```\n\n| Variable Name                              | config/key.yaml                           | env                                             |\n| ------------------------------------------ | ----------------------------------------- | ----------------------------------------------- |\n| OPENAI_API_KEY # Replace with your own key | OPENAI_API_KEY: \"sk-...\"                  | export OPENAI_API_KEY=\"sk-...\"                  |\n| OPENAI_API_BASE # Optional                 | OPENAI_API_BASE: \"https://<YOUR_SITE>/v1\" | export OPENAI_API_BASE=\"https://<YOUR_SITE>/v1\" |\n\n## Tutorial: Initiating a startup\n\n```shell\n# Run the script\npython startup.py \"Write a cli snake game\"\n# Do not hire an engineer to implement the project\npython startup.py \"Write a cli snake game\" --implement False\n# Hire an engineer and perform code reviews\npython startup.py \"Write a cli snake game\" --code_review True\n```\n\nAfter running the script, you can find your new project in the `workspace/` directory.\n\n### Preference of Platform or Tool\n\nYou can tell which platform or tool you want to use when stating your requirements.\n\n```shell\npython startup.py \"Write a cli snake game based on pygame\"\n```\n\n### Usage\n\n```\nNAME\n    startup.py - We are a software startup comprised of AI. By investing in us, you are empowering a future filled with limitless possibilities.\n\nSYNOPSIS\n    startup.py IDEA <flags>\n\nDESCRIPTION\n    We are a software startup comprised of AI. By investing in us, you are empowering a future filled with limitless possibilities.\n\nPOSITIONAL ARGUMENTS\n    IDEA\n        Type: str\n        Your innovative idea, such as \"Creating a snake game.\"\n\nFLAGS\n    --investment=INVESTMENT\n        Type: float\n        Default: 3.0\n        As an investor, you have the opportunity to contribute a certain dollar amount to this AI company.\n    --n_round=N_ROUND\n        Type: int\n        Default: 5\n\nNOTES\n    You can also use flags syntax for POSITIONAL ARGUMENTS\n```\n\n### Code walkthrough\n\n```python\nfrom metagpt.software_company import SoftwareCompany\nfrom metagpt.roles import ProjectManager, ProductManager, Architect, Engineer\n\nasync def startup(idea: str, investment: float = 3.0, n_round: int = 5):\n    \"\"\"Run a startup. Be a boss.\"\"\"\n    company = SoftwareCompany()\n    company.hire([ProductManager(), Architect(), ProjectManager(), Engineer()])\n    company.invest(investment)\n    company.start_project(idea)\n    await company.run(n_round=n_round)\n```\n\nYou can check `examples` for more details on single role (with knowledge base) and LLM only examples.\n\n## QuickStart\n\nIt is difficult to install and configure the local environment for some users. The following tutorials will allow you to quickly experience the charm of MetaGPT.\n\n- [MetaGPT quickstart](https://deepwisdom.feishu.cn/wiki/CyY9wdJc4iNqArku3Lncl4v8n2b)\n\nTry it on Huggingface Space\n- https://huggingface.co/spaces/deepwisdom/MetaGPT\n\n## Citation\n\nFor now, cite the [Arxiv paper](https://arxiv.org/abs/2308.00352):\n\n```bibtex\n@misc{hong2023metagpt,\n      title={MetaGPT: Meta Programming for Multi-Agent Collaborative Framework},\n      author={Sirui Hong and Xiawu Zheng and Jonathan Chen and Yuheng Cheng and Jinlin Wang and Ceyao Zhang and Zili Wang and Steven Ka Shing Yau and Zijuan Lin and Liyang Zhou and Chenyu Ran and Lingfeng Xiao and Chenglin Wu},\n      year={2023},\n      eprint={2308.00352},\n      archivePrefix={arXiv},\n      primaryClass={cs.AI}\n}\n```\n\n## Contact Information\n\nIf you have any questions or feedback about this project, please feel free to contact us. We highly appreciate your suggestions!\n\n- **Email:** alexanderwu@fuzhi.ai\n- **GitHub Issues:** For more technical inquiries, you can also create a new issue in our [GitHub repository](https://github.com/geekan/metagpt/issues).\n\nWe will respond to all questions within 2-3 business days.\n\n## Demo\n\nhttps://github.com/geekan/MetaGPT/assets/2707039/5e8c1062-8c35-440f-bb20-2b0320f8d27d\n\n## Join us\n\n📢 Join Our Discord Channel!\nhttps://discord.gg/ZRHeExS6xv\n\nLooking forward to seeing you there! 🎉\n"
    },
    {
      "name": "gilbertalgordo/semantic-kernel",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/69397216?s=40&v=4",
      "owner": "gilbertalgordo",
      "repo_name": "semantic-kernel",
      "description": "Integrate cutting-edge LLM technology quickly and easily into your apps",
      "homepage": "https://gilbertalgordo.github.io/semantic-kernel/",
      "language": "C#",
      "created_at": "2023-07-14T10:11:51Z",
      "updated_at": "2024-11-18T04:41:49Z",
      "topics": [],
      "readme": "# Semantic Kernel\n\n## Status\n\n- Python <br/>\n  [![Python package](https://img.shields.io/pypi/v/semantic-kernel)](https://pypi.org/project/semantic-kernel/)\n- .NET <br/>\n  [![Nuget package](https://img.shields.io/nuget/vpre/Microsoft.SemanticKernel)](https://www.nuget.org/packages/Microsoft.SemanticKernel/)[![dotnet Docker](https://github.com/microsoft/semantic-kernel/actions/workflows/dotnet-ci-docker.yml/badge.svg?branch=main)](https://github.com/microsoft/semantic-kernel/actions/workflows/dotnet-ci-docker.yml)[![dotnet Windows](https://github.com/microsoft/semantic-kernel/actions/workflows/dotnet-ci-windows.yml/badge.svg?branch=main)](https://github.com/microsoft/semantic-kernel/actions/workflows/dotnet-ci-windows.yml)\n\n## Overview\n\n[![License: MIT](https://img.shields.io/github/license/microsoft/semantic-kernel)](https://github.com/microsoft/semantic-kernel/blob/main/LICENSE)\n[![Discord](https://img.shields.io/discord/1063152441819942922?label=Discord&logo=discord&logoColor=white&color=d82679)](https://aka.ms/SKDiscord)\n\n[Semantic Kernel](https://learn.microsoft.com/en-us/semantic-kernel/overview/)\nis an SDK that integrates Large Language Models (LLMs) like\n[OpenAI](https://platform.openai.com/docs/introduction),\n[Azure OpenAI](https://azure.microsoft.com/en-us/products/ai-services/openai-service),\nand [Hugging Face](https://huggingface.co/)\nwith conventional programming languages like C#, Python, and Java. Semantic Kernel achieves this\nby allowing you to define [plugins](https://learn.microsoft.com/en-us/semantic-kernel/concepts/plugins)\nthat can be chained together\nin just a [few lines of code](https://learn.microsoft.com/en-us/semantic-kernel/ai-orchestration/chaining-functions?tabs=Csharp#using-the-runasync-method-to-simplify-your-code).\n\nWhat makes Semantic Kernel _special_, however, is its ability to _automatically_ orchestrate\nplugins with AI. With Semantic Kernel\n[planners](https://learn.microsoft.com/en-us/semantic-kernel/ai-orchestration/planner), you\ncan ask an LLM to generate a plan that achieves a user's unique goal. Afterwards,\nSemantic Kernel will execute the plan for the user.\n\nIt provides:\n\n- abstractions for AI services (such as chat, text to images, audio to text, etc.) and memory stores\n- implementations of those abstractions for services from [OpenAI](https://platform.openai.com/docs/introduction), [Azure OpenAI](https://azure.microsoft.com/en-us/products/ai-services/openai-service), [Hugging Face](https://huggingface.co/), local models, and more, and for a multitude of vector databases, such as those from [Chroma](https://docs.trychroma.com/getting-started), [Qdrant](https://qdrant.tech/), [Milvus](https://milvus.io/), and [Azure](https://learn.microsoft.com/en-us/azure/search/search-what-is-azure-search)\n- a common representation for [plugins](https://learn.microsoft.com/en-us/semantic-kernel/ai-orchestration/plugins), which can then be orchestrated automatically by AI\n- the ability to create such plugins from a multitude of sources, including from OpenAPI specifications, prompts, and arbitrary code written in the target language\n- extensible support for prompt management and rendering, including built-in handling of common formats like Handlebars and Liquid\n- and a wealth of functionality layered on top of these abstractions, such as filters for responsible AI, dependency injection integration, and more.\n\nSemantic Kernel is utilized by enterprises due to its flexibility, modularity and observability. Backed with security enhancing capabilities like telemetry support, and hooks and filters so you’ll feel confident you’re delivering responsible AI solutions at scale.\nSemantic Kernel was designed to be future proof, easily connecting your code to the latest AI models evolving with the technology as it advances. When new models are released, you’ll simply swap them out without needing to rewrite your entire codebase.\n\n#### Please star the repo to show your support for this project!\n\n![Enterprise-ready](https://learn.microsoft.com/en-us/semantic-kernel/media/enterprise-ready.png)\n\n## Getting started with Semantic Kernel\n\nThe Semantic Kernel SDK is available in C#, Python, and Java. To get started, choose your preferred language below. See the [Feature Matrix](https://learn.microsoft.com/en-us/semantic-kernel/get-started/supported-languages) for a breakdown of\nfeature parity between our currently supported languages.\n\n<table width=100%>\n  <tbody>\n    <tr>\n      <td>\n        <img align=\"left\" width=52px src=\"https://user-images.githubusercontent.com/371009/230673036-fad1e8e6-5d48-49b1-a9c1-6f9834e0d165.png\">\n        <div>\n          <a href=\"dotnet/README.md\">Using Semantic Kernel in C#</a> &nbsp<br/>\n        </div>\n      </td>\n      <td>\n        <img align=\"left\" width=52px src=\"https://raw.githubusercontent.com/devicons/devicon/master/icons/python/python-original.svg\">\n        <div>\n          <a href=\"python/README.md\">Using Semantic Kernel in Python</a>\n        </div>\n      </td>\n      <td>\n        <img align=\"left\" width=52px height=52px src=\"https://upload.wikimedia.org/wikipedia/en/3/30/Java_programming_language_logo.svg\" alt=\"Java logo\">\n        <div>\n          <a href=\"https://github.com/microsoft/semantic-kernel/blob/main/java/README.md\">Using Semantic Kernel in Java</a>\n        </div>\n      </td>\n    </tr>\n  </tbody>\n</table>\n\nThe quickest way to get started with the basics is to get an API key\nfrom either OpenAI or Azure OpenAI and to run one of the C#, Python, and Java console applications/scripts below.\n\n### For C#:\n\n1. Go to the Quick start page [here](https://learn.microsoft.com/en-us/semantic-kernel/get-started/quick-start-guide?pivots=programming-language-csharp) and follow the steps to dive in.\n2. After Installing the SDK, we advise you follow the steps and code detailed to write your first console app.\n   ![dotnetmap](https://learn.microsoft.com/en-us/semantic-kernel/media/dotnetmap.png)\n\n### For Python:\n\n1. Go to the Quick start page [here](https://learn.microsoft.com/en-us/semantic-kernel/get-started/quick-start-guide?pivots=programming-language-python) and follow the steps to dive in.\n2. You'll need to ensure that you toggle to Python in the the Choose a programming language table at the top of the page.\n   ![pythonmap](https://learn.microsoft.com/en-us/semantic-kernel/media/pythonmap.png)\n\n### For Java:\n\nThe Java code is in the [semantic-kernel-java](https://github.com/microsoft/semantic-kernel-java) repository. See\n[semantic-kernel-java build](https://github.com/microsoft/semantic-kernel-java/blob/main/BUILD.md) for instructions on\nhow to build and run the Java code.\n\nPlease file Java Semantic Kernel specific issues in\nthe [semantic-kernel-java](https://github.com/microsoft/semantic-kernel-java) repository.\n\n## Learning how to use Semantic Kernel\n\nThe fastest way to learn how to use Semantic Kernel is with our C# and Python Jupyter notebooks. These notebooks\ndemonstrate how to use Semantic Kernel with code snippets that you can run with a push of a button.\n\n- [Getting Started with C# notebook](dotnet/notebooks/00-getting-started.ipynb)\n- [Getting Started with Python notebook](python/samples/getting_started/00-getting-started.ipynb)\n\nOnce you've finished the getting started notebooks, you can then check out the main walkthroughs\non our Learn site. Each sample comes with a completed C# and Python project that you can run locally.\n\n1. 📖 [Getting Started](https://learn.microsoft.com/en-us/semantic-kernel/get-started/quick-start-guide)\n1. 🔌 [Detailed Samples](https://learn.microsoft.com/en-us/semantic-kernel/get-started/detailed-samples)\n1. 💡 [Concepts](https://learn.microsoft.com/en-us/semantic-kernel/concepts/kernel)\n\nFinally, refer to our API references for more details on the C# and Python APIs:\n\n- [C# API reference](https://learn.microsoft.com/en-us/dotnet/api/microsoft.semantickernel?view=semantic-kernel-dotnet)\n- [Python API reference](https://learn.microsoft.com/en-us/python/api/semantic-kernel/semantic_kernel?view=semantic-kernel-python)\n- Java API reference (coming soon)\n\n## Visual Studio Code extension: design semantic functions with ease\n\nThe Semantic Kernel extension for Visual Studio Code makes it easy to design and test semantic functions. The extension provides an interface for designing semantic functions and allows you to test them with the push of a button with your existing models and data.\n\n## Join the community\n\nWe welcome your contributions and suggestions to SK community! One of the easiest\nways to participate is to engage in discussions in the GitHub repository.\nBug reports and fixes are welcome!\n\nFor new features, components, or extensions, please open an issue and discuss with\nus before sending a PR. This is to avoid rejection as we might be taking the core\nin a different direction, but also to consider the impact on the larger ecosystem.\n\nTo learn more and get started:\n\n- Read the [documentation](https://aka.ms/sk/learn)\n- Learn how to [contribute](https://learn.microsoft.com/en-us/semantic-kernel/support/contributing) to the project\n- Ask questions in the [GitHub discussions](https://github.com/microsoft/semantic-kernel/discussions)\n- Ask questions in the [Discord community](https://aka.ms/SKDiscord)\n\n- Attend [regular office hours and SK community events](COMMUNITY.md)\n- Follow the team on our [blog](https://aka.ms/sk/blog)\n\n## Contributor Wall of Fame\n\n[![semantic-kernel contributors](https://contrib.rocks/image?repo=microsoft/semantic-kernel)](https://github.com/microsoft/semantic-kernel/graphs/contributors)\n\n## Code of Conduct\n\nThis project has adopted the\n[Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the\n[Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/)\nor contact [opencode@microsoft.com](mailto:opencode@microsoft.com)\nwith any additional questions or comments.\n\n## License\n\nCopyright (c) Microsoft Corporation. All rights reserved.\n\nLicensed under the [MIT](LICENSE) license.\n"
    },
    {
      "name": "jordanbean-msft/openai-agents-fastapi",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/84806584?s=40&v=4",
      "owner": "jordanbean-msft",
      "repo_name": "openai-agents-fastapi",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2024-09-04T00:34:05Z",
      "updated_at": "2024-11-04T12:28:46Z",
      "topics": [],
      "readme": "# openai-agents-fastapi\n\n![architecture](./.img/architecture.png)\n\n## Disclaimer\n\n**THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.**\n\n## Prerequisites\n\n- [Azure CLI](https://docs.microsoft.com/en-us/cli/azure/install-azure-cli)\n- Azure subscription & resource group\n- [Python 3.12](https://www.python.org/downloads/release/python-3123/)\n- [Azure Developer CLI](https://learn.microsoft.com/en-us/azure/developer/azure-developer-cli/install-azd?tabs=winget-windows%2Cbrew-mac%2Cscript-linux&pivots=os-windows)\n- [Terraform CLI](https://developer.hashicorp.com/terraform/tutorials/azure-get-started/install-cli)\n- [Docker](https://docs.docker.com/engine/install/)\n\n## Deployment\n\n## Local debugging (PowerShell)\n\n1.  Create a virtual environment for the Python to run in\n\n\t```powershell\n\tcd src/api\n\n\tpython -m venv .venv\n\t```\n\n1.  Activate the virtual environment\n\n\t```powershell\n\t./.venv/Scripts/activate\n\t```\n\n1.  Install the requirements\n\n\t```powershell\n\tpip install -r ./requirements.txt\n\t```\n\n1.  Create a `.env` file with the following values (specify your own values):\n\n\t```powershell\n\tAZURE_OPENAI_ENDPOINT=\n    OPENAI_API_VERSION=2024-08-01-preview\n    OPENAI_DEPLOYMENT_NAME=\n    CLIENT_ID=\n    CLIENT_SECRET=\n    TENANT_ID=\n    OPENAI_CLIENT_ID=\n    APIM_SUBSCRIPTION_KEY=\n    APPLICATION_INSIGHTS_CONNECTION_STRING=\n    AZURE_OPENAI_API_KEY=\n    USE_APIM=\n\t```\n\n1.  Run the API\n\n\t```powershell\n\tpython -m uvicorn app.main:app --reload --env-file .env --log-level debug\n\t```\n\n\tAlternatively, you can open this repo in VS Code and use the included `.vscode/launch.json` file to launch the app.\n\n1.  Query the API endpoint (this will take a minute or two to return)\n\n\t```powershell\n\t$LoginParameters = @{ Uri = \"http://localhost:8000/v1/analyze\"; Method = \"POST\"; Headers = @{ \"Content-Type\"= \"application/json\" } }\n\n\t$Body = (@{ \"stockTicker1\": \"MSFT\", \"companyName1\": \"Microsoft\", \"stockTicker2\": \"TSLA\", \"companyName2\": \"Tesla\" }) | ConvertTo-Json\n\n\tInvoke-WebRequest @LoginParameters -Body $Body | Select-Object Content | Format-Table -Wrap\n\t```\n\n1.  The output will look something like this.\n\n    ```json\n    {\n        \"stockTicker1\": \"MSFT\",\n        \"companyName1\": \"Microsoft\",\n        \"stockTicker2\": \"TSLA\",\n        \"companyName2\": \"Tesla\",\n        \"chat_results\": [\n            \"### Stock Prices Analysis\\n\\n#### Microsoft (MSFT) Stock Prices:\\n1. **2024-09-06:** \\n   - Previous Close: $408.39\\n   - Current Close: $401.7\\n   - Change: -$6.69\\n   \\n2. **2024-09-09:** \\n   - Previous Close: $401.7\\n   - Current Close: $405.72\\n   - Change: +$4.02\\n   \\n3. **2024-09-11:** \\n   - Previous Close: $414.0\\n   - Current Close: $423.04\\n   - Change: +$9.04\\n   \\n#### Tesla (TSLA) Stock Prices:\\n1. **2024-04-24:** \\n   - Previous Close: $180.0\\n   - Current Close: $203.4\\n   - Change: +$23.4\\n   \\n2. **2024-09-11:** \\n   - Previous Close: $216.27\\n   - Current Close: $226.17\\n   - Change: +$9.9\\n   \\n3. **2024-09-06:**\\n   - Previous Close: $230.17\\n   - Current Close: $240.73\\n   - Change: +$10.56\\n   \\n### News Articles Analysis\\n\\n#### Microsoft (MSFT) News:\\n1. **2024-09-06 - Goldman Sachs Analyst Insights:**\\n   - Discussion on factors influencing the tech sector, including potential Federal Reserve rate cuts, the 2024 presidential election, and advancements in generative AI.\\n   - Potential for increased volatility in tech stocks, including Microsoft.\\n\\n2. **2024-09-09 - Nvidia Sell-Off:**\\n   - Analysts addressed the recent sell-off in Nvidia’s stock, indicating it was an exaggerated market reaction.\\n   - The sentiment towards Nvidia influenced perceptions of other tech giants, including Microsoft.\\n\\n3. **2024-09-11 - Microsoft's Financial Performance:**\\n   - Microsoft reported impressive revenue growth driven by its cloud services and AI initiatives.\\n   - Positive market reaction due to its strong financial health and growth potential.\\n\\n#### Tesla (TSLA) News:\\n1. **2024-04-24 - Tesla's Q1 Earnings Report:**\\n   - Despite a decline in revenue and net income, the announcement of accelerating the launch of more affordable vehicles led to a surge in stock price.\\n\\n2. **2024-09-11 - Analysts Set New Price Targets:**\\n   - Analysts set new optimistic price targets based on Tesla’s advancements in autonomous driving technology and market presence.\\n\\n3. **2024-09-06 - FSD Release for Europe and China:**\\n   - Positive reception by investors due to the release of Full Self-Driving software for Europe and China, highlighting Tesla’s progress in autonomous driving technology.\\n\\n### Correlation Analysis\\n\\nIn analyzing the stock prices and news articles from both Microsoft and Tesla, the following observations can be made:\\n\\n- **Tech Sector Influence:** Both companies are significantly influenced by broader tech sector sentiments. For instance, discussions on generative AI and Federal Reserve rate cuts affect tech stocks like Microsoft. Similarly, advancements in technology, such as Tesla’s autonomous driving technology, drive investor optimism.\\n\\n- **Performance-Driven Responses:** Positive financial performance reports and strategic advancements (e.g., Microsoft's revenue growth, Tesla’s vehicle launch acceleration and FSD software release) result in positive stock price reactions for both companies.\\n\\n- **Investor Sentiment:** News articles often reflect market sentiment, which impacts stock prices. For example, optimistic targets set by analysts boost Tesla’s stock, while Microsoft's strong performance in cloud and AI is mirrored in its stock prices.\\n\\nWhile both companies operate in different segments (software/cloud services vs. electric vehicles/autonomous driving), they show a correlation in how broader tech sector developments and positive news concerning technological advancements and financial performance influence their stock prices. However, a direct correlation between their stock prices may not be evident from this short observation period alone, requiring a more extended data set for a robust conclusion.\",\n            \"Overall, while both Microsoft and Tesla show independent movements in their stock prices based on company-specific news and performance, there are broader market trends and sector-wide influences that simultaneously affect them. The advancements in technology and investor sentiment driven by optimistic targets or significant product releases can create correlated movements in tech stocks generally, even though the specifics of their industries differ. Therefore, while direct correlation from brief data isn’t strong, certain tech sector factors can create a synchronous impact on both Microsoft and Tesla stock prices.\"\n        ]\n    }\n    ```\n\n## Links\n"
    },
    {
      "name": "franciszchen/MAP",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/16402098?s=40&v=4",
      "owner": "franciszchen",
      "repo_name": "MAP",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2024-10-15T16:02:06Z",
      "updated_at": "2025-03-14T02:30:42Z",
      "topics": [],
      "readme": "# MAP\n![Image](https://github.com/franciszchen/MAP/blob/main/fig/MAP_framework_v17.jpg)\n\n## 1. Environment setting\n```\nstep 1.\npip install --upgrade metagpt\n\nstep 2.\nmetagpt --init-config\n\nstep 3. \nmodify your own config.yaml as follow with your openai_key\n\nllm:\n  api_type: \"openai\"\n  api_version: \"2024-02-15-preview\"\n  base_url: \"your_url\"\n  api_key: 'your_key'\n  model: \"openai\"\n```\n\n## 2. Inference\n```\nstep 1. cd ./MetaGPT-main/examples/clinic\n\nstep 2. python MAgent.py\n```\n## 3. Download\n```\nModel_download_url:xxx\nDataset_download_url: xxx\n```\n\n\n"
    },
    {
      "name": "nholuongut/aks-store-demo",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/58627821?s=40&v=4",
      "owner": "nholuongut",
      "repo_name": "aks-store-demo",
      "description": "Sample microservices app for AKS demos, tutorials, and experiments",
      "homepage": null,
      "language": "Bicep",
      "created_at": "2024-10-25T12:03:31Z",
      "updated_at": "2024-12-31T08:50:10Z",
      "topics": [],
      "readme": "# AKS Store Demo\n\n![](https://i.imgur.com/waxVImv.png)\n### [View all Roadmaps](https://github.com/nholuongut/all-roadmaps) &nbsp;&middot;&nbsp; [Best Practices](https://github.com/nholuongut/all-roadmaps/blob/main/public/best-practices/) &nbsp;&middot;&nbsp; [Questions](https://www.linkedin.com/in/nholuong/)\n<br/>\n\n---\npage_type: sample\nlanguages:\n- azdeveloper\n- go\n- javascript\n- rust\n- nodejs\n- python\n- bicep\n- terraform\n- dockerfile\nproducts:\n- azure\n- azure-kubernetes-service\n- azure-openai\n- azure-cosmos-db\n- azure-container-registry\n- azure-service-bus\n- azure-monitor\n- azure-log-analytics\n- azure-managed-grafana\n- azure-key-vault\nurlFragment: aks-store-demo\nname: AKS Store Demo\ndescription: This sample demo app consists of a group of containerized microservices that can be easily deployed into an Azure Kubernetes Service (AKS) cluster. \n---\n<!-- YAML front-matter schema: https://review.learn.microsoft.com/en-us/help/contribute/samples/process/onboarding?branch=main#supported-metadata-fields-for-readmemd -->\n\n# AKS Store Demo\n\nThis sample demo app consists of a group of containerized microservices that can be easily deployed into an Azure Kubernetes Service (AKS) cluster. This is meant to show a realistic scenario using a polyglot architecture, event-driven design, and common open source back-end services (eg - RabbitMQ, MongoDB). The application also leverages OpenAI's GPT-3 models to generate product descriptions. This can be done using either [Azure OpenAI](https://learn.microsoft.com/azure/ai-services/openai/overview) or [OpenAI](https://openai.com/).\n\nThis application is inspired by another demo app called [Red Dog](https://github.com/Azure/reddog-code).\n\n> [!NOTE]\n> This is not meant to be an example of perfect code to be used in production, but more about showing a realistic application running in AKS. \n\n<!-- \nTo walk through a quick deployment of this application, see the [AKS Quickstart](https://learn.microsoft.com/azure/aks/learn/quick-kubernetes-deploy-cli).\n\nTo walk through a complete experience where this code is packaged into container images, uploaded to Azure Container Registry, and then run in and AKS cluster, see the [AKS Tutorials](https://learn.microsoft.com/azure/aks/tutorial-kubernetes-prepare-app).\n\n -->\n\n## Architecture\n\nThe application has the following services: \n\n| Service | Description |\n| --- | --- |\n| `makeline-service` | This service handles processing orders from the queue and completing them (Golang) |\n| `order-service` | This service is used for placing orders (Javascript) |\n| `product-service` | This service is used to perform CRUD operations on products (Rust) |\n| `store-front` | Web app for customers to place orders (Vue.js) |\n| `store-admin` | Web app used by store employees to view orders in queue and manage products (Vue.js) | \n| `virtual-customer` | Simulates order creation on a scheduled basis (Rust) |\n| `virtual-worker` | Simulates order completion on a scheduled basis (Rust) |\n| `ai-service` | Optional service for adding generative text and graphics creation (Python) |\n| `mongodb` | MongoDB instance for persisted data |\n| `rabbitmq` | RabbitMQ for an order queue |\n\n![Logical Application Architecture Diagram](assets/demo-arch-with-openai.png)\n\n## Run the app on Azure Kubernetes Service (AKS)\n\nTo learn how to deploy this app on AKS, see [Quickstart: Deploy an Azure Kubernetes Service (AKS) cluster using Azure CLI](https://learn.microsoft.com/azure/aks/learn/quick-kubernetes-deploy-cli).\n\n> [!NOTE]\n> The above article shows a simplified version of the store app with some services removed. For the full application, you can use the `aks-store-all-in-one.yaml` file in this repo.\n\n## Run on any Kubernetes\n\nThis application uses public images stored in GitHub Container Registry and Microsoft Container Registry (MCR). Once your Kubernetes cluster of choice is setup, you can deploy the full app with the below commands.\n\nThis deployment deploys everything except the `ai-service` that integrates OpenAI. If you want to try integrating the OpenAI component, take a look at this article: [Deploy an application that uses OpenAI on Azure Kubernetes Service (AKS)](https://learn.microsoft.com/azure/aks/open-ai-quickstart?tabs=aoai).\n\n```bash\nkubectl create ns pets\n\nkubectl apply -f https://raw.githubusercontent.com/nholuongut/aks-store-demo/main/aks-store-all-in-one.yaml -n pets\n```\n\n## Run the app locally\n\nThe application is designed to be [run in an AKS cluster](#run-the-app-on-aks), but can also be run locally using Docker Compose.\n\n> [!TIP]\n> You must have [Docker Desktop](https://www.docker.com/products/docker-desktop) installed to run this app locally. If you do not have it installed locally, you can try opening this repo in a [GitHub Codespace instead](#run-the-app-with-github-codespaces)\n\nTo run this app locally:\n\nClone the repo to your development computer and navigate to the directory:\n\n```console\ngit clone https://github.com/nholuongut/aks-store-demo.git\ncd aks-store-demo\n```\n\nConfigure your Azure OpenAI or OpenAI API keys in [`docker-compose.yml`](./docker-compose.yml) using the environment variables in the `ai-service` section:\n\n```yaml\n  ai-service:\n    build: src/ai-service\n    container_name: 'ai-service'\n    ...\n    environment:\n      - USE_AZURE_OPENAI=True # set to False if you are not using Azure OpenAI\n      - AZURE_OPENAI_DEPLOYMENT_NAME= # required if using Azure OpenAI\n      - AZURE_OPENAI_ENDPOINT= # required if using Azure OpenAI\n      - OPENAI_API_KEY= # always required\n      - OPENAI_ORG_ID= # required if using OpenAI\n    ...\n```\n\nAlternatively, if you do not have access to Azure OpenAI or OpenAI API keys, you can run the app without the `ai-service` by commenting out the `ai-service` section in [`docker-compose.yml`](./docker-compose.yml). For example:\n\n```yaml\n#  ai-service:\n#    build: src/ai-service\n#    container_name: 'ai-service'\n...\n#    networks:\n#      - backend_services\n```\n\nStart the app using `docker compose`. For example:\n\n```bash\ndocker compose up\n```\n\nTo stop the app, you can hit the `CTRL+C` key combination in the terminal window where the app is running.\n\n## Run the app with GitHub Codespaces\n\nThis repo also includes [DevContainer configuration](./.devcontainer/devcontainer.json), so you can open the repo using [GitHub Codespaces](https://docs.github.com/en/codespaces/overview). This will allow you to run the app in a container in the cloud, without having to install Docker on your local machine. When the Codespace is created, you can run the app using the same instructions as above.\n\n[![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://github.com/codespaces/new?hide_repo_select=true&ref=main&repo=648726487)\n\n## Deploy the app to Azure using Azure Developer CLI\n\nSee the [Azure Developer CLI](./docs/azd.md) documentation for instructions on how to quickly deploy the app to Azure.\n\n## Additional Resources\n\n- AKS Documentation. https://learn.microsoft.com/azure/aks\n- Kubernetes Learning Path. https://azure.microsoft.com/resources/kubernetes-learning-path \n\n# 🚀 I'm are always open to your feedback.  Please contact as bellow information:\n### [Contact ]\n* [Name: nho Luong]\n* [Skype](luongutnho_skype)\n* [Github](https://github.com/nholuongut/)\n* [Linkedin](https://www.linkedin.com/in/nholuong/)\n* [Email Address](luongutnho@hotmail.com)\n\n![](https://i.imgur.com/waxVImv.png)\n![](Donate.png)\n[![ko-fi](https://ko-fi.com/img/githubbutton_sm.svg)](https://ko-fi.com/nholuong)\n\n# License\n* Nho Luong (c). All Rights Reserved.🌟"
    },
    {
      "name": "nacoline/MetaMG400",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/96275268?s=40&v=4",
      "owner": "nacoline",
      "repo_name": "MetaMG400",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2024-10-23T12:25:43Z",
      "updated_at": "2024-10-25T09:17:51Z",
      "topics": [],
      "readme": ""
    },
    {
      "name": "RalphHightower/semantic-kernel",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/32745442?s=40&v=4",
      "owner": "RalphHightower",
      "repo_name": "semantic-kernel",
      "description": "Integrate cutting-edge LLM technology quickly and easily into your apps",
      "homepage": "https://aka.ms/semantic-kernel",
      "language": "C#",
      "created_at": "2023-03-03T17:04:28Z",
      "updated_at": "2025-03-15T06:12:22Z",
      "topics": [],
      "readme": "# Semantic Kernel\n\n## Status\n\n- Python <br/>\n  [![Python package](https://img.shields.io/pypi/v/semantic-kernel)](https://pypi.org/project/semantic-kernel/)\n- .NET <br/>\n  [![Nuget package](https://img.shields.io/nuget/vpre/Microsoft.SemanticKernel)](https://www.nuget.org/packages/Microsoft.SemanticKernel/)[![dotnet Docker](https://github.com/microsoft/semantic-kernel/actions/workflows/dotnet-ci-docker.yml/badge.svg?branch=main)](https://github.com/microsoft/semantic-kernel/actions/workflows/dotnet-ci-docker.yml)[![dotnet Windows](https://github.com/microsoft/semantic-kernel/actions/workflows/dotnet-ci-windows.yml/badge.svg?branch=main)](https://github.com/microsoft/semantic-kernel/actions/workflows/dotnet-ci-windows.yml)\n\n## Overview\n\n[![License: MIT](https://img.shields.io/github/license/microsoft/semantic-kernel)](https://github.com/microsoft/semantic-kernel/blob/main/LICENSE)\n[![Discord](https://img.shields.io/discord/1063152441819942922?label=Discord&logo=discord&logoColor=white&color=d82679)](https://aka.ms/SKDiscord)\n\n[Semantic Kernel](https://learn.microsoft.com/en-us/semantic-kernel/overview/)\nis an SDK that integrates Large Language Models (LLMs) like\n[OpenAI](https://platform.openai.com/docs/introduction),\n[Azure OpenAI](https://azure.microsoft.com/en-us/products/ai-services/openai-service),\nand [Hugging Face](https://huggingface.co/)\nwith conventional programming languages like C#, Python, and Java. Semantic Kernel achieves this\nby allowing you to define [plugins](https://learn.microsoft.com/en-us/semantic-kernel/concepts/plugins)\nthat can be chained together\nin just a [few lines of code](https://learn.microsoft.com/en-us/semantic-kernel/ai-orchestration/chaining-functions?tabs=Csharp#using-the-runasync-method-to-simplify-your-code).\n\nWhat makes Semantic Kernel _special_, however, is its ability to _automatically_ orchestrate\nplugins with AI. With Semantic Kernel\n[planners](https://learn.microsoft.com/en-us/semantic-kernel/ai-orchestration/planner), you\ncan ask an LLM to generate a plan that achieves a user's unique goal. Afterwards,\nSemantic Kernel will execute the plan for the user.\n\nIt provides:\n\n- abstractions for AI services (such as chat, text to images, audio to text, etc.) and memory stores\n- implementations of those abstractions for services from [OpenAI](https://platform.openai.com/docs/introduction), [Azure OpenAI](https://azure.microsoft.com/en-us/products/ai-services/openai-service), [Hugging Face](https://huggingface.co/), local models, and more, and for a multitude of vector databases, such as those from [Chroma](https://docs.trychroma.com/getting-started), [Qdrant](https://qdrant.tech/), [Milvus](https://milvus.io/), and [Azure](https://learn.microsoft.com/en-us/azure/search/search-what-is-azure-search)\n- a common representation for [plugins](https://learn.microsoft.com/en-us/semantic-kernel/ai-orchestration/plugins), which can then be orchestrated automatically by AI\n- the ability to create such plugins from a multitude of sources, including from OpenAPI specifications, prompts, and arbitrary code written in the target language\n- extensible support for prompt management and rendering, including built-in handling of common formats like Handlebars and Liquid\n- and a wealth of functionality layered on top of these abstractions, such as filters for responsible AI, dependency injection integration, and more.\n\nSemantic Kernel is utilized by enterprises due to its flexibility, modularity and observability. Backed with security enhancing capabilities like telemetry support, and hooks and filters so you’ll feel confident you’re delivering responsible AI solutions at scale.\nSemantic Kernel was designed to be future proof, easily connecting your code to the latest AI models evolving with the technology as it advances. When new models are released, you’ll simply swap them out without needing to rewrite your entire codebase.\n\n#### Please star the repo to show your support for this project!\n\n![Enterprise-ready](https://learn.microsoft.com/en-us/semantic-kernel/media/enterprise-ready.png)\n\n## Getting started with Semantic Kernel\n\nThe Semantic Kernel SDK is available in C#, Python, and Java. To get started, choose your preferred language below. See the [Feature Matrix](https://learn.microsoft.com/en-us/semantic-kernel/get-started/supported-languages) for a breakdown of\nfeature parity between our currently supported languages.\n\n<table width=100%>\n  <tbody>\n    <tr>\n      <td>\n        <img align=\"left\" width=52px src=\"https://user-images.githubusercontent.com/371009/230673036-fad1e8e6-5d48-49b1-a9c1-6f9834e0d165.png\">\n        <div>\n          <a href=\"dotnet/README.md\">Using Semantic Kernel in C#</a> &nbsp<br/>\n        </div>\n      </td>\n      <td>\n        <img align=\"left\" width=52px src=\"https://raw.githubusercontent.com/devicons/devicon/master/icons/python/python-original.svg\">\n        <div>\n          <a href=\"python/README.md\">Using Semantic Kernel in Python</a>\n        </div>\n      </td>\n      <td>\n        <img align=\"left\" width=52px height=52px src=\"https://upload.wikimedia.org/wikipedia/en/3/30/Java_programming_language_logo.svg\" alt=\"Java logo\">\n        <div>\n          <a href=\"https://github.com/microsoft/semantic-kernel-java/blob/main/README.md\">Using Semantic Kernel in Java</a>\n        </div>\n      </td>\n    </tr>\n  </tbody>\n</table>\n\nThe quickest way to get started with the basics is to get an API key\nfrom either OpenAI or Azure OpenAI and to run one of the C#, Python, and Java console applications/scripts below.\n\n### For C#:\n\n1. Go to the Quick start page [here](https://learn.microsoft.com/en-us/semantic-kernel/get-started/quick-start-guide?pivots=programming-language-csharp) and follow the steps to dive in.\n2. After Installing the SDK, we advise you follow the steps and code detailed to write your first console app.\n   ![dotnetmap](https://learn.microsoft.com/en-us/semantic-kernel/media/dotnetmap.png)\n\n### For Python:\n\n1. Go to the Quick start page [here](https://learn.microsoft.com/en-us/semantic-kernel/get-started/quick-start-guide?pivots=programming-language-python) and follow the steps to dive in.\n2. You'll need to ensure that you toggle to Python in the Choose a programming language table at the top of the page.\n   ![pythonmap](https://learn.microsoft.com/en-us/semantic-kernel/media/pythonmap.png)\n\n### For Java:\n\nThe Java code is in the [semantic-kernel-java](https://github.com/microsoft/semantic-kernel-java) repository. See\n[semantic-kernel-java build](https://github.com/microsoft/semantic-kernel-java/blob/main/BUILD.md) for instructions on\nhow to build and run the Java code.\n\nPlease file Java Semantic Kernel specific issues in\nthe [semantic-kernel-java](https://github.com/microsoft/semantic-kernel-java) repository.\n\n## Learning how to use Semantic Kernel\n\nThe fastest way to learn how to use Semantic Kernel is with our C# and Python Jupyter notebooks. These notebooks\ndemonstrate how to use Semantic Kernel with code snippets that you can run with a push of a button.\n\n- [Getting Started with C# notebook](dotnet/notebooks/00-getting-started.ipynb)\n- [Getting Started with Python notebook](python/samples/getting_started/00-getting-started.ipynb)\n\nOnce you've finished the getting started notebooks, you can then check out the main walkthroughs\non our Learn site. Each sample comes with a completed C# and Python project that you can run locally.\n\n1. 📖 [Getting Started](https://learn.microsoft.com/en-us/semantic-kernel/get-started/quick-start-guide)\n1. 🔌 [Detailed Samples](https://learn.microsoft.com/en-us/semantic-kernel/get-started/detailed-samples)\n1. 💡 [Concepts](https://learn.microsoft.com/en-us/semantic-kernel/concepts/kernel)\n\nFinally, refer to our API references for more details on the C# and Python APIs:\n\n- [C# API reference](https://learn.microsoft.com/en-us/dotnet/api/microsoft.semantickernel?view=semantic-kernel-dotnet)\n- [Python API reference](https://learn.microsoft.com/en-us/python/api/semantic-kernel/semantic_kernel?view=semantic-kernel-python)\n- Java API reference (coming soon)\n\n## Visual Studio Code extension: design semantic functions with ease\n\nThe Semantic Kernel extension for Visual Studio Code makes it easy to design and test semantic functions. The extension provides an interface for designing semantic functions and allows you to test them with the push of a button with your existing models and data.\n\n## Join the community\n\nWe welcome your contributions and suggestions to SK community! One of the easiest\nways to participate is to engage in discussions in the GitHub repository.\nBug reports and fixes are welcome!\n\nFor new features, components, or extensions, please open an issue and discuss with\nus before sending a PR. This is to avoid rejection as we might be taking the core\nin a different direction, but also to consider the impact on the larger ecosystem.\n\nTo learn more and get started:\n\n- Read the [documentation](https://aka.ms/sk/learn)\n- Learn how to [contribute](https://learn.microsoft.com/en-us/semantic-kernel/support/contributing) to the project\n- Ask questions in the [GitHub discussions](https://github.com/microsoft/semantic-kernel/discussions)\n- Ask questions in the [Discord community](https://aka.ms/SKDiscord)\n\n- Attend [regular office hours and SK community events](COMMUNITY.md)\n- Follow the team on our [blog](https://aka.ms/sk/blog)\n\n## Contributor Wall of Fame\n\n[![semantic-kernel contributors](https://contrib.rocks/image?repo=microsoft/semantic-kernel)](https://github.com/microsoft/semantic-kernel/graphs/contributors)\n\n## Code of Conduct\n\nThis project has adopted the\n[Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the\n[Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/)\nor contact [opencode@microsoft.com](mailto:opencode@microsoft.com)\nwith any additional questions or comments.\n\n## License\n\nCopyright (c) Microsoft Corporation. All rights reserved.\n\nLicensed under the [MIT](LICENSE) license.\n"
    },
    {
      "name": "XiangJinyu/Aflow",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/137690584?s=40&v=4",
      "owner": "XiangJinyu",
      "repo_name": "Aflow",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2024-10-10T11:00:23Z",
      "updated_at": "2024-11-12T01:19:46Z",
      "topics": [],
      "readme": "\n# MetaGPT: The Multi-Agent Framework\n\n<p align=\"center\">\n<a href=\"\"><img src=\"docs/resources/MetaGPT-new-log.png\" alt=\"MetaGPT logo: Enable GPT to work in software company, collaborating to tackle more complex tasks.\" width=\"150px\"></a>\n</p>\n\n<p align=\"center\">\n<b>Assign different roles to GPTs to form a collaborative entity for complex tasks.</b>\n</p>\n\n<p align=\"center\">\n<a href=\"docs/README_CN.md\"><img src=\"https://img.shields.io/badge/文档-中文版-blue.svg\" alt=\"CN doc\"></a>\n<a href=\"README.md\"><img src=\"https://img.shields.io/badge/document-English-blue.svg\" alt=\"EN doc\"></a>\n<a href=\"docs/README_JA.md\"><img src=\"https://img.shields.io/badge/ドキュメント-日本語-blue.svg\" alt=\"JA doc\"></a>\n<a href=\"https://opensource.org/licenses/MIT\"><img src=\"https://img.shields.io/badge/License-MIT-blue.svg\" alt=\"License: MIT\"></a>\n<a href=\"docs/ROADMAP.md\"><img src=\"https://img.shields.io/badge/ROADMAP-路线图-blue\" alt=\"roadmap\"></a>\n<a href=\"https://discord.gg/DYn29wFk9z\"><img src=\"https://dcbadge.vercel.app/api/server/DYn29wFk9z?style=flat\" alt=\"Discord Follow\"></a>\n<a href=\"https://twitter.com/MetaGPT_\"><img src=\"https://img.shields.io/twitter/follow/MetaGPT?style=social\" alt=\"Twitter Follow\"></a>\n</p>\n\n<p align=\"center\">\n   <a href=\"https://vscode.dev/redirect?url=vscode://ms-vscode-remote.remote-containers/cloneInVolume?url=https://github.com/geekan/MetaGPT\"><img src=\"https://img.shields.io/static/v1?label=Dev%20Containers&message=Open&color=blue&logo=visualstudiocode\" alt=\"Open in Dev Containers\"></a>\n   <a href=\"https://codespaces.new/geekan/MetaGPT\"><img src=\"https://img.shields.io/badge/Github_Codespace-Open-blue?logo=github\" alt=\"Open in GitHub Codespaces\"></a>\n   <a href=\"https://huggingface.co/spaces/deepwisdom/MetaGPT\" target=\"_blank\"><img alt=\"Hugging Face\" src=\"https://img.shields.io/badge/%F0%9F%A4%97%20-Hugging%20Face-blue?color=blue&logoColor=white\" /></a>\n</p>\n\n## News\n🚀 Mar. 29, 2024: [v0.8.0](https://github.com/geekan/MetaGPT/releases/tag/v0.8.0) released. Now you can use Data Interpreter ([arxiv](https://arxiv.org/abs/2402.18679), [example](https://docs.deepwisdom.ai/main/en/DataInterpreter/), [code](https://github.com/geekan/MetaGPT/tree/main/examples/di)) via pypi package import. Meanwhile, we integrated RAG module and supported multiple new LLMs.\n\n🚀 Feb. 08, 2024: [v0.7.0](https://github.com/geekan/MetaGPT/releases/tag/v0.7.0) released, supporting assigning different LLMs to different Roles. We also introduced [Data Interpreter](https://github.com/geekan/MetaGPT/blob/main/examples/di/README.md), a powerful agent capable of solving a wide range of real-world problems.\n\n🚀 Jan. 16, 2024: Our paper [MetaGPT: Meta Programming for A Multi-Agent Collaborative Framework\n](https://arxiv.org/abs/2308.00352) accepted for **oral presentation (top 1.2%)** at ICLR 2024, **ranking #1** in the LLM-based Agent category.\n\n🚀 Jan. 03, 2024: [v0.6.0](https://github.com/geekan/MetaGPT/releases/tag/v0.6.0) released, new features include serialization, upgraded OpenAI package and supported multiple LLM, provided [minimal example for debate](https://github.com/geekan/MetaGPT/blob/main/examples/debate_simple.py) etc.\n\n🚀 Dec. 15, 2023: [v0.5.0](https://github.com/geekan/MetaGPT/releases/tag/v0.5.0) released, introducing some experimental features such as incremental development, multilingual, multiple programming languages, etc.\n\n🔥 Nov. 08, 2023: MetaGPT is selected into [Open100: Top 100 Open Source achievements](https://www.benchcouncil.org/evaluation/opencs/annual.html).\n\n🔥 Sep. 01, 2023: MetaGPT tops GitHub Trending Monthly for the **17th time** in August 2023.\n\n🌟 Jun. 30, 2023: MetaGPT is now open source.\n\n🌟 Apr. 24, 2023: First line of MetaGPT code committed.\n\n## Software Company as Multi-Agent System\n\n1. MetaGPT takes a **one line requirement** as input and outputs **user stories / competitive analysis / requirements / data structures / APIs / documents, etc.**\n2. Internally, MetaGPT includes **product managers / architects / project managers / engineers.** It provides the entire process of a **software company along with carefully orchestrated SOPs.**\n   1. `Code = SOP(Team)` is the core philosophy. We materialize SOP and apply it to teams composed of LLMs.\n\n![A software company consists of LLM-based roles](docs/resources/software_company_cd.jpeg)\n\n<p align=\"center\">Software Company Multi-Agent Schematic (Gradually Implementing)</p>\n\n## Get Started\n\n### Installation\n\n> Ensure that Python 3.9+ is installed on your system. You can check this by using: `python --version`.  \n> You can use conda like this: `conda create -n metagpt python=3.9 && conda activate metagpt`\n\n```bash\npip install --upgrade metagpt\n# or `pip install --upgrade git+https://github.com/geekan/MetaGPT.git`\n# or `git clone https://github.com/geekan/MetaGPT && cd MetaGPT && pip install --upgrade -e .`\n```\n\nFor detailed installation guidance, please refer to [cli_install](https://docs.deepwisdom.ai/main/en/guide/get_started/installation.html#install-stable-version)\n or [docker_install](https://docs.deepwisdom.ai/main/en/guide/get_started/installation.html#install-with-docker)\n\n### Configuration\n\nYou can init the config of MetaGPT by running the following command, or manually create `~/.metagpt/config2.yaml` file:\n```bash\n# Check https://docs.deepwisdom.ai/main/en/guide/get_started/configuration.html for more details\nmetagpt --init-config  # it will create ~/.metagpt/config2.yaml, just modify it to your needs\n```\n\nYou can configure `~/.metagpt/config2.yaml` according to the [example](https://github.com/geekan/MetaGPT/blob/main/config/config2.example.yaml) and [doc](https://docs.deepwisdom.ai/main/en/guide/get_started/configuration.html):\n\n```yaml\nllm:\n  api_type: \"openai\"  # or azure / ollama / groq etc. Check LLMType for more options\n  model: \"gpt-4-turbo\"  # or gpt-3.5-turbo\n  base_url: \"https://api.openai.com/v1\"  # or forward url / other llm url\n  api_key: \"YOUR_API_KEY\"\n```\n\n### Usage\n\nAfter installation, you can use MetaGPT at CLI\n\n```bash\nmetagpt \"Create a 2048 game\"  # this will create a repo in ./workspace\n```\n\nor use it as library\n\n```python\nfrom metagpt.software_company import generate_repo, ProjectRepo\nrepo: ProjectRepo = generate_repo(\"Create a 2048 game\")  # or ProjectRepo(\"<path>\")\nprint(repo)  # it will print the repo structure with files\n```\n\nYou can also use [Data Interpreter](https://github.com/geekan/MetaGPT/tree/main/examples/di) to write code:\n\n```python\nimport asyncio\nfrom metagpt.roles.di.data_interpreter import DataInterpreter\n\nasync def main():\n    di = DataInterpreter()\n    await di.run(\"Run data analysis on sklearn Iris dataset, include a plot\")\n\nasyncio.run(main())  # or await main() in a jupyter notebook setting\n```\n\n\n### QuickStart & Demo Video\n- Try it on [MetaGPT Huggingface Space](https://huggingface.co/spaces/deepwisdom/MetaGPT)\n- [Matthew Berman: How To Install MetaGPT - Build A Startup With One Prompt!!](https://youtu.be/uT75J_KG_aY)\n- [Official Demo Video](https://github.com/geekan/MetaGPT/assets/2707039/5e8c1062-8c35-440f-bb20-2b0320f8d27d)\n\nhttps://github.com/geekan/MetaGPT/assets/34952977/34345016-5d13-489d-b9f9-b82ace413419\n\n## Tutorial\n\n- 🗒 [Online Document](https://docs.deepwisdom.ai/main/en/)\n- 💻 [Usage](https://docs.deepwisdom.ai/main/en/guide/get_started/quickstart.html)  \n- 🔎 [What can MetaGPT do?](https://docs.deepwisdom.ai/main/en/guide/get_started/introduction.html)\n- 🛠 How to build your own agents? \n  - [MetaGPT Usage & Development Guide | Agent 101](https://docs.deepwisdom.ai/main/en/guide/tutorials/agent_101.html)\n  - [MetaGPT Usage & Development Guide | MultiAgent 101](https://docs.deepwisdom.ai/main/en/guide/tutorials/multi_agent_101.html)\n- 🧑‍💻 Contribution\n  - [Develop Roadmap](docs/ROADMAP.md)\n- 🔖 Use Cases\n  - [Data Interpreter](https://docs.deepwisdom.ai/main/en/guide/use_cases/agent/interpreter/intro.html)\n  - [Debate](https://docs.deepwisdom.ai/main/en/guide/use_cases/multi_agent/debate.html)\n  - [Researcher](https://docs.deepwisdom.ai/main/en/guide/use_cases/agent/researcher.html)\n  - [Recepit Assistant](https://docs.deepwisdom.ai/main/en/guide/use_cases/agent/receipt_assistant.html)\n- ❓ [FAQs](https://docs.deepwisdom.ai/main/en/guide/faq.html)\n\n## Support\n\n### Discord Join US\n\n📢 Join Our [Discord Channel](https://discord.gg/ZRHeExS6xv)! Looking forward to seeing you there! 🎉\n\n### Contributor form\n\n📝 [Fill out the form](https://airtable.com/appInfdG0eJ9J4NNL/pagK3Fh1sGclBvVkV/form) to become a contributor. We are looking forward to your participation!\n\n### Contact Information\n\nIf you have any questions or feedback about this project, please feel free to contact us. We highly appreciate your suggestions!\n\n- **Email:** alexanderwu@deepwisdom.ai\n- **GitHub Issues:** For more technical inquiries, you can also create a new issue in our [GitHub repository](https://github.com/geekan/metagpt/issues).\n\nWe will respond to all questions within 2-3 business days.\n\n## Citation\n\nTo stay updated with the latest research and development, follow [@MetaGPT_](https://twitter.com/MetaGPT_) on Twitter. \n\nTo cite [MetaGPT](https://arxiv.org/abs/2308.00352) or [Data Interpreter](https://arxiv.org/abs/2402.18679) in publications, please use the following BibTeX entries.\n\n```bibtex\n@misc{hong2023metagpt,\n      title={MetaGPT: Meta Programming for A Multi-Agent Collaborative Framework}, \n      author={Sirui Hong and Mingchen Zhuge and Jonathan Chen and Xiawu Zheng and Yuheng Cheng and Ceyao Zhang and Jinlin Wang and Zili Wang and Steven Ka Shing Yau and Zijuan Lin and Liyang Zhou and Chenyu Ran and Lingfeng Xiao and Chenglin Wu and Jürgen Schmidhuber},\n      year={2023},\n      eprint={2308.00352},\n      archivePrefix={arXiv},\n      primaryClass={cs.AI}\n}\n@misc{hong2024data,\n      title={Data Interpreter: An LLM Agent For Data Science}, \n      author={Sirui Hong and Yizhang Lin and Bang Liu and Bangbang Liu and Binhao Wu and Danyang Li and Jiaqi Chen and Jiayi Zhang and Jinlin Wang and Li Zhang and Lingyao Zhang and Min Yang and Mingchen Zhuge and Taicheng Guo and Tuo Zhou and Wei Tao and Wenyi Wang and Xiangru Tang and Xiangtao Lu and Xiawu Zheng and Xinbing Liang and Yaying Fei and Yuheng Cheng and Zongze Xu and Chenglin Wu},\n      year={2024},\n      eprint={2402.18679},\n      archivePrefix={arXiv},\n      primaryClass={cs.AI}\n}\n\n```\n\n"
    },
    {
      "name": "albertaga27/aoai-insurance-rm-vanilla-agents",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/159526192?s=40&v=4",
      "owner": "albertaga27",
      "repo_name": "aoai-insurance-rm-vanilla-agents",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2024-10-02T06:30:26Z",
      "updated_at": "2025-01-17T08:55:33Z",
      "topics": [],
      "readme": "AI Agentic Framework Solution Accelerator for Insurance Advisors\nWelcome to the AI Agentic Framework Solution Accelerator for Insurance Advisors. This repository contains a comprehensive solution that empowers insurance advisors by leveraging AI-driven agentic frameworks. The solution comprises a backend Azure Function app and a frontend Streamlit web application.\n\nOverview\nThis solution provides an AI-powered assistant designed specifically for insurance advisors. It utilizes OpenAI's GPT-4 architecture to understand user queries, maintain conversation history, and deliver intelligent, context-aware responses. The assistant is capable of handling complex interactions, making it a valuable tool for advisors seeking to enhance client engagement.\n\nFeatures\nIntelligent Assistant: Offers context-aware and insightful responses to user inquiries.\nConversation Management: Stores and retrieves conversation history using Azure Cosmos DB.\nAgentic Framework: Implements an agentic workflow to manage conversations and agent interactions.\nUser-Friendly Frontend: Streamlit web application provides an intuitive interface.\nScalable Backend: Azure Function app ensures reliable and scalable processing.\nCustomizable: Easily extendable to fit specific business requirements.\nArchitecture\nThe solution consists of two main components:\n\nBackend: An Azure Function app (http_trigger) that handles HTTP requests, processes user messages, manages conversation history, and interacts with AI agents using the genai_vanilla_agents framework.\n\nFrontend: A Streamlit web application that serves as the user interface, allowing insurance advisors to interact seamlessly with the AI assistant.\n\nPrerequisites\nPython 3.8+\nAzure Account: For deploying the Azure Function app and using Azure Cosmos DB.\nOpenAI API Key: Access to GPT-4 functionalities.\nAzure Cosmos DB: For storing conversation histories.\nAzure Functions Core Tools: For local development and deployment.\nGit: Version control system\n\nAzure resources:\nAzure Function App: To host your backend function.\nAzure Cosmos DB Account: With a database and container to store conversation histories.\nStorage Account: Required for the Function App.\nApplication Insights: For monitoring.\nManaged Identity: So the Function App can securely access Cosmos DB using DefaultAzureCredential.\nApp Service Plan: For the Streamlit app if you plan to deploy it to Azure (optional)."
    },
    {
      "name": "Bryan-Roe-ai/semantic-kernel",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/190993195?s=40&v=4",
      "owner": "Bryan-Roe-ai",
      "repo_name": "semantic-kernel",
      "description": "Integrate cutting-edge LLM technology quickly and easily into your apps",
      "homepage": "https://bryan-roe-ai.github.io/semantic-kernel/",
      "language": "C#",
      "created_at": "2023-12-07T00:12:04Z",
      "updated_at": "2025-04-20T17:58:21Z",
      "topics": [],
      "readme": "# Semantic Kernel\n\nIntegrate cutting-edge LLM technology quickly and easily into your apps.\n\n## Table of Contents\n1. [Introduction](#introduction)\n2. [Getting Started](#getting-started)\n3. [Usage](#usage)\n4. [Configuration](#configuration)\n5. [Workflows](#workflows)\n6. [Branch Protection Rules](#branch-protection-rules)\n7. [CI/CD Pipeline Efficiency](#cicd-pipeline-efficiency)\n8. [Contributing](#contributing)\n9. [License](#license)\n\n## Introduction\nProvide an introduction to the project, its goals, and key features.\n\n## Getting Started\nDetailed instructions on how to set up and start using the project.\n\n## Usage\n### Using Semantic Kernel in C#\n![C# Logo](https://user-images.githubusercontent.com/371009/230673036-fad1e8e6-5d48-49b1-a9c1-6f9834e0d165.png)\n\n[Using Semantic Kernel in C#](dotnet/README.md)\n\n### Using Semantic Kernel in Python\n![Python Logo](https://raw.githubusercontent.com/devicons/devicon/master/icons/python/python-original.svg)\n\n[Using Semantic Kernel in Python](python/README.md)\n\nInclude examples and explanations for other supported languages.\n\n## Configuration\nDetailed steps to configure the project.\n\n## Workflows\n### Handling 'Not Found' Error\nWe have added a new workflow to handle the \"Not Found\" error.\n\n#### Configuration\nTo configure the new workflow, follow these steps:\n\n1. **Create a new workflow file**: Add a new workflow file `.github/workflows/handle-not-found-error.yml`.\n2. **Define the workflow**: Add the following content to the file:\n    ```yaml\n    name: Handle Not Found Error\n    on:\n      pull_request:\n        types: [opened, synchronize]\n    jobs:\n      handle-not-found-error:\n        runs-on: ubuntu-latest\n        steps:\n          - name: Checkout code\n            uses: actions/checkout@v4\n          - name: Check for Not Found Error\n            run: |\n              echo \"Checking for Not Found error...\"\n              echo \"Handling Not Found error...\"\n    ```\n3. **Run the workflow**: The workflow will automatically run on pull request events.\n\n### SSRF Detection\nInclude detailed steps and explanations for the SSRF detection workflow.\n\n## Branch Protection Rules\nExplain the branch protection rules and include a link to the GitHub documentation on branch protection.\n\n## CI/CD Pipeline Efficiency\n### Parallel Jobs\nModify `.circleci/config.yml` to run `test`, `build`, and `deploy` jobs in parallel.\n\n### Caching\nImplement Docker layer caching in `.github/workflows/azure-container-webapp.yml`.\n\n### Multi-Stage Builds\nUse multi-stage Docker builds in `.github/workflows/azure-container-webapp.yml`.\n\n### Automation of Issue Management\nAdd auto-labeling and auto-assigning logic in `.github/workflows/label-issues.yml`.\n\n## Contributing\nProvide guidelines for contributing, reporting issues, and requesting features. Link to `CONTRIBUTING.md` if it exists.\n\n   on:\n     push:\n       branches: [ \"main\" ]\n     pull_request:\n       branches: [ \"main\" ]\n     schedule:\n       - cron: '0 0 * * 0'\n\n   jobs:\n     ssrf-detection:\n       runs-on: ubuntu-latest\n       steps:\n         - name: Checkout code\n           uses: actions/checkout@v4\n\n         - name: Run SSRF detection\n           run: |\n             # Add your SSRF detection script or tool here\n             echo \"Running SSRF detection...\"\n   ```\n\n3. **Run the workflow**: The workflow will automatically run on push, pull request, and scheduled events. It will detect and address any SSRF vulnerabilities in the codebase.\n\nBy following these steps, you can ensure that SSRF vulnerabilities are detected and addressed in your codebase, enhancing the security of your project.\n\n## New Workflow for Handling \"Not Found\" Error\n\nWe have added a new workflow to handle the \"Not Found\" error when trying to use GitHub Copilot workspace on pull requests. This workflow is designed to detect and address the specific error.\n\n### Configuration\n\nTo configure the new workflow, follow these steps:\n\n1. **Create a new workflow file**: Add a new workflow file named `.github/workflows/handle-not-found-error.yml` to the repository.\n\n2. **Define the workflow**: Add the following content to the workflow file:\n\n   ```yaml\n   name: Handle Not Found Error\n\n   on:\n     pull_request:\n       types: [opened, synchronize]\n\n   jobs:\n     handle-not-found-error:\n       runs-on: ubuntu-latest\n       steps:\n         - name: Checkout code\n           uses: actions/checkout@v4\n\n         - name: Check for Not Found Error\n           run: |\n             # Add your script or tool to check for the Not Found error here\n             echo \"Checking for Not Found error...\"\n             # Handle the Not Found error appropriately\n             echo \"Handling Not Found error...\"\n   ```\n\n3. **Run the workflow**: The workflow will automatically run on pull request events. It will detect and address the \"Not Found\" error when trying to use GitHub Copilot workspace on pull requests.\n\nBy following these steps, you can ensure that the \"Not Found\" error is detected and addressed in your codebase, enhancing the functionality of your project.\n\n## Contribution Guidelines\n\nWe welcome contributions from the community! To contribute to this project, please follow these guidelines:\n\n1. **Fork the repository**: Create a fork of the repository to work on your changes.\n\n2. **Create a branch**: Create a new branch for your changes.\n\n   ```bash\n   git checkout -b my-feature-branch\n   ```\n\n3. **Make your changes**: Implement your changes in the new branch.\n\n4. **Test your changes**: Ensure that your changes do not break any existing functionality and pass all tests.\n\n5. **Commit your changes**: Commit your changes with a descriptive commit message.\n\n   ```bash\n   git commit -m \"Add new feature\"\n   ```\n\n6. **Push your changes**: Push your changes to your forked repository.\n\n   ```bash\n   git push origin my-feature-branch\n   ```\n\n7. **Create a pull request**: Open a pull request to merge your changes into the main repository.\n\n8. **Review and feedback**: Address any feedback or comments from the maintainers during the review process.\n\n9. **Merge**: Once your pull request is approved, it will be merged into the main repository.\n\nThank you for your contributions!\n\nFor more detailed guidelines on contributing, refer to the `CONTRIBUTING.md` file in the root directory of the repository.\n"
    },
    {
      "name": "balakreshnan/ConstRFP",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/25058438?s=40&v=4",
      "owner": "balakreshnan",
      "repo_name": "ConstRFP",
      "description": "Construction, Architecture, Engineering",
      "homepage": null,
      "language": "Python",
      "created_at": "2024-09-27T22:07:39Z",
      "updated_at": "2025-01-07T18:43:02Z",
      "topics": [],
      "readme": "# ConstRFP\nConstruction, Architecture, Engineering\n"
    },
    {
      "name": "patooworld/autogen",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/114192133?s=40&v=4",
      "owner": "patooworld",
      "repo_name": "autogen",
      "description": "Enable Next-Gen Large Language Model Applications Unification in one place. designed and polished with Next-Gen AI elevator ''KnowALL\"",
      "homepage": "https://patooworld.github.io/autogen/",
      "language": "Jupyter Notebook",
      "created_at": "2024-01-28T04:01:28Z",
      "updated_at": "2024-10-10T07:09:25Z",
      "topics": [
        "javascript",
        "jupyter-notebook",
        "mdx",
        "python"
      ],
      "readme": "<a name=\"readme-top\"></a>\n\n\n<div align=\"center\">\n\n<img src=\"https://microsoft.github.io/autogen/img/ag.svg\" alt=\"AutoGen Logo\" width=\"100\">\n\n![Python Version](https://img.shields.io/badge/3.8%20%7C%203.9%20%7C%203.10%20%7C%203.11%20%7C%203.12-blue) [![PyPI - Version](https://img.shields.io/pypi/v/autogen-agentchat)](https://pypi.org/project/autogen-agentchat/)\n[![NuGet version](https://badge.fury.io/nu/AutoGen.Core.svg)](https://badge.fury.io/nu/AutoGen.Core)\n\n[![Twitter](https://img.shields.io/twitter/url/https/twitter.com/cloudposse.svg?style=social&label=Follow%20%40pyautogen)](https://twitter.com/pyautogen)\n\n</div>\n\n# AutoGen\n\nAutoGen is an open-source programming framework for building AI agents and facilitating cooperation among multiple agents to solve tasks. AutoGen aims to streamline the development and research of agentic AI, much like PyTorch does for Deep Learning. It offers features such as agents capable of interacting with each other, facilitates the use of various large language models (LLMs) and tool use support, autonomous and human-in-the-loop workflows, and multi-agent conversation patterns.\n\n> [!IMPORTANT]\n> In order to better align with a new multi-packaging structure we have coming very soon, AutoGen is now available on PyPi as [`autogen-agentchat`](https://pypi.org/project/autogen-agentchat/) as of version `0.2.36`. This is the official package for the AutoGen project.\n\n\n> [!NOTE]\n> *Note for contributors and users*</b>: [microsoft/autogen](https://aka.ms/autogen-gh) is the original repository of AutoGen project and it is under active development and maintenance under MIT license. We welcome contributions from developers and organizations worldwide. Our goal is to foster a collaborative and inclusive community where diverse perspectives and expertise can drive innovation and enhance the project's capabilities. We acknowledge the invaluable contributions from our existing contributors, as listed in [contributors.md](./CONTRIBUTORS.md). Whether you are an individual contributor or represent an organization, we invite you to join us in shaping the future of this project. For further information please also see [Microsoft open-source contributing guidelines](https://github.com/microsoft/autogen?tab=readme-ov-file#contributing).\n>\n> -_Maintainers (Sept 6th, 2024)_\n\n\n![AutoGen Overview](https://github.com/microsoft/autogen/blob/main/website/static/img/autogen_agentchat.png)\n\n- AutoGen enables building next-gen LLM applications based on [multi-agent conversations](https://microsoft.github.io/autogen/docs/Use-Cases/agent_chat) with minimal effort. It simplifies the orchestration, automation, and optimization of a complex LLM workflow. It maximizes the performance of LLM models and overcomes their weaknesses.\n- It supports [diverse conversation patterns](https://microsoft.github.io/autogen/docs/Use-Cases/agent_chat#supporting-diverse-conversation-patterns) for complex workflows. With customizable and conversable agents, developers can use AutoGen to build a wide range of conversation patterns concerning conversation autonomy,\n  the number of agents, and agent conversation topology.\n- It provides a collection of working systems with different complexities. These systems span a [wide range of applications](https://microsoft.github.io/autogen/docs/Use-Cases/agent_chat#diverse-applications-implemented-with-autogen) from various domains and complexities. This demonstrates how AutoGen can easily support diverse conversation patterns.\n- AutoGen provides [enhanced LLM inference](https://microsoft.github.io/autogen/docs/Use-Cases/enhanced_inference#api-unification). It offers utilities like API unification and caching, and advanced usage patterns, such as error handling, multi-config inference, context programming, etc.\n\nAutoGen was created out of collaborative [research](https://microsoft.github.io/autogen/docs/Research) from Microsoft, Penn State University, and the University of Washington.\n\n<p align=\"right\" style=\"font-size: 14px; color: #555; margin-top: 20px;\">\n  <a href=\"#readme-top\" style=\"text-decoration: none; color: blue; font-weight: bold;\">\n    ↑ Back to Top ↑\n  </a>\n</p>\n\n\n\n## News\n<details>\n\n<summary>Expand</summary>\n\n:fire: June 6, 2024: WIRED publishes a new article on AutoGen: [Chatbot Teamwork Makes the AI Dream Work](https://www.wired.com/story/chatbot-teamwork-makes-the-ai-dream-work/) based on interview with [Adam Fourney](https://github.com/afourney).\n\n:fire: June 4th, 2024: Microsoft Research Forum publishes new update and video on [AutoGen and Complex Tasks](https://www.microsoft.com/en-us/research/video/autogen-update-complex-tasks-and-agents/) presented by [Adam Fourney](https://github.com/afourney).\n\n:fire: May 29, 2024: DeepLearning.ai launched a new short course [AI Agentic Design Patterns with AutoGen](https://www.deeplearning.ai/short-courses/ai-agentic-design-patterns-with-autogen), made in collaboration with Microsoft and Penn State University, and taught by AutoGen creators [Chi Wang](https://github.com/sonichi) and [Qingyun Wu](https://github.com/qingyun-wu).\n\n:fire: May 24, 2024: Foundation Capital published an article on [Forbes: The Promise of Multi-Agent AI](https://www.forbes.com/sites/joannechen/2024/05/24/the-promise-of-multi-agent-ai/?sh=2c1e4f454d97) and a video [AI in the Real World Episode 2: Exploring Multi-Agent AI and AutoGen with Chi Wang](https://www.youtube.com/watch?v=RLwyXRVvlNk).\n\n:fire: May 13, 2024: [The Economist](https://www.economist.com/science-and-technology/2024/05/13/todays-ai-models-are-impressive-teams-of-them-will-be-formidable) published an article about multi-agent systems (MAS) following a January 2024 interview with [Chi Wang](https://github.com/sonichi).\n\n:fire: May 11, 2024: [AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation](https://openreview.net/pdf?id=uAjxFFing2) received the best paper award at the [ICLR 2024 LLM Agents Workshop](https://llmagents.github.io/).\n\n:fire: Apr 26, 2024: [AutoGen.NET](https://microsoft.github.io/autogen-for-net/) is available for .NET developers! Thanks [XiaoYun Zhang](https://www.linkedin.com/in/xiaoyun-zhang-1b531013a/)\n\n:fire: Apr 17, 2024: Andrew Ng cited AutoGen in [The Batch newsletter](https://www.deeplearning.ai/the-batch/issue-245/) and [What's next for AI agentic workflows](https://youtu.be/sal78ACtGTc?si=JduUzN_1kDnMq0vF) at Sequoia Capital's AI Ascent (Mar 26).\n\n:fire: Mar 3, 2024: What's new in AutoGen? 📰[Blog](https://microsoft.github.io/autogen/blog/2024/03/03/AutoGen-Update); 📺[Youtube](https://www.youtube.com/watch?v=j_mtwQiaLGU).\n\n:fire: Mar 1, 2024: the first AutoGen multi-agent experiment on the challenging [GAIA](https://huggingface.co/spaces/gaia-benchmark/leaderboard) benchmark achieved the No. 1 accuracy in all the three levels.\n\n<!-- :tada: Jan 30, 2024: AutoGen is highlighted by Peter Lee in Microsoft Research Forum [Keynote](https://t.co/nUBSjPDjqD). -->\n\n:tada: Dec 31, 2023: [AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation Framework](https://arxiv.org/abs/2308.08155) is selected by [TheSequence: My Five Favorite AI Papers of 2023](https://thesequence.substack.com/p/my-five-favorite-ai-papers-of-2023).\n\n<!-- :fire: Nov 24: pyautogen [v0.2](https://github.com/microsoft/autogen/releases/tag/v0.2.0) is released with many updates and new features compared to v0.1.1. It switches to using openai-python v1. Please read the [migration guide](https://microsoft.github.io/autogen/docs/Installation#python). -->\n\n<!-- :fire: Nov 11: OpenAI's Assistants are available in AutoGen and interoperatable with other AutoGen agents! Checkout our [blogpost](https://microsoft.github.io/autogen/blog/2023/11/13/OAI-assistants) for details and examples. -->\n\n:tada: Nov 8, 2023: AutoGen is selected into [Open100: Top 100 Open Source achievements](https://www.benchcouncil.org/evaluation/opencs/annual.html) 35 days after spinoff from [FLAML](https://github.com/microsoft/FLAML).\n\n<!-- :tada: Nov 6, 2023: AutoGen is mentioned by Satya Nadella in a [fireside chat](https://youtu.be/0pLBvgYtv6U). -->\n\n<!-- :tada: Nov 1, 2023: AutoGen is the top trending repo on GitHub in October 2023. -->\n\n<!-- :tada: Oct 03, 2023: AutoGen spins off from [FLAML](https://github.com/microsoft/FLAML) on GitHub. -->\n\n<!-- :tada: Aug 16: Paper about AutoGen on [arxiv](https://arxiv.org/abs/2308.08155). -->\n\n:tada: Mar 29, 2023: AutoGen is first created in [FLAML](https://github.com/microsoft/FLAML).\n\n<!--\n:fire: FLAML is highlighted in OpenAI's [cookbook](https://github.com/openai/openai-cookbook#related-resources-from-around-the-web).\n\n:fire: [autogen](https://microsoft.github.io/autogen/) is released with support for ChatGPT and GPT-4, based on [Cost-Effective Hyperparameter Optimization for Large Language Model Generation Inference](https://arxiv.org/abs/2303.04673).\n\n:fire: FLAML supports Code-First AutoML & Tuning – Private Preview in [Microsoft Fabric Data Science](https://learn.microsoft.com/en-us/fabric/data-science/). -->\n\n</details>\n\n## Roadmaps\n\nTo see what we are working on and what we plan to work on, please check our\n[Roadmap Issues](https://aka.ms/autogen-roadmap).\n\n<p align=\"right\" style=\"font-size: 14px; color: #555; margin-top: 20px;\">\n  <a href=\"#readme-top\" style=\"text-decoration: none; color: blue; font-weight: bold;\">\n    ↑ Back to Top ↑\n  </a>\n</p>\n\n## Quickstart\nThe easiest way to start playing is\n1. Click below to use the GitHub Codespace\n\n    [![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://codespaces.new/microsoft/autogen?quickstart=1)\n\n 2. Copy OAI_CONFIG_LIST_sample to ./notebook folder, name to OAI_CONFIG_LIST, and set the correct configuration.\n 3. Start playing with the notebooks!\n\n*NOTE*: OAI_CONFIG_LIST_sample lists GPT-4 as the default model, as this represents our current recommendation, and is known to work well with AutoGen. If you use a model other than GPT-4, you may need to revise various system prompts (especially if using weaker models like GPT-3.5-turbo). Moreover, if you use models other than those hosted by OpenAI or Azure, you may incur additional risks related to alignment and safety. Proceed with caution if updating this default.\n\n<p align=\"right\" style=\"font-size: 14px; color: #555; margin-top: 20px;\">\n  <a href=\"#readme-top\" style=\"text-decoration: none; color: blue; font-weight: bold;\">\n    ↑ Back to Top ↑\n  </a>\n</p>\n\n## [Installation](https://microsoft.github.io/autogen/docs/Installation)\n### Option 1. Install and Run AutoGen in Docker\n\nFind detailed instructions for users [here](https://microsoft.github.io/autogen/docs/installation/Docker#step-1-install-docker), and for developers [here](https://microsoft.github.io/autogen/docs/Contribute#docker-for-development).\n\n### Option 2. Install AutoGen Locally\n\nAutoGen requires **Python version >= 3.8, < 3.13**. It can be installed from pip:\n\n```bash\npip install autogen-agentchat~=0.2\n```\n\nMinimal dependencies are installed without extra options. You can install extra options based on the feature you need.\n\n<!-- For example, use the following to install the dependencies needed by the [`blendsearch`](https://microsoft.github.io/FLAML/docs/Use-Cases/Tune-User-Defined-Function#blendsearch-economical-hyperparameter-optimization-with-blended-search-strategy) option.\n```bash\npip install \"autogen-agentchat[blendsearch]~=0.2\"\n``` -->\n\nFind more options in [Installation](https://microsoft.github.io/autogen/docs/Installation#option-2-install-autogen-locally-using-virtual-environment).\n\n<!-- Each of the [`notebook examples`](https://github.com/microsoft/autogen/tree/main/notebook) may require a specific option to be installed. -->\n\nEven if you are installing and running AutoGen locally outside of docker, the recommendation and default behavior of agents is to perform [code execution](https://microsoft.github.io/autogen/docs/FAQ/#code-execution) in docker. Find more instructions and how to change the default behaviour [here](https://microsoft.github.io/autogen/docs/Installation#code-execution-with-docker-(default)).\n\nFor LLM inference configurations, check the [FAQs](https://microsoft.github.io/autogen/docs/FAQ#set-your-api-endpoints).\n\n<p align=\"right\" style=\"font-size: 14px; color: #555; margin-top: 20px;\">\n  <a href=\"#readme-top\" style=\"text-decoration: none; color: blue; font-weight: bold;\">\n    ↑ Back to Top ↑\n  </a>\n</p>\n\n## Multi-Agent Conversation Framework\n\nAutogen enables the next-gen LLM applications with a generic [multi-agent conversation](https://microsoft.github.io/autogen/docs/Use-Cases/agent_chat) framework. It offers customizable and conversable agents that integrate LLMs, tools, and humans.\nBy automating chat among multiple capable agents, one can easily make them collectively perform tasks autonomously or with human feedback, including tasks that require using tools via code.\n\nFeatures of this use case include:\n\n- **Multi-agent conversations**: AutoGen agents can communicate with each other to solve tasks. This allows for more complex and sophisticated applications than would be possible with a single LLM.\n- **Customization**: AutoGen agents can be customized to meet the specific needs of an application. This includes the ability to choose the LLMs to use, the types of human input to allow, and the tools to employ.\n- **Human participation**: AutoGen seamlessly allows human participation. This means that humans can provide input and feedback to the agents as needed.\n\nFor [example](https://github.com/microsoft/autogen/blob/main/test/twoagent.py),\n\n```python\nfrom autogen import AssistantAgent, UserProxyAgent, config_list_from_json\n# Load LLM inference endpoints from an env variable or a file\n# See https://microsoft.github.io/autogen/docs/FAQ#set-your-api-endpoints\n# and OAI_CONFIG_LIST_sample\nconfig_list = config_list_from_json(env_or_file=\"OAI_CONFIG_LIST\")\n# You can also set config_list directly as a list, for example, config_list = [{'model': 'gpt-4', 'api_key': '<your OpenAI API key here>'},]\nassistant = AssistantAgent(\"assistant\", llm_config={\"config_list\": config_list})\nuser_proxy = UserProxyAgent(\"user_proxy\", code_execution_config={\"work_dir\": \"coding\", \"use_docker\": False}) # IMPORTANT: set to True to run code in docker, recommended\nuser_proxy.initiate_chat(assistant, message=\"Plot a chart of NVDA and TESLA stock price change YTD.\")\n# This initiates an automated chat between the two agents to solve the task\n```\n\nThis example can be run with\n\n```python\npython test/twoagent.py\n```\n\nAfter the repo is cloned.\nThe figure below shows an example conversation flow with AutoGen.\n![Agent Chat Example](https://github.com/microsoft/autogen/blob/main/website/static/img/chat_example.png)\n\nAlternatively, the [sample code](https://github.com/microsoft/autogen/blob/main/samples/simple_chat.py) here allows a user to chat with an AutoGen agent in ChatGPT style.\nPlease find more [code examples](https://microsoft.github.io/autogen/docs/Examples#automated-multi-agent-chat) for this feature.\n\n<p align=\"right\" style=\"font-size: 14px; color: #555; margin-top: 20px;\">\n  <a href=\"#readme-top\" style=\"text-decoration: none; color: blue; font-weight: bold;\">\n    ↑ Back to Top ↑\n  </a>\n</p>\n\n## Enhanced LLM Inferences\n\nAutogen also helps maximize the utility out of the expensive LLMs such as ChatGPT and GPT-4. It offers [enhanced LLM inference](https://microsoft.github.io/autogen/docs/Use-Cases/enhanced_inference#api-unification) with powerful functionalities like caching, error handling, multi-config inference and templating.\n\n<!-- For example, you can optimize generations by LLM with your own tuning data, success metrics, and budgets.\n\n```python\n# perform tuning for openai<1\nconfig, analysis = autogen.Completion.tune(\n    data=tune_data,\n    metric=\"success\",\n    mode=\"max\",\n    eval_func=eval_func,\n    inference_budget=0.05,\n    optimization_budget=3,\n    num_samples=-1,\n)\n# perform inference for a test instance\nresponse = autogen.Completion.create(context=test_instance, **config)\n```\n\nPlease find more [code examples](https://microsoft.github.io/autogen/docs/Examples#tune-gpt-models) for this feature. -->\n\n<p align=\"right\" style=\"font-size: 14px; color: #555; margin-top: 20px;\">\n  <a href=\"#readme-top\" style=\"text-decoration: none; color: blue; font-weight: bold;\">\n    ↑ Back to Top ↑\n  </a>\n</p>\n\n## Documentation\n\nYou can find detailed documentation about AutoGen [here](https://microsoft.github.io/autogen/).\n\nIn addition, you can find:\n\n- [Research](https://microsoft.github.io/autogen/docs/Research), [blogposts](https://microsoft.github.io/autogen/blog) around AutoGen, and [Transparency FAQs](https://github.com/microsoft/autogen/blob/main/TRANSPARENCY_FAQS.md)\n\n- [Contributing guide](https://microsoft.github.io/autogen/docs/Contribute)\n\n- [Roadmap](https://github.com/orgs/microsoft/projects/989/views/3)\n\n<p align=\"right\" style=\"font-size: 14px; color: #555; margin-top: 20px;\">\n  <a href=\"#readme-top\" style=\"text-decoration: none; color: blue; font-weight: bold;\">\n    ↑ Back to Top ↑\n  </a>\n</p>\n\n## Related Papers\n\n[AutoGen Studio](https://www.microsoft.com/en-us/research/publication/autogen-studio-a-no-code-developer-tool-for-building-and-debugging-multi-agent-systems/)\n\n```\n@inproceedings{dibia2024studio,\n      title={AutoGen Studio: A No-Code Developer Tool for Building and Debugging Multi-Agent Systems},\n      author={Victor Dibia and Jingya Chen and Gagan Bansal and Suff Syed and Adam Fourney and Erkang (Eric) Zhu and Chi Wang and Saleema Amershi},\n      year={2024},\n      booktitle={Pre-Print}\n}\n```\n\n[AutoGen](https://aka.ms/autogen-pdf)\n\n```\n@inproceedings{wu2023autogen,\n      title={AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation Framework},\n      author={Qingyun Wu and Gagan Bansal and Jieyu Zhang and Yiran Wu and Beibin Li and Erkang Zhu and Li Jiang and Xiaoyun Zhang and Shaokun Zhang and Jiale Liu and Ahmed Hassan Awadallah and Ryen W White and Doug Burger and Chi Wang},\n      year={2024},\n      booktitle={COLM},\n}\n```\n\n[EcoOptiGen](https://arxiv.org/abs/2303.04673)\n\n```\n@inproceedings{wang2023EcoOptiGen,\n    title={Cost-Effective Hyperparameter Optimization for Large Language Model Generation Inference},\n    author={Chi Wang and Susan Xueqing Liu and Ahmed H. Awadallah},\n    year={2023},\n    booktitle={AutoML'23},\n}\n```\n\n[MathChat](https://arxiv.org/abs/2306.01337)\n\n```\n@inproceedings{wu2023empirical,\n    title={An Empirical Study on Challenging Math Problem Solving with GPT-4},\n    author={Yiran Wu and Feiran Jia and Shaokun Zhang and Hangyu Li and Erkang Zhu and Yue Wang and Yin Tat Lee and Richard Peng and Qingyun Wu and Chi Wang},\n    year={2023},\n    booktitle={ArXiv preprint arXiv:2306.01337},\n}\n```\n\n[AgentOptimizer](https://arxiv.org/pdf/2402.11359)\n\n```\n@article{zhang2024training,\n  title={Training Language Model Agents without Modifying Language Models},\n  author={Zhang, Shaokun and Zhang, Jieyu and Liu, Jiale and Song, Linxin and Wang, Chi and Krishna, Ranjay and Wu, Qingyun},\n  journal={ICML'24},\n  year={2024}\n}\n```\n\n[StateFlow](https://arxiv.org/abs/2403.11322)\n```\n@article{wu2024stateflow,\n  title={StateFlow: Enhancing LLM Task-Solving through State-Driven Workflows},\n  author={Wu, Yiran and Yue, Tianwei and Zhang, Shaokun and Wang, Chi and Wu, Qingyun},\n  journal={arXiv preprint arXiv:2403.11322},\n  year={2024}\n}\n```\n\n<p align=\"right\" style=\"font-size: 14px; color: #555; margin-top: 20px;\">\n  <a href=\"#readme-top\" style=\"text-decoration: none; color: blue; font-weight: bold;\">\n    ↑ Back to Top ↑\n  </a>\n</p>\n\n## Contributing\n\nThis project welcomes contributions and suggestions. Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit <https://cla.opensource.microsoft.com>.\n\nIf you are new to GitHub, [here](https://opensource.guide/how-to-contribute/#how-to-submit-a-contribution) is a detailed help source on getting involved with development on GitHub.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information, see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n<p align=\"right\" style=\"font-size: 14px; color: #555; margin-top: 20px;\">\n  <a href=\"#readme-top\" style=\"text-decoration: none; color: blue; font-weight: bold;\">\n    ↑ Back to Top ↑\n  </a>\n</p>\n\n## Contributors Wall\n<a href=\"https://github.com/microsoft/autogen/graphs/contributors\">\n  <img src=\"https://contrib.rocks/image?repo=microsoft/autogen&max=204\" />\n</a>\n\n<p align=\"right\" style=\"font-size: 14px; color: #555; margin-top: 20px;\">\n  <a href=\"#readme-top\" style=\"text-decoration: none; color: blue; font-weight: bold;\">\n    ↑ Back to Top ↑\n  </a>\n</p>\n\n# Legal Notices\n\nMicrosoft and any contributors grant you a license to the Microsoft documentation and other content\nin this repository under the [Creative Commons Attribution 4.0 International Public License](https://creativecommons.org/licenses/by/4.0/legalcode),\nsee the [LICENSE](LICENSE) file, and grant you a license to any code in the repository under the [MIT License](https://opensource.org/licenses/MIT), see the\n[LICENSE-CODE](LICENSE-CODE) file.\n\nMicrosoft, Windows, Microsoft Azure, and/or other Microsoft products and services referenced in the documentation\nmay be either trademarks or registered trademarks of Microsoft in the United States and/or other countries.\nThe licenses for this project do not grant you rights to use any Microsoft names, logos, or trademarks.\nMicrosoft's general trademark guidelines can be found at http://go.microsoft.com/fwlink/?LinkID=254653.\n\nPrivacy information can be found at https://go.microsoft.com/fwlink/?LinkId=521839\n\nMicrosoft and any contributors reserve all other rights, whether under their respective copyrights, patents,\nor trademarks, whether by implication, estoppel, or otherwise.\n\n<p align=\"right\" style=\"font-size: 14px; color: #555; margin-top: 20px;\">\n  <a href=\"#readme-top\" style=\"text-decoration: none; color: blue; font-weight: bold;\">\n    ↑ Back to Top ↑\n  </a>\n</p>\n"
    },
    {
      "name": "orellanoyamile/database_chatbot",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/131058655?s=40&v=4",
      "owner": "orellanoyamile",
      "repo_name": "database_chatbot",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2024-09-16T18:58:13Z",
      "updated_at": "2024-10-08T16:19:14Z",
      "topics": [],
      "readme": ""
    },
    {
      "name": "ms-us-rcg-cloud-innovation/openai-patterns",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/138236455?s=40&v=4",
      "owner": "ms-us-rcg-cloud-innovation",
      "repo_name": "openai-patterns",
      "description": null,
      "homepage": null,
      "language": "TSQL",
      "created_at": "2024-04-25T14:03:29Z",
      "updated_at": "2025-02-03T17:53:47Z",
      "topics": [],
      "readme": "# OpenAI Toolkit: Patterns and Practices for RAG / Bring Your Own Data\n\nHere, you'll find a collection of straightforward examples showcasing different ways to integrate and utilize OpenAI's capabilities. We've broken down each method into clear, manageable parts, making it easy for anyone to grasp and apply, regardless of their technical background. Our aim is to demystify AI, one pattern at a time, using plain language and practical examples. Whether you're a beginner or looking to expand your toolkit, this resource is designed for you!\n\n## OpenAI Review\n\nWhen delving into the world of OpenAI, there are two fundamental concepts that are essential to understand:\n\n- Chat Completion / General LLM: This refers to the use of large language models (LLM) like GPT (Generative Pre-trained Transformer) for generating text.\n\n- Embeddings: The process of converting data, such as text or images, into a vector (numeric) format. \n\n\n### GPT Models (LLM's)\n\nGPT (Generative Pre-trained Transformer) models are a type of artificial intelligence developed by OpenAI that specialize in generating text. These models are trained on vast amounts of text data to learn patterns, nuances, and the structure of human language. Once trained, GPT models can generate coherent, contextually relevant text based on the input they receive.\n\nThis diagram illustrates the interaction between a user and a GPT model. However, it's important to note that there is a bit more to the flow than it appears. \n![alt text](diagrams/gpt1.png)\n\nBefore responding, the user's query is first transformed (tokenized) and embedded by the GPT model, which helps in understanding the context and content of the question more effectively.\n![alt text](diagrams/gpt2.png)\n\nWe will explore this pattern in more detail in the RAG / Bring Your Own Data flows.\n\n### Embeddings\n\nEmbeddings are a powerful technique used to convert raw data—like text, images, or even sounds—into a structured, numerical format known as vectors. These vectors are constructed in such a way that they capture the essential aspects of the data, like meaning, context, or relationships, in a format that machines can understand and process.\n\n![alt text](diagrams/embeddingsGeneral.png)\n\n#### Embeddings Model Specifications\n\n| Model ID                             | Max Request (tokens) | Output Dimensions \n| ------------------------------------ | -------------------- | ----------------- |\n| `text-embedding-ada-002 (version 2)` | 8,191                | 1,536             |\n| `text-embedding-ada-002 (version 1)` | 2,046                | 1,536             |\n| `text-embedding-3-large`             | 8,191                | 3,072             |\n| `text-embedding-3-small`             | 8,191                | 1,536             |\n\n#### Understanding the Specifications\n\n- **Model ID**: Identifies the specific version of the embedding model\n  - The model you will specify when calling the embeddings endpoint\n- **Max Request (tokens)**: Indicates the maximum number of tokens that can be processed in a single request \n  - This comes into play when we have lots of data that will need to be embedded.\n  - When planning to embed a large ammount of data we will need to break it up into 'chunks' <= to the Max Request tokens of the respective embedding model  \n- **Output Dimensions**: Specifies the size of the output vector for each embedding.\n  - After embedding we will need a place to store the vector. A few data store options include Azure AI Search, MongoDB, Redis Enterprise \n  - When configuring the data store we need to ensure that the vector field dimensions are set to the output dimensions of our model\n\n## Retrieval Augmented Generation (RAG) / Bring Your Own Data (BYOD) Overview\n\nThe RAG pattern involves integrating a retrieval component with a generative model, such as GPT. This approach enables the model to pull relevant information from a predefined dataset or knowledge base before generating a response. By querying external data in real-time, the model can provide more accurate, informed, and contextually relevant outputs. The \"bring your own data\" aspect allows users to customize the external datasets according to their specific needs, thus grounding the model's responses in targeted, relevant information. This methodology not only improves the quality of the responses but also enhances the model’s ability to handle specific topics or detailed queries beyond its initial training data.\n\n#### Vector Store\n\nA common way of implementing RAG is via a vector store such as Azure AI Search, Mongo CosmosDB and Redis Enterprise. \n\nVector store databases have 2 primary fields of intrest\n\n- **Embedding**: The Vector representation of the content (Embedding)\n  - This is what we ultimately run the similarity search against.\n- **Content**: The actual text content that was embedded\n  - This is what is passed to the GPT model to then formulate the answer\n\n##### Getting Data Into a Vector Store\n\nAs mentioned, in order to get the data into a vector store, we first embed the data and store the embedding along with the text content.\n\n![alt text](diagrams/vectordb1.png)\n\n##### Getting Data out of the Vector Store\n\nIn order to get the data relevant to the user question out of the vector store we first embed the user question, perform a vector similarity search and return the corresponding content to the language model so that it can arrive at the answer based on stored information.\n\n![alt text](diagrams/vectordb2.png)\n\n\n\n\n\n\n\n\n\n\n\n"
    },
    {
      "name": "pnd-andrei/SmartResume",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/76188072?s=40&v=4",
      "owner": "pnd-andrei",
      "repo_name": "SmartResume",
      "description": "Smart Resume is an AI-powered application that reformulates and enhances resumes to a standardized format using AI models, making HR processes more efficient.",
      "homepage": "https://github.com/pnd-andrei/SmartResume",
      "language": "Python",
      "created_at": "2024-09-10T13:48:52Z",
      "updated_at": "2024-09-10T16:03:55Z",
      "topics": [],
      "readme": "# **Smart Resume**\n\n**Smart Resume** is an AI-powered application designed to streamline the process of reformatting and enhancing resumes. It takes resumes in PDF format and uses advanced AI models to restructure and reorganize them into a standardized format. This application is particularly useful for companies specializing in open-sourcing talent, allowing HR teams to efficiently search and enhance resumes based on specific project requirements.\n\n## **Features**\n\n- **Upload, Delete, and View Resumes**: A dedicated page where users can upload resumes in PDF format, view the uploaded resumes, or delete them from the database.\n- **Similarity Search**: Enables HR teams to perform a similarity search across the database using a query. The system will find resumes that closely match the query.\n- **Resume Enhancement**: AI-powered enhancement for resumes, optimizing their content to match the query. This feature is particularly useful for improving candidate fit for specific projects.\n- **Secure Authentication**: A robust email-based authentication system ensures that only authorized users can access the application.\n- **Modular AI Integration**: The application supports multiple AI models, allowing users to perform enhancement using OpenAI’s models, as well as other open-source models from Ollama and Hugging Face.\n\n## Technology Stack\n\n### **Backend**\n<p align=\"left\">\n  <img src=\"./assets/django.png\" alt=\"Django Logo\" width=\"50\" style=\"vertical-align: bottom; margin-right: 40px;\"/>\n  &nbsp;&nbsp;&nbsp;Django Rest Framework: Provides a flexible API interface for handling resume uploads, management, and querying.\n</p>\n\n<p align=\"left\">\n  <img src=\"./assets/sqlite.png\" alt=\"SQLite Logo\" width=\"50\" style=\"vertical-align: bottom; margin-right: 40px;\"/>\n  &nbsp;&nbsp;&nbsp;SQLite3: Stores the resumes and user information.\n</p>\n\n<p align=\"left\">\n  <img src=\"./assets/chroma.png\" alt=\"ChromaDB Logo\" width=\"40\" style=\"vertical-align: bottom; margin-right: 40px;\"/>\n  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;ChromaDB: A vector database used for similarity searches across resumes.\n</p>\n\n<p align=\"left\">\n  <img src=\"./assets/ngrok.png\" alt=\"ngrok Logo\" width=\"50\" style=\"vertical-align: bottom; margin-right: 40px;\"/>\n  &nbsp;&nbsp;&nbsp;Ngrok: Used for hosting the application and exposing local servers to the internet.\n</p>\n\n### **AI Models & Integration**\n<p align=\"left\">\n  &nbsp;&nbsp;\n  <img src=\"./assets/openai.png\" alt=\"OpenAI Logo\" width=\"30\" style=\"vertical-align: bottom; margin-right: 40px;\"/>\n  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;OpenAI: Utilizes advanced models for enhancing and restructuring resume content.\n</p>\n\n<p align=\"left\">\n  &nbsp;\n  <img src=\"./assets/ollama.png\" alt=\"Ollama Logo\" width=\"40\" style=\"vertical-align: bottom; margin-right: 40px;\"/>\n  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Ollama: Supports open-source models for AI-driven resume enhancement.\n</p>\n\n<p align=\"left\">\n  <img src=\"./assets/huggingface.png\" alt=\"Hugging Face Logo\" width=\"60\" style=\"vertical-align: bottom; margin-right: 40px;\"/>\n  &nbsp;&nbsp;&nbsp;Hugging Face: Leverages powerful transformers from Hugging Face for NLP tasks.<br>\n</p>\n\n<p align=\"left\">\n  <img src=\"./assets/langchain.png\" alt=\"LangChain Logo\" width=\"70\" style=\"vertical-align: bottom; margin-right: 40px;\"/>\n  &nbsp;LangChain & Semantic Kernel: Integrates the various AI models into the application.\n</p>\n\n## **How it Works**\n\n1. **Upload Resumes**: HR teams can upload resumes in PDF format through the user interface. The resumes are stored in an SQLite3 database.\n2. **Similarity Search**: Use a query to search through the database of resumes. The application performs a similarity search using ChromaDB, identifying resumes that are most relevant to the given query.\n3. **Resume Enhancement**: After selecting relevant resumes, users can enhance them using AI models. This involves reformatting and restructuring the resume’s content to better match the project requirements, ensuring a stronger match between the candidate’s skills and the project needs.\n4. **Flexible AI Models**: The modular nature of the application allows HR teams to choose between OpenAI’s proprietary models or any other open-source AI models (e.g., from Hugging Face or Ollama) for resume enhancement.\n\n## **Use Case**\n\nSmart Resume is particularly effective for HR teams in companies that specialize in **open-sourcing talent**. It allows them to:\n\n- Efficiently search through a large database of resumes using custom queries.\n- Automatically enhance the resumes of selected candidates to make them more suitable for specific projects.\n- Save time and improve the overall process of selecting and optimizing candidate resumes for particular projects.\n\nFor example, HR teams can query the resume database based on project-specific skills, identify potential candidates, and then enhance their resumes so that they better align with the project’s requirements before presenting them to clients.\n\n## Screenshots\n\nHere are some screenshots of the application:\n\n### Register Page\n<img src=\"./assets/register.png\" alt=\"Register Page\" width=\"300\"/>\n\n### Login Page\n<img src=\"./assets/login.png\" alt=\"Login Page\" width=\"300\"/>\n\n### Resumes Page\n<img src=\"./assets/resumes.png\" alt=\"Resumes Page\" width=\"300\"/>\n\n### Similarity search Page\n<img src=\"./assets/search.png\" alt=\"Similarity search Page\" width=\"300\"/>\n\n### Fine-tuning Options\n<img src=\"./assets/tuning.png\" alt=\"Fine-tuning options\" width=\"300\"/>\n\n### Enhancing Page\n<img src=\"./assets/smart_1.png\" alt=\"Enhancing Page\" width=\"300\"/>\n<img src=\"./assets/smart_2.png\" alt=\"Enhancing Page\" width=\"300\"/>\n\n## **Setup Instructions**\n\nTo get the application running locally:\n\n1. **Clone the repository**:\n    ```bash\n    git clone https://github.com/pnd-andrei/Smart-Resume.git\n    cd resumecentral\n    ```\n\n2. **Create a virtual environment**:\n    ```bash\n    python -m venv env\n    source env/bin/activate\n    ```\n\n3. **Install the required dependencies**:\n    ```bash\n    pip install -r requirements.txt\n    poetry install\n    ```\n\n4. **Run database migrations**:\n    ```bash\n    python manage.py migrate\n    ```\n\n5. **Run the application**:\n    ```bash\n    python manage.py runserver\n    ```\n\n6. **Set up Ngrok (for external access)**:\n    ```bash\n    ngrok http 8000\n    ```\n\n"
    },
    {
      "name": "Xin-Ray/AiCoach",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/113844301?s=40&v=4",
      "owner": "Xin-Ray",
      "repo_name": "AiCoach",
      "description": null,
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2024-08-20T01:07:38Z",
      "updated_at": "2024-08-25T14:57:34Z",
      "topics": [],
      "readme": "# AiCoach"
    },
    {
      "name": "kenichi-segawa/AI-Search-Example",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/54873006?s=40&v=4",
      "owner": "kenichi-segawa",
      "repo_name": "AI-Search-Example",
      "description": null,
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2024-08-21T13:56:24Z",
      "updated_at": "2024-10-19T02:01:46Z",
      "topics": [],
      "readme": "\n\n# Environment Variables\nTo run the notebook, the following environment variables need to be set:\n\n```plaintext\n# Azure AI Search\nAZURE_SEARCH_SERVICE_ENDPOINT=''\nAZURE_SEARCH_ADMIN_KEY=''\n\n# Storage Account\nAZURE_STORAGE_CONNECTION_STRING=''\nAZURE_STORAGE_CONTAINER_NAME=''\n\n# Azure OpenAI \nAZURE_OPENAI_ENDPOINT=''\nAZURE_OPENAI_API_KEY=''\nAZURE_OPENAI_EMBEDDING_DEPLOYED_MODEL=''\n"
    },
    {
      "name": "abirharrasse/LLM_Judging_Architectures",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/81148161?s=40&v=4",
      "owner": "abirharrasse",
      "repo_name": "LLM_Judging_Architectures",
      "description": "A framework for evaluating LLM outputs using LLMs as 🗣️ advocates, featuring a ⚖️ judge and jury system for dynamic assessment",
      "homepage": "",
      "language": "Python",
      "created_at": "2024-08-19T12:25:12Z",
      "updated_at": "2024-12-27T08:04:45Z",
      "topics": [],
      "readme": "# LLM-Based-Judging-Architectures\n\n<p align=\"center\">\n  <img src=\"images/architecture.png\" alt=\"Architectures Diagram\" width=\"90%\" />\n</p>\n\n## Background\nAs large language models (LLMs) evolve, evaluating their outputs becomes increasingly complex. Traditional methods, such as human assessments, are often costly and inconsistent, while automated metrics may fail to capture the nuances of LLM performance.\n\nTo address these challenges, we propose a novel framework that uses LLMs as **advocates** in a dynamic, **courtroom-inspired** system. This approach involves LLMs acting as advocates, judges, and juries to provide a comprehensive evaluation of model outputs.\n\nThis framework integrates decision theory, bounded rationality, and economic incentives to offer a robust evaluation method. Subsequent sections will detail the architecture and experiments validating its effectiveness.\n\n\n\n## Data\n\n### Dataset\n\nThis project uses the `lmsys/mt_bench_human_judgments` dataset from Hugging Face.\n\n### Preprocessing\n\nThe script `src/preprocess_mt_bench.py` processes the raw data into an Excel file (`data/mt_bench_human_judgments.xlsx`) with the following columns:\n\n- **Question**: Aggregated user questions.\n- **Response_A**: Responses from Model A.\n- **Response_B**: Responses from Model B.\n- **Model_A_Score**: Binary score for Model A (1 for a win, 0 for a loss).\n- **Model_B_Score**: Binary score for Model B (1 for a win, 0 for a loss).\n\nRun the preprocessing with:\n```bash\npython data/preprocess_mt_bench.py\n```\n## Installation and Setup\n\nOur agentic architecture is built on **MetaGPT**, a framework designed to efficiently manage interactions between agents using shared memory.\n\nTo use the MetaGPT framework, follow these steps:\n\n1. **Clone the Repository**\n   \nFirst, clone the repository and navigate to the relevant directory:\n```bash\ngit clone https://github.com/abirharrasse/LLM-Judging-Architectures  && cd LLM-Judging-Architectures/MetaGPT_LLM_advocates\n```\n3. **Install Dependencies**\n   \nInstall the necessary packages:\n```bash\npip install --upgrade -e .\npip install together -q\n```\n3. Initialize Configuration\n\nSet up the configuration file for the MetaGPT framework:\n```bash\nmetagpt --init-config\n```\nBefore running this command, ensure you're in the correct directory:\n```bash\nimport os\nos.chdir('/content/LLM-Judging-Architectures/MetaGPT_LLM_advocates')\nprint(os.getcwd())\n```\n4. Set API Keys\n   \nSet the required API keys to run your experiments:\n```bash\nos.environ['OPENAI_API_KEY'] = 'sk-'\nos.environ['TOGETHER_API_KEY'] = '' \nos.environ['CLAUDE_API_KEY'] = ''\nos.environ['GEMINI_API_KEY'] = ''\nos.environ['COHERE_API_KEY'] = ''\n```\n## Running the evaluation architectures\n\n#### Single Advocate Multi-Round Evaluation (SAMRE): \nTo run the **SAMRE** evaluation framework:\nBegin by accessing the directory `LLM_Judging_Architectures`:\n```bash\nimport os\nos.chdir('/content/LLM_Judging_Architectures')\nprint(os.getcwd())\n```\nthen call the `samre_experiment` function with the appropriate parameters: \n```bash\n! python utils/experiments.py --experiment samre --model_pl \"mistral\" --temperature 0.7 --question \"What is the impact of AI on healthcare?\" --answer1 \"AI can improve diagnostic accuracy.\" --answer2 \"AI might introduce bias in diagnosis.\" --investment 0.1 --n_rounds 4 --n_juries 3\n\n```\nWhere:\n\n- **model**: Select one of the models supported by our framework, accessible via Together or OpenAI.\n- **question**: The question to which answer1 and answer2 respond.\n- **answer1** and **answer2**: The answers to be evaluated.\n- **investment**: The maximum cost allocated for the experiment.\n- **n_rounds**: The number of evaluation rounds to conduct.\n- **n_juries**: The number of juries involved in the evaluation.\n\n\n\n#### Multi-Advocate One-Round Evaluation (MORE) \n\nTo run the **MORE** evaluation framework:\nBegin by accessing the directory `LLM_Judging_Architectures`:\n```bash\nimport os\nos.chdir('/content/LLM_Judging_Architectures')\nprint(os.getcwd())\n```\nthen call the `more_experiment` function with the appropriate parameters: \n```bash\n! python utils/experiments.py --experiment more --model_pl \"mistral\" --temperature 0.7 --question \"How does AI influence education?\" --answer1 \"AI personalizes learning.\" --answer2 \"AI could limit creativity.\" --n_advocates 2 --investment 3  --n_rounds 1\n\n\n```\nWhere:\n- **n_advocates**: The number of advocates involved in the evaluation.\n- **n_rounds**: The number of evaluation rounds to conduct.\n\n\n## Full Dataset Evaluation\n\nTo execute our architectures on the entire `mt-bench` dataset and ompare it to the `basemodel.py`, use the following code:\n```bash\n!python utils/main.py\n```\n"
    },
    {
      "name": "pablosalvador10/gbb-ai-smart-document-processing",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/31255154?s=40&v=4",
      "owner": "pablosalvador10",
      "repo_name": "gbb-ai-smart-document-processing",
      "description": "Leveraging Azure AI to intelligently classify, tag, and extract information from documents at scale. 📄✨",
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2024-08-11T16:12:24Z",
      "updated_at": "2025-03-13T16:29:46Z",
      "topics": [],
      "readme": "# 🚀 Advanced OCR, Intelligent Tagging, and NER with Azure AI \n\nAzure AI offers advanced capabilities for document classification, enabling enterprises to automate and streamline document processing workflows. This project leverages Azure AI Document Intelligence, Large Language Models (LLMs), and Small Language Models (SLMs) to accurately classify and extract information from commercial documents like invoices, receipts, and contracts. The result is searchable, structured data from previously unstructured sources, enhanced by semantic natural language understanding with a vector-based approach.\n\nWe will explore the latest technologies and approaches, helping you make informed decisions to leverage advanced Optical Character Recognition (OCR), Named Entity Recognition (NER), summarization, vectorization, and indexing to make your data searchable.\n\nFor more context and a detailed explanation, please refer to the full blog post [here](https://pabloaicorner.hashnode.dev/modernize-your-document-management-with-azure-ai-and-generative-ai-advance-ocr-intelligent-tagging-and-ner).\n\n## 🧭 Guide to Decision-Making \n\nChoosing the right technology is crucial in engineering decision-making. Refer to the mind map below for guidance on aligning your needs with the best approach.\n\n![Mind Map](utils/images/image.png)\n\nBelow are the approaches to various technologies and methods, each linked to a corresponding notebook for detailed code examples and evaluation methodology.\n\n### 1. Define Your Success Thresholds\n\n- **Evaluation Criteria**: Learn how to choose the right methodology to compare multiple approaches using quality metrics like accuracy. For a multiclass problem like our use case, see the code [here](01-build-evaluation-methodology.ipynb).\n\n### 2. Document Classification (OCR)\n\n- **OCR + LLM**: First, scan the document using OCR, then pass it to an LLM/SLM for extracting the targeted information. See the code [here](04-classification-document-ocr-llm.ipynb).\n- **Leveraging Multimodality**: Utilize advanced multimodal models like GPT-4 Omni or Phi-3 Vision that can directly accept images for classification. See the code [here](02-classification-document-llm-slm-multimodal.ipynb).\n- **Fine-Tuning Neural Document Intelligence Models**: Fine-tune pre-trained Azure AI Document Intelligence models with your own data for improved accuracy. See the code [here](03-classification-custom-document-intelligence.ipynb).\n\n### 3. Extracting Content\n\n- **Leveraging Multimodality**: Utilize advanced multimodal models like GPT-4 Omni to extract content and summarize per document. See the code [here](05-entity-extraction-document-intelligence.ipynb).\n\n### 4. Make Your Data Searchable\n\n- **Vectorization and Indexing**: Use Azure OpenAI to vectorize and the Push SDK to index documents into Azure AI Search, enabling state-of-the-art retrieval approaches and advanced search capabilities. See the code [here](06-make-your-data-searchable.ipynb).\n\n## 📂 Case Study: Making Your Enterprise Unlabeled Document Archives Searchable\n\n### Problem\n\nEnterprises often have millions of documents stored in their archives. These documents are typically unprocessed or manually processed, leading to inefficiencies and potential human error. Our objective is to categorize (\"tag\") these documents into 16 initial categories using fine-tuned neural document intelligence models.\n\nOnce classified, we will focus on the documents labeled as invoices and enable a \"chat with your invoices\" functionality. This involves addressing OCR (Optical Character Recognition), classification, and NER (Named Entity Recognition) challenges simultaneously. By doing so, companies can efficiently tag their documentation, vectorize it, and index it into a vector database, making their data easily searchable.\n\n### Data\n\nWe use the RVL-CDIP dataset, consisting of 400,000 grayscale images in 16 classes. For this prototype, we selected 100 samples per class, split into 70% for training and 30% for validation. For more information about the RVL-CDIP dataset, please refer to the [dataset page](https://huggingface.co/datasets/aharley/rvl_cdip) on Hugging Face 📚.\n\n### Solution\n\n![Pipeline Diagram](utils/images/image-1.png)\n\n1. **Document Classification**: \n   - We classify documents into 16 categories by fine-tuning pre-trained neural models using Azure AI Document Intelligence.\n   - See the code [here](03-classification-custom-document-intelligence.ipynb).\n\n2. **Key Elements Extraction and Summarization**: \n   - Extract and summarize key elements (NER) from documents classified as invoices using language models with multimodality capability for contextual entity extraction and summarization.\n   - Enforce proper validation and convert them into a structured format (JSON) with the support of the pydantic and instructor libraries.\n   - See the code [here](05-entity-extraction-document-intelligence.ipynb).\n\n3. **Data Indexing and Vectorization**: \n   - Index and vectorize the JSON containing the key information and summarization per document into Azure AI Search.\n   - This allows your enterprise to query the documents in a \"Bing-like\" manner and make your previously unlabeled data searchable.\n   - See the code [here](06-make-your-data-searchable.ipynb).\n\n## 📚 Resources\n\n- **Document Intelligence**: For detailed information on Document Intelligence AI and its components, visit our [Documentation](https://learn.microsoft.com/en-us/azure/ai-services/document-intelligence/?view=doc-intel-4.0.0).\n- **Azure AI Studio**: Check out our [Tutorials](https://learn.microsoft.com/en-us/azure/ai-studio/what-is-ai-studio) for hands-on guides on Azure AI Studio.\n- **Azure AI Agentic Frameworks**: Explore the [Azure AI Agentic Frameworks Repository](https://github.com/pablosalvador10/gbbai-azure-ai-agentic-frameworks) for frameworks and tools to build AI agents.\n- **LLM/SLM Evaluation Framework**: Check out the [LLM/SLM Evaluation Framework Repository](https://github.com/pablosalvador10/gbb-ai-llm-slm-evaluation-framework) for evaluating large and small language models.\n\n## Contributing\n\nWe welcome contributions to enhance the capabilities and features of this project. Please read our [contributing guidelines](CONTRIBUTING.md) for more information.\n\n### Disclaimer\n> [!IMPORTANT]\n> This software is provided for demonstration purposes only. It is not intended to be relied upon for any purpose. The creators of this software make no representations or warranties of any kind, express or implied, about the completeness, accuracy, reliability, suitability or availability with respect to the software or the information, products, services, or related graphics contained in the software for any purpose. Any reliance you place on such information is therefore strictly at your own risk."
    },
    {
      "name": "pfekrati/langchain-vs-semantickernel",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/106823811?s=40&v=4",
      "owner": "pfekrati",
      "repo_name": "langchain-vs-semantickernel",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2024-08-14T11:20:48Z",
      "updated_at": "2024-11-21T19:47:20Z",
      "topics": [],
      "readme": ""
    },
    {
      "name": "Imperial-EE-Microsoft/github-app-public",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/169689686?s=40&v=4",
      "owner": "Imperial-EE-Microsoft",
      "repo_name": "github-app-public",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2024-07-29T06:42:19Z",
      "updated_at": "2024-10-08T10:35:51Z",
      "topics": [],
      "readme": "# Co-op-translator\nCo-Operator is designed to automate the tedious and time-consuming task of localization in large repositories. This involves translating all images, documents, and code examples to make them accessible to developers worldwide. By automating these processes, we aim to streamline repository management and enhance productivity.\n\n> [!WARNING]\n> This GitHub repository is only a public mirror of a private repository used for a proof-of-concept demo. Please do not install any links or apps as this repository is not actively being maintained.\n\n## User-Side Setup\n1. access co-op-translator [here](https://github.com/apps/co-op-translator)\n2. click install\n3. choose the account/organization and repositories you want to install co-op-translator on\n4. you will be redirected to app homepage, click invite robot to invite robot account to the repo as cooperator\n5. active the repo, and co-op-translator is working!\n\n## Developer Setup\n\n1. **Create and Enter Your Own Virtual Environment**  \n   ```bash\n   python -m venv venv\n   # Windows\n   .\\venv\\Scripts\\activate\n   # Mac\n   source venv/bin/activate\n   # To leave the virtual environment\n   deactivate\n   ```\n2. **install dependencies**\n    ```bash\n    pip install -r requirements.txt\n    cd frontend && npm install\n    ```\n\n3. **build frontend in `\\frontend`**\n    ```bash\n    npm run build\n    ```\n\n4. **start backend server**\n    ```bash\n    # . Make sure you are in the root directory of the project.\n    `cd backend`\n    `python manage.py runserver 8080`\n    ```\n5. **api keys**\n   Create a `.env` file following the `.env.template` file, filling in API keys and endpoints and the information of the GitHub app.\n\n\n7. **deploy & debug**\\\n   use ngrok for debugging, by exposing the server to the internet.. Currently to use ngrok, you need to create a personal account and reserve a free url to test OAuth. Run a command similar to the below, but use your domain name.\\\n   \\\n    **Note**: In the `.env` file, update SERVER_URL to the ngrok url.\n\n    ```bash\n    # replace domain=... with your own domain\n    ngrok http --domain=perfect-liked-yak.ngrok-free.app 8080\n    ngrok http --domain=better-eternal-filly.ngrok-free.app 8080\n    ```\n\n    You can also run `ngrok.sh` to automatically start the ngrok tunnel. The script fetches the `SERVER_URL` automatically from the `.env` file, so make sure you have it listed there.\n\n\nTo set up ngrok, follow steps 1-2 [here](https://ngrok.com/docs/getting-started/) but set up a reserved url on the website portal instead of a random one.\n\n<!-- ## Shortcut Run\n- After installing all dependencies, just call `run.sh` from the root directory to set everything up automatically -->\n\n## Project Structure\n![Software Structure](./structure.png)\n\n## Backend Structure\n### Backend Apps\nThere are four main apps in the backend project: \n  <!-- 1. dashboard -->\n  1. github_app\n     - High level calls\n  2. github_auth\n     - Handles user authentication\n  3. translate\n     - For code to call models and translate documentation files \n     -  For code to call Github API to get info of the repository  \n4. webhook_handler\n    - handle subscribed event from user's github repository. (e.g. new push on main branch)\n\n\n### backend endpoint\n\n| Endpoint              | Description                                           |\n|-----------------------|-------------------------------------------------------|\n| api/webhook           | Automatic update translation for new pull request.               |\n| auth/github/login     | GitHub login page for user                            | \n| auth/github/callback  | Callback URL after GitHub login.                      |\n| translate/translate/?repo_id={}  | initial translation of a repo        |\n\n**translate/translate**:\n<!-- **Todo: This is outdated, update later** -->\n- assume co-op branch does not exist\n  - create a branch\n  - translate all files and docs, push them to the new branch\n  - create a push request to main branch\n- input needed:\n    - repo_id: id of repo want to access\n\n**auth/github/login**:\n- after user sign in, it will redirected to main page\n- user choose which repo to work on, then data will be loaded\n\n**api/webhook**:\n- receive subscribed event from github repo and handle them\n- subscribed event:\n  - installation of github app\n  - uninstallation of github app\n  - new push to repository\n  - push request created by co-op-translator accepted or deleted\n\n## State\n- [ ] To do: Come up with a state diagram\n\n## Translation Structure\n\n### Markdown File locations\n\n- **README.md Translation**: \n  - A `translation` folder will be created at the same directory level as `README.md`.\n  - The translated file will be named `README.lg.md`, where `lg` is the language code.\n\n- **Other .md Files Translation**: \n  - All documents to be translated are assumed located in `/project_root/docs`.\n  - For each language, a separate folder will be created within `docs`, maintaining the same structure as the original.\n  - For example, the translated version of `/project_root/docs/file_path` will be located at `/project_root/docs/language/file_path`.\n\n- **Sample Structure**: \n```\n/project_root\n    /docs\n        /installation.md\n        /screenshot.png\n        /api\n            /services.md\n        /usage.md\n        /fr\n            /installation.md\n            /usage.md\n            /api\n                /services.md\n        /es\n            /installation.md\n            /usage.md\n            /api\n                /services.md\n        /translated_images\n            /screenshot<hash>.fr.png\n            /screenshot<hash>.es.png\n    /src\n        /readme.md\n        /readme_img.png\n        /translations\n            /readme.fr.md\n            /readme.es.md\n        /translated_images\n            /readme_img<hash>.fr.png\n            /readme_img<hash>.es.png\n```\n### Info:\n- All markdown files are translated from the primary language (English in this case) to other languages (French, Spanish, etc.)\n- Any changes to the translated markdown files are left ignored, they are the translator's edits\n  - All images used by the markdown file are 'translated' and stored in a separate folder near to the translated markdown files\n  - The translated images are stored in a separate folder, and the image filename has a hash taken from the image path in order to prevent file naming conflicts\n  - This is because a md file could reference an image from two different directories, but they have the same name. However, they are all stored together for their translated versions\n  - This also prevents us from having to use a database to keep track of images\n  - When an image is updated in the primary language:\n    - Generate the hash from the image path and append it to the image filename to get the translated image filename \n    - [Search the github repo](https://stackoverflow.com/questions/25022016/get-all-file-names-from-a-github-repo-through-the-github-api) for all instances of &lt;img_filename&gt;&lt;hash&gt;.xx.y\n      - xx is the country code which are any two alphabets\n      - y is the file extension which can be any image used in markdown files\n      - The github api will return their filepaths\n    - Generate all new translated images.\n    - Using the filepaths from the api, replace the old translated images with the new images\n- Only a change in the primary language markdown file will trigger a re-translation of the other languages\n- This may use a lot of tokens, however, in major open source projects, commits to main are not very frequent, so this should not be a problem\n\n# API\n\n### GitHub Authentication (`github_auth`)\n\n- **Login with GitHub**: `/auth/github/login/`  \n  Initiates the login process with GitHub.\n\n- **GitHub OAuth Callback**: `/auth/github/callback/`  \n  Handles the callback from GitHub after the OAuth flow, completing the authentication process.\n\n- **Check GitHub Login Status**: `/auth/github/status/`  \n  Checks and returns the current GitHub login status of the user.\n\n### Translation (`translate`)\n\n- **Initialize Translation**: `/translate/init/`  \n  Send an invite for the robot to join the GitHub repo. This is automatically called during `translate/translate/`.\n\n- **Get Repositories**: `/translate/repos/`  \n  Get a list of all repositories for the current login user from the Django database, including their monitored status. For the frontend\n\n- **Enable Monitoring**: `/translate/repos/monitor/<int:repo_id>/true/`  \n  Enables monitoring for changes in the specified repository. A monitored repository gains a new `co-op-translator` branch and the translations are updated to follow the main branch.\n\n- **Disable Monitoring**: `/translate/repos/monitor/<int:repo_id>/false/`  \n  Disables monitoring for the specified repository. The `co-op-translator` branch is deleted.\n\n- **Translate Markdown**: `/translate/translate/`  \n  Endpoint to translate markdown files in a repository, and create a pull request with the changes.\n\n### Webhook Handler (`webhook_handler`)\n\n- **Webhook Endpoint**: `/api/webhook/`  \n  Handles webhook events from GitHub.\n\n- **Refresh Installed Repositories**: `/api/refresh/`  \n  Triggers a refresh of the installed repositories on the app's db with newly fetched repos that the github bot can access\n\n\n\n# Bugs\n## Image Duplication Issue\n  - There are two separate areas for translation; either markdown files inside the docs directory or the README files that can be placed anywhere in the repository\n  - Translated images that are in the docs directory cannot be accessed by translated README files, and translated images for README files cannot be accessed by translated markdown files in the docs directory.\n  - So if an image is used in both a README file and a markdown file, it will need to have two copies, one in the docs directory and one in the root directory. \n## Image Translations\n  - There is an issue where all images in the repository will be translated regardless of whether they are used in the markdown files or not. This is because the image translation is done separately from the markdown translation on a detect-all basis.\n  - To fix this requires the whole app to be changed so that it only translate images that are detected during the markdown translation process. This will require a major overhaul of the translation process and the way images are stored and accessed.\n"
    },
    {
      "name": "umermansoor/llm-ai-coach",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/862952?s=40&v=4",
      "owner": "umermansoor",
      "repo_name": "llm-ai-coach",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2024-07-21T03:35:36Z",
      "updated_at": "2024-08-13T10:02:15Z",
      "topics": [],
      "readme": ""
    },
    {
      "name": "avidunixuser/AOAICoE",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/2113048?s=40&v=4",
      "owner": "avidunixuser",
      "repo_name": "AOAICoE",
      "description": null,
      "homepage": null,
      "language": "HTML",
      "created_at": "2024-07-22T18:34:37Z",
      "updated_at": "2025-01-14T18:35:55Z",
      "topics": [],
      "readme": ""
    },
    {
      "name": "YoutacRandS-VA/autogen",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/146207501?s=40&v=4",
      "owner": "YoutacRandS-VA",
      "repo_name": "autogen",
      "description": "Enable Next-Gen Large Language Model Applications. Join our Discord: https://discord.gg/pAbnFJrkgZ",
      "homepage": "https://microsoft.github.io/autogen/",
      "language": "Jupyter Notebook",
      "created_at": "2023-09-30T23:58:05Z",
      "updated_at": "2024-06-26T02:25:04Z",
      "topics": [],
      "readme": "<a name=\"readme-top\"></a>\n\n[![PyPI version](https://badge.fury.io/py/pyautogen.svg)](https://badge.fury.io/py/pyautogen)\n[![Build](https://github.com/microsoft/autogen/actions/workflows/python-package.yml/badge.svg)](https://github.com/microsoft/autogen/actions/workflows/python-package.yml)\n![Python Version](https://img.shields.io/badge/3.8%20%7C%203.9%20%7C%203.10%20%7C%203.11%20%7C%203.12-blue)\n[![Downloads](https://static.pepy.tech/badge/pyautogen/week)](https://pepy.tech/project/pyautogen)\n[![Discord](https://img.shields.io/discord/1153072414184452236?logo=discord&style=flat)](https://aka.ms/autogen-dc)\n[![Twitter](https://img.shields.io/twitter/url/https/twitter.com/cloudposse.svg?style=social&label=Follow%20%40pyautogen)](https://twitter.com/pyautogen)\n\n[![NuGet version](https://badge.fury.io/nu/AutoGen.Core.svg)](https://badge.fury.io/nu/AutoGen.Core)\n\n# AutoGen\n[📚 Cite paper](#related-papers).\n<!-- <p align=\"center\">\n    <img src=\"https://github.com/microsoft/autogen/blob/main/website/static/img/flaml.svg\"  width=200>\n    <br>\n</p> -->\n:fire: May 29, 2024: DeepLearning.ai launched a new short course [AI Agentic Design Patterns with AutoGen](https://www.deeplearning.ai/short-courses/ai-agentic-design-patterns-with-autogen), made in collaboration with Microsoft and Penn State University, and taught by AutoGen creators [Chi Wang](https://github.com/sonichi) and [Qingyun Wu](https://github.com/qingyun-wu).\n\n:fire: May 24, 2024: Foundation Capital published an article on [Forbes: The Promise of Multi-Agent AI](https://www.forbes.com/sites/joannechen/2024/05/24/the-promise-of-multi-agent-ai/?sh=2c1e4f454d97) and a video [AI in the Real World Episode 2: Exploring Multi-Agent AI and AutoGen with Chi Wang](https://www.youtube.com/watch?v=RLwyXRVvlNk).\n\n:fire: May 13, 2024: [The Economist](https://www.economist.com/science-and-technology/2024/05/13/todays-ai-models-are-impressive-teams-of-them-will-be-formidable) published an article about multi-agent systems (MAS) following a January 2024 interview with [Chi Wang](https://github.com/sonichi).\n\n:fire: May 11, 2024: [AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation](https://openreview.net/pdf?id=uAjxFFing2) received the best paper award at the [ICLR 2024 LLM Agents Workshop](https://llmagents.github.io/).\n\n:fire: Apr 26, 2024: [AutoGen.NET](https://microsoft.github.io/autogen-for-net/) is available for .NET developers!\n\n:fire: Apr 17, 2024: Andrew Ng cited AutoGen in [The Batch newsletter](https://www.deeplearning.ai/the-batch/issue-245/) and [What's next for AI agentic workflows](https://youtu.be/sal78ACtGTc?si=JduUzN_1kDnMq0vF) at Sequoia Capital's AI Ascent (Mar 26).\n\n:fire: Mar 3, 2024: What's new in AutoGen? 📰[Blog](https://microsoft.github.io/autogen/blog/2024/03/03/AutoGen-Update); 📺[Youtube](https://www.youtube.com/watch?v=j_mtwQiaLGU).\n\n:fire: Mar 1, 2024: the first AutoGen multi-agent experiment on the challenging [GAIA](https://huggingface.co/spaces/gaia-benchmark/leaderboard) benchmark achieved the No. 1 accuracy in all the three levels.\n\n<!-- :tada: Jan 30, 2024: AutoGen is highlighted by Peter Lee in Microsoft Research Forum [Keynote](https://t.co/nUBSjPDjqD). -->\n\n:tada: Dec 31, 2023: [AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation Framework](https://arxiv.org/abs/2308.08155) is selected by [TheSequence: My Five Favorite AI Papers of 2023](https://thesequence.substack.com/p/my-five-favorite-ai-papers-of-2023).\n\n<!-- :fire: Nov 24: pyautogen [v0.2](https://github.com/microsoft/autogen/releases/tag/v0.2.0) is released with many updates and new features compared to v0.1.1. It switches to using openai-python v1. Please read the [migration guide](https://microsoft.github.io/autogen/docs/Installation#python). -->\n\n<!-- :fire: Nov 11: OpenAI's Assistants are available in AutoGen and interoperatable with other AutoGen agents! Checkout our [blogpost](https://microsoft.github.io/autogen/blog/2023/11/13/OAI-assistants) for details and examples. -->\n\n:tada: Nov 8, 2023: AutoGen is selected into [Open100: Top 100 Open Source achievements](https://www.benchcouncil.org/evaluation/opencs/annual.html) 35 days after spinoff from [FLAML](https://github.com/microsoft/FLAML).\n\n<!-- :tada: Nov 6, 2023: AutoGen is mentioned by Satya Nadella in a [fireside chat](https://youtu.be/0pLBvgYtv6U). -->\n\n<!-- :tada: Nov 1, 2023: AutoGen is the top trending repo on GitHub in October 2023. -->\n\n<!-- :tada: Oct 03, 2023: AutoGen spins off from [FLAML](https://github.com/microsoft/FLAML) on GitHub. -->\n\n<!-- :tada: Aug 16: Paper about AutoGen on [arxiv](https://arxiv.org/abs/2308.08155). -->\n\n:tada: Mar 29, 2023: AutoGen is first created in [FLAML](https://github.com/microsoft/FLAML).\n\n<!--\n:fire: FLAML is highlighted in OpenAI's [cookbook](https://github.com/openai/openai-cookbook#related-resources-from-around-the-web).\n\n:fire: [autogen](https://microsoft.github.io/autogen/) is released with support for ChatGPT and GPT-4, based on [Cost-Effective Hyperparameter Optimization for Large Language Model Generation Inference](https://arxiv.org/abs/2303.04673).\n\n:fire: FLAML supports Code-First AutoML & Tuning – Private Preview in [Microsoft Fabric Data Science](https://learn.microsoft.com/en-us/fabric/data-science/). -->\n\n<p align=\"right\" style=\"font-size: 14px; color: #555; margin-top: 20px;\">\n  <a href=\"#readme-top\" style=\"text-decoration: none; color: blue; font-weight: bold;\">\n    ↑ Back to Top ↑\n  </a>\n</p>\n\n## What is AutoGen\n\nAutoGen is a framework that enables the development of LLM applications using multiple agents that can converse with each other to solve tasks. AutoGen agents are customizable, conversable, and seamlessly allow human participation. They can operate in various modes that employ combinations of LLMs, human inputs, and tools.\n\n![AutoGen Overview](https://github.com/microsoft/autogen/blob/main/website/static/img/autogen_agentchat.png)\n\n- AutoGen enables building next-gen LLM applications based on [multi-agent conversations](https://microsoft.github.io/autogen/docs/Use-Cases/agent_chat) with minimal effort. It simplifies the orchestration, automation, and optimization of a complex LLM workflow. It maximizes the performance of LLM models and overcomes their weaknesses.\n- It supports [diverse conversation patterns](https://microsoft.github.io/autogen/docs/Use-Cases/agent_chat#supporting-diverse-conversation-patterns) for complex workflows. With customizable and conversable agents, developers can use AutoGen to build a wide range of conversation patterns concerning conversation autonomy,\n  the number of agents, and agent conversation topology.\n- It provides a collection of working systems with different complexities. These systems span a [wide range of applications](https://microsoft.github.io/autogen/docs/Use-Cases/agent_chat#diverse-applications-implemented-with-autogen) from various domains and complexities. This demonstrates how AutoGen can easily support diverse conversation patterns.\n- AutoGen provides [enhanced LLM inference](https://microsoft.github.io/autogen/docs/Use-Cases/enhanced_inference#api-unification). It offers utilities like API unification and caching, and advanced usage patterns, such as error handling, multi-config inference, context programming, etc.\n\nAutoGen is created out of collaborative [research](https://microsoft.github.io/autogen/docs/Research) from Microsoft, Penn State University, and the University of Washington.\n\n<p align=\"right\" style=\"font-size: 14px; color: #555; margin-top: 20px;\">\n  <a href=\"#readme-top\" style=\"text-decoration: none; color: blue; font-weight: bold;\">\n    ↑ Back to Top ↑\n  </a>\n</p>\n\n## Roadmaps\n\nTo see what we are working on and what we plan to work on, please check our\n[Roadmap Issues](https://aka.ms/autogen-roadmap).\n\n<p align=\"right\" style=\"font-size: 14px; color: #555; margin-top: 20px;\">\n  <a href=\"#readme-top\" style=\"text-decoration: none; color: blue; font-weight: bold;\">\n    ↑ Back to Top ↑\n  </a>\n</p>\n\n## Quickstart\nThe easiest way to start playing is\n1. Click below to use the GitHub Codespace\n\n    [![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://codespaces.new/microsoft/autogen?quickstart=1)\n\n 2. Copy OAI_CONFIG_LIST_sample to ./notebook folder, name to OAI_CONFIG_LIST, and set the correct configuration.\n 3. Start playing with the notebooks!\n\n*NOTE*: OAI_CONFIG_LIST_sample lists GPT-4 as the default model, as this represents our current recommendation, and is known to work well with AutoGen. If you use a model other than GPT-4, you may need to revise various system prompts (especially if using weaker models like GPT-3.5-turbo). Moreover, if you use models other than those hosted by OpenAI or Azure, you may incur additional risks related to alignment and safety. Proceed with caution if updating this default.\n\n<p align=\"right\" style=\"font-size: 14px; color: #555; margin-top: 20px;\">\n  <a href=\"#readme-top\" style=\"text-decoration: none; color: blue; font-weight: bold;\">\n    ↑ Back to Top ↑\n  </a>\n</p>\n\n## [Installation](https://microsoft.github.io/autogen/docs/Installation)\n### Option 1. Install and Run AutoGen in Docker\n\nFind detailed instructions for users [here](https://microsoft.github.io/autogen/docs/installation/Docker#step-1-install-docker), and for developers [here](https://microsoft.github.io/autogen/docs/Contribute#docker-for-development).\n\n### Option 2. Install AutoGen Locally\n\nAutoGen requires **Python version >= 3.8, < 3.13**. It can be installed from pip:\n\n```bash\npip install pyautogen\n```\n\nMinimal dependencies are installed without extra options. You can install extra options based on the feature you need.\n\n<!-- For example, use the following to install the dependencies needed by the [`blendsearch`](https://microsoft.github.io/FLAML/docs/Use-Cases/Tune-User-Defined-Function#blendsearch-economical-hyperparameter-optimization-with-blended-search-strategy) option.\n```bash\npip install \"pyautogen[blendsearch]\"\n``` -->\n\nFind more options in [Installation](https://microsoft.github.io/autogen/docs/Installation#option-2-install-autogen-locally-using-virtual-environment).\n\n<!-- Each of the [`notebook examples`](https://github.com/microsoft/autogen/tree/main/notebook) may require a specific option to be installed. -->\n\nEven if you are installing and running AutoGen locally outside of docker, the recommendation and default behavior of agents is to perform [code execution](https://microsoft.github.io/autogen/docs/FAQ/#code-execution) in docker. Find more instructions and how to change the default behaviour [here](https://microsoft.github.io/autogen/docs/Installation#code-execution-with-docker-(default)).\n\nFor LLM inference configurations, check the [FAQs](https://microsoft.github.io/autogen/docs/FAQ#set-your-api-endpoints).\n\n<p align=\"right\" style=\"font-size: 14px; color: #555; margin-top: 20px;\">\n  <a href=\"#readme-top\" style=\"text-decoration: none; color: blue; font-weight: bold;\">\n    ↑ Back to Top ↑\n  </a>\n</p>\n\n## Multi-Agent Conversation Framework\n\nAutogen enables the next-gen LLM applications with a generic [multi-agent conversation](https://microsoft.github.io/autogen/docs/Use-Cases/agent_chat) framework. It offers customizable and conversable agents that integrate LLMs, tools, and humans.\nBy automating chat among multiple capable agents, one can easily make them collectively perform tasks autonomously or with human feedback, including tasks that require using tools via code.\n\nFeatures of this use case include:\n\n- **Multi-agent conversations**: AutoGen agents can communicate with each other to solve tasks. This allows for more complex and sophisticated applications than would be possible with a single LLM.\n- **Customization**: AutoGen agents can be customized to meet the specific needs of an application. This includes the ability to choose the LLMs to use, the types of human input to allow, and the tools to employ.\n- **Human participation**: AutoGen seamlessly allows human participation. This means that humans can provide input and feedback to the agents as needed.\n\nFor [example](https://github.com/microsoft/autogen/blob/main/test/twoagent.py),\n\n```python\nfrom autogen import AssistantAgent, UserProxyAgent, config_list_from_json\n# Load LLM inference endpoints from an env variable or a file\n# See https://microsoft.github.io/autogen/docs/FAQ#set-your-api-endpoints\n# and OAI_CONFIG_LIST_sample\nconfig_list = config_list_from_json(env_or_file=\"OAI_CONFIG_LIST\")\n# You can also set config_list directly as a list, for example, config_list = [{'model': 'gpt-4', 'api_key': '<your OpenAI API key here>'},]\nassistant = AssistantAgent(\"assistant\", llm_config={\"config_list\": config_list})\nuser_proxy = UserProxyAgent(\"user_proxy\", code_execution_config={\"work_dir\": \"coding\", \"use_docker\": False}) # IMPORTANT: set to True to run code in docker, recommended\nuser_proxy.initiate_chat(assistant, message=\"Plot a chart of NVDA and TESLA stock price change YTD.\")\n# This initiates an automated chat between the two agents to solve the task\n```\n\nThis example can be run with\n\n```python\npython test/twoagent.py\n```\n\nAfter the repo is cloned.\nThe figure below shows an example conversation flow with AutoGen.\n![Agent Chat Example](https://github.com/microsoft/autogen/blob/main/website/static/img/chat_example.png)\n\nAlternatively, the [sample code](https://github.com/microsoft/autogen/blob/main/samples/simple_chat.py) here allows a user to chat with an AutoGen agent in ChatGPT style.\nPlease find more [code examples](https://microsoft.github.io/autogen/docs/Examples#automated-multi-agent-chat) for this feature.\n\n<p align=\"right\" style=\"font-size: 14px; color: #555; margin-top: 20px;\">\n  <a href=\"#readme-top\" style=\"text-decoration: none; color: blue; font-weight: bold;\">\n    ↑ Back to Top ↑\n  </a>\n</p>\n\n## Enhanced LLM Inferences\n\nAutogen also helps maximize the utility out of the expensive LLMs such as ChatGPT and GPT-4. It offers [enhanced LLM inference](https://microsoft.github.io/autogen/docs/Use-Cases/enhanced_inference#api-unification) with powerful functionalities like caching, error handling, multi-config inference and templating.\n\n<!-- For example, you can optimize generations by LLM with your own tuning data, success metrics, and budgets.\n\n```python\n# perform tuning for openai<1\nconfig, analysis = autogen.Completion.tune(\n    data=tune_data,\n    metric=\"success\",\n    mode=\"max\",\n    eval_func=eval_func,\n    inference_budget=0.05,\n    optimization_budget=3,\n    num_samples=-1,\n)\n# perform inference for a test instance\nresponse = autogen.Completion.create(context=test_instance, **config)\n```\n\nPlease find more [code examples](https://microsoft.github.io/autogen/docs/Examples#tune-gpt-models) for this feature. -->\n\n<p align=\"right\" style=\"font-size: 14px; color: #555; margin-top: 20px;\">\n  <a href=\"#readme-top\" style=\"text-decoration: none; color: blue; font-weight: bold;\">\n    ↑ Back to Top ↑\n  </a>\n</p>\n\n## Documentation\n\nYou can find detailed documentation about AutoGen [here](https://microsoft.github.io/autogen/).\n\nIn addition, you can find:\n\n- [Research](https://microsoft.github.io/autogen/docs/Research), [blogposts](https://microsoft.github.io/autogen/blog) around AutoGen, and [Transparency FAQs](https://github.com/microsoft/autogen/blob/main/TRANSPARENCY_FAQS.md)\n\n- [Discord](https://aka.ms/autogen-dc)\n\n- [Contributing guide](https://microsoft.github.io/autogen/docs/Contribute)\n\n- [Roadmap](https://github.com/orgs/microsoft/projects/989/views/3)\n\n<p align=\"right\" style=\"font-size: 14px; color: #555; margin-top: 20px;\">\n  <a href=\"#readme-top\" style=\"text-decoration: none; color: blue; font-weight: bold;\">\n    ↑ Back to Top ↑\n  </a>\n</p>\n\n## Related Papers\n\n[AutoGen](https://arxiv.org/abs/2308.08155)\n\n```\n@inproceedings{wu2023autogen,\n      title={AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation Framework},\n      author={Qingyun Wu and Gagan Bansal and Jieyu Zhang and Yiran Wu and Beibin Li and Erkang Zhu and Li Jiang and Xiaoyun Zhang and Shaokun Zhang and Jiale Liu and Ahmed Hassan Awadallah and Ryen W White and Doug Burger and Chi Wang},\n      year={2023},\n      eprint={2308.08155},\n      archivePrefix={arXiv},\n      primaryClass={cs.AI}\n}\n```\n\n[EcoOptiGen](https://arxiv.org/abs/2303.04673)\n\n```\n@inproceedings{wang2023EcoOptiGen,\n    title={Cost-Effective Hyperparameter Optimization for Large Language Model Generation Inference},\n    author={Chi Wang and Susan Xueqing Liu and Ahmed H. Awadallah},\n    year={2023},\n    booktitle={AutoML'23},\n}\n```\n\n[MathChat](https://arxiv.org/abs/2306.01337)\n\n```\n@inproceedings{wu2023empirical,\n    title={An Empirical Study on Challenging Math Problem Solving with GPT-4},\n    author={Yiran Wu and Feiran Jia and Shaokun Zhang and Hangyu Li and Erkang Zhu and Yue Wang and Yin Tat Lee and Richard Peng and Qingyun Wu and Chi Wang},\n    year={2023},\n    booktitle={ArXiv preprint arXiv:2306.01337},\n}\n```\n\n[AgentOptimizer](https://arxiv.org/pdf/2402.11359)\n\n```\n@article{zhang2024training,\n  title={Training Language Model Agents without Modifying Language Models},\n  author={Zhang, Shaokun and Zhang, Jieyu and Liu, Jiale and Song, Linxin and Wang, Chi and Krishna, Ranjay and Wu, Qingyun},\n  journal={ICML'24},\n  year={2024}\n}\n```\n\n<p align=\"right\" style=\"font-size: 14px; color: #555; margin-top: 20px;\">\n  <a href=\"#readme-top\" style=\"text-decoration: none; color: blue; font-weight: bold;\">\n    ↑ Back to Top ↑\n  </a>\n</p>\n\n## Contributing\n\nThis project welcomes contributions and suggestions. Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit <https://cla.opensource.microsoft.com>.\n\nIf you are new to GitHub, [here](https://opensource.guide/how-to-contribute/#how-to-submit-a-contribution) is a detailed help source on getting involved with development on GitHub.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information, see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n<p align=\"right\" style=\"font-size: 14px; color: #555; margin-top: 20px;\">\n  <a href=\"#readme-top\" style=\"text-decoration: none; color: blue; font-weight: bold;\">\n    ↑ Back to Top ↑\n  </a>\n</p>\n\n## Contributors Wall\n<a href=\"https://github.com/microsoft/autogen/graphs/contributors\">\n  <img src=\"https://contrib.rocks/image?repo=microsoft/autogen&max=204\" />\n</a>\n\n<p align=\"right\" style=\"font-size: 14px; color: #555; margin-top: 20px;\">\n  <a href=\"#readme-top\" style=\"text-decoration: none; color: blue; font-weight: bold;\">\n    ↑ Back to Top ↑\n  </a>\n</p>\n\n# Legal Notices\n\nMicrosoft and any contributors grant you a license to the Microsoft documentation and other content\nin this repository under the [Creative Commons Attribution 4.0 International Public License](https://creativecommons.org/licenses/by/4.0/legalcode),\nsee the [LICENSE](LICENSE) file, and grant you a license to any code in the repository under the [MIT License](https://opensource.org/licenses/MIT), see the\n[LICENSE-CODE](LICENSE-CODE) file.\n\nMicrosoft, Windows, Microsoft Azure, and/or other Microsoft products and services referenced in the documentation\nmay be either trademarks or registered trademarks of Microsoft in the United States and/or other countries.\nThe licenses for this project do not grant you rights to use any Microsoft names, logos, or trademarks.\nMicrosoft's general trademark guidelines can be found at http://go.microsoft.com/fwlink/?LinkID=254653.\n\nPrivacy information can be found at https://privacy.microsoft.com/en-us/\n\nMicrosoft and any contributors reserve all other rights, whether under their respective copyrights, patents,\nor trademarks, whether by implication, estoppel, or otherwise.\n\n<p align=\"right\" style=\"font-size: 14px; color: #555; margin-top: 20px;\">\n  <a href=\"#readme-top\" style=\"text-decoration: none; color: blue; font-weight: bold;\">\n    ↑ Back to Top ↑\n  </a>\n</p>\n"
    },
    {
      "name": "sghao/kaggle",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/24216745?s=40&v=4",
      "owner": "sghao",
      "repo_name": "kaggle",
      "description": "kaggle competition code",
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2024-05-18T10:56:02Z",
      "updated_at": "2024-07-15T02:35:21Z",
      "topics": [],
      "readme": "# kaggle\nkaggle competition code\n"
    },
    {
      "name": "meslubi2021/semantic-kernel-starters",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/145295387?s=40&v=4",
      "owner": "meslubi2021",
      "repo_name": "semantic-kernel-starters",
      "description": "Starter Projects for Semantic Kernel",
      "homepage": null,
      "language": "C#",
      "created_at": "2023-11-10T14:30:13Z",
      "updated_at": "2024-10-28T16:48:55Z",
      "topics": [],
      "readme": "# Semantic Kernel Starters\n\nThis repository contains starter projects for the [Semantic Kernel](https://github.com/microsoft/semantic-kernel). Each starter is a self-contained application using a different programming language and application runtime.\n\n## Usage\n\n- `git clone https://github.com/semantic-kernel/semantic-kernel-starters`\n- `code <any-sample-folder>`\n- Follow the instructions in each sample's README for setting up and running the sample\nAlternatively, you can use the [Semantic Kernel Tools](https://marketplace.visualstudio.com/items?itemName=ms-semantic-kernel.semantic-kernel) to \"Create a New App\" using the starters\n\n## Getting Started\n\n- [C# Hello World](sk-csharp-hello-world): The Hello World C# console application starter for the Semantic Kernel.\n- [C# Console Chat](sk-csharp-console-chat): A console application based chat starter for the Semantic Kernel.\n- [C# Azure Functions](sk-csharp-azure-functions): The Hello World C# Azure Functions starter for the Semantic Kernel.\n- [Python Hello World](sk-python-hello-world): The Hello World Python console application starter for the Semantic Kernel.\n- [Python Azure Functions](sk-python-azure-functions): The Hello World Python Azure Functions starter for the Semantic Kernel.\n- [Typescript Console Chat](sk-typescript-console-chat): A console application based chat starter for the Semantic Kernel. \n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n"
    },
    {
      "name": "meslubi2021/semantic-kernel",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/145295387?s=40&v=4",
      "owner": "meslubi2021",
      "repo_name": "semantic-kernel",
      "description": "Integrate cutting-edge LLM technology quickly and easily into your apps",
      "homepage": "https://aka.ms/semantic-kernel",
      "language": "C#",
      "created_at": "2023-10-23T22:46:33Z",
      "updated_at": "2024-10-28T16:48:46Z",
      "topics": [],
      "readme": "# Semantic Kernel\n\n[![Python package](https://img.shields.io/pypi/v/semantic-kernel)](https://pypi.org/project/semantic-kernel/)\n[![Nuget package](https://img.shields.io/nuget/vpre/Microsoft.SemanticKernel)](https://www.nuget.org/packages/Microsoft.SemanticKernel/)\n[![dotnet Docker](https://github.com/microsoft/semantic-kernel/actions/workflows/dotnet-ci-docker.yml/badge.svg?branch=main)](https://github.com/microsoft/semantic-kernel/actions/workflows/dotnet-ci-docker.yml)\n[![dotnet Windows](https://github.com/microsoft/semantic-kernel/actions/workflows/dotnet-ci-windows.yml/badge.svg?branch=main)](https://github.com/microsoft/semantic-kernel/actions/workflows/dotnet-ci-windows.yml)\n[![License: MIT](https://img.shields.io/github/license/microsoft/semantic-kernel)](https://github.com/microsoft/semantic-kernel/blob/main/LICENSE)\n[![Discord](https://img.shields.io/discord/1063152441819942922?label=Discord&logo=discord&logoColor=white&color=d82679)](https://aka.ms/SKDiscord)\n\n[Semantic Kernel](https://learn.microsoft.com/en-us/semantic-kernel/overview/)\nis an SDK that integrates Large Language Models (LLMs) like\n[OpenAI](https://platform.openai.com/docs/introduction),\n[Azure OpenAI](https://azure.microsoft.com/en-us/products/ai-services/openai-service),\nand [Hugging Face](https://huggingface.co/)\nwith conventional programming languages like C#, Python, and Java. Semantic Kernel achieves this\nby allowing you to define [plugins](https://learn.microsoft.com/en-us/semantic-kernel/ai-orchestration/plugins)\nthat can be chained together\nin just a [few lines of code](https://learn.microsoft.com/en-us/semantic-kernel/ai-orchestration/chaining-functions?tabs=Csharp#using-the-runasync-method-to-simplify-your-code).\n\nWhat makes Semantic Kernel _special_, however, is its ability to _automatically_ orchestrate\nplugins with AI. With Semantic Kernel\n[planners](https://learn.microsoft.com/en-us/semantic-kernel/ai-orchestration/planner), you\ncan ask an LLM to generate a plan that achieves a user's unique goal. Afterwards,\nSemantic Kernel will execute the plan for the user.\n\n#### Please star the repo to show your support for this project!\n\n![Orchestrating plugins with planner](https://learn.microsoft.com/en-us/semantic-kernel/media/kernel-infographic.png)\n\n\n\n## Getting started with Semantic Kernel\n\nThe Semantic Kernel SDK is available in C#, Python, and Java. To get started, choose your preferred language below. See the [Feature Matrix](https://learn.microsoft.com/en-us/semantic-kernel/get-started/supported-languages) to see a breakdown of\nfeature parity between our currently supported languages.\n\n<table width=100%>\n  <tbody>\n    <tr>\n      <td>\n        <img align=\"left\" width=52px src=\"https://user-images.githubusercontent.com/371009/230673036-fad1e8e6-5d48-49b1-a9c1-6f9834e0d165.png\">\n        <div>\n          <a href=\"dotnet/README.md\">Using Semantic Kernel in C#</a> &nbsp<br/>\n        </div>\n      </td>\n      <td>\n        <img align=\"left\" width=52px src=\"https://raw.githubusercontent.com/devicons/devicon/master/icons/python/python-original.svg\">\n        <div>\n          <a href=\"python/README.md\">Using Semantic Kernel in Python</a>\n        </div>\n      </td>\n      <td>\n        <img align=\"left\" width=52px height=52px src=\"https://upload.wikimedia.org/wikipedia/en/3/30/Java_programming_language_logo.svg\" alt=\"Java logo\">\n        <div>\n          <a href=\"https://github.com/microsoft/semantic-kernel/blob/experimental-java/java/README.md\">Using Semantic Kernel in Java</a>\n        </div>\n      </td>\n    </tr>\n  </tbody>\n</table>\n\nThe quickest way to get started with the basics is to get an API key\nfrom either OpenAI or Azure OpenAI and to run one of the C#, Python, and Java console applications/scripts below.\n\n### For C#:\n\n1. Create a new console app.\n2. Add the semantic kernel nuget `Microsoft.SemanticKernel`.\n3. Copy the code from [here](dotnet/README.md) into the app `Program.cs` file.\n4. Replace the configuration placeholders for API key and other params with your key and settings.\n5. Run with `F5` or `dotnet run`\n\n### For Python:\n\n1. Install the pip package: `python -m pip install semantic-kernel`.\n2. Create a new script e.g. `hello-world.py`.\n3. Store your API key and settings in an `.env` file as described [here](python/README.md).\n4. Copy the code from [here](python/README.md) into the `hello-world.py` script.\n5. Run the python script.\n\n### For Java:\n\n1. Clone and checkout the experimental Java branch: `git clone -b experimental-java https://github.com/microsoft/semantic-kernel.git`\n2. Follow the instructions [here](https://github.com/microsoft/semantic-kernel/blob/experimental-java/java/samples/sample-code/README.md)\n\n## Learning how to use Semantic Kernel\n\nThe fastest way to learn how to use Semantic Kernel is with our C# and Python Jupyter notebooks. These notebooks\ndemonstrate how to use Semantic Kernel with code snippets that you can run with a push of a button.\n\n- [Getting Started with C# notebook](dotnet/notebooks/00-getting-started.ipynb)\n- [Getting Started with Python notebook](python/notebooks/00-getting-started.ipynb)\n\nOnce you've finished the getting started notebooks, you can then check out the main walkthroughs\non our Learn site. Each sample comes with a completed C# and Python project that you can run locally.\n\n1. 📖 [Overview of the kernel](https://learn.microsoft.com/en-us/semantic-kernel/ai-orchestration/)\n1. 🔌 [Understanding AI plugins](https://learn.microsoft.com/en-us/semantic-kernel/ai-orchestration/plugins)\n1. 👄 [Creating semantic functions](https://learn.microsoft.com/en-us/semantic-kernel/ai-orchestration/semantic-functions)\n1. 💽 [Creating native functions](https://learn.microsoft.com/en-us/semantic-kernel/ai-orchestration/native-functions)\n1. ⛓️ [Chaining functions together](https://learn.microsoft.com/en-us/semantic-kernel/ai-orchestration/chaining-functions)\n1. 🤖 [Auto create plans with planner](https://learn.microsoft.com/en-us/semantic-kernel/ai-orchestration/planner)\n1. 💡 [Create and run a ChatGPT plugin](https://learn.microsoft.com/en-us/semantic-kernel/ai-orchestration/chatgpt-plugins)\n\nFinally, refer to our API references for more details on the C# and Python APIs:\n\n- [C# API reference](https://learn.microsoft.com/en-us/dotnet/api/microsoft.semantickernel?view=semantic-kernel-dotnet)\n- Python API reference (coming soon)\n\n## Chat Copilot: see what's possible with Semantic Kernel\n\nIf you're interested in seeing a full end-to-end example of how to use Semantic Kernel, check out\nour [Chat Copilot](https://github.com/microsoft/chat-copilot) reference application. Chat Copilot\nis a chatbot that demonstrates the power of Semantic Kernel. By combining plugins, planners, and personas,\nwe demonstrate how you can build a chatbot that can maintain long-running conversations with users while\nalso leveraging plugins to integrate with other services.\n\n![Chat Copilot answering a question](https://learn.microsoft.com/en-us/semantic-kernel/media/chat-copilot-in-action.gif)\n\nYou can run the app yourself by downloading it from its [GitHub repo](https://github.com/microsoft/chat-copilot).\n\n## Visual Studio Code extension: design semantic functions with ease\n\nThe [Semantic Kernel extension for Visual Studio Code](https://learn.microsoft.com/en-us/semantic-kernel/vs-code-tools/)\nmakes it easy to design and test semantic functions. The extension provides an interface for\ndesigning semantic functions and allows you to test them with a push of a button with your\nexisting models and data.\n\n![Semantic Kernel extension for Visual Studio Code](https://learn.microsoft.com/en-us/semantic-kernel/media/vs-code-extension.png)\n\nIn the above screenshot, you can see the extension in action:\n\n- Syntax highlighting for semantic functions\n- Code completion for semantic functions\n- LLM model picker\n- Run button to test the semantic function with your input data\n\n## Check out our other repos!\n\nIf you like Semantic Kernel, you may also be interested in other repos the Semantic Kernel team supports:\n\n| Repo                                                                              | Description                                                                                   |\n| --------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------- |\n| [Chat Copilot](https://github.com/microsoft/chat-copilot)                         | A reference application that demonstrates how to build a chatbot with Semantic Kernel.        |\n| [Semantic Kernel Docs](https://github.com/MicrosoftDocs/semantic-kernel-docs)     | The home for Semantic Kernel documentation that appears on the Microsoft learn site.          |\n| [Semantic Kernel Starters](https://github.com/microsoft/semantic-kernel-starters) | Starter projects for Semantic Kernel to make it easier to get started.                        |\n| [Semantic Memory](https://github.com/microsoft/semantic-memory)                   | A service that allows you to create pipelines for ingesting, storing, and querying knowledge. |\n\n## Join the community\n\nWe welcome your contributions and suggestions to SK community! One of the easiest\nways to participate is to engage in discussions in the GitHub repository.\nBug reports and fixes are welcome!\n\nFor new features, components, or extensions, please open an issue and discuss with\nus before sending a PR. This is to avoid rejection as we might be taking the core\nin a different direction, but also to consider the impact on the larger ecosystem.\n\nTo learn more and get started:\n\n- Read the [documentation](https://aka.ms/sk/learn)\n- Learn how to [contribute](https://learn.microsoft.com/en-us/semantic-kernel/get-started/contributing) to the project\n- Join the [Discord community](https://aka.ms/SKDiscord)\n- Attend [regular office hours and SK community events](COMMUNITY.md)\n- Follow the team on our [blog](https://aka.ms/sk/blog)\n\n## Code of Conduct\n\nThis project has adopted the\n[Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the\n[Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/)\nor contact [opencode@microsoft.com](mailto:opencode@microsoft.com)\nwith any additional questions or comments.\n\n## License\n\nCopyright (c) Microsoft Corporation. All rights reserved.\n\nLicensed under the [MIT](LICENSE) license.\n"
    },
    {
      "name": "HenriSchulte-MS/Basic-Semantic-Kernel-Copilot",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/77101781?s=40&v=4",
      "owner": "HenriSchulte-MS",
      "repo_name": "Basic-Semantic-Kernel-Copilot",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2024-06-18T08:34:56Z",
      "updated_at": "2024-11-15T13:29:28Z",
      "topics": [],
      "readme": "# Basic Semantic Kernel Copilot\n\nThis repo demonstrates basic chat copilot capabilities of Semantic Kernel in Python. The example code includes the following:\n- Basic copilot chat loop rendering output to the terminal\n- Connecting Azure OpenAI chat and embedding models to the kernel\n- Adding built-in Semantic Kernel plugins\n- Custom semantic and native plugins\n- Custom RAG plugin using Azure AI Search (using keyword search)\n\nFor a step-by-step explanation of the code see [Getting started with Semantic Kernel](https://learn.microsoft.com/en-us/semantic-kernel/get-started/quick-start-guide?pivots=programming-language-python).\n\n## Running the Copilot\n1. Clone this repo and open it in, e.g., VS Code.\n1. Rename [.env.example](.env.example) to ```.env``` and populate it with your keys and endpoints for Azure AI / Azure OpenAI and Azure AI Search.\n1. Install the [requirements](requirements.txt) using ```pip install -r requirements.txt```.\n1. Customize the [example plugin](src/plugins/Example/example_plugin.py) and [search plugin](src/plugins/Example/example_plugin.py) to enable the kernel to execute custom code.\n1. Run the [app.py](src/app.py) to start the copilot. Type your chat input in the terminal.\n"
    },
    {
      "name": "joansoleroig/Predictive-Maintenance-Streamlit-App",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/119736097?s=40&v=4",
      "owner": "joansoleroig",
      "repo_name": "Predictive-Maintenance-Streamlit-App",
      "description": "Predictive Maintenance Dashboard leverages ML to forecast machine failures, enhancing maintenance planning. Built with Streamlit, it features interactive predictions, data exploration, and visualization tools. Clone, install, and run with streamlit run app.py.",
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2024-05-13T17:19:02Z",
      "updated_at": "2025-02-18T09:03:16Z",
      "topics": [],
      "readme": "# Machine Predictive Maintenance Classification\n![Python](https://img.shields.io/badge/python-3.11-blue.svg)\n![scikit-learn](https://img.shields.io/badge/scikit--learn-0.24.2-orange.svg)\n![Streamlit](https://img.shields.io/badge/Streamlit-0.87.0-blueviolet.svg)\n![Pandas](https://img.shields.io/badge/Pandas-1.3.3-brightgreen.svg)\n![Joblib](https://img.shields.io/badge/Joblib-1.1.1-yellow.svg)\n\nThis application is designed to predict machine failure for predictive maintenance using machine learning. It utilizes a synthetic dataset with 10,000 data points and 14 features. The application is built using a Random Forest model to classify whether the machine will experience failure or not based on the provided inputs.\n### You can find the app up and running here: https://predictive-maintenance-app.streamlit.app/\n\n### Kaggle Dataset\nClick [here](https://www.kaggle.com/datasets/shivamb/machine-predictive-maintenance-classification) to view the dataset used in this project.\n\n## Dataset Description\n\nThe dataset consists of the following features:\n- `UID`: Unique identifier ranging from 1 to 10000.\n- `productID`: Product quality variant with letters L, M, or H, and a variant-specific serial number.\n- `air temperature [K]`: Generated using a random walk process, later normalized to a standard deviation of 2 K around 300 K.\n- `process temperature [K]`: Generated using a random walk process, normalized to a standard deviation of 1 K, added to the air temperature plus 10 K.\n- `rotational speed [rpm]`: Calculated from power of 2860 W, overlaid with normally distributed noise.\n- `torque [Nm]`: Torque values are normally distributed around 40 Nm with an Ïƒ = 10 Nm and no negative values.\n- `tool wear [min]`: The quality variants H/M/L add 5/3/2 minutes of tool wear to the used tool in the process.\n- `machine failure`: A label that indicates whether the machine has failed in this particular data point for any of the following failure modes.\n\n## How to Use the Application\n\nTo use the application, follow these steps:\n1. Make sure you have Python installed on your system.\n2. Install the required packages by running `pip install streamlit pandas scikit-learn` in your terminal or command prompt.\n3. Clone this repository to your local machine.\n\n## Running the Application\n\nNavigate to the cloned repository in your terminal or command prompt, then run the following command:\n\n```bash\nstreamlit run app.py\n```\n"
    },
    {
      "name": "paaschdigital/autogen",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/100975984?s=40&v=4",
      "owner": "paaschdigital",
      "repo_name": "autogen",
      "description": "A programming framework for agentic AI. Discord: https://aka.ms/autogen-dc. Roadmap: https://aka.ms/autogen-roadmap",
      "homepage": "https://microsoft.github.io/autogen/",
      "language": "Jupyter Notebook",
      "created_at": "2024-05-26T14:37:57Z",
      "updated_at": "2024-08-31T18:13:48Z",
      "topics": [],
      "readme": "<a name=\"readme-top\"></a>\n\n[![PyPI version](https://badge.fury.io/py/pyautogen.svg)](https://badge.fury.io/py/pyautogen)\n[![Build](https://github.com/microsoft/autogen/actions/workflows/python-package.yml/badge.svg)](https://github.com/microsoft/autogen/actions/workflows/python-package.yml)\n![Python Version](https://img.shields.io/badge/3.8%20%7C%203.9%20%7C%203.10%20%7C%203.11%20%7C%203.12-blue)\n[![Downloads](https://static.pepy.tech/badge/pyautogen/week)](https://pepy.tech/project/pyautogen)\n[![Discord](https://img.shields.io/discord/1153072414184452236?logo=discord&style=flat)](https://aka.ms/autogen-dc)\n[![Twitter](https://img.shields.io/twitter/url/https/twitter.com/cloudposse.svg?style=social&label=Follow%20%40pyautogen)](https://twitter.com/pyautogen)\n\n[![NuGet version](https://badge.fury.io/nu/AutoGen.Core.svg)](https://badge.fury.io/nu/AutoGen.Core)\n\n# AutoGen\n[📚 Cite paper](#related-papers).\n<!-- <p align=\"center\">\n    <img src=\"https://github.com/microsoft/autogen/blob/main/website/static/img/flaml.svg\"  width=200>\n    <br>\n</p> -->\n:fire: May 29, 2024: DeepLearning.ai launched a new short course [AI Agentic Design Patterns with AutoGen](https://www.deeplearning.ai/short-courses/ai-agentic-design-patterns-with-autogen), made in collaboration with Microsoft and Penn State University, and taught by AutoGen creators [Chi Wang](https://github.com/sonichi) and [Qingyun Wu](https://github.com/qingyun-wu).\n\n:fire: May 24, 2024: Foundation Capital published an article on [Forbes: The Promise of Multi-Agent AI](https://www.forbes.com/sites/joannechen/2024/05/24/the-promise-of-multi-agent-ai/?sh=2c1e4f454d97) and a video [AI in the Real World Episode 2: Exploring Multi-Agent AI and AutoGen with Chi Wang](https://www.youtube.com/watch?v=RLwyXRVvlNk).\n\n:fire: May 13, 2024: [The Economist](https://www.economist.com/science-and-technology/2024/05/13/todays-ai-models-are-impressive-teams-of-them-will-be-formidable) published an article about multi-agent systems (MAS) following a January 2024 interview with [Chi Wang](https://github.com/sonichi).\n\n:fire: May 11, 2024: [AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation](https://openreview.net/pdf?id=uAjxFFing2) received the best paper award at the [ICLR 2024 LLM Agents Workshop](https://llmagents.github.io/).\n\n:fire: Apr 26, 2024: [AutoGen.NET](https://microsoft.github.io/autogen-for-net/) is available for .NET developers!\n\n:fire: Apr 17, 2024: Andrew Ng cited AutoGen in [The Batch newsletter](https://www.deeplearning.ai/the-batch/issue-245/) and [What's next for AI agentic workflows](https://youtu.be/sal78ACtGTc?si=JduUzN_1kDnMq0vF) at Sequoia Capital's AI Ascent (Mar 26).\n\n:fire: Mar 3, 2024: What's new in AutoGen? 📰[Blog](https://microsoft.github.io/autogen/blog/2024/03/03/AutoGen-Update); 📺[Youtube](https://www.youtube.com/watch?v=j_mtwQiaLGU).\n\n:fire: Mar 1, 2024: the first AutoGen multi-agent experiment on the challenging [GAIA](https://huggingface.co/spaces/gaia-benchmark/leaderboard) benchmark achieved the No. 1 accuracy in all the three levels.\n\n<!-- :tada: Jan 30, 2024: AutoGen is highlighted by Peter Lee in Microsoft Research Forum [Keynote](https://t.co/nUBSjPDjqD). -->\n\n:tada: Dec 31, 2023: [AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation Framework](https://arxiv.org/abs/2308.08155) is selected by [TheSequence: My Five Favorite AI Papers of 2023](https://thesequence.substack.com/p/my-five-favorite-ai-papers-of-2023).\n\n<!-- :fire: Nov 24: pyautogen [v0.2](https://github.com/microsoft/autogen/releases/tag/v0.2.0) is released with many updates and new features compared to v0.1.1. It switches to using openai-python v1. Please read the [migration guide](https://microsoft.github.io/autogen/docs/Installation#python). -->\n\n<!-- :fire: Nov 11: OpenAI's Assistants are available in AutoGen and interoperatable with other AutoGen agents! Checkout our [blogpost](https://microsoft.github.io/autogen/blog/2023/11/13/OAI-assistants) for details and examples. -->\n\n:tada: Nov 8, 2023: AutoGen is selected into [Open100: Top 100 Open Source achievements](https://www.benchcouncil.org/evaluation/opencs/annual.html) 35 days after spinoff from [FLAML](https://github.com/microsoft/FLAML).\n\n<!-- :tada: Nov 6, 2023: AutoGen is mentioned by Satya Nadella in a [fireside chat](https://youtu.be/0pLBvgYtv6U). -->\n\n<!-- :tada: Nov 1, 2023: AutoGen is the top trending repo on GitHub in October 2023. -->\n\n<!-- :tada: Oct 03, 2023: AutoGen spins off from [FLAML](https://github.com/microsoft/FLAML) on GitHub. -->\n\n<!-- :tada: Aug 16: Paper about AutoGen on [arxiv](https://arxiv.org/abs/2308.08155). -->\n\n:tada: Mar 29, 2023: AutoGen is first created in [FLAML](https://github.com/microsoft/FLAML).\n\n<!--\n:fire: FLAML is highlighted in OpenAI's [cookbook](https://github.com/openai/openai-cookbook#related-resources-from-around-the-web).\n\n:fire: [autogen](https://microsoft.github.io/autogen/) is released with support for ChatGPT and GPT-4, based on [Cost-Effective Hyperparameter Optimization for Large Language Model Generation Inference](https://arxiv.org/abs/2303.04673).\n\n:fire: FLAML supports Code-First AutoML & Tuning – Private Preview in [Microsoft Fabric Data Science](https://learn.microsoft.com/en-us/fabric/data-science/). -->\n\n<p align=\"right\" style=\"font-size: 14px; color: #555; margin-top: 20px;\">\n  <a href=\"#readme-top\" style=\"text-decoration: none; color: blue; font-weight: bold;\">\n    ↑ Back to Top ↑\n  </a>\n</p>\n\n## What is AutoGen\n\nAutoGen is an open-source programming framework for building AI agents and facilitating cooperation among multiple agents to solve tasks. AutoGen aims to streamline the development and research of agentic AI, much like PyTorch does for Deep Learning. It offers features such as agents capable of interacting with each other, facilitates the use of various large language models (LLMs) and tool use support, autonomous and human-in-the-loop workflows, and multi-agent conversation patterns.\n\n**Open Source Statement**: The project welcomes contributions from developers and organizations worldwide. Our goal is to foster a collaborative and inclusive community where diverse perspectives and expertise can drive innovation and enhance the project's capabilities. Whether you are an individual contributor or represent an organization, we invite you to join us in shaping the future of this project. Together, we can build something truly remarkable.\n\nThe project is currently maintained by a [dynamic group of volunteers](https://butternut-swordtail-8a5.notion.site/410675be605442d3ada9a42eb4dfef30?v=fa5d0a79fd3d4c0f9c112951b2831cbb&pvs=4) from several different organizations. Contact project administrators Chi Wang and Qingyun Wu via auto-gen@outlook.com if you are interested in becoming a maintainer.\n\n\n![AutoGen Overview](https://github.com/microsoft/autogen/blob/main/website/static/img/autogen_agentchat.png)\n\n- AutoGen enables building next-gen LLM applications based on [multi-agent conversations](https://microsoft.github.io/autogen/docs/Use-Cases/agent_chat) with minimal effort. It simplifies the orchestration, automation, and optimization of a complex LLM workflow. It maximizes the performance of LLM models and overcomes their weaknesses.\n- It supports [diverse conversation patterns](https://microsoft.github.io/autogen/docs/Use-Cases/agent_chat#supporting-diverse-conversation-patterns) for complex workflows. With customizable and conversable agents, developers can use AutoGen to build a wide range of conversation patterns concerning conversation autonomy,\n  the number of agents, and agent conversation topology.\n- It provides a collection of working systems with different complexities. These systems span a [wide range of applications](https://microsoft.github.io/autogen/docs/Use-Cases/agent_chat#diverse-applications-implemented-with-autogen) from various domains and complexities. This demonstrates how AutoGen can easily support diverse conversation patterns.\n- AutoGen provides [enhanced LLM inference](https://microsoft.github.io/autogen/docs/Use-Cases/enhanced_inference#api-unification). It offers utilities like API unification and caching, and advanced usage patterns, such as error handling, multi-config inference, context programming, etc.\n\nAutoGen is created out of collaborative [research](https://microsoft.github.io/autogen/docs/Research) from Microsoft, Penn State University, and the University of Washington.\n\n<p align=\"right\" style=\"font-size: 14px; color: #555; margin-top: 20px;\">\n  <a href=\"#readme-top\" style=\"text-decoration: none; color: blue; font-weight: bold;\">\n    ↑ Back to Top ↑\n  </a>\n</p>\n\n## Roadmaps\n\nTo see what we are working on and what we plan to work on, please check our\n[Roadmap Issues](https://aka.ms/autogen-roadmap).\n\n<p align=\"right\" style=\"font-size: 14px; color: #555; margin-top: 20px;\">\n  <a href=\"#readme-top\" style=\"text-decoration: none; color: blue; font-weight: bold;\">\n    ↑ Back to Top ↑\n  </a>\n</p>\n\n## Quickstart\nThe easiest way to start playing is\n1. Click below to use the GitHub Codespace\n\n    [![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://codespaces.new/microsoft/autogen?quickstart=1)\n\n 2. Copy OAI_CONFIG_LIST_sample to ./notebook folder, name to OAI_CONFIG_LIST, and set the correct configuration.\n 3. Start playing with the notebooks!\n\n*NOTE*: OAI_CONFIG_LIST_sample lists GPT-4 as the default model, as this represents our current recommendation, and is known to work well with AutoGen. If you use a model other than GPT-4, you may need to revise various system prompts (especially if using weaker models like GPT-3.5-turbo). Moreover, if you use models other than those hosted by OpenAI or Azure, you may incur additional risks related to alignment and safety. Proceed with caution if updating this default.\n\n<p align=\"right\" style=\"font-size: 14px; color: #555; margin-top: 20px;\">\n  <a href=\"#readme-top\" style=\"text-decoration: none; color: blue; font-weight: bold;\">\n    ↑ Back to Top ↑\n  </a>\n</p>\n\n## [Installation](https://microsoft.github.io/autogen/docs/Installation)\n### Option 1. Install and Run AutoGen in Docker\n\nFind detailed instructions for users [here](https://microsoft.github.io/autogen/docs/installation/Docker#step-1-install-docker), and for developers [here](https://microsoft.github.io/autogen/docs/Contribute#docker-for-development).\n\n### Option 2. Install AutoGen Locally\n\nAutoGen requires **Python version >= 3.8, < 3.13**. It can be installed from pip:\n\n```bash\npip install pyautogen\n```\n\nMinimal dependencies are installed without extra options. You can install extra options based on the feature you need.\n\n<!-- For example, use the following to install the dependencies needed by the [`blendsearch`](https://microsoft.github.io/FLAML/docs/Use-Cases/Tune-User-Defined-Function#blendsearch-economical-hyperparameter-optimization-with-blended-search-strategy) option.\n```bash\npip install \"pyautogen[blendsearch]\"\n``` -->\n\nFind more options in [Installation](https://microsoft.github.io/autogen/docs/Installation#option-2-install-autogen-locally-using-virtual-environment).\n\n<!-- Each of the [`notebook examples`](https://github.com/microsoft/autogen/tree/main/notebook) may require a specific option to be installed. -->\n\nEven if you are installing and running AutoGen locally outside of docker, the recommendation and default behavior of agents is to perform [code execution](https://microsoft.github.io/autogen/docs/FAQ/#code-execution) in docker. Find more instructions and how to change the default behaviour [here](https://microsoft.github.io/autogen/docs/Installation#code-execution-with-docker-(default)).\n\nFor LLM inference configurations, check the [FAQs](https://microsoft.github.io/autogen/docs/FAQ#set-your-api-endpoints).\n\n<p align=\"right\" style=\"font-size: 14px; color: #555; margin-top: 20px;\">\n  <a href=\"#readme-top\" style=\"text-decoration: none; color: blue; font-weight: bold;\">\n    ↑ Back to Top ↑\n  </a>\n</p>\n\n## Multi-Agent Conversation Framework\n\nAutogen enables the next-gen LLM applications with a generic [multi-agent conversation](https://microsoft.github.io/autogen/docs/Use-Cases/agent_chat) framework. It offers customizable and conversable agents that integrate LLMs, tools, and humans.\nBy automating chat among multiple capable agents, one can easily make them collectively perform tasks autonomously or with human feedback, including tasks that require using tools via code.\n\nFeatures of this use case include:\n\n- **Multi-agent conversations**: AutoGen agents can communicate with each other to solve tasks. This allows for more complex and sophisticated applications than would be possible with a single LLM.\n- **Customization**: AutoGen agents can be customized to meet the specific needs of an application. This includes the ability to choose the LLMs to use, the types of human input to allow, and the tools to employ.\n- **Human participation**: AutoGen seamlessly allows human participation. This means that humans can provide input and feedback to the agents as needed.\n\nFor [example](https://github.com/microsoft/autogen/blob/main/test/twoagent.py),\n\n```python\nfrom autogen import AssistantAgent, UserProxyAgent, config_list_from_json\n# Load LLM inference endpoints from an env variable or a file\n# See https://microsoft.github.io/autogen/docs/FAQ#set-your-api-endpoints\n# and OAI_CONFIG_LIST_sample\nconfig_list = config_list_from_json(env_or_file=\"OAI_CONFIG_LIST\")\n# You can also set config_list directly as a list, for example, config_list = [{'model': 'gpt-4', 'api_key': '<your OpenAI API key here>'},]\nassistant = AssistantAgent(\"assistant\", llm_config={\"config_list\": config_list})\nuser_proxy = UserProxyAgent(\"user_proxy\", code_execution_config={\"work_dir\": \"coding\", \"use_docker\": False}) # IMPORTANT: set to True to run code in docker, recommended\nuser_proxy.initiate_chat(assistant, message=\"Plot a chart of NVDA and TESLA stock price change YTD.\")\n# This initiates an automated chat between the two agents to solve the task\n```\n\nThis example can be run with\n\n```python\npython test/twoagent.py\n```\n\nAfter the repo is cloned.\nThe figure below shows an example conversation flow with AutoGen.\n![Agent Chat Example](https://github.com/microsoft/autogen/blob/main/website/static/img/chat_example.png)\n\nAlternatively, the [sample code](https://github.com/microsoft/autogen/blob/main/samples/simple_chat.py) here allows a user to chat with an AutoGen agent in ChatGPT style.\nPlease find more [code examples](https://microsoft.github.io/autogen/docs/Examples#automated-multi-agent-chat) for this feature.\n\n<p align=\"right\" style=\"font-size: 14px; color: #555; margin-top: 20px;\">\n  <a href=\"#readme-top\" style=\"text-decoration: none; color: blue; font-weight: bold;\">\n    ↑ Back to Top ↑\n  </a>\n</p>\n\n## Enhanced LLM Inferences\n\nAutogen also helps maximize the utility out of the expensive LLMs such as ChatGPT and GPT-4. It offers [enhanced LLM inference](https://microsoft.github.io/autogen/docs/Use-Cases/enhanced_inference#api-unification) with powerful functionalities like caching, error handling, multi-config inference and templating.\n\n<!-- For example, you can optimize generations by LLM with your own tuning data, success metrics, and budgets.\n\n```python\n# perform tuning for openai<1\nconfig, analysis = autogen.Completion.tune(\n    data=tune_data,\n    metric=\"success\",\n    mode=\"max\",\n    eval_func=eval_func,\n    inference_budget=0.05,\n    optimization_budget=3,\n    num_samples=-1,\n)\n# perform inference for a test instance\nresponse = autogen.Completion.create(context=test_instance, **config)\n```\n\nPlease find more [code examples](https://microsoft.github.io/autogen/docs/Examples#tune-gpt-models) for this feature. -->\n\n<p align=\"right\" style=\"font-size: 14px; color: #555; margin-top: 20px;\">\n  <a href=\"#readme-top\" style=\"text-decoration: none; color: blue; font-weight: bold;\">\n    ↑ Back to Top ↑\n  </a>\n</p>\n\n## Documentation\n\nYou can find detailed documentation about AutoGen [here](https://microsoft.github.io/autogen/).\n\nIn addition, you can find:\n\n- [Research](https://microsoft.github.io/autogen/docs/Research), [blogposts](https://microsoft.github.io/autogen/blog) around AutoGen, and [Transparency FAQs](https://github.com/microsoft/autogen/blob/main/TRANSPARENCY_FAQS.md)\n\n- [Discord](https://aka.ms/autogen-dc)\n\n- [Contributing guide](https://microsoft.github.io/autogen/docs/Contribute)\n\n- [Roadmap](https://github.com/orgs/microsoft/projects/989/views/3)\n\n<p align=\"right\" style=\"font-size: 14px; color: #555; margin-top: 20px;\">\n  <a href=\"#readme-top\" style=\"text-decoration: none; color: blue; font-weight: bold;\">\n    ↑ Back to Top ↑\n  </a>\n</p>\n\n## Related Papers\n\n[AutoGen](https://arxiv.org/abs/2308.08155)\n\n```\n@inproceedings{wu2023autogen,\n      title={AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation Framework},\n      author={Qingyun Wu and Gagan Bansal and Jieyu Zhang and Yiran Wu and Beibin Li and Erkang Zhu and Li Jiang and Xiaoyun Zhang and Shaokun Zhang and Jiale Liu and Ahmed Hassan Awadallah and Ryen W White and Doug Burger and Chi Wang},\n      year={2023},\n      eprint={2308.08155},\n      archivePrefix={arXiv},\n      primaryClass={cs.AI}\n}\n```\n\n[EcoOptiGen](https://arxiv.org/abs/2303.04673)\n\n```\n@inproceedings{wang2023EcoOptiGen,\n    title={Cost-Effective Hyperparameter Optimization for Large Language Model Generation Inference},\n    author={Chi Wang and Susan Xueqing Liu and Ahmed H. Awadallah},\n    year={2023},\n    booktitle={AutoML'23},\n}\n```\n\n[MathChat](https://arxiv.org/abs/2306.01337)\n\n```\n@inproceedings{wu2023empirical,\n    title={An Empirical Study on Challenging Math Problem Solving with GPT-4},\n    author={Yiran Wu and Feiran Jia and Shaokun Zhang and Hangyu Li and Erkang Zhu and Yue Wang and Yin Tat Lee and Richard Peng and Qingyun Wu and Chi Wang},\n    year={2023},\n    booktitle={ArXiv preprint arXiv:2306.01337},\n}\n```\n\n[AgentOptimizer](https://arxiv.org/pdf/2402.11359)\n\n```\n@article{zhang2024training,\n  title={Training Language Model Agents without Modifying Language Models},\n  author={Zhang, Shaokun and Zhang, Jieyu and Liu, Jiale and Song, Linxin and Wang, Chi and Krishna, Ranjay and Wu, Qingyun},\n  journal={ICML'24},\n  year={2024}\n}\n```\n\n[StateFlow](https://arxiv.org/abs/2403.11322)\n```\n@article{wu2024stateflow,\n  title={StateFlow: Enhancing LLM Task-Solving through State-Driven Workflows},\n  author={Wu, Yiran and Yue, Tianwei and Zhang, Shaokun and Wang, Chi and Wu, Qingyun},\n  journal={arXiv preprint arXiv:2403.11322},\n  year={2024}\n}\n```\n\n<p align=\"right\" style=\"font-size: 14px; color: #555; margin-top: 20px;\">\n  <a href=\"#readme-top\" style=\"text-decoration: none; color: blue; font-weight: bold;\">\n    ↑ Back to Top ↑\n  </a>\n</p>\n\n## Contributing\n\nThis project welcomes contributions and suggestions. Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit <https://cla.opensource.microsoft.com>.\n\nIf you are new to GitHub, [here](https://opensource.guide/how-to-contribute/#how-to-submit-a-contribution) is a detailed help source on getting involved with development on GitHub.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information, see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n<p align=\"right\" style=\"font-size: 14px; color: #555; margin-top: 20px;\">\n  <a href=\"#readme-top\" style=\"text-decoration: none; color: blue; font-weight: bold;\">\n    ↑ Back to Top ↑\n  </a>\n</p>\n\n## Contributors Wall\n<a href=\"https://github.com/microsoft/autogen/graphs/contributors\">\n  <img src=\"https://contrib.rocks/image?repo=microsoft/autogen&max=204\" />\n</a>\n\n<p align=\"right\" style=\"font-size: 14px; color: #555; margin-top: 20px;\">\n  <a href=\"#readme-top\" style=\"text-decoration: none; color: blue; font-weight: bold;\">\n    ↑ Back to Top ↑\n  </a>\n</p>\n\n# Legal Notices\n\nMicrosoft and any contributors grant you a license to the Microsoft documentation and other content\nin this repository under the [Creative Commons Attribution 4.0 International Public License](https://creativecommons.org/licenses/by/4.0/legalcode),\nsee the [LICENSE](LICENSE) file, and grant you a license to any code in the repository under the [MIT License](https://opensource.org/licenses/MIT), see the\n[LICENSE-CODE](LICENSE-CODE) file.\n\nMicrosoft, Windows, Microsoft Azure, and/or other Microsoft products and services referenced in the documentation\nmay be either trademarks or registered trademarks of Microsoft in the United States and/or other countries.\nThe licenses for this project do not grant you rights to use any Microsoft names, logos, or trademarks.\nMicrosoft's general trademark guidelines can be found at http://go.microsoft.com/fwlink/?LinkID=254653.\n\nPrivacy information can be found at https://privacy.microsoft.com/en-us/\n\nMicrosoft and any contributors reserve all other rights, whether under their respective copyrights, patents,\nor trademarks, whether by implication, estoppel, or otherwise.\n\n<p align=\"right\" style=\"font-size: 14px; color: #555; margin-top: 20px;\">\n  <a href=\"#readme-top\" style=\"text-decoration: none; color: blue; font-weight: bold;\">\n    ↑ Back to Top ↑\n  </a>\n</p>\n"
    },
    {
      "name": "jmservera/miscdemos",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/8036360?s=40&v=4",
      "owner": "jmservera",
      "repo_name": "miscdemos",
      "description": "A miscelanea of all demos and templates to deploy them",
      "homepage": null,
      "language": "Bicep",
      "created_at": "2024-04-16T13:40:52Z",
      "updated_at": "2025-02-26T13:14:28Z",
      "topics": [],
      "readme": "# Miscellaneus demos\n\nAn attempt to have a compendium of demos with some templates to deploy the needed resources in Azure.\n\n* [Event Grid for IoT](./eventgrid/README.md): a demo of how to use Azure Event Grid MQTT capabilities to build an IoT environment, with some devices to test features like telemetry gathering, C2D messages, security, dashboard building, etc.\n* [OCPP Server](./ocpp-server/README.md): an example deployment of a website and a Web PubSub service to simulate an OCPP 1.6 service behind an Application Gateway. Demonstrates:\n  * Application Gateway rewrite rules\n  * Private Endpoints\n  * WebApp Virtual Network integration\n  * Web PubSub custom protocol\n  * NAT Gateway\n* [AI](./ai/): different demos demonstrating UX for AI with FluentAI, some Microsoft Copilot extension demos and a couple of Semantic Kernel simple demos for a console app and a web app.\n* [Prompt Engineering Lab](https://jmservera.github.io/miscdemos/prompt-engineering): this repo contains the source of the Prompt Engineering Lab, a hands-on lab for learning the basic principles of Prompt Engineering.\n"
    },
    {
      "name": "ehimen-io/resume-helper",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/43546100?s=40&v=4",
      "owner": "ehimen-io",
      "repo_name": "resume-helper",
      "description": "AI assisted resume-helper",
      "homepage": null,
      "language": "Python",
      "created_at": "2024-05-01T18:17:13Z",
      "updated_at": "2024-09-01T06:25:38Z",
      "topics": [],
      "readme": "# resume-helper\n## Description\n\nAn AI assisted resume helper to streamline the process of tailoring resumes to jobs.\n\nIt will:\n1. Generate bullet points relevant to the job posting\n2. Estimate the years of experience needed for the job\n3. List the tech stack needed to be a successful applicant\n\nIn addition, `resume-helper` also features a chat interface to ask questions about your resume and your chances of getting the job.\n\n## Installation\n\nThis project requires Python 3.7 or higher. \n\n1. Clone the repository:\n    ```\n    git clone https://github.com/ehimen-io/resume-helper.git\n    cd resume-helper\n    ```\n\n2. Create a virtual environment (optional, but recommended):\n    ```\n    python3 -m venv env\n    source env/bin/activate\n    ```\n\n3. Install the required packages:\n    ```\n    pip install -r requirements.txt\n    ```\n\n## Configuration\n\nThis project uses the `python-dotenv` package for environment variables.\n> At this time, It is important to have an OpenAI API key to run this project.\n> Navigate to https://platform.openai.com/docs/quickstart to understand how to create your own API key.\n\n1. Copy the `.env.example` file and create a new `.env` file:\n    ```\n    cp .env.example .env\n    ```\n\n2. Open the `.env` file and set your environment variables (specifically your OpenAI API key).\n\n## Running the Project\n\nAfter installing the dependencies and setting up the environment variables, you can run the project with:\n```\npython main.py\n```\n\n## Usage\nAfter running the project:\n1. Copy the job description into `/resources/job.txt`\n2. Copy your resume into `/resources/resume.txt`\n3. Type `g` and press `Enter` to generate resume bullet points, tech stack and deduce the years of experience \n4. Type `c` and press `Enter` to enter the chat interface:\n   1. Type `qc` and press `Enter` while in the chat interface to exit the chat\n5. Type `q` and press `Enter` at any point to exit the application\n\n## Feature requests\nPlease open an issue for all feature requests and bugs.\n\n"
    },
    {
      "name": "yuxiaobopp/metagpt_spider",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/22901629?s=40&v=4",
      "owner": "yuxiaobopp",
      "repo_name": "metagpt_spider",
      "description": "python spider,base on metagpt v0.8.0",
      "homepage": null,
      "language": "Python",
      "created_at": "2024-04-18T02:24:43Z",
      "updated_at": "2024-04-20T14:39:28Z",
      "topics": [],
      "readme": "# metagpt_spider\npython spider,base on metagpt v0.8.0\n"
    },
    {
      "name": "Maximiliano-Villanueva/orchestrator-semantic-kernel-api",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/76200479?s=40&v=4",
      "owner": "Maximiliano-Villanueva",
      "repo_name": "orchestrator-semantic-kernel-api",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2024-04-16T13:46:18Z",
      "updated_at": "2024-04-16T13:50:50Z",
      "topics": [],
      "readme": ""
    },
    {
      "name": "onhello-automation/otto",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/83440597?s=40&v=4",
      "owner": "onhello-automation",
      "repo_name": "otto",
      "description": "Automate control of your PC",
      "homepage": "",
      "language": "Python",
      "created_at": "2024-03-04T02:25:15Z",
      "updated_at": "2024-05-06T03:04:50Z",
      "topics": [],
      "readme": "# otto\nHelp AI control your PC.\n\nWe'll try to write some custom code for specific scenarios in specific applications like responding in Microsoft Teams and writing code in Visual Studio Code.\nThis will help us learn what is possible with the available tools.\nThen we can try to generalize the integration so that a model can be given the available actions as context and it can decide what should be done in the application or in other applications.\n\nSee the [Windows app](./python/README.md) folder for setup instructions.\n"
    },
    {
      "name": "cerberuswilliam/MetaGPT",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/39875358?s=40&v=4",
      "owner": "cerberuswilliam",
      "repo_name": "MetaGPT",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2024-03-17T09:50:34Z",
      "updated_at": "2024-03-17T09:54:33Z",
      "topics": [],
      "readme": "\n# MetaGPT: The Multi-Agent Framework\n\n<p align=\"center\">\n<a href=\"\"><img src=\"docs/resources/MetaGPT-new-log.png\" alt=\"MetaGPT logo: Enable GPT to work in software company, collaborating to tackle more complex tasks.\" width=\"150px\"></a>\n</p>\n\n<p align=\"center\">\n<b>Assign different roles to GPTs to form a collaborative entity for complex tasks.</b>\n</p>\n\n<p align=\"center\">\n<a href=\"docs/README_CN.md\"><img src=\"https://img.shields.io/badge/文档-中文版-blue.svg\" alt=\"CN doc\"></a>\n<a href=\"README.md\"><img src=\"https://img.shields.io/badge/document-English-blue.svg\" alt=\"EN doc\"></a>\n<a href=\"docs/README_JA.md\"><img src=\"https://img.shields.io/badge/ドキュメント-日本語-blue.svg\" alt=\"JA doc\"></a>\n<a href=\"https://opensource.org/licenses/MIT\"><img src=\"https://img.shields.io/badge/License-MIT-blue.svg\" alt=\"License: MIT\"></a>\n<a href=\"docs/ROADMAP.md\"><img src=\"https://img.shields.io/badge/ROADMAP-路线图-blue\" alt=\"roadmap\"></a>\n<a href=\"https://discord.gg/DYn29wFk9z\"><img src=\"https://dcbadge.vercel.app/api/server/DYn29wFk9z?style=flat\" alt=\"Discord Follow\"></a>\n<a href=\"https://twitter.com/MetaGPT_\"><img src=\"https://img.shields.io/twitter/follow/MetaGPT?style=social\" alt=\"Twitter Follow\"></a>\n</p>\n\n<p align=\"center\">\n   <a href=\"https://vscode.dev/redirect?url=vscode://ms-vscode-remote.remote-containers/cloneInVolume?url=https://github.com/geekan/MetaGPT\"><img src=\"https://img.shields.io/static/v1?label=Dev%20Containers&message=Open&color=blue&logo=visualstudiocode\" alt=\"Open in Dev Containers\"></a>\n   <a href=\"https://codespaces.new/geekan/MetaGPT\"><img src=\"https://img.shields.io/badge/Github_Codespace-Open-blue?logo=github\" alt=\"Open in GitHub Codespaces\"></a>\n   <a href=\"https://huggingface.co/spaces/deepwisdom/MetaGPT\" target=\"_blank\"><img alt=\"Hugging Face\" src=\"https://img.shields.io/badge/%F0%9F%A4%97%20-Hugging%20Face-blue?color=blue&logoColor=white\" /></a>\n</p>\n\n## News\n🚀 Mar. 14, 2024: Our Data Interpreter paper is on [arxiv](https://arxiv.org/abs/2402.18679). Check the [example](https://docs.deepwisdom.ai/main/en/DataInterpreter/) and [code](https://github.com/geekan/MetaGPT/tree/main/examples/di)!\n\n🚀 Feb. 08, 2024: [v0.7.0](https://github.com/geekan/MetaGPT/releases/tag/v0.7.0) released, supporting assigning different LLMs to different Roles. We also introduced [Data Interpreter](https://github.com/geekan/MetaGPT/blob/main/examples/di/README.md), a powerful agent capable of solving a wide range of real-world problems.\n\n🚀 Jan. 16, 2024: Our paper [MetaGPT: Meta Programming for A Multi-Agent Collaborative Framework\n](https://arxiv.org/abs/2308.00352) accepted for oral presentation **(top 1.2%)** at ICLR 2024, **ranking #1** in the LLM-based Agent category.\n\n🚀 Jan. 03, 2024: [v0.6.0](https://github.com/geekan/MetaGPT/releases/tag/v0.6.0) released, new features include serialization, upgraded OpenAI package and supported multiple LLM, provided [minimal example for debate](https://github.com/geekan/MetaGPT/blob/main/examples/debate_simple.py) etc.\n\n🚀 Dec. 15, 2023: [v0.5.0](https://github.com/geekan/MetaGPT/releases/tag/v0.5.0) released, introducing some experimental features such as **incremental development**, **multilingual**, **multiple programming languages**, etc.\n\n🔥 Nov. 08, 2023: MetaGPT is selected into [Open100: Top 100 Open Source achievements](https://www.benchcouncil.org/evaluation/opencs/annual.html).\n\n🔥 Sep. 01, 2023: MetaGPT tops GitHub Trending Monthly for the **17th time** in August 2023.\n\n🌟 Jun. 30, 2023: MetaGPT is now open source.\n\n🌟 Apr. 24, 2023: First line of MetaGPT code committed.\n\n## Software Company as Multi-Agent System\n\n1. MetaGPT takes a **one line requirement** as input and outputs **user stories / competitive analysis / requirements / data structures / APIs / documents, etc.**\n2. Internally, MetaGPT includes **product managers / architects / project managers / engineers.** It provides the entire process of a **software company along with carefully orchestrated SOPs.**\n   1. `Code = SOP(Team)` is the core philosophy. We materialize SOP and apply it to teams composed of LLMs.\n\n![A software company consists of LLM-based roles](docs/resources/software_company_cd.jpeg)\n\n<p align=\"center\">Software Company Multi-Agent Schematic (Gradually Implementing)</p>\n\n## Install\n\n### Pip installation\n\n> Ensure that Python 3.9+ is installed on your system. You can check this by using: `python --version`.  \n> You can use conda like this: `conda create -n metagpt python=3.9 && conda activate metagpt`\n\n```bash\npip install metagpt\n# https://docs.deepwisdom.ai/main/en/guide/get_started/configuration.html\nmetagpt --init-config  # it will create ~/.metagpt/config2.yaml, just modify it to your needs\n```\n\n### Configuration\n\nYou can configure `~/.metagpt/config2.yaml` according to the [example](https://github.com/geekan/MetaGPT/blob/main/config/config2.example.yaml) and [doc](https://docs.deepwisdom.ai/main/en/guide/get_started/configuration.html):\n\n```yaml\nllm:\n  api_type: \"openai\"  # or azure / ollama / open_llm etc. Check LLMType for more options\n  model: \"gpt-4-turbo-preview\"  # or gpt-3.5-turbo-1106 / gpt-4-1106-preview\n  base_url: \"https://api.openai.com/v1\"  # or forward url / other llm url\n  api_key: \"YOUR_API_KEY\"\n```\n\n### Usage\n\nAfter installation, you can use it as CLI\n\n```bash\nmetagpt \"Create a 2048 game\"  # this will create a repo in ./workspace\n```\n\nor you can use it as library\n\n```python\nfrom metagpt.software_company import generate_repo, ProjectRepo\nrepo: ProjectRepo = generate_repo(\"Create a 2048 game\")  # or ProjectRepo(\"<path>\")\nprint(repo)  # it will print the repo structure with files\n```\n\ndetail installation please refer to [cli_install](https://docs.deepwisdom.ai/main/en/guide/get_started/installation.html#install-stable-version)\n or [docker_install](https://docs.deepwisdom.ai/main/en/guide/get_started/installation.html#install-with-docker)\n\n### Docker installation\n<details><summary><strong>⏬ Step 1: Download metagpt image and prepare config2.yaml </strong><i>:: click to expand ::</i></summary>\n<div>\n\n```bash\ndocker pull metagpt/metagpt:latest\nmkdir -p /opt/metagpt/{config,workspace}\ndocker run --rm metagpt/metagpt:latest cat /app/metagpt/config/config2.yaml > /opt/metagpt/config/config2.yaml\nvim /opt/metagpt/config/config2.yaml # Change the config\n```\n\n</div>\n</details>\n\n<details><summary><strong>⏬ Step 2: Run metagpt container </strong><i>:: click to expand ::</i></summary>\n<div>\n\n```bash\ndocker run --name metagpt -d \\\n    --privileged \\\n    -v /opt/metagpt/config/config2.yaml:/app/metagpt/config/config2.yaml \\\n    -v /opt/metagpt/workspace:/app/metagpt/workspace \\\n    metagpt/metagpt:latest\n```\n\n</div>\n</details>\n\n<details><summary><strong>⏬ Step 3: Use metagpt </strong><i>:: click to expand ::</i></summary>\n<div>\n\n```bash\ndocker exec -it metagpt /bin/bash\n$ metagpt \"Create a 2048 game\"  # this will create a repo in ./workspace\n```\n\n</div>\n</details>\n\n### QuickStart & Demo Video\n- Try it on [MetaGPT Huggingface Space](https://huggingface.co/spaces/deepwisdom/MetaGPT)\n- [Matthew Berman: How To Install MetaGPT - Build A Startup With One Prompt!!](https://youtu.be/uT75J_KG_aY)\n- [Official Demo Video](https://github.com/geekan/MetaGPT/assets/2707039/5e8c1062-8c35-440f-bb20-2b0320f8d27d)\n\nhttps://github.com/geekan/MetaGPT/assets/34952977/34345016-5d13-489d-b9f9-b82ace413419\n\n## Tutorial\n\n- 🗒 [Online Document](https://docs.deepwisdom.ai/main/en/)\n- 💻 [Usage](https://docs.deepwisdom.ai/main/en/guide/get_started/quickstart.html)  \n- 🔎 [What can MetaGPT do?](https://docs.deepwisdom.ai/main/en/guide/get_started/introduction.html)\n- 🛠 How to build your own agents? \n  - [MetaGPT Usage & Development Guide | Agent 101](https://docs.deepwisdom.ai/main/en/guide/tutorials/agent_101.html)\n  - [MetaGPT Usage & Development Guide | MultiAgent 101](https://docs.deepwisdom.ai/main/en/guide/tutorials/multi_agent_101.html)\n- 🧑‍💻 Contribution\n  - [Develop Roadmap](docs/ROADMAP.md)\n- 🔖 Use Cases\n  - [Debate](https://docs.deepwisdom.ai/main/en/guide/use_cases/multi_agent/debate.html)\n  - [Researcher](https://docs.deepwisdom.ai/main/en/guide/use_cases/agent/researcher.html)\n  - [Recepit Assistant](https://docs.deepwisdom.ai/main/en/guide/use_cases/agent/receipt_assistant.html)\n- ❓ [FAQs](https://docs.deepwisdom.ai/main/en/guide/faq.html)\n\n## Support\n\n### Discard Join US\n📢 Join Our [Discord Channel](https://discord.gg/ZRHeExS6xv)!\n\nLooking forward to seeing you there! 🎉\n\n### Contact Information\n\nIf you have any questions or feedback about this project, please feel free to contact us. We highly appreciate your suggestions!\n\n- **Email:** alexanderwu@deepwisdom.ai\n- **GitHub Issues:** For more technical inquiries, you can also create a new issue in our [GitHub repository](https://github.com/geekan/metagpt/issues).\n\nWe will respond to all questions within 2-3 business days.\n\n## Citation\n\nIf you use MetaGPT or Data Interpreter in a research paper, please cite our work as follows:\n\n```bibtex\n@misc{hong2023metagpt,\n      title={MetaGPT: Meta Programming for A Multi-Agent Collaborative Framework}, \n      author={Sirui Hong and Mingchen Zhuge and Jonathan Chen and Xiawu Zheng and Yuheng Cheng and Ceyao Zhang and Jinlin Wang and Zili Wang and Steven Ka Shing Yau and Zijuan Lin and Liyang Zhou and Chenyu Ran and Lingfeng Xiao and Chenglin Wu and Jürgen Schmidhuber},\n      year={2023},\n      eprint={2308.00352},\n      archivePrefix={arXiv},\n      primaryClass={cs.AI}\n}\n@misc{hong2024data,\n      title={Data Interpreter: An LLM Agent For Data Science}, \n      author={Sirui Hong and Yizhang Lin and Bang Liu and Bangbang Liu and Binhao Wu and Danyang Li and Jiaqi Chen and Jiayi Zhang and Jinlin Wang and Li Zhang and Lingyao Zhang and Min Yang and Mingchen Zhuge and Taicheng Guo and Tuo Zhou and Wei Tao and Wenyi Wang and Xiangru Tang and Xiangtao Lu and Xiawu Zheng and Xinbing Liang and Yaying Fei and Yuheng Cheng and Zongze Xu and Chenglin Wu},\n      year={2024},\n      eprint={2402.18679},\n      archivePrefix={arXiv},\n      primaryClass={cs.AI}\n}\n\n```\n\n"
    },
    {
      "name": "luoguomao1995/ai_courseware",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/37164874?s=40&v=4",
      "owner": "luoguomao1995",
      "repo_name": "ai_courseware",
      "description": null,
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2024-03-17T08:06:53Z",
      "updated_at": "2024-06-16T12:58:14Z",
      "topics": [],
      "readme": "# AGI 课堂《AI 大模型全栈工程师》课件\n\n## 声明\n\n本目录下的所有文件，包括但不限于文本、图像、音频、视频和其他格式均为瓜皮汤（海南）教育科技有限公司（以下简称“本公司”）所有。版权受中华人民共和国法律及国际法律的保护。\n\n未经本公司明确的书面授权，任何组织或个人不得复制、分发、传播、出版或以任何方式使用这些文件的任何部分。此外，未经授权，不得对这些文件进行修改、创作衍生作品或以任何商业目的使用。\n\n如果你并非《AI 大模型全栈工程师》课程的学员，但获得这些文件，你应该立即删除这些文件。\n\n如果想保留，请到[此页面](https://agiclass.feishu.cn/docx/Z3Aed6qXboiF8gxGuaccNHxanOc)了解课程内容并购买。\n\n本公司保留随时修订本版权声明的权利。\n\n\n\n## 本地运行课件\n\n```python\npip3 install notebook\ncd <课件目录>\njupyter notebook\n```\n\n## 课件风格\n\n### 基础风格要求\n\n1. 文字精炼，没废话。这是个课件、笔记，不是教材\n2. 代码简洁，没冗余。不分散学员注意力\n3. 避免大段文字，多用列表，这样更清晰\n4. 只有一个一级标题，是该次课的名称\n4. 内容从二级标题开始，可以使用三级、四级标题，但要尽量减少层级，这样能保持从头到尾最极致的流畅\n6. 中文和英文、数字之间，要有一个空格，这样排版更美观\n\n### 关键内容提示\n\n用 alert 框突出关键内容。\n\n### 划重点\n\n对最最核心、最最重要的知识点总结，要**划重点**。使用用如下代码，呈现为绿色，表示自信、稳健。\n\n```html\n<div class=\"alert alert-success\">\n<b>划重点：</b>\n<ol>\n<li>把 ChatGPT 看做是一个函数，给输入，<b>生成</b>输出</li>\n<li>任何业务问题，都可以用语言描述，成为 ChatGPT 的输入</li>\n<li>就能<b>生成</b>业务问题的结果</li>\n</ol>\n</div>\n```\n### 引起注意\n\n其它要引起注意的内容，用如下代码，呈现为橙色，表示友好。\n\n```html\n<div class=\"alert alert-warning\">\n<b>建议：</b>\n<li>会编程几乎是唯一要求，但并不用特别擅长。AI 的强大，使得用好它的门槛也低了很多。</li>\n<li>Python 是课程主语言。不熟悉也没关系，都看得明白。需要上手编时，正好用 AI 帮忙。</li>\n</div>\n```\n    \n```html\n<div class=\"alert alert-warning\">\n<b>思考：</b>你觉得哪些应用算是 AI？\n</div>\n```\n\n"
    },
    {
      "name": "shiningwhite-cmd/ViCatcher",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/83100977?s=40&v=4",
      "owner": "shiningwhite-cmd",
      "repo_name": "ViCatcher",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2024-03-15T07:17:14Z",
      "updated_at": "2025-03-25T11:59:11Z",
      "topics": [],
      "readme": "\n# ViCatcher:在大众视频平台上构建系统化知识学习体验\n\n![失去图片链接](./docs/img-intro.png)\n\nViCatcher 是一个实时的视频辅助学习系统，支持学习者搜索大众视频平台的视频，为学习者整理多个视频中知识的关联，组织学习路径，并提供学习引导与支持。ViCatcher 提供了不同于MOOC等体系化视频知识平台的学习方式，有望支持个性化、灵活的视频知识学习。\n\n## 安装\n> 确保系统已安装 Python 3.9 或更高版本。可以使用以下命令来检查：`python --version`\n\n```bash\npip install -r requirements.txt\npython run.py\n```\n首先安装依赖项文件，之后运行`run.py`这一python文件，即可启动本系统。\n\n## 详细介绍\n![失去图片链接](./docs/ppt1.png)\n![失去图片链接](./docs/ppt2.png)\n![失去图片链接](./docs/ppt3.png)\n![失去图片链接](./docs/ppt4.png)\n![失去图片链接](./docs/ppt5.png)\n![失去图片链接](./docs/ppt6.png)\n![失去图片链接](./docs/ppt7.png)\n![失去图片链接](./docs/ppt8.png)\n![失去图片链接](./docs/ppt9.png)\n![失去图片链接](./docs/ppt10.png)\n\n## 演示视频\n[演示视频链接(需要VPN)](https://vimeo.com/970307777)\n\n[演示视频链接(备用)](https://www.yuque.com/shanxiaobai/am2sfg/on80uxte0p58wz8r?singleDoc)\n\n## 联系方式\n这个项目目前还有数个工程上的问题亟需解决。如果你对这个项目感兴趣，或者有任何的建议，欢迎通过邮箱：qi.zheng@cs.zju.edu.cn 联系我。\n"
    },
    {
      "name": "cerberuswilliam/worldMetaGPT",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/39875358?s=40&v=4",
      "owner": "cerberuswilliam",
      "repo_name": "worldMetaGPT",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2024-03-15T00:17:45Z",
      "updated_at": "2024-03-15T00:55:09Z",
      "topics": [],
      "readme": "\n# MetaGPT: The Multi-Agent Framework\n\n<p align=\"center\">\n<a href=\"\"><img src=\"docs/resources/MetaGPT-new-log.png\" alt=\"MetaGPT logo: Enable GPT to work in software company, collaborating to tackle more complex tasks.\" width=\"150px\"></a>\n</p>\n\n<p align=\"center\">\n<b>Assign different roles to GPTs to form a collaborative entity for complex tasks.</b>\n</p>\n\n<p align=\"center\">\n<a href=\"docs/README_CN.md\"><img src=\"https://img.shields.io/badge/文档-中文版-blue.svg\" alt=\"CN doc\"></a>\n<a href=\"README.md\"><img src=\"https://img.shields.io/badge/document-English-blue.svg\" alt=\"EN doc\"></a>\n<a href=\"docs/README_JA.md\"><img src=\"https://img.shields.io/badge/ドキュメント-日本語-blue.svg\" alt=\"JA doc\"></a>\n<a href=\"https://opensource.org/licenses/MIT\"><img src=\"https://img.shields.io/badge/License-MIT-blue.svg\" alt=\"License: MIT\"></a>\n<a href=\"docs/ROADMAP.md\"><img src=\"https://img.shields.io/badge/ROADMAP-路线图-blue\" alt=\"roadmap\"></a>\n<a href=\"https://discord.gg/DYn29wFk9z\"><img src=\"https://dcbadge.vercel.app/api/server/DYn29wFk9z?style=flat\" alt=\"Discord Follow\"></a>\n<a href=\"https://twitter.com/MetaGPT_\"><img src=\"https://img.shields.io/twitter/follow/MetaGPT?style=social\" alt=\"Twitter Follow\"></a>\n</p>\n\n<p align=\"center\">\n   <a href=\"https://vscode.dev/redirect?url=vscode://ms-vscode-remote.remote-containers/cloneInVolume?url=https://github.com/geekan/MetaGPT\"><img src=\"https://img.shields.io/static/v1?label=Dev%20Containers&message=Open&color=blue&logo=visualstudiocode\" alt=\"Open in Dev Containers\"></a>\n   <a href=\"https://codespaces.new/geekan/MetaGPT\"><img src=\"https://img.shields.io/badge/Github_Codespace-Open-blue?logo=github\" alt=\"Open in GitHub Codespaces\"></a>\n   <a href=\"https://huggingface.co/spaces/deepwisdom/MetaGPT\" target=\"_blank\"><img alt=\"Hugging Face\" src=\"https://img.shields.io/badge/%F0%9F%A4%97%20-Hugging%20Face-blue?color=blue&logoColor=white\" /></a>\n</p>\n\n## News\n🚀 Mar. 14, 2024: Our Data Interpreter paper is on [arxiv](https://arxiv.org/abs/2402.18679). Check the [example](https://docs.deepwisdom.ai/main/en/DataInterpreter/) and [code](https://github.com/geekan/MetaGPT/tree/main/examples/di)!\n\n🚀 Feb. 08, 2024: [v0.7.0](https://github.com/geekan/MetaGPT/releases/tag/v0.7.0) released, supporting assigning different LLMs to different Roles. We also introduced [Data Interpreter](https://github.com/geekan/MetaGPT/blob/main/examples/di/README.md), a powerful agent capable of solving a wide range of real-world problems.\n\n🚀 Jan. 16, 2024: Our paper [MetaGPT: Meta Programming for A Multi-Agent Collaborative Framework\n](https://arxiv.org/abs/2308.00352) accepted for oral presentation **(top 1.2%)** at ICLR 2024, **ranking #1** in the LLM-based Agent category.\n\n🚀 Jan. 03, 2024: [v0.6.0](https://github.com/geekan/MetaGPT/releases/tag/v0.6.0) released, new features include serialization, upgraded OpenAI package and supported multiple LLM, provided [minimal example for debate](https://github.com/geekan/MetaGPT/blob/main/examples/debate_simple.py) etc.\n\n🚀 Dec. 15, 2023: [v0.5.0](https://github.com/geekan/MetaGPT/releases/tag/v0.5.0) released, introducing some experimental features such as **incremental development**, **multilingual**, **multiple programming languages**, etc.\n\n🔥 Nov. 08, 2023: MetaGPT is selected into [Open100: Top 100 Open Source achievements](https://www.benchcouncil.org/evaluation/opencs/annual.html).\n\n🔥 Sep. 01, 2023: MetaGPT tops GitHub Trending Monthly for the **17th time** in August 2023.\n\n🌟 Jun. 30, 2023: MetaGPT is now open source.\n\n🌟 Apr. 24, 2023: First line of MetaGPT code committed.\n\n## Software Company as Multi-Agent System\n\n1. MetaGPT takes a **one line requirement** as input and outputs **user stories / competitive analysis / requirements / data structures / APIs / documents, etc.**\n2. Internally, MetaGPT includes **product managers / architects / project managers / engineers.** It provides the entire process of a **software company along with carefully orchestrated SOPs.**\n   1. `Code = SOP(Team)` is the core philosophy. We materialize SOP and apply it to teams composed of LLMs.\n\n![A software company consists of LLM-based roles](docs/resources/software_company_cd.jpeg)\n\n<p align=\"center\">Software Company Multi-Agent Schematic (Gradually Implementing)</p>\n\n## Install\n\n### Pip installation\n\n> Ensure that Python 3.9+ is installed on your system. You can check this by using: `python --version`.  \n> You can use conda like this: `conda create -n metagpt python=3.9 && conda activate metagpt`\n\n```bash\npip install metagpt\n# https://docs.deepwisdom.ai/main/en/guide/get_started/configuration.html\nmetagpt --init-config  # it will create ~/.metagpt/config2.yaml, just modify it to your needs\n```\n\n### Configuration\n\nYou can configure `~/.metagpt/config2.yaml` according to the [example](https://github.com/geekan/MetaGPT/blob/main/config/config2.example.yaml) and [doc](https://docs.deepwisdom.ai/main/en/guide/get_started/configuration.html):\n\n```yaml\nllm:\n  api_type: \"openai\"  # or azure / ollama / open_llm etc. Check LLMType for more options\n  model: \"gpt-4-turbo-preview\"  # or gpt-3.5-turbo-1106 / gpt-4-1106-preview\n  base_url: \"https://api.openai.com/v1\"  # or forward url / other llm url\n  api_key: \"YOUR_API_KEY\"\n```\n\n### Usage\n\nAfter installation, you can use it as CLI\n\n```bash\nmetagpt \"Create a 2048 game\"  # this will create a repo in ./workspace\n```\n\nor you can use it as library\n\n```python\nfrom metagpt.software_company import generate_repo, ProjectRepo\nrepo: ProjectRepo = generate_repo(\"Create a 2048 game\")  # or ProjectRepo(\"<path>\")\nprint(repo)  # it will print the repo structure with files\n```\n\ndetail installation please refer to [cli_install](https://docs.deepwisdom.ai/main/en/guide/get_started/installation.html#install-stable-version)\n or [docker_install](https://docs.deepwisdom.ai/main/en/guide/get_started/installation.html#install-with-docker)\n\n### Docker installation\n<details><summary><strong>⏬ Step 1: Download metagpt image and prepare config2.yaml </strong><i>:: click to expand ::</i></summary>\n<div>\n\n```bash\ndocker pull metagpt/metagpt:latest\nmkdir -p /opt/metagpt/{config,workspace}\ndocker run --rm metagpt/metagpt:latest cat /app/metagpt/config/config2.yaml > /opt/metagpt/config/config2.yaml\nvim /opt/metagpt/config/config2.yaml # Change the config\n```\n\n</div>\n</details>\n\n<details><summary><strong>⏬ Step 2: Run metagpt container </strong><i>:: click to expand ::</i></summary>\n<div>\n\n```bash\ndocker run --name metagpt -d \\\n    --privileged \\\n    -v /opt/metagpt/config/config2.yaml:/app/metagpt/config/config2.yaml \\\n    -v /opt/metagpt/workspace:/app/metagpt/workspace \\\n    metagpt/metagpt:latest\n```\n\n</div>\n</details>\n\n<details><summary><strong>⏬ Step 3: Use metagpt </strong><i>:: click to expand ::</i></summary>\n<div>\n\n```bash\ndocker exec -it metagpt /bin/bash\n$ metagpt \"Create a 2048 game\"  # this will create a repo in ./workspace\n```\n\n</div>\n</details>\n\n### QuickStart & Demo Video\n- Try it on [MetaGPT Huggingface Space](https://huggingface.co/spaces/deepwisdom/MetaGPT)\n- [Matthew Berman: How To Install MetaGPT - Build A Startup With One Prompt!!](https://youtu.be/uT75J_KG_aY)\n- [Official Demo Video](https://github.com/geekan/MetaGPT/assets/2707039/5e8c1062-8c35-440f-bb20-2b0320f8d27d)\n\nhttps://github.com/geekan/MetaGPT/assets/34952977/34345016-5d13-489d-b9f9-b82ace413419\n\n## Tutorial\n\n- 🗒 [Online Document](https://docs.deepwisdom.ai/main/en/)\n- 💻 [Usage](https://docs.deepwisdom.ai/main/en/guide/get_started/quickstart.html)  \n- 🔎 [What can MetaGPT do?](https://docs.deepwisdom.ai/main/en/guide/get_started/introduction.html)\n- 🛠 How to build your own agents? \n  - [MetaGPT Usage & Development Guide | Agent 101](https://docs.deepwisdom.ai/main/en/guide/tutorials/agent_101.html)\n  - [MetaGPT Usage & Development Guide | MultiAgent 101](https://docs.deepwisdom.ai/main/en/guide/tutorials/multi_agent_101.html)\n- 🧑‍💻 Contribution\n  - [Develop Roadmap](docs/ROADMAP.md)\n- 🔖 Use Cases\n  - [Debate](https://docs.deepwisdom.ai/main/en/guide/use_cases/multi_agent/debate.html)\n  - [Researcher](https://docs.deepwisdom.ai/main/en/guide/use_cases/agent/researcher.html)\n  - [Recepit Assistant](https://docs.deepwisdom.ai/main/en/guide/use_cases/agent/receipt_assistant.html)\n- ❓ [FAQs](https://docs.deepwisdom.ai/main/en/guide/faq.html)\n\n## Support\n\n### Discard Join US\n📢 Join Our [Discord Channel](https://discord.gg/ZRHeExS6xv)!\n\nLooking forward to seeing you there! 🎉\n\n### Contact Information\n\nIf you have any questions or feedback about this project, please feel free to contact us. We highly appreciate your suggestions!\n\n- **Email:** alexanderwu@deepwisdom.ai\n- **GitHub Issues:** For more technical inquiries, you can also create a new issue in our [GitHub repository](https://github.com/geekan/metagpt/issues).\n\nWe will respond to all questions within 2-3 business days.\n\n## Citation\n\nIf you use MetaGPT or Data Interpreter in a research paper, please cite our work as follows:\n\n```bibtex\n@misc{hong2023metagpt,\n      title={MetaGPT: Meta Programming for A Multi-Agent Collaborative Framework}, \n      author={Sirui Hong and Mingchen Zhuge and Jonathan Chen and Xiawu Zheng and Yuheng Cheng and Ceyao Zhang and Jinlin Wang and Zili Wang and Steven Ka Shing Yau and Zijuan Lin and Liyang Zhou and Chenyu Ran and Lingfeng Xiao and Chenglin Wu and Jürgen Schmidhuber},\n      year={2023},\n      eprint={2308.00352},\n      archivePrefix={arXiv},\n      primaryClass={cs.AI}\n}\n@misc{hong2024data,\n      title={Data Interpreter: An LLM Agent For Data Science}, \n      author={Sirui Hong and Yizhang Lin and Bang Liu and Bangbang Liu and Binhao Wu and Danyang Li and Jiaqi Chen and Jiayi Zhang and Jinlin Wang and Li Zhang and Lingyao Zhang and Min Yang and Mingchen Zhuge and Taicheng Guo and Tuo Zhou and Wei Tao and Wenyi Wang and Xiangru Tang and Xiangtao Lu and Xiawu Zheng and Xinbing Liang and Yaying Fei and Yuheng Cheng and Zongze Xu and Chenglin Wu},\n      year={2024},\n      eprint={2402.18679},\n      archivePrefix={arXiv},\n      primaryClass={cs.AI}\n}\n\n```\n\n"
    },
    {
      "name": "nandumsft/promtfloweval",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/123025520?s=40&v=4",
      "owner": "nandumsft",
      "repo_name": "promtfloweval",
      "description": "This repository contains flows that can be used to evaluate LLM applications",
      "homepage": null,
      "language": "Python",
      "created_at": "2024-02-29T16:26:04Z",
      "updated_at": "2024-03-10T03:35:36Z",
      "topics": [],
      "readme": "This repo contains flows that can be used to evaluate LLM apps/prompts"
    },
    {
      "name": "kimtth/fastapi-oai-showcase",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/13846660?s=40&v=4",
      "owner": "kimtth",
      "repo_name": "fastapi-oai-showcase",
      "description": "Minimal API Server (🚧Work in progress). All features are not necessary to demo. Just minimum to work. / ui-repo: nextjs-typescript-learn ",
      "homepage": "",
      "language": "Python",
      "created_at": "2023-06-22T11:07:06Z",
      "updated_at": "2024-03-11T07:22:28Z",
      "topics": [],
      "readme": "\n## API Specification\n\n- `http://127.0.0.1:5000/docs`\n\n  <img src=\"docs/api_spec.png\" alt=\"apis\" width=\"350\"/>"
    },
    {
      "name": "SehejSoni/try",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/134284071?s=40&v=4",
      "owner": "SehejSoni",
      "repo_name": "try",
      "description": null,
      "homepage": null,
      "language": "HTML",
      "created_at": "2024-02-28T09:09:54Z",
      "updated_at": "2024-02-28T12:23:57Z",
      "topics": [],
      "readme": "<h1 align=\"center\">mineflayer-collectblock</h1>\n<p align=\"center\"><i>A small utility plugin for allowing users to collect blocks using a higher level API.</i></p>\n\n<p align=\"center\">\n  <img src=\"https://github.com/TheDudeFromCI/mineflayer-collectblock/workflows/Build/badge.svg\" />\n  <a href=\"https://www.npmjs.com/package/mineflayer-collectblock\"><img src=\"https://img.shields.io/npm/v/mineflayer-collectblock\" /></a>\n  <img src=\"https://img.shields.io/github/repo-size/TheDudeFromCI/mineflayer-collectblock\" />\n  <img src=\"https://img.shields.io/npm/dm/mineflayer-collectblock\" />\n  <img src=\"https://img.shields.io/github/contributors/TheDudeFromCI/mineflayer-collectblock\" />\n  <img src=\"https://img.shields.io/github/license/TheDudeFromCI/mineflayer-collectblock\" />\n</p>\n\n---\n## This is a modified version to better support Voyager\n\n## Showcase\n\nYou can see a video of the plugin in action, [here.](https://youtu.be/5T_rcCnNnf4)\nThe source code of the bot in the video can be seen in the examples folder, [here.](https://github.com/TheDudeFromCI/mineflayer-collectblock/blob/master/examples/collector.js)\n\n### Description\n\nThis plugin is a wrapper for mineflayer that allows for easier API usage when collecting blocks or item drops. This plugin is designed to reduce some of the boilerplate code based around the act of pathfinding to a block _(handled by_ ***mineflayer-pathfinder***_)_, selecting the best tool to mine that block _(handled by_ ***mineflayer-tool***_)_, actually mining it, then moving to collect the item drops from that block. This plugin allows for all of that basic concept to be wrapped up into a single API function.\n\nIn addition to the usage above, some additional quality of life features are available in this plugin. These include the ability to automatically deposit items into a chest when the bot's inventory is full, collecting new tools from a chest if the bot doesn't currently have a required tool _(also handled by_ ***mineflayer-tool***_)_, and allowing for queueing of multiple blocks or item drops to the collection task, so they can be processed later.\n\n### Getting Started\n\nThis plugin is built using Node and can be installed using:\n```bash\nnpm install --save mineflayer-collectblock\n```\n\n### Simple Bot\n\nThe brief description goes here.\n\n```js\n// Create your bot\nconst mineflayer = require(\"mineflayer\")\nconst bot = mineflayer.createBot({\n  host: 'localhost',\n  username: 'Player',\n})\nlet mcData\n\n// Load collect block\nbot.loadPlugin(require('mineflayer-collectblock').plugin)\n\nasync function collectGrass() {\n  // Find a nearby grass block\n  const grass = bot.findBlock({\n    matching: mcData.blocksByName.grass_block.id,\n    maxDistance: 64\n  })\n\n  if (grass) {\n    // If we found one, collect it.\n    try {\n      await bot.collectBlock.collect(grass)\n      collectGrass() // Collect another grass block\n    } catch (err) {\n      console.log(err) // Handle errors, if any\n    }\n  }\n}\n\n// On spawn, start collecting all nearby grass\nbot.once('spawn', () => {\n  mcData = require('minecraft-data')(bot.version)\n  collectGrass()\n})\n```\n\n### Documentation\n\n[API](https://github.com/TheDudeFromCI/mineflayer-collectblock/blob/master/docs/api.md)\n\n[Examples](https://github.com/TheDudeFromCI/mineflayer-collectblock/tree/master/examples)\n\n### License\n\nThis project uses the [MIT](https://github.com/TheDudeFromCI/mineflayer-collectblock/blob/master/LICENSE) license.\n\n### Contributions\n\nThis project is accepting PRs and Issues. See something you think can be improved? Go for it! Any and all help is highly appreciated!\n\nFor larger changes, it is recommended to discuss these changes in the issues tab before writing any code. It's also preferred to make many smaller PRs than one large one, where applicable.\n"
    },
    {
      "name": "kdcllc/nlp-sql-in-a-box",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/13120940?s=40&v=4",
      "owner": "kdcllc",
      "repo_name": "nlp-sql-in-a-box",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2024-02-21T19:20:50Z",
      "updated_at": "2024-02-21T19:24:18Z",
      "topics": [],
      "readme": "# NLP to SQL in-a-box examples\n\n![I stand with Israel](./docs/IStandWithIsrael.png)\n\nWelcome to the **NLP2SQL** repository, where you can find different approaches and tools for natural language processing (NLP) translations to SQL.\n\nWith **NLP2SQL**, you can use the state-of-the-art technologies of Azure Open AI, Semantic Kernel, and LangChan to turn your natural language queries into SQL commands that can run on any SQL Server database. This will make your data analysis easier and faster, without the hassle of learning SQL syntax.\n\n[![License: MIT](https://img.shields.io/badge/License-MIT-blue.svg)](https://opensource.org/licenses/MIT)\n\n## Hire me\n\nPlease send [email](mailto:kingdavidconsulting@gmail.com) if you consider to **hire me**.\n\n[![buymeacoffee](https://www.buymeacoffee.com/assets/img/custom_images/orange_img.png)](https://www.buymeacoffee.com/vyve0og)\n\n## Give a Star! :star\n\nIf you like or are using this project to learn or start your solution, please give it a star. Thanks!\n\n## Prerequisites\n\n* An [Azure subscription](https://azure.microsoft.com/en-us/free/).\n* Install latest version of [Azure CLI](https://docs.microsoft.com/en-us/cli/azure/install-azure-cli-windows?view=azure-cli-latest)\n* Install [ODBC Driver for SQL Server](https://learn.microsoft.com/en-us/sql/connect/odbc/download-odbc-driver-for-sql-server) \n\n## Deploy to Azure\n\n1. Clone this repository locally: \n\n    ```\n    git clone https://github.com/kdcllc/nlp-sql-in-a-box\n    ```\n2. Deploy resources:\n\n- Azure SQL Server/Database\n- Azure OpenAI Endpoint and deploy models\n- Azure Key Vault (Optional) to store secrets\n\n## Post Deployment\n\nOnce your resources have been deployed you will need to do the following to get the app up and running:\n\n1. Add your client IP4 address in the Azure SQL Server Firewall rules:       \n    * If you don't know how to add your IP Address to your SQL Server follow this link -> [Create a server-level firewall rule in Azure portal](https://learn.microsoft.com/en-us/azure/azure-sql/database/firewall-create-server-level-portal-quickstart)\n\n2. [Create fake data](./src/datasetup/)\n\n## Bash Scripts\n\n### `add-env-file.sh` add secret from Azure Key Vault to `.env` file for local testing only\n\nThis bash script is used to create a `.env` file and populate it with secrets from an Azure Key Vault.\n\nThe script prompts the user to input the name of the `.env` file and the name of the Azure Key Vault. It then retrieves the secrets for a predefined list of environment variables from the Key Vault and writes them to the `.env` file.\n\nThe environment variables retrieved by this script are:\n\n* AZURE-OPENAI-DEPLOYMENT-NAME\n* AZURE-OPENAI-ENDPOINT\n* AZURE-OPENAI-API-KEY\n* SERVER-NAME\n* DATABASE-NAME\n* SQLADMIN-USER\n* SQL-PASSWORD\n\nTo run the script, you need to make it executable with the command `chmod +x add-env-file.sh` and then execute it with `./add-env-file.sh`.\n\n### `add-secrets-kv.sh` adds secrets to Azure Key Valut\n\nThis bash script is used to add secrets from environment variables in a `.env` file to an Azure Key Vault.\n\nThe script prompts the user to input the name of the Azure Key Vault. It then retrieves the values of a predefined list of environment variables from the `.env` file and adds them as secrets to the Key Vault.\n\nThe environment variables handled by this script are:\n\n* AZURE_OPENAI_DEPLOYMENT_NAME\n* AZURE_OPENAI_ENDPOINT\n* AZURE_OPENAI_API_KEY\n* SERVER_NAME\n* DATABASE_NAME\n* SQLADMIN_USER\n* SQL_PASSWORD\n\nBefore running the script, you need to make it executable with the command `chmod +x add-secrets-kv.sh`. Then, you can execute it with `./add-secrets-kv.sh`.\n\nPlease note that the script requires the user to be logged into Azure CLI. If the user is not logged in, the script will output \"User is not logged in\".\n\n### `version.sh` to pin version only of the listed main packages\n\nThis bash script reads a `requirements.txt` file and prints each Python package name along with its installed version.\n\nThe script accepts an optional argument which is the path to the `requirements.txt` file. If no argument is provided, it defaults to using a file named `requirements.txt` in the current directory.\n\nThe script checks if the specified `requirements.txt` file exists, and whether Python and pip are installed on the system. It then reads each line of the `requirements.txt` file, skipping lines that are empty or contain '-r'. For each package listed in the file, it uses pip to find the installed version of the package and prints the package name and version in the format `package==version`.\n\nTo run the script, you need to make it executable with the command `chmod +x version.sh` and then execute it with `./version.sh` or `./version.sh <path_to_requirements.txt>`.\n\nPlease note that this script requires Python and pip to be installed on your system.\n\n### `create-venv.sh` run this script to quickly create and install python environment\n\nThis bash script creates a Python virtual environment and installs the packages listed in a `requirements.txt` file.\n\nThe script accepts two optional arguments:\n\n1. The path to the `requirements.txt` file. If no argument is provided, it defaults to a file named `requirements.txt` in the current directory.\n2. The name for the virtual environment. If no argument is provided, it defaults to `.venv`.\n\nThe script first checks if the provided `requirements.txt` file exists. It then creates a virtual environment with the specified name (or `.venv` if no name is provided), activates the environment, and installs the packages listed in the `requirements.txt` file.\n\nTo run the script, you need to make it executable with the command `chmod +x create-venv.sh`. Then, you can execute it with `./create-venv.sh` or `./create-venv.sh <path_to_requirements.txt> <environment_name>`.\n\nPlease note that this script requires Python 3 and pip to be installed on your system.\n\n### `delete-venv.sh` run this script to quickly remove python environment\n\nThis bash script deletes a Python virtual environment.\n\nThe script accepts one optional argument:\n\n1. The name of the virtual environment to be deleted. If no argument is provided, it defaults to `.venv`.\n\nThe script first deactivates the virtual environment (if it's active) and then removes the directory associated with the environment.\n\nTo run the script, you need to make it executable with the command `chmod +x delete-venv.sh`. Then, you can execute it with `./delete-venv.sh` or `./delete-venv.sh <environment_name>`.\n\nPlease note that this script requires the virtual environment to be in the same directory from where the script is being run.\n\n\n## Sample questions\n\n* How many locations are there?\n* How many wells are there were water cut is more than 95?\n\n\n## References\n\n* [Revolutionizing SQL Queries with Azure Open AI and Semantic Kernel](https://techcommunity.microsoft.com/t5/analytics-on-azure-blog/revolutionizing-sql-queries-with-azure-open-ai-and-semantic/ba-p/3913513)\n\n* [NL2SQL with LangChain and Azure SQL Database](https://devblogs.microsoft.com/azure-sql/nl2sql-with-langchain-and-azure-sql-database/)\n\n* [SQL-AI-samples (LangChain) repo](https://github.com/Azure-Samples/SQL-AI-samples/tree/main/AzureSQLDatabase/LangChain)\n\n* [LangChain SQL Database Toolkit](https://python.langchain.com/docs/integrations/toolkits/sql_database)\n"
    },
    {
      "name": "AymenSegni/ecoai-app",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/28219843?s=40&v=4",
      "owner": "AymenSegni",
      "repo_name": "ecoai-app",
      "description": "Sample e-commerce microservices app that is used for live project demos and experiments. The app demonstrates a realistic scenario using a polyglot architecture, event-driven design, and common open source back-end services such as RabbitMQ and MongoDB. Additionally, the application uses OpenAI's GPT-3 models to generate product descriptions.",
      "homepage": null,
      "language": "Rust",
      "created_at": "2024-02-11T13:01:43Z",
      "updated_at": "2024-02-11T13:41:09Z",
      "topics": [],
      "readme": "# ecoai-app\n\nSample e-commerce microservices app that is used for live project demos and experiments. The app demonstrates a realistic scenario using a polyglot architecture, event-driven design, and common open source back-end services such as RabbitMQ and MongoDB. Additionally, the application uses OpenAI's GPT-3 models to generate product descriptions.\nThis can be done using either [Azure OpenAI](https://learn.microsoft.com/azure/ai-services/openai/overview) or [OpenAI](https://openai.com/).\n\nThis application is inspired by another demo app called [Red Dog](https://github.com/Azure/reddog-code).\n\n> [!NOTE]\n> This is not meant to be an example of perfect code to be used in production, but more about showing a realistic application running in AKS. \n\n## Architecture\n\nThe application has the following services: \n\n| Service | Description |\n| --- | --- |\n| `makeline-service` | This service handles processing orders from the queue and completing them (Golang) |\n| `order-service` | This service is used for placing orders (Javascript) |\n| `product-service` | This service is used to perform CRUD operations on products (Rust) |\n| `store-front` | Web app for customers to place orders (Vue.js) |\n| `store-admin` | Web app used by store employees to view orders in queue and manage products (Vue.js) | \n| `virtual-customer` | Simulates order creation on a scheduled basis (Rust) |\n| `virtual-worker` | Simulates order completion on a scheduled basis (Rust) |\n| `ai-service` | Optional service for adding generative text and graphics creation (Python) |\n| `mongodb` | MongoDB instance for persisted data |\n| `rabbitmq` | RabbitMQ for an order queue |\n\n![Logical Application Architecture Diagram](assets/demo-arch-with-openai.png)\n\n\n## Run the app locally using Docker\n\nThe application is designed to be run in Kubernetes, but can also be run locally using Docker Compose.\n\n> [!TIP]\n> You must have Docker Desktop tool, such as Orbstack or Docker Desktop, installed to run this app locally. \n\nTo run this app locally:\n\n1. Configure your Azure OpenAI or OpenAI API keys in [`docker-compose.yml`](./docker-compose.yml) using the environment variables in the `ai-service` section:\n\n```yaml\n  ai-service:\n    build: src/ai-service\n    container_name: 'ai-service'\n    ...\n    environment:\n      - USE_AZURE_OPENAI=True # set to False if you are not using Azure OpenAI\n      - AZURE_OPENAI_DEPLOYMENT_NAME= # required if using Azure OpenAI\n      - AZURE_OPENAI_ENDPOINT= # required if using Azure OpenAI\n      - OPENAI_API_KEY= # always required\n      - OPENAI_ORG_ID= # required if using OpenAI\n    ...\n```\n\nAlternatively, if you do not have access to Azure OpenAI or OpenAI API keys, you can run the app without the `ai-service` by commenting out the `ai-service` section in [`docker-compose.yml`](./docker-compose.yml). For example:\n\n```yaml\n#  ai-service:\n#    build: src/ai-service\n#    container_name: 'ai-service'\n...\n#    networks:\n#      - backend_services\n```\n\nStart the app using `docker compose`. For example:\n\n```bash\ndocker compose up\n```\n\nIf everything goes well, you can access the result containers in your desktop tool.\nThe following screenshots are taken from my local Orbstack:\n\n- ![Docker containers](assets/containers.png)\n- ![Store Admin service](assets/store-admin.png)\n- ![Front Store service](assets/front-end-store.png)\n- ![Product service](assets/product-service.png)\n\nTo stop the app, you can hit the `CTRL+C` key combination in the terminal window where the app is running.\n"
    },
    {
      "name": "martcpp/MetaGPT",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/103220475?s=40&v=4",
      "owner": "martcpp",
      "repo_name": "MetaGPT",
      "description": "🌟 The Multi-Agent Framework: Given one line Requirement, return PRD, Design, Tasks, Repo",
      "homepage": "",
      "language": "Python",
      "created_at": "2023-08-08T10:13:22Z",
      "updated_at": "2023-08-18T20:40:25Z",
      "topics": [],
      "readme": "# MetaGPT: The Multi-Agent Framework\n\n<p align=\"center\">\n<a href=\"\"><img src=\"docs/resources/MetaGPT-logo.jpeg\" alt=\"MetaGPT logo: Enable GPT to work in software company, collaborating to tackle more complex tasks.\" width=\"150px\"></a>\n</p>\n\n<p align=\"center\">\n<b>Assign different roles to GPTs to form a collaborative software entity for complex tasks.</b>\n</p>\n\n<p align=\"center\">\n<a href=\"docs/README_CN.md\"><img src=\"https://img.shields.io/badge/文档-中文版-blue.svg\" alt=\"CN doc\"></a>\n<a href=\"README.md\"><img src=\"https://img.shields.io/badge/document-English-blue.svg\" alt=\"EN doc\"></a>\n<a href=\"docs/README_JA.md\"><img src=\"https://img.shields.io/badge/ドキュメント-日本語-blue.svg\" alt=\"JA doc\"></a>\n<a href=\"https://discord.gg/wCp6Q3fsAk\"><img src=\"https://img.shields.io/badge/Discord-Join-blue?logo=discord&logoColor=white&color=blue\" alt=\"Discord Follow\"></a>\n<a href=\"https://opensource.org/licenses/MIT\"><img src=\"https://img.shields.io/badge/License-MIT-blue.svg\" alt=\"License: MIT\"></a>\n<a href=\"docs/ROADMAP.md\"><img src=\"https://img.shields.io/badge/ROADMAP-路线图-blue\" alt=\"roadmap\"></a>\n<a href=\"https://twitter.com/DeepWisdom2019\"><img src=\"https://img.shields.io/twitter/follow/MetaGPT?style=social\" alt=\"Twitter Follow\"></a>\n</p>\n\n<p align=\"center\">\n   <a href=\"https://airtable.com/appInfdG0eJ9J4NNL/shrEd9DrwVE3jX6oz\"><img src=\"https://img.shields.io/badge/AgentStore-Waitlist-ffc107?logoColor=white\" alt=\"AgentStore Waitlist\"></a>\n   <a href=\"https://vscode.dev/redirect?url=vscode://ms-vscode-remote.remote-containers/cloneInVolume?url=https://github.com/geekan/MetaGPT\"><img src=\"https://img.shields.io/static/v1?label=Dev%20Containers&message=Open&color=blue&logo=visualstudiocode\" alt=\"Open in Dev Containers\"></a>\n   <a href=\"https://codespaces.new/geekan/MetaGPT\"><img src=\"https://img.shields.io/badge/Github_Codespace-Open-blue?logo=github\" alt=\"Open in GitHub Codespaces\"></a>\n   <a href=\"https://huggingface.co/spaces/deepwisdom/MetaGPT\" target=\"_blank\"><img alt=\"Hugging Face\" src=\"https://img.shields.io/badge/%F0%9F%A4%97%20-Hugging%20Face-blue?color=blue&logoColor=white\" /></a>\n</p>\n\n1. MetaGPT takes a **one line requirement** as input and outputs **user stories / competitive analysis / requirements / data structures / APIs / documents, etc.**\n2. Internally, MetaGPT includes **product managers / architects / project managers / engineers.** It provides the entire process of a **software company along with carefully orchestrated SOPs.**\n   1. `Code = SOP(Team)` is the core philosophy. We materialize SOP and apply it to teams composed of LLMs.\n\n![A software company consists of LLM-based roles](docs/resources/software_company_cd.jpeg)\n\n<p align=\"center\">Software Company Multi-Role Schematic (Gradually Implementing)</p>\n\n## MetaGPT's Abilities\n\n\nhttps://github.com/geekan/MetaGPT/assets/34952977/34345016-5d13-489d-b9f9-b82ace413419\n\n\n\n## Examples (fully generated by GPT-4)\n\nFor example, if you type `python startup.py \"Design a RecSys like Toutiao\"`, you would get many outputs, one of them is data & api design\n\n![Jinri Toutiao Recsys Data & API Design](docs/resources/workspace/content_rec_sys/resources/data_api_design.png)\n\nIt costs approximately **$0.2** (in GPT-4 API fees) to generate one example with analysis and design, and around **$2.0** for a full project.\n\n\n\n\n## Installation\n\n### Installation Video Guide\n\n- [Matthew Berman: How To Install MetaGPT - Build A Startup With One Prompt!!](https://youtu.be/uT75J_KG_aY)\n\n### Traditional Installation\n\n```bash\n# Step 1: Ensure that NPM is installed on your system. Then install mermaid-js. (If you don't have npm in your computer, please go to the Node.js offical website to install Node.js https://nodejs.org/ and then you will have npm tool in your computer.)\nnpm --version\nsudo npm install -g @mermaid-js/mermaid-cli\n\n# Step 2: Ensure that Python 3.9+ is installed on your system. You can check this by using:\npython --version\n\n# Step 3: Clone the repository to your local machine, and install it.\ngit clone https://github.com/geekan/metagpt\ncd metagpt\npip install -e.\n```\n\n**Note:**\n\n- If already have Chrome, Chromium, or MS Edge installed, you can skip downloading Chromium by setting the environment variable\n  `PUPPETEER_SKIP_CHROMIUM_DOWNLOAD` to `true`.\n\n- Some people are [having issues](https://github.com/mermaidjs/mermaid.cli/issues/15) installing this tool globally. Installing it locally is an alternative solution,\n\n  ```bash\n  npm install @mermaid-js/mermaid-cli\n  ```\n\n- don't forget to the configuration for mmdc in config.yml\n\n  ```yml\n  PUPPETEER_CONFIG: \"./config/puppeteer-config.json\"\n  MMDC: \"./node_modules/.bin/mmdc\"\n  ```\n\n- if `pip install -e.` fails with error `[Errno 13] Permission denied: '/usr/local/lib/python3.11/dist-packages/test-easy-install-13129.write-test'`, try instead running `pip install -e. --user`\n\n- To convert Mermaid charts to SVG, PNG, and PDF formats. In addition to the Node.js version of Mermaid-CLI, you now have the option to use Python version Playwright, pyppeteer or mermaid.ink for this task.\n\n  - Playwright\n    - **Install Playwright**\n\n    ```bash\n    pip install playwright\n    ```\n\n    - **Install the Required Browsers**\n\n    to support PDF conversion, please install Chrominum.\n\n    ```bash\n    playwright install --with-deps chromium\n    ```\n\n    - **modify `config.yaml`**\n\n    uncomment MERMAID_ENGINE from config.yaml and change it to `playwright`\n\n    ```yaml\n    MERMAID_ENGINE: playwright\n    ```\n\n  - pyppeteer\n    - **Install pyppeteer**\n\n    ```bash\n    pip install pyppeteer\n    ```\n\n    - **Use your own Browsers**\n\n    pyppeteer alow you use installed browsers,  please set the following envirment\n    \n    ```bash\n    export PUPPETEER_EXECUTABLE_PATH = /path/to/your/chromium or edge or chrome\n    ```\n\n    please do not use this command to install browser, it is too old\n\n    ```bash\n    pyppeteer-install\n    ```\n\n    - **modify `config.yaml`**\n\n    uncomment MERMAID_ENGINE from config.yaml and change it to `pyppeteer`\n\n    ```yaml\n    MERMAID_ENGINE: pyppeteer\n    ```\n\n  - mermaid.ink\n    - **modify `config.yaml`**\n\n    uncomment MERMAID_ENGINE from config.yaml and change it to `ink`\n\n    ```yaml\n    MERMAID_ENGINE: ink\n    ```  \n\n    Note: this method does not support pdf export.\n\n### Installation by Docker\n\n```bash\n# Step 1: Download metagpt official image and prepare config.yaml\ndocker pull metagpt/metagpt:latest\nmkdir -p /opt/metagpt/{config,workspace}\ndocker run --rm metagpt/metagpt:latest cat /app/metagpt/config/config.yaml > /opt/metagpt/config/key.yaml\nvim /opt/metagpt/config/key.yaml # Change the config\n\n# Step 2: Run metagpt demo with container\ndocker run --rm \\\n    --privileged \\\n    -v /opt/metagpt/config/key.yaml:/app/metagpt/config/key.yaml \\\n    -v /opt/metagpt/workspace:/app/metagpt/workspace \\\n    metagpt/metagpt:latest \\\n    python startup.py \"Write a cli snake game\"\n\n# You can also start a container and execute commands in it\ndocker run --name metagpt -d \\\n    --privileged \\\n    -v /opt/metagpt/config/key.yaml:/app/metagpt/config/key.yaml \\\n    -v /opt/metagpt/workspace:/app/metagpt/workspace \\\n    metagpt/metagpt:latest\n\ndocker exec -it metagpt /bin/bash\n$ python startup.py \"Write a cli snake game\"\n```\n\nThe command `docker run ...` do the following things:\n\n- Run in privileged mode to have permission to run the browser\n- Map host directory `/opt/metagpt/config` to container directory `/app/metagpt/config`\n- Map host directory `/opt/metagpt/workspace` to container directory `/app/metagpt/workspace`\n- Execute the demo command `python startup.py \"Write a cli snake game\"`\n\n### Build image by yourself\n\n```bash\n# You can also build metagpt image by yourself.\ngit clone https://github.com/geekan/MetaGPT.git\ncd MetaGPT && docker build -t metagpt:custom .\n```\n\n## Configuration\n\n- Configure your `OPENAI_API_KEY` in any of `config/key.yaml / config/config.yaml / env`\n- Priority order: `config/key.yaml > config/config.yaml > env`\n\n```bash\n# Copy the configuration file and make the necessary modifications.\ncp config/config.yaml config/key.yaml\n```\n\n| Variable Name                              | config/key.yaml                           | env                                             |\n| ------------------------------------------ | ----------------------------------------- | ----------------------------------------------- |\n| OPENAI_API_KEY # Replace with your own key | OPENAI_API_KEY: \"sk-...\"                  | export OPENAI_API_KEY=\"sk-...\"                  |\n| OPENAI_API_BASE # Optional                 | OPENAI_API_BASE: \"https://<YOUR_SITE>/v1\" | export OPENAI_API_BASE=\"https://<YOUR_SITE>/v1\" |\n\n## Tutorial: Initiating a startup\n\n```shell\n# Run the script\npython startup.py \"Write a cli snake game\"\n# Do not hire an engineer to implement the project\npython startup.py \"Write a cli snake game\" --implement False\n# Hire an engineer and perform code reviews\npython startup.py \"Write a cli snake game\" --code_review True\n```\n\nAfter running the script, you can find your new project in the `workspace/` directory.\n\n### Preference of Platform or Tool\n\nYou can tell which platform or tool you want to use when stating your requirements.\n\n```shell\npython startup.py \"Write a cli snake game based on pygame\"\n```\n\n### Usage\n\n```\nNAME\n    startup.py - We are a software startup comprised of AI. By investing in us, you are empowering a future filled with limitless possibilities.\n\nSYNOPSIS\n    startup.py IDEA <flags>\n\nDESCRIPTION\n    We are a software startup comprised of AI. By investing in us, you are empowering a future filled with limitless possibilities.\n\nPOSITIONAL ARGUMENTS\n    IDEA\n        Type: str\n        Your innovative idea, such as \"Creating a snake game.\"\n\nFLAGS\n    --investment=INVESTMENT\n        Type: float\n        Default: 3.0\n        As an investor, you have the opportunity to contribute a certain dollar amount to this AI company.\n    --n_round=N_ROUND\n        Type: int\n        Default: 5\n\nNOTES\n    You can also use flags syntax for POSITIONAL ARGUMENTS\n```\n\n### Code walkthrough\n\n```python\nfrom metagpt.software_company import SoftwareCompany\nfrom metagpt.roles import ProjectManager, ProductManager, Architect, Engineer\n\nasync def startup(idea: str, investment: float = 3.0, n_round: int = 5):\n    \"\"\"Run a startup. Be a boss.\"\"\"\n    company = SoftwareCompany()\n    company.hire([ProductManager(), Architect(), ProjectManager(), Engineer()])\n    company.invest(investment)\n    company.start_project(idea)\n    await company.run(n_round=n_round)\n```\n\nYou can check `examples` for more details on single role (with knowledge base) and LLM only examples.\n\n## QuickStart\n\nIt is difficult to install and configure the local environment for some users. The following tutorials will allow you to quickly experience the charm of MetaGPT.\n\n- [MetaGPT quickstart](https://deepwisdom.feishu.cn/wiki/CyY9wdJc4iNqArku3Lncl4v8n2b)\n\nTry it on Huggingface Space\n- https://huggingface.co/spaces/deepwisdom/MetaGPT\n\n## Citation\n\nFor now, cite the [Arxiv paper](https://arxiv.org/abs/2308.00352):\n\n```bibtex\n@misc{hong2023metagpt,\n      title={MetaGPT: Meta Programming for Multi-Agent Collaborative Framework},\n      author={Sirui Hong and Xiawu Zheng and Jonathan Chen and Yuheng Cheng and Jinlin Wang and Ceyao Zhang and Zili Wang and Steven Ka Shing Yau and Zijuan Lin and Liyang Zhou and Chenyu Ran and Lingfeng Xiao and Chenglin Wu},\n      year={2023},\n      eprint={2308.00352},\n      archivePrefix={arXiv},\n      primaryClass={cs.AI}\n}\n```\n\n## Contact Information\n\nIf you have any questions or feedback about this project, please feel free to contact us. We highly appreciate your suggestions!\n\n- **Email:** alexanderwu@fuzhi.ai\n- **GitHub Issues:** For more technical inquiries, you can also create a new issue in our [GitHub repository](https://github.com/geekan/metagpt/issues).\n\nWe will respond to all questions within 2-3 business days.\n\n## Demo\n\nhttps://github.com/geekan/MetaGPT/assets/2707039/5e8c1062-8c35-440f-bb20-2b0320f8d27d\n\n## Join us\n\n📢 Join Our Discord Channel!\nhttps://discord.gg/ZRHeExS6xv\n\nLooking forward to seeing you there! 🎉\n"
    },
    {
      "name": "dan-redcupit/MetaGPT",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/8456328?s=40&v=4",
      "owner": "dan-redcupit",
      "repo_name": "MetaGPT",
      "description": "🌟 The Multi-Agent Framework: Given one line Requirement, return PRD, Design, Tasks, Repo",
      "homepage": "",
      "language": null,
      "created_at": "2023-11-24T07:27:15Z",
      "updated_at": "2023-12-15T20:41:35Z",
      "topics": [],
      "readme": "# MetaGPT: The Multi-Agent Framework\n\n<p align=\"center\">\n<a href=\"\"><img src=\"docs/resources/MetaGPT-new-log.png\" alt=\"MetaGPT logo: Enable GPT to work in software company, collaborating to tackle more complex tasks.\" width=\"150px\"></a>\n</p>\n\n<p align=\"center\">\n<b>Assign different roles to GPTs to form a collaborative software entity for complex tasks.</b>\n</p>\n\n<p align=\"center\">\n<a href=\"docs/README_CN.md\"><img src=\"https://img.shields.io/badge/文档-中文版-blue.svg\" alt=\"CN doc\"></a>\n<a href=\"README.md\"><img src=\"https://img.shields.io/badge/document-English-blue.svg\" alt=\"EN doc\"></a>\n<a href=\"docs/README_JA.md\"><img src=\"https://img.shields.io/badge/ドキュメント-日本語-blue.svg\" alt=\"JA doc\"></a>\n<a href=\"https://discord.gg/DYn29wFk9z\"><img src=\"https://dcbadge.vercel.app/api/server/DYn29wFk9z?style=flat\" alt=\"Discord Follow\"></a>\n<a href=\"https://opensource.org/licenses/MIT\"><img src=\"https://img.shields.io/badge/License-MIT-blue.svg\" alt=\"License: MIT\"></a>\n<a href=\"docs/ROADMAP.md\"><img src=\"https://img.shields.io/badge/ROADMAP-路线图-blue\" alt=\"roadmap\"></a>\n<a href=\"https://twitter.com/MetaGPT_\"><img src=\"https://img.shields.io/twitter/follow/MetaGPT?style=social\" alt=\"Twitter Follow\"></a>\n</p>\n\n<p align=\"center\">\n   <a href=\"https://vscode.dev/redirect?url=vscode://ms-vscode-remote.remote-containers/cloneInVolume?url=https://github.com/geekan/MetaGPT\"><img src=\"https://img.shields.io/static/v1?label=Dev%20Containers&message=Open&color=blue&logo=visualstudiocode\" alt=\"Open in Dev Containers\"></a>\n   <a href=\"https://codespaces.new/geekan/MetaGPT\"><img src=\"https://img.shields.io/badge/Github_Codespace-Open-blue?logo=github\" alt=\"Open in GitHub Codespaces\"></a>\n   <a href=\"https://huggingface.co/spaces/deepwisdom/MetaGPT\" target=\"_blank\"><img alt=\"Hugging Face\" src=\"https://img.shields.io/badge/%F0%9F%A4%97%20-Hugging%20Face-blue?color=blue&logoColor=white\" /></a>\n</p>\n\n1. MetaGPT takes a **one line requirement** as input and outputs **user stories / competitive analysis / requirements / data structures / APIs / documents, etc.**\n2. Internally, MetaGPT includes **product managers / architects / project managers / engineers.** It provides the entire process of a **software company along with carefully orchestrated SOPs.**\n   1. `Code = SOP(Team)` is the core philosophy. We materialize SOP and apply it to teams composed of LLMs.\n\n![A software company consists of LLM-based roles](docs/resources/software_company_cd.jpeg)\n\n<p align=\"center\">Software Company Multi-Role Schematic (Gradually Implementing)</p>\n\n\n\n## Install\n\n### Pip installation\n\n```bash\n# Step 1: Ensure that Python 3.9+ is installed on your system. You can check this by using:\n# You can use conda to initialize a new python env\n#     conda create -n metagpt python=3.9\n#     conda activate metagpt\npython3 --version\n\n# Step 2: Clone the repository to your local machine for latest version, and install it.\ngit clone https://github.com/geekan/MetaGPT.git\ncd MetaGPT\npip3 install -e.     # or pip3 install metagpt  # for stable version\n\n# Step 3: run the startup.py\n# setup your OPENAI_API_KEY in key.yaml copy from config.yaml\npython3 startup.py \"Write a cli snake game\"\n\n# Step 4 [Optional]: If you want to save the artifacts like diagrams such as quadrant chart, system designs, sequence flow in the workspace, you can execute the step before Step 3. By default, the framework is compatible, and the entire process can be run completely without executing this step.\n# If executing, ensure that NPM is installed on your system. Then install mermaid-js. (If you don't have npm in your computer, please go to the Node.js official website to install Node.js https://nodejs.org/ and then you will have npm tool in your computer.)\nnpm --version\nsudo npm install -g @mermaid-js/mermaid-cli\n```\n\ndetail installation please refer to [cli_install](https://docs.deepwisdom.ai/guide/get_started/installation.html#install-stable-version)\n\n### Docker installation\n\n```bash\n# Step 1: Download metagpt official image and prepare config.yaml\ndocker pull metagpt/metagpt:latest\nmkdir -p /opt/metagpt/{config,workspace}\ndocker run --rm metagpt/metagpt:latest cat /app/metagpt/config/config.yaml > /opt/metagpt/config/key.yaml\nvim /opt/metagpt/config/key.yaml # Change the config\n\n# Step 2: Run metagpt demo with container\ndocker run --rm \\\n    --privileged \\\n    -v /opt/metagpt/config/key.yaml:/app/metagpt/config/key.yaml \\\n    -v /opt/metagpt/workspace:/app/metagpt/workspace \\\n    metagpt/metagpt:latest \\\n    python startup.py \"Write a cli snake game\"\n```\n\ndetail installation please refer to [docker_install](https://docs.deepwisdom.ai/guide/get_started/installation.html#install-with-docker)\n\n### QuickStart & Demo Video\n- Try it on [MetaGPT Huggingface Space](https://huggingface.co/spaces/deepwisdom/MetaGPT)\n- [Matthew Berman: How To Install MetaGPT - Build A Startup With One Prompt!!](https://youtu.be/uT75J_KG_aY)\n- [Official Demo Video](https://github.com/geekan/MetaGPT/assets/2707039/5e8c1062-8c35-440f-bb20-2b0320f8d27d)\n\nhttps://github.com/geekan/MetaGPT/assets/34952977/34345016-5d13-489d-b9f9-b82ace413419\n\n## Tutorial\n\n- 🗒 [Online Document](https://docs.deepwisdom.ai/)\n- 💻 [Usage](https://docs.deepwisdom.ai/guide/get_started/quickstart.html)  \n- 🔎 [What can MetaGPT do?](https://docs.deepwisdom.ai/guide/get_started/introduction.html)\n- 🛠 How to build your own agents? \n  - [MetaGPT Usage & Development Guide | Agent 101](https://docs.deepwisdom.ai/guide/tutorials/agent_101.html)\n  - [MetaGPT Usage & Development Guide | MultiAgent 101](https://docs.deepwisdom.ai/guide/tutorials/multi_agent_101.html)\n- 🧑‍💻 Contribution\n  - [Develop Roadmap](docs/ROADMAP.md)\n- 🔖 Use Cases\n  - [Debate](https://docs.deepwisdom.ai/guide/use_cases/multi_agent/debate.html)\n  - [Researcher](https://docs.deepwisdom.ai/guide/use_cases/agent/researcher.html)\n  - [Recepit Assistant](https://docs.deepwisdom.ai/guide/use_cases/agent/receipt_assistant.html)\n- ❓ [FAQs](https://docs.deepwisdom.ai/guide/faq.html)\n\n## Support\n\n### Discard Join US\n📢 Join Our [Discord Channel](https://discord.gg/ZRHeExS6xv)!\n\nLooking forward to seeing you there! 🎉\n\n### Contact Information\n\nIf you have any questions or feedback about this project, please feel free to contact us. We highly appreciate your suggestions!\n\n- **Email:** alexanderwu@fuzhi.ai\n- **GitHub Issues:** For more technical inquiries, you can also create a new issue in our [GitHub repository](https://github.com/geekan/metagpt/issues).\n\nWe will respond to all questions within 2-3 business days.\n\n## Citation\n\nFor now, cite the [arXiv paper](https://arxiv.org/abs/2308.00352):\n\n```bibtex\n@misc{hong2023metagpt,\n      title={MetaGPT: Meta Programming for A Multi-Agent Collaborative Framework}, \n      author={Sirui Hong and Mingchen Zhuge and Jonathan Chen and Xiawu Zheng and Yuheng Cheng and Ceyao Zhang and Jinlin Wang and Zili Wang and Steven Ka Shing Yau and Zijuan Lin and Liyang Zhou and Chenyu Ran and Lingfeng Xiao and Chenglin Wu and Jürgen Schmidhuber},\n      year={2023},\n      eprint={2308.00352},\n      archivePrefix={arXiv},\n      primaryClass={cs.AI}\n}\n```\n"
    },
    {
      "name": "Gonzivang/SecondTry",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/29268941?s=40&v=4",
      "owner": "Gonzivang",
      "repo_name": "SecondTry",
      "description": null,
      "homepage": null,
      "language": "Java",
      "created_at": "2024-01-23T19:05:09Z",
      "updated_at": "2024-04-29T07:55:37Z",
      "topics": [],
      "readme": ""
    },
    {
      "name": "castillosebastian/genai3",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/17081544?s=40&v=4",
      "owner": "castillosebastian",
      "repo_name": "genai3",
      "description": null,
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2024-01-19T16:39:23Z",
      "updated_at": "2024-01-31T13:47:08Z",
      "topics": [],
      "readme": "# GenAI Project Template and Notes (periodically updated)\n\nThis repository maintains a limited selection of code, resources and articles related to the field of GenAI and its application to chatbots (with a focus on RAG-type architectures). The general purpose is to test the solutions proposed in this field using LLMs. Any contribution is welcome.    \n\nIn this context, we divided the content of our notes and testing following the GenAI development applications model proposed in the course dictated by Deeplearning.ai. The model or template has 6 steps: \n\n1. Define use case\n2. Chose an existing model or pre-train your own\n3. Adapt and align model\n   1. prompt engineering\n   2. fine tunning\n   3. align with human feedback\n4. Evaluate\n5. Optimize\n6. Deploy\n\n## RAG-Paradigms\n\n![Gao et all, 2023](image/RAG-Paradigms.png)\n[arxiv](http://arxiv.org/abs/2312.10997)\n\n## Tools and resources:\n  \n1. Use case: **GenAI for chatbot in the Finance Sector with RAG**:   \n   1. As a linguistic object, financial statements are characterized by a unique blend of features. They consist of formal, technical language with a heavy reliance on specialized financial and accounting terminology. The structure is highly standardized and regulated, ensuring a consistent format across various documents. The language is predominantly objective, focusing on quantitative data and factual information. It's also legally cautious, often including disclaimers and cautionary statements. Narrative elements are present, especially in sections like Management’s Discussion and Analysis (MD&A), providing qualitative insights. The use of passive voice is common, emphasizing actions and results over the entities performing them. Additionally, these documents feature a mix of concise yet comprehensive descriptions, ensuring clarity and specificity. Speculative language is used carefully in forward-looking statements, indicating projections and expectations. Companies may also be cautious in revealing sensitive data that could advantage competitors. Therefore, while financial statements provide key financial data, the presentation is often calibrated to serve both transparency and corporate strategy. \n   2. Data characteristics and format? Rich format documents!\n      1. Tables: [Langchain_1](https://blog.langchain.dev/benchmarking-rag-on-tables/), Microsoft [tabletransformer](https://github.com/microsoft/table-transformer), [RAG-Table](https://github.com/langchain-ai/langchain/blob/master/cookbook/Semi_Structured_RAG.ipynb?ref=blog.langchain.dev), \n         1. Table_reasoning [RegHNT](https://arxiv.org/pdf/2209.07692.pdf), [github](https://github.com/castillosebastian/RegHNT)\n         2. Table_reasoning [UniRPG](https://aclanthology.org/2022.emnlp-main.508.pdf), [github](https://github.com/phddamuge/UniRPG)\n      2. Stream_response: [langchain_1](https://github.com/pinecone-io/examples/blob/master/learn/generation/langchain/handbook/09-langchain-streaming/09-langchain-streaming.ipynb), [langchain](https://medium.com/databutton/stream-langchain-ai-abstractions-and-responses-in-your-web-app-langchain-tools-in-action-e37907779437)\n      3. Multimodality: [Langchain](https://github.com/langchain-ai/langchain/blob/master/cookbook/Semi_structured_multi_modal_RAG_LLaMA2.ipynb?ref=blog.langchain.dev), [RAG-table-image](https://github.com/langchain-ai/langchain/blob/master/cookbook/Semi_structured_and_multi_modal_RAG.ipynb?ref=blog.langchain.dev),      \n   3. Legal compliance requirements? *High Risk AI* according to [EU_AI_Act](https://artificialintelligenceact.com/), [Consumer_Financial_Protectional_Bureau_US_on_Chatbots](https://www.consumerfinance.gov/data-research/research-reports/chatbots-in-consumer-finance/chatbots-in-consumer-finance/), \n   4. Solutions (well, almost): \n      1. [RAG-ADVACE_Azure-AISearch-OpenAI](https://github.com/Azure-Samples/azure-search-openai-demo/blob/main/README.md), [RAG-SIPLE_Azure_AISearch-OpenAI](https://github.com/Azure-Samples/chat-with-your-data-solution-accelerator/blob/main/docs/LOCAL_DEPLOYMENT.md)\n         1. Test-OpenAI-Chat: [Playground](https://platform.openai.com/playground?mode=chat)\n      2. [SECInsights](https://github.com/run-llama/sec-insights)\n      3. [OpenAI-RAG](https://cookbook.openai.com/examples/fine-tuned_qa/ft_retrieval_augmented_generation_qdrant)\n      4. [Azure-GPT-RAG](https://github.com/Azure/GPT-RAG/tree/main) - [youtube:globant!](https://www.youtube.com/watch?v=ICsf4yirieA)\n      5. [OpenAI-Langchain-Redis:FinTemplate](https://github.com/langchain-ai/langchain/tree/master/templates/rag-redis)\n      6. [OpenAI-Agents-Finacial](https://medium.com/gitconnected/mastering-openai-assistants-api-building-an-ai-financial-analyst-to-forecast-stock-trend-17a45c77607a), [colab](https://github.com/castillosebastian/genai0/blob/014a9db7fd25ac9d11e12f1bb94659c754341d72/related_works/Cloud_VM/Financial_Statement_Analyst_Using_OpenAIAssistantsAPI_v1.ipynb#L879)\n   \n2. Existing models:\n   1. [GPT-Playground](https://platform.openai.com/playground)\n   2. [LLama2-chat](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf)  \n   3. [FinMA](https://huggingface.co/ChanceFocus/finma-7b-full)\n   4. [FinGPT](https://huggingface.co/FinGPT)\n   5. [Mistral-7b](https://huggingface.co/docs/transformers/main/model_doc/mistral)\n   6. [Mistral-8x7b-SMoe](https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1), [mistral-on-colab](https://github.com/dvmazur/mixtral-offloading/blob/master/notebooks/demo.ipynb), [2](https://huggingface.co/blog/mixtral), [3](https://arxiv.org/abs/2101.03961), [4](https://arxiv.org/pdf/2305.14705.pdf)      \n   7. [finBert](https://huggingface.co/yiyanghkust/finbert-pretrain)\n   8. [FinanceConnect-13b](https://huggingface.co/ceadar-ie/FinanceConnect-13B)\n   9.  [LLM360](https://www.llm360.ai/)\n   10. [Phi-2](https://huggingface.co/microsoft/phi-2)\n   11. ...\n   12. Private Models: [BloombergGPT](https://arxiv.org/abs/2303.17564), interesting info (e.g.Training Chronicles)  \n\n3. Adapt and Align (AA):\n   1. Agregations and Math:\n      1. [LLM-Compiler](https://github.com/SqueezeAILab/LLMCompiler), [llama-api](https://github.com/run-llama/llama-hub/blob/main/llama_hub/llama_packs/agents/llm_compiler/llm_compiler.ipynb)\n   2. AA:Prompt:    \n      1. [MedPrompt](https://arxiv.org/abs/2311.16452)\n      2. [PROMT_GUIDE](https://www.promptingguide.ai/)\n      3. [OPENAI_PROMPT_GUIDE]    \n   3. AA:FineTune:   \n      1. [AdaptLLMstoDomains](https://huggingface.co/AdaptLLM/finance-LLM)\n      2. [ft_llama2_LoRA](https://arxiv.org/abs/2308.13032): summarization and NER.\n      3. ...\n      - Datasets:\n        - [FinTalk19k](https://huggingface.co/datasets/ceadar-ie/FinTalk-19k)\n        - [Alpaca](https://huggingface.co/datasets/tatsu-lab/alpaca)\n        - [EDGAR-CORPUS](https://huggingface.co/datasets/eloukas/edgar-corpus)\n        - [FinancialReports_HuggF](https://huggingface.co/datasets/JanosAudran/financial-reports-sec)\n   4. AA: Aling and HF    \n      1. [Pearl](https://pearlagent.github.io/)\n      2. [DPO](https://arxiv.org/pdf/2305.18290.pdf)\n\n4. Evaluation\n   1. [Promptbench](https://promptbench.readthedocs.io/en/latest/examples/basic.html)\n   2. [TrueLens](https://www.trulens.org/), [2](https://blog.llamaindex.ai/build-and-evaluate-llm-apps-with-llamaindex-and-trulens-6749e030d83c)\n   3. [Lanmgchain-Huggingface](https://www.philschmid.de/evaluate-llm), or [Langchain](https://docs.smith.langchain.com/evaluation/evaluator-implementations?ref=blog.langchain.dev#correctness-qa-evaluation), or [Langchain](https://github.com/langchain-ai/langsmith-cookbook/blob/main/testing-examples/qa-correctness/qa-correctness.ipynb)    \n   4. [Promptfoo]([https://github.com/promptfoo/promptfoo/blob/main/README.md)\n5. Benchmarks\n   1. [FinanceBench](https://huggingface.co/datasets/PatronusAI/financebench), [github](https://github.com/patronus-ai/financebench/tree/main), [whitepaper](https://techcommunity.microsoft.com/t5/ai-azure-ai-services-blog/azure-cognitive-search-outperforming-vector-search-with-hybrid/ba-p/3929167)\n   2. [FinQA](https://aclanthology.org/2021.emnlp-main.300/), [github](https://github.com/castillosebastian/FinQA)\n   3. [TAT-QA](https://aclanthology.org/2021.acl-long.254.pdf), [github](https://nextplusplus.github.io/TAT-QA/)\n   4. [ConFIRM](https://arxiv.org/abs/2310.13001), [github](https://github.com/WilliamGazeley/ConFIRM)\n   5. [FLANG-FLUE](https://aclanthology.org/2022.emnlp-main.148.pdf), [huggingface](https://huggingface.co/datasets/SALT-NLP/FLUE-FiQA),\n\n6. Optimize\n   1. [FineTunning_OpeAI](https://platform.openai.com/docs/guides/fine-tuning/fine-tuning-examples)\n\n7. Deploy\n   1. [Azure-AISearch-OpenAI](https://github.com/Azure-Samples/azure-search-openai-demo/blob/main/README.md), for creating dataset see [ConFIRM](https://arxiv.org/abs/2310.13001),[github](https://github.com/WilliamGazeley/ConFIRM)\n\n\n## Scripts Tested\n\nAll testing is made in a VM on Google Cloud free tier: 24 vCPU, 84G RAM, 100G Disk, Ubuntu 22. I made an installation [script](https://github.com/castillosebastian/genai0/blob/main/related_works/Cloud_VM/instalar.sh) to run a non-secure IDE. When the installation is finish, to create the Python environment follow:\n\na. `python3 -m venv ~/.genai0`   \nb. `source  ~/.genai0/bin/activate`   \nc. `python3 -m pip install --upgrade pip`   \nd. `pip install -r requirements.txt`    \ne. Set interpreter in Project settings: Type and select \"Python: Select Interpreter.\" Choose the interpreter from your .genai0 virtual environment. It should be something like /root/.genai0/bin/python3.     \n\n1. Open-Soruce models tested\n   1. [Zphyr-7b_gen_exploration](https://github.com/castillosebastian/genai0/blob/main/related_works/Cloud_VM/rag2_ok_HugFace-zepyyr.py)\n   2. [llama-2-chat-13b-ggml_Q4](https://github.com/castillosebastian/genai0/blob/main/related_works/Cloud_VM/rag3_ok_LLama2-13b_Q4.py)\n   3. [FinMa-7b-full_part1](https://github.com/castillosebastian/genai0/blob/main/related_works/Cloud_VM/rag4_FinMA-7bfull.py)\n   4. [Mistral7b-Q4](https://github.com/castillosebastian/genai0/blob/main/related_works/Cloud_VM/rag5_Mistral7b_Q4.py)\n\n2. Private Model tested:\n   1. [gpt-3.5-turbo-trulens-eval](https://github.com/castillosebastian/genai0/blob/main/related_works/RAG_DeeplearningAI/L1-Advanced_RAG_Pipeline.ipynb)\n\n## Querying Strategies and VectorDB\n1. QS:   \n   2. Basics\n   3. Advaced\n      1. SubQuestionQueryEngine for complex questions\n      2. Small-to-big retrieval for improved precision\n      3. Metadata filtering, also for improved precision\n      4. Hybrid search including traditional search engine techniques: [IMPORTATN](https://techcommunity.microsoft.com/t5/ai-azure-ai-services-blog/azure-cognitive-search-outperforming-vector-search-with-hybrid/ba-p/3929167)\n      5. Recursive Retrieval for complex documents: [RecursiveRetriver](https://llamahub.ai/l/llama_packs-dense_x_retrieval?from=llama_packs)\n      6. Text to SQL\n      7. Multi-document agents that can combine all of these techniques\n      8. Ensembles: [EnsembleRetriever](https://python.langchain.com/docs/modules/data_connection/retrievers/ensemble?ref=blog.langchain.dev)\n2. VDB (for performace comparison: [vectorview](https://benchmark.vectorview.ai/vectordbs.html), [ANN-Benchmarks](https://ann-benchmarks.com/index.html))\n   1. [Qdrant](https://qdrant.tech/documentation/), [llamaQdrant](https://docs.llamaindex.ai/en/stable/examples/vector_stores/QdrantIndexDemo.html), [performance_evaluation](https://qdrant.tech/benchmarks/)\n   2. [Azure_AI-Searh_docu](https://learn.microsoft.com/en-us/azure/search/vector-search-filters), [vector-search](https://github.com/Azure/azure-search-vector-samples), [code](https://github.com/Azure/azure-search-vector-samples/tree/main/demo-python)\n3. DB configuration:\n   1. [RAG in Azure AI Search](https://learn.microsoft.com/en-us/azure/search/retrieval-augmented-generation-overview), [video](https://ignite.microsoft.com/en-US/sessions/18618ca9-0e4d-4f9d-9a28-0bc3ef5cf54e?source=sessions)\n\n\n## Api and +\n0. [OpenAI_chat-completion_endpoint](https://platform.openai.com/docs/api-reference/chat/create)\n1. [FMP](https://site.financialmodelingprep.com/developer/docs?ref=mlq.ai#Company-Financial-Statements): financial statements, historical-data, etc\n2. [SecFillingsDownloader](https://github.com/jadchaar/sec-edgar-downloader)\n3. [Polygon](https://polygon.io/stocks?utm_term=polygon%20io&utm_campaign=Brand+-+ALL+(Conv+Value+tROAS)&utm_source=adwords&utm_medium=ppc&hsa_acc=4299129556&hsa_cam=14536485495&hsa_grp=132004734661&hsa_ad=614838466716&hsa_src=g&hsa_tgt=aud-1438727183434:kwd-994300255560&hsa_kw=polygon%20io&hsa_mt=e&hsa_net=adwords&hsa_ver=3&gad_source=1&gclid=CjwKCAiA-P-rBhBEEiwAQEXhH2_6W2Y2rhx8W6-T9v6UseLYYpMfBHCbXw_ayo5-cWpfUCHOoMQFXRoCGVIQAvD_BwE )\n4. [Diffbot-API](https://www.diffbot.com/)\n\n## Semantic Kernel\n1. [Doc](https://learn.microsoft.com/es-mx/semantic-kernel/)\n2. [Glossary](https://github.com/microsoft/semantic-kernel/blob/main/docs/GLOSSARY.md)\n3. [Repo](https://github.com/microsoft/semantic-kernel)\n4. [Repo-Python-Examples](https://github.com/microsoft/semantic-kernel/blob/main/python/README.md), [Examples2](https://github.com/microsoft/semantic-kernel/tree/main/python/samples/kernel-syntax-examples)\n5. [Videos](https://www.youtube.com/playlist?list=PL20mfA9efrmMmLEy1fhFDvB_OmUpNUFqB)\n6. [PLUGINS_SK_&_OpenAI](https://platform.openai.com/docs/plugins/getting-started/) \n7. [RAG_lite](https://charotamine.medium.com/rag-semantic-kernel-langchain-azure-openai-dc701f5f4d2b)\n\n## Bib and ref\n1. [LLMs_in_finance](https://arxiv.org/abs/2311.10723)\n2. [RAG-Survey](https://arxiv.org/abs/2312.10997v1?utm_source=substack&utm_medium=email)\n3. [FinanceBench](https://huggingface.co/datasets/PatronusAI/financebench), [github](https://github.com/patronus-ai/financebench/tree/main)\n4. AzureVidSeries: [1](https://ignite.microsoft.com/en-US/sessions/24cfc794-f932-4f36-9dbe-d7daa1a1b27c), \n5. [Azure-AI-Search](https://learn.microsoft.com/en-us/azure/search/vector-search-how-to-query?tabs=query-2023-11-01%2Cfilter-2023-11-01)\n6. [Terrible-RAG-Systems](https://jxnl.github.io/blog/writing/2024/01/07/inverted-thinking-rag/#we-should-not-have-to-build-special-injestion-pipelines)\n7. [semantic-kernell](https://github.com/microsoft/semantic-kernel/blob/main/python/README.md)\n\n## Toy App\n\n![RAGbot_App](image/RAGbot.png)\n\n# Demo APP\n\n1. [Demo APP Azure-AISearch-OpenAI](https://github.com/Azure-Samples/azure-search-openai-demo/blob/main/README.md),[1](https://learn.microsoft.com/en-us/azure/search/retrieval-augmented-generation-overview), [2](https://learn.microsoft.com/en-us/azure/developer/python/get-started-app-chat-template?tabs=github-codespaces),\n\n2. **VectorDB**\n- [document-intelligence-layout](https://learn.microsoft.com/en-us/azure/ai-services/document-intelligence/concept-layout?view=doc-intel-4.0.0)\n- [indexes](https://learn.microsoft.com/en-us/azure/search/search-what-is-an-index)\n  \n3. **Retriever**\n\n![search-strategy-comparison](image/Retriever-search-stragegi-comparison.png)\n\n- [hybrid+reranking](https://techcommunity.microsoft.com/t5/ai-azure-ai-services-blog/azure-cognitive-search-outperforming-vector-search-with-hybrid/ba-p/3929167)\n- [vector_search](https://learn.microsoft.com/en-us/azure/search/retrieval-augmented-generation-overview)\n\n1. **Generation**\n- [Langchain-cookbook](https://python.langchain.com/cookbook)\n\n\n\n![Alt text](image/sk.png)\n\n# Logical Design\n1. Ask\n   1. Stepwise-Planner: [file](semantic-kernel/python/notebooks/05-using-the-planner.ipynb)\n      1. Plugins: \n2. Answer\n\n# Recipes_Examples: \n- https://github.com/alexchaomander/SK-Recipes/tree/main\n- https://github.com/rajib76/semantic_kernel_examples/tree/main\n\n# Integration\n- SK-Langchain: https://blog.langchain.dev/langchain-expands-collaboration-with-microsoft/"
    },
    {
      "name": "msalemor/gpt-assistants",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/44649358?s=40&v=4",
      "owner": "msalemor",
      "repo_name": "gpt-assistants",
      "description": "A collection of OpenAI Assistants",
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2024-01-12T04:11:49Z",
      "updated_at": "2024-05-07T06:29:40Z",
      "topics": [],
      "readme": "# AI Assistants\n\n## Overview\n\nThe Assistants API enables you to create AI assistants in your own applications. These assistants are equipped with instructions and can utilize various models, tools, and knowledge to answer user questions. Currently, the API supports three types of tools: Code Interpreter, Retrieval, and Function calling. More tools will be added in the future.\n\n## Benefits of using assistants\n\n| Feature | Description |\n|--------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Threads | Contains Messages and automatically handles the truncation of content to fit within the context of a model. |\n| Files | Allows importing file content of different file formats. Can be used in tools such as Retrieval scenarios and analysis with Code Interpreter. |\n| Tools | Includes Code Interpreter, Knowledge Retrieval, and Function calling.<br>&nbsp;- Code Interpreter allows executing code snippets.<br>&nbsp;- Knowledge Retrieval automatically chunks and embeds content in files for augmented retrieval scenarios.<br>&nbsp;- Function calling enables calling functions. |\n| Tool composition | Enables using multiple tools in one Assistant. |\n\n## Foundational concepts\n\n| Term | Definition |\n|--------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Assistant | An AI system specifically designed to utilize OpenAI's models and call tools. |\n| Thread | A session of conversation between a user and an Assistant. Threads store Messages and automatically handle truncation to ensure that content fits within the context of the model. |\n| Message | A piece of communication generated by either an Assistant or a user. Messages can contain text, images, and other files. Messages are stored as a list within a Thread. |\n| Run | An instance of an Assistant being invoked on a Thread. The Assistant utilizes its configuration and the Messages within the Thread to perform tasks by calling models and tools. During a Run, the Assistant appends Messages to the Thread. |\n| Run Step | A detailed record of the individual actions taken by the Assistant during a Run. These steps can include calling tools or generating Messages. Examining Run Steps provides insight into how the Assistant arrives at its final results. |\n\n## Lifecycle\n\n| Status | Description |\n|--------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| queued | When Runs are initially created or when the required action is completed, they are placed in a queued status. They should quickly transition to an in_progress status. | | in_progress | While in_progress, the Assistant utilizes the model and tools to perform steps. The progress of the Run can be monitored by examining the Run Steps. |\n| completed | The Run has successfully finished! You can now access all the Messages added by the Assistant to the Thread, as well as the steps taken by the Run. You can also continue the conversation by adding more user Messages to the Thread and initiating another Run. |\n| requires_action | When using the Function calling tool, the Run will transition to a required_action state once the model determines the names and arguments of the functions to be called. You must then execute those functions and submit the outputs before the Run can proceed. If the outputs are not provided before the expires_at timestamp (approximately 10 minutes after creation), the Run will move to an expired status. |\n| expired | This occurs when the function calling outputs were not submitted before the expires_at timestamp and the Run expires. Additionally, if the Run takes too long to execute and exceeds the time specified in expires_at, our systems will mark the Run as expired. |\n| cancelling | You can attempt to cancel an in_progress Run by using the Cancel Run endpoint. Once the cancellation attempt is successful, the status of the Run will change to cancelled. However, cancellation is not guaranteed and may not always be possible. |\n| cancelled | The Run was successfully cancelled. |\n| failed | The reason for the failure can be viewed by examining the last_error object in the Run. The timestamp for the failure will be recorded under failed_at. |\n\n### Reference Sample code\n\nReference:\n\n- [OpenAI Assistant Sample](https://github.com/openai/openai-python/blob/main/examples/assistant.py)\n\n```python\nimport time\nfrom openai import AzureOpenAI\n\n# gets API Key from environment variable OPENAI_API_KEY\nclient = AzureOpenAI(api_key=api_key,\n        api_version=api_version,\n        azure_endpoint=api_endpoint)\n\n# Create an asssitant\nassistant = client.beta.assistants.create(\n    name=\"Math Tutor\",\n    instructions=\"You are a personal math tutor. Write and run code to answer math questions.\",\n    tools=[{\"type\": \"code_interpreter\"}],\n    model=\"gpt-4-1106-preview\",\n)\n\n# Create a thread\nthread = client.beta.threads.create()\n\n# Create a message\nmessage = client.beta.threads.messages.create(\n    thread_id=thread.id,\n    role=\"user\",\n    content=\"I need to solve the equation `3x + 11 = 14`. Can you help me?\",\n)\n\n# Create a run\nrun = client.beta.threads.runs.create(\n    thread_id=thread.id,\n    assistant_id=assistant.id,\n    instructions=\"Please address the user as Jane Doe. The user has a premium account.\",\n)\n\n# Check the status of a run\nprint(\"checking assistant status. \")\nwhile True:\n    # Get the run information\n    run = client.beta.threads.runs.retrieve(thread_id=thread.id, run_id=run.id)\n\n    # If the run.status has changed to completed\n    if run.status == \"completed\":\n        print(\"done!\")\n        # Get the messages for a thread\n        messages = client.beta.threads.messages.list(thread_id=thread.id)\n\n        # Print the messages\n        print(\"messages: \")\n        for message in messages:\n            assert message.content[0].type == \"text\"\n            print({\"role\": message.role, \"message\": message.content[0].text.value})\n        \n        # Dispose of the assistant\n        client.beta.assistants.delete(assistant.id)\n\n        # Dispose of the thread\n        client.beta.threads.delete(thread.id)\n\n        break\n    else:\n        print(\"in progress...\")\n        time.sleep(5)\n```\n\n## Sample Assistants - Jupyter notebooks\n\n| Topic | Description |\n|----------------------|--------------------------------------------------|\n| [Assistant-01: Foundational Concepts](notebooks/assistant-01-code_interpreter.ipynb) | Showcases the foundational concepts of Assistants such as threads, messages, runs, and tools. |\n| [Assistant-04: Financial Summary bot](notebooks/assistant-04-function_la_email.ipynb) | Using Code Interpreter and Function calling this bot can summarize a financial news article, extract the ticker symbols from the article, get the latest stock prices, and email a report. |\n| [Assistant-06: Portpolio Reporting bot](notebooks/assistant-06-code_function_la_email.ipynb) | Using Code Interpreter and Function calling, this bot can get a CSV portfolio file, calculate the latest portfolio value based on the latest stock prices, point out the best and the worst investment, and email a report to a client. |\n| [Assistant-10: HR bot](notebooks/assistant-10-Travelbot.ipynb) | Using the Retrieval tool and providing files on different HR-related topics, the bot can answer employees' questions about these topics. |\n| [Assistant-11: Travel bot](notebooks/assistant-11-Travelbot.ipynb) | Using Code Interpreter, Function calling, and Retrieval, this bot can answer users' questions from a travel itinerary, split and calculate shared expenses related to the travel, and block a user's Outlook calendar related to the travel. |\n"
    },
    {
      "name": "silarsis/assistant",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/2221181?s=40&v=4",
      "owner": "silarsis",
      "repo_name": "assistant",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2023-04-19T16:38:43Z",
      "updated_at": "2024-09-25T12:23:24Z",
      "topics": [],
      "readme": "# Assistant\n\nThis project is to tie together various LLM-related pieces, to try and build an AI assistant that can research and provide feedback on things I'm working on.\n\n## How to Use\n\nCheckout the source code, cd to the \"agent\" directory, install dependencies by running:\n\n`poetry install`\n\nIf you don't have poetry installed already, please see https://python-poetry.org/docs/ for instructions.\n\nThen run `python ./client_gradio.py` to start the agent, and look for the locahost URI printed in the output - you should\nbe able to web browse to that.\n\nAPI keys are now entered in the app itself, so once you're running you can fold out the appropriate entries on the sidebar\nand fill them out. Note, without an API key for one of the LLMs (Azure, OpenAI, etc) not much works ;)\n\nIf you want to do certain things, you'll need the containers running - we're down to a text-to-speech and a chromadb for the google docs\nplugin. Simply run `docker-compose up -d` and it should run the containers for you - or, if you want to point at a SaaS solution,\ncheck the .env file. `docker-compose build --no-cache --pull` for building the docker images.\n\n## Goals\n\nAn app that does the following:\n\n  * Listens to mic and does speech-to-text\n  * Speaks for responses\n  * Ties together multiple LLM in a similar way to Jarvis\n  * Can Google and do self-feedback in the way auto-gpt does\n  * Pluggable models and a clear abstraction for them in the code, for future extensibility\n  * A longer-term memory and the ability to use that memory with local models (ie. sidestep the current API limits)\n  * Visual inputs from a camera and image recognition\n\nBeyond the above, this repo is for experimentation and to help me understand things like fine-tuning and in-context learning\n\n## Process\n\nMost of the code you'll find in here is either from other projects, or from ChatGPT or CoPilot.\nOther projects that have been an inspiration include:\n\n  * AutoGPT\n  * Dalai\n  * Milvus\n  * https://github.com/Venthe/chatgpt4all-webui\n  * https://github.com/randaller/llama-chat\n  * https://open-assistant.io/\n\n** Prompting for a task I want the agent to be able to do:\n\nGiven a document to analyse, you should do the following steps:\n\n* Load the document into your document store\n* Search the document for primary subject matter\n* Search your memory for the same subject matter\n* Search the web for websites pertaining to this subject matter\n* Save websites that talk about this subject matter\n* Based on what you've learnt from the websites and your memory, provide an analysis of the initial document.\n\nYou have the following tools to help you do this:\n\n* DocStore to load the document, or the Google doc loader for google docs\n* web scraper for websites\n* memory to store scraped websites and the document itself"
    },
    {
      "name": "vinod-soni-microsoft/thinking-out-loud-librarian-whisper",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/117770765?s=40&v=4",
      "owner": "vinod-soni-microsoft",
      "repo_name": "thinking-out-loud-librarian-whisper",
      "description": "Thinking out loud librarian app based on whisper model",
      "homepage": null,
      "language": "Python",
      "created_at": "2024-01-04T18:54:00Z",
      "updated_at": "2024-01-04T18:59:17Z",
      "topics": [],
      "readme": "# Thinking out loud librarian - Whisper model\nThinking out loud librarian - Whisper model\n\n# Steps\n1. go to git command prompt\n2. go to scr folder\n3. create .env file\n4. add variable in .env file and add key OPENAI_API_KEY and specify key value (you can create key at openai portal).\n5. create virtual environment\n6. cd src\n7. python -m venv .venv\n8. source .venv/bin/activate\n9. pip install -r requirements.txt\n10. Run the code\n11. uvicorn main:app\n12. uvicorn main:app --reload\n\n### Instructor\n\nVinod Soni\n"
    },
    {
      "name": "vinod-soni-microsoft/chain-of-thought",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/117770765?s=40&v=4",
      "owner": "vinod-soni-microsoft",
      "repo_name": "chain-of-thought",
      "description": "Integrating chain-of-thought reasoning into your app",
      "homepage": null,
      "language": "Python",
      "created_at": "2024-01-04T15:27:07Z",
      "updated_at": "2024-01-04T15:48:01Z",
      "topics": [],
      "readme": "# Integrating chain-of-thought reasoning into your app\nIntegrating chain-of-thought reasoning into your app\n\n# Steps\n1. go to git command prompt\n2. go to scr folder\n3. create .env file\n4. add variable in .env file and add key OPENAI_API_KEY and specify key value (you can create key at openai portal).\n5. create virtual environment\n6. cd src\n7. python -m venv .venv\n8. source .venv/bin/activate\n9. pip install -r requirements.txt\n10. Run the code\n11. uvicorn main:app\n12. uvicorn main:app --reload\n\n\n### Instructor\n\nVinod Soni\n"
    },
    {
      "name": "vinod-soni-microsoft/simple-summarizer-with-semantic-kernel",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/117770765?s=40&v=4",
      "owner": "vinod-soni-microsoft",
      "repo_name": "simple-summarizer-with-semantic-kernel",
      "description": "simple summarizer with semantic kernel",
      "homepage": null,
      "language": "Python",
      "created_at": "2024-01-03T21:36:58Z",
      "updated_at": "2024-01-04T05:48:35Z",
      "topics": [],
      "readme": "# A Simple summarizer with Semantic Kernel\nA Simple summarizer with Semantic Kernel\n\n\n# Steps\n1. go to git command prompt\n2. go to scr folder\n3. create .env file\n4. add variable in .env file and add key OPENAI_API_KEY and specify key value (you can create key at openai portal).\n5. create virtual environment\n6. cd src\n7. python -m venv .venv\n8. source .venv/bin/activate\n9. pip install -r requirements.txt\n10. Run the code\n11. uvicorn main:app\n12. uvicorn main:app --reload\n\n### Instructor\n\nVinod Soni\n\n                            \n\nCheck out my other courses on [LinkedIn Learning](https://www.linkedin.com/learning/instructors/denys-linkov).\n\n[lil-course-url]: https://www.linkedin.com/learning/building-apps-with-ai-tools-chatgpt-semantic-kernel-and-langchain?dApp=59033956&leis=LAA\n[lil-thumbnail-url]: https://media.licdn.com/dms/image/D560DAQENsHoqO_7Y1Q/learning-public-crop_675_1200/0/1694035139061?e=2147483647&v=beta&t=cCnp-C_2NN9JnQM_fHZlVyjfMrGzfgpiGXv5i4Y_5mM\n"
    },
    {
      "name": "awkwardindustries/starter-sk-python-functions",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/31149154?s=40&v=4",
      "owner": "awkwardindustries",
      "repo_name": "starter-sk-python-functions",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2023-12-19T21:02:27Z",
      "updated_at": "2024-01-04T00:40:13Z",
      "topics": [],
      "readme": "# Starter Project for Azure Function (Python) with Semantic Kernel\n\n## Resources\n\n- Azure OpenAI Service\n- Azure Bing Search\n- Azure Functions\n\n## Build and run\n\n### 0. Choose your development environment\n\n#### (Option) Local Development\n\nYou can develop in this project locally on your own machine. You'll need to:\n1. If you choose to run the project locally within a devcontainer, all necessary software and libraries will be included and run within the devcontainer. If you choose to run locally without a devcontainer, you will need to ensure that you have the necessary software and libraries installed. You will need:\n   - [Azure CLI](https://learn.microsoft.com/en-us/cli/azure/install-azure-cli)\n   - [Azure Functions Core Tools](https://learn.microsoft.com/en-us/azure/azure-functions/functions-run-local#install-the-azure-functions-core-tools)\n   - [Python 3.9 or greater](https://wiki.python.org/moin/BeginnersGuide/Download)\n   - [Git](https://git-scm.com/book/en/v2/Getting-Started-Installing-Git)\n   - Visual Studio Code extensions (optional, but expected in documentation)\n      - Azure Functions (`ms-azuretools.vscode-azurefunctions`)\n      - Azurite (`Azurite.azurite`)\n      - Azure Storage (`ms-azuretools.vscode-azurestorage`)\n      - Python (`ms-python.python`)\n      - REST Client (`humao.rest-client`)\n      - Semantic Kernel Tools (`ms-semantic-kernel.semantic-kernel`)\n1. Clone this repository to your local machine.\n\n#### (Option) GitHub Codespaces with devcontainers\n\nAlternatively, you can use GitHub Codeswpaces with devcontainers. This method allows you to use a pre-configured development environment and avoids the need to install anything on your local machine. You'll need to:\n\n1. Navigate to the repository on GitHub.\n1. Click on the 'Code' button and then select 'Open with Codespaces'.\n1. Click on the '+ New codespace' button.\n1. After a few minutes, your codespace should be ready, and you'll be taken to an online version of Visual Studio Code.\n\nThis codespace is a full-fledged development environment, and you can write and run your code just like you would on your local machine. The devcontainer configuration in the repository sets up all the necessary software and libraries for you.\n\n### 1. Setup your environment\n\n1. Copy the provided `local.settings.json.example` to a new `local.settings.json` file.\n1. If you have not already created your Azure resources, you'll need to create them now. Otherwise, you'll just need the information available from the deployments, keys, and endpoints.\n   - [Create and deploy an Azure OpenAI Service resource](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/create-resource?pivots=web-portal)\n   - [Create a Bing search resource](https://portal.azure.com/#create/microsoft.bingsearch)\n1. Correctly assign the following in your `local.settings.json` based on your Azure OpenAI Service and the chat completion model deployment you want to use: \n   - `AZURE_OPEN_AI__CHAT_COMPLETION_DEPLOYMENT_NAME`\n   - `AZURE_OPEN_AI__ENDPOINT`\n   - `AZURE_OPEN_AI__API_KEY`\n   - `AZURE_BING_SEARCH__API_KEY`\n1. Create a Python virtual environment (venv). In your Visual Studio Code terminal at the project root, execute the command:\n   ```sh\n   python -m venv .venv\n   ```\n\n### 2. Debug\n\n1. Start the Azurite services\n   - In Visual Studio Code, open the command palette (**F1**) and select **Azurite: Start** to start the Blob, Queue, and Table services.\n1. Debug the Azure Functions\n   - In Visual Studio Code, press **F5**. You should see a terminal window appear that will install any requirements (if not already installed), and run the Azure Functions Core Tools host. When ready, it should list the functions available:\n      ```sh\n      # EXAMPLE OUTPUT WHEN READY...\n      Azure Functions Core Tools\n      Core Tools Version:       4.0.5455 Commit hash: N/A  (64-bit)\n      Function Runtime Version: 4.27.5.21554\n      \n      [2023-12-20T15:31:14.052Z] Customer packages not in sys path. This should never happen! \n      [2023-12-20T15:31:17.231Z] Worker process started and initialized.\n      \n      Functions:\n      \n              http_trigger_sample:  http://localhost:7071/api/http_trigger_sample\n      \n              orchestrate_chat_completion:  http://localhost:7071/api/orchestrate_chat_completion\n      \n      For detailed output, run func with --verbose flag.\n      [2023-12-20T15:31:22.442Z] Host lock lease acquired by instance ID '00000000000000000000000068F265FA'.\n      ```\n   - Visual Studio Code should be attached to the process at this time, so you may place breakpoints where needed within your function code.\n1. Stop debugging\n   - You can disconnect the debugging process, or you can `<CTRL-C>` kill the *func host start* terminal window. That will ensure the underlying Functions Host is stopped, and the debugging process should also be released.\n   - Note that you may see errors at the time of the forced shutdown, but that is most likely due only to the shutdown.\n\n#### Troubleshooting\n\n##### Popup: Failed to verify \"AzureWebJobsStorage\" connection specified in \"local.settings.json\". Is the local emulator installed and running?\n\nYou may cancel this popup and address the issue with the following.\n\nIf you're debugging locally with the Azurite emulator, make sure your `local.settings.json` file's `AzureWebJobsStorage` is set to `UseDevelopmentStorage=true` and the Azurite service is started (**F1** then **Azurite: Start**).\n\nIf you're using a deployed Azure Storage Account, verify the connection string assigned to your `AzureWebJobsStorage` is correct and that the storage account is accessible from your local machine.\n\n##### Popup: Could not find the task 'func: host start'.\n\nYou may cancel this popup and address the issue with the following.\n\nYou may not have installed the Azure Functions extension in Visual Studio Code, or it may not be configured if running in the devcontainer or GitHub codespaces. Press **F1** then **Tasks: Run Task** then select **func: extensions install** or (if it is not visible) navigate to it by selecting the **func** directory then selecting **func: extensions install**.\n\n### 3. Evaluate\n\nOpen the `tests.http` file in Visual Studio Code. If the *REST Client* extension is installed as expected, there should be a **Send Request** link above the HTTP requests to trigger the functions. Click **Send Request** above the function you wish to trigger.\n\nThe terminal window will display any output from the Azure Function, and the HTTP response will be presented in a new window."
    },
    {
      "name": "rom212/semantic_skill_evaluation",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/54595299?s=40&v=4",
      "owner": "rom212",
      "repo_name": "semantic_skill_evaluation",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2023-12-12T00:26:10Z",
      "updated_at": "2024-01-02T20:59:06Z",
      "topics": [],
      "readme": "# Combining Semantic Kernel with Azure Machine Learning Prompt Flow\n\nThis repository is the sample code for the Tech Community blog [Combining Semantic Kernel with Azure Machine Learning Prompt Flow](https://techcommunity.microsoft.com/) (TODO: replace link once published)\n\n## Semantic Kernel Spam Classification Skill\n\nIn the `skills/ClassificationSkill/Spam` directory, use the `/skprompt.txt` file to build your skill (aka prompt used to define and steer the skill) and the `config.json` file to adjust the Azure OpenAI Service call parameters, including `temperature` and `top_p` parameters.\n\n## Prompt Flow Python Node Source Files\n\nYou will find the source code for components to the flow in the root folder. Refer to the blog post to see where/when to use each within Prompt Flow"
    },
    {
      "name": "SMC-Presales-Accelerators/azure-data-chat",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/156377865?s=40&v=4",
      "owner": "SMC-Presales-Accelerators",
      "repo_name": "azure-data-chat",
      "description": null,
      "homepage": null,
      "language": "TypeScript",
      "created_at": "2023-12-06T03:53:55Z",
      "updated_at": "2024-10-14T15:46:37Z",
      "topics": [],
      "readme": "# Azure Data Chat\r\n\r\n\r\n\r\n[![Deploy to Azure](https://aka.ms/deploytoazurebutton)](https://portal.azure.com/#create/Microsoft.Template/uri/https%3A%2F%2Fraw.githubusercontent.com%2FSMC-Presales-Accelerators%2Fazure-data-chat%2Fmain%2FDeployment%2Fmain.json) [![Visualize](https://raw.githubusercontent.com/Azure/azure-quickstart-templates/master/1-CONTRIBUTION-GUIDE/images/visualizebutton.svg?sanitize=true)](http://armviz.io/#/?load=https%3A%2F%2Fraw.githubusercontent.com%2FSMC-Presales-Accelerators%2Fazure-data-chat%2Fmain%2FDeployment%2Fmain.json)\r\n"
    },
    {
      "name": "minerva-ed/SustAInSim",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/150646217?s=40&v=4",
      "owner": "minerva-ed",
      "repo_name": "SustAInSim",
      "description": null,
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2023-12-02T14:43:00Z",
      "updated_at": "2023-12-02T21:06:52Z",
      "topics": [],
      "readme": "# GAIA: Diverse Stakeholder Representation through AI\n![logo](logo.png)\n\nSustAInSim is a pioneering platform designed to revolutionize discussions around campus sustainability projects. By leveraging sophisticated AI agents, it represents a broad spectrum of stakeholders, including students from various backgrounds, professors, administrators, and environmental entities. This tool aims to cultivate a more inclusive, informed, and sustainability-focused dialogue across campus.\n\n<h2>Key Features</h2>\n\n1. **AI-Powered Stakeholder Representation:** Simulates diverse campus stakeholders including students, faculty, and environmental factors using AI agents.\n\n2. **Interactive Sustainability Discussions:** Facilitates dynamic dialogues on sustainability issues, integrating real-time environmental data.\n\n3. **Scenario Exploration:** Users can create and explore various sustainability scenarios, assessing potential outcomes.\n\n4. **Summarizer Agent:** An AI summarizer provides concise overviews of discussions, highlighting key points and perspectives.\n\n5. **Decision-Making and Feedback:** Users input decisions on sustainability projects, which are then analyzed by the AI for sentiment feedback.\n\n6. **Sentiment Analysis:** The platform assesses AI stakeholder responses to user decisions, offering insights into potential community reactions.\n\n<h2>Requirements for running</h2>\n\nThe required packages needed to run SustAInSim are included in the ***requirements.txt*** file.\n\nThe semantic kernel package can be installed using the command `python3 -m pip install semantic-kernel`.\n\nOther packages can be installed using `pip3 install ...` command. \n"
    },
    {
      "name": "ajonathan/intro-to-intelligent-apps",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/15071719?s=40&v=4",
      "owner": "ajonathan",
      "repo_name": "intro-to-intelligent-apps",
      "description": "This repository introduces and helps organizations get started with building Intelligent Apps and incorporating Large Language Models (LLMs) via AI Orchestration into them.",
      "homepage": null,
      "language": null,
      "created_at": "2023-11-02T11:20:29Z",
      "updated_at": "2023-11-03T09:53:01Z",
      "topics": [],
      "readme": "# Introduction to Building AI Apps\n\nThis repository introduces and helps organizations get started with building AI Apps and incorporating Large Language Models (LLMs) into them.\n\n## Workshop Agenda\n\nThe objective of this workshop is to practice realistic AI orchestration scenarios towards learning how to build intelligent apps.\nAt the end of the workshop you will: \n* Know how to use prompt engineering techniques for effective generative AI responses on OpenAI\n* Understand the implications of the usage of tokens and embeddings when interacting with an LLM\n* Have experience in leveraging AI orchestrators like Langchain/ Semantic Kernel with Azure OpenAI\n* Have evaluated different Vectorstores like Qdrant/ Azure Cognitive Search to enhance LLM responses with your data and context\n* Know how to turn a business scenario with data, context and user input into an intelligent application on Azure\n\n### 🌅 Morning (9:00 – 12:15)\n\n> *Focus: Introduction, First Steps & Prompt Engineering*\n\n* 📣 Intro (30min)\n  * Introductions & Setting Expectations\n  * Use Case Ideation & Brainstorming\n* 📣 [Intro to Azure OpenAI, Prompt Engineering & Demos (105min)](presentations/README.md)\n  * Azure OpenAI Service\n  * Demo(s)\n  * Break\n  * 🧑🏼‍💻 [Lab #1 - Hands-on with Prompt Engineering Exercises](labs/01-prompts/README.md)\n* 📣 [Intro to AI Orchestration (60min)](presentations/README.md)\n  * AI Orchestration\n  * Demo(s)\n\n### 🌆 Afternoon (1:15 – 4:30)\n\n> *Focus: Building AI Apps & Incorporating LLMs*\n\n* 📣 [Intro to AI Orchestration Continued (135min)](presentations/README.md)\n  * 💻 [Lab #2 - Hands-on with Integrating AI Exercises](labs/02-integrating-ai/README.md)\n  * 💻 [Lab #3 - Hands-on with AI Orchestration Exercises](labs/03-orchestration/README.md)\n  * 💻 [Lab #4 - Hands-on with Deploying AI Exercises](labs/04-deploy-ai/README.md)\n  * Break\n* Wrapping-up (60min)\n  * Use Case Validation\n  * QnA & Closing Remarks\n\n\n## Getting Started with Workshop Preparation\n\nThe steps in this section will take you through setting up Azure OpenAI and some configuration files so that you can complete all of the hands-on labs successfully.\n\n* [Preparation](labs/00-setup/README.md)\n\n## Post Workshop Next Steps\n\nWhen you're done with this workshop and ready to move on, the following may be useful.\n\n* [Next Steps](docs/next_steps.md)\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n"
    },
    {
      "name": "oktay-be/ai-playground",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/55450535?s=40&v=4",
      "owner": "oktay-be",
      "repo_name": "ai-playground",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2023-11-26T10:12:48Z",
      "updated_at": "2023-11-26T22:39:18Z",
      "topics": [],
      "readme": "# ai-playground\n\nInstall packages: \npip install -r requirements.txt"
    },
    {
      "name": "kirenz/lab-langchain-functions",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/48259550?s=40&v=4",
      "owner": "kirenz",
      "repo_name": "lab-langchain-functions",
      "description": null,
      "homepage": null,
      "language": "HTML",
      "created_at": "2023-11-03T14:01:58Z",
      "updated_at": "2024-08-22T16:47:57Z",
      "topics": [],
      "readme": "# Welcome to our lab 👋\n\nYou can open the Jupyter Notebooks in Colab (on this [site](https://kirenz.github.io/lab-langchain-functions/slide.html)) or in GitHub Codespaces (see instructions below).\n\n## Create Codespace\n\n\n[![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://codespaces.new/kirenz/lab-langchain-functions?quickstart=1)\n\n1. Simply choose the *default* settings and create the Codespace.\n\n2. After the container is created, you will see the following text in the terminal (wait until the installation process is done):\n\n```bash\n\t\"postCreateCommand\": \"pip3 install --user -r requirements.txt\"\n```\n\n\n## Configure your Codespace\n\nAfter your Codespace is ready, you may want to: \n\n1. Click on *activate* if you see the pop-up *Thanks for installing vscode-icons* (in the lower right right corner of your screen)\n\n2. If needed, provide **API-keys** (e.g. OpenAi or Hugging Face) in the file `.env-template`. Then save your changes and rename the file to `.env`\n\n3. Open *Extensions*, select *Atom One Dark Theme* and click on *Set Color Theme* to change the user interface\n\n4. Open *Explorer* and open the folder 📂 *code*. Now you can select a Jupyter Notebook\n\n5. Click on the *Kernel* picker, choose *Python environments* and select the latest Python version \n\n6. Note that the changes you make during the container session are not saved (this is only done with commits to the GitHub repo)\n\n7. When you are done, you can click on the connection icon `><` in the lower left corner of the VS Code user interface and select *Stop Current Codespace* from the menu\n\n8. Visit the [GitHub Codespaces overview](https://github.com/codespaces) and make sure that you don't have active Codespaces\n"
    },
    {
      "name": "enwask/due-diligence",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/144351164?s=40&v=4",
      "owner": "enwask",
      "repo_name": "due-diligence",
      "description": "LLM-powered product comparison helper",
      "homepage": "https://devpost.com/software/due-diligence-lb6hvt",
      "language": "Python",
      "created_at": "2023-10-07T16:57:20Z",
      "updated_at": "2024-05-16T09:52:14Z",
      "topics": [],
      "readme": "# Due Diligence\n\nDue Diligence is your AI-powered shopping helper, originally developed in 36 hours for the Knight Hacks 2023 hackathon. The project was an honorable mention for Royal Bank of Canada's prompt, \"improving shopping experiences with generative AI\". It also won MLH's \"best use of GitHub\".\n\nThis web application allows you to search for a type of product and gives you a digestible comparison of resulting\nproducts and their specifications. You can also save products to your own personal list for later viewing, or share\nproduct lists and comparison with others.\n\nSponsored products and ads are filtered out on our end, giving you organic product results that you can trust. We search\nproduct data from the retailer of your choice, including Best Buy, Ebay, and Etsy (more coming soon). Product data is\ncompiled and run through our AI model to extract the most important information, separating concrete specifications\nfrom marketing fluff and miscellaneous noise. This allows you to make the most informed decision possible from our\ndetailed comparison results.\n"
    },
    {
      "name": "sethiaarun/semantic-kernel-example-ChatGPT-Plugin",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/109227248?s=40&v=4",
      "owner": "sethiaarun",
      "repo_name": "semantic-kernel-example-ChatGPT-Plugin",
      "description": "Microsoft Semantic Kernel - Example application creating ChatGPT plugin using Native and Semantic function with Python and Azure Function App, OpenAI Spec",
      "homepage": "",
      "language": "Python",
      "created_at": "2023-09-28T01:13:24Z",
      "updated_at": "2023-10-04T18:48:56Z",
      "topics": [],
      "readme": "# semantic-kernel-example-ChatGPT-Plugin\nMicrosoft Semantic Kernel - Example application creating ChatGPT plugin using Native and Semantic function with Python and Azure Function App. In this example code, we demonstrate how to combine native functions with semantic functions to correctly answer word problems like What is the \"square root of 634?\", \"square root of 144\", \"What is 42 plus 1513\" or \"multiply 2 times 4\", etc.\n\nIn this example, the code plugin is built with OpenAI standard specifications. The example uses a plugin manifest file that points to an accompanying [OpenAPI](https://www.openapis.org/) specification. Plugins defined in this way can be used by any application supporting the OpenAI specification, including Semantic Kernel and ChatGPT.\n\n## Microsoft Semantic Kernel\n\nThe [Microsoft Semantic kernel](https://learn.microsoft.com/en-us/semantic-kernel/ai-orchestration/kernel/?tabs=Csharp) is responsible for managing resources that are necessary to run \"code\" in an AI application. This includes managing the configuration, services, and plugins necessary for native code and AI services to run together.\n\nSemantic Kernel makes it easy to run AI services alongside **native code** by treating calls to AI services as their first-class citizens called \"semantic functions.\"\n\n## AI Plugins\n\n[AI Plugins](https://learn.microsoft.com/en-us/semantic-kernel/ai-orchestration/plugins/?tabs=Csharp) in Semantic Kernel are the fundamental building blocks of Semantic Kernel and can interoperate with plugins in ChatGPT, Bing, and Microsoft 365. With plugins, you can encapsulate capabilities into a single unit of functionality that the kernel can run. Plugins can consist of both **native code** and requests to AI services via **semantic functions**.\n\n### Out-of-the-box Plugin\n\nTo provide a degree of standardization across Semantic Kernel implementations, the GitHub repo has several plugins available out-of-the-box depending on the language you are using. These plugins are often referred to as [Core plugins](https://learn.microsoft.com/en-us/semantic-kernel/ai-orchestration/plugins/out-of-the-box-plugins?tabs=python#core-plugins). \n\nIn this example we are using `ConversationSummarySkill` to summarize a conversation with `GetIntent` semnatic function.\n\n### Semantic Functions\n\nSemantic Functions listen to users' asks and respond with a natural language response within your AI app. AI app operates very much like the human body; the Prompt represents \"Ear\", the Response as \"Mouth\", and the LLM model as \"brain\". Semantic Kernel uses connectors to connect the Prompt and the Response to the \"brain\". This allows you to easily swap out the AI services (\"brain\") without rewriting code.\n\nWe are going to use [Semantic Kernel prompt template language](https://learn.microsoft.com/en-us/semantic-kernel/prompt-engineering/prompt-template-syntax) to create natural language prompts, generate responses, extract information, etc.\n\n#### `GetIntent` \n\n[`GetIntent`](./plugins/OrchestratorPlugin/GetIntent/) Semantic function is defined to understand the intent of a user's input and then take some action based on that intent. The following [configurations](https://learn.microsoft.com/en-us/semantic-kernel/prompt-engineering/configure-prompts) and out-of-the-box plugins are added to the intent so that the LLM chooses the correct intent.\n\n- options - We want the LLM to choose the correct intent; for that, we are using the `options` configuration to choose the correct intent by providing it with more context and a constrained list of options.\n- history - A history variable is added to the prompt configuration to include the previous conversation.\n- [ConversationSummarySkill](https://learn.microsoft.com/en-us/azure/architecture/ai-ml/guide/conversation-summarization) Plugin - Summarize the conversation history before asking for the intent; this will avoid too many tokens being used.\n\n#### `GetNumber`\n\n[`GetNumber`](./plugins/OrchestratorPlugin/GetNumbers/) semnatic function is defined to pull the numbers from the user's input. This semantic function uses few-shot learning (user defined example conversation) to demonstrate to the LLM how to correctly extract the numbers from the user's request and output them in JSON format. This will allow us to easily pass the numbers to the Sqrt and Multiply functions.\n\n#### `CreateResponse`\n\n[`CreateResponse`](./plugins/OrchestratorPlugin/CreateResponse/) smenatic function si defined to create resonse for with original_request.\n\n### Native Functions\n\nWith Native Functions, you can have the Semantic kernel call C# or Python code directly so you can manipulate data or perform other operations, perform a task LLMs cannot do easily on their own. For example, you want to perform a task based on the intent that can be achieved using Semantic Functions. Now, if the user wants to send an email, you'll need to make the necessary API calls to send an email; this can be done using Native Functions.\n\n#### Math Functions (Math.py File)\n\nAll native functions are public methods of [Math.py](./plugins/MathPlugin/Math.py) class that represents your plugin. Using [SKFunction decorator](https://learn.microsoft.com/en-us/semantic-kernel/ai-orchestration/plugins/native-functions/using-the-skfunction-decorator?tabs=Csharp#use-the-skfunction-decorator-to-define-a-native-function) we inform Semantic Kernel that It is a native function and will automatically register it with the kernel when the plugin is loaded.\n\n### ExtractNumbersFromJson\n\nThe `ExtractNumbersFromJson` native function takes the JSON string from the input variable and extract the numbers from it into the context object. \n\n#### RouteRequest\n\n[RouteRequest](./plugins/OrchestratorPlugin/OrchestratorPlugin.py) is another Native function; It will leverage Semantic Functions (`GetIntent` and `GetNumber`) and call the appropriate Native functions (Math functions) based on the user's intent. Since the RouteRequest function helps orchestrate the flow, we have added the same to the `OrchestratorPlugin` plugin. The `route_request` function is decorated using the SKFunction decorator to inform Semantic Kernel that It is a native function. This plugin will run other functions, we'll need to pass the kernel to the plugin during initialization.\n\nUsing Semantic Kernel piping capabilities we orchestrates (semantic and native functions) pipelines with an `input` variable in the context object; It allows you to stream output from one semantic function to the next.\n\n   `get_numbers` -> `extract_numbers_from_json` -> `math_function` -> `create_response`\n\n## Kernel Util\n\nThe Kernel util loads the kernel with all the functions that are needed by the RouteRequest function. If we do not appropriately load the GetIntent, GetNumbers, Sqrt, and Multiply functions, If we do not appropriately load the GetIntent, GetNumbers, Sqrt, and Multiply functions, the RouteRequest function will fail when it tries to call them.\n\n## ChatGPT Plugin\n\nThe ChatGPT Plugin consists of three things: an app wrapped in an API, a manifest file, and an OpenAPI specification.\n\n![alt](./images/MathSkill.png)\n\n## Prerequisites\n\n- [Azure Functions Core Tools](https://www.npmjs.com/package/azure-functions-core-tools)\n- [VSCode](https://code.visualstudio.com/download)\n- Python >= 3.10\n- OpenAI API Key\n- Setup python venv\n\n## Implementations\n\nWe have two different ways to expose Semantic Kernel plugin:\n\n#### Using [Python Flask](https://flask.palletsprojects.com/en/3.0.x/) Framework with [Azure Functions](https://learn.microsoft.com/en-us/samples/azure-samples/flask-app-on-azure-functions/azure-functions-python-create-flask-app/)\n\nIt exposes following [endpoints](./SemanticApp/main.py):\n\n1. Plugin manifest endpoint - `/.well-known/ai-plugin.json`\n2. OpenAPI Specification endpoint - `/openapi.yaml`\n3. Function endpoint - `/skills/math`\n\n*[`./SemanticApp/openapi.yaml`] is created from `/swagger/spec` API exposed from [Python Tornado Sswirl](https://github.com/rnduldulaojr/tornado-swirl)\n\n#### Using [Python Tornado](https://www.tornadoweb.org/en/stable/) Web Server\n\nIt exposes following [endpoints](./tornadoapp/main.py); where OpenAPI specs are created using [Python Tornado Sswirl](https://github.com/rnduldulaojr/tornado-swirl):\n\n1. Plugin manifest endpoint - `/.well-known/ai-plugin.json`\n2. OpenAPI Specification endpoint - json format `/swagger/spec` and UI `/swagger/spec.html`  <br/>\n  <a href=\"./images/swagger.png\" target=\"_blank\"><img src=\"./images/swagger.png\" width=\"30%\" height=\"20%\"></a>\n  <br/>\n  <a href=\"./images/swagger_json.png\" target=\"_blank\"><img src=\"./images/swagger_json.png\" width=\"30%\" height=\"5%\"></a>\n3. Function endpoint - `/skills/math`\n\n## How to run?\n\n#### From Command Line or Powershell:\n\n- Activate Python venv\n- Append root directory of the project to PYTHONPATH\n- `pip install -r requirements.txt`\n- run `func host start` to start Azure function application on port 7070\n- run `python .\\tornadoapp\\main.py` to start Tornado webserver at port 8888\n\n#### From VSCOde:\n\nRun from VSCode, [follow steps](https://learn.microsoft.com/en-us/azure/azure-functions/functions-develop-vs-code?tabs=node-v3%2Cpython-v2%2Cisolated-process&pivots=programming-language-csharp#run-functions-locally)\n\nThe [launch.json](./.vscode/launch.json) has the configuration for \"Python: TornadoApp\" and \"Attach to Python Functions.\" Using \"Run & Debug,\" you can start Azure Function locally or Tornado Web Server.\n\nInvoke Math Skill:\n\n`curl --request POST http://localhost:<<port>>/skills/math -H \"Content-Type: application/json\" -d \"{\\\"prompt\\\":\\\"square root of 144?\\\"}\"`\n\n## Planner\n\nThe [Orchestrator](./plugins/OrchestratorPlugin/OrchestratorPlugin.py) orchestrated all of the functions on behalf of the user. This, however, is not a scalable solution because it would require the app developer to predict all possible requests that the user could make. So instead, we can use [planner](https://learn.microsoft.com/en-us/semantic-kernel/ai-orchestration/planners/?tabs=Csharp) to automatically orchestrate functions on the fly using a planner.\n\nYou can read more about the [planner](https://learn.microsoft.com/en-us/semantic-kernel/ai-orchestration/planners/?tabs=Csharp) and [when to use](https://learn.microsoft.com/en-us/semantic-kernel/ai-orchestration/planners/?tabs=Csharp#when-to-use-planner).\n\nThe [client application](./planner_client_app.py) has an example code using `SequentialPlanner`, in this example, we are importing the plugin using locally (:grey_question: not remotely by importing plugin from manifest URL,I did not find API for Python similar to C# `kernel.ImportChatGptPluginSkillFromUrlAsync`)\n\nIf you want to know how planner is working along with OpenAI services, I would suggest enable debugging from `/.vscode/launch.json` by setting `\"justMyCode\": false`. You will find the execution flow of this [client application](https://github.com/sethiaarun/semantic-kernel-example-ChatGPT-Plugin/blob/main/planner_client_app.py) is as following:\n\n1. The `create_plan_async` loads all relevant functions manual (provided by Kernel object), in this case we are providing `OrchestratorPlugin` and `MathPlugin`.\n2. It sets them in the context (available_functions)\n3. The [`sk_function`] (https://github.com/microsoft/semantic-kernel/blob/main/python/semantic_kernel/orchestration/sk_function.py) can generate render prompt from prompt_template_engine with the help of context variables and prompt_template (in this case it uses https://github.com/microsoft/semantic-kernel/blob/main/python/semantic_kernel/planning/sequential_planner/Skills/SequentialPlanning/skprompt.txt). \n4. Once we have rendered prompt, it uses the  OpenAIChatCompletion connector from openai service (https://github.com/microsoft/semantic-kernel/blob/main/python/semantic_kernel/connectors/ai/open_ai/services/open_ai_chat_completion.py) for chat request (prompt_to_message = [(\"user\", prompt)]), the response from OpenAI is a result plan. Semantic Kernel executes the resulting plan.\n\nManually I ran the same from the OpenAI PlayGround:\n\n<a href=\"./images/rendered_plan_result_plan.png\" target=\"_blank\"><img src=\"./images/rendered_plan_result_plan.png\" width=\"30%\" height=\"5%\"></a> <br/>\n\n**You can run this application ** from VSCode using \"Run & Debug\" or from the terminal (`pip install requirements.txt` and Append the root directory of the project to PYTHONPATH).\n\n## Deploy\n\nYou can deploy Semantic Kernel to Azure using Azure functions by following steps [listed here.](https://devblogs.microsoft.com/semantic-kernel/how-to-deploy-semantic-kernel-to-azure-in-minutes/)\n\n"
    },
    {
      "name": "nicholassolomon/DeepLearning.AI.SemanticKernel.Course",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/48126957?s=40&v=4",
      "owner": "nicholassolomon",
      "repo_name": "DeepLearning.AI.SemanticKernel.Course",
      "description": "Notebooks and files re-creating DL.AI Semantic Kernel presentation using all HuggingFace assets.",
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2023-09-17T20:13:40Z",
      "updated_at": "2024-06-23T16:21:03Z",
      "topics": [],
      "readme": "# DeepLearning.AI.SemanticKernel.Course\nNotebooks and files re-creating DL.AI Semantic Kernel presentation using all HuggingFace assets, presented by John .\n\n## How Business Thinkers Can Start Building AI Plugins With Semantic Kernel\n\n- https://learn.deeplearning.ai/microsoft-semantic-kernel/lesson/1/introduction\n\n## Repos\n\n - https://github.com/microsoft/semantic-kernel\n - https://github.com/microsoft/semantic-kernel-starters\n\n## Also\n - https://github.com/microsoft/semantic-kernel/blob/main/python/README.md\n - https://github.com/microsoft/semantic-kernel/blob/main/python/notebooks/00-getting-started.ipynb\n"
    },
    {
      "name": "raffertyuy/RazGPT",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/1037626?s=40&v=4",
      "owner": "raffertyuy",
      "repo_name": "RazGPT",
      "description": "This repository contains various GPT-related prototypes.",
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2023-04-22T14:16:16Z",
      "updated_at": "2025-04-13T07:20:10Z",
      "topics": [],
      "readme": "## Background\nThis repo contains a collection of applications as I learn about developing OpenAI-infused applications.\nAs most of the apps here are inspired from sample apps from other repos, please also see the `README.md` in the respective folders.\n\nIf you'd like to learn more, please check [my series of blog posts](https://www.raffertyuy.com/raztype/building-openai-infused-apps/), and other posts in the [same blog](https://www.raffertyuy.com).\n\n\n## References\n### API and SDK References (alphabetical order)\n- [AutoGen](https://microsoft.github.io/autogen)\n- [LangChain (GitHub)](https://github.com/hwchase17/langchain)\n- [LangChain (Python)](https://python.langchain.com/docs/get_started/introduction)\n- [LangChain (JS)](https://js.langchain.com/docs/get_started/introduction)\n- [LiteLLM](https://litellm.ai)\n- [Llama Index](https://github.com/run-llama/llama_index)\n- [OpenAI API Reference](https://platform.openai.com/docs/api-reference/introduction)\n- [OpenAI SDK Reference](https://platform.openai.com/docs/libraries/python-library)\n- [Semantic Kernel](https://github.com/microsoft/semantic-kernel)\n\n\n### Code Reference Compilation (alphabetical order)\n1. [Azure Chat](https://github.com/microsoft/azurechat): ChatGPT that allows uploading of a single document.\n2. [Azure OpenAI + Search (Demo)](https://github.com/Azure-Samples/azure-search-openai-demo): this is the very first demo app created by Microsoft\n3. [Azure OpenAI + Search (JavaScript)](https://github.com/Azure-Samples/azure-search-openai-javascript): haven't tried, but it appears to be a cleaner version of #2.\n4. [Azure OpenAI on Your Data Web Application](https://github.com/microsoft/sample-app-aoai-chatGPT): This is the code that is deployed by [Azure OpenAI on your Data](https://learn.microsoft.com/en-us/azure/ai-services/openai/use-your-data-quickstart?tabs=command-line%2Cpython&pivots=programming-language-studio)\n5. [Chat with Your Data (Solution Accelerator)](https://github.com/Azure-Samples/chat-with-your-data-solution-accelerator): similar to #1, but has a UI/UX for the admin page.\n6. [Chatbot UI](https://github.com/mckaywrigley/chatbot-ui): A basic ChatGPT UI implementation. This is not by Microsoft but I really like this one.\n7. [Enterprise RAG](https://github.com/Azure/GPT-RAG/): This solution accelerator uses private endpoints and private links, for enterprises with network hardening requirements.\n8. [LibreChat](https://docs.librechat.ai/): Open-source clone of ChatGPT. Looks promising, but I haven't tried it yet.\n9. [PubSec Info Assistant](https://github.com/microsoft/PubSec-Info-Assistant)\n10. [Semantic Kernel Chat Copilot](https://github.com/microsoft/chat-copilot): The official Semantic Kernel ChatGPT application. Read more [here](https://learn.microsoft.com/en-us/semantic-kernel/chat-copilot/).\n\nI wrote about some of the above in this [blog post](https://www.raffertyuy.com/raztype/azure-openai-starters/)."
    },
    {
      "name": "amitpuri/LLM-Text-Completion-Semantic-Kernel",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/6460233?s=40&v=4",
      "owner": "amitpuri",
      "repo_name": "LLM-Text-Completion-Semantic-Kernel",
      "description": "LLM Text Completion using Semantic-Kernel",
      "homepage": "",
      "language": "Python",
      "created_at": "2023-08-10T13:06:15Z",
      "updated_at": "2024-07-09T23:39:56Z",
      "topics": [
        "openai",
        "python",
        "semantic-kernel",
        "text-completion"
      ],
      "readme": "# LLM-Text-Completion using Semantic-Kernel\n\nThe `sk-python-Text-Completion` console application demonstrates how to execute a semantic function.\n\n## Prerequisites\n\n- [Python](https://www.python.org/downloads/) 3.8 and above\n  - [Poetry](https://python-poetry.org/) is used for packaging and dependency management\n  - [Semantic Kernel Tools](https://marketplace.visualstudio.com/items?itemName=ms-semantic-kernel.semantic-kernel)\n\n## Configuring the starter\n\nThe starter can be configured with a `.env` file in the project which holds api keys and other secrets and configurations.\n\nMake sure you have an\n[Open AI API Key](https://openai.com/api/) or\n[Azure Open AI service key](https://learn.microsoft.com/azure/cognitive-services/openai/quickstart?pivots=rest-api)\n\nCopy the `.env.example` file to a new file named `.env`. Then, copy those keys into the `.env` file:\n\n```\nOPENAI_API_KEY=\"\"\nOPENAI_ORG_ID=\"\"\nAZURE_OPENAI_DEPLOYMENT_NAME=\"\"\nAZURE_OPENAI_ENDPOINT=\"\"\nAZURE_OPENAI_API_KEY=\"\"\n```\n\n## Running the starter\n\nTo run the console application within Visual Studio Code, just hit `F5`.\nAs configured in `launch.json` and `tasks.json`, Visual Studio Code will run `poetry install` followed by `python text_completion/main.py`\n\nTo build and run the console application from the terminal use the following commands:\n\n```powershell\npoetry install\npoetry run python text_completion/main.py\n```\n\n# Sample output\n\nTARGET AUDIENCE:\nAI enthusiasts, developers, and researchers.\n\n+++++\n\nPURPOSE:\nThe purpose of this article is to explain the concept of Generative AI to the target audience. The article will provide a detailed explanation of Generative AI, its applications, and how it works. The article will also provide examples of Generative AI in real-world scenarios.\n\n+++++\n\nLENGTH:\n1000 words\n\nGenerative AI: A Comprehensive Guide\n\nArtificial Intelligence (AI) has been a buzzword for the past few years, and it has been transforming the way we live and work. AI has been used in various fields, including healthcare, finance, and entertainment. One of the most exciting areas of AI is Generative AI. In this article, we will explore Generative AI, its applications, and how it works.\n\nWhat is Generative AI?\n\nGenerative AI is a type of AI that can create new data that is similar to the data it has been trained on. It is a subset of machine learning that uses deep neural networks to generate new data. Generative AI can be used to create images, videos, music, and even text. It is a powerful tool that can be used in various fields, including art, design, and entertainment.\n\nHow does Generative AI work?\n\nGenerative AI works by using deep neural networks to learn the patterns in the data it has been trained on. The neural network consists of layers of nodes that are connected to each other. Each node in the network performs a simple mathematical operation on the input data and passes the result to the next layer. The output of the last layer is the generated data.\n\nThe neural network is trained on a dataset that contains examples of the data that it needs to generate. For example, if we want to generate images of cats, we would train the neural network on a dataset of cat images. During training, the neural network learns the patterns in the data and uses them to generate new data that is similar to the training data.\n\nApplications of Generative AI\n\nGenerative AI has many applications in various fields. Some of the most exciting applications of Generative AI are:\n\n1. Art and Design: Generative AI can be used to create art and design. It can generate images, videos, and music that are unique and creative.\n\n2. Gaming: Generative AI can be used to create game content, such as levels, characters, and items.\n\n3. Healthcare: Generative AI can be used to generate synthetic data that can be used to train medical models. This can help in the development of new drugs and treatments.\n\n4. Finance: Generative AI can be used to generate synthetic financial data that can be used to train financial models. This can help in predicting stock prices and other financial indicators.\n\nExamples of Generative AI\n\n1. StyleGAN: StyleGAN is a Generative AI model that can generate high-quality images of faces. It was developed by NVIDIA and has been used in various applications, including art and design.\n\n2. GPT-3: GPT-3 is a Generative AI model that can generate human-like text. It was developed by OpenAI and has been used in various applications, including chatbots and language translation.\n\n3. DeepDream: DeepDream is a Generative AI model that can generate images that are surreal and dream-like. It was developed by Google and has been used in various applications, including art and design.\n\nConclusion\n\nGenerative AI is a powerful tool that can be used in various fields, including art, design, and entertainment. It works by using deep neural networks to learn the patterns in the data it has been trained on and generate new data that is similar to the training data. Generative AI has many applications, including art and design, gaming, healthcare, and finance. With the development of new Generative AI models, we can expect to see more exciting applications of Generative AI in the future.\n\nReferences:\n\n1. https://www.nvidia.com/en-us/research/ai-playground\n2. https://openai.com/blog/gpt-3-apps\n3. https://ai.googleblog.com/2015/06/inceptionism-going-deeper-into-neural.html\n\nEND OF OUTPUT\n----------\n\nFurther, simplify using LangChain \n- [Text Completion via LangChain](https://github.com/amitpuri/LLM-Text-Completion-langchain)\n\n- [Text Completion via OpenAI Python](https://github.com/amitpuri/LLM-Text-Completion)\n- [Semantic Kernel Starters](https://github.com/microsoft/semantic-kernel-starters)\n    -  for ChatGPT Plugin, check Semantic Kernel Python Flask Starter\n \nMore comprehensive demos are available on [LLM Scenarios, Use cases on the Gradio app](https://github.com/amitpuri/ask-picturize-it)\n\n"
    },
    {
      "name": "JustinMeimar/hack-gpt-dev",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/32406396?s=40&v=4",
      "owner": "JustinMeimar",
      "repo_name": "hack-gpt-dev",
      "description": "development repo for the hack-gpt event",
      "homepage": null,
      "language": "Python",
      "created_at": "2023-07-20T14:40:21Z",
      "updated_at": "2023-07-21T01:42:47Z",
      "topics": [],
      "readme": "## Hack GPT Dev Repo\n\n### App Structure\n```\n.\n├── app\n│   ├── api             // Middleware\n│   ├── backend         // Python backend (Where we run models) \n│   ├── client          // React Frontend (Where we hanle modle I/O)\n│   ├── manage.py       // Script to run server\n│   ├── media           // Images \n│   └── static          // Static Files like .html and .css\n├── env                 // Virtual Environment\n├── README.md           // This file\n└── requirements.txt    // Python dependencies\n```\n\n### Installation\n\nClone repo\n`git clone git@github.com:JustinMeimar/hack-gpt-dev.git`\n\nNavigate into repo\n`cd hack-gpt-dev`\n\nMake a virtual environment\n`python -m venv env` \n\nInstall dependencies\n`pip install -r requirements.txt`\n\nInstall the frontend\n`npm install`\n\nBuild the frontend\n`cd /app/client`\n`npm build`\n\nRun the server locally\n`cd app && python manage.py runserver`\n\n### OpenAI Authentication\nCreate the file `app/api/llm/.env` and add the OpenAI Credentials in the form\n```\nOPENAI_API_KEY = <\"OPEN_AI_API_KEY\">\nOPENAI_ORG_ID = \"<\"OPENAI_ORG_ID\">\n```  \n\n### Making Changes\n\nThe React frontend is served by the file `app/client/build/index.html`, so to make changes to the frontend run `npm run build` from `app/client`"
    },
    {
      "name": "philmui/asdrp2023",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/78625?s=40&v=4",
      "owner": "philmui",
      "repo_name": "asdrp2023",
      "description": null,
      "homepage": null,
      "language": "Jupyter Notebook",
      "created_at": "2023-06-23T01:30:09Z",
      "updated_at": "2024-01-28T21:40:06Z",
      "topics": [],
      "readme": "\n# ASDRP 2023\n\nRepo with lecture, demo, and experiemental notebooks and apps.\n"
    },
    {
      "name": "placerda/gpt-oyd-ingestion",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/6265211?s=40&v=4",
      "owner": "placerda",
      "repo_name": "gpt-oyd-ingestion",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2023-06-26T19:35:22Z",
      "updated_at": "2024-08-14T06:16:40Z",
      "topics": [],
      "readme": "# gpt on your data ingestion\n\n## Pre-reqs\n\n- Cognitive Search Service\n- Form Recognizer Service\n- Azure Storage Account\n- Python and PIP 3\n- Input data in pdf or png format\n\n## Quick start\n\n**1) Copy input data to a folder**\n\nCreate a data folder in the project root and add input PDFs or PNGs to it.\n\n```data/```\n\n**2) Configure environment variables**\n\nRename [.env.template](.env.template) to ```.env``` and fill values accordingly to your environment.\n\nTo use vector search (```VECTOR_INDEX=\"True\"```) your service needs to be this feature activated.\n\n**3) Install Libraries**\n\n```pip3 install -r ./requirements.txt```\n\nTo use vector search you need to connect to [Azure SDK Python Dev Feed](https://dev.azure.com/azure-sdk/public/_artifacts/feed/azure-sdk-for-python/connect/pip) and run:\n\n```pip3 install -r ./requirements.dev.txt```\n\n\n**4) Execute ingestion script** \n\nIn a terminal (bash) execute the following line\n\n```./data_ingestion.sh```\n\n## References\n\nAzure Cognitive Search [Vector Index](https://github.com/Azure/cognitive-search-vector-pr/)"
    },
    {
      "name": "showpune/windup-prompt",
      "stars": 1,
      "img": "https://avatars.githubusercontent.com/u/1787505?s=40&v=4",
      "owner": "showpune",
      "repo_name": "windup-prompt",
      "description": null,
      "homepage": null,
      "language": "Python",
      "created_at": "2023-05-08T02:58:04Z",
      "updated_at": "2023-05-10T22:27:32Z",
      "topics": [],
      "readme": "# windup-prompt: generate the windup rules by open AI\n## Introduce\nThree way to use OpenAI to generate windup rules\n1) By chat with system prompt\n2) A sequence of asks to get more context, make the result more precise\n3) Use sematical kernal to make the ask plan to generate the rule\n\n## Installation\n1. Configration the connection to open AI\n```\ncp .env-sample .env\n```\nConfig the connection string to azure open ai service, to create the azure open ai service and get the configuration, check [link](https://github.com/showpune/windup-prompt/blob/master/how-to.md)) \n\nInstall the requirements\n```\npython -m pip install -r requirements.txt\n```\n\n## Simple: Generate the rules directly\nRun the code \n```\npython simple-main.py\n```\nUse the [system prompt](https://github.com/showpune/windup-prompt/blob/master/prompt/system.txt) and [user prompt](https://github.com/showpune/windup-prompt/blob/master/prompt/chat/ask.txt) to generate the windup rules, input any platform you want to migration:\n![image](https://user-images.githubusercontent.com/1787505/236997115-126f8b89-ff2a-4410-90bf-c808ddedf41d.png)\n\n\n## Chat Sequence\n* Since the Open AI can't get all the context in one ask to generate the rules, you can use a sequence chat to get more information, like the service partern, API pattern and special concept, and then use the information as assist prompt to generate the rule\n* You can find the sequence of chat from [Chat Sequence](https://github.com/showpune/windup-prompt/tree/master/prompt/windup).\n* The chat will asked by the sequence of the folder name, we need the sequence since some ask is dependent. Like we have the special concept of a platform, then we can ask the API pattern\nTest the sequence ask by \n```\npython pipe-main.py\n```\nThe result looks like\n\n![image](https://user-images.githubusercontent.com/1787505/236998182-bef8aabd-8c6e-4215-9c8d-66623d086460.png)\n![image](https://user-images.githubusercontent.com/1787505/236998273-61187753-8f07-4ab3-add2-2f268f42753f.png)\n![image](https://user-images.githubusercontent.com/1787505/236998302-db6b8583-93d5-4e1c-882c-e2cadb06d1bf.png)\n\n## Semantical Kernal: Generate the rule by auto-gpt\nThere are more ask\n1) Should I care about the sequence in [Chat Sequence](https://github.com/showpune/windup-prompt/tree/master/prompt/windup)?\n2) Is there any more skills shared by others I can use to generate the windup rules?\n3) Is the skills I defined make sense?\nLet sematical kernal to handle it: It will read all the skills from  [Chat Sequence](https://github.com/showpune/windup-prompt/tree/master/prompt/windup), merge with system skills and make a plan to generate the windup rules\n```\npython pipe-main.py\n```\n![image](https://user-images.githubusercontent.com/1787505/236999884-8e9d14cc-2a97-4bec-ab98-e9913553c312.png)\n"
    }
  ],
  "total_dependents_number": 308,
  "public_dependents_number": 308,
  "private_dependents_number": -308,
  "public_dependents_stars": 623,
  "badges": {
    "total_doc_url": "[![Generated by github-dependents-info](https://img.shields.io/static/v1?label=Used%20by&message=308&color=informational&logo=slickpic)](https://github.com/nvuillam/github-dependents-info)",
    "total": "[![Generated by github-dependents-info](https://img.shields.io/static/v1?label=Used%20by&message=308&color=informational&logo=slickpic)](https://github.com/microsoft/semantic-kernel/network/dependents)",
    "public": "[![Generated by github-dependents-info](https://img.shields.io/static/v1?label=Used%20by%20(public)&message=308&color=informational&logo=slickpic)](https://github.com/microsoft/semantic-kernel/network/dependents)",
    "private": "[![Generated by github-dependents-info](https://img.shields.io/static/v1?label=Used%20by%20(private)&message=-308&color=informational&logo=slickpic)](https://github.com/microsoft/semantic-kernel/network/dependents)",
    "stars": "[![Generated by github-dependents-info](https://img.shields.io/static/v1?label=Used%20by%20(stars)&message=623&color=informational&logo=slickpic)](https://github.com/microsoft/semantic-kernel/network/dependents)"
  },
  "packages": [
    [
      {
        "id": "UGFja2FnZS0zNjMxNjM5MDk1",
        "name": "semantic-kernel",
        "url": "https://github.com/microsoft/semantic-kernel/network/dependents?package_id=UGFja2FnZS0zNjMxNjM5MDk1",
        "public_dependent_stars": 623,
        "public_dependents": [
          {
            "name": "geekan/MetaGPT",
            "stars": 54895,
            "img": "https://avatars.githubusercontent.com/u/2707039?s=40&v=4",
            "owner": "geekan",
            "repo_name": "MetaGPT"
          },
          {
            "name": "microsoft/semantic-kernel",
            "stars": 24146,
            "img": "https://avatars.githubusercontent.com/u/6154722?s=40&v=4",
            "owner": "microsoft",
            "repo_name": "semantic-kernel"
          },
          {
            "name": "microsoft/ai-agents-for-beginners",
            "stars": 16627,
            "img": "https://avatars.githubusercontent.com/u/6154722?s=40&v=4",
            "owner": "microsoft",
            "repo_name": "ai-agents-for-beginners"
          },
          {
            "name": "google/A2A",
            "stars": 12967,
            "img": "https://avatars.githubusercontent.com/u/1342004?s=40&v=4",
            "owner": "google",
            "repo_name": "A2A"
          },
          {
            "name": "Chainlit/chainlit",
            "stars": 9384,
            "img": "https://avatars.githubusercontent.com/u/128686189?s=40&v=4",
            "owner": "Chainlit",
            "repo_name": "chainlit"
          },
          {
            "name": "Azure/PyRIT",
            "stars": 2425,
            "img": "https://avatars.githubusercontent.com/u/6844498?s=40&v=4",
            "owner": "Azure",
            "repo_name": "PyRIT"
          },
          {
            "name": "Azure-Samples/chat-with-your-data-solution-accelerator",
            "stars": 994,
            "img": "https://avatars.githubusercontent.com/u/1844662?s=40&v=4",
            "owner": "Azure-Samples",
            "repo_name": "chat-with-your-data-solution-accelerator"
          },
          {
            "name": "akshata29/entaoai",
            "stars": 859,
            "img": "https://avatars.githubusercontent.com/u/18509807?s=40&v=4",
            "owner": "akshata29",
            "repo_name": "entaoai"
          },
          {
            "name": "Azure-Samples/miyagi",
            "stars": 741,
            "img": "https://avatars.githubusercontent.com/u/1844662?s=40&v=4",
            "owner": "Azure-Samples",
            "repo_name": "miyagi"
          },
          {
            "name": "NVIDIA/AgentIQ",
            "stars": 717,
            "img": "https://avatars.githubusercontent.com/u/1728152?s=40&v=4",
            "owner": "NVIDIA",
            "repo_name": "AgentIQ"
          },
          {
            "name": "Azure-Samples/AI-Gateway",
            "stars": 551,
            "img": "https://avatars.githubusercontent.com/u/1844662?s=40&v=4",
            "owner": "Azure-Samples",
            "repo_name": "AI-Gateway"
          },
          {
            "name": "Azure/co-op-translator",
            "stars": 413,
            "img": "https://avatars.githubusercontent.com/u/6844498?s=40&v=4",
            "owner": "Azure",
            "repo_name": "co-op-translator"
          },
          {
            "name": "microsoft/semantic-kernel-starters",
            "stars": 370,
            "img": "https://avatars.githubusercontent.com/u/6154722?s=40&v=4",
            "owner": "microsoft",
            "repo_name": "semantic-kernel-starters"
          },
          {
            "name": "Azure/aistudio-copilot-sample",
            "stars": 314,
            "img": "https://avatars.githubusercontent.com/u/6844498?s=40&v=4",
            "owner": "Azure",
            "repo_name": "aistudio-copilot-sample"
          },
          {
            "name": "microsoft/Conversation-Knowledge-Mining-Solution-Accelerator",
            "stars": 271,
            "img": "https://avatars.githubusercontent.com/u/6154722?s=40&v=4",
            "owner": "microsoft",
            "repo_name": "Conversation-Knowledge-Mining-Solution-Accelerator"
          },
          {
            "name": "microsoft/semanticworkbench",
            "stars": 259,
            "img": "https://avatars.githubusercontent.com/u/6154722?s=40&v=4",
            "owner": "microsoft",
            "repo_name": "semanticworkbench"
          },
          {
            "name": "sambanova/ai-starter-kit",
            "stars": 220,
            "img": "https://avatars.githubusercontent.com/u/80118584?s=40&v=4",
            "owner": "sambanova",
            "repo_name": "ai-starter-kit"
          },
          {
            "name": "Azure-Samples/aks-store-demo",
            "stars": 215,
            "img": "https://avatars.githubusercontent.com/u/1844662?s=40&v=4",
            "owner": "Azure-Samples",
            "repo_name": "aks-store-demo"
          },
          {
            "name": "Azure/intro-to-intelligent-apps",
            "stars": 199,
            "img": "https://avatars.githubusercontent.com/u/6844498?s=40&v=4",
            "owner": "Azure",
            "repo_name": "intro-to-intelligent-apps"
          },
          {
            "name": "happyapplehorse/gptui",
            "stars": 142,
            "img": "https://avatars.githubusercontent.com/u/80760121?s=40&v=4",
            "owner": "happyapplehorse",
            "repo_name": "gptui"
          },
          {
            "name": "chtrembl/azure-cloud",
            "stars": 134,
            "img": "https://avatars.githubusercontent.com/u/62187826?s=40&v=4",
            "owner": "chtrembl",
            "repo_name": "azure-cloud"
          },
          {
            "name": "alibaba/app-controller",
            "stars": 132,
            "img": "https://avatars.githubusercontent.com/u/1961952?s=40&v=4",
            "owner": "alibaba",
            "repo_name": "app-controller"
          },
          {
            "name": "Azure-Samples/python-ai-agent-frameworks-demos",
            "stars": 125,
            "img": "https://avatars.githubusercontent.com/u/1844662?s=40&v=4",
            "owner": "Azure-Samples",
            "repo_name": "python-ai-agent-frameworks-demos"
          },
          {
            "name": "neo4j-product-examples/graphrag-examples",
            "stars": 125,
            "img": "https://avatars.githubusercontent.com/u/100449908?s=40&v=4",
            "owner": "neo4j-product-examples",
            "repo_name": "graphrag-examples"
          },
          {
            "name": "Josephrp/DataTonic",
            "stars": 87,
            "img": "https://avatars.githubusercontent.com/u/18212928?s=40&v=4",
            "owner": "Josephrp",
            "repo_name": "DataTonic"
          },
          {
            "name": "smile-wingbow/pudding-robot",
            "stars": 86,
            "img": "https://avatars.githubusercontent.com/u/174078102?s=40&v=4",
            "owner": "smile-wingbow",
            "repo_name": "pudding-robot"
          },
          {
            "name": "cxbxmxcx/GPT-Agents",
            "stars": 80,
            "img": "https://avatars.githubusercontent.com/u/10665060?s=40&v=4",
            "owner": "cxbxmxcx",
            "repo_name": "GPT-Agents"
          },
          {
            "name": "Azure/azureml-assets",
            "stars": 67,
            "img": "https://avatars.githubusercontent.com/u/6844498?s=40&v=4",
            "owner": "Azure",
            "repo_name": "azureml-assets"
          },
          {
            "name": "microsoft/EveryoneCanCode-US",
            "stars": 67,
            "img": "https://avatars.githubusercontent.com/u/6154722?s=40&v=4",
            "owner": "microsoft",
            "repo_name": "EveryoneCanCode-US"
          },
          {
            "name": "microsoft/ai-developer",
            "stars": 65,
            "img": "https://avatars.githubusercontent.com/u/6154722?s=40&v=4",
            "owner": "microsoft",
            "repo_name": "ai-developer"
          },
          {
            "name": "AzureCosmosDB/CosmosAIGraph",
            "stars": 62,
            "img": "https://avatars.githubusercontent.com/u/33746873?s=40&v=4",
            "owner": "AzureCosmosDB",
            "repo_name": "CosmosAIGraph"
          },
          {
            "name": "docqai/docq",
            "stars": 62,
            "img": "https://avatars.githubusercontent.com/u/132401033?s=40&v=4",
            "owner": "docqai",
            "repo_name": "docq"
          },
          {
            "name": "Azure-Samples/gen-ai-bot-in-a-box",
            "stars": 61,
            "img": "https://avatars.githubusercontent.com/u/1844662?s=40&v=4",
            "owner": "Azure-Samples",
            "repo_name": "gen-ai-bot-in-a-box"
          },
          {
            "name": "microsoft/promptflow-resource-hub",
            "stars": 58,
            "img": "https://avatars.githubusercontent.com/u/6154722?s=40&v=4",
            "owner": "microsoft",
            "repo_name": "promptflow-resource-hub"
          },
          {
            "name": "Azure/gpt-rag-orchestrator",
            "stars": 57,
            "img": "https://avatars.githubusercontent.com/u/6844498?s=40&v=4",
            "owner": "Azure",
            "repo_name": "gpt-rag-orchestrator"
          },
          {
            "name": "neo4j-product-examples/graphrag-contract-review",
            "stars": 56,
            "img": "https://avatars.githubusercontent.com/u/100449908?s=40&v=4",
            "owner": "neo4j-product-examples",
            "repo_name": "graphrag-contract-review"
          },
          {
            "name": "Azure-Samples/aistudio-python-langchain-sample",
            "stars": 54,
            "img": "https://avatars.githubusercontent.com/u/1844662?s=40&v=4",
            "owner": "Azure-Samples",
            "repo_name": "aistudio-python-langchain-sample"
          },
          {
            "name": "Azure/azure-ai-cli",
            "stars": 54,
            "img": "https://avatars.githubusercontent.com/u/6844498?s=40&v=4",
            "owner": "Azure",
            "repo_name": "azure-ai-cli"
          },
          {
            "name": "Azure-Samples/moneta-agents",
            "stars": 51,
            "img": "https://avatars.githubusercontent.com/u/1844662?s=40&v=4",
            "owner": "Azure-Samples",
            "repo_name": "moneta-agents"
          },
          {
            "name": "Josephrp/scitonic",
            "stars": 51,
            "img": "https://avatars.githubusercontent.com/u/18212928?s=40&v=4",
            "owner": "Josephrp",
            "repo_name": "scitonic"
          },
          {
            "name": "gnosis/prediction-market-agent",
            "stars": 50,
            "img": "https://avatars.githubusercontent.com/u/24954468?s=40&v=4",
            "owner": "gnosis",
            "repo_name": "prediction-market-agent"
          },
          {
            "name": "Azure-Samples/semantic-kernel-workshop",
            "stars": 49,
            "img": "https://avatars.githubusercontent.com/u/1844662?s=40&v=4",
            "owner": "Azure-Samples",
            "repo_name": "semantic-kernel-workshop"
          },
          {
            "name": "sail-sg/FlowReasoner",
            "stars": 44,
            "img": "https://avatars.githubusercontent.com/u/85740051?s=40&v=4",
            "owner": "sail-sg",
            "repo_name": "FlowReasoner"
          },
          {
            "name": "SageMindsAI/Agience",
            "stars": 40,
            "img": "https://avatars.githubusercontent.com/u/196980564?s=40&v=4",
            "owner": "SageMindsAI",
            "repo_name": "Agience"
          },
          {
            "name": "ucl-docaider/docAider",
            "stars": 38,
            "img": "https://avatars.githubusercontent.com/u/172435741?s=40&v=4",
            "owner": "ucl-docaider",
            "repo_name": "docAider"
          },
          {
            "name": "farzad528/mcp-server-azure-ai-agents",
            "stars": 36,
            "img": "https://avatars.githubusercontent.com/u/40604067?s=40&v=4",
            "owner": "farzad528",
            "repo_name": "mcp-server-azure-ai-agents"
          },
          {
            "name": "john0isaac/rag-semantic-kernel-mongodb-vcore",
            "stars": 33,
            "img": "https://avatars.githubusercontent.com/u/64026625?s=40&v=4",
            "owner": "john0isaac",
            "repo_name": "rag-semantic-kernel-mongodb-vcore"
          },
          {
            "name": "Azure-Samples/semantic-kernel-advanced-usage",
            "stars": 31,
            "img": "https://avatars.githubusercontent.com/u/1844662?s=40&v=4",
            "owner": "Azure-Samples",
            "repo_name": "semantic-kernel-advanced-usage"
          },
          {
            "name": "sonnhfit/SonAgent",
            "stars": 31,
            "img": "https://avatars.githubusercontent.com/u/38657002?s=40&v=4",
            "owner": "sonnhfit",
            "repo_name": "SonAgent"
          },
          {
            "name": "Azure-Samples/aistudio-python-quickstart-sample",
            "stars": 26,
            "img": "https://avatars.githubusercontent.com/u/1844662?s=40&v=4",
            "owner": "Azure-Samples",
            "repo_name": "aistudio-python-quickstart-sample"
          },
          {
            "name": "alexchaomander/semantic-kernel-v1.0-hackathon",
            "stars": 25,
            "img": "https://avatars.githubusercontent.com/u/5111035?s=40&v=4",
            "owner": "alexchaomander",
            "repo_name": "semantic-kernel-v1.0-hackathon"
          },
          {
            "name": "Josephrp/LablabAutogen",
            "stars": 25,
            "img": "https://avatars.githubusercontent.com/u/18212928?s=40&v=4",
            "owner": "Josephrp",
            "repo_name": "LablabAutogen"
          },
          {
            "name": "denniszielke/agentic-playground",
            "stars": 23,
            "img": "https://avatars.githubusercontent.com/u/11569044?s=40&v=4",
            "owner": "denniszielke",
            "repo_name": "agentic-playground"
          },
          {
            "name": "Azure/agent-innovator-lab",
            "stars": 23,
            "img": "https://avatars.githubusercontent.com/u/6844498?s=40&v=4",
            "owner": "Azure",
            "repo_name": "agent-innovator-lab"
          },
          {
            "name": "Azure-Samples/ai-multi-agent-presentation-builder",
            "stars": 22,
            "img": "https://avatars.githubusercontent.com/u/1844662?s=40&v=4",
            "owner": "Azure-Samples",
            "repo_name": "ai-multi-agent-presentation-builder"
          },
          {
            "name": "farzad528/azure-ai-agents-playground",
            "stars": 21,
            "img": "https://avatars.githubusercontent.com/u/40604067?s=40&v=4",
            "owner": "farzad528",
            "repo_name": "azure-ai-agents-playground"
          },
          {
            "name": "microsoft/chimera",
            "stars": 21,
            "img": "https://avatars.githubusercontent.com/u/6154722?s=40&v=4",
            "owner": "microsoft",
            "repo_name": "chimera"
          },
          {
            "name": "HenriSchulte-MS/FlightBookingWithAIAgents",
            "stars": 21,
            "img": "https://avatars.githubusercontent.com/u/77101781?s=40&v=4",
            "owner": "HenriSchulte-MS",
            "repo_name": "FlightBookingWithAIAgents"
          },
          {
            "name": "Azure-Samples/az-ai-kickstarter",
            "stars": 20,
            "img": "https://avatars.githubusercontent.com/u/1844662?s=40&v=4",
            "owner": "Azure-Samples",
            "repo_name": "az-ai-kickstarter"
          },
          {
            "name": "Ori-Replication/MetaGPT_webui",
            "stars": 20,
            "img": "https://avatars.githubusercontent.com/u/109295054?s=40&v=4",
            "owner": "Ori-Replication",
            "repo_name": "MetaGPT_webui"
          },
          {
            "name": "Azure-Samples/nlp-sql-in-a-box",
            "stars": 19,
            "img": "https://avatars.githubusercontent.com/u/1844662?s=40&v=4",
            "owner": "Azure-Samples",
            "repo_name": "nlp-sql-in-a-box"
          },
          {
            "name": "lfbraz/semantic-kernel-nltosql",
            "stars": 19,
            "img": "https://avatars.githubusercontent.com/u/15925853?s=40&v=4",
            "owner": "lfbraz",
            "repo_name": "semantic-kernel-nltosql"
          },
          {
            "name": "irarainey/az-copilot",
            "stars": 18,
            "img": "https://avatars.githubusercontent.com/u/74962459?s=40&v=4",
            "owner": "irarainey",
            "repo_name": "az-copilot"
          },
          {
            "name": "joowon-dm-snu/fastcampus-chatgpt-intro-frameworks",
            "stars": 18,
            "img": "https://avatars.githubusercontent.com/u/69464062?s=40&v=4",
            "owner": "joowon-dm-snu",
            "repo_name": "fastcampus-chatgpt-intro-frameworks"
          },
          {
            "name": "chriscrcodes/talk-to-your-factory",
            "stars": 17,
            "img": "https://avatars.githubusercontent.com/u/46497089?s=40&v=4",
            "owner": "chriscrcodes",
            "repo_name": "talk-to-your-factory"
          },
          {
            "name": "alfredzouang/CognitiveSearchChatGPTDemo",
            "stars": 17,
            "img": "https://avatars.githubusercontent.com/u/5335164?s=40&v=4",
            "owner": "alfredzouang",
            "repo_name": "CognitiveSearchChatGPTDemo"
          },
          {
            "name": "aymenfurter/azure-transcript-search-openai-demo",
            "stars": 16,
            "img": "https://avatars.githubusercontent.com/u/20464460?s=40&v=4",
            "owner": "aymenfurter",
            "repo_name": "azure-transcript-search-openai-demo"
          },
          {
            "name": "Azure-Samples/multi-agent-workshop",
            "stars": 15,
            "img": "https://avatars.githubusercontent.com/u/1844662?s=40&v=4",
            "owner": "Azure-Samples",
            "repo_name": "multi-agent-workshop"
          },
          {
            "name": "kevon217/custom-agency-swarms",
            "stars": 15,
            "img": "https://avatars.githubusercontent.com/u/13077896?s=40&v=4",
            "owner": "kevon217",
            "repo_name": "custom-agency-swarms"
          },
          {
            "name": "johnmaeda/pizzashop-and-ai",
            "stars": 15,
            "img": "https://avatars.githubusercontent.com/u/2140796?s=40&v=4",
            "owner": "johnmaeda",
            "repo_name": "pizzashop-and-ai"
          },
          {
            "name": "Azure-Samples/llm-agent-ops-toolkit-sk",
            "stars": 14,
            "img": "https://avatars.githubusercontent.com/u/1844662?s=40&v=4",
            "owner": "Azure-Samples",
            "repo_name": "llm-agent-ops-toolkit-sk"
          },
          {
            "name": "Azure-Samples/container-apps-dynamic-sessions-samples",
            "stars": 14,
            "img": "https://avatars.githubusercontent.com/u/1844662?s=40&v=4",
            "owner": "Azure-Samples",
            "repo_name": "container-apps-dynamic-sessions-samples"
          },
          {
            "name": "msalemor/llm-use-cases",
            "stars": 14,
            "img": "https://avatars.githubusercontent.com/u/44649358?s=40&v=4",
            "owner": "msalemor",
            "repo_name": "llm-use-cases"
          },
          {
            "name": "microsoft/agents-humanoversight",
            "stars": 13,
            "img": "https://avatars.githubusercontent.com/u/6154722?s=40&v=4",
            "owner": "microsoft",
            "repo_name": "agents-humanoversight"
          },
          {
            "name": "THUDM/DataSciBench",
            "stars": 13,
            "img": "https://avatars.githubusercontent.com/u/48590610?s=40&v=4",
            "owner": "THUDM",
            "repo_name": "DataSciBench"
          },
          {
            "name": "Azure-Samples/sk-advanced-orchestration",
            "stars": 13,
            "img": "https://avatars.githubusercontent.com/u/1844662?s=40&v=4",
            "owner": "Azure-Samples",
            "repo_name": "sk-advanced-orchestration"
          },
          {
            "name": "wmeints/effective-llm-applications",
            "stars": 13,
            "img": "https://avatars.githubusercontent.com/u/1550763?s=40&v=4",
            "owner": "wmeints",
            "repo_name": "effective-llm-applications"
          },
          {
            "name": "microsoft/sk-workshop",
            "stars": 13,
            "img": "https://avatars.githubusercontent.com/u/6154722?s=40&v=4",
            "owner": "microsoft",
            "repo_name": "sk-workshop"
          },
          {
            "name": "kmavrodis/noRAG-multiagent-doc-qna",
            "stars": 12,
            "img": "https://avatars.githubusercontent.com/u/156899979?s=40&v=4",
            "owner": "kmavrodis",
            "repo_name": "noRAG-multiagent-doc-qna"
          },
          {
            "name": "smile-wingbow/MihaGPT",
            "stars": 12,
            "img": "https://avatars.githubusercontent.com/u/174078102?s=40&v=4",
            "owner": "smile-wingbow",
            "repo_name": "MihaGPT"
          },
          {
            "name": "evmin/az-ai-kickstarter",
            "stars": 11,
            "img": "https://avatars.githubusercontent.com/u/5507517?s=40&v=4",
            "owner": "evmin",
            "repo_name": "az-ai-kickstarter"
          },
          {
            "name": "microsoft/kagami",
            "stars": 11,
            "img": "https://avatars.githubusercontent.com/u/6154722?s=40&v=4",
            "owner": "microsoft",
            "repo_name": "kagami"
          },
          {
            "name": "msalemor/adventureworks-viewer-ai",
            "stars": 11,
            "img": "https://avatars.githubusercontent.com/u/44649358?s=40&v=4",
            "owner": "msalemor",
            "repo_name": "adventureworks-viewer-ai"
          },
          {
            "name": "Joining-AI/JoinAgent",
            "stars": 11,
            "img": "https://avatars.githubusercontent.com/u/154000096?s=40&v=4",
            "owner": "Joining-AI",
            "repo_name": "JoinAgent"
          },
          {
            "name": "chanchimin/AgentMonitor",
            "stars": 10,
            "img": "https://avatars.githubusercontent.com/u/75533759?s=40&v=4",
            "owner": "chanchimin",
            "repo_name": "AgentMonitor"
          },
          {
            "name": "antronic/maid_semantic_kernel",
            "stars": 10,
            "img": "https://avatars.githubusercontent.com/u/2222477?s=40&v=4",
            "owner": "antronic",
            "repo_name": "maid_semantic_kernel"
          },
          {
            "name": "alfredodeza/azure-chat-demo",
            "stars": 10,
            "img": "https://avatars.githubusercontent.com/u/317847?s=40&v=4",
            "owner": "alfredodeza",
            "repo_name": "azure-chat-demo"
          },
          {
            "name": "Shailender-Youtube/Multi-Agent-Semantic-Kernel-Azure-Agent-Service-Python",
            "stars": 9,
            "img": "https://avatars.githubusercontent.com/u/174622601?s=40&v=4",
            "owner": "Shailender-Youtube",
            "repo_name": "Multi-Agent-Semantic-Kernel-Azure-Agent-Service-Python"
          },
          {
            "name": "Azure-Samples/teams-chat-with-your-data-solution-accelerator",
            "stars": 9,
            "img": "https://avatars.githubusercontent.com/u/1844662?s=40&v=4",
            "owner": "Azure-Samples",
            "repo_name": "teams-chat-with-your-data-solution-accelerator"
          },
          {
            "name": "sdta25196/good-good-study-day-day-up",
            "stars": 9,
            "img": "https://avatars.githubusercontent.com/u/31121105?s=40&v=4",
            "owner": "sdta25196",
            "repo_name": "good-good-study-day-day-up"
          },
          {
            "name": "Boykai/octo-microhack-rag-ai-and-your-data",
            "stars": 8,
            "img": "https://avatars.githubusercontent.com/u/9115570?s=40&v=4",
            "owner": "Boykai",
            "repo_name": "octo-microhack-rag-ai-and-your-data"
          },
          {
            "name": "jbernec/gen-ai-prototypes",
            "stars": 8,
            "img": "https://avatars.githubusercontent.com/u/92888747?s=40&v=4",
            "owner": "jbernec",
            "repo_name": "gen-ai-prototypes"
          },
          {
            "name": "IntelliTect/EssentialCSharp.Web",
            "stars": 8,
            "img": "https://avatars.githubusercontent.com/u/793568?s=40&v=4",
            "owner": "IntelliTect",
            "repo_name": "EssentialCSharp.Web"
          },
          {
            "name": "rashedtalukder/sharepoint-azure-openai-rag",
            "stars": 8,
            "img": "https://avatars.githubusercontent.com/u/9218468?s=40&v=4",
            "owner": "rashedtalukder",
            "repo_name": "sharepoint-azure-openai-rag"
          },
          {
            "name": "alkampfergit/SemanticKernelPlayground",
            "stars": 8,
            "img": "https://avatars.githubusercontent.com/u/358545?s=40&v=4",
            "owner": "alkampfergit",
            "repo_name": "SemanticKernelPlayground"
          },
          {
            "name": "briancabbott/ChatNow",
            "stars": 8,
            "img": "https://avatars.githubusercontent.com/u/2147648?s=40&v=4",
            "owner": "briancabbott",
            "repo_name": "ChatNow"
          },
          {
            "name": "Azure/azure-ai-agents-labs",
            "stars": 7,
            "img": "https://avatars.githubusercontent.com/u/6844498?s=40&v=4",
            "owner": "Azure",
            "repo_name": "azure-ai-agents-labs"
          },
          {
            "name": "ShivamGoyal03/RoamMind",
            "stars": 7,
            "img": "https://avatars.githubusercontent.com/u/93383164?s=40&v=4",
            "owner": "ShivamGoyal03",
            "repo_name": "RoamMind"
          },
          {
            "name": "microsoft/Hygeia",
            "stars": 7,
            "img": "https://avatars.githubusercontent.com/u/6154722?s=40&v=4",
            "owner": "microsoft",
            "repo_name": "Hygeia"
          },
          {
            "name": "wunderwuzzi23/wuzzi-chat",
            "stars": 7,
            "img": "https://avatars.githubusercontent.com/u/35349594?s=40&v=4",
            "owner": "wunderwuzzi23",
            "repo_name": "wuzzi-chat"
          },
          {
            "name": "ROBROICH/intro-to-intelligent-apps",
            "stars": 7,
            "img": "https://avatars.githubusercontent.com/u/51105958?s=40&v=4",
            "owner": "ROBROICH",
            "repo_name": "intro-to-intelligent-apps"
          },
          {
            "name": "kimtth/azure-openai-llm-cookbook",
            "stars": 6,
            "img": "https://avatars.githubusercontent.com/u/13846660?s=40&v=4",
            "owner": "kimtth",
            "repo_name": "azure-openai-llm-cookbook"
          },
          {
            "name": "luckypamula/azure-ai-agents-labs",
            "stars": 6,
            "img": "https://avatars.githubusercontent.com/u/26860411?s=40&v=4",
            "owner": "luckypamula",
            "repo_name": "azure-ai-agents-labs"
          },
          {
            "name": "microsoft/llmsecops-hands-on-lab",
            "stars": 6,
            "img": "https://avatars.githubusercontent.com/u/6154722?s=40&v=4",
            "owner": "microsoft",
            "repo_name": "llmsecops-hands-on-lab"
          },
          {
            "name": "ptbdnr/snippets",
            "stars": 6,
            "img": "https://avatars.githubusercontent.com/u/32688266?s=40&v=4",
            "owner": "ptbdnr",
            "repo_name": "snippets"
          },
          {
            "name": "pablosalvador10/gbbai-azure-ai-agentic-frameworks",
            "stars": 6,
            "img": "https://avatars.githubusercontent.com/u/31255154?s=40&v=4",
            "owner": "pablosalvador10",
            "repo_name": "gbbai-azure-ai-agentic-frameworks"
          },
          {
            "name": "microsoft/asclepius",
            "stars": 6,
            "img": "https://avatars.githubusercontent.com/u/6154722?s=40&v=4",
            "owner": "microsoft",
            "repo_name": "asclepius"
          },
          {
            "name": "microsoft/polonius",
            "stars": 6,
            "img": "https://avatars.githubusercontent.com/u/6154722?s=40&v=4",
            "owner": "microsoft",
            "repo_name": "polonius"
          },
          {
            "name": "Geniusning/AI",
            "stars": 6,
            "img": "https://avatars.githubusercontent.com/u/27037651?s=40&v=4",
            "owner": "Geniusning",
            "repo_name": "AI"
          },
          {
            "name": "minerva-ed/MinervAI",
            "stars": 6,
            "img": "https://avatars.githubusercontent.com/u/150646217?s=40&v=4",
            "owner": "minerva-ed",
            "repo_name": "MinervAI"
          },
          {
            "name": "embedelite/embedelite-sk-plugin",
            "stars": 6,
            "img": "https://avatars.githubusercontent.com/u/128965123?s=40&v=4",
            "owner": "embedelite",
            "repo_name": "embedelite-sk-plugin"
          },
          {
            "name": "jplck/from-single-to-multi-agent",
            "stars": 5,
            "img": "https://avatars.githubusercontent.com/u/8427174?s=40&v=4",
            "owner": "jplck",
            "repo_name": "from-single-to-multi-agent"
          },
          {
            "name": "MSDLLCpapers/teal-agents",
            "stars": 5,
            "img": "https://avatars.githubusercontent.com/u/137432561?s=40&v=4",
            "owner": "MSDLLCpapers",
            "repo_name": "teal-agents"
          },
          {
            "name": "adamhockemeyer/chat-with-my-apis",
            "stars": 5,
            "img": "https://avatars.githubusercontent.com/u/20075292?s=40&v=4",
            "owner": "adamhockemeyer",
            "repo_name": "chat-with-my-apis"
          },
          {
            "name": "lukaskellerstein/ai",
            "stars": 5,
            "img": "https://avatars.githubusercontent.com/u/12882519?s=40&v=4",
            "owner": "lukaskellerstein",
            "repo_name": "ai"
          },
          {
            "name": "microsoft/mlops-llm-application-service",
            "stars": 5,
            "img": "https://avatars.githubusercontent.com/u/6154722?s=40&v=4",
            "owner": "microsoft",
            "repo_name": "mlops-llm-application-service"
          },
          {
            "name": "Hakurei-Reimu-Gensokyo/text2sqlWithRHLF_MAC_SQL_2024",
            "stars": 5,
            "img": "https://avatars.githubusercontent.com/u/42296668?s=40&v=4",
            "owner": "Hakurei-Reimu-Gensokyo",
            "repo_name": "text2sqlWithRHLF_MAC_SQL_2024"
          },
          {
            "name": "pigg-ai/conductor",
            "stars": 5,
            "img": "https://avatars.githubusercontent.com/u/178526897?s=40&v=4",
            "owner": "pigg-ai",
            "repo_name": "conductor"
          },
          {
            "name": "Joining-AI/RepoAnnotator",
            "stars": 5,
            "img": "https://avatars.githubusercontent.com/u/154000096?s=40&v=4",
            "owner": "Joining-AI",
            "repo_name": "RepoAnnotator"
          },
          {
            "name": "renepajta/intro-to-intelligent-apps",
            "stars": 5,
            "img": "https://avatars.githubusercontent.com/u/6043628?s=40&v=4",
            "owner": "renepajta",
            "repo_name": "intro-to-intelligent-apps"
          },
          {
            "name": "rajib76/semantic_kernel_examples",
            "stars": 5,
            "img": "https://avatars.githubusercontent.com/u/16340036?s=40&v=4",
            "owner": "rajib76",
            "repo_name": "semantic_kernel_examples"
          },
          {
            "name": "MIBlue119/storystudio",
            "stars": 5,
            "img": "https://avatars.githubusercontent.com/u/9137836?s=40&v=4",
            "owner": "MIBlue119",
            "repo_name": "storystudio"
          },
          {
            "name": "embedelite/sk-hackathon",
            "stars": 5,
            "img": "https://avatars.githubusercontent.com/u/128965123?s=40&v=4",
            "owner": "embedelite",
            "repo_name": "sk-hackathon"
          },
          {
            "name": "ayushib4/smartFin",
            "stars": 5,
            "img": "https://avatars.githubusercontent.com/u/62727780?s=40&v=4",
            "owner": "ayushib4",
            "repo_name": "smartFin"
          },
          {
            "name": "arashaga/agents",
            "stars": 4,
            "img": "https://avatars.githubusercontent.com/u/1166344?s=40&v=4",
            "owner": "arashaga",
            "repo_name": "agents"
          },
          {
            "name": "microsoft/contractor",
            "stars": 4,
            "img": "https://avatars.githubusercontent.com/u/6154722?s=40&v=4",
            "owner": "microsoft",
            "repo_name": "contractor"
          },
          {
            "name": "samitugal/semantic-kernel-plugins",
            "stars": 4,
            "img": "https://avatars.githubusercontent.com/u/57317518?s=40&v=4",
            "owner": "samitugal",
            "repo_name": "semantic-kernel-plugins"
          },
          {
            "name": "microsoft/Modernize-your-code-solution-accelerator",
            "stars": 4,
            "img": "https://avatars.githubusercontent.com/u/6154722?s=40&v=4",
            "owner": "microsoft",
            "repo_name": "Modernize-your-code-solution-accelerator"
          },
          {
            "name": "microsoft/multi-agent-calibrator",
            "stars": 4,
            "img": "https://avatars.githubusercontent.com/u/6154722?s=40&v=4",
            "owner": "microsoft",
            "repo_name": "multi-agent-calibrator"
          },
          {
            "name": "FabianSchurig/promptflow-tool-semantic-kernel",
            "stars": 4,
            "img": "https://avatars.githubusercontent.com/u/11216482?s=40&v=4",
            "owner": "FabianSchurig",
            "repo_name": "promptflow-tool-semantic-kernel"
          },
          {
            "name": "mxagar/generative_ai_udacity",
            "stars": 4,
            "img": "https://avatars.githubusercontent.com/u/7673198?s=40&v=4",
            "owner": "mxagar",
            "repo_name": "generative_ai_udacity"
          },
          {
            "name": "Azure/gpt-rag-securityhub",
            "stars": 4,
            "img": "https://avatars.githubusercontent.com/u/6844498?s=40&v=4",
            "owner": "Azure",
            "repo_name": "gpt-rag-securityhub"
          },
          {
            "name": "rashedtalukder/ai-gbb-learning-shop",
            "stars": 4,
            "img": "https://avatars.githubusercontent.com/u/9218468?s=40&v=4",
            "owner": "rashedtalukder",
            "repo_name": "ai-gbb-learning-shop"
          },
          {
            "name": "Space3D-Bench/RAG3D-Chat",
            "stars": 4,
            "img": "https://avatars.githubusercontent.com/u/178774314?s=40&v=4",
            "owner": "Space3D-Bench",
            "repo_name": "RAG3D-Chat"
          },
          {
            "name": "Zhou-CyberSecurity-AI/ATBA",
            "stars": 4,
            "img": "https://avatars.githubusercontent.com/u/35444743?s=40&v=4",
            "owner": "Zhou-CyberSecurity-AI",
            "repo_name": "ATBA"
          },
          {
            "name": "gbaeke/semantic-kernel-demo",
            "stars": 4,
            "img": "https://avatars.githubusercontent.com/u/8395842?s=40&v=4",
            "owner": "gbaeke",
            "repo_name": "semantic-kernel-demo"
          },
          {
            "name": "WuJunde/werewolf_ai_agents",
            "stars": 4,
            "img": "https://avatars.githubusercontent.com/u/23454271?s=40&v=4",
            "owner": "WuJunde",
            "repo_name": "werewolf_ai_agents"
          },
          {
            "name": "dmavani25/MinervAI",
            "stars": 4,
            "img": "https://avatars.githubusercontent.com/u/89651168?s=40&v=4",
            "owner": "dmavani25",
            "repo_name": "MinervAI"
          },
          {
            "name": "MG-Cafe/SemanticKernel_PromptFlow",
            "stars": 4,
            "img": "https://avatars.githubusercontent.com/u/79723187?s=40&v=4",
            "owner": "MG-Cafe",
            "repo_name": "SemanticKernel_PromptFlow"
          },
          {
            "name": "JustinMeimar/hack-gpt",
            "stars": 4,
            "img": "https://avatars.githubusercontent.com/u/32406396?s=40&v=4",
            "owner": "JustinMeimar",
            "repo_name": "hack-gpt"
          },
          {
            "name": "2025NKUCS-agent/NKK-GPT",
            "stars": 3,
            "img": "https://avatars.githubusercontent.com/u/204149293?s=40&v=4",
            "owner": "2025NKUCS-agent",
            "repo_name": "NKK-GPT"
          },
          {
            "name": "rodanthi-alexiou/Devoxx2025-AzureAIAgents",
            "stars": 3,
            "img": "https://avatars.githubusercontent.com/u/61449673?s=40&v=4",
            "owner": "rodanthi-alexiou",
            "repo_name": "Devoxx2025-AzureAIAgents"
          },
          {
            "name": "mdsiprojects/agenticai",
            "stars": 3,
            "img": "https://avatars.githubusercontent.com/u/73212683?s=40&v=4",
            "owner": "mdsiprojects",
            "repo_name": "agenticai"
          },
          {
            "name": "corticalstack/azure-ai-foundry-examples",
            "stars": 3,
            "img": "https://avatars.githubusercontent.com/u/3995321?s=40&v=4",
            "owner": "corticalstack",
            "repo_name": "azure-ai-foundry-examples"
          },
          {
            "name": "vikasgautam18/agentic_ai_with_semantic_kernel",
            "stars": 3,
            "img": "https://avatars.githubusercontent.com/u/39011958?s=40&v=4",
            "owner": "vikasgautam18",
            "repo_name": "agentic_ai_with_semantic_kernel"
          },
          {
            "name": "sambanova/integrations",
            "stars": 3,
            "img": "https://avatars.githubusercontent.com/u/80118584?s=40&v=4",
            "owner": "sambanova",
            "repo_name": "integrations"
          },
          {
            "name": "denniszielke/ai-agents-workshop",
            "stars": 3,
            "img": "https://avatars.githubusercontent.com/u/11569044?s=40&v=4",
            "owner": "denniszielke",
            "repo_name": "ai-agents-workshop"
          },
          {
            "name": "whiteduck-training/wd-ai-hackathon",
            "stars": 3,
            "img": "https://avatars.githubusercontent.com/u/185513507?s=40&v=4",
            "owner": "whiteduck-training",
            "repo_name": "wd-ai-hackathon"
          },
          {
            "name": "alphavector/all",
            "stars": 3,
            "img": "https://avatars.githubusercontent.com/u/11805788?s=40&v=4",
            "owner": "alphavector",
            "repo_name": "all"
          },
          {
            "name": "colincmac/ai-rag-techniques-on-azure",
            "stars": 3,
            "img": "https://avatars.githubusercontent.com/u/99688332?s=40&v=4",
            "owner": "colincmac",
            "repo_name": "ai-rag-techniques-on-azure"
          },
          {
            "name": "Agentic-Insights/sk-python-labs",
            "stars": 3,
            "img": "https://avatars.githubusercontent.com/u/171188275?s=40&v=4",
            "owner": "Agentic-Insights",
            "repo_name": "sk-python-labs"
          },
          {
            "name": "khchan/building-blocks-ai",
            "stars": 3,
            "img": "https://avatars.githubusercontent.com/u/1270511?s=40&v=4",
            "owner": "khchan",
            "repo_name": "building-blocks-ai"
          },
          {
            "name": "jakeatmsft/promptflow_patterns",
            "stars": 3,
            "img": "https://avatars.githubusercontent.com/u/47987698?s=40&v=4",
            "owner": "jakeatmsft",
            "repo_name": "promptflow_patterns"
          },
          {
            "name": "Lyn4ever29/pipy_server",
            "stars": 3,
            "img": "https://avatars.githubusercontent.com/u/25952589?s=40&v=4",
            "owner": "Lyn4ever29",
            "repo_name": "pipy_server"
          },
          {
            "name": "XpiritBV/AzOpenAI-RAG-ChatApp",
            "stars": 3,
            "img": "https://avatars.githubusercontent.com/u/9567984?s=40&v=4",
            "owner": "XpiritBV",
            "repo_name": "AzOpenAI-RAG-ChatApp"
          },
          {
            "name": "matthewbolanos/sk-party-planning-committee",
            "stars": 3,
            "img": "https://avatars.githubusercontent.com/u/1409190?s=40&v=4",
            "owner": "matthewbolanos",
            "repo_name": "sk-party-planning-committee"
          },
          {
            "name": "akshata29/aoaiworkshop",
            "stars": 3,
            "img": "https://avatars.githubusercontent.com/u/18509807?s=40&v=4",
            "owner": "akshata29",
            "repo_name": "aoaiworkshop"
          },
          {
            "name": "jordanbean-msft/semantic-kernel-intent",
            "stars": 3,
            "img": "https://avatars.githubusercontent.com/u/84806584?s=40&v=4",
            "owner": "jordanbean-msft",
            "repo_name": "semantic-kernel-intent"
          },
          {
            "name": "ai-workshop-ise/ai-hands-on-lab",
            "stars": 3,
            "img": "https://avatars.githubusercontent.com/u/157475392?s=40&v=4",
            "owner": "ai-workshop-ise",
            "repo_name": "ai-hands-on-lab"
          },
          {
            "name": "Josephrp/trueralablabsubmission",
            "stars": 3,
            "img": "https://avatars.githubusercontent.com/u/18212928?s=40&v=4",
            "owner": "Josephrp",
            "repo_name": "trueralablabsubmission"
          },
          {
            "name": "msalemor/sk-summarizer-pattern",
            "stars": 3,
            "img": "https://avatars.githubusercontent.com/u/44649358?s=40&v=4",
            "owner": "msalemor",
            "repo_name": "sk-summarizer-pattern"
          },
          {
            "name": "aymenfurter/azure-chat-with-your-photos-demo",
            "stars": 3,
            "img": "https://avatars.githubusercontent.com/u/20464460?s=40&v=4",
            "owner": "aymenfurter",
            "repo_name": "azure-chat-with-your-photos-demo"
          },
          {
            "name": "passadis/react-semantic-kernel",
            "stars": 3,
            "img": "https://avatars.githubusercontent.com/u/53148138?s=40&v=4",
            "owner": "passadis",
            "repo_name": "react-semantic-kernel"
          },
          {
            "name": "Azure-For-Everyone/sk-hotelfiltering",
            "stars": 3,
            "img": "https://avatars.githubusercontent.com/u/104997505?s=40&v=4",
            "owner": "Azure-For-Everyone",
            "repo_name": "sk-hotelfiltering"
          },
          {
            "name": "Ananya-AJ/Chest-Xray_Medical_Report_generation",
            "stars": 3,
            "img": "https://avatars.githubusercontent.com/u/111623197?s=40&v=4",
            "owner": "Ananya-AJ",
            "repo_name": "Chest-Xray_Medical_Report_generation"
          },
          {
            "name": "jonathanscholtes/azure-ai-foundry-agentic-workshop",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/4681774?s=40&v=4",
            "owner": "jonathanscholtes",
            "repo_name": "azure-ai-foundry-agentic-workshop"
          },
          {
            "name": "microsoft/microhacks-rag-ai",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/6154722?s=40&v=4",
            "owner": "microsoft",
            "repo_name": "microhacks-rag-ai"
          },
          {
            "name": "edemnati/hackathon-may-2025",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/22805269?s=40&v=4",
            "owner": "edemnati",
            "repo_name": "hackathon-may-2025"
          },
          {
            "name": "Qredence/Agentic-Kernel",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/148253410?s=40&v=4",
            "owner": "Qredence",
            "repo_name": "Agentic-Kernel"
          },
          {
            "name": "HosseinZahed/multi-agent-cloud-architect",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/19933353?s=40&v=4",
            "owner": "HosseinZahed",
            "repo_name": "multi-agent-cloud-architect"
          },
          {
            "name": "Azure-Samples/analyst",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/1844662?s=40&v=4",
            "owner": "Azure-Samples",
            "repo_name": "analyst"
          },
          {
            "name": "ambarishg/SEMANTIC-KERNEL-AGENTS",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/3221164?s=40&v=4",
            "owner": "ambarishg",
            "repo_name": "SEMANTIC-KERNEL-AGENTS"
          },
          {
            "name": "karan-pers/realtime-audio-acs",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/203891145?s=40&v=4",
            "owner": "karan-pers",
            "repo_name": "realtime-audio-acs"
          },
          {
            "name": "ckane/ctigor",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/3454769?s=40&v=4",
            "owner": "ckane",
            "repo_name": "ctigor"
          },
          {
            "name": "Azure-Samples/tutor",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/1844662?s=40&v=4",
            "owner": "Azure-Samples",
            "repo_name": "tutor"
          },
          {
            "name": "hifaz1012/multi-agents-healthcare-insurance",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/6141650?s=40&v=4",
            "owner": "hifaz1012",
            "repo_name": "multi-agents-healthcare-insurance"
          },
          {
            "name": "wbingli/ai-agent-research",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/390168?s=40&v=4",
            "owner": "wbingli",
            "repo_name": "ai-agent-research"
          },
          {
            "name": "kuljotSB/semantic-kernel",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/147621230?s=40&v=4",
            "owner": "kuljotSB",
            "repo_name": "semantic-kernel"
          },
          {
            "name": "ganachan/Project_Maria_Accelerator_tts",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/70560170?s=40&v=4",
            "owner": "ganachan",
            "repo_name": "Project_Maria_Accelerator_tts"
          },
          {
            "name": "dbroeglin/az-ai-scaffolding-test-l300",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/243852?s=40&v=4",
            "owner": "dbroeglin",
            "repo_name": "az-ai-scaffolding-test-l300"
          },
          {
            "name": "samelhousseini/mm_doc_proc",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/108335186?s=40&v=4",
            "owner": "samelhousseini",
            "repo_name": "mm_doc_proc"
          },
          {
            "name": "Tyler-R-Kendrick/learn-sk",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/145080887?s=40&v=4",
            "owner": "Tyler-R-Kendrick",
            "repo_name": "learn-sk"
          },
          {
            "name": "scallighan/labby",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/91623699?s=40&v=4",
            "owner": "scallighan",
            "repo_name": "labby"
          },
          {
            "name": "Thoams0211/Stock-Insight",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/97227379?s=40&v=4",
            "owner": "Thoams0211",
            "repo_name": "Stock-Insight"
          },
          {
            "name": "Joining-AI/JoinQwen",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/154000096?s=40&v=4",
            "owner": "Joining-AI",
            "repo_name": "JoinQwen"
          },
          {
            "name": "Bistu-OSSDT-2024/12-project-12",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/172448273?s=40&v=4",
            "owner": "Bistu-OSSDT-2024",
            "repo_name": "12-project-12"
          },
          {
            "name": "sqlserverworkshops/OpenAI-DataPro",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/48881186?s=40&v=4",
            "owner": "sqlserverworkshops",
            "repo_name": "OpenAI-DataPro"
          },
          {
            "name": "Eightina/arxiv-agent",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/81902383?s=40&v=4",
            "owner": "Eightina",
            "repo_name": "arxiv-agent"
          },
          {
            "name": "georgeIshaq/Q-Hack",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/123267653?s=40&v=4",
            "owner": "georgeIshaq",
            "repo_name": "Q-Hack"
          },
          {
            "name": "synedra/vector_linkedin",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/77410784?s=40&v=4",
            "owner": "synedra",
            "repo_name": "vector_linkedin"
          },
          {
            "name": "Cataldir/semantic-kernel-py-training",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/29005497?s=40&v=4",
            "owner": "Cataldir",
            "repo_name": "semantic-kernel-py-training"
          },
          {
            "name": "Gavinbeauch13/Accommodate",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/100885952?s=40&v=4",
            "owner": "Gavinbeauch13",
            "repo_name": "Accommodate"
          },
          {
            "name": "percebus/openai-sandbox-py",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/5483088?s=40&v=4",
            "owner": "percebus",
            "repo_name": "openai-sandbox-py"
          },
          {
            "name": "teerasej/nextflow-semantic-kernel-python-pycon-2023",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/85179?s=40&v=4",
            "owner": "teerasej",
            "repo_name": "nextflow-semantic-kernel-python-pycon-2023"
          },
          {
            "name": "MTCMarkFranco/python-sql-Interpreter",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/36431095?s=40&v=4",
            "owner": "MTCMarkFranco",
            "repo_name": "python-sql-Interpreter"
          },
          {
            "name": "madebygps/cosmos-vector-aoai",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/6733686?s=40&v=4",
            "owner": "madebygps",
            "repo_name": "cosmos-vector-aoai"
          },
          {
            "name": "areibman/MetaGPT",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/14807319?s=40&v=4",
            "owner": "areibman",
            "repo_name": "MetaGPT"
          },
          {
            "name": "MohammedFadin/semantic-kernel-easier-start",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/2106384?s=40&v=4",
            "owner": "MohammedFadin",
            "repo_name": "semantic-kernel-easier-start"
          },
          {
            "name": "iamjoel/four-dimensional-space-bag",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/2120155?s=40&v=4",
            "owner": "iamjoel",
            "repo_name": "four-dimensional-space-bag"
          },
          {
            "name": "sinhaGuild/sk-playground",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/18661288?s=40&v=4",
            "owner": "sinhaGuild",
            "repo_name": "sk-playground"
          },
          {
            "name": "layogtima/termi",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/119887692?s=40&v=4",
            "owner": "layogtima",
            "repo_name": "termi"
          },
          {
            "name": "ChinmaySheth/TMLSWorkshop",
            "stars": 2,
            "img": "https://avatars.githubusercontent.com/u/25516398?s=40&v=4",
            "owner": "ChinmaySheth",
            "repo_name": "TMLSWorkshop"
          },
          {
            "name": "hamzaelgh/edge-rag",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/198704502?s=40&v=4",
            "owner": "hamzaelgh",
            "repo_name": "edge-rag"
          },
          {
            "name": "saytyarnorngloreia/ai-agents-for-beginners",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/205020882?s=40&v=4",
            "owner": "saytyarnorngloreia",
            "repo_name": "ai-agents-for-beginners"
          },
          {
            "name": "CassandraOfTroy/data-modernization",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/58147404?s=40&v=4",
            "owner": "CassandraOfTroy",
            "repo_name": "data-modernization"
          },
          {
            "name": "jo99112/sg-ai-dev-tools",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/127510384?s=40&v=4",
            "owner": "jo99112",
            "repo_name": "sg-ai-dev-tools"
          },
          {
            "name": "julicjung/ai-agents",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/12299771?s=40&v=4",
            "owner": "julicjung",
            "repo_name": "ai-agents"
          },
          {
            "name": "pablocast/gbbai-ai-foundry-sdk-workshop",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/18387357?s=40&v=4",
            "owner": "pablocast",
            "repo_name": "gbbai-ai-foundry-sdk-workshop"
          },
          {
            "name": "kunanba/agents",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/51249060?s=40&v=4",
            "owner": "kunanba",
            "repo_name": "agents"
          },
          {
            "name": "yingding/AI-Agent-Workshop-2025",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/1073701?s=40&v=4",
            "owner": "yingding",
            "repo_name": "AI-Agent-Workshop-2025"
          },
          {
            "name": "arashaga/agents-hackathon",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/1166344?s=40&v=4",
            "owner": "arashaga",
            "repo_name": "agents-hackathon"
          },
          {
            "name": "shubbham28/financial-agent",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/29838339?s=40&v=4",
            "owner": "shubbham28",
            "repo_name": "financial-agent"
          },
          {
            "name": "sanjeevkumar761/eval-with-azure-eval-sdk",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/1672998?s=40&v=4",
            "owner": "sanjeevkumar761",
            "repo_name": "eval-with-azure-eval-sdk"
          },
          {
            "name": "sifinell/document-analysis-questionnaire-system",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/15127653?s=40&v=4",
            "owner": "sifinell",
            "repo_name": "document-analysis-questionnaire-system"
          },
          {
            "name": "strudel0209/kyc-sk",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/9446504?s=40&v=4",
            "owner": "strudel0209",
            "repo_name": "kyc-sk"
          },
          {
            "name": "mgboot/ai-tutor",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/55988909?s=40&v=4",
            "owner": "mgboot",
            "repo_name": "ai-tutor"
          },
          {
            "name": "Azure/multiagent-frameworks",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/6844498?s=40&v=4",
            "owner": "Azure",
            "repo_name": "multiagent-frameworks"
          },
          {
            "name": "Progressive-Insurance/chainlit",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/33731436?s=40&v=4",
            "owner": "Progressive-Insurance",
            "repo_name": "chainlit"
          },
          {
            "name": "mard-llm/vetbert-basic",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/202657196?s=40&v=4",
            "owner": "mard-llm",
            "repo_name": "vetbert-basic"
          },
          {
            "name": "HeCoded/MetaGPT",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/167261630?s=40&v=4",
            "owner": "HeCoded",
            "repo_name": "MetaGPT"
          },
          {
            "name": "motern88/MetaGPT_Fork",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/171646543?s=40&v=4",
            "owner": "motern88",
            "repo_name": "MetaGPT_Fork"
          },
          {
            "name": "thissideup124/autogenBP",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/160380090?s=40&v=4",
            "owner": "thissideup124",
            "repo_name": "autogenBP"
          },
          {
            "name": "pablocast/gbbai-azure-ai-foundry-multi-agents",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/18387357?s=40&v=4",
            "owner": "pablocast",
            "repo_name": "gbbai-azure-ai-foundry-multi-agents"
          },
          {
            "name": "kanethuong/sample-semantic-kernel",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/80303444?s=40&v=4",
            "owner": "kanethuong",
            "repo_name": "sample-semantic-kernel"
          },
          {
            "name": "XinmiaoYan/ai-agents-for-beginners",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/83053884?s=40&v=4",
            "owner": "XinmiaoYan",
            "repo_name": "ai-agents-for-beginners"
          },
          {
            "name": "onwardplatforms/agently",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/141597825?s=40&v=4",
            "owner": "onwardplatforms",
            "repo_name": "agently"
          },
          {
            "name": "Deepesh1024/Video_Analysis",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/75719377?s=40&v=4",
            "owner": "Deepesh1024",
            "repo_name": "Video_Analysis"
          },
          {
            "name": "mazer-rakham/copilot-intent-history-connector",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/11220261?s=40&v=4",
            "owner": "mazer-rakham",
            "repo_name": "copilot-intent-history-connector"
          },
          {
            "name": "ManufacturingCSU/AI_Agent_Catalog",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/83834200?s=40&v=4",
            "owner": "ManufacturingCSU",
            "repo_name": "AI_Agent_Catalog"
          },
          {
            "name": "pablocast/gbbai-sk-workshop",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/18387357?s=40&v=4",
            "owner": "pablocast",
            "repo_name": "gbbai-sk-workshop"
          },
          {
            "name": "cyberkoolman/fsi-call-center",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/5167152?s=40&v=4",
            "owner": "cyberkoolman",
            "repo_name": "fsi-call-center"
          },
          {
            "name": "onwardplatforms/agent-foundry",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/141597825?s=40&v=4",
            "owner": "onwardplatforms",
            "repo_name": "agent-foundry"
          },
          {
            "name": "saurabhvartak1982/skagents",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/48907314?s=40&v=4",
            "owner": "saurabhvartak1982",
            "repo_name": "skagents"
          },
          {
            "name": "szetinglau/IFU-translation",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/146206163?s=40&v=4",
            "owner": "szetinglau",
            "repo_name": "IFU-translation"
          },
          {
            "name": "Unity-Media-and-social-communication/MetaGPT",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/180525404?s=40&v=4",
            "owner": "Unity-Media-and-social-communication",
            "repo_name": "MetaGPT"
          },
          {
            "name": "JuhyunLee0/multiagent-sk",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/152219307?s=40&v=4",
            "owner": "JuhyunLee0",
            "repo_name": "multiagent-sk"
          },
          {
            "name": "nileshvj2/aoai-poc-hub",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/111443992?s=40&v=4",
            "owner": "nileshvj2",
            "repo_name": "aoai-poc-hub"
          },
          {
            "name": "smaruf/python-study",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/10070242?s=40&v=4",
            "owner": "smaruf",
            "repo_name": "python-study"
          },
          {
            "name": "StevenWangler/Blizzard",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/28370918?s=40&v=4",
            "owner": "StevenWangler",
            "repo_name": "Blizzard"
          },
          {
            "name": "dreamguo/metagpt-werewolf",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/33998583?s=40&v=4",
            "owner": "dreamguo",
            "repo_name": "metagpt-werewolf"
          },
          {
            "name": "gilbertalgordo/semantic-kernel",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/69397216?s=40&v=4",
            "owner": "gilbertalgordo",
            "repo_name": "semantic-kernel"
          },
          {
            "name": "jordanbean-msft/openai-agents-fastapi",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/84806584?s=40&v=4",
            "owner": "jordanbean-msft",
            "repo_name": "openai-agents-fastapi"
          },
          {
            "name": "franciszchen/MAP",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/16402098?s=40&v=4",
            "owner": "franciszchen",
            "repo_name": "MAP"
          },
          {
            "name": "nholuongut/aks-store-demo",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/58627821?s=40&v=4",
            "owner": "nholuongut",
            "repo_name": "aks-store-demo"
          },
          {
            "name": "nacoline/MetaMG400",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/96275268?s=40&v=4",
            "owner": "nacoline",
            "repo_name": "MetaMG400"
          },
          {
            "name": "RalphHightower/semantic-kernel",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/32745442?s=40&v=4",
            "owner": "RalphHightower",
            "repo_name": "semantic-kernel"
          },
          {
            "name": "XiangJinyu/Aflow",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/137690584?s=40&v=4",
            "owner": "XiangJinyu",
            "repo_name": "Aflow"
          },
          {
            "name": "albertaga27/aoai-insurance-rm-vanilla-agents",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/159526192?s=40&v=4",
            "owner": "albertaga27",
            "repo_name": "aoai-insurance-rm-vanilla-agents"
          },
          {
            "name": "Bryan-Roe-ai/semantic-kernel",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/190993195?s=40&v=4",
            "owner": "Bryan-Roe-ai",
            "repo_name": "semantic-kernel"
          },
          {
            "name": "balakreshnan/ConstRFP",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/25058438?s=40&v=4",
            "owner": "balakreshnan",
            "repo_name": "ConstRFP"
          },
          {
            "name": "patooworld/autogen",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/114192133?s=40&v=4",
            "owner": "patooworld",
            "repo_name": "autogen"
          },
          {
            "name": "orellanoyamile/database_chatbot",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/131058655?s=40&v=4",
            "owner": "orellanoyamile",
            "repo_name": "database_chatbot"
          },
          {
            "name": "ms-us-rcg-cloud-innovation/openai-patterns",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/138236455?s=40&v=4",
            "owner": "ms-us-rcg-cloud-innovation",
            "repo_name": "openai-patterns"
          },
          {
            "name": "pnd-andrei/SmartResume",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/76188072?s=40&v=4",
            "owner": "pnd-andrei",
            "repo_name": "SmartResume"
          },
          {
            "name": "Xin-Ray/AiCoach",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/113844301?s=40&v=4",
            "owner": "Xin-Ray",
            "repo_name": "AiCoach"
          },
          {
            "name": "kenichi-segawa/AI-Search-Example",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/54873006?s=40&v=4",
            "owner": "kenichi-segawa",
            "repo_name": "AI-Search-Example"
          },
          {
            "name": "abirharrasse/LLM_Judging_Architectures",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/81148161?s=40&v=4",
            "owner": "abirharrasse",
            "repo_name": "LLM_Judging_Architectures"
          },
          {
            "name": "pablosalvador10/gbb-ai-smart-document-processing",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/31255154?s=40&v=4",
            "owner": "pablosalvador10",
            "repo_name": "gbb-ai-smart-document-processing"
          },
          {
            "name": "pfekrati/langchain-vs-semantickernel",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/106823811?s=40&v=4",
            "owner": "pfekrati",
            "repo_name": "langchain-vs-semantickernel"
          },
          {
            "name": "Imperial-EE-Microsoft/github-app-public",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/169689686?s=40&v=4",
            "owner": "Imperial-EE-Microsoft",
            "repo_name": "github-app-public"
          },
          {
            "name": "umermansoor/llm-ai-coach",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/862952?s=40&v=4",
            "owner": "umermansoor",
            "repo_name": "llm-ai-coach"
          },
          {
            "name": "avidunixuser/AOAICoE",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/2113048?s=40&v=4",
            "owner": "avidunixuser",
            "repo_name": "AOAICoE"
          },
          {
            "name": "YoutacRandS-VA/autogen",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/146207501?s=40&v=4",
            "owner": "YoutacRandS-VA",
            "repo_name": "autogen"
          },
          {
            "name": "sghao/kaggle",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/24216745?s=40&v=4",
            "owner": "sghao",
            "repo_name": "kaggle"
          },
          {
            "name": "meslubi2021/semantic-kernel-starters",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/145295387?s=40&v=4",
            "owner": "meslubi2021",
            "repo_name": "semantic-kernel-starters"
          },
          {
            "name": "meslubi2021/semantic-kernel",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/145295387?s=40&v=4",
            "owner": "meslubi2021",
            "repo_name": "semantic-kernel"
          },
          {
            "name": "HenriSchulte-MS/Basic-Semantic-Kernel-Copilot",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/77101781?s=40&v=4",
            "owner": "HenriSchulte-MS",
            "repo_name": "Basic-Semantic-Kernel-Copilot"
          },
          {
            "name": "joansoleroig/Predictive-Maintenance-Streamlit-App",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/119736097?s=40&v=4",
            "owner": "joansoleroig",
            "repo_name": "Predictive-Maintenance-Streamlit-App"
          },
          {
            "name": "paaschdigital/autogen",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/100975984?s=40&v=4",
            "owner": "paaschdigital",
            "repo_name": "autogen"
          },
          {
            "name": "jmservera/miscdemos",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/8036360?s=40&v=4",
            "owner": "jmservera",
            "repo_name": "miscdemos"
          },
          {
            "name": "ehimen-io/resume-helper",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/43546100?s=40&v=4",
            "owner": "ehimen-io",
            "repo_name": "resume-helper"
          },
          {
            "name": "yuxiaobopp/metagpt_spider",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/22901629?s=40&v=4",
            "owner": "yuxiaobopp",
            "repo_name": "metagpt_spider"
          },
          {
            "name": "Maximiliano-Villanueva/orchestrator-semantic-kernel-api",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/76200479?s=40&v=4",
            "owner": "Maximiliano-Villanueva",
            "repo_name": "orchestrator-semantic-kernel-api"
          },
          {
            "name": "onhello-automation/otto",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/83440597?s=40&v=4",
            "owner": "onhello-automation",
            "repo_name": "otto"
          },
          {
            "name": "cerberuswilliam/MetaGPT",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/39875358?s=40&v=4",
            "owner": "cerberuswilliam",
            "repo_name": "MetaGPT"
          },
          {
            "name": "luoguomao1995/ai_courseware",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/37164874?s=40&v=4",
            "owner": "luoguomao1995",
            "repo_name": "ai_courseware"
          },
          {
            "name": "shiningwhite-cmd/ViCatcher",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/83100977?s=40&v=4",
            "owner": "shiningwhite-cmd",
            "repo_name": "ViCatcher"
          },
          {
            "name": "cerberuswilliam/worldMetaGPT",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/39875358?s=40&v=4",
            "owner": "cerberuswilliam",
            "repo_name": "worldMetaGPT"
          },
          {
            "name": "nandumsft/promtfloweval",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/123025520?s=40&v=4",
            "owner": "nandumsft",
            "repo_name": "promtfloweval"
          },
          {
            "name": "kimtth/fastapi-oai-showcase",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/13846660?s=40&v=4",
            "owner": "kimtth",
            "repo_name": "fastapi-oai-showcase"
          },
          {
            "name": "SehejSoni/try",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/134284071?s=40&v=4",
            "owner": "SehejSoni",
            "repo_name": "try"
          },
          {
            "name": "kdcllc/nlp-sql-in-a-box",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/13120940?s=40&v=4",
            "owner": "kdcllc",
            "repo_name": "nlp-sql-in-a-box"
          },
          {
            "name": "AymenSegni/ecoai-app",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/28219843?s=40&v=4",
            "owner": "AymenSegni",
            "repo_name": "ecoai-app"
          },
          {
            "name": "martcpp/MetaGPT",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/103220475?s=40&v=4",
            "owner": "martcpp",
            "repo_name": "MetaGPT"
          },
          {
            "name": "dan-redcupit/MetaGPT",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/8456328?s=40&v=4",
            "owner": "dan-redcupit",
            "repo_name": "MetaGPT"
          },
          {
            "name": "Gonzivang/SecondTry",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/29268941?s=40&v=4",
            "owner": "Gonzivang",
            "repo_name": "SecondTry"
          },
          {
            "name": "castillosebastian/genai3",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/17081544?s=40&v=4",
            "owner": "castillosebastian",
            "repo_name": "genai3"
          },
          {
            "name": "msalemor/gpt-assistants",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/44649358?s=40&v=4",
            "owner": "msalemor",
            "repo_name": "gpt-assistants"
          },
          {
            "name": "silarsis/assistant",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/2221181?s=40&v=4",
            "owner": "silarsis",
            "repo_name": "assistant"
          },
          {
            "name": "vinod-soni-microsoft/thinking-out-loud-librarian-whisper",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/117770765?s=40&v=4",
            "owner": "vinod-soni-microsoft",
            "repo_name": "thinking-out-loud-librarian-whisper"
          },
          {
            "name": "vinod-soni-microsoft/chain-of-thought",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/117770765?s=40&v=4",
            "owner": "vinod-soni-microsoft",
            "repo_name": "chain-of-thought"
          },
          {
            "name": "vinod-soni-microsoft/simple-summarizer-with-semantic-kernel",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/117770765?s=40&v=4",
            "owner": "vinod-soni-microsoft",
            "repo_name": "simple-summarizer-with-semantic-kernel"
          },
          {
            "name": "awkwardindustries/starter-sk-python-functions",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/31149154?s=40&v=4",
            "owner": "awkwardindustries",
            "repo_name": "starter-sk-python-functions"
          },
          {
            "name": "rom212/semantic_skill_evaluation",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/54595299?s=40&v=4",
            "owner": "rom212",
            "repo_name": "semantic_skill_evaluation"
          },
          {
            "name": "SMC-Presales-Accelerators/azure-data-chat",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/156377865?s=40&v=4",
            "owner": "SMC-Presales-Accelerators",
            "repo_name": "azure-data-chat"
          },
          {
            "name": "minerva-ed/SustAInSim",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/150646217?s=40&v=4",
            "owner": "minerva-ed",
            "repo_name": "SustAInSim"
          },
          {
            "name": "ajonathan/intro-to-intelligent-apps",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/15071719?s=40&v=4",
            "owner": "ajonathan",
            "repo_name": "intro-to-intelligent-apps"
          },
          {
            "name": "oktay-be/ai-playground",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/55450535?s=40&v=4",
            "owner": "oktay-be",
            "repo_name": "ai-playground"
          },
          {
            "name": "kirenz/lab-langchain-functions",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/48259550?s=40&v=4",
            "owner": "kirenz",
            "repo_name": "lab-langchain-functions"
          },
          {
            "name": "enwask/due-diligence",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/144351164?s=40&v=4",
            "owner": "enwask",
            "repo_name": "due-diligence"
          },
          {
            "name": "sethiaarun/semantic-kernel-example-ChatGPT-Plugin",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/109227248?s=40&v=4",
            "owner": "sethiaarun",
            "repo_name": "semantic-kernel-example-ChatGPT-Plugin"
          },
          {
            "name": "nicholassolomon/DeepLearning.AI.SemanticKernel.Course",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/48126957?s=40&v=4",
            "owner": "nicholassolomon",
            "repo_name": "DeepLearning.AI.SemanticKernel.Course"
          },
          {
            "name": "raffertyuy/RazGPT",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/1037626?s=40&v=4",
            "owner": "raffertyuy",
            "repo_name": "RazGPT"
          },
          {
            "name": "amitpuri/LLM-Text-Completion-Semantic-Kernel",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/6460233?s=40&v=4",
            "owner": "amitpuri",
            "repo_name": "LLM-Text-Completion-Semantic-Kernel"
          },
          {
            "name": "JustinMeimar/hack-gpt-dev",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/32406396?s=40&v=4",
            "owner": "JustinMeimar",
            "repo_name": "hack-gpt-dev"
          },
          {
            "name": "philmui/asdrp2023",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/78625?s=40&v=4",
            "owner": "philmui",
            "repo_name": "asdrp2023"
          },
          {
            "name": "placerda/gpt-oyd-ingestion",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/6265211?s=40&v=4",
            "owner": "placerda",
            "repo_name": "gpt-oyd-ingestion"
          },
          {
            "name": "showpune/windup-prompt",
            "stars": 1,
            "img": "https://avatars.githubusercontent.com/u/1787505?s=40&v=4",
            "owner": "showpune",
            "repo_name": "windup-prompt"
          }
        ],
        "public_dependents_number": 308,
        "private_dependents_number": -308,
        "total_dependents_number": 308,
        "badges": {
          "total": "[![Generated by github-dependents-info](https://img.shields.io/static/v1?label=Used%20by&message=308&color=informational&logo=slickpic)](https://github.com/microsoft/semantic-kernel/network/dependents?package_id=UGFja2FnZS0zNjMxNjM5MDk1)",
          "public": "[![Generated by github-dependents-info](https://img.shields.io/static/v1?label=Used%20by%20(public)&message=308&color=informational&logo=slickpic)](https://github.com/microsoft/semantic-kernel/network/dependents?package_id=UGFja2FnZS0zNjMxNjM5MDk1)",
          "private": "[![Generated by github-dependents-info](https://img.shields.io/static/v1?label=Used%20by%20(private)&message=-308&color=informational&logo=slickpic)](https://github.com/microsoft/semantic-kernel/network/dependents?package_id=UGFja2FnZS0zNjMxNjM5MDk1)",
          "stars": "[![Generated by github-dependents-info](https://img.shields.io/static/v1?label=Used%20by%20(stars)&message=623&color=informational&logo=slickpic)](https://github.com/microsoft/semantic-kernel/network/dependents?package_id=UGFja2FnZS0zNjMxNjM5MDk1)"
        }
      },
      {
        "id": "UGFja2FnZS00MTAxMTkyMjMz",
        "name": "github.com/microsoft/semantic-kernel",
        "url": "https://github.com/microsoft/semantic-kernel/network/dependents?package_id=UGFja2FnZS00MTAxMTkyMjMz",
        "public_dependent_stars": 0,
        "public_dependents": [],
        "public_dependents_number": 0,
        "private_dependents_number": 0,
        "total_dependents_number": 0,
        "badges": {
          "total": "[![Generated by github-dependents-info](https://img.shields.io/static/v1?label=Used%20by&message=0&color=informational&logo=slickpic)](https://github.com/microsoft/semantic-kernel/network/dependents?package_id=UGFja2FnZS00MTAxMTkyMjMz)",
          "public": "[![Generated by github-dependents-info](https://img.shields.io/static/v1?label=Used%20by%20(public)&message=0&color=informational&logo=slickpic)](https://github.com/microsoft/semantic-kernel/network/dependents?package_id=UGFja2FnZS00MTAxMTkyMjMz)",
          "private": "[![Generated by github-dependents-info](https://img.shields.io/static/v1?label=Used%20by%20(private)&message=0&color=informational&logo=slickpic)](https://github.com/microsoft/semantic-kernel/network/dependents?package_id=UGFja2FnZS00MTAxMTkyMjMz)",
          "stars": "[![Generated by github-dependents-info](https://img.shields.io/static/v1?label=Used%20by%20(stars)&message=0&color=informational&logo=slickpic)](https://github.com/microsoft/semantic-kernel/network/dependents?package_id=UGFja2FnZS00MTAxMTkyMjMz)"
        }
      }
    ]
  ]
}